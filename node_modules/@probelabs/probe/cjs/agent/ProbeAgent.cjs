"use strict";
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __esm = (fn, res) => function __init() {
  return fn && (res = (0, fn[__getOwnPropNames(fn)[0]])(fn = 0)), res;
};
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: true }) : target,
  mod
));
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// src/agent/tokenCounter.js
var import_tiktoken, TokenCounter;
var init_tokenCounter = __esm({
  "src/agent/tokenCounter.js"() {
    "use strict";
    import_tiktoken = require("tiktoken");
    TokenCounter = class {
      constructor() {
        try {
          this.tokenizer = (0, import_tiktoken.get_encoding)("cl100k_base");
          this.contextSize = 0;
          this.history = [];
          this.requestTokens = 0;
          this.responseTokens = 0;
          this.currentRequestTokens = 0;
          this.currentResponseTokens = 0;
          this.cacheCreationTokens = 0;
          this.cacheReadTokens = 0;
          this.currentCacheCreationTokens = 0;
          this.currentCacheReadTokens = 0;
          this.cachedPromptTokens = 0;
          this.currentCachedPromptTokens = 0;
        } catch (error) {
          console.error("Error initializing tokenizer:", error);
          this.tokenizer = null;
          this.contextSize = 0;
          this.requestTokens = 0;
          this.responseTokens = 0;
          this.currentRequestTokens = 0;
          this.currentResponseTokens = 0;
          this.cacheCreationTokens = 0;
          this.cacheReadTokens = 0;
          this.currentCacheCreationTokens = 0;
          this.currentCacheReadTokens = 0;
          this.cachedPromptTokens = 0;
          this.currentCachedPromptTokens = 0;
          this.history = [];
        }
        this.debug = process.env.DEBUG === "1";
      }
      /**
       * Count tokens in a string using tiktoken or fallback method
       * @param {string} text - The text to count tokens for
       * @returns {number} - The number of tokens
       */
      countTokens(text) {
        if (typeof text !== "string") {
          text = String(text);
        }
        if (this.tokenizer) {
          try {
            const tokens = this.tokenizer.encode(text);
            return tokens.length;
          } catch (error) {
            return Math.ceil(text.length / 4);
          }
        } else {
          return Math.ceil(text.length / 4);
        }
      }
      /**
       * Add to request token count (manual counting, less used now with recordUsage)
       * @param {string|number} input - The text to count tokens for or the token count directly
       */
      addRequestTokens(input) {
        let tokenCount = 0;
        if (typeof input === "number") {
          tokenCount = input;
        } else if (typeof input === "string") {
          tokenCount = this.countTokens(input);
        } else {
          console.warn("[WARN] Invalid input type for addRequestTokens:", typeof input);
          return;
        }
        this.requestTokens += tokenCount;
        this.currentRequestTokens = tokenCount;
        if (this.debug) {
          console.log(`[DEBUG] (Manual) Added ${tokenCount} request tokens. Total: ${this.requestTokens}, Current: ${this.currentRequestTokens}`);
        }
      }
      /**
       * Add to response token count (manual counting, less used now with recordUsage)
       * @param {string|number} input - The text to count tokens for or the token count directly
       */
      addResponseTokens(input) {
        let tokenCount = 0;
        if (typeof input === "number") {
          tokenCount = input;
        } else if (typeof input === "string") {
          tokenCount = this.countTokens(input);
        } else {
          console.warn("[WARN] Invalid input type for addResponseTokens:", typeof input);
          return;
        }
        this.responseTokens += tokenCount;
        this.currentResponseTokens = tokenCount;
        if (this.debug) {
          console.log(`[DEBUG] (Manual) Added ${tokenCount} response tokens. Total: ${this.responseTokens}, Current: ${this.currentResponseTokens}`);
        }
      }
      /**
       * Record token usage from the AI SDK's result for a single LLM call.
       * This resets 'current' counters and updates totals.
       * @param {Object} usage - The usage object { promptTokens, completionTokens, totalTokens }
       * @param {Object} providerMetadata - Metadata possibly containing cache info
       */
      recordUsage(usage, providerMetadata) {
        if (!usage) {
          console.warn("[WARN] No usage information provided to recordUsage");
          return;
        }
        this.currentRequestTokens = 0;
        this.currentResponseTokens = 0;
        this.currentCacheCreationTokens = 0;
        this.currentCacheReadTokens = 0;
        this.currentCachedPromptTokens = 0;
        const promptTokens = Number(usage.promptTokens) || 0;
        const completionTokens = Number(usage.completionTokens) || 0;
        this.currentRequestTokens = promptTokens;
        this.currentResponseTokens = completionTokens;
        this.requestTokens += promptTokens;
        this.responseTokens += completionTokens;
        if (providerMetadata?.anthropic) {
          const cacheCreation = Number(providerMetadata.anthropic.cacheCreationInputTokens) || 0;
          const cacheRead = Number(providerMetadata.anthropic.cacheReadInputTokens) || 0;
          this.currentCacheCreationTokens = cacheCreation;
          this.currentCacheReadTokens = cacheRead;
          this.cacheCreationTokens += cacheCreation;
          this.cacheReadTokens += cacheRead;
          if (this.debug) {
            console.log(`[DEBUG] Anthropic cache tokens (current): creation=${cacheCreation}, read=${cacheRead}`);
          }
        }
        if (providerMetadata?.openai) {
          const cachedPrompt = Number(providerMetadata.openai.cachedPromptTokens) || 0;
          this.currentCachedPromptTokens = cachedPrompt;
          this.cachedPromptTokens += cachedPrompt;
          if (this.debug) {
            console.log(`[DEBUG] OpenAI cached prompt tokens (current): ${cachedPrompt}`);
          }
        }
        if (this.debug) {
          console.log(
            `[DEBUG] Recorded usage: current(req=${this.currentRequestTokens}, resp=${this.currentResponseTokens}), total(req=${this.requestTokens}, resp=${this.responseTokens})`
          );
          console.log(`[DEBUG] Total cache tokens: Anthropic(create=${this.cacheCreationTokens}, read=${this.cacheReadTokens}), OpenAI(prompt=${this.cachedPromptTokens})`);
        }
      }
      /**
       * Calculate the current context window size based on provided messages or internal history.
       * @param {Array|null} messages - Optional messages array to use for calculation. If null, uses internal this.history.
       * @returns {number} - Total tokens estimated in the context window.
       */
      calculateContextSize(messages = null) {
        const msgsToCount = messages !== null ? messages : this.history;
        let totalTokens = 0;
        if (this.debug && messages === null) {
          console.log(`[DEBUG] Calculating context size from internal history (${this.history.length} messages)`);
        }
        for (const msg of msgsToCount) {
          let messageTokens = 0;
          messageTokens += 4;
          if (typeof msg.content === "string") {
            messageTokens += this.countTokens(msg.content);
          } else if (Array.isArray(msg.content)) {
            for (const item of msg.content) {
              if (item.type === "text" && typeof item.text === "string") {
                messageTokens += this.countTokens(item.text);
              } else {
                messageTokens += this.countTokens(JSON.stringify(item));
              }
            }
          } else if (msg.content) {
            messageTokens += this.countTokens(JSON.stringify(msg.content));
          }
          if (msg.toolCalls) {
            messageTokens += this.countTokens(JSON.stringify(msg.toolCalls));
            messageTokens += 5;
          }
          if (msg.role === "tool" && msg.toolCallId) {
            messageTokens += this.countTokens(msg.toolCallId);
            messageTokens += 5;
          }
          if (msg.toolCallResults) {
            messageTokens += this.countTokens(JSON.stringify(msg.toolCallResults));
            messageTokens += 5;
          }
          totalTokens += messageTokens;
        }
        if (messages === null) {
          this.contextSize = totalTokens;
          if (this.debug) {
            console.log(`[DEBUG] Updated internal context size: ${this.contextSize} tokens`);
          }
        }
        return totalTokens;
      }
      /**
       * Update internal history and recalculate internal context window size.
       * @param {Array} messages - New message history array.
       */
      updateHistory(messages) {
        if (!Array.isArray(messages)) {
          console.warn("[WARN] updateHistory called with non-array:", messages);
          this.history = [];
        } else {
          this.history = [...messages];
        }
        this.calculateContextSize();
        if (this.debug) {
          console.log(`[DEBUG] History updated (${this.history.length} messages). Recalculated context size: ${this.contextSize}`);
        }
      }
      /**
       * Clear all counters and internal history. Reset context size.
       */
      clear() {
        this.requestTokens = 0;
        this.responseTokens = 0;
        this.currentRequestTokens = 0;
        this.currentResponseTokens = 0;
        this.cacheCreationTokens = 0;
        this.cacheReadTokens = 0;
        this.currentCacheCreationTokens = 0;
        this.currentCacheReadTokens = 0;
        this.cachedPromptTokens = 0;
        this.currentCachedPromptTokens = 0;
        this.history = [];
        this.contextSize = 0;
        if (this.debug) {
          console.log("[DEBUG] TokenCounter cleared: usage, history, and context size reset.");
        }
      }
      /**
       * Start a new conversation turn - reset CURRENT token counters.
       * Calculates context size based on history *before* the new turn.
       */
      startNewTurn() {
        this.currentRequestTokens = 0;
        this.currentResponseTokens = 0;
        this.currentCacheCreationTokens = 0;
        this.currentCacheReadTokens = 0;
        this.currentCachedPromptTokens = 0;
        this.calculateContextSize();
        if (this.debug) {
          console.log("[DEBUG] TokenCounter: New turn started. Current counters reset.");
          console.log(`[DEBUG] Context size at start of turn: ${this.contextSize} tokens`);
        }
      }
      /**
       * Get the current token usage state including context size.
       * Recalculates context size from internal history before returning.
       * @returns {Object} - Object containing current turn, total session, and context window usage.
       */
      getTokenUsage() {
        const currentContextSize = this.calculateContextSize();
        const currentCacheRead = this.currentCacheReadTokens + this.currentCachedPromptTokens;
        const currentCacheWrite = this.currentCacheCreationTokens;
        const totalCacheRead = this.cacheReadTokens + this.cachedPromptTokens;
        const totalCacheWrite = this.cacheCreationTokens;
        const usageData = {
          contextWindow: currentContextSize,
          // Use the freshly calculated value
          current: {
            // Usage for the *last* LLM call recorded
            request: this.currentRequestTokens,
            response: this.currentResponseTokens,
            total: this.currentRequestTokens + this.currentResponseTokens,
            cacheRead: currentCacheRead,
            cacheWrite: currentCacheWrite,
            cacheTotal: currentCacheRead + currentCacheWrite,
            // Keep detailed breakdown if needed
            anthropic: {
              cacheCreation: this.currentCacheCreationTokens,
              cacheRead: this.currentCacheReadTokens
            },
            openai: {
              cachedPrompt: this.currentCachedPromptTokens
            }
          },
          total: {
            // Accumulated usage over the session
            request: this.requestTokens,
            response: this.responseTokens,
            total: this.requestTokens + this.responseTokens,
            cacheRead: totalCacheRead,
            cacheWrite: totalCacheWrite,
            cacheTotal: totalCacheRead + totalCacheWrite,
            // Keep detailed breakdown if needed
            anthropic: {
              cacheCreation: this.cacheCreationTokens,
              cacheRead: this.cacheReadTokens
            },
            openai: {
              cachedPrompt: this.cachedPromptTokens
            }
          }
        };
        if (this.debug) {
        }
        return usageData;
      }
    };
  }
});

// src/directory-resolver.js
async function getPackageBinDir() {
  const debug = process.env.DEBUG === "1" || process.env.VERBOSE === "1";
  if (debug) {
    console.log("DEBUG: Starting probe binary directory resolution");
  }
  if (process.env.PROBE_BINARY_PATH) {
    if (debug) {
      console.log(`DEBUG: Checking PROBE_BINARY_PATH: ${process.env.PROBE_BINARY_PATH}`);
    }
    const binaryPath = process.env.PROBE_BINARY_PATH;
    if (await import_fs_extra.default.pathExists(binaryPath)) {
      const binDir = import_path.default.dirname(binaryPath);
      if (await canWriteToDirectory(binDir)) {
        if (debug) {
          console.log(`DEBUG: Using PROBE_BINARY_PATH directory: ${binDir}`);
        }
        return binDir;
      }
    } else {
      console.warn(`Warning: PROBE_BINARY_PATH ${binaryPath} does not exist`);
    }
  }
  if (process.env.PROBE_CACHE_DIR) {
    if (debug) {
      console.log(`DEBUG: Checking PROBE_CACHE_DIR: ${process.env.PROBE_CACHE_DIR}`);
    }
    const cacheDir = import_path.default.join(process.env.PROBE_CACHE_DIR, "bin");
    if (await ensureDirectory(cacheDir)) {
      if (debug) {
        console.log(`DEBUG: Using PROBE_CACHE_DIR: ${cacheDir}`);
      }
      return cacheDir;
    }
  }
  const packageRoot = await findPackageRoot();
  if (packageRoot) {
    if (debug) {
      console.log(`DEBUG: Found package root: ${packageRoot}`);
    }
    const packageBinDir = import_path.default.join(packageRoot, "bin");
    if (await ensureDirectory(packageBinDir) && await canWriteToDirectory(packageBinDir)) {
      if (debug) {
        console.log(`DEBUG: Using package bin directory: ${packageBinDir}`);
      }
      return packageBinDir;
    } else if (debug) {
      console.log(`DEBUG: Package bin directory ${packageBinDir} not writable, trying fallbacks`);
    }
  }
  const homeCache = import_path.default.join(import_os.default.homedir(), ".probe", "bin");
  if (debug) {
    console.log(`DEBUG: Trying home cache directory: ${homeCache}`);
  }
  if (await ensureDirectory(homeCache)) {
    if (debug) {
      console.log(`DEBUG: Using home cache directory: ${homeCache}`);
    }
    return homeCache;
  }
  const tempCache = import_path.default.join(import_os.default.tmpdir(), "probe-cache", "bin");
  if (debug) {
    console.log(`DEBUG: Trying temp cache directory: ${tempCache}`);
  }
  if (await ensureDirectory(tempCache)) {
    if (debug) {
      console.log(`DEBUG: Using temp cache directory: ${tempCache}`);
    }
    return tempCache;
  }
  const errorMessage = [
    "Could not find a writable directory for probe binary.",
    "Tried the following locations:",
    packageRoot ? `- Package bin directory: ${import_path.default.join(packageRoot, "bin")}` : "- Package root not found",
    `- Home cache directory: ${homeCache}`,
    `- Temp cache directory: ${tempCache}`,
    "",
    "You can override the location using environment variables:",
    "- PROBE_BINARY_PATH=/path/to/probe (direct path to binary)",
    "- PROBE_CACHE_DIR=/path/to/cache (cache directory, binary will be in /bin subdirectory)"
  ].join("\n");
  throw new Error(errorMessage);
}
async function findPackageRoot() {
  const debug = process.env.DEBUG === "1" || process.env.VERBOSE === "1";
  let currentDir = __dirname;
  const rootDir = import_path.default.parse(currentDir).root;
  if (debug) {
    console.log(`DEBUG: Starting package root search from: ${currentDir}`);
  }
  while (currentDir !== rootDir) {
    const packageJsonPath = import_path.default.join(currentDir, "package.json");
    try {
      if (await import_fs_extra.default.pathExists(packageJsonPath)) {
        const packageJson = await import_fs_extra.default.readJson(packageJsonPath);
        if (debug) {
          console.log(`DEBUG: Found package.json at ${packageJsonPath}, name: ${packageJson.name}`);
        }
        if (packageJson.name === "@probelabs/probe") {
          if (debug) {
            console.log(`DEBUG: Found probe package root: ${currentDir}`);
          }
          return currentDir;
        }
      }
    } catch (err) {
      if (debug) {
        console.log(`DEBUG: Error reading package.json at ${packageJsonPath}: ${err.message}`);
      }
    }
    currentDir = import_path.default.dirname(currentDir);
  }
  if (debug) {
    console.log("DEBUG: Package root not found, reached filesystem root");
  }
  return null;
}
async function ensureDirectory(dirPath) {
  const debug = process.env.DEBUG === "1" || process.env.VERBOSE === "1";
  try {
    await import_fs_extra.default.ensureDir(dirPath);
    const testFile = import_path.default.join(dirPath, ".probe-write-test");
    await import_fs_extra.default.writeFile(testFile, "test");
    await import_fs_extra.default.remove(testFile);
    if (debug) {
      console.log(`DEBUG: Directory ${dirPath} is writable`);
    }
    return true;
  } catch (error) {
    if (debug) {
      console.log(`DEBUG: Directory ${dirPath} not writable: ${error.message}`);
    }
    return false;
  }
}
async function canWriteToDirectory(dirPath) {
  const debug = process.env.DEBUG === "1" || process.env.VERBOSE === "1";
  try {
    const exists = await import_fs_extra.default.pathExists(dirPath);
    if (!exists) {
      if (debug) {
        console.log(`DEBUG: Directory ${dirPath} does not exist`);
      }
      return false;
    }
    const testFile = import_path.default.join(dirPath, ".probe-write-test");
    await import_fs_extra.default.writeFile(testFile, "test");
    await import_fs_extra.default.remove(testFile);
    if (debug) {
      console.log(`DEBUG: Directory ${dirPath} is writable`);
    }
    return true;
  } catch (error) {
    if (debug) {
      console.log(`DEBUG: Directory ${dirPath} not writable: ${error.message}`);
    }
    return false;
  }
}
var import_path, import_os, import_fs_extra, import_url, __filename, __dirname;
var init_directory_resolver = __esm({
  "src/directory-resolver.js"() {
    "use strict";
    import_path = __toESM(require("path"), 1);
    import_os = __toESM(require("os"), 1);
    import_fs_extra = __toESM(require("fs-extra"), 1);
    import_url = require("url");
    __filename = (0, import_url.fileURLToPath)("file:///");
    __dirname = import_path.default.dirname(__filename);
  }
});

// src/downloader.js
function detectOsArch() {
  const osType = import_os2.default.platform();
  const archType = import_os2.default.arch();
  let osInfo;
  let archInfo;
  switch (osType) {
    case "linux":
      osInfo = {
        type: "linux",
        keywords: ["linux", "Linux", "gnu"]
      };
      break;
    case "darwin":
      osInfo = {
        type: "darwin",
        keywords: ["darwin", "Darwin", "mac", "Mac", "apple", "Apple", "osx", "OSX"]
      };
      break;
    case "win32":
      osInfo = {
        type: "windows",
        keywords: ["windows", "Windows", "msvc", "pc-windows"]
      };
      break;
    default:
      throw new Error(`Unsupported operating system: ${osType}`);
  }
  switch (archType) {
    case "x64":
      archInfo = {
        type: "x86_64",
        keywords: ["x86_64", "amd64", "x64", "64bit", "64-bit"]
      };
      break;
    case "arm64":
      archInfo = {
        type: "aarch64",
        keywords: ["arm64", "aarch64", "arm", "ARM"]
      };
      break;
    default:
      throw new Error(`Unsupported architecture: ${archType}`);
  }
  if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
    console.log(`Detected OS: ${osInfo.type}, Architecture: ${archInfo.type}`);
  }
  return { os: osInfo, arch: archInfo };
}
function constructAssetInfo(version, osInfo, archInfo) {
  let platform;
  let extension;
  switch (osInfo.type) {
    case "linux":
      platform = `${archInfo.type}-unknown-linux-gnu`;
      extension = "tar.gz";
      break;
    case "darwin":
      platform = `${archInfo.type}-apple-darwin`;
      extension = "tar.gz";
      break;
    case "windows":
      platform = `${archInfo.type}-pc-windows-msvc`;
      extension = "zip";
      break;
    default:
      throw new Error(`Unsupported OS type: ${osInfo.type}`);
  }
  const assetName = `probe-v${version}-${platform}.${extension}`;
  const checksumName = `${assetName}.sha256`;
  const baseUrl = `https://github.com/${REPO_OWNER}/${REPO_NAME}/releases/download/v${version}`;
  const assetUrl = `${baseUrl}/${assetName}`;
  const checksumUrl = `${baseUrl}/${checksumName}`;
  if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
    console.log(`Constructed asset URL: ${assetUrl}`);
  }
  return {
    name: assetName,
    url: assetUrl,
    checksumName,
    checksumUrl
  };
}
async function getLatestRelease(version) {
  if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
    console.log("Fetching release information...");
  }
  try {
    let releaseUrl;
    if (version) {
      releaseUrl = `https://api.github.com/repos/${REPO_OWNER}/${REPO_NAME}/releases/tags/v${version}`;
    } else {
      releaseUrl = `https://api.github.com/repos/${REPO_OWNER}/${REPO_NAME}/releases`;
    }
    const response = await import_axios.default.get(releaseUrl);
    if (response.status !== 200) {
      throw new Error(`Failed to fetch release information: ${response.statusText}`);
    }
    let releaseData;
    if (version) {
      releaseData = response.data;
    } else {
      if (!Array.isArray(response.data) || response.data.length === 0) {
        throw new Error("No releases found");
      }
      releaseData = response.data[0];
    }
    const tag = releaseData.tag_name;
    const assets = releaseData.assets.map((asset) => ({
      name: asset.name,
      url: asset.browser_download_url
    }));
    if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
      console.log(`Found release: ${tag} with ${assets.length} assets`);
    }
    return { tag, assets };
  } catch (error) {
    if (import_axios.default.isAxiosError(error) && error.response?.status === 404) {
      if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
        console.log(`Release v${version} not found, trying to fetch all releases...`);
      }
      const response = await import_axios.default.get(`https://api.github.com/repos/${REPO_OWNER}/${REPO_NAME}/releases`);
      if (response.data.length === 0) {
        throw new Error("No releases found");
      }
      let bestRelease = response.data[0];
      if (version && version !== "0.0.0") {
        if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
          console.log(`Looking for releases matching version: ${version}`);
          console.log(`Available releases: ${response.data.slice(0, 5).map((r) => r.tag_name).join(", ")}...`);
        }
        for (const release of response.data) {
          const releaseTag = release.tag_name.startsWith("v") ? release.tag_name.substring(1) : release.tag_name;
          if (releaseTag === version) {
            if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
              console.log(`Found exact matching release: ${release.tag_name}`);
            }
            bestRelease = release;
            break;
          }
        }
        if (bestRelease === response.data[0]) {
          const versionParts = version.split(/[\.-]/);
          const majorMinor = versionParts.slice(0, 2).join(".");
          if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
            console.log(`Looking for releases matching major.minor: ${majorMinor}`);
          }
          for (const release of response.data) {
            const releaseTag = release.tag_name.startsWith("v") ? release.tag_name.substring(1) : release.tag_name;
            const releaseVersionParts = releaseTag.split(/[\.-]/);
            const releaseMajorMinor = releaseVersionParts.slice(0, 2).join(".");
            if (releaseMajorMinor === majorMinor) {
              if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
                console.log(`Found matching major.minor release: ${release.tag_name}`);
              }
              bestRelease = release;
              break;
            }
          }
        }
      }
      const tag = bestRelease.tag_name;
      const assets = bestRelease.assets.map((asset) => ({
        name: asset.name,
        url: asset.browser_download_url
      }));
      if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
        console.log(`Using release: ${tag} with ${assets.length} assets`);
      }
      return { tag, assets };
    }
    throw error;
  }
}
function findBestAsset(assets, osInfo, archInfo) {
  if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
    console.log(`Finding appropriate binary for ${osInfo.type} ${archInfo.type}...`);
  }
  let bestAsset = null;
  let bestScore = 0;
  for (const asset of assets) {
    if (asset.name.endsWith(".sha256") || asset.name.endsWith(".md5") || asset.name.endsWith(".asc")) {
      continue;
    }
    if (osInfo.type === "windows" && asset.name.match(/darwin|linux/)) {
      if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
        console.log(`Skipping non-Windows binary: ${asset.name}`);
      }
      continue;
    } else if (osInfo.type === "darwin" && asset.name.match(/windows|msvc|linux/)) {
      if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
        console.log(`Skipping non-macOS binary: ${asset.name}`);
      }
      continue;
    } else if (osInfo.type === "linux" && asset.name.match(/darwin|windows|msvc/)) {
      if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
        console.log(`Skipping non-Linux binary: ${asset.name}`);
      }
      continue;
    }
    let score = 0;
    if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
      console.log(`Evaluating asset: ${asset.name}`);
    }
    let osMatched = false;
    for (const keyword of osInfo.keywords) {
      if (asset.name.includes(keyword)) {
        score += 10;
        osMatched = true;
        if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
          console.log(`  OS match found (${keyword}): +10, score = ${score}`);
        }
        break;
      }
    }
    for (const keyword of archInfo.keywords) {
      if (asset.name.includes(keyword)) {
        score += 5;
        if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
          console.log(`  Arch match found (${keyword}): +5, score = ${score}`);
        }
        break;
      }
    }
    if (asset.name.startsWith(`${BINARY_NAME}-`)) {
      score += 3;
      if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
        console.log(`  Binary name match: +3, score = ${score}`);
      }
    }
    if (osMatched && score >= 15) {
      score += 5;
      if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
        console.log(`  OS+Arch bonus: +5, score = ${score}`);
      }
    }
    if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
      console.log(`  Final score for ${asset.name}: ${score}`);
    }
    if (score === 23) {
      if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
        console.log(`Found perfect match: ${asset.name}`);
      }
      return asset;
    }
    if (score > bestScore) {
      bestScore = score;
      bestAsset = asset;
      if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
        console.log(`  New best asset: ${asset.name} (score: ${score})`);
      }
    }
  }
  if (!bestAsset) {
    throw new Error(`Could not find a suitable binary for ${osInfo.type} ${archInfo.type}`);
  }
  if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
    console.log(`Selected asset: ${bestAsset.name} (score: ${bestScore})`);
  }
  return bestAsset;
}
async function downloadAsset(asset, outputDir) {
  await import_fs_extra2.default.ensureDir(outputDir);
  const assetPath = import_path2.default.join(outputDir, asset.name);
  if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
    console.log(`Downloading ${asset.name}...`);
  }
  const assetResponse = await import_axios.default.get(asset.url, { responseType: "arraybuffer" });
  await import_fs_extra2.default.writeFile(assetPath, Buffer.from(assetResponse.data));
  const checksumUrl = asset.checksumUrl || `${asset.url}.sha256`;
  const checksumFileName = asset.checksumName || `${asset.name}.sha256`;
  let checksumPath = null;
  try {
    if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
      console.log(`Downloading checksum...`);
    }
    const checksumResponse = await import_axios.default.get(checksumUrl);
    checksumPath = import_path2.default.join(outputDir, checksumFileName);
    await import_fs_extra2.default.writeFile(checksumPath, checksumResponse.data);
  } catch (error) {
    if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
      console.log("No checksum file found, skipping verification");
    }
  }
  return { assetPath, checksumPath };
}
async function verifyChecksum(assetPath, checksumPath) {
  if (!checksumPath) {
    return true;
  }
  if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
    console.log(`Verifying checksum...`);
  }
  const checksumContent = await import_fs_extra2.default.readFile(checksumPath, "utf-8");
  const expectedChecksum = checksumContent.trim().split(" ")[0];
  const fileBuffer = await import_fs_extra2.default.readFile(assetPath);
  const actualChecksum = (0, import_crypto.createHash)("sha256").update(fileBuffer).digest("hex");
  if (expectedChecksum !== actualChecksum) {
    console.error(`Checksum verification failed!`);
    console.error(`Expected: ${expectedChecksum}`);
    console.error(`Actual: ${actualChecksum}`);
    return false;
  }
  if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
    console.log(`Checksum verified successfully`);
  }
  return true;
}
async function extractBinary(assetPath, outputDir) {
  if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
    console.log(`Extracting ${import_path2.default.basename(assetPath)}...`);
  }
  const assetName = import_path2.default.basename(assetPath);
  const isWindows = import_os2.default.platform() === "win32";
  const binaryName = isWindows ? `${BINARY_NAME}.exe` : `${BINARY_NAME}-binary`;
  const binaryPath = import_path2.default.join(outputDir, binaryName);
  try {
    const extractDir = import_path2.default.join(outputDir, "temp_extract");
    await import_fs_extra2.default.ensureDir(extractDir);
    if (assetName.endsWith(".tar.gz") || assetName.endsWith(".tgz")) {
      if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
        console.log(`Extracting tar.gz to ${extractDir}...`);
      }
      await import_tar.default.extract({
        file: assetPath,
        cwd: extractDir
      });
    } else if (assetName.endsWith(".zip")) {
      if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
        console.log(`Extracting zip to ${extractDir}...`);
      }
      await exec(`unzip -q "${assetPath}" -d "${extractDir}"`);
    } else {
      if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
        console.log(`Copying binary directly to ${binaryPath}`);
      }
      await import_fs_extra2.default.copyFile(assetPath, binaryPath);
      if (!isWindows) {
        await import_fs_extra2.default.chmod(binaryPath, 493);
      }
      await import_fs_extra2.default.remove(extractDir);
      if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
        console.log(`Binary installed to ${binaryPath}`);
      }
      return binaryPath;
    }
    if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
      console.log(`Searching for binary in extracted files...`);
    }
    const findBinary = async (dir) => {
      const entries = await import_fs_extra2.default.readdir(dir, { withFileTypes: true });
      for (const entry of entries) {
        const fullPath = import_path2.default.join(dir, entry.name);
        if (entry.isDirectory()) {
          const result = await findBinary(fullPath);
          if (result) return result;
        } else if (entry.isFile()) {
          if (entry.name === binaryName || entry.name === BINARY_NAME || isWindows && entry.name.endsWith(".exe")) {
            return fullPath;
          }
        }
      }
      return null;
    };
    const binaryFilePath = await findBinary(extractDir);
    if (!binaryFilePath) {
      const allFiles = await import_fs_extra2.default.readdir(extractDir, { recursive: true });
      console.error(`Binary not found in extracted files. Found: ${allFiles.join(", ")}`);
      throw new Error(`Binary not found in the archive.`);
    }
    if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
      console.log(`Found binary at ${binaryFilePath}`);
      console.log(`Copying binary to ${binaryPath}`);
    }
    await import_fs_extra2.default.copyFile(binaryFilePath, binaryPath);
    if (!isWindows) {
      await import_fs_extra2.default.chmod(binaryPath, 493);
    }
    await import_fs_extra2.default.remove(extractDir);
    if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
      console.log(`Binary successfully installed to ${binaryPath}`);
    }
    return binaryPath;
  } catch (error) {
    console.error(`Error extracting binary: ${error instanceof Error ? error.message : String(error)}`);
    throw error;
  }
}
async function getVersionInfo(binDir) {
  try {
    const versionInfoPath = import_path2.default.join(binDir, "version-info.json");
    if (await import_fs_extra2.default.pathExists(versionInfoPath)) {
      const content = await import_fs_extra2.default.readFile(versionInfoPath, "utf-8");
      return JSON.parse(content);
    }
    return null;
  } catch (error) {
    console.warn(`Warning: Could not read version info: ${error}`);
    return null;
  }
}
async function saveVersionInfo(version, binDir) {
  const versionInfo = {
    version,
    lastUpdated: (/* @__PURE__ */ new Date()).toISOString()
  };
  const versionInfoPath = import_path2.default.join(binDir, "version-info.json");
  await import_fs_extra2.default.writeFile(versionInfoPath, JSON.stringify(versionInfo, null, 2));
  if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
    console.log(`Version info saved: ${version} at ${versionInfoPath}`);
  }
}
async function getPackageVersion() {
  try {
    const possiblePaths = [
      import_path2.default.resolve(__dirname2, "..", "package.json"),
      // When installed from npm: src/../package.json
      import_path2.default.resolve(__dirname2, "..", "..", "package.json")
      // In development: src/../../package.json
    ];
    for (const packageJsonPath of possiblePaths) {
      try {
        if (import_fs_extra2.default.existsSync(packageJsonPath)) {
          if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
            console.log(`Found package.json at: ${packageJsonPath}`);
          }
          const packageJson = JSON.parse(import_fs_extra2.default.readFileSync(packageJsonPath, "utf-8"));
          if (packageJson.version) {
            if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
              console.log(`Using version from package.json: ${packageJson.version}`);
            }
            return packageJson.version;
          }
        }
      } catch (err) {
        console.error(`Error reading package.json at ${packageJsonPath}:`, err);
      }
    }
    return "0.0.0";
  } catch (error) {
    console.error("Error getting package version:", error);
    return "0.0.0";
  }
}
async function downloadProbeBinary(version) {
  try {
    const localDir = await getPackageBinDir();
    if (!version || version === "0.0.0") {
      version = await getPackageVersion();
    }
    if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
      console.log(`Downloading probe binary (version: ${version || "latest"})...`);
      console.log(`Using binary directory: ${localDir}`);
    }
    const isWindows = import_os2.default.platform() === "win32";
    const binaryName = isWindows ? `${BINARY_NAME}.exe` : `${BINARY_NAME}-binary`;
    const binaryPath = import_path2.default.join(localDir, binaryName);
    if (await import_fs_extra2.default.pathExists(binaryPath)) {
      const versionInfo = await getVersionInfo(localDir);
      if (versionInfo && versionInfo.version === version) {
        if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
          console.log(`Using existing binary at ${binaryPath} (version: ${versionInfo.version})`);
        }
        return binaryPath;
      }
      if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
        console.log(`Existing binary version (${versionInfo?.version || "unknown"}) doesn't match requested version (${version}). Downloading new version...`);
      }
    }
    const { os: osInfo, arch: archInfo } = detectOsArch();
    let versionToUse = version;
    let bestAsset;
    let tagVersion;
    if (!versionToUse || versionToUse === "0.0.0") {
      if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
        console.log("No specific version requested, will use the latest release");
      }
      const { tag, assets } = await getLatestRelease(void 0);
      tagVersion = tag.startsWith("v") ? tag.substring(1) : tag;
      bestAsset = findBestAsset(assets, osInfo, archInfo);
      if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
        console.log(`Found release version: ${tagVersion}`);
      }
    } else {
      if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
        console.log(`Direct download for version: ${versionToUse}`);
      }
      tagVersion = versionToUse;
      bestAsset = constructAssetInfo(versionToUse, osInfo, archInfo);
    }
    const { assetPath, checksumPath } = await downloadAsset(bestAsset, localDir);
    const checksumValid = await verifyChecksum(assetPath, checksumPath);
    if (!checksumValid) {
      throw new Error("Checksum verification failed");
    }
    const extractedBinaryPath = await extractBinary(assetPath, localDir);
    await saveVersionInfo(tagVersion, localDir);
    try {
      await import_fs_extra2.default.remove(assetPath);
      if (checksumPath) {
        await import_fs_extra2.default.remove(checksumPath);
      }
    } catch (err) {
      if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
        console.log(`Warning: Could not clean up temporary files: ${err}`);
      }
    }
    if (process.env.DEBUG === "1" || process.env.VERBOSE === "1") {
      console.log(`Binary successfully installed at ${extractedBinaryPath} (version: ${tagVersion})`);
    }
    return extractedBinaryPath;
  } catch (error) {
    console.error("Error downloading probe binary:", error);
    throw error;
  }
}
var import_axios, import_fs_extra2, import_path2, import_crypto, import_util, import_child_process, import_tar, import_os2, import_url2, exec, REPO_OWNER, REPO_NAME, BINARY_NAME, __filename2, __dirname2;
var init_downloader = __esm({
  "src/downloader.js"() {
    "use strict";
    import_axios = __toESM(require("axios"), 1);
    import_fs_extra2 = __toESM(require("fs-extra"), 1);
    import_path2 = __toESM(require("path"), 1);
    import_crypto = require("crypto");
    import_util = require("util");
    import_child_process = require("child_process");
    import_tar = __toESM(require("tar"), 1);
    import_os2 = __toESM(require("os"), 1);
    import_url2 = require("url");
    init_utils();
    init_directory_resolver();
    exec = (0, import_util.promisify)(import_child_process.exec);
    REPO_OWNER = "probelabs";
    REPO_NAME = "probe";
    BINARY_NAME = "probe";
    __filename2 = (0, import_url2.fileURLToPath)("file:///");
    __dirname2 = import_path2.default.dirname(__filename2);
  }
});

// src/utils.js
async function getBinaryPath(options = {}) {
  const { forceDownload = false, version } = options;
  if (probeBinaryPath && !forceDownload && import_fs_extra3.default.existsSync(probeBinaryPath)) {
    return probeBinaryPath;
  }
  if (process.env.PROBE_PATH && import_fs_extra3.default.existsSync(process.env.PROBE_PATH) && !forceDownload) {
    probeBinaryPath = process.env.PROBE_PATH;
    return probeBinaryPath;
  }
  const binDir = await getPackageBinDir();
  const isWindows = process.platform === "win32";
  const binaryName = isWindows ? "probe.exe" : "probe";
  const binaryPath = import_path3.default.join(binDir, binaryName);
  if (import_fs_extra3.default.existsSync(binaryPath) && !forceDownload) {
    probeBinaryPath = binaryPath;
    return probeBinaryPath;
  }
  console.log(`${forceDownload ? "Force downloading" : "Binary not found. Downloading"} probe binary...`);
  probeBinaryPath = await downloadProbeBinary(version);
  return probeBinaryPath;
}
function buildCliArgs(options, flagMap) {
  const cliArgs = [];
  for (const [key, flag] of Object.entries(flagMap)) {
    if (key in options) {
      const value = options[key];
      if (typeof value === "boolean") {
        if (value) {
          cliArgs.push(flag);
        }
      } else if (Array.isArray(value)) {
        for (const item of value) {
          cliArgs.push(flag, item);
        }
      } else if (value !== void 0 && value !== null) {
        cliArgs.push(flag, value.toString());
      }
    }
  }
  return cliArgs;
}
function escapeString(str) {
  if (process.platform === "win32") {
    return `"${str.replace(/"/g, '\\"')}"`;
  } else {
    return `'${str.replace(/'/g, "'\\''")}'`;
  }
}
var import_path3, import_fs_extra3, import_url3, __filename3, __dirname3, probeBinaryPath;
var init_utils = __esm({
  "src/utils.js"() {
    "use strict";
    import_path3 = __toESM(require("path"), 1);
    import_fs_extra3 = __toESM(require("fs-extra"), 1);
    import_url3 = require("url");
    init_downloader();
    init_directory_resolver();
    __filename3 = (0, import_url3.fileURLToPath)("file:///");
    __dirname3 = import_path3.default.dirname(__filename3);
    probeBinaryPath = "";
  }
});

// src/search.js
async function search(options) {
  if (!options || !options.path) {
    throw new Error("Path is required");
  }
  if (!options.query) {
    throw new Error("Query is required");
  }
  const binaryPath = await getBinaryPath(options.binaryOptions || {});
  const cliArgs = buildCliArgs(options, SEARCH_FLAG_MAP);
  if (options.json) {
    cliArgs.push("--format", "json");
  }
  if (!options.maxTokens) {
    options.maxTokens = 1e4;
    cliArgs.push("--max-tokens", "10000");
  }
  if (!options.timeout) {
    options.timeout = 30;
    cliArgs.push("--timeout", "30");
  }
  if (options.language) {
    if (!cliArgs.includes("--language")) {
      cliArgs.push("--language", options.language);
    }
  }
  if (options.exact) {
    if (!cliArgs.includes("--exact")) {
      cliArgs.push("--exact");
    }
  }
  if (!options.session && process.env.PROBE_SESSION_ID) {
    options.session = process.env.PROBE_SESSION_ID;
  }
  const queries = Array.isArray(options.query) ? options.query : [options.query];
  if (process.env.DEBUG === "1") {
    let logMessage = `
Search: query="${queries[0]}" path="${options.path}"`;
    if (options.maxResults) logMessage += ` maxResults=${options.maxResults}`;
    logMessage += ` maxTokens=${options.maxTokens}`;
    logMessage += ` timeout=${options.timeout}`;
    if (options.allowTests) logMessage += " allowTests=true";
    if (options.language) logMessage += ` language=${options.language}`;
    if (options.exact) logMessage += " exact=true";
    if (options.session) logMessage += ` session=${options.session}`;
    console.error(logMessage);
  }
  const positionalArgs = [];
  if (queries.length > 0) {
    positionalArgs.push(escapeString(queries[0]));
  }
  positionalArgs.push(escapeString(options.path));
  const command = `${binaryPath} search ${cliArgs.join(" ")} ${positionalArgs.join(" ")}`;
  try {
    const { stdout, stderr } = await execAsync(command, {
      shell: true,
      timeout: options.timeout * 1e3
      // Convert seconds to milliseconds
    });
    if (stderr && process.env.DEBUG) {
      console.error(`stderr: ${stderr}`);
    }
    let resultCount = 0;
    let tokenCount = 0;
    let bytesCount = 0;
    const lines = stdout.split("\n");
    for (const line of lines) {
      if (line.startsWith("```") && !line.includes("```language")) {
        resultCount++;
      }
    }
    const totalBytesMatch = stdout.match(/Total bytes returned:\s*(\d+)/i);
    if (totalBytesMatch && totalBytesMatch[1]) {
      bytesCount = parseInt(totalBytesMatch[1], 10);
    }
    const totalTokensMatch = stdout.match(/Total tokens returned:\s*(\d+)/i);
    if (totalTokensMatch && totalTokensMatch[1]) {
      tokenCount = parseInt(totalTokensMatch[1], 10);
    } else {
      const tokenMatch = stdout.match(/Tokens:?\s*(\d+)/i) || stdout.match(/(\d+)\s*tokens/i) || stdout.match(/token count:?\s*(\d+)/i);
      if (tokenMatch && tokenMatch[1]) {
        tokenCount = parseInt(tokenMatch[1], 10);
      } else {
        tokenCount = options.maxTokens;
      }
    }
    if (process.env.DEBUG === "1") {
      let resultsMessage = `
Search results: ${resultCount} matches, ${tokenCount} tokens`;
      if (bytesCount > 0) {
        resultsMessage += `, ${bytesCount} bytes`;
      }
      console.error(resultsMessage);
    }
    if (options.json) {
      try {
        return JSON.parse(stdout);
      } catch (error) {
        console.error("Error parsing JSON output:", error);
        return stdout;
      }
    }
    return stdout;
  } catch (error) {
    if (error.code === "ETIMEDOUT" || error.killed) {
      const timeoutMessage = `Search operation timed out after ${options.timeout} seconds.
Command: ${command}`;
      console.error(timeoutMessage);
      throw new Error(timeoutMessage);
    }
    const errorMessage = `Error executing search command: ${error.message}
Command: ${command}`;
    throw new Error(errorMessage);
  }
}
var import_child_process2, import_util2, execAsync, SEARCH_FLAG_MAP;
var init_search = __esm({
  "src/search.js"() {
    "use strict";
    import_child_process2 = require("child_process");
    import_util2 = require("util");
    init_utils();
    execAsync = (0, import_util2.promisify)(import_child_process2.exec);
    SEARCH_FLAG_MAP = {
      filesOnly: "--files-only",
      ignore: "--ignore",
      excludeFilenames: "--exclude-filenames",
      reranker: "--reranker",
      frequencySearch: "--frequency",
      exact: "--exact",
      maxResults: "--max-results",
      maxBytes: "--max-bytes",
      maxTokens: "--max-tokens",
      allowTests: "--allow-tests",
      noMerge: "--no-merge",
      mergeThreshold: "--merge-threshold",
      session: "--session",
      timeout: "--timeout",
      language: "--language"
    };
  }
});

// src/query.js
async function query(options) {
  if (!options || !options.path) {
    throw new Error("Path is required");
  }
  if (!options.pattern) {
    throw new Error("Pattern is required");
  }
  const binaryPath = await getBinaryPath(options.binaryOptions || {});
  const cliArgs = buildCliArgs(options, QUERY_FLAG_MAP);
  if (options.json && !options.format) {
    cliArgs.push("--format", "json");
  }
  cliArgs.push(escapeString(options.pattern), escapeString(options.path));
  if (process.env.DEBUG === "1") {
    let logMessage = `Query: pattern="${options.pattern}" path="${options.path}"`;
    if (options.language) logMessage += ` language=${options.language}`;
    if (options.maxResults) logMessage += ` maxResults=${options.maxResults}`;
    if (options.allowTests) logMessage += " allowTests=true";
    console.error(logMessage);
  }
  const command = `${binaryPath} query ${cliArgs.join(" ")}`;
  try {
    const { stdout, stderr } = await execAsync2(command);
    if (stderr) {
      console.error(`stderr: ${stderr}`);
    }
    let resultCount = 0;
    const lines = stdout.split("\n");
    for (const line of lines) {
      if (line.startsWith("```") && !line.includes("```language")) {
        resultCount++;
      }
    }
    if (process.env.DEBUG === "1") {
      console.error(`Query results: ${resultCount} matches`);
    }
    if (options.json || options.format === "json") {
      try {
        return JSON.parse(stdout);
      } catch (error) {
        console.error("Error parsing JSON output:", error);
        return stdout;
      }
    }
    return stdout;
  } catch (error) {
    const errorMessage = `Error executing query command: ${error.message}
Command: ${command}`;
    throw new Error(errorMessage);
  }
}
var import_child_process3, import_util3, execAsync2, QUERY_FLAG_MAP;
var init_query = __esm({
  "src/query.js"() {
    "use strict";
    import_child_process3 = require("child_process");
    import_util3 = require("util");
    init_utils();
    execAsync2 = (0, import_util3.promisify)(import_child_process3.exec);
    QUERY_FLAG_MAP = {
      language: "--language",
      ignore: "--ignore",
      allowTests: "--allow-tests",
      maxResults: "--max-results",
      format: "--format"
    };
  }
});

// src/extract.js
async function extract(options) {
  if (!options) {
    throw new Error("Options object is required");
  }
  if ((!options.files || !Array.isArray(options.files) || options.files.length === 0) && !options.inputFile) {
    throw new Error("Either files array or inputFile must be provided");
  }
  const binaryPath = await getBinaryPath(options.binaryOptions || {});
  const cliArgs = buildCliArgs(options, EXTRACT_FLAG_MAP);
  if (options.json && !options.format) {
    cliArgs.push("--format", "json");
  }
  if (options.files && Array.isArray(options.files) && options.files.length > 0) {
    for (const file of options.files) {
      cliArgs.push(escapeString(file));
    }
  }
  if (process.env.DEBUG === "1") {
    let logMessage = `
Extract:`;
    if (options.files && options.files.length > 0) {
      logMessage += ` files="${options.files.join(", ")}"`;
    }
    if (options.inputFile) logMessage += ` inputFile="${options.inputFile}"`;
    if (options.allowTests) logMessage += " allowTests=true";
    if (options.contextLines) logMessage += ` contextLines=${options.contextLines}`;
    if (options.format) logMessage += ` format=${options.format}`;
    if (options.json) logMessage += " json=true";
    console.error(logMessage);
  }
  const command = `${binaryPath} extract ${cliArgs.join(" ")}`;
  try {
    const { stdout, stderr } = await execAsync3(command);
    if (stderr) {
      console.error(`stderr: ${stderr}`);
    }
    let tokenUsage = {
      requestTokens: 0,
      responseTokens: 0,
      totalTokens: 0
    };
    if (options.files && Array.isArray(options.files)) {
      tokenUsage.requestTokens = options.files.join(" ").length / 4;
    } else if (options.inputFile) {
      tokenUsage.requestTokens = options.inputFile.length / 4;
    }
    if (stdout.includes("Total tokens returned:")) {
      const tokenMatch = stdout.match(/Total tokens returned: (\d+)/);
      if (tokenMatch && tokenMatch[1]) {
        tokenUsage.responseTokens = parseInt(tokenMatch[1], 10);
        tokenUsage.totalTokens = tokenUsage.requestTokens + tokenUsage.responseTokens;
      }
    }
    let output = stdout;
    if (!output.includes("Token Usage:")) {
      output += `
Token Usage:
  Request tokens: ${tokenUsage.requestTokens}
  Response tokens: ${tokenUsage.responseTokens}
  Total tokens: ${tokenUsage.totalTokens}
`;
    }
    if (options.json || options.format === "json") {
      try {
        const jsonOutput = JSON.parse(stdout);
        if (!jsonOutput.token_usage) {
          jsonOutput.token_usage = {
            request_tokens: tokenUsage.requestTokens,
            response_tokens: tokenUsage.responseTokens,
            total_tokens: tokenUsage.totalTokens
          };
        }
        return jsonOutput;
      } catch (error) {
        console.error("Error parsing JSON output:", error);
        return output;
      }
    }
    return output;
  } catch (error) {
    const errorMessage = `Error executing extract command: ${error.message}
Command: ${command}`;
    throw new Error(errorMessage);
  }
}
var import_child_process4, import_util4, execAsync3, EXTRACT_FLAG_MAP;
var init_extract = __esm({
  "src/extract.js"() {
    "use strict";
    import_child_process4 = require("child_process");
    import_util4 = require("util");
    init_utils();
    execAsync3 = (0, import_util4.promisify)(import_child_process4.exec);
    EXTRACT_FLAG_MAP = {
      allowTests: "--allow-tests",
      contextLines: "--context",
      format: "--format",
      inputFile: "--input-file"
    };
  }
});

// src/tools/common.js
function parseXmlToolCall(xmlString) {
  const toolMatch = xmlString.match(/<([a-zA-Z0-9_]+)>([\s\S]*?)<\/\1>/);
  if (!toolMatch) {
    return null;
  }
  const toolName = toolMatch[1];
  const innerContent = toolMatch[2];
  const params = {};
  const paramRegex = /<([a-zA-Z0-9_]+)>([\s\S]*?)<\/\1>/g;
  let paramMatch;
  while ((paramMatch = paramRegex.exec(innerContent)) !== null) {
    const paramName = paramMatch[1];
    let paramValue = paramMatch[2].trim();
    if (paramValue.toLowerCase() === "true") {
      paramValue = true;
    } else if (paramValue.toLowerCase() === "false") {
      paramValue = false;
    } else if (!isNaN(paramValue) && paramValue.trim() !== "") {
      const num = Number(paramValue);
      if (Number.isFinite(num)) {
        paramValue = num;
      }
    }
    params[paramName] = paramValue;
  }
  if (toolName === "attempt_completion") {
    const resultMatch = innerContent.match(/<result>([\s\S]*?)<\/result>/);
    if (resultMatch) {
      params["result"] = resultMatch[1].trim();
    }
    const commandMatch = innerContent.match(/<command>([\s\S]*?)<\/command>/);
    if (commandMatch) {
      params["command"] = commandMatch[1].trim();
    }
  }
  return { toolName, params };
}
var import_zod, searchSchema, querySchema, extractSchema, attemptCompletionSchema, searchToolDefinition, queryToolDefinition, extractToolDefinition, attemptCompletionToolDefinition, searchDescription, queryDescription, extractDescription;
var init_common = __esm({
  "src/tools/common.js"() {
    "use strict";
    import_zod = require("zod");
    searchSchema = import_zod.z.object({
      query: import_zod.z.string().describe("Search query with Elasticsearch syntax. Use + for important terms."),
      path: import_zod.z.string().optional().default(".").describe('Path to search in. For dependencies use "go:github.com/owner/repo", "js:package_name", or "rust:cargo_name" etc.'),
      allow_tests: import_zod.z.boolean().optional().default(false).describe("Allow test files in search results"),
      exact: import_zod.z.boolean().optional().default(false).describe("Perform exact search without tokenization (case-insensitive)"),
      maxResults: import_zod.z.number().optional().describe("Maximum number of results to return"),
      maxTokens: import_zod.z.number().optional().default(1e4).describe("Maximum number of tokens to return"),
      language: import_zod.z.string().optional().describe("Limit search to files of a specific programming language")
    });
    querySchema = import_zod.z.object({
      pattern: import_zod.z.string().describe("AST pattern to search for. Use $NAME for variable names, $$$PARAMS for parameter lists, etc."),
      path: import_zod.z.string().optional().default(".").describe("Path to search in"),
      language: import_zod.z.string().optional().default("rust").describe("Programming language to use for parsing"),
      allow_tests: import_zod.z.boolean().optional().default(false).describe("Allow test files in search results")
    });
    extractSchema = import_zod.z.object({
      file_path: import_zod.z.string().optional().describe("Path to the file to extract from. Can include line numbers or symbol names"),
      input_content: import_zod.z.string().optional().describe("Text content to extract file paths from"),
      line: import_zod.z.number().optional().describe("Start line number to extract a specific code block"),
      end_line: import_zod.z.number().optional().describe("End line number for extracting a range of lines"),
      allow_tests: import_zod.z.boolean().optional().default(false).describe("Allow test files and test code blocks"),
      context_lines: import_zod.z.number().optional().default(10).describe("Number of context lines to include"),
      format: import_zod.z.string().optional().default("plain").describe("Output format (plain, markdown, json, color)")
    });
    attemptCompletionSchema = import_zod.z.object({
      result: import_zod.z.string().describe("The final result of the task. Formulate this result in a way that is final and does not require further input from the user. Do not end your result with questions or offers for further assistance."),
      command: import_zod.z.string().optional().describe("A CLI command to execute to show a live demo of the result to the user (e.g., `open index.html`). Do not use commands like `echo` or `cat` that merely print text.")
    });
    searchToolDefinition = `
## search
Description: Search code in the repository using Elasticsearch query syntax (except field based queries, e.g. "filename:..." NOT supported).

You need to focus on main keywords when constructing the query, and always use elastic search syntax like OR AND and brackets to group keywords.
Parameters:
- query: (required) Search query with Elasticsearch syntax. You can use + for important terms, and - for negation.
- path: (required) Path to search in. All dependencies located in /dep folder, under language sub folders, like this: "/dep/go/github.com/owner/repo", "/dep/js/package_name", or "/dep/rust/cargo_name" etc. YOU SHOULD ALWAYS provide FULL PATH when searching dependencies, including depency name.
- allow_tests: (optional, default: false) Allow test files in search results (true/false).
- exact: (optional, default: false) Perform exact pricise search. Use it when you already know function or struct name, or some other code block, and want exact match.
- maxResults: (optional) Maximum number of results to return (number).
- maxTokens: (optional, default: 10000) Maximum number of tokens to return (number).
- language: (optional) Limit search to files of a specific programming language (e.g., 'rust', 'js', 'python', 'go' etc.).


Usage Example:

<examples>

User: How to calculate the total amount in the payments module?
<search>
<query>calculate AND payment</query>
<path>src/utils</path>
<allow_tests>false</allow_tests>
</search>

User: How do the user authentication and authorization work?
<search>
<query>+user and (authentification OR authroization OR authz)</query>
<path>.</path>
<allow_tests>true</allow_tests>
<language>go</language>
</search>

User: Find all react imports in the project.
<search>
<query>import { react }</query>
<path>.</path>
<exact>true</exact>
<language>js</language>
</search>


User: Find how decompoud library works?
<search>
<query>import { react }</query>
<path>/dep/rust/decompound</path>
<language>rust</language>
</search>

</examples>
`;
    queryToolDefinition = `
## query
Description: Search code using ast-grep structural pattern matching. Use this tool to find specific code structures like functions, classes, or methods.
Parameters:
- pattern: (required) AST pattern to search for. Use $NAME for variable names, $$$PARAMS for parameter lists, etc.
- path: (optional, default: '.') Path to search in.
- language: (optional, default: 'rust') Programming language to use for parsing.
- allow_tests: (optional, default: false) Allow test files in search results (true/false).
Usage Example:

<examples>

<query>
<pattern>function $FUNC($$$PARAMS) { $$$BODY }</pattern>
<path>src/parser</path>
<language>js</language>
</query>

</examples>
`;
    extractToolDefinition = `
## extract
Description: Extract code blocks from files based on file paths and optional line numbers. Use this tool to see complete context after finding relevant files. It can be used to read full files as well. 
Full file extraction should be the LAST RESORT! Always prefer search.

Parameters:
- file_path: (required) Path to the file to extract from. Can include line numbers or symbol names (e.g., 'src/main.rs:10-20', 'src/utils.js#myFunction').
- line: (optional) Start line number to extract a specific code block. Use with end_line for ranges.
- end_line: (optional) End line number for extracting a range of lines.
- allow_tests: (optional, default: false) Allow test files and test code blocks (true/false).
Usage Example:

<examples>

User: How RankManager works
<extract>
<file_path>src/search/ranking.rs#RankManager</file_path>
</extract>

User: Lets read the whole file
<extract>
<file_path>src/search/ranking.rs</file_path>
</extract>

User: Read the first 10 lines of the file
<extract>
<file_path>src/search/ranking.rs</file_path>
<line>1</line>
<end_line>10</end_line>
</extract>

User: Read file inside the dependency
<extract>
<file_path>/dep/go/github.com/gorilla/mux/router.go</file_path>
</extract>


</examples>
`;
    attemptCompletionToolDefinition = `
## attempt_completion
Description: Use this tool ONLY when the task is fully complete and you have received confirmation of success for all previous tool uses. Presents the final result to the user.
Parameters:
- result: (required) The final result of the task. Formulate this result concisely and definitively. Do not end with questions or offers for further assistance. Ensure that answer fully addresses the user's request, and a clear and detailed maneer.
- command: (optional) A CLI command to demonstrate the result (e.g., 'open index.html'). Avoid simple print commands like 'echo'.
Usage Example:
<attempt_completion>
<result>I have refactored the search module according to the requirements and verified the tests pass.</result>
<command>cargo test --lib</command>
</attempt_completion>
`;
    searchDescription = "Search code in the repository using Elasticsearch-like query syntax. Use this tool first for any code-related questions.";
    queryDescription = "Search code using ast-grep structural pattern matching. Use this tool to find specific code structures like functions, classes, or methods.";
    extractDescription = "Extract code blocks from files based on file paths and optional line numbers. Use this tool to see complete context after finding relevant files.";
  }
});

// src/tools/vercel.js
var import_ai, searchTool, queryTool, extractTool;
var init_vercel = __esm({
  "src/tools/vercel.js"() {
    "use strict";
    import_ai = require("ai");
    init_search();
    init_query();
    init_extract();
    init_common();
    searchTool = (options = {}) => {
      const { sessionId, maxTokens = 1e4, debug = false } = options;
      return (0, import_ai.tool)({
        name: "search",
        description: searchDescription,
        parameters: searchSchema,
        execute: async ({ query: searchQuery, path: path6, allow_tests, exact, maxTokens: paramMaxTokens, language }) => {
          try {
            const effectiveMaxTokens = paramMaxTokens || maxTokens;
            let searchPath = path6 || options.defaultPath || ".";
            if ((searchPath === "." || searchPath === "./") && options.defaultPath) {
              if (debug) {
                console.error(`Using default path "${options.defaultPath}" instead of "${searchPath}"`);
              }
              searchPath = options.defaultPath;
            }
            if (debug) {
              console.error(`Executing search with query: "${searchQuery}", path: "${searchPath}", exact: ${exact ? "true" : "false"}, language: ${language || "all"}, session: ${sessionId || "none"}`);
            }
            const results = await search({
              query: searchQuery,
              path: searchPath,
              allow_tests,
              exact,
              json: false,
              maxTokens: effectiveMaxTokens,
              session: sessionId,
              // Pass session ID if provided
              language
              // Pass language parameter if provided
            });
            return results;
          } catch (error) {
            console.error("Error executing search command:", error);
            return `Error executing search command: ${error.message}`;
          }
        }
      });
    };
    queryTool = (options = {}) => {
      const { debug = false } = options;
      return (0, import_ai.tool)({
        name: "query",
        description: queryDescription,
        parameters: querySchema,
        execute: async ({ pattern, path: path6, language, allow_tests }) => {
          try {
            let queryPath = path6 || options.defaultPath || ".";
            if ((queryPath === "." || queryPath === "./") && options.defaultPath) {
              if (debug) {
                console.error(`Using default path "${options.defaultPath}" instead of "${queryPath}"`);
              }
              queryPath = options.defaultPath;
            }
            if (debug) {
              console.error(`Executing query with pattern: "${pattern}", path: "${queryPath}", language: ${language || "auto"}`);
            }
            const results = await query({
              pattern,
              path: queryPath,
              language,
              allow_tests,
              json: false
            });
            return results;
          } catch (error) {
            console.error("Error executing query command:", error);
            return `Error executing query command: ${error.message}`;
          }
        }
      });
    };
    extractTool = (options = {}) => {
      const { debug = false } = options;
      return (0, import_ai.tool)({
        name: "extract",
        description: extractDescription,
        parameters: extractSchema,
        execute: async ({ file_path, input_content, line, end_line, allow_tests, context_lines, format }) => {
          try {
            let extractPath = options.defaultPath || ".";
            if ((extractPath === "." || extractPath === "./") && options.defaultPath) {
              if (debug) {
                console.error(`Using default path "${options.defaultPath}" instead of "${extractPath}"`);
              }
              extractPath = options.defaultPath;
            }
            if (debug) {
              if (file_path) {
                console.error(`Executing extract with file: "${file_path}", path: "${extractPath}", context lines: ${context_lines || 10}`);
              } else if (input_content) {
                console.error(`Executing extract with input content, path: "${extractPath}", context lines: ${context_lines || 10}`);
              }
            }
            let tempFilePath = null;
            let extractOptions = { path: extractPath };
            if (input_content) {
              const { writeFileSync, unlinkSync } = await import("fs");
              const { join } = await import("path");
              const { tmpdir } = await import("os");
              const { randomUUID: randomUUID4 } = await import("crypto");
              tempFilePath = join(tmpdir(), `probe-extract-${randomUUID4()}.txt`);
              writeFileSync(tempFilePath, input_content);
              if (debug) {
                console.error(`Created temporary file for input content: ${tempFilePath}`);
              }
              extractOptions = {
                inputFile: tempFilePath,
                allowTests: allow_tests,
                contextLines: context_lines,
                format
              };
            } else if (file_path) {
              const files = [file_path];
              extractOptions = {
                files,
                allowTests: allow_tests,
                contextLines: context_lines,
                format
              };
            } else {
              throw new Error("Either file_path or input_content must be provided");
            }
            const results = await extract(extractOptions);
            if (tempFilePath) {
              const { unlinkSync } = await import("fs");
              try {
                unlinkSync(tempFilePath);
                if (debug) {
                  console.error(`Removed temporary file: ${tempFilePath}`);
                }
              } catch (cleanupError) {
                console.error(`Warning: Failed to remove temporary file: ${cleanupError.message}`);
              }
            }
            return results;
          } catch (error) {
            console.error("Error executing extract command:", error);
            return `Error executing extract command: ${error.message}`;
          }
        }
      });
    };
  }
});

// src/tools/langchain.js
var init_langchain = __esm({
  "src/tools/langchain.js"() {
    "use strict";
    init_search();
    init_query();
    init_extract();
    init_common();
  }
});

// src/tools/system-message.js
var DEFAULT_SYSTEM_MESSAGE;
var init_system_message = __esm({
  "src/tools/system-message.js"() {
    "use strict";
    DEFAULT_SYSTEM_MESSAGE = `[Persona & Objective]

You are Probe, a specialized code intelligence assistant. Your objective is to accurately answer questions about multi-language codebases by effectively using your available tools: \`search\`, \`query\`, and \`extract\`.

[Core Workflow & Principles]

1.  **Tool-First Always:** Immediately use tools for any code-related query. Do not guess or use general knowledge.
2.  **Mandatory Path:** ALL tool calls (\`search\`, \`query\`, \`extract\`) MUST include the \`path\` argument. Use \`"."\` for the whole project, specific directories/files (e.g., \`"src/api"\`, \`"pkg/utils/helpers.py"\`), or dependency syntax (e.g., \`"go:github.com/gin-gonic/gin"\`, \`"js:@ai-sdk/anthropic"\`, \`"rust:serde"\`).
3.  **Start with \`search\`:**
    *   **Keywords are Key:** Formulate queries like you would in Elasticsearch. Use specific keywords, boolean operators (\`AND\`, \`OR\`, \`NOT\`), and exact phrases (\`""\`). This is NOT a simple text search.
    *   **Iterate if Needed:** If initial results are too broad or insufficient, **repeat the exact same \`search\` query** to get the next page of results (pagination). Reuse the \`sessionID\` if provided by the previous identical search. If results are irrelevant, refine the keywords (add terms, use \`NOT\`, try synonyms).
4.  **Analyze & Refine:** Review \`search\` results (snippets, file paths).
    *   Use \`query\` if you need code based on *structure* (AST patterns) within specific files/directories identified by \`search\`.
    *   Use \`extract\` if \`search\` or \`query\` identified the exact location (file, symbol, line range) and you need the full definition or more context.
5.  **Synthesize & Cite:** Construct the answer *only* from tool outputs. ALWAYS cite the specific file paths and relevant locations (symbols, line numbers) found. Adapt detail to the likely user role (developer vs. PM).
6.  **Clarify Sparingly:** If an initial \`search\` attempt completely fails due to ambiguity, ask a *specific* question to guide the next search. Don't ask before trying a search first.

[Tool Reference]

*   \`search\`
    *   **Purpose:** Find relevant code snippets/files using keyword-based search (like Elasticsearch). Locate named symbols. Search project code or dependencies.
    *   **Syntax:** \`query\` (Elasticsearch-like string: keywords, \`AND\`, \`OR\`, \`NOT\`, \`""\` exact phrases), \`path\` (Mandatory: \`"."\`, \`"path/to/dir"\`, \`"path/to/file.ext"\`, \`"go:pkg"\`, \`"js:npm_module"\`, \`"rust:crate"\`), \`exact\` (Optional: Set to \`true\` for case-insensitive exact matching without tokenization).
    *   **Features:** Returns snippets/paths. Supports pagination (repeat query). Caching via \`sessionID\` (reuse if returned). Use \`exact\` flag when you need precise matching of terms.
*   \`query\`
    *   **Purpose:** Find code by its *structure* (AST patterns) within specific files/directories, typically after \`search\`.
    *   **Syntax:** \`pattern\` (ast-grep pattern), \`language\` (e.g., "go", "python").
    *   **Mandatory Argument:** \`path\` (file or directory path, e.g., \`"src/services"\`, \`"app/main.py"\`).
*   \`extract\`
    *   **Purpose:** Retrieve specific code blocks or entire files *after* \`search\` or \`query\` identifies the target.
    *   **Syntax:** Optional \`#symbol\` (e.g., \`#MyClass\`), \`#Lstart-Lend\` (e.g., \`#L50-L75\`).
    *   **Mandatory Argument:** \`path\` (specific file path, e.g., \`"src/utils/helpers.go"\`, or dependency file like \`"go:github.com/gin-gonic/gin/context.go"\`).

[Examples]

*   **Example 1: Finding a Specific Function Definition**
    *   User: "Show me the code for the \`calculate_total\` function in our payments module."
    *   Probe Action 1: \`search\` query: \`"calculate_total"\`, path: \`"src/payments"\` (Targeted search in the likely directory)
    *   (Analysis: Search returns a clear hit in \`src/payments/logic.py\`.)
    *   Probe Action 2: \`extract\` path: \`"src/payments/logic.py#calculate_total"\`
    *   (Response: Provide the extracted function code, citing \`src/payments/logic.py#calculate_total\`.)

*   **Example 2: Investigating Initialization**
    *   User: "Where is the primary configuration for the Redis cache loaded?"
    *   Probe Action 1: \`search\` query: \`redis AND (config OR load OR init OR setup) NOT test\`, path: \`"."\`
    *   (Analysis: Results point towards \`pkg/cache/redis.go\` and a function \`LoadRedisConfig\`.)
    *   Probe Action 2: \`extract\` path: \`"pkg/cache/redis.go#LoadRedisConfig"\`
    *   (Response: Explain config loading based on the extracted \`LoadRedisConfig\` function, citing \`pkg/cache/redis.go#LoadRedisConfig\`.)

*   **Example 3: Understanding Usage of a Dependency Feature**
    *   User: "How are we using the \`createAnthropic\` function from the \`@ai-sdk/anthropic\` library?"
    *   Probe Action 1: \`search\` query: \`"createAnthropic"\`, path: \`"."\` (Search project code for usage)
    *   (Analysis: Find usage in \`src/ai/providers.ts\`. Want to understand the library function itself better.)
    *   Probe Action 2: \`search\` query: \`"createAnthropic"\`, path: \`"js:@ai-sdk/anthropic"\` (Search within the specific dependency)
    *   (Analysis: Search locates the definition within the dependency code, e.g., \`node_modules/@ai-sdk/anthropic/dist/index.js\` or similar mapped path.)
    *   Probe Action 3: \`extract\` path: \`"js:@ai-sdk/anthropic/dist/index.js#createAnthropic"\` (Extract the specific function *from the dependency*. Note: Actual file path within dependency might vary, use the one found by search).
    *   (Response: Show how \`createAnthropic\` is used in \`src/ai/providers.ts\`, and explain its purpose based on the extracted definition from the \`@ai-sdk/anthropic\` library, citing both files.)

*   **Example 4: Exploring Error Handling Patterns**
    *   User: "What's the standard way errors are wrapped or handled in our Go backend services?"
    *   Probe Action 1: \`search\` query: \`error AND (wrap OR handle OR new) AND lang:go NOT test\`, path: \`"service/"\` (Focus on service directories)
    *   (Analysis: Many results. See frequent use of \`fmt.Errorf\` and a custom \`errors.Wrap\` in several files like \`service/user/handler.go\`.)
    *   Probe Action 2: \`search\` query: \`import AND "pkg/errors"\`, path: \`"service/"\` (Check where a potential custom error package is used)
    *   (Analysis: Confirms \`pkg/errors\` is widely used.)
    *   Probe Action 3: \`query\` language: \`go\`, pattern: \`errors.Wrap($$$)\`, path: \`"service/"\` (Find structural usage of the custom wrapper)
    *   (Response: Summarize error handling: Mention standard \`fmt.Errorf\` and the prevalent use of a custom \`errors.Wrap\` function from \`pkg/errors\`, providing examples from locations found by search/query like \`service/user/handler.go\`.)`;
  }
});

// src/tools/index.js
var tools;
var init_tools = __esm({
  "src/tools/index.js"() {
    "use strict";
    init_vercel();
    init_langchain();
    init_common();
    init_system_message();
    init_vercel();
    init_system_message();
    tools = {
      searchTool: searchTool(),
      queryTool: queryTool(),
      extractTool: extractTool(),
      DEFAULT_SYSTEM_MESSAGE
    };
  }
});

// src/utils/file-lister.js
async function listFilesByLevel(options) {
  const {
    directory,
    maxFiles = 100,
    respectGitignore = true
  } = options;
  if (!import_fs.default.existsSync(directory)) {
    throw new Error(`Directory does not exist: ${directory}`);
  }
  const gitDirExists = import_fs.default.existsSync(import_path4.default.join(directory, ".git"));
  if (gitDirExists && respectGitignore) {
    try {
      return await listFilesUsingGit(directory, maxFiles);
    } catch (error) {
      console.error(`Warning: Failed to use git ls-files: ${error.message}`);
      console.error("Falling back to manual file listing");
    }
  }
  return await listFilesByLevelManually(directory, maxFiles, respectGitignore);
}
async function listFilesUsingGit(directory, maxFiles) {
  const { stdout } = await execAsync4("git ls-files", { cwd: directory });
  const files = stdout.split("\n").filter(Boolean);
  const sortedFiles = files.sort((a, b) => {
    const depthA = a.split(import_path4.default.sep).length;
    const depthB = b.split(import_path4.default.sep).length;
    return depthA - depthB;
  });
  return sortedFiles.slice(0, maxFiles);
}
async function listFilesByLevelManually(directory, maxFiles, respectGitignore) {
  let ignorePatterns = [];
  if (respectGitignore) {
    ignorePatterns = loadGitignorePatterns(directory);
  }
  const result = [];
  const queue = [{ dir: directory, level: 0 }];
  while (queue.length > 0 && result.length < maxFiles) {
    const { dir, level } = queue.shift();
    try {
      const entries = import_fs.default.readdirSync(dir, { withFileTypes: true });
      const files = entries.filter((entry) => entry.isFile());
      for (const file of files) {
        if (result.length >= maxFiles) break;
        const filePath = import_path4.default.join(dir, file.name);
        const relativePath = import_path4.default.relative(directory, filePath);
        if (shouldIgnore(relativePath, ignorePatterns)) continue;
        result.push(relativePath);
      }
      const dirs = entries.filter((entry) => entry.isDirectory());
      for (const subdir of dirs) {
        const subdirPath = import_path4.default.join(dir, subdir.name);
        const relativeSubdirPath = import_path4.default.relative(directory, subdirPath);
        if (shouldIgnore(relativeSubdirPath, ignorePatterns)) continue;
        if (subdir.name === "node_modules" || subdir.name === ".git") continue;
        queue.push({ dir: subdirPath, level: level + 1 });
      }
    } catch (error) {
      console.error(`Warning: Could not read directory ${dir}: ${error.message}`);
    }
  }
  return result;
}
function loadGitignorePatterns(directory) {
  const gitignorePath = import_path4.default.join(directory, ".gitignore");
  if (!import_fs.default.existsSync(gitignorePath)) {
    return [];
  }
  try {
    const content = import_fs.default.readFileSync(gitignorePath, "utf8");
    return content.split("\n").map((line) => line.trim()).filter((line) => line && !line.startsWith("#"));
  } catch (error) {
    console.error(`Warning: Could not read .gitignore: ${error.message}`);
    return [];
  }
}
function shouldIgnore(filePath, ignorePatterns) {
  if (!ignorePatterns.length) return false;
  for (const pattern of ignorePatterns) {
    if (pattern === filePath) return true;
    if (pattern.endsWith("/") && filePath.startsWith(pattern)) return true;
    if (pattern.startsWith("*.") && filePath.endsWith(pattern.substring(1))) return true;
    if (pattern.startsWith("*") && filePath.endsWith(pattern.substring(1))) return true;
    if (pattern.endsWith("*") && filePath.startsWith(pattern.substring(0, pattern.length - 1))) return true;
  }
  return false;
}
var import_fs, import_path4, import_util5, import_child_process5, execAsync4;
var init_file_lister = __esm({
  "src/utils/file-lister.js"() {
    "use strict";
    import_fs = __toESM(require("fs"), 1);
    import_path4 = __toESM(require("path"), 1);
    import_util5 = require("util");
    import_child_process5 = require("child_process");
    execAsync4 = (0, import_util5.promisify)(import_child_process5.exec);
  }
});

// src/index.js
var init_index = __esm({
  "src/index.js"() {
    "use strict";
    init_search();
    init_query();
    init_extract();
    init_utils();
    init_tools();
    init_file_lister();
    init_system_message();
    init_common();
    init_vercel();
    init_ProbeAgent();
  }
});

// src/agent/tools.js
function createTools(configOptions) {
  return {
    searchTool: searchTool(configOptions),
    queryTool: queryTool(configOptions),
    extractTool: extractTool(configOptions)
  };
}
function parseXmlToolCallWithThinking(xmlString) {
  const thinkingMatch = xmlString.match(/<thinking>([\s\S]*?)<\/thinking>/);
  const thinkingContent = thinkingMatch ? thinkingMatch[1].trim() : null;
  const cleanedXmlString = xmlString.replace(/<thinking>[\s\S]*?<\/thinking>/g, "").trim();
  const parsedTool = parseXmlToolCall(cleanedXmlString);
  if (process.env.DEBUG === "1" && thinkingContent) {
    console.log(`[DEBUG] AI Thinking Process:
${thinkingContent}`);
  }
  return parsedTool;
}
var import_crypto2, implementToolDefinition, listFilesToolDefinition, searchFilesToolDefinition;
var init_tools2 = __esm({
  "src/agent/tools.js"() {
    "use strict";
    init_index();
    import_crypto2 = require("crypto");
    implementToolDefinition = `
## implement
Description: Implement a given task. Can modify files. Can be used ONLY if task explicitly stated that something requires modification or implementation.

Parameters:
- task: (required) The task description. Should be as detailed as possible, ideally pointing to exact files which needs be modified or created.
- autoCommits: (optional) Whether to enable auto-commits in aider. Default is false.

Usage Example:

<examples>

User: Can you implement a function to calculate Fibonacci numbers in main.js?
<implement>
<task>Implement a recursive function to calculate the nth Fibonacci number in main.js</task>
</implement>

User: Can you implement a function to calculate Fibonacci numbers in main.js with auto-commits?
<implement>
<task>Implement a recursive function to calculate the nth Fibonacci number in main.js</task>
<autoCommits>true</autoCommits>
</implement>

</examples>
`;
    listFilesToolDefinition = `
## listFiles
Description: List files and directories in a specified location.

Parameters:
- directory: (optional) The directory path to list files from. Defaults to current directory if not specified.

Usage Example:

<examples>

User: Can you list the files in the src directory?
<listFiles>
<directory>src</directory>
</listFiles>

User: What files are in the current directory?
<listFiles>
</listFiles>

</examples>
`;
    searchFilesToolDefinition = `
## searchFiles
Description: Find files with name matching a glob pattern with recursive search capability.

Parameters:
- pattern: (required) The glob pattern to search for (e.g., "**/*.js", "*.md").
- directory: (optional) The directory to search in. Defaults to current directory if not specified.
- recursive: (optional) Whether to search recursively. Defaults to true.

Usage Example:

<examples>

User: Can you find all JavaScript files in the project?
<searchFiles>
<pattern>**/*.js</pattern>
</searchFiles>

User: Find all markdown files in the docs directory, but only at the top level.
<searchFiles>
<pattern>*.md</pattern>
<directory>docs</directory>
<recursive>false</recursive>
</searchFiles>

</examples>
`;
  }
});

// src/agent/probeTool.js
function isSessionCancelled(sessionId) {
  return activeToolExecutions.get(sessionId)?.cancelled || false;
}
function registerToolExecution(sessionId) {
  if (!sessionId) return;
  if (!activeToolExecutions.has(sessionId)) {
    activeToolExecutions.set(sessionId, { cancelled: false });
  } else {
    activeToolExecutions.get(sessionId).cancelled = false;
  }
}
function clearToolExecutionData(sessionId) {
  if (!sessionId) return;
  if (activeToolExecutions.has(sessionId)) {
    activeToolExecutions.delete(sessionId);
    if (process.env.DEBUG === "1") {
      console.log(`Cleared tool execution data for session: ${sessionId}`);
    }
  }
}
function createWrappedTools(baseTools) {
  const wrappedTools = {};
  if (baseTools.searchTool) {
    wrappedTools.searchToolInstance = wrapToolWithEmitter(
      baseTools.searchTool,
      "search",
      baseTools.searchTool.execute
    );
  }
  if (baseTools.queryTool) {
    wrappedTools.queryToolInstance = wrapToolWithEmitter(
      baseTools.queryTool,
      "query",
      baseTools.queryTool.execute
    );
  }
  if (baseTools.extractTool) {
    wrappedTools.extractToolInstance = wrapToolWithEmitter(
      baseTools.extractTool,
      "extract",
      baseTools.extractTool.execute
    );
  }
  return wrappedTools;
}
var import_child_process6, import_util6, import_crypto3, import_events, import_fs2, import_fs3, import_path5, import_glob, toolCallEmitter, activeToolExecutions, wrapToolWithEmitter, listFilesTool, searchFilesTool, listFilesToolInstance, searchFilesToolInstance;
var init_probeTool = __esm({
  "src/agent/probeTool.js"() {
    "use strict";
    init_index();
    import_child_process6 = require("child_process");
    import_util6 = require("util");
    import_crypto3 = require("crypto");
    import_events = require("events");
    import_fs2 = __toESM(require("fs"), 1);
    import_fs3 = require("fs");
    import_path5 = __toESM(require("path"), 1);
    import_glob = require("glob");
    toolCallEmitter = new import_events.EventEmitter();
    activeToolExecutions = /* @__PURE__ */ new Map();
    wrapToolWithEmitter = (tool2, toolName, baseExecute) => {
      return {
        ...tool2,
        // Spread schema, description etc.
        execute: async (params) => {
          const debug = process.env.DEBUG === "1";
          const toolSessionId = params.sessionId || (0, import_crypto3.randomUUID)();
          if (debug) {
            console.log(`[DEBUG] probeTool: Executing ${toolName} for session ${toolSessionId}`);
          }
          registerToolExecution(toolSessionId);
          let executionError = null;
          let result = null;
          try {
            const toolCallStartData = {
              timestamp: (/* @__PURE__ */ new Date()).toISOString(),
              name: toolName,
              args: params,
              status: "started"
            };
            if (debug) {
              console.log(`[DEBUG] probeTool: Emitting toolCallStart:${toolSessionId}`);
            }
            toolCallEmitter.emit(`toolCall:${toolSessionId}`, toolCallStartData);
            if (isSessionCancelled(toolSessionId)) {
              if (debug) {
                console.log(`Tool execution cancelled before start for ${toolSessionId}`);
              }
              throw new Error(`Tool execution cancelled for session ${toolSessionId}`);
            }
            result = await baseExecute(params);
            if (isSessionCancelled(toolSessionId)) {
              if (debug) {
                console.log(`Tool execution cancelled after completion for ${toolSessionId}`);
              }
              throw new Error(`Tool execution cancelled for session ${toolSessionId}`);
            }
          } catch (error) {
            executionError = error;
            if (debug) {
              console.error(`[DEBUG] probeTool: Error in ${toolName}:`, error);
            }
          }
          if (executionError) {
            const toolCallErrorData = {
              timestamp: (/* @__PURE__ */ new Date()).toISOString(),
              name: toolName,
              args: params,
              error: executionError.message || "Unknown error",
              status: "error"
            };
            if (debug) {
              console.log(`[DEBUG] probeTool: Emitting toolCall:${toolSessionId} (error)`);
            }
            toolCallEmitter.emit(`toolCall:${toolSessionId}`, toolCallErrorData);
            throw executionError;
          } else {
            if (isSessionCancelled(toolSessionId)) {
              if (process.env.DEBUG === "1") {
                console.log(`Tool execution finished but session was cancelled for ${toolSessionId}`);
              }
              throw new Error(`Tool execution cancelled for session ${toolSessionId}`);
            }
            const toolCallData = {
              timestamp: (/* @__PURE__ */ new Date()).toISOString(),
              name: toolName,
              args: params,
              // Safely preview result
              resultPreview: typeof result === "string" ? result.length > 200 ? result.substring(0, 200) + "..." : result : result ? JSON.stringify(result).substring(0, 200) + "..." : "No Result",
              status: "completed"
            };
            if (debug) {
              console.log(`[DEBUG] probeTool: Emitting toolCall:${toolSessionId} (completed)`);
            }
            toolCallEmitter.emit(`toolCall:${toolSessionId}`, toolCallData);
            return result;
          }
        }
      };
    };
    listFilesTool = {
      execute: async (params) => {
        const { directory = "." } = params;
        try {
          const files = await listFilesByLevel({
            directory,
            maxFiles: 100,
            respectGitignore: !process.env.PROBE_NO_GITIGNORE || process.env.PROBE_NO_GITIGNORE === "",
            cwd: process.cwd()
          });
          return files;
        } catch (error) {
          throw new Error(`Failed to list files: ${error.message}`);
        }
      }
    };
    searchFilesTool = {
      execute: async (params) => {
        const { pattern, directory = ".", recursive = true } = params;
        if (!pattern) {
          throw new Error("Pattern is required for file search");
        }
        try {
          const options = {
            cwd: directory,
            ignore: ["node_modules/**", ".git/**"],
            absolute: false
          };
          if (!recursive) {
            options.deep = 1;
          }
          const files = await (0, import_glob.glob)(pattern, options);
          return files;
        } catch (error) {
          throw new Error(`Failed to search files: ${error.message}`);
        }
      }
    };
    listFilesToolInstance = wrapToolWithEmitter(listFilesTool, "listFiles", listFilesTool.execute);
    searchFilesToolInstance = wrapToolWithEmitter(searchFilesTool, "searchFiles", searchFilesTool.execute);
  }
});

// src/agent/schemaUtils.js
function cleanSchemaResponse(response) {
  if (!response || typeof response !== "string") {
    return response;
  }
  const trimmed = response.trim();
  const firstBracket = Math.min(
    trimmed.indexOf("{") >= 0 ? trimmed.indexOf("{") : Infinity,
    trimmed.indexOf("[") >= 0 ? trimmed.indexOf("[") : Infinity
  );
  const lastBracket = Math.max(
    trimmed.lastIndexOf("}"),
    trimmed.lastIndexOf("]")
  );
  if (firstBracket < Infinity && lastBracket >= 0 && firstBracket < lastBracket) {
    const beforeFirstBracket = trimmed.substring(0, firstBracket).trim();
    if (beforeFirstBracket === "" || beforeFirstBracket.match(/^```\w*$/)) {
      return trimmed.substring(firstBracket, lastBracket + 1);
    }
  }
  return response;
}
function validateJsonResponse(response) {
  try {
    const parsed = JSON.parse(response);
    return { isValid: true, parsed };
  } catch (error) {
    return { isValid: false, error: error.message };
  }
}
function isJsonSchema(schema) {
  if (!schema || typeof schema !== "string") {
    return false;
  }
  const trimmedSchema = schema.trim().toLowerCase();
  const jsonIndicators = [
    trimmedSchema.startsWith("{") && trimmedSchema.includes("}"),
    trimmedSchema.startsWith("[") && trimmedSchema.includes("]"),
    trimmedSchema.includes('"type"') && trimmedSchema.includes("object"),
    trimmedSchema.includes('"properties"'),
    trimmedSchema.includes("json"),
    trimmedSchema.includes("application/json")
  ];
  return jsonIndicators.some((indicator) => indicator);
}
function createJsonCorrectionPrompt(invalidResponse, schema, error, detailedError = "") {
  let prompt = `Your previous response is not valid JSON and cannot be parsed. Here's what you returned:

${invalidResponse}

Error: ${error}`;
  if (detailedError && detailedError !== error) {
    prompt += `
Detailed Error: ${detailedError}`;
  }
  prompt += `

Please correct your response to be valid JSON that matches this schema:
${schema}

Return ONLY the corrected JSON, with no additional text or markdown formatting.`;
  return prompt;
}
function extractMermaidFromMarkdown(response) {
  if (!response || typeof response !== "string") {
    return { diagrams: [], cleanedResponse: response };
  }
  const mermaidBlockRegex = /```mermaid([^\n]*)\n([\s\S]*?)```/gi;
  const diagrams = [];
  let match;
  while ((match = mermaidBlockRegex.exec(response)) !== null) {
    const attributes = match[1] ? match[1].trim() : "";
    const fullContent = match[2].trim();
    diagrams.push({
      content: fullContent,
      fullMatch: match[0],
      startIndex: match.index,
      endIndex: match.index + match[0].length,
      attributes
    });
  }
  return { diagrams, cleanedResponse: response };
}
async function validateMermaidDiagram(diagram) {
  if (!diagram || typeof diagram !== "string") {
    return { isValid: false, error: "Empty or invalid diagram input" };
  }
  try {
    const trimmedDiagram = diagram.trim();
    if (trimmedDiagram.includes("```")) {
      return {
        isValid: false,
        error: "Diagram contains markdown code block markers",
        detailedError: "Mermaid diagram should not contain ``` markers when extracted from markdown"
      };
    }
    const diagramPatterns = [
      { pattern: /^(graph|flowchart)/i, type: "flowchart" },
      { pattern: /^sequenceDiagram/i, type: "sequence" },
      { pattern: /^gantt/i, type: "gantt" },
      { pattern: /^pie/i, type: "pie" },
      { pattern: /^stateDiagram/i, type: "state" },
      { pattern: /^classDiagram/i, type: "class" },
      { pattern: /^erDiagram/i, type: "er" },
      { pattern: /^journey/i, type: "journey" },
      { pattern: /^gitgraph/i, type: "gitgraph" },
      { pattern: /^requirementDiagram/i, type: "requirement" },
      { pattern: /^C4Context/i, type: "c4" }
    ];
    let diagramType = null;
    for (const { pattern, type } of diagramPatterns) {
      if (pattern.test(trimmedDiagram)) {
        diagramType = type;
        break;
      }
    }
    if (!diagramType) {
      return {
        isValid: false,
        error: "Diagram does not match any known Mermaid diagram pattern",
        detailedError: "The diagram must start with a valid Mermaid diagram type (graph, sequenceDiagram, gantt, pie, etc.)"
      };
    }
    const lines = trimmedDiagram.split("\n");
    for (let i = 0; i < lines.length; i++) {
      const line = lines[i].trim();
      if (!line) continue;
      if (diagramType === "flowchart") {
        const brackets = line.match(/\[[^\]]*$/);
        if (brackets) {
          return {
            isValid: false,
            error: `Unclosed bracket on line ${i + 1}`,
            detailedError: `Line "${line}" contains an unclosed bracket`
          };
        }
      }
      if (diagramType === "sequence") {
        if (line.includes("->>") && !line.includes(":")) {
          return {
            isValid: false,
            error: `Missing colon in sequence message on line ${i + 1}`,
            detailedError: `Line "${line}" appears to be a sequence message but is missing a colon`
          };
        }
      }
    }
    return {
      isValid: true,
      diagramType
    };
  } catch (error) {
    return {
      isValid: false,
      error: error.message || "Unknown mermaid parsing error",
      detailedError: error.stack || error.toString()
    };
  }
}
async function validateMermaidResponse(response) {
  const { diagrams } = extractMermaidFromMarkdown(response);
  if (diagrams.length === 0) {
    return { isValid: false, diagrams: [], errors: ["No mermaid diagrams found in response"] };
  }
  const results = [];
  const errors = [];
  for (let i = 0; i < diagrams.length; i++) {
    const diagramObj = diagrams[i];
    const validation = await validateMermaidDiagram(diagramObj.content);
    results.push({
      ...diagramObj,
      ...validation
    });
    if (!validation.isValid) {
      errors.push(`Diagram ${i + 1}: ${validation.error}`);
    }
  }
  const isValid = results.every((result) => result.isValid);
  return {
    isValid,
    diagrams: results,
    errors: errors.length > 0 ? errors : void 0
  };
}
async function validateAndFixMermaidResponse(response, options = {}) {
  const { schema, debug, path: path6, provider, model, tracer } = options;
  const validation = await validateMermaidResponse(response);
  if (validation.isValid) {
    return {
      ...validation,
      wasFixed: false,
      originalResponse: response,
      fixedResponse: response
    };
  }
  if (!validation.diagrams || validation.diagrams.length === 0) {
    return {
      ...validation,
      wasFixed: false,
      originalResponse: response,
      fixedResponse: response
    };
  }
  if (debug) {
    console.error("[DEBUG] Invalid Mermaid diagrams detected, starting specialized fixing agent...");
  }
  try {
    const mermaidFixer = new MermaidFixingAgent({
      path: path6,
      provider,
      model,
      debug,
      tracer
    });
    let fixedResponse = response;
    const fixingResults = [];
    const { diagrams } = extractMermaidFromMarkdown(response);
    const invalidDiagrams = validation.diagrams.map((result, index) => ({ ...result, originalIndex: index })).filter((result) => !result.isValid).reverse();
    for (const invalidDiagram of invalidDiagrams) {
      try {
        const fixedContent = await mermaidFixer.fixMermaidDiagram(
          invalidDiagram.content,
          [invalidDiagram.error],
          { diagramType: invalidDiagram.diagramType }
        );
        if (fixedContent && fixedContent !== invalidDiagram.content) {
          const originalDiagram = diagrams[invalidDiagram.originalIndex];
          const attributesStr = originalDiagram.attributes ? ` ${originalDiagram.attributes}` : "";
          const newCodeBlock = `\`\`\`mermaid${attributesStr}
${fixedContent}
\`\`\``;
          fixedResponse = fixedResponse.slice(0, originalDiagram.startIndex) + newCodeBlock + fixedResponse.slice(originalDiagram.endIndex);
          fixingResults.push({
            diagramIndex: invalidDiagram.originalIndex,
            wasFixed: true,
            originalContent: invalidDiagram.content,
            fixedContent,
            originalError: invalidDiagram.error
          });
          if (debug) {
            console.error(`[DEBUG] Fixed diagram ${invalidDiagram.originalIndex + 1}: ${invalidDiagram.error}`);
          }
        } else {
          fixingResults.push({
            diagramIndex: invalidDiagram.originalIndex,
            wasFixed: false,
            originalContent: invalidDiagram.content,
            originalError: invalidDiagram.error,
            fixingError: "No valid fix generated"
          });
        }
      } catch (error) {
        fixingResults.push({
          diagramIndex: invalidDiagram.originalIndex,
          wasFixed: false,
          originalContent: invalidDiagram.content,
          originalError: invalidDiagram.error,
          fixingError: error.message
        });
        if (debug) {
          console.error(`[DEBUG] Failed to fix diagram ${invalidDiagram.originalIndex + 1}: ${error.message}`);
        }
      }
    }
    const finalValidation = await validateMermaidResponse(fixedResponse);
    const wasActuallyFixed = fixingResults.some((result) => result.wasFixed);
    return {
      ...finalValidation,
      wasFixed: wasActuallyFixed,
      originalResponse: response,
      fixedResponse,
      fixingResults,
      tokenUsage: mermaidFixer.getTokenUsage()
    };
  } catch (error) {
    if (debug) {
      console.error(`[DEBUG] Mermaid fixing agent failed: ${error.message}`);
    }
    return {
      ...validation,
      wasFixed: false,
      originalResponse: response,
      fixedResponse: response,
      fixingError: error.message
    };
  }
}
var MermaidFixingAgent;
var init_schemaUtils = __esm({
  "src/agent/schemaUtils.js"() {
    "use strict";
    MermaidFixingAgent = class {
      constructor(options = {}) {
        this.ProbeAgent = null;
        this.options = {
          sessionId: options.sessionId || `mermaid-fixer-${Date.now()}`,
          path: options.path || process.cwd(),
          provider: options.provider,
          model: options.model,
          debug: options.debug,
          tracer: options.tracer,
          // Set to false since we're only fixing syntax, not implementing code
          allowEdit: false
        };
      }
      /**
       * Get the specialized prompt for mermaid diagram fixing
       */
      getMermaidFixingPrompt() {
        return `You are a world-class Mermaid diagram syntax correction specialist. Your expertise lies in analyzing and fixing Mermaid diagram syntax errors while preserving the original intent, structure, and semantic meaning.

CORE RESPONSIBILITIES:
- Analyze Mermaid diagrams for syntax errors and structural issues  
- Fix syntax errors while maintaining the original diagram's logical flow
- Ensure diagrams follow proper Mermaid syntax rules and best practices
- Handle all diagram types: flowchart, sequence, gantt, pie, state, class, er, journey, gitgraph, requirement, c4

MERMAID DIAGRAM TYPES & SYNTAX RULES:
1. **Flowchart/Graph**: Start with 'graph' or 'flowchart', use proper node definitions and arrows
2. **Sequence**: Start with 'sequenceDiagram', use proper participant and message syntax
3. **Gantt**: Start with 'gantt', use proper date formats and task definitions
4. **State**: Start with 'stateDiagram-v2', use proper state transitions
5. **Class**: Start with 'classDiagram', use proper class and relationship syntax
6. **Entity-Relationship**: Start with 'erDiagram', use proper entity and relationship syntax

FIXING METHODOLOGY:
1. **Identify diagram type** from the first line or content analysis
2. **Validate syntax** against Mermaid specification for that diagram type
3. **Fix errors systematically**:
   - Unclosed brackets, parentheses, or quotes
   - Missing or incorrect arrows and connectors
   - Invalid node IDs or labels
   - Incorrect formatting for diagram-specific elements
4. **Preserve semantic meaning** - never change the intended flow or relationships
5. **Use proper escaping** for special characters and spaces
6. **Ensure consistency** in naming conventions and formatting

CRITICAL RULES:
- ALWAYS output only the corrected Mermaid code within a \`\`\`mermaid code block
- NEVER add explanations, comments, or additional text outside the code block
- PRESERVE the original diagram's intended meaning and flow
- FIX syntax errors without changing the logical structure
- ENSURE the output is valid, parseable Mermaid syntax

When presented with a broken Mermaid diagram, analyze it thoroughly and provide the corrected version that maintains the original intent while fixing all syntax issues.`;
      }
      /**
       * Initialize the ProbeAgent if not already done
       */
      async initializeAgent() {
        if (!this.ProbeAgent) {
          const { ProbeAgent: ProbeAgent2 } = await Promise.resolve().then(() => (init_ProbeAgent(), ProbeAgent_exports));
          this.ProbeAgent = ProbeAgent2;
        }
        if (!this.agent) {
          this.agent = new this.ProbeAgent({
            sessionId: this.options.sessionId,
            customPrompt: this.getMermaidFixingPrompt(),
            path: this.options.path,
            provider: this.options.provider,
            model: this.options.model,
            debug: this.options.debug,
            tracer: this.options.tracer,
            allowEdit: this.options.allowEdit
          });
        }
        return this.agent;
      }
      /**
       * Fix a single Mermaid diagram using the specialized agent
       * @param {string} diagramContent - The broken Mermaid diagram content
       * @param {Array} originalErrors - Array of errors detected in the original diagram
       * @param {Object} diagramInfo - Additional context about the diagram (type, position, etc.)
       * @returns {Promise<string>} - The corrected Mermaid diagram
       */
      async fixMermaidDiagram(diagramContent, originalErrors = [], diagramInfo = {}) {
        await this.initializeAgent();
        const errorContext = originalErrors.length > 0 ? `

Detected errors: ${originalErrors.join(", ")}` : "";
        const diagramTypeHint = diagramInfo.diagramType ? `

Expected diagram type: ${diagramInfo.diagramType}` : "";
        const prompt = `Analyze and fix the following Mermaid diagram.${errorContext}${diagramTypeHint}

Broken Mermaid diagram:
\`\`\`mermaid
${diagramContent}
\`\`\`

Provide only the corrected Mermaid diagram within a mermaid code block. Do not add any explanations or additional text.`;
        try {
          const result = await this.agent.answer(prompt, [], {
            schema: "Return only valid Mermaid diagram code within ```mermaid code block"
          });
          const extractedDiagram = this.extractCorrectedDiagram(result);
          return extractedDiagram || result;
        } catch (error) {
          if (this.options.debug) {
            console.error(`[DEBUG] Mermaid fixing failed: ${error.message}`);
          }
          throw new Error(`Failed to fix Mermaid diagram: ${error.message}`);
        }
      }
      /**
       * Extract the corrected diagram from the agent's response
       * @param {string} response - The agent's response
       * @returns {string} - The extracted mermaid diagram
       */
      extractCorrectedDiagram(response) {
        const mermaidMatch = response.match(/```mermaid\s*\n([\s\S]*?)\n```/);
        if (mermaidMatch) {
          return mermaidMatch[1].trim();
        }
        const codeMatch = response.match(/```\s*\n([\s\S]*?)\n```/);
        if (codeMatch) {
          return codeMatch[1].trim();
        }
        return response.replace(/```\w*\n?/g, "").replace(/\n?```/g, "").trim();
      }
      /**
       * Get token usage information from the specialized agent
       * @returns {Object} - Token usage statistics
       */
      getTokenUsage() {
        return this.agent ? this.agent.getTokenUsage() : null;
      }
      /**
       * Cancel any ongoing operations
       */
      cancel() {
        if (this.agent) {
          this.agent.cancel();
        }
      }
    };
  }
});

// src/agent/ProbeAgent.js
var ProbeAgent_exports = {};
__export(ProbeAgent_exports, {
  ProbeAgent: () => ProbeAgent
});
module.exports = __toCommonJS(ProbeAgent_exports);
var import_anthropic, import_openai, import_google, import_ai2, import_crypto4, import_events2, MAX_TOOL_ITERATIONS, MAX_HISTORY_MESSAGES, ProbeAgent;
var init_ProbeAgent = __esm({
  "src/agent/ProbeAgent.js"() {
    import_anthropic = require("@ai-sdk/anthropic");
    import_openai = require("@ai-sdk/openai");
    import_google = require("@ai-sdk/google");
    import_ai2 = require("ai");
    import_crypto4 = require("crypto");
    import_events2 = require("events");
    init_tokenCounter();
    init_tools2();
    init_probeTool();
    init_index();
    init_schemaUtils();
    MAX_TOOL_ITERATIONS = parseInt(process.env.MAX_TOOL_ITERATIONS || "30", 10);
    MAX_HISTORY_MESSAGES = 100;
    ProbeAgent = class {
      /**
       * Create a new ProbeAgent instance
       * @param {Object} options - Configuration options
       * @param {string} [options.sessionId] - Optional session ID
       * @param {string} [options.customPrompt] - Custom prompt to replace the default system message
       * @param {string} [options.promptType] - Predefined prompt type (architect, code-review, support)
       * @param {boolean} [options.allowEdit=false] - Allow the use of the 'implement' tool
       * @param {string} [options.path] - Search directory path
       * @param {string} [options.provider] - Force specific AI provider
       * @param {string} [options.model] - Override model name
       * @param {boolean} [options.debug] - Enable debug mode
       */
      constructor(options = {}) {
        this.sessionId = options.sessionId || (0, import_crypto4.randomUUID)();
        this.customPrompt = options.customPrompt || null;
        this.promptType = options.promptType || "code-explorer";
        this.allowEdit = !!options.allowEdit;
        this.debug = options.debug || process.env.DEBUG === "1";
        this.cancelled = false;
        this.tracer = options.tracer || null;
        this.allowedFolders = options.path ? [options.path] : [process.cwd()];
        this.clientApiProvider = options.provider || null;
        this.clientApiKey = null;
        this.clientApiUrl = null;
        this.tokenCounter = new TokenCounter();
        if (this.debug) {
          console.log(`[DEBUG] Generated session ID for agent: ${this.sessionId}`);
          console.log(`[DEBUG] Maximum tool iterations configured: ${MAX_TOOL_ITERATIONS}`);
          console.log(`[DEBUG] Allow Edit (implement tool): ${this.allowEdit}`);
        }
        this.initializeTools();
        this.initializeModel();
        this.history = [];
        this.events = new import_events2.EventEmitter();
      }
      /**
       * Initialize tools with configuration
       */
      initializeTools() {
        const configOptions = {
          sessionId: this.sessionId,
          debug: this.debug,
          defaultPath: this.allowedFolders.length > 0 ? this.allowedFolders[0] : process.cwd(),
          allowedFolders: this.allowedFolders
        };
        const baseTools = createTools(configOptions);
        const wrappedTools = createWrappedTools(baseTools);
        this.toolImplementations = {
          search: wrappedTools.searchToolInstance,
          query: wrappedTools.queryToolInstance,
          extract: wrappedTools.extractToolInstance,
          listFiles: listFilesToolInstance,
          searchFiles: searchFilesToolInstance
        };
      }
      /**
       * Initialize the AI model based on available API keys and forced provider setting
       */
      initializeModel() {
        const anthropicApiKey = process.env.ANTHROPIC_API_KEY;
        const openaiApiKey = process.env.OPENAI_API_KEY;
        const googleApiKey = process.env.GOOGLE_API_KEY;
        const llmBaseUrl = process.env.LLM_BASE_URL;
        const anthropicApiUrl = process.env.ANTHROPIC_API_URL || llmBaseUrl;
        const openaiApiUrl = process.env.OPENAI_API_URL || llmBaseUrl;
        const googleApiUrl = process.env.GOOGLE_API_URL || llmBaseUrl;
        const modelName = process.env.MODEL_NAME;
        const forceProvider = this.clientApiProvider || (process.env.FORCE_PROVIDER ? process.env.FORCE_PROVIDER.toLowerCase() : null);
        if (this.debug) {
          console.log(`[DEBUG] Available API keys: Anthropic=${!!anthropicApiKey}, OpenAI=${!!openaiApiKey}, Google=${!!googleApiKey}`);
          console.log(`[DEBUG] Force provider: ${forceProvider || "(not set)"}`);
          if (modelName) console.log(`[DEBUG] Model override: ${modelName}`);
        }
        if (forceProvider) {
          if (forceProvider === "anthropic" && anthropicApiKey) {
            this.initializeAnthropicModel(anthropicApiKey, anthropicApiUrl, modelName);
            return;
          } else if (forceProvider === "openai" && openaiApiKey) {
            this.initializeOpenAIModel(openaiApiKey, openaiApiUrl, modelName);
            return;
          } else if (forceProvider === "google" && googleApiKey) {
            this.initializeGoogleModel(googleApiKey, googleApiUrl, modelName);
            return;
          }
          console.warn(`WARNING: Forced provider "${forceProvider}" selected but required API key is missing or invalid! Falling back to auto-detection.`);
        }
        if (anthropicApiKey) {
          this.initializeAnthropicModel(anthropicApiKey, anthropicApiUrl, modelName);
        } else if (openaiApiKey) {
          this.initializeOpenAIModel(openaiApiKey, openaiApiUrl, modelName);
        } else if (googleApiKey) {
          this.initializeGoogleModel(googleApiKey, googleApiUrl, modelName);
        } else {
          throw new Error("No API key provided. Please set ANTHROPIC_API_KEY, OPENAI_API_KEY, or GOOGLE_API_KEY environment variable.");
        }
      }
      /**
       * Initialize Anthropic model
       */
      initializeAnthropicModel(apiKey, apiUrl, modelName) {
        this.provider = (0, import_anthropic.createAnthropic)({
          apiKey,
          ...apiUrl && { baseURL: apiUrl }
        });
        this.model = modelName || "claude-opus-4-1-20250805";
        this.apiType = "anthropic";
        if (this.debug) {
          console.log(`Using Anthropic API with model: ${this.model}${apiUrl ? ` (URL: ${apiUrl})` : ""}`);
        }
      }
      /**
       * Initialize OpenAI model
       */
      initializeOpenAIModel(apiKey, apiUrl, modelName) {
        this.provider = (0, import_openai.createOpenAI)({
          compatibility: "strict",
          apiKey,
          ...apiUrl && { baseURL: apiUrl }
        });
        this.model = modelName || "gpt-5-thinking";
        this.apiType = "openai";
        if (this.debug) {
          console.log(`Using OpenAI API with model: ${this.model}${apiUrl ? ` (URL: ${apiUrl})` : ""}`);
        }
      }
      /**
       * Initialize Google model
       */
      initializeGoogleModel(apiKey, apiUrl, modelName) {
        this.provider = (0, import_google.createGoogleGenerativeAI)({
          apiKey,
          ...apiUrl && { baseURL: apiUrl }
        });
        this.model = modelName || "gemini-2.5-pro";
        this.apiType = "google";
        if (this.debug) {
          console.log(`Using Google API with model: ${this.model}${apiUrl ? ` (URL: ${apiUrl})` : ""}`);
        }
      }
      /**
       * Get the system message with instructions for the AI (XML Tool Format)
       */
      async getSystemMessage() {
        let toolDefinitions = `
${searchToolDefinition}
${queryToolDefinition}
${extractToolDefinition}
${listFilesToolDefinition}
${searchFilesToolDefinition}
${attemptCompletionToolDefinition}
`;
        if (this.allowEdit) {
          toolDefinitions += `${implementToolDefinition}
`;
        }
        let xmlToolGuidelines = `
# Tool Use Formatting

Tool use MUST be formatted using XML-style tags. The tool name is enclosed in opening and closing tags, and each parameter is similarly enclosed within its own set of tags. You MUST use exactly ONE tool call per message until you are ready to complete the task.

Structure:
<tool_name>
<parameter1_name>value1</parameter1_name>
<parameter2_name>value2</parameter2_name>
...
</tool_name>

Example:
<search>
<query>error handling</query>
<path>src/search</path>
</search>

# Thinking Process

Before using a tool, analyze the situation within <thinking></thinking> tags. This helps you organize your thoughts and make better decisions.

Example:
<thinking>
I need to find code related to error handling in the search module. The most appropriate tool for this is the search tool, which requires a query parameter and a path parameter. I have both the query ("error handling") and the path ("src/search"), so I can proceed with the search.
</thinking>

# Tool Use Guidelines

1. Think step-by-step about how to achieve the user's goal.
2. Use <thinking></thinking> tags to analyze the situation and determine the appropriate tool.
3. Choose **one** tool that helps achieve the current step.
4. Format the tool call using the specified XML format. Ensure all required parameters are included.
5. **You MUST respond with exactly one tool call in the specified XML format in each turn.**
6. Wait for the tool execution result, which will be provided in the next message (within a <tool_result> block).
7. Analyze the tool result and decide the next step. If more tool calls are needed, repeat steps 2-6.
8. If the task is fully complete and all previous steps were successful, use the \`<attempt_completion>\` tool to provide the final answer. This is the ONLY way to finish the task.
9. If you cannot proceed (e.g., missing information, invalid request), explain the issue clearly before using \`<attempt_completion>\` with an appropriate message in the \`<result>\` tag.

Available Tools:
- search: Search code using keyword queries.
- query: Search code using structural AST patterns.
- extract: Extract specific code blocks or lines from files.
- listFiles: List files and directories in a specified location.
- searchFiles: Find files matching a glob pattern with recursive search capability.
${this.allowEdit ? "- implement: Implement a feature or fix a bug using aider.\n" : ""}
- attempt_completion: Finalize the task and provide the result to the user.
`;
        const commonInstructions = `<instructions>
Follow these instructions carefully:
1. Analyze the user's request.
2. Use <thinking></thinking> tags to analyze the situation and determine the appropriate tool for each step.
3. Use the available tools step-by-step to fulfill the request.
4. You should always prefer the \`search\` tool for code-related questions. Read full files only if really necessary.
5. Ensure to get really deep and understand the full picture before answering.
6. You MUST respond with exactly ONE tool call per message, using the specified XML format, until the task is complete.
7. Wait for the tool execution result (provided in the next user message in a <tool_result> block) before proceeding to the next step.
8. Once the task is fully completed, use the '<attempt_completion>' tool to provide the final result. This is the ONLY way to signal completion.
9. Prefer concise and focused search queries. Use specific keywords and phrases to narrow down results.
</instructions>
`;
        const predefinedPrompts = {
          "code-explorer": `You are ProbeChat Code Explorer, a specialized AI assistant focused on helping developers, product managers, and QAs understand and navigate codebases. Your primary function is to answer questions based on code, explain how systems work, and provide insights into code functionality using the provided code analysis tools.

When exploring code:
- Provide clear, concise explanations based on user request
- Find and highlight the most relevant code snippets, if required
- Trace function calls and data flow through the system
- Try to understand the user's intent and provide relevant information
- Understand high level picture
- Balance detail with clarity in your explanations`,
          "architect": `You are ProbeChat Architect, a specialized AI assistant focused on software architecture and design. Your primary function is to help users understand, analyze, and design software systems using the provided code analysis tools.

When analyzing code:
- Focus on high-level design patterns and system organization
- Identify architectural patterns and component relationships
- Evaluate system structure and suggest architectural improvements
- Consider scalability, maintainability, and extensibility in your analysis`,
          "code-review": `You are ProbeChat Code Reviewer, a specialized AI assistant focused on code quality and best practices. Your primary function is to help users identify issues, suggest improvements, and ensure code follows best practices using the provided code analysis tools.

When reviewing code:
- Look for bugs, edge cases, and potential issues
- Identify performance bottlenecks and optimization opportunities
- Check for security vulnerabilities and best practices
- Evaluate code style and consistency
- Provide specific, actionable suggestions with code examples where appropriate`,
          "code-review-template": `You are going to perform code review according to provided user rules. Ensure to review only code provided in diff and latest commit, if provided. However you still need to fully understand how modified code works, and read dependencies if something is not clear.`,
          "engineer": `You are senior engineer focused on software architecture and design.
Before jumping on the task you first, in details analyse user request, and try to provide elegant and concise solution.
If solution is clear, you can jump to implementation right away, if not, you can ask user a clarification question, by calling attempt_completion tool, with required details.

Before jumping to implementation:
- Focus on high-level design patterns and system organization
- Identify architectural patterns and component relationships
- Evaluate system structure and suggest architectural improvements
- Focus on backward compatibility.
- Consider scalability, maintainability, and extensibility in your analysis

During the implementation:
- Avoid implementing special cases
- Do not forget to add the tests`,
          "support": `You are ProbeChat Support, a specialized AI assistant focused on helping developers troubleshoot issues and solve problems. Your primary function is to help users diagnose errors, understand unexpected behaviors, and find solutions using the provided code analysis tools.

When troubleshooting:
- Focus on finding root causes, not just symptoms
- Explain concepts clearly with appropriate context
- Provide step-by-step guidance to solve problems
- Suggest diagnostic steps to verify solutions
- Consider edge cases and potential complications
- Be empathetic and patient in your explanations`
        };
        let systemMessage = "";
        if (this.customPrompt) {
          systemMessage = "<role>" + this.customPrompt + "</role>";
          if (this.debug) {
            console.log(`[DEBUG] Using custom prompt`);
          }
        } else if (this.promptType && predefinedPrompts[this.promptType]) {
          systemMessage = "<role>" + predefinedPrompts[this.promptType] + "</role>";
          if (this.debug) {
            console.log(`[DEBUG] Using predefined prompt: ${this.promptType}`);
          }
          systemMessage += commonInstructions;
        } else {
          systemMessage = "<role>" + predefinedPrompts["code-explorer"] + "</role>";
          if (this.debug) {
            console.log(`[DEBUG] Using default prompt: code explorer`);
          }
          systemMessage += commonInstructions;
        }
        systemMessage += `
${xmlToolGuidelines}
`;
        systemMessage += `
# Tools Available
${toolDefinitions}
`;
        const searchDirectory = this.allowedFolders.length > 0 ? this.allowedFolders[0] : process.cwd();
        if (this.debug) {
          console.log(`[DEBUG] Generating file list for base directory: ${searchDirectory}...`);
        }
        try {
          const files = await listFilesByLevel({
            directory: searchDirectory,
            maxFiles: 100,
            respectGitignore: !process.env.PROBE_NO_GITIGNORE || process.env.PROBE_NO_GITIGNORE === "",
            cwd: process.cwd()
          });
          systemMessage += `
# Repository Structure

You are working with a repository located at: ${searchDirectory}

Here's an overview of the repository structure (showing up to 100 most relevant files):

\`\`\`
${files}
\`\`\`

`;
        } catch (error) {
          if (this.debug) {
            console.log(`[DEBUG] Could not generate file list: ${error.message}`);
          }
          systemMessage += `
# Repository Structure

You are working with a repository located at: ${searchDirectory}

`;
        }
        if (this.allowedFolders.length > 0) {
          systemMessage += `
**Important**: For security reasons, you can only search within these allowed folders: ${this.allowedFolders.join(", ")}

`;
        }
        return systemMessage;
      }
      /**
       * Answer a question using the agentic flow
       * @param {string} message - The user's question
       * @param {Array} [images] - Optional array of image data (base64 strings or URLs)
       * @param {Object|string} [schemaOrOptions] - Can be either:
       *   - A string: JSON schema for structured output (backwards compatible)
       *   - An object: Options object with schema and other options
       * @param {string} [schemaOrOptions.schema] - JSON schema string for structured output
       * @returns {Promise<string>} - The final answer
       */
      async answer(message, images = [], schemaOrOptions = {}) {
        if (!message || typeof message !== "string" || message.trim().length === 0) {
          throw new Error("Message is required and must be a non-empty string");
        }
        let options = {};
        if (typeof schemaOrOptions === "string") {
          options = { schema: schemaOrOptions };
        } else {
          options = schemaOrOptions || {};
        }
        try {
          const systemMessage = await this.getSystemMessage();
          let userMessage = { role: "user", content: message.trim() };
          if (images && images.length > 0) {
            userMessage.content = [
              { type: "text", text: message.trim() },
              ...images.map((image) => ({
                type: "image",
                image
              }))
            ];
          }
          let currentMessages = [
            { role: "system", content: systemMessage },
            ...this.history,
            // Include previous conversation history
            userMessage
          ];
          let currentIteration = 0;
          let completionAttempted = false;
          let finalResult = "I was unable to complete your request due to reaching the maximum number of tool iterations.";
          const maxIterations = options.schema ? MAX_TOOL_ITERATIONS + 4 : MAX_TOOL_ITERATIONS;
          if (this.debug) {
            console.log(`[DEBUG] Starting agentic flow for question: ${message.substring(0, 100)}...`);
            if (options.schema) {
              console.log(`[DEBUG] Schema provided, using extended iteration limit: ${maxIterations} (base: ${MAX_TOOL_ITERATIONS})`);
            }
          }
          while (currentIteration < maxIterations && !completionAttempted) {
            currentIteration++;
            if (this.cancelled) throw new Error("Request was cancelled by the user");
            if (this.debug) {
              console.log(`
[DEBUG] --- Tool Loop Iteration ${currentIteration}/${maxIterations} ---`);
              console.log(`[DEBUG] Current messages count for AI call: ${currentMessages.length}`);
            }
            if (this.tracer) {
              this.tracer.addEvent("iteration.start", {
                "iteration": currentIteration,
                "max_iterations": maxIterations,
                "message_count": currentMessages.length
              });
            }
            if (currentIteration === maxIterations) {
              const warningMessage = `\u26A0\uFE0F WARNING: You have reached the maximum tool iterations limit (${maxIterations}). This is your final message. Please respond with the data you have so far. If something was not completed, honestly state what was not done and provide any partial results or recommendations you can offer.`;
              currentMessages.push({
                role: "user",
                content: warningMessage
              });
              if (this.debug) {
                console.log(`[DEBUG] Added max iterations warning message at iteration ${currentIteration}`);
              }
            }
            this.tokenCounter.calculateContextSize(currentMessages);
            if (this.debug) {
              console.log(`[DEBUG] Estimated context tokens BEFORE LLM call (Iter ${currentIteration}): ${this.tokenCounter.contextSize}`);
            }
            let maxResponseTokens = 4e3;
            if (this.model.includes("claude-3-opus") || this.model.startsWith("gpt-4-")) {
              maxResponseTokens = 4096;
            } else if (this.model.includes("claude-3-5-sonnet") || this.model.startsWith("gpt-4o")) {
              maxResponseTokens = 8192;
            }
            let assistantResponseContent = "";
            try {
              const executeAIRequest = async () => {
                const result = await (0, import_ai2.streamText)({
                  model: this.provider(this.model),
                  messages: currentMessages,
                  maxTokens: maxResponseTokens,
                  temperature: 0.3
                });
                for await (const delta of result.textStream) {
                  assistantResponseContent += delta;
                }
                const usage = await result.usage;
                if (usage) {
                  this.tokenCounter.recordUsage(usage, result.experimental_providerMetadata);
                }
                return result;
              };
              if (this.tracer) {
                await this.tracer.withSpan("ai.request", executeAIRequest, {
                  "ai.model": this.model,
                  "ai.provider": this.clientApiProvider || "auto",
                  "iteration": currentIteration,
                  "max_tokens": maxResponseTokens,
                  "temperature": 0.3,
                  "message_count": currentMessages.length
                });
              } else {
                await executeAIRequest();
              }
            } catch (error) {
              console.error(`Error during streamText (Iter ${currentIteration}):`, error);
              finalResult = `Error: Failed to get response from AI model during iteration ${currentIteration}. ${error.message}`;
              throw new Error(finalResult);
            }
            const parsedTool = parseXmlToolCallWithThinking(assistantResponseContent);
            if (parsedTool) {
              const { toolName, params } = parsedTool;
              if (this.debug) console.log(`[DEBUG] Parsed tool call: ${toolName} with params:`, params);
              if (toolName === "attempt_completion") {
                completionAttempted = true;
                const validation = attemptCompletionSchema.safeParse(params);
                if (validation.success) {
                  finalResult = validation.data.result;
                  if (this.debug) console.log(`[DEBUG] Task completed successfully with result: ${finalResult.substring(0, 100)}...`);
                } else {
                  console.error(`[ERROR] Invalid attempt_completion parameters:`, validation.error);
                  finalResult = "Error: Invalid completion attempt. The task could not be completed properly.";
                }
                break;
              } else {
                if (this.toolImplementations[toolName]) {
                  try {
                    const toolParams = { ...params, sessionId: this.sessionId };
                    this.events.emit("toolCall", {
                      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
                      name: toolName,
                      args: toolParams,
                      status: "started"
                    });
                    const executeToolCall = async () => {
                      return await this.toolImplementations[toolName].execute(toolParams);
                    };
                    let toolResult;
                    try {
                      if (this.tracer) {
                        toolResult = await this.tracer.withSpan("tool.call", executeToolCall, {
                          "tool.name": toolName,
                          "tool.params": JSON.stringify(toolParams).substring(0, 500),
                          "iteration": currentIteration
                        });
                      } else {
                        toolResult = await executeToolCall();
                      }
                      this.events.emit("toolCall", {
                        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
                        name: toolName,
                        args: toolParams,
                        resultPreview: typeof toolResult === "string" ? toolResult.length > 200 ? toolResult.substring(0, 200) + "..." : toolResult : toolResult ? JSON.stringify(toolResult).substring(0, 200) + "..." : "No Result",
                        status: "completed"
                      });
                    } catch (toolError) {
                      this.events.emit("toolCall", {
                        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
                        name: toolName,
                        args: toolParams,
                        error: toolError.message || "Unknown error",
                        status: "error"
                      });
                      throw toolError;
                    }
                    currentMessages.push({ role: "assistant", content: assistantResponseContent });
                    currentMessages.push({
                      role: "user",
                      content: `<tool_result>
${typeof toolResult === "string" ? toolResult : JSON.stringify(toolResult, null, 2)}
</tool_result>`
                    });
                    if (this.debug) {
                      console.log(`[DEBUG] Tool ${toolName} executed successfully. Result length: ${typeof toolResult === "string" ? toolResult.length : JSON.stringify(toolResult).length}`);
                    }
                  } catch (error) {
                    console.error(`[ERROR] Tool execution failed for ${toolName}:`, error);
                    currentMessages.push({ role: "assistant", content: assistantResponseContent });
                    currentMessages.push({
                      role: "user",
                      content: `<tool_result>
Error: ${error.message}
</tool_result>`
                    });
                  }
                } else {
                  console.error(`[ERROR] Unknown tool: ${toolName}`);
                  currentMessages.push({ role: "assistant", content: assistantResponseContent });
                  currentMessages.push({
                    role: "user",
                    content: `<tool_result>
Error: Unknown tool '${toolName}'. Available tools: ${Object.keys(this.toolImplementations).join(", ")}
</tool_result>`
                  });
                }
              }
            } else {
              currentMessages.push({ role: "assistant", content: assistantResponseContent });
              currentMessages.push({
                role: "user",
                content: "Please use one of the available tools to help answer the question, or use attempt_completion if you have enough information to provide a final answer."
              });
              if (this.debug) {
                console.log(`[DEBUG] No tool call detected in assistant response. Prompting for tool use.`);
              }
            }
            if (currentMessages.length > MAX_HISTORY_MESSAGES) {
              const messagesBefore = currentMessages.length;
              const systemMsg = currentMessages[0];
              const recentMessages = currentMessages.slice(-MAX_HISTORY_MESSAGES + 1);
              currentMessages = [systemMsg, ...recentMessages];
              if (this.debug) {
                console.log(`[DEBUG] Trimmed message history from ${messagesBefore} to ${currentMessages.length} messages`);
              }
            }
          }
          if (currentIteration >= maxIterations && !completionAttempted) {
            console.warn(`[WARN] Max tool iterations (${maxIterations}) reached for session ${this.sessionId}. Returning current error state.`);
          }
          this.history = currentMessages.map((msg) => ({ ...msg }));
          if (this.history.length > MAX_HISTORY_MESSAGES) {
            const messagesBefore = this.history.length;
            this.history = this.history.slice(-MAX_HISTORY_MESSAGES);
            if (this.debug) {
              console.log(`[DEBUG] Trimmed stored history from ${messagesBefore} to ${this.history.length} messages`);
            }
          }
          this.tokenCounter.updateHistory(this.history);
          if (options.schema && !options._schemaFormatted) {
            if (this.debug) {
              console.log("[DEBUG] Schema provided, applying automatic formatting...");
            }
            try {
              const schemaPrompt = `Now you need to respond according to this schema:

${options.schema}

Please reformat your previous response to match this schema exactly. Only return the formatted response, no additional text.`;
              finalResult = await this.answer(schemaPrompt, [], {
                ...options,
                _schemaFormatted: true
              });
              finalResult = cleanSchemaResponse(finalResult);
              try {
                const mermaidValidation = await validateAndFixMermaidResponse(finalResult, {
                  debug: this.debug,
                  path: this.allowedFolders[0],
                  provider: this.clientApiProvider,
                  model: this.model
                });
                if (mermaidValidation.wasFixed) {
                  finalResult = mermaidValidation.fixedResponse;
                  if (this.debug) {
                    console.log(`[DEBUG] Mermaid diagrams fixed`);
                    if (mermaidValidation.fixingResults) {
                      mermaidValidation.fixingResults.forEach((fixResult, index) => {
                        if (fixResult.wasFixed) {
                          console.log(`[DEBUG] Fixed diagram ${index + 1}: ${fixResult.originalError}`);
                        }
                      });
                    }
                  }
                }
              } catch (error) {
                if (this.debug) {
                  console.log(`[DEBUG] Mermaid validation failed: ${error.message}`);
                }
              }
              if (isJsonSchema(options.schema)) {
                const validation = validateJsonResponse(finalResult);
                if (!validation.isValid) {
                  if (this.debug) {
                    console.log("[DEBUG] JSON validation failed:", validation.error);
                  }
                  const correctionPrompt = createJsonCorrectionPrompt(
                    finalResult,
                    options.schema,
                    validation.error
                  );
                  finalResult = await this.answer(correctionPrompt, [], {
                    ...options,
                    _schemaFormatted: true
                  });
                  finalResult = cleanSchemaResponse(finalResult);
                  const finalValidation = validateJsonResponse(finalResult);
                  if (!finalValidation.isValid && this.debug) {
                    console.log("[DEBUG] JSON still invalid after correction:", finalValidation.error);
                  }
                }
              }
            } catch (error) {
              console.error("[ERROR] Schema formatting failed:", error);
            }
          }
          return finalResult;
        } catch (error) {
          console.error(`[ERROR] ProbeAgent.answer failed:`, error);
          clearToolExecutionData(this.sessionId);
          throw error;
        }
      }
      /**
       * Get token usage information
       * @returns {Object} Token usage data
       */
      getTokenUsage() {
        return this.tokenCounter.getTokenUsage();
      }
      /**
       * Clear conversation history and reset counters
       */
      clearHistory() {
        this.history = [];
        this.tokenCounter.clear();
        clearToolExecutionData(this.sessionId);
        if (this.debug) {
          console.log(`[DEBUG] Cleared conversation history and reset counters for session ${this.sessionId}`);
        }
      }
      /**
       * Cancel the current request
       */
      cancel() {
        this.cancelled = true;
        if (this.debug) {
          console.log(`[DEBUG] Agent cancelled for session ${this.sessionId}`);
        }
      }
    };
  }
});
init_ProbeAgent();
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  ProbeAgent
});

version: "1.0"

# Simple agent: task-refinement
# - Collects user input, refines it with AI (skip code context), and loops until refined.
# - Returns final refined text via the `finish` step output.

steps:
  ask:
    type: human-input
    group: task-refinement
    if: "!(outputs && outputs['ask'])"
    # No explicit event trigger; run in CLI by default but guard via if
    prompt: |
      {% assign last_refine = outputs_history.refine | last %}
      {% if last_refine and last_refine.refined == false %}
      {{ last_refine.text }}
      {% else %}
      Provide the task you want to accomplish. Be specific about constraints
      (inputs, outputs, environment, success criteria).
      {% endif %}
    multiline: false
    allow_empty: false
    # No on_success.goto required — refine depends_on ask

  refine:
    type: ai
    group: task-refinement
    # Run only after 'ask' — dependency drives ordering
    depends_on: [ask]
    # Execute once per run; forward-run loops do not need to re-run refine
    if: "!(outputs && outputs['refine'])"
    ai:
      skip_code_context: true
      disableTools: true
      system_prompt: |
        You are a helpful, precise task refinement assistant (role: requirements-analyst).
        Your goal is to get to an agreed, testable task definition and clear acceptance criteria.
        - Refine the user's task into an unambiguous, executable description with minimal assumptions.
        - Define how correctness will be validated (objective success criteria and measurables).
        - If information is missing, set refined=false and ask_user=true and put a single, specific
          clarification question in the "text" field (one question at a time).
        - If everything is sufficient, set refined=true and put the final refined wording in "text".
        - Be succinct and concrete. Prefer measurable outcomes over vague phrasing.
        - You can't mark plan as refined until explicit user confirmation/agreement is implied.
    # Schema ensures the agent either finalizes or asks to clarify
    schema:
      type: object
      additionalProperties: false
      properties:
        refined: { type: boolean, description: "true if the task is fully specified and accepted" }
        text: { type: string, description: "final refined task or question to user" }
      required: [refined, text]
    prompt: |
      <history>
        {% assign umsgs = outputs_history.ask | default: [] %}
        {% assign amsgs = outputs_history.refine | default: [] %}
        {% assign merged = umsgs | concat: amsgs | sort: 'ts' %}
        {% for m in merged %}
          {% if m.refined != nil %}
            <assistant>{{ m.text }}</assistant>
          {% else %}
            <user>{{ m.text }}</user>
          {% endif %}
        {% endfor %}
      </history>

      <input>
        {{ outputs['ask'].text }}
      </input>

    # Loop control using fail_if + on_fail (no goto_js)
    fail_if: "output['refined'] !== true"
    on_fail:
      goto: ask

  plan-commands:
    type: ai
    group: task-refinement
    depends_on: [refine]
    reuse_ai_session: refine
    session_mode: append
    ai:
      # Allow tools so the model can inspect the repo context if needed
      disableTools: false
      skip_code_context: true
      system_prompt: |
        You are a Task Validation Planner.
        Produce a deterministic, minimal list of shell commands that, when executed in order,
        validate that the refined task is complete for THIS repository/local env.
        Constraints:
        - Commands must be safe and non-destructive. Do not delete files or push to remotes.
        - Prefer read-only checks and standard project commands (build, lint, test) when present.
        - Each item is a single shell line; you may use && or || inside a line if necessary.
        - Favor idempotent commands. Avoid interactive prompts.
        - Detect package manager/tooling pragmatically (npm/pnpm/yarn/bun, eslint/biome, jest/vitest, etc.).
        Return JSON with an array of strings under "commands" and optional "notes".
    schema:
      type: object
      additionalProperties: false
      properties:
        commands:
          type: array
          minItems: 1
          items: { type: string, minLength: 1 }
        notes: { type: string }
      required: [commands]
    prompt: |
      Refined task:
      {{ outputs['refine'].text }}

      If the repository has build/lint/test, include them. Otherwise propose basic checks that still
      demonstrate completion (e.g., typecheck, compile, format verification, smoke run).

      {% assign prev_conf_hist = outputs_history['confirm-interpret'] | default: [] %}
      {% assign prev_run_hist = outputs_history['run-commands'] | default: [] %}
      {% assign last_failed = nil %}
      {% for r in prev_run_hist %}
        {% if r.failed and r.failed > 0 %}
          {% assign last_failed = r %}
        {% endif %}
      {% endfor %}
      {% assign last_conf = prev_conf_hist | last %}
      {% assign prev_run_count = prev_run_hist | size %}
      {% assign last_run = prev_run_hist | last %}
      {% if last_failed or prev_run_count > 0 %}
      Previous attempt failed. Here are the details to learn from:
      - Previous commands: {{ last_conf.commands | to_json }}
      - Run results: {{ last_failed | default: last_run | to_json }}
      Please revise the commands to address failures. Keep the list minimal and deterministic.
      {% endif %}

      Output strictly JSON per schema. No prose around it.
    # No on_success routing needed; dependents naturally follow in the DAG

  ask-confirm:
    type: human-input
    group: task-refinement
    depends_on: [plan-commands]
    prompt: |
      Here is the proposed validation command list (to run sequentially):
      {% for c in outputs['plan-commands'].commands %}
      {{ forloop.index }}. {{ c }}
      {% endfor %}

      Confirm running these? Reply "yes" to accept, or provide edits:
      - JSON array of commands, e.g. ["npm ci", "npm test"], or
      - Plain text with one command per line.
    placeholder: "yes | or paste modified list..."
    multiline: true
    allow_empty: true
    default: "yes"

  confirm-interpret:
    type: ai
    group: task-refinement
    depends_on: [plan-commands, ask-confirm]
    ai:
      skip_code_context: true
      disableTools: true
      system_prompt: |
        You are a confirmation interpreter. Given the planned command list and the user's reply,
        decide whether to proceed to execution with a normalized list of shell commands, or
        return for replanning.
        - If the user says yes/approve, set proceed=true and provide commands as-is.
        - If the user supplied edits (JSON array or one per line), parse, trim, dedupe, and set proceed=true with commands.
        - If the reply indicates high-level changes (not runnable commands), set proceed=false and include a short reason.
        Output strictly JSON per schema.
    schema:
      type: object
      additionalProperties: false
      properties:
        proceed: { type: boolean }
        commands:
          type: array
          items: { type: string, minLength: 1 }
        reason: { type: string }
      required: [proceed]
    prompt: |
      Planned commands:
      {{ outputs['plan-commands'].commands | to_json }}

      User reply:
      {{ outputs['ask-confirm'].text }}

      Return JSON per schema. If proceed=true, commands must be a non-empty array of shell lines.
    fail_if: "output && output.proceed !== true"
    on_fail:
      goto: plan-commands

  run-commands:
    type: command
    criticality: internal
    group: task-refinement
    depends_on: [confirm-interpret]
    assume:
      - "outputs['confirm-interpret']?.proceed === true"
      - "(outputs['confirm-interpret']?.commands?.length ?? 0) > 0"
    exec: |
      node <<'NODE'
      const { spawn } = require('child_process');
      const cmds = {{ outputs['confirm-interpret'] | to_json }}.commands || [];
      const results = [];
      const runOne = (cmd) => new Promise((resolve) => {
        const child = spawn('bash', ['-lc', cmd], { stdio: ['ignore', 'pipe', 'pipe'] });
        let out = '', err = '';
        const started = Date.now();
        child.stdout.on('data', d => (out += d.toString()));
        child.stderr.on('data', d => (err += d.toString()));
        child.on('close', code => {
          results.push({ cmd, code, stdout: out, stderr: err, durationMs: Date.now() - started });
          resolve();
        });
      });
      (async () => {
        for (const c of cmds) { await runOne(c); }
        const failed = results.filter(r => Number(r.code||0) !== 0).length;
        process.stdout.write(JSON.stringify({ failed, results }));
      })().catch(e => { process.stdout.write(JSON.stringify({ failed: 1, error: String(e) })); process.exit(0); });
      NODE
    output_format: json
    schema:
      type: object
      additionalProperties: true
      properties:
        failed: { type: number }
        results:
          type: array
          items:
            type: object
            additionalProperties: true
            properties:
              cmd: { type: string }
              code: {}
              stdout: { type: string }
              stderr: { type: string }
              durationMs: { type: number }
            required: [cmd, code]
      required: [failed]
    fail_if: "output && Number(output.failed||0) > 0"
    on_fail:
      goto: plan-commands

  finish:
    type: log
    group: task-refinement
    depends_on: [run-commands]
    if: "(outputs && outputs['run-commands'] && Number((outputs['run-commands'].failed||0)) === 0) && !(outputs && outputs['finish'])"
    message: |
      ✅ Refined Task:
      {{ outputs['refine'].text }}

      ✅ Validation commands (final):
      {% for c in outputs['confirm-interpret'].commands %}
      - {{ c }}
      {% endfor %}
    level: info
    include_pr_context: false
    include_dependencies: false
    include_metadata: false

tests:
  defaults:
    strict: true
    ai_provider: mock
  cases:
    - name: one-pass-refinement
      description: Single turn; AI is happy and returns refined text.
      event: manual
      fixture: local.minimal
      mocks:
        ask: "Build a small Node CLI that prints \"hello\""
        refine:
          refined: true
          text: "Create a Node.js CLI (using Node >=18) that prints 'hello' when run; include usage example and exit code 0."
        plan-commands:
          commands: ["echo hello-build", "echo hello-lint", "echo hello-test"]
        ask-confirm: "yes"
        confirm-interpret:
          proceed: true
          commands: ["echo hello-build","echo hello-lint","echo hello-test"]
        run-commands:
          stdout: '{"failed":0,"results":[{"cmd":"echo hello-build","code":0},{"cmd":"echo hello-lint","code":0},{"cmd":"echo hello-test","code":0}]}'
      expect:
        calls:
          - step: plan-commands
            exactly: 1
          - step: ask-confirm
            exactly: 1
          - step: confirm-interpret
            exactly: 1
          - step: run-commands
            exactly: 1
          - step: ask
            exactly: 1
          - step: refine
            exactly: 1
          - step: finish
            exactly: 1
        outputs:
          - step: run-commands
            path: failed
            equals: 0

    - name: multi-turn-refinement-loop
      description: Two clarifying turns followed by a final refinement; manual-only chat.
      event: manual
      fixture: local.minimal
      mocks:
        ask[]:
          - "Create a CI job"
          - "GitHub Actions; run on push to main"
          - "Use Node 18 and npm ci + npm test"
        refine[]:
          - { refined: false, ask_user: true, text: "Which CI platform and trigger conditions?" }
          - { refined: false, ask_user: true, text: "What Node version and commands should run?" }
          - { refined: true, text: "Set up GitHub Actions workflow: on push to main, use Node 18.x, cache npm, run npm ci && npm test." }
        plan-commands:
          commands: ["echo build", "echo lint", "echo test"]
        ask-confirm: "yes"
        confirm-interpret:
          proceed: true
          commands: ["echo build","echo lint","echo test"]
        run-commands:
          stdout: '{"failed":0,"results":[{"cmd":"echo build","code":0},{"cmd":"echo lint","code":0},{"cmd":"echo test","code":0}]}'
      expect:
        calls:
          - step: ask
            exactly: 3
          - step: refine
            exactly: 3
          - step: plan-commands
            exactly: 1
          - step: ask-confirm
            exactly: 1
          - step: confirm-interpret
            exactly: 1
          - step: run-commands
            exactly: 1
          - step: finish
            exactly: 1
        prompts:
          # Ask prompt should surface the last refine clarification
          - step: ask
            index: 1
            contains:
              - "Which CI platform and trigger conditions?"
          - step: ask
            index: last
            contains:
              - "What Node version and commands should run?"
          - step: refine
            index: last
            contains:
              - "Which CI platform and trigger conditions?"
              - "Use Node 18 and npm ci + npm test"
              - "What Node version and commands should run?"
          # Keep prompt assertions resilient to minor formatting changes by using 'contains'
          # instead of a single large regex.
        outputs:
          # Ensure the final successful refinement carries a timestamp and expected text
          - step: refine
            where: { path: refined, equals: true }
            path: ts
            matches: "^\\d{10,}$"
          - step: refine
            where: { path: refined, equals: true }
            path: text
            matches: "(?is).*GitHub Actions.*Node 18.*npm ci.*npm test.*"
          - step: run-commands
            path: failed
            equals: 0

    - name: plan-and-run-success
      description: Plans commands, user confirms, runs successfully, finishes.
      event: manual
      fixture: local.minimal
      mocks:
        ask: "Add CI to run build, lint, and tests"
        refine:
          refined: true
          text: "Ensure project builds, lints, and tests pass via CI"
        plan-commands:
          commands: ["echo build", "echo lint", "echo test"]
        ask-confirm: "yes"
        confirm-interpret:
          proceed: true
          commands: ["echo build","echo lint","echo test"]
        run-commands:
          stdout: '{"failed":0,"results":[{"cmd":"echo build","code":0},{"cmd":"echo lint","code":0},{"cmd":"echo test","code":0}]}'
      expect:
        calls:
          - step: ask
            exactly: 1
          - step: refine
            exactly: 1
          - step: plan-commands
            exactly: 1
          - step: ask-confirm
            exactly: 1
          - step: confirm-interpret
            exactly: 1
          - step: run-commands
            exactly: 1
          - step: finish
            exactly: 1
        outputs:
          - step: run-commands
            path: failed
            equals: 0

    - name: plan-run-fail-then-refine
      description: First run fails, planner adjusts, second run passes, finish.
      event: manual
      fixture: local.minimal
      mocks:
        ask: "Verify app passes tests"
        refine:
          refined: true
          text: "Run tests to verify app correctness"
        plan-commands[]:
          - { commands: ["node -e \"process.exit(1)\""] }
          - { commands: ["node -e \"process.exit(0)\""] }
        ask-confirm[]:
          - "yes"
          - "yes"
        confirm-interpret:
          proceed: true
          commands: ["node -e \"process.exit(1)\""]
        run-commands[]:
          - { stdout: '{"failed":1,"results":[{"cmd":"node -e \\"process.exit(1)\\"","code":1,"stderr":"fail"}]}' , exit_code: 1 }
          - { stdout: '{"failed":0,"results":[{"cmd":"node -e \\"process.exit(0)\\"","code":0}]}' }
      expect:
        calls:
          - step: ask
            exactly: 2
          - step: refine
            exactly: 2
          - step: plan-commands
            exactly: 2
          - step: ask-confirm
            exactly: 2
          - step: confirm-interpret
            exactly: 2
          - step: run-commands
            exactly: 2
          - step: finish
            exactly: 1
        prompts:
          - step: plan-commands
            index: last
            contains:
              - "Previous attempt failed"
              - "Run results"

    - name: confirm-amend-json
      description: User pastes JSON array of edited commands; interpreter parses and proceeds.
      event: manual
      fixture: local.minimal
      mocks:
        ask: "Make sure project installs deps and runs tests"
        refine:
          refined: true
          text: "Install dependencies and run tests"
        plan-commands:
          commands: ["npm test"]
        ask-confirm: '["npm ci","npm test"]'
        confirm-interpret:
          proceed: true
          commands: ["npm ci","npm test"]
        run-commands:
          stdout: '{"failed":0,"results":[{"cmd":"npm ci","code":0},{"cmd":"npm test","code":0}]}'
      expect:
        calls:
          - step: ask
            exactly: 1
          - step: refine
            exactly: 1
          - step: plan-commands
            exactly: 1
          - step: ask-confirm
            exactly: 1
          - step: confirm-interpret
            exactly: 1
          - step: run-commands
            exactly: 1
          - step: finish
            exactly: 1
        outputs:
          - step: confirm-interpret
            path: proceed
            equals: true
          - step: confirm-interpret
            path: commands.length
            equals: 2

    - name: confirm-amend-lines
      description: User pastes multi-line commands with duplicates; interpreter dedupes and proceeds.
      event: manual
      fixture: local.minimal
      mocks:
        ask: "Run lint and tests"
        refine:
          refined: true
          text: "Run lint and tests"
        plan-commands:
          commands: ["npm run lint","npm test"]
        ask-confirm: |
          npm test\nnpm test\n  npm run lint  
        confirm-interpret:
          proceed: true
          commands: ["npm test","npm run lint"]
        run-commands:
          stdout: '{"failed":0,"results":[{"cmd":"npm test","code":0},{"cmd":"npm run lint","code":0}]}'
      expect:
        calls:
          - step: ask
            exactly: 1
          - step: refine
            exactly: 1
          - step: plan-commands
            exactly: 1
          - step: ask-confirm
            exactly: 1
          - step: confirm-interpret
            exactly: 1
          - step: run-commands
            exactly: 1
          - step: finish
            exactly: 1
        outputs:
          - step: confirm-interpret
            path: proceed
            equals: true
          - step: confirm-interpret
            path: commands.length
            equals: 2

    - name: user-declines-replan-then-accept
      description: User declines; interpreter routes to replan; user then accepts; run succeeds.
      event: manual
      fixture: local.minimal
      mocks:
        ask: "Set up lint and test checks"
        refine:
          refined: true
          text: "Ensure lint and tests run"
        plan-commands[]:
          - { commands: ["npm test"] }
          - { commands: ["npm run lint","npm test"] }
        ask-confirm[]:
          - "No, add lint too"
          - "yes"
        confirm-interpret[]:
          - { proceed: false, reason: "needs lint" }
          - { proceed: true, commands: ["npm run lint","npm test"] }
        run-commands:
          stdout: '{"failed":0,"results":[{"cmd":"npm run lint","code":0},{"cmd":"npm test","code":0}]}'
      expect:
        calls:
          - step: ask
            at_least: 1
          - step: refine
            at_least: 1
          - step: plan-commands
            exactly: 2
          - step: ask-confirm
            exactly: 2
          - step: confirm-interpret
            exactly: 2
          - step: run-commands
            exactly: 1
          - step: finish
            exactly: 1
        outputs:
          - step: confirm-interpret
            index: last
            path: proceed
            equals: true

    - name: run-fail-then-success-loop
      description: First run fails; planner sees history; second run succeeds; prompt shows failure context.
      event: manual
      strict: false
      fixture: local.minimal
      mocks:
        ask: "Verify app tests pass"
        refine:
          refined: true
          text: "Run tests to verify app correctness"
        plan-commands[]:
          - { commands: ["node -e \"process.exit(1)\""] }
          - { commands: ["echo ok"] }
        ask-confirm[]:
          - "yes"
          - "yes"
        confirm-interpret[]:
          - { proceed: true, commands: ["node -e \"process.exit(1)\""] }
          - { proceed: true, commands: ["echo ok"] }
        run-commands[]:
          - { stdout: '{"failed":1,"results":[{"cmd":"node -e \\"process.exit(1)\\"","code":1,"stderr":"fail"}]}' , exit_code: 1 }
          - { stdout: '{"failed":0,"results":[{"cmd":"echo ok","code":0}]}' }
      expect:
        calls:
          - step: ask
            at_least: 1
          - step: refine
            at_least: 1
          - step: plan-commands
            exactly: 2
          - step: ask-confirm
            exactly: 2
          - step: confirm-interpret
            exactly: 2
          - step: run-commands
            exactly: 2
        prompts:
          - step: plan-commands
            index: last
            contains:
              - "Previous attempt failed"
              - "Run results"

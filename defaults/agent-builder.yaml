version: "1.0"

# Agent Build (cwd-first): generate or improve a Visor agent config purely via YAML.
# - Writes to current working directory by default (VISOR_AGENT_OUT_DIR, defaults ".").
# - To improve an existing config in-place, run the CLI with: visor build <path/to/agent.yaml>
#   The YAML detects mode by reading the file; missing/empty => new, else improve.

steps:
  brief:
    type: human-input
    group: agent-build
    on: [manual]
    # no dependencies; can run standalone in manual prompt-only mode
    prompt: |
      Provide a concise brief for the agent to build.
      Include: purpose, events, essential steps, and minimal success criteria.
    placeholder: "e.g., Agent that labels PRs by change size and runs jest"
    multiline: false
    allow_empty: true
    default: "Create a minimal agent that can scaffold other agents (config + tests)."
    on_success:
      goto: draft
      goto_event: schedule

  start:
    type: log
    group: agent-build
    on: [issue_opened]
    message: "start"
    level: debug
    # no goto; detect-mode runs on event filtering

  detect-mode:
    type: command
    group: agent-build
    on: [manual, issue_opened, schedule]
    if: "Boolean(env.VISOR_AGENT_PATH || env.VISOR_AGENT_OUT_DIR || env.VISOR_AGENT_MODE == 'improve')"
    exec: echo "{}"
    transform: |
      {% assign mode = 'new' %}
      {% assign exists = false %}
      {% assign empty = true %}
      {% if env.VISOR_AGENT_MODE == 'improve' %}
        {% assign mode = 'improve' %}
      {% else %}
        {% capture cfg %}{% readfile env.VISOR_AGENT_PATH %}{% endcapture %}
        {% assign has_error = cfg contains 'Error reading file' or cfg contains 'Invalid file path' %}
        {% assign trimmed = cfg | strip %}
        {% assign empty = trimmed == '' %}
        {% assign exists = has_error == false %}
        {% if exists and empty == false %}{% assign mode = 'improve' %}{% endif %}
      {% endif %}
      {"path":"{{ env.VISOR_AGENT_PATH }}","exists": {{ exists }}, "empty": {{ empty }}, "mode": "{{ mode }}"}
    # no on_success routing; downstream steps run based on event + if guards

  load-existing:
    type: command
    group: agent-build
    depends_on: [detect-mode]
    on: [issue_opened, schedule]
    if: "env.VISOR_AGENT_MODE == 'improve' || (outputs['detect-mode'] && outputs['detect-mode'].mode == 'improve')"
    exec: |
      node <<'NODE'
      const fs=require('fs'); const path=require('path');
      const cfgPath = path.resolve(process.env.VISOR_AGENT_PATH);
      if (!fs.existsSync(cfgPath)) { console.log(JSON.stringify({ ok:false, error:'not_found', path:cfgPath })); process.exit(0); }
      const dir = path.dirname(cfgPath);
      const slug = path.basename(cfgPath).replace(/\.ya?ml$/i,'');
      const testsPath = process.env.VISOR_AGENT_TESTS_PATH ? path.resolve(process.env.VISOR_AGENT_TESTS_PATH) : path.join(dir, slug + '.tests.yaml');
      let cfgText = ''; let testsText = '';
      try { cfgText = fs.readFileSync(cfgPath,'utf8'); } catch {}
      try { testsText = fs.readFileSync(testsPath,'utf8'); } catch {}
      const files = [];
      if (cfgText) files.push({ path: cfgPath, content: cfgText });
      if (testsText) files.push({ path: testsPath, content: testsText });
      console.log(JSON.stringify({ ok:true, slug, config_path: cfgPath, tests_path: testsPath, files }));
      NODE
    output_format: json
    on_success:
      goto: write
      goto_event: schedule

  draft:
    type: ai
    group: agent-build
    on: [issue_opened, schedule]
    if: "String((env.VISOR_AGENT_MODE || 'new')).toLowerCase() !== 'improve'"
    ai_prompt_type: engineer
    ai:
      provider: mock
      skip_code_context: true
    schema:
      type: object
      properties:
        slug: { type: string }
        summary: { type: string }
        files:
          type: array
          items:
            type: object
            properties: { path: { type: string }, content: { type: string } }
            required: [path, content]
      required: [slug, files]
    prompt: |
      Generate a new Visor agent with YAML config and YAML tests only. Keep it simple and tailored to this repository.

      Brief:
      {{ outputs['brief'] | default: 'Create a minimal agent that can scaffold other agents (config + tests).' }}

      Return JSON fields: slug, summary, files[] (path+content). Only content matters; paths will be normalized to the target path.
      Include two files: <slug>.yaml and <slug>.tests.yaml.
    on_success:
      goto: write
      goto_event: schedule

  write:
    type: command
    group: agent-build
    # Triggered only via goto from draft/refine/load-existing to avoid early evaluation
    on: [issue_opened, schedule]
    if: "((((outputs||{})['refine']||{}).files) || (((outputs||{})['draft']||{}).files) || (((outputs||{})['load-existing']||{}).files)) && (env.VISOR_AGENT_PATH || env.VISOR_AGENT_OUT_DIR)"
    exec: |
      {% assign set = outputs['refine'].files | default: outputs['draft'].files | default: outputs['load-existing'].files %}
      cat > .visor-agent-files.json <<'JSON'
      {{ set | to_json | default: '[]' }}
      JSON
      node <<'NODE'
      const fs = require('fs');
      const path = require('path');
      const yaml = require('js-yaml');
      // Read AI/refine results
      let files = [];
      try { files = JSON.parse(fs.readFileSync('.visor-agent-files.json','utf8')) || []; } catch {}
      // Destination path (single co-located file)
      const targetPath = process.env.VISOR_AGENT_PATH ? path.resolve(process.env.VISOR_AGENT_PATH) : path.resolve(String(process.env.VISOR_AGENT_OUT_DIR||'.'), 'agent.yaml');
      const ensureObj = (v) => (v && typeof v === 'object' ? v : {});
      const loadYaml = (t) => { try { return ensureObj(yaml.load(String(t||''))); } catch { return {}; } };
      let configDoc = {};
      let testsDoc = {};
      for (const f of files) {
        const doc = loadYaml(f.content);
        if (doc && typeof doc === 'object') {
          const p = String((f && f.path) || '');
          const looksLikeTests = /\.tests\.ya?ml$/i.test(p) || ('cases' in doc) || ('defaults' in doc);
          if (doc.tests && typeof doc.tests === 'object') {
            testsDoc = doc;
          } else if (looksLikeTests && Object.keys(testsDoc).length === 0) {
            // Treat as tests source even without top-level 'tests'
            testsDoc = doc;
          } else if (!configDoc || Object.keys(configDoc).length === 0) {
            configDoc = doc;
          }
        }
      }
      const finalDoc = {};
      const cfg = ensureObj(configDoc);
      // Copy all non-tests top-level keys from config
      for (const k of Object.keys(cfg)) { if (k !== 'tests' && k !== 'extends') finalDoc[k] = cfg[k]; }
      if (!finalDoc.version) finalDoc.version = '1.0';
      // If no steps provided in config, create a minimal hello step as a sane fallback
      if (!finalDoc.steps || typeof finalDoc.steps !== 'object' || Object.keys(finalDoc.steps).length === 0) {
        finalDoc.steps = {
          hello: { type: 'log', message: 'hello from agent-builder', level: 'info', on: ['issue_opened'] }
        };
      }
      // Merge tests:
      // - If testsDoc.tests is an object → use it (valid path)
      // - Else if cfg.tests is an object → use it
      // - Else if a tests file exists but is malformed → preserve AS-IS (to let tests-validate fail)
      // - Else (no tests provided at all) → create a minimal valid tests block
      let testsBlock = undefined;
      if (testsDoc && typeof testsDoc.tests === 'object') {
        testsBlock = testsDoc.tests;
      } else if (cfg.tests && typeof cfg.tests === 'object') {
        testsBlock = cfg.tests;
      } else if (testsDoc && Object.keys(testsDoc).length > 0) {
        // Preserve malformed shape to trigger validator failure in the first pass
        // Examples: testsDoc = { version, extends, cases: [...] } (no top-level tests)
        testsBlock = testsDoc as any;
      } else {
        testsBlock = {
          defaults: { strict: true, ai_provider: 'mock' },
          cases: [ { name: 'smoke', event: 'issue_opened', fixture: 'gh.issue_open.minimal', expect: {} } ],
        };
      }
      finalDoc.tests = testsBlock;
      // Write one file only
      fs.mkdirSync(path.dirname(targetPath), { recursive: true });
      fs.writeFileSync(targetPath, yaml.dump(finalDoc), 'utf8');
      console.log(JSON.stringify({ written: [targetPath], slug: (cfg.slug || 'agent'), config_path: targetPath, tests_path: targetPath, mode: '{{ outputs['detect-mode'].mode | default: 'new' }}' }));
      NODE
    output_format: json

  config-lint:
    type: command
    group: agent-quality
    depends_on: [write]
    on: [issue_opened, schedule]
    if: "((outputs||{})['write'])"
    exec: |
      node dist/index.js validate --config {{ outputs['write'].config_path }}
    output_format: text
    # rely on provider exit code / issues, avoid double-routing via fail_if
    fail_if: "false"
    on_fail:
      goto: refine
      goto_event: schedule

  tests-validate:
    type: command
    group: agent-quality
    depends_on: [write]
    on: [issue_opened, schedule]
    if: "((outputs||{})['write'])"
    exec: |
      node <<'NODE'
      const fs=require('fs'); const path=require('path'); const yaml=require('js-yaml');
      const p = path.resolve('{{ outputs['write'].tests_path }}');
      let text=''; try { text = fs.readFileSync(p,'utf8'); } catch { console.log('read_error'); process.exit(2); }
      let doc; try { doc = yaml.load(text) || {}; } catch { console.log('parse_error'); process.exit(3); }
      if (typeof doc !== 'object' || !doc) { console.log('doc_type_error'); process.exit(4); }
      const t = doc.tests;
      if (!t || typeof t !== 'object') { console.log('tests_missing_or_not_object'); process.exit(5); }
      const allowed = new Set(['defaults','fixtures','cases']);
      for (const k of Object.keys(t)) if (!allowed.has(k)) { console.log('tests_has_additional_properties'); process.exit(6); }
      if (!Array.isArray(t.cases)) { console.log('cases_missing'); process.exit(7); }
      console.log('valid');
      NODE
    output_format: text
    fail_if: "output && output.trim() !== 'valid'"
    on_fail:
      goto: refine
      goto_event: schedule

  verify-tests:
    type: command
    group: agent-quality
    depends_on: [write, config-lint, tests-validate]
    on: [issue_opened, schedule]
    if: "((outputs||{})['write']) && !(String(((outputs||{})['tests-validate']||'')).toLowerCase().includes('failed'))"
    exec: |
      node dist/index.js test --config {{ outputs['write'].tests_path }} --json - --bail
    output_format: json
    fail_if: "output && Number(output.failures||0) > 0"
    on_fail:
      goto: refine
      goto_event: schedule
    env:
      VISOR_TEST_SUMMARY_SILENT: "true"
    # no on_success routing; dependents will be scheduled by the initial goto to 'write'

  code-review:
    type: ai
    group: agent-quality
    depends_on: [verify-tests]
    on: [issue_opened, schedule]
    if: "((outputs||{})['write'])"
    ai_prompt_type: engineer
    ai:
      provider: mock
      skip_code_context: true
    schema:
      type: object
      properties:
        ok: { type: 'boolean' }
        feedback: { type: 'string' }
        concerns: { type: 'array', items: { type: 'string' } }
      required: [ok]
    prompt: |
      Validate the generated agent against the brief and its tests. Ensure it:
      - Directly addresses the brief without over-complication.
      - Includes tests that cover success and failure cases.

      Brief:
      {{ outputs['brief'] | default: 'Create a minimal agent that can scaffold other agents (config + tests).' }}

      Tests validate (text):
      {{ outputs['tests-validate'] }}

      Tests run (JSON):
      {{ outputs['verify-tests'] | json }}

      Respond with JSON: { ok: boolean, feedback: string, concerns?: string[] }.
    fail_if: "output && output.ok === false"
    on_fail:
      goto: refine
    # no on_success routing; dependents will be scheduled by the initial goto to 'write'

  refine:
    type: ai
    group: agent-build
    reuse_ai_session: draft
    session_mode: append
    on: [schedule]
    ai_prompt_type: engineer
    schema:
      type: object
      properties:
        files:
          type: array
          items:
            type: object
            properties: { path: { type: string }, content: { type: string } }
            required: [path, content]
      required: [files]
    prompt: |
      Refine the files so that lint and tests pass. Keep changes focused and minimal.

      Brief:
      {{ outputs['brief'] }}

      Config validate (text):
      {{ outputs['config-lint'] }}

      Tests validate (text):
      {{ outputs['tests-validate'] }}

      Tests run (JSON):
      {{ outputs['verify-tests'] | json }}
    on_success:
      goto: write

  cleanup:
    type: command
    group: agent-quality
    depends_on: [code-review]
    on: [issue_opened, schedule]
    if: "((outputs||{})['write'])"
    exec: |
      node <<'NODE'
      const fs=require('fs');
      const written = (process.env.WRITTEN && process.env.WRITTEN.split('\n').filter(Boolean)) || [];
      const force = String(process.env.VISOR_AGENT_CLEANUP_FORCE||'').toLowerCase()==='true';
      const removed=[];
      for (const f of written){ const isTmp = /(^|\/)tmp\//.test(f); if (isTmp || force) { try{ if(fs.existsSync(f)){ fs.unlinkSync(f); removed.push(f);} }catch{} } }
      try{ if(fs.existsSync('.visor-agent-files.json')) fs.unlinkSync('.visor-agent-files.json'); }catch{}
      console.log(JSON.stringify({ removed }));
      NODE
    output_format: json
    # finish depends_on cleanup; it will run automatically in the same forward-run

  finish:
    type: log
    group: agent-build
    on: [issue_opened, schedule]
    depends_on: [cleanup]
    message: "✅ Agent build finished: files written, linted, tested, and reviewed."
    level: info
    on_success:
      goto_js: |
        const enabled = String(env.VISOR_CHAT_MODE||'').toLowerCase()==='true';
        if (!enabled) return null;
        const max = Number(env.VISOR_CHAT_MAX_LOOPS||1);
        const count = (outputs_history['brief']||[]).length|0;
        return count < max ? 'detect-mode' : null;
      goto_event: manual

tests:
  defaults:
    strict: true
    ai_provider: mock
    fail_on_unexpected_calls: true

  cases:
    - name: new-write-to-cwd
      description: Generates to output/ (cwd) and cleans up forcibly.
      event: issue_opened
      fixture: gh.issue_open.minimal
      mocks:
        draft:
          slug: helloagent
          files:
            - path: helloagent.yaml
              content: |
                version: "1.0"
                output:
                  pr_comment:
                    format: markdown
                    group_by: check
                    collapse: true
                steps:
                  hello:
                    type: log
                    message: "hi"
                    level: info
                    on: [issue_opened]
            - path: helloagent.tests.yaml
              content: |
                version: "1.0"
                extends: "helloagent.yaml"
                tests:
                  defaults: { strict: true, ai_provider: mock }
                  cases:
                    - name: ok
                      event: issue_opened
                      fixture: gh.issue_open.minimal
                      expect:
                        calls:
                          - step: hello
                            exactly: 1
        verify-tests:
          failures: 0
          results:
            - name: ok
              passed: true
        code-review:
          ok: true
          feedback: "LGTM"
        cleanup:
          removed: []
      expect:
        calls:
          - step: start
            exactly: 1
          - step: detect-mode
            exactly: 1
          - step: draft
            exactly: 1
          - step: write
            exactly: 1
          - step: config-lint
            exactly: 1
          - step: tests-validate
            exactly: 1
          - step: verify-tests
            exactly: 1
          - step: code-review
            exactly: 1
          - step: cleanup
            exactly: 1
          - step: finish
            exactly: 1
        prompts:
          - step: draft
            index: last
            not_contains:
              - "<pull_request>"
              - "<files_summary>"
      env:
        VISOR_AGENT_OUT_DIR: "output"
        VISOR_AGENT_CLEANUP_FORCE: "true"

    - name: improve-existing
      description: Improves an existing config in-place using load-existing + refine.
      event: issue_opened
      fixture: gh.issue_open.minimal
      mocks:
        detect-mode:
          path: "tmp/improv.yaml"
          exists: true
          empty: false
          mode: "improve"
        load-existing:
          ok: true
          slug: improv
          config_path: tmp/improv.yaml
          tests_path: tmp/improv.tests.yaml
          files:
            - path: tmp/improv.yaml
              content: |
                version: "1.0"
                output:
                  pr_comment:
                    format: markdown
                    group_by: check
                    collapse: true
                steps:
                  hello:
                    type: log
                    message: "hello"
                    level: info
                    on: [issue_opened]
            - path: tmp/improv.tests.yaml
              content: |
                version: "1.0"
                extends: "improv.yaml"
                tests:
                  defaults: { strict: true, ai_provider: mock }
                  cases:
                    - name: failing
                      event: issue_opened
                      fixture: gh.issue_open.minimal
                      expect:
                        calls:
                          - step: hello
                            exactly: 2
        refine:
          files:
            - path: tmp/improv.tests.yaml
              content: |
                version: "1.0"
                extends: "improv.yaml"
                tests:
                  defaults: { strict: true, ai_provider: mock }
                  cases:
                    - name: fixed
                      event: issue_opened
                      fixture: gh.issue_open.minimal
                      expect:
                        calls:
                          - step: hello
                            exactly: 1
        verify-tests:
          failures: 0
          results:
            - name: fixed
              passed: true
        code-review:
          ok: true
          feedback: "Refined existing"
      expect:
        calls:
          - step: start
            exactly: 1
          - step: detect-mode
            exactly: 1
          - step: load-existing
            exactly: 1
          - step: write
            exactly: 1
          - step: config-lint
            exactly: 1
          - step: tests-validate
            exactly: 1
          - step: verify-tests
            exactly: 1
          - step: code-review
            exactly: 1
          - step: cleanup
            exactly: 1
          - step: finish
            exactly: 1
      env:
        VISOR_AGENT_MODE: "improve"
        VISOR_AGENT_OUT_DIR: "tmp"
        VISOR_AGENT_PATH: "tmp/improv.yaml"
        VISOR_AGENT_TESTS_PATH: "tmp/improv.tests.yaml"

    - name: loop-on-tests-validate
      description: tests-validate fails first, refine fixes, loop passes.
      event: issue_opened
      fixture: gh.issue_open.minimal
      mocks:
        draft:
          slug: tval
          files:
            - path: tval.yaml
              content: |
                version: "1.0"
                output:
                  pr_comment:
                    format: markdown
                    group_by: check
                    collapse: true
                steps:
                  hello:
                    type: log
                    message: "hi"
                    level: info
                    on: [issue_opened]
            - path: tval.tests.yaml
              content: |
                version: "1.0"
                extends: "tval.yaml"
                # wrong shape on purpose
                cases:
                  - name: wrong-shape
        refine:
          files:
            - path: tval.tests.yaml
              content: |
                version: "1.0"
                extends: "tval.yaml"
                tests:
                  defaults: { strict: true, ai_provider: mock }
                  cases:
                    - name: ok
                      event: issue_opened
                      fixture: gh.issue_open.minimal
                      expect: {}
        verify-tests:
          failures: 0
          results:
            - name: ok
              passed: true
        code-review:
          ok: true
          feedback: "OK"
      expect:
        calls:
          - step: start
            exactly: 1
          - step: detect-mode
            exactly: 1
          - step: draft
            exactly: 1
          - step: refine
            exactly: 1
          - step: write
            exactly: 2
          - step: config-lint
            exactly: 2
          - step: tests-validate
            exactly: 2
          - step: verify-tests
            exactly: 1
          - step: code-review
            exactly: 1
          - step: cleanup
            exactly: 1
          - step: finish
            exactly: 1
      env:
        VISOR_AGENT_OUT_DIR: "tmp"

    - name: chat-loop-manual
      description: Chat loop returns to brief once, then stops.
      event: manual
      fixture: local.minimal
      mocks:
        detect-mode:
          path: "tmp/nonexistent.yaml"
          exists: false
          empty: true
          mode: "new"
        brief[]:
          - "first brief"
          - "second brief"
        draft:
          slug: chatg
          files:
            - path: chatg.yaml
              content: |
                version: "1.0"
                output:
                  pr_comment:
                    format: markdown
                    group_by: check
                    collapse: true
                steps:
                  hello:
                    type: log
                    message: "hi"
                    level: info
                    on: [manual, issue_opened]
            - path: chatg.tests.yaml
              content: |
                version: "1.0"
                extends: "chatg.yaml"
                tests:
                  defaults: { strict: true, ai_provider: mock }
                  cases:
                    - name: ok
                      event: manual
                      fixture: local.minimal
                      expect:
                        calls:
                          - step: hello
                            exactly: 1
        verify-tests:
          failures: 0
          results:
            - name: ok
              passed: true
        code-review:
          ok: true
          feedback: "LGTM"
      expect:
        calls:
          - step: brief
            at_least: 1
          - step: detect-mode
            at_least: 1
          - step: draft
            at_least: 1
          - step: write
            at_least: 1
          - step: config-lint
            at_least: 1
          - step: tests-validate
            at_least: 1
          - step: verify-tests
            at_least: 1
          - step: code-review
            at_least: 1
          - step: cleanup
            at_least: 1
          - step: finish
            at_least: 1
      env:
        VISOR_AGENT_OUT_DIR: "tmp"
        VISOR_AGENT_PATH: "tmp/nonexistent.yaml"
        VISOR_CHAT_MODE: "true"
        VISOR_CHAT_MAX_LOOPS: "1"
        VISOR_TEST_MODE: "true"

    - name: prompt-never-includes-context-even-when-allowed
      description: Even if tests allow code context, draft disables it at step level.
      event: manual
      fixture: local.minimal
      expect:
        calls:
          - step: brief
            exactly: 1
          - step: draft
            exactly: 1
        prompts:
          - step: draft
            index: last
            not_contains:
              - "<pull_request>"
              - "<files_summary>"
      env:
        VISOR_TEST_ALLOW_CODE_CONTEXT: "true"

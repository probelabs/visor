version: "1.0"

# Example configuration showing how to use if conditions to control check execution
# The if condition determines whether a check should run based on context

steps:
  # Run only on main branch
  production-check:
    type: ai
    schema: code-review
    prompt: "Check production readiness and deployment safety"
    on: [pr_opened, pr_updated]
    if: 'branch == "main" || baseBranch == "main"'
    fail_if: "output.issues.some(i => i.severity === 'critical')"

  # Run only when there are file changes
  incremental-review:
    type: ai
    schema: code-review
    prompt: "Review changed files for issues"
    on: [pr_opened, pr_updated]
    if: "filesCount > 0"
    fail_if: "output.issues.some(i => i.severity === 'error')"

  # Run only on feature branches
  feature-validation:
    type: ai
    schema: code-review
    prompt: "Validate feature implementation"
    on: [pr_opened, pr_updated]
    if: 'startsWith(branch, "feature/")'
    fail_if: "output.issues.some(i => i.severity === 'critical')"

  # Run only in CI environment
  ci-only-check:
    type: ai
    schema: security
    prompt: "Comprehensive security scan"
    on: [pr_opened, pr_updated]
    if: 'env.CI == "true"'
    fail_if: "output.issues.length > 0"

  # Run when specific files are changed
  auth-security-check:
    type: ai
    schema: security
    prompt: "Security review for authentication changes"
    on: [pr_opened, pr_updated]
    # Run when auth-related files are modified
    if: 'contains(filesChanged, "auth") || contains(filesChanged, "login") || contains(filesChanged, "session")'
    fail_if: "output.issues.length > 0"

  # Complex condition - run on main or when many files changed
  comprehensive-review:
    type: ai
    schema: code-review
    prompt: "Comprehensive code review"
    on: [pr_opened, pr_updated]
    # Run on main branch OR when more than 10 files changed
    if: 'branch == "main" || filesCount > 10'
    fail_if: "output.issues.some(i => i.severity === 'critical')"

  # Always run (default behavior)
  standard-check:
    type: ai
    schema: code-review
    prompt: "Standard code review"
    on: [pr_opened, pr_updated]
    if: "always()"  # Explicitly always run
    fail_if: "output.issues.some(i => i.severity === 'critical')"

  # Never run (can be enabled by removing/changing condition)
  disabled-check:
    type: ai
    schema: code-review
    prompt: "Currently disabled check"
    on: [pr_opened, pr_updated]
    if: "!always()"  # Never run
    fail_if: "output.issues.some(i => i.severity === 'critical')"

  # Dependency-based execution
  security-check:
    type: ai
    schema: security
    prompt: "Security vulnerability scan"
    on: [pr_opened, pr_updated]
    if: "always()"
    fail_if: "output.issues.some(i => i.severity === 'critical')"

  performance-check:
    type: ai
    schema: code-review
    prompt: "Performance analysis"
    on: [pr_opened, pr_updated]
    if: "always()"
    fail_if: "output.issues.filter(i => i.severity === 'error').length > 2"

  # Run only if security check found issues
  security-remediation:
    type: ai
    schema: code-review
    prompt: "Suggest security fixes"
    on: [pr_opened, pr_updated]
    depends_on: [security-check]
    # Only run if security check found issues (use length() function)
    if: 'outputs["security-check"] && length(outputs["security-check"].issues) > 0'
    fail_if: "output.issues.some(i => i.severity === 'critical')"

  # Run only if all previous checks passed
  final-validation:
    type: ai
    schema: code-review
    prompt: "Final validation before merge"
    on: [pr_opened, pr_updated]
    depends_on: [security-check, performance-check]
    # Only run if both dependencies passed (check for specific severities)
    if: |
      outputs["security-check"] &&
      !hasIssue(outputs["security-check"].issues, "severity", "critical") &&
      !hasIssue(outputs["security-check"].issues, "severity", "error") &&
      outputs["performance-check"] &&
      !hasIssue(outputs["performance-check"].issues, "severity", "critical") &&
      !hasIssue(outputs["performance-check"].issues, "severity", "error")
    fail_if: "output.issues.length > 0"

  # Environment-specific checks
  staging-check:
    type: ai
    schema: code-review
    prompt: "Staging environment validation"
    on: [pr_opened, pr_updated]
    # Run only in staging environment
    if: 'env.ENVIRONMENT == "staging"'
    fail_if: "output.issues.some(i => i.severity === 'critical')"

  # Time-based or conditional skip
  weekend-check:
    type: ai
    schema: code-review
    prompt: "Weekend maintenance check"
    on: [pr_opened, pr_updated]
    # Could be used with env vars set by CI to control execution
    if: 'env.RUN_WEEKEND_CHECKS == "true"'
    fail_if: "output.issues.some(i => i.severity === 'critical')"

output:
  pr_comment:
    format: markdown
    group_by: check
    collapse: true

# Available variables in if conditions:
# - checkName: Name of the current check
# - branch: Current branch name
# - baseBranch: Target/base branch name
# - filesChanged: Array of changed file paths
# - filesCount: Number of changed files
# - event: Event that triggered the check (e.g., "pr_opened", "manual")
# - env: Environment variables
# - outputs: Raw ReviewSummary objects from previous checks (for dependent checks)
#   Example: outputs["check-name"].issues, outputs["check-name"].suggestions
# - metadata: Additional metadata (hasChanges, branch, event, checkName)

# Available functions (GitHub Actions-style):
# - contains(haystack, needle): Check if string/array contains value
# - startsWith(str, prefix): Check if string starts with prefix
# - endsWith(str, suffix): Check if string ends with suffix
# - always(): Always returns true
# - length(array|string|object): Get length of array, string, or object keys
# - hasIssue(issues, field, value): Check if any issue has field matching value
# - countIssues(issues, field, value): Count issues with field matching value
# - hasFileMatching(issues, pattern): Check if any issue file contains pattern
# - Standard operators: ==, !=, >, <, >=, <=, &&, ||, !
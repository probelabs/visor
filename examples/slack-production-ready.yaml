# Production-Ready Slack Bot Configuration Example
# This configuration demonstrates best practices for production deployment
# with optimal performance tuning, security, and monitoring

version: '1.0'

# HTTP Server Configuration
http_server:
  enabled: true
  port: 8080
  # In production, use a reverse proxy (nginx/caddy) for HTTPS termination

# Slack Bot Configuration
slack:
  bots:
    - id: production-bot
      # Use a descriptive, bot-specific endpoint
      endpoint: "/bots/slack/production"

      # Security: Always use environment variables for secrets
      signing_secret: "${SLACK_SIGNING_SECRET}"
      bot_token: "${SLACK_BOT_TOKEN}"

      # Mention and thread handling
      mentions: direct
      threads: required

      # Conversation fetch configuration
      fetch:
        scope: thread
        # Optimized for typical conversations (40 messages covers ~90% of threads)
        max_messages: 40

        # Cache configuration - optimized for production
        cache:
          # 15-minute TTL balances freshness with API call reduction
          ttl_seconds: 900
          # Large enough to cache active threads, small enough to fit in memory
          max_threads: 500

      # Channel access control
      # IMPORTANT: Always restrict bot to specific channels in production
      channel_allowlist:
        - "C*SUPPORT"    # Support channels
        - "C*ENGINEERING" # Engineering channels
        - "CPROD*"        # Production channels
        # Use wildcards strategically to match channel patterns

      # Worker pool configuration - optimized for medium deployment
      worker_pool:
        # Queue capacity for smooth operation during spikes
        # Note: Pool size is controlled by Visor's max_parallelism (default: 3)
        queue_capacity: 100
        # 5-minute timeout for long-running workflows
        task_timeout: 300000

      # Rate limiting - essential for production
      rate_limiting:
        enabled: true

        # Bot-level limits (protects against overall bot abuse)
        bot:
          # 60 requests/minute = 1 request/second average
          requests_per_minute: 60
          # 1000 requests/hour = ~17 requests/minute sustained
          requests_per_hour: 1000
          # Max 10 concurrent workflows (matches Slack API recommendations)
          concurrent_requests: 10

        # User-level limits (prevents individual user abuse)
        user:
          # 10 requests/minute per user
          requests_per_minute: 10
          # 100 requests/hour per user
          requests_per_hour: 100
          # Max 2 concurrent requests per user
          concurrent_requests: 2

        # Channel-level limits (prevents channel spam)
        channel:
          # 20 requests/minute per channel
          requests_per_minute: 20
          # 500 requests/hour per channel
          requests_per_hour: 500
          # Max 5 concurrent requests per channel
          concurrent_requests: 5

        # Rate limit actions
        actions:
          # Send ephemeral message to user when rate limited
          send_ephemeral_message: true
          ephemeral_message: "You've exceeded the rate limit. Please wait before trying again."
          # Don't queue requests - reject immediately for clearer feedback
          queue_when_near_limit: false

        # Storage: use memory for single-instance, Redis for multi-instance
        storage:
          type: memory
          # For multi-instance deployments, use Redis:
          # type: redis
          # redis:
          #   url: "${REDIS_URL}"
          #   key_prefix: "visor:ratelimit:"

      # Cache observability - enable monitoring endpoints
      cache_observability:
        enable_cache_endpoints: true
        # Protect admin operations with token
        cache_admin_token: "${CACHE_ADMIN_TOKEN}"

      # Cache prewarming - faster startup with warm cache
      cache_prewarming:
        enabled: true
        # Prewarm support channels on startup
        channels:
          - "CSUPPORT"
          - "CENGINEERING"
        # Max 5 concurrent prewarm operations
        concurrency: 5
        # 100ms delay between prewarm requests
        rate_limit_ms: 100

      # Response configuration
      response:
        # Auto-update on error for better UX
        auto_update_on_error: true

      # Workflow selection
      workflow: production-workflow

# Production workflow configuration
checks:
  production-workflow:
    type: workflow
    steps:
      # Step 1: Log incoming request
      - logger:
          type: log
          message: |
            Production workflow triggered
            Bot: {{ bot.botId }}
            Channel: {{ bot.channel }}
            User: {{ bot.user }}
            Thread: {{ bot.threadTs }}

      # Step 2: Validate request (security check)
      - validate-request:
          type: noop
          # Ensure bot context is present
          if: 'bot && bot.user && bot.channel'
          fail_if: '!bot || !bot.user || !bot.channel'

      # Step 3: Process user request
      - process-request:
          type: command
          command: echo
          args:
            - "Processing production request from {{ bot.user }}"

      # Step 4: Generate response
      - generate-response:
          type: noop
          # In production, replace with actual AI/logic
          transform_js: |
            return {
              text: "Production bot response",
              metadata: {
                processedAt: new Date().toISOString(),
                userId: context.bot?.user,
                channelId: context.bot?.channel
              }
            };

# Production Deployment Checklist:
# ✅ Environment variables set (SLACK_SIGNING_SECRET, SLACK_BOT_TOKEN, CACHE_ADMIN_TOKEN)
# ✅ HTTPS configured via reverse proxy (nginx/caddy)
# ✅ Channel allowlist configured
# ✅ Rate limiting enabled
# ✅ Worker pool sized appropriately
# ✅ Cache TTL optimized (900s recommended)
# ✅ Cache observability enabled for monitoring
# ✅ Cache prewarming configured for faster startup
# ✅ Monitoring/logging infrastructure in place
# ✅ Alerting configured for errors and rate limit violations
# ✅ Backup/recovery plan documented
# ✅ Load testing completed
# ✅ Security audit passed

# Memory Estimation (for this configuration):
# - Cache: ~40 MB (500 threads × 40 messages × 2KB)
# - Worker Pool: ~60 MB (5 workers × 10MB + 100 queue × 0.5MB)
# - Rate Limit: ~5 MB
# - Overhead: ~50 MB
# - Total: ~155 MB
#
# Recommended server specs:
# - Memory: 512 MB minimum (1 GB recommended)
# - CPU: 1 core minimum (2 cores recommended)
# - Network: Low latency to Slack API (<100ms)

# Performance Characteristics:
# - Throughput: ~60 requests/minute sustained, 100+ burst
# - Latency: <2s average response time
# - Concurrency: 10 simultaneous workflows
# - Cache hit rate: 70-90% (with prewarming)
# - API calls: ~6-12 per minute (with cache)

# Scaling Guidelines:
# - Small deployment (<100 users): Use this config as-is
# - Medium deployment (100-500 users): Increase worker_pool.size to 7, max_threads to 750
# - Large deployment (500+ users): Increase worker_pool.size to 10, max_threads to 1000, enable Redis persistence
# - Multi-instance: Enable Redis for rate limiting and cache persistence

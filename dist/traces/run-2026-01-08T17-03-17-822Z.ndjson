{"name":"visor.run","attributes":{"started":true}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"jira-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"zendesk-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"log-request","visor.provider.type":"log"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"019b00a5-2364-474d-89c2-32788058e66a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"5b1eb3be-525d-44e1-944a-26c9e89cd85b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"019b00a5-2364-474d-89c2-32788058e66a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"5b1eb3be-525d-44e1-944a-26c9e89cd85b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"019b00a5-2364-474d-89c2-32788058e66a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"5b1eb3be-525d-44e1-944a-26c9e89cd85b"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-keys","visor.provider.type":"script"}}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-ids","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"history\":{}},\"outputs_history\":{},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"include_comments\":true,\"max_tickets\":5,\"download_assets\":true,\"cache_dir\":\".cache/zendesk\",\"text\":\"hi!\\nhi!\\n\"},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.output":"{\"data\":[],\"count\":0}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"019b00a5-2364-474d-89c2-32788058e66a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"019b00a5-2364-474d-89c2-32788058e66a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"019b00a5-2364-474d-89c2-32788058e66a"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"no-issues-fallback","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"no-issues-fallback","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"extract-keys\":{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767891905211},\"fetch-issues\":{\"issues\":[]},\"history\":{\"extract-keys\":[{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767891905211}],\"fetch-issues\":[{\"issues\":[]}]}},\"outputs_history\":{\"extract-keys\":[{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767891905211}],\"fetch-issues\":[{\"issues\":[]}]},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"text\":\"hi!\\nhi!\\n\",\"jql\":\"\",\"custom_field_aliases\":{},\"include_comments\":true,\"max_issues\":3,\"ticket_prefixes\":[\"TT\",\"DX\"]},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.check","attributes":{"visor.check.id":"no-issues-fallback","visor.check.output":"\"<jira_context><issue_count>0</issue_count><message>No Jira issue keys found in the provided text</message></jira_context>\""},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"019b00a5-2364-474d-89c2-32788058e66a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"019b00a5-2364-474d-89c2-32788058e66a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"019b00a5-2364-474d-89c2-32788058e66a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"019b00a5-2364-474d-89c2-32788058e66a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"019b00a5-2364-474d-89c2-32788058e66a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"5b1eb3be-525d-44e1-944a-26c9e89cd85b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"5b1eb3be-525d-44e1-944a-26c9e89cd85b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"5b1eb3be-525d-44e1-944a-26c9e89cd85b"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"no-tickets-fallback","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"no-tickets-fallback","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"extract-ids\":{\"data\":[],\"count\":0,\"ts\":1767891905246},\"fetch-tickets\":{\"issues\":[]},\"history\":{\"extract-ids\":[{\"data\":[],\"count\":0,\"ts\":1767891905246}],\"fetch-tickets\":[{\"issues\":[]}]}},\"outputs_history\":{\"extract-ids\":[{\"data\":[],\"count\":0,\"ts\":1767891905246}],\"fetch-tickets\":[{\"issues\":[]}]},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"include_comments\":true,\"max_tickets\":5,\"download_assets\":true,\"cache_dir\":\".cache/zendesk\",\"text\":\"hi!\\nhi!\\n\"},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.check","attributes":{"visor.check.id":"no-tickets-fallback","visor.check.output":"\"<zendesk_context><ticket_count>0</ticket_count><message>No Zendesk ticket IDs found in the provided text</message></zendesk_context>\""},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"5b1eb3be-525d-44e1-944a-26c9e89cd85b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"5b1eb3be-525d-44e1-944a-26c9e89cd85b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"5b1eb3be-525d-44e1-944a-26c9e89cd85b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"5b1eb3be-525d-44e1-944a-26c9e89cd85b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"5b1eb3be-525d-44e1-944a-26c9e89cd85b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"5b1eb3be-525d-44e1-944a-26c9e89cd85b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"5b1eb3be-525d-44e1-944a-26c9e89cd85b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"5b1eb3be-525d-44e1-944a-26c9e89cd85b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"5b1eb3be-525d-44e1-944a-26c9e89cd85b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"5b1eb3be-525d-44e1-944a-26c9e89cd85b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"5b1eb3be-525d-44e1-944a-26c9e89cd85b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"route-intent","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.input.context":"{\"pr\":{\"number\":0,\"title\":\"Local Analysis: feat/slack-frontend-v2 (70 modified)\",\"author\":\"Leonid Bugaev\",\"branch\":\"feat/slack-frontend-v2\",\"base\":\"main\"},\"files\":[{\"filename\":\"README.md\",\"additions\":7,\"deletions\":1,\"changes\":8,\"patch\":\"diff --git a/README.md b/README.md\\nindex 41131861..d92f02cc 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -534,6 +534,12 @@ steps:\\n     session_mode: append    # Shares history for full conversation\\n ```\\n \\n+You can also reuse the **same check‚Äôs** session when it loops back to itself, using:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` with `session_mode: append`\\n+\\n+See the standalone example at `examples/session-reuse-self.yaml` and the detailed guide in [docs/advanced-ai.md](docs/advanced-ai.md).\\n+\\n Learn more: [docs/advanced-ai.md](docs/advanced-ai.md)\\n \\n ## üìã Schema-Template System\\n\",\"status\":\"modified\"},{\"filename\":\"docs/advanced-ai.md\",\"additions\":61,\"deletions\":2,\"changes\":63,\"patch\":\"diff --git a/docs/advanced-ai.md b/docs/advanced-ai.md\\nindex bb8e684a..879285ed 100644\\n--- a/docs/advanced-ai.md\\n+++ b/docs/advanced-ai.md\\n@@ -1,7 +1,7 @@\\n ## üß† Advanced AI Features\\n \\n ### AI Session Reuse\\n-Use `reuse_ai_session: true` on dependent checks to continue conversation context with the AI across checks. This improves follow‚Äëups and consistency.\\n+Use `reuse_ai_session` on checks to continue conversation context with the AI across steps. This improves follow‚Äëups and consistency for follow‚Äëon analysis and chat‚Äëstyle flows.\\n \\n **Session Modes:**\\n - **`clone` (default)**: Creates a copy of the conversation history. Each check gets an independent session with the same starting context. Changes made by one check don't affect others.\\n@@ -26,6 +26,65 @@ steps:\\n     depends_on: [security-remediation]\\n     reuse_ai_session: true\\n     session_mode: append  # Share history - sees full conversation\\n+\\n+#### Reusing your own session: `reuse_ai_session: self`\\n+\\n+Sometimes the step you want to loop back into is the AI step itself (e.g. Slack assistants or multi‚Äëturn internal tools). For that case you can use:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` ‚Äì the step reuses its **own** Probe session when it runs again in the same engine run.\\n+- `session_mode: append` ‚Äì makes the follow‚Äëup behave like a normal conversation turn.\\n+\\n+On the first run of the step, Visor creates a new ProbeAgent session and registers it. If routing (`on_success.goto`, `goto_js`, etc.) later jumps back to the same step within the same run, the engine:\\n+\\n+- Finds the last result for that step in the current run.\\n+- Reads the `sessionId` stored in the result.\\n+- Calls the AI provider again using `executeReviewWithSessionReuse` with that session id.\\n+\\n+Simple example (no transport wiring, just CLI/tests):\\n+\\n+```yaml\\n+version: \\\"2.0\\\"\\n+\\n+steps:\\n+  seed:\\n+    type: script\\n+    content: |\\n+      return { text: \\\"hello from seed\\\" };\\n+\\n+  convo:\\n+    type: ai\\n+    depends_on: [seed]\\n+    reuse_ai_session: self\\n+    session_mode: append\\n+    ai:\\n+      provider: mock\\n+      model: mock\\n+      disableTools: true\\n+      allowedTools: []\\n+      system_prompt: \\\"You are a tiny echo assistant.\\\"\\n+    prompt: |\\n+      Seed message: {{ outputs['seed'].text }}\\n+\\n+      Past convo outputs in this run:\\n+      {% assign hist = outputs_history['convo'] | default: empty %}\\n+      {% if hist and hist.size > 0 %}\\n+      {% for h in hist %}\\n+      - Previous reply {{ forloop.index }}.\\n+      {% endfor %}\\n+      {% else %}\\n+      - No previous replies yet.\\n+      {% endif %}\\n+    on_success:\\n+      goto_js: |\\n+        // Example: re‚Äëenter this step up to 3 times in a single run\\n+        return attempt < 3 ? 'convo' : null;\\n+```\\n+\\n+The corresponding testable example lives at:\\n+\\n+- `examples/session-reuse-self.yaml`\\n+\\n+This keeps the configuration small but shows how to wire `reuse_ai_session: self` and `session_mode: append` without touching higher‚Äëlevel workflows like `tyk-assistant`.\\n ```\\n \\n **When to use each mode:**\\n\",\"status\":\"modified\"},{\"filename\":\"docs/recipes.md\",\"additions\":149,\"deletions\":3,\"changes\":152,\"patch\":\"diff --git a/docs/recipes.md b/docs/recipes.md\\nindex 1015bc6c..1089e9a5 100644\\n--- a/docs/recipes.md\\n+++ b/docs/recipes.md\\n@@ -321,8 +321,154 @@ Tip: When you define a JSON Schema, you generally do **not** need to tell the mo\\n - Avoid noisy fallbacks like `(outputs['x']?.kind ?? '') === 'status'` when `outputs['x']?.kind === 'status'` is equivalent.\\n - These conventions apply uniformly to any provider (`ai`, `command`, `script`, `github`, `http_client`, etc).\\n \\n+### Command step best practices\\n+\\n+When using `type: command` steps:\\n+\\n+**Avoid external tool dependencies** like `jq`, `yq`, `python`, etc.:\\n+- They may not be installed in all environments (GitHub Actions, Docker, CI)\\n+- Use `transform_js` to parse and transform output instead\\n+- Keep shell commands simple: `grep`, `sed`, `awk`, `sort`, `head` are universally available\\n+\\n+```yaml\\n+# Bad - requires jq\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | jq -R -s 'split(\\\"\\\\n\\\")'\\n+  parseJson: true\\n+\\n+# Good - use transform_js for parsing\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | sort -u\\n+  transform_js: |\\n+    const lines = (output || '').trim().split('\\\\n').filter(Boolean);\\n+    return { data: lines, count: lines.length };\\n+```\\n+\\n+**Prefer line-separated output** over JSON from shell:\\n+- Simple to parse with `transform_js`\\n+- No need for `parseJson: true`\\n+- More robust across different shells/environments\\n+\\n+**Use transform_js for structured output**:\\n+- The sandbox provides `output` (command stdout as string)\\n+- Return an object with the fields you need\\n+- Works consistently across all environments\\n+\\n+### Testing workflows with `--no-mocks`\\n+\\n+The `--no-mocks` flag runs your test cases with real providers instead of injecting mock responses. This is essential for:\\n+\\n+1. **Debugging integration issues** - See actual API responses and errors\\n+2. **Capturing realistic mock data** - Get real output to copy into your test cases\\n+3. **Validating credentials** - Verify environment variables are set correctly\\n+4. **Developing new workflows** - Build tests incrementally with real data\\n+\\n+#### Basic usage\\n+\\n+```bash\\n+# Run all test cases with real providers\\n+visor test --config my-workflow.yaml --no-mocks\\n+\\n+# Run a specific test case with real providers\\n+visor test --config my-workflow.yaml --no-mocks --only \\\"my-test-case\\\"\\n+```\\n+\\n+#### Suggested mocks output\\n+\\n+When running with `--no-mocks`, Visor captures each step's output and prints it as YAML you can copy directly into your test case:\\n+\\n+```\\n+üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)\\n+   Step outputs will be captured and printed as suggested mocks\\n+\\n+... test execution ...\\n+\\n+üìã Suggested mocks (copy to your test case):\\n+mocks:\\n+  extract-keys:\\n+    data:\\n+      - PROJ-123\\n+      - DEV-456\\n+    count: 2\\n+  fetch-issues:\\n+    data:\\n+      - key: PROJ-123\\n+        summary: Fix authentication bug\\n+        status: In Progress\\n+```\\n+\\n+Copy the YAML under `mocks:` into your test case's `mocks:` section.\\n+\\n+#### Workflow for building tests\\n+\\n+1. **Start with a minimal test case** (no mocks):\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+   ```\\n+\\n+2. **Run with `--no-mocks`** to capture real outputs:\\n+   ```bash\\n+   visor test --config workflow.yaml --no-mocks --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+3. **Copy the suggested mocks** into your test case:\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+         mocks:\\n+           extract-keys:\\n+             data: [\\\"PROJ-123\\\"]\\n+             count: 1\\n+           # ... rest of captured mocks\\n+   ```\\n+\\n+4. **Add assertions** based on the real data:\\n+   ```yaml\\n+         expect:\\n+           workflow_output:\\n+             - path: issue_count\\n+               equals: 1\\n+   ```\\n+\\n+5. **Run normally** to verify mocks work:\\n+   ```bash\\n+   visor test --config workflow.yaml --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+#### Debugging with `--no-mocks`\\n+\\n+When a test fails with mocks, use `--no-mocks` to see what's actually happening:\\n+\\n+```bash\\n+# See real API responses and errors\\n+visor test --config workflow.yaml --no-mocks --only \\\"failing-test\\\"\\n+\\n+# Common issues revealed:\\n+# - Missing or expired credentials\\n+# - API endpoint changes\\n+# - Unexpected response formats\\n+# - Network/timeout issues\\n+```\\n+\\n+The real error messages and responses help identify whether the issue is with your mocks or the actual integration.\\n+\\n ### More examples\\n \\n-- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags  \\n-- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration  \\n+- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags\\n+- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration\\n - `examples/` ‚Äì MCP, Jira, and advanced configs\\n\",\"status\":\"modified\"},{\"filename\":\"package-lock.json\",\"additions\":252,\"deletions\":9,\"changes\":261,\"patch\":\"diff --git a/package-lock.json b/package-lock.json\\nindex 5fb3ad89..e5b10d27 100644\\n--- a/package-lock.json\\n+++ b/package-lock.json\\n@@ -7,6 +7,7 @@\\n     \\\"\\\": {\\n       \\\"name\\\": \\\"@probelabs/visor\\\",\\n       \\\"version\\\": \\\"0.1.42\\\",\\n+      \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"MIT\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@actions/core\\\": \\\"^1.11.1\\\",\\n@@ -16,7 +17,7 @@\\n         \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n         \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n         \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n         \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n         \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -54,6 +55,7 @@\\n         \\\"husky\\\": \\\"^9.1.7\\\",\\n         \\\"jest\\\": \\\"^30.1.3\\\",\\n         \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+        \\\"patch-package\\\": \\\"^8.0.1\\\",\\n         \\\"prettier\\\": \\\"^3.6.2\\\",\\n         \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n         \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n@@ -5942,9 +5944,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/maid\\\": {\\n-      \\\"version\\\": \\\"0.0.21\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.21.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-H3EA2tDjWsAPgXZAwfHrNoEXnHmQFS9+JGDK/YpeiF3g3FxPgnWBgtgYPgtNuETQsEXxR1RzDgQVJL2JHWRRHw==\\\",\\n+      \\\"version\\\": \\\"0.0.22\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.22.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-FYcyyP4BlVrHQmBznXFPpMbvUqFI6wbecHUUe4S+yIeDvJ8W4bP0ndZ0cMoc2p7hekWYAG4RtlohQBr8e4TKOg==\\\",\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@types/dagre\\\": \\\"^0.7.53\\\",\\n@@ -5960,9 +5962,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/probe\\\": {\\n-      \\\"version\\\": \\\"0.6.0-rc169\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc169.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-/laV3+1TSjASru8gGSvGwyjQ7jalMkFuZETl35uv37J1Hb4eG95CbKGU5NihcCrLXR44q1lKJiQGHXcAav2mBA==\\\",\\n+      \\\"version\\\": \\\"0.6.0-rc176\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc176.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-w/pAvnz2/yXb4FQXZV2QazxuW02xsI/wRg36yxZoLSG0O/hIncjF4rquZe3O+emceI2Ks5zbrA7cgQEdpwT8vA==\\\",\\n       \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n@@ -5972,7 +5974,7 @@\\n         \\\"@ai-sdk/openai\\\": \\\"^2.0.10\\\",\\n         \\\"@anthropic-ai/claude-agent-sdk\\\": \\\"^0.1.46\\\",\\n         \\\"@modelcontextprotocol/sdk\\\": \\\"^1.0.0\\\",\\n-        \\\"@probelabs/maid\\\": \\\"^0.0.21\\\",\\n+        \\\"@probelabs/maid\\\": \\\"^0.0.22\\\",\\n         \\\"adm-zip\\\": \\\"^0.5.16\\\",\\n         \\\"ai\\\": \\\"^5.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -8192,6 +8194,13 @@\\n         \\\"node\\\": \\\">= 20\\\"\\n       }\\n     },\\n+    \\\"node_modules/@yarnpkg/lockfile\\\": {\\n+      \\\"version\\\": \\\"1.1.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@yarnpkg/lockfile/-/lockfile-1.1.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-GpSwvyXOcOOlV70vbnzjj4fW5xW/FdUF6nQEt1ENy7m4ZCczi1+/buVUPAqmGfqznsORNFzUMjctTIp8a9tuCQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"BSD-2-Clause\\\"\\n+    },\\n     \\\"node_modules/accepts\\\": {\\n       \\\"version\\\": \\\"1.3.8\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/accepts/-/accepts-1.3.8.tgz\\\",\\n@@ -9180,6 +9189,25 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/call-bind\\\": {\\n+      \\\"version\\\": \\\"1.0.8\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind/-/call-bind-1.0.8.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-oKlSFMcMwpUg2ednkhQ454wfWiU/ul3CkJe/PEHcTKuiX6RpbehUiFMXu13HalGZxfUwCQzZG747YXBn1im9ww==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind-apply-helpers\\\": \\\"^1.0.0\\\",\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"set-function-length\\\": \\\"^1.2.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/call-bind-apply-helpers\\\": {\\n       \\\"version\\\": \\\"1.0.2\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind-apply-helpers/-/call-bind-apply-helpers-1.0.2.tgz\\\",\\n@@ -10613,6 +10641,24 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/define-data-property\\\": {\\n+      \\\"version\\\": \\\"1.1.4\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/define-data-property/-/define-data-property-1.1.4.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-rBMvIzlpA8v6E+SJZoo++HAYqsLrkg7MSfIinMPFhmkorw7X+dOXVJQs+QT69zGkzMyfDnIMN2Wid1+NbL3T+A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/define-lazy-prop\\\": {\\n       \\\"version\\\": \\\"3.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/define-lazy-prop/-/define-lazy-prop-3.0.0.tgz\\\",\\n@@ -11794,6 +11840,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/find-yarn-workspace-root\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/find-yarn-workspace-root/-/find-yarn-workspace-root-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-1IMnbjt4KzsQfnhnzNd8wUEgXZ44IzZaZmnLYx7D5FZlaHt2gW20Cri8Q+E/t5tIj4+epTBub+2Zxu/vNILzqQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Apache-2.0\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"micromatch\\\": \\\"^4.0.2\\\"\\n+      }\\n+    },\\n     \\\"node_modules/fix-dts-default-cjs-exports\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/fix-dts-default-cjs-exports/-/fix-dts-default-cjs-exports-1.0.1.tgz\\\",\\n@@ -12267,6 +12323,19 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/has-property-descriptors\\\": {\\n+      \\\"version\\\": \\\"1.0.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/has-property-descriptors/-/has-property-descriptors-1.0.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-55JNKuIW+vq4Ke1BjOTjM2YctQIvCT7GFzHwmfZPGo5wnrgkid0YQtnAleFSqumZm4az3n2BS+erby5ipJdgrg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/has-symbols\\\": {\\n       \\\"version\\\": \\\"1.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/has-symbols/-/has-symbols-1.1.0.tgz\\\",\\n@@ -12784,6 +12853,13 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/isarray\\\": {\\n+      \\\"version\\\": \\\"2.0.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/isarray/-/isarray-2.0.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-xHjhDr3cNBK0BzdUJSPXZntQUx/mwMS5Rw4A7lPJ90XGAO6ISP/ePDNuo0vhqOZU+UD5JoodwCAAoZQd3FeAKw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\"\\n+    },\\n     \\\"node_modules/isexe\\\": {\\n       \\\"version\\\": \\\"2.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz\\\",\\n@@ -13594,6 +13670,26 @@\\n       \\\"integrity\\\": \\\"sha512-NM8/P9n3XjXhIZn1lLhkFaACTOURQXjWhV4BA/RnOv8xvgqtqpAX9IO4mRQxSx1Rlo4tqzeqb0sOlruaOy3dug==\\\",\\n       \\\"license\\\": \\\"MIT\\\"\\n     },\\n+    \\\"node_modules/json-stable-stringify\\\": {\\n+      \\\"version\\\": \\\"1.3.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify/-/json-stable-stringify-1.3.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-qtYiSSFlwot9XHtF9bD9c7rwKjr+RecWT//ZnPvSmEjpV5mmPOCN4j8UjY5hbjNkOwZ/jQv3J6R1/pL7RwgMsg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind\\\": \\\"^1.0.8\\\",\\n+        \\\"call-bound\\\": \\\"^1.0.4\\\",\\n+        \\\"isarray\\\": \\\"^2.0.5\\\",\\n+        \\\"jsonify\\\": \\\"^0.0.1\\\",\\n+        \\\"object-keys\\\": \\\"^1.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/json-stable-stringify-without-jsonify\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify-without-jsonify/-/json-stable-stringify-without-jsonify-1.0.1.tgz\\\",\\n@@ -13633,6 +13729,16 @@\\n         \\\"graceful-fs\\\": \\\"^4.1.6\\\"\\n       }\\n     },\\n+    \\\"node_modules/jsonify\\\": {\\n+      \\\"version\\\": \\\"0.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/jsonify/-/jsonify-0.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-2/Ki0GcmuqSrgFyelQq9M05y7PS0mEwuIzrf3f1fPqkVDVRvZrPZtVSMHxdgo8Aq0sxAOb/cr2aqqA3LeWHVPg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Public Domain\\\",\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/katex\\\": {\\n       \\\"version\\\": \\\"0.16.25\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/katex/-/katex-0.16.25.tgz\\\",\\n@@ -13676,6 +13782,16 @@\\n       \\\"integrity\\\": \\\"sha512-Ls993zuzfayK269Svk9hzpeGUKob/sIgZzyHYdjQoAdQetRKpOLj+k/QQQ/6Qi0Yz65mlROrfd+Ev+1+7dz9Kw==\\\",\\n       \\\"dev\\\": true\\n     },\\n+    \\\"node_modules/klaw-sync\\\": {\\n+      \\\"version\\\": \\\"6.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/klaw-sync/-/klaw-sync-6.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-nIeuVSzdCCs6TDPTqI8w1Yre34sSq7AkZ4B3sfOBbI2CgVSB4Du4aLQijFU2+lhAFCwt9+42Hel6lQNIv6AntQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"graceful-fs\\\": \\\"^4.1.11\\\"\\n+      }\\n+    },\\n     \\\"node_modules/kolorist\\\": {\\n       \\\"version\\\": \\\"1.8.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/kolorist/-/kolorist-1.8.0.tgz\\\",\\n@@ -14785,6 +14901,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n       }\\n     },\\n+    \\\"node_modules/object-keys\\\": {\\n+      \\\"version\\\": \\\"1.1.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/object-keys/-/object-keys-1.1.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NuAESUOUMrlIXOfHKzD6bpPu3tYt3xvjNdRIQ+FeT0lNb4K8WR70CaDxhuNguS2XG+GjkyMwOzsN5ZktImfhLA==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/ohash\\\": {\\n       \\\"version\\\": \\\"2.0.11\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/ohash/-/ohash-2.0.11.tgz\\\",\\n@@ -15022,6 +15148,95 @@\\n         \\\"node\\\": \\\">= 0.8\\\"\\n       }\\n     },\\n+    \\\"node_modules/patch-package\\\": {\\n+      \\\"version\\\": \\\"8.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/patch-package/-/patch-package-8.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-VsKRIA8f5uqHQ7NGhwIna6Bx6D9s/1iXlA1hthBVBEbkq+t4kXD0HHt+rJhf/Z+Ci0F/HCB2hvn0qLdLG+Qxlw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"@yarnpkg/lockfile\\\": \\\"^1.1.0\\\",\\n+        \\\"chalk\\\": \\\"^4.1.2\\\",\\n+        \\\"ci-info\\\": \\\"^3.7.0\\\",\\n+        \\\"cross-spawn\\\": \\\"^7.0.3\\\",\\n+        \\\"find-yarn-workspace-root\\\": \\\"^2.0.0\\\",\\n+        \\\"fs-extra\\\": \\\"^10.0.0\\\",\\n+        \\\"json-stable-stringify\\\": \\\"^1.0.2\\\",\\n+        \\\"klaw-sync\\\": \\\"^6.0.0\\\",\\n+        \\\"minimist\\\": \\\"^1.2.6\\\",\\n+        \\\"open\\\": \\\"^7.4.2\\\",\\n+        \\\"semver\\\": \\\"^7.5.3\\\",\\n+        \\\"slash\\\": \\\"^2.0.0\\\",\\n+        \\\"tmp\\\": \\\"^0.2.4\\\",\\n+        \\\"yaml\\\": \\\"^2.2.2\\\"\\n+      },\\n+      \\\"bin\\\": {\\n+        \\\"patch-package\\\": \\\"index.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14\\\",\\n+        \\\"npm\\\": \\\">5\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/ci-info\\\": {\\n+      \\\"version\\\": \\\"3.9.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/ci-info/-/ci-info-3.9.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NIxF55hv4nSqQswkAeiOi1r83xy8JldOFDTWiug55KBu9Jnblncd2U6ViHmYgHf01TPZS77NJBhBMKdWj9HQMQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"funding\\\": [\\n+        {\\n+          \\\"type\\\": \\\"github\\\",\\n+          \\\"url\\\": \\\"https://github.com/sponsors/sibiraj-s\\\"\\n+        }\\n+      ],\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/is-docker\\\": {\\n+      \\\"version\\\": \\\"2.2.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/is-docker/-/is-docker-2.2.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-F+i2BKsFrH66iaUFc0woD8sLy8getkwTwtOBjvs56Cx4CgJDeKQeqfz8wAYiSb8JOprWhHH5p77PbmYCvvUuXQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"bin\\\": {\\n+        \\\"is-docker\\\": \\\"cli.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/open\\\": {\\n+      \\\"version\\\": \\\"7.4.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/open/-/open-7.4.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-MVHddDVweXZF3awtlAS+6pgKLlm/JgxZ90+/NBurBoQctVOOB/zDdVjcyPzQ+0laDGbsWgrRkflI65sQeOgT9Q==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"is-docker\\\": \\\"^2.0.0\\\",\\n+        \\\"is-wsl\\\": \\\"^2.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/slash\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/slash/-/slash-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-ZYKh3Wh2z1PpEXWr0MpSBZ0V6mZHAQfYevttO11c51CaWjGTaadiKZ+wVt1PbMlDV5qhMFslpZCemhwOK7C89A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=6\\\"\\n+      }\\n+    },\\n     \\\"node_modules/path-data-parser\\\": {\\n       \\\"version\\\": \\\"0.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/path-data-parser/-/path-data-parser-0.1.0.tgz\\\",\\n@@ -16473,6 +16688,24 @@\\n         \\\"node\\\": \\\">= 0.8.0\\\"\\n       }\\n     },\\n+    \\\"node_modules/set-function-length\\\": {\\n+      \\\"version\\\": \\\"1.2.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/set-function-length/-/set-function-length-1.2.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-pgRc4hJ4/sNjWCSS9AmnS40x3bNMDTknHgL5UaMBTMyJnU90EgWh1Rz+MC9eFu4BuN/UwZjKQuY/1v3rM7HMfg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"define-data-property\\\": \\\"^1.1.4\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"function-bind\\\": \\\"^1.1.2\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\",\\n+        \\\"has-property-descriptors\\\": \\\"^1.0.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/setprototypeof\\\": {\\n       \\\"version\\\": \\\"1.2.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/setprototypeof/-/setprototypeof-1.2.0.tgz\\\",\\n@@ -17384,6 +17617,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/tmp\\\": {\\n+      \\\"version\\\": \\\"0.2.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/tmp/-/tmp-0.2.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-voyz6MApa1rQGUxT3E+BK7/ROe8itEx7vD8/HEvt4xwXucvQ5G5oeEiHkmHZJuBO21RpOf+YYm9MOivj709jow==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14.14\\\"\\n+      }\\n+    },\\n     \\\"node_modules/tmpl\\\": {\\n       \\\"version\\\": \\\"1.0.5\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/tmpl/-/tmpl-1.0.5.tgz\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"package.json\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/package.json b/package.json\\nindex f9ccbb6c..302d728f 100644\\n--- a/package.json\\n+++ b/package.json\\n@@ -49,6 +49,7 @@\\n     \\\"test:with-build\\\": \\\"npm run build:cli && jest\\\",\\n     \\\"test:yaml\\\": \\\"node dist/index.js test --progress compact\\\",\\n     \\\"prepare\\\": \\\"husky\\\",\\n+    \\\"postinstall\\\": \\\"patch-package\\\",\\n     \\\"pre-commit\\\": \\\"lint-staged\\\",\\n     \\\"deploy:site\\\": \\\"cd site && npx wrangler pages deploy . --project-name=visor-site --commit-dirty=true\\\",\\n     \\\"deploy:worker\\\": \\\"npx wrangler deploy\\\",\\n@@ -96,7 +97,7 @@\\n     \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n     \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n     \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n     \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n     \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n     \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -143,6 +144,7 @@\\n     \\\"husky\\\": \\\"^9.1.7\\\",\\n     \\\"jest\\\": \\\"^30.1.3\\\",\\n     \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+    \\\"patch-package\\\": \\\"^8.0.1\\\",\\n     \\\"prettier\\\": \\\"^3.6.2\\\",\\n     \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n     \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"src/ai-review-service.ts\",\"additions\":101,\"deletions\":151,\"changes\":252,\"patch\":\"diff --git a/src/ai-review-service.ts b/src/ai-review-service.ts\\nindex 6b9c5b2e..f18accea 100644\\n--- a/src/ai-review-service.ts\\n+++ b/src/ai-review-service.ts\\n@@ -65,6 +65,13 @@ export interface AIReviewConfig {\\n   allowBash?: boolean;\\n   // Advanced bash command execution configuration\\n   bashConfig?: import('./types/config').BashConfig;\\n+  // Optional workspace root and allowed folders for ProbeAgent.\\n+  // When provided, these are forwarded to ProbeAgent so tools like search/query\\n+  // operate inside the isolated workspace/projects instead of the Visor repo root.\\n+  path?: string;\\n+  allowedFolders?: string[];\\n+  // Completion prompt for post-completion validation/review (runs after attempt_completion)\\n+  completionPrompt?: string;\\n }\\n \\n export interface AIDebugInfo {\\n@@ -278,7 +285,7 @@ export class AIReviewService {\\n     try {\\n       const call = this.callProbeAgent(prompt, schema, debugInfo, checkName, sessionId);\\n       const timeoutMs = Math.max(0, this.config.timeout || 0);\\n-      const { response, effectiveSchema } =\\n+      const { response, effectiveSchema, sessionId: usedSessionId } =\\n         timeoutMs > 0 ? await this.withTimeout(call, timeoutMs, 'AI review') : await call;\\n       const processingTime = Date.now() - startTime;\\n \\n@@ -290,6 +297,11 @@ export class AIReviewService {\\n \\n       const result = this.parseAIResponse(response, debugInfo, effectiveSchema);\\n \\n+      // Expose the session ID used for this call so the engine can reuse it later\\n+      try {\\n+        (result as any).sessionId = usedSessionId;\\n+      } catch {}\\n+\\n       if (debugInfo) {\\n         result.debug = debugInfo;\\n       }\\n@@ -651,17 +663,34 @@ ${prContext}${slackContextXml}\\n     const prContextInfo = prInfo as PRInfo & {\\n       isPRContext?: boolean;\\n       includeCodeContext?: boolean;\\n+      slackConversation?: unknown;\\n     };\\n     const isIssue = prContextInfo.isIssue === true;\\n \\n     // Check if we should include code context (diffs)\\n     const isPRContext = prContextInfo.isPRContext === true;\\n-    // In PR context, always include diffs. Otherwise check the flag.\\n-    const includeCodeContext = isPRContext || prContextInfo.includeCodeContext !== false;\\n+    const isSlackMode = prContextInfo.slackConversation !== undefined;\\n+\\n+    // Determine whether to include code context:\\n+    // - In explicit PR context (GitHub PR events), always include diffs\\n+    // - In Slack mode, default to NO code context unless explicitly requested\\n+    // - Otherwise, include code context unless explicitly disabled\\n+    let includeCodeContext: boolean;\\n+    if (isPRContext) {\\n+      includeCodeContext = true;\\n+    } else if (isSlackMode) {\\n+      // In Slack mode, only include code context if explicitly set to true\\n+      includeCodeContext = prContextInfo.includeCodeContext === true;\\n+    } else {\\n+      // Default: include unless explicitly disabled\\n+      includeCodeContext = prContextInfo.includeCodeContext !== false;\\n+    }\\n \\n     // Log the decision for transparency (debug level)\\n     if (isPRContext) {\\n       log('üîç Including full code diffs in AI context (PR mode)');\\n+    } else if (isSlackMode && !includeCodeContext) {\\n+      log('üí¨ Slack mode: excluding code diffs (use includeCodeContext: true to enable)');\\n     } else if (!includeCodeContext) {\\n       log('üìä Including only file summary in AI context (no diffs)');\\n     } else {\\n@@ -1537,7 +1566,15 @@ ${'='.repeat(60)}\\n     debugInfo?: AIDebugInfo,\\n     _checkName?: string,\\n     providedSessionId?: string\\n-  ): Promise<{ response: string; effectiveSchema?: string }> {\\n+  ): Promise<{ response: string; effectiveSchema?: string; sessionId: string }> {\\n+    // Derive a stable session ID for this call so the engine can reuse it later\\n+    const sessionId =\\n+      providedSessionId ||\\n+      (() => {\\n+        const timestamp = new Date().toISOString();\\n+        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n+      })();\\n+\\n     // Handle mock model/provider\\n     if (this.config.model === 'mock' || this.config.provider === 'mock') {\\n       const inJest = !!process.env.JEST_WORKER_ID;\\n@@ -1545,19 +1582,17 @@ ${'='.repeat(60)}\\n       if (!inJest) {\\n         // Fast path for CLI/integration: synthesize a mock response without invoking ProbeAgent\\n         const response = await this.generateMockResponse(prompt, _checkName, schema);\\n-        return { response, effectiveSchema: typeof schema === 'object' ? 'custom' : schema };\\n+        return {\\n+          response,\\n+          effectiveSchema: typeof schema === 'object' ? 'custom' : schema,\\n+          sessionId,\\n+        };\\n       }\\n       // In unit tests, still invoke ProbeAgent so tests can assert on options (schema) passed in\\n       // Fall through to normal flow below\\n     }\\n \\n     // Create ProbeAgent instance with proper options\\n-    const sessionId =\\n-      providedSessionId ||\\n-      (() => {\\n-        const timestamp = new Date().toISOString();\\n-        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n-      })();\\n \\n     log('ü§ñ Creating ProbeAgent for AI review...');\\n     log(`üÜî Session ID: ${sessionId}`);\\n@@ -1674,6 +1709,35 @@ ${'='.repeat(60)}\\n         (options as any).bashConfig = this.config.bashConfig;\\n       }\\n \\n+      // Pass completion prompt for post-completion validation/review\\n+      if (this.config.completionPrompt !== undefined) {\\n+        (options as any).completionPrompt = this.config.completionPrompt;\\n+      }\\n+\\n+      // Propagate workspace / allowed folders to ProbeAgent so that tools\\n+      // operate inside the isolated workspace and project checkouts instead\\n+      // of the Visor repository root.\\n+      try {\\n+        const cfgAny: any = this.config as any;\\n+        const allowedFolders = cfgAny.allowedFolders as string[] | undefined;\\n+        const workspacePath =\\n+          cfgAny.workspacePath || cfgAny.path || (Array.isArray(allowedFolders) && allowedFolders[0]);\\n+        if (Array.isArray(allowedFolders) && allowedFolders.length > 0) {\\n+          (options as any).allowedFolders = allowedFolders;\\n+          if (!options.path && workspacePath) {\\n+            (options as any).path = workspacePath;\\n+          }\\n+          log(`üóÇÔ∏è ProbeAgent workspace config:`);\\n+          log(`   path (cwd): ${(options as any).path}`);\\n+          log(`   allowedFolders[0]: ${allowedFolders[0]}`);\\n+        } else if (workspacePath) {\\n+          (options as any).path = workspacePath;\\n+          log(`üóÇÔ∏è ProbeAgent path: ${workspacePath} (no allowedFolders)`);\\n+        }\\n+      } catch {\\n+        // Best-effort only; fall back to ProbeAgent defaults on error.\\n+      }\\n+\\n       // Add provider-specific options if configured\\n       if (this.config.provider) {\\n         // Map claude-code to anthropic for ProbeAgent compatibility\\n@@ -2037,7 +2101,7 @@ ${'='.repeat(60)}\\n         log(`üîß Debug: Registered AI session for potential reuse: ${sessionId}`);\\n       }\\n \\n-      return { response, effectiveSchema };\\n+      return { response, effectiveSchema, sessionId };\\n     } catch (error) {\\n       console.error('‚ùå ProbeAgent failed:', error);\\n       throw new Error(\\n@@ -2191,13 +2255,23 @@ ${'='.repeat(60)}\\n         // For other schemas (code-review, etc.), extract and parse JSON with boundary detection\\n         log('üîç Extracting JSON from AI response...');\\n \\n-        // Try direct parsing first - if AI returned pure JSON\\n+        // Sanitize response: strip BOM, zero-width chars, and other invisible characters\\n+        // that can cause JSON parsing to fail even when the text looks valid\\n+        const sanitizedResponse = response\\n+          .replace(/^\\\\uFEFF/, '') // BOM\\n+          .replace(/[\\\\u200B-\\\\u200D\\\\uFEFF\\\\u00A0]/g, '') // Zero-width chars, NBSP\\n+          .replace(/[\\\\x00-\\\\x08\\\\x0B\\\\x0C\\\\x0E-\\\\x1F\\\\x7F]/g, '') // Control chars (except \\\\t \\\\n \\\\r)\\n+          .trim();\\n+\\n+        // Try direct JSON parsing - no bracket-matching extraction\\n+        // JSON validation is offloaded to Probe agent when schema is provided\\n         try {\\n-          reviewData = JSON.parse(response.trim());\\n+          reviewData = JSON.parse(sanitizedResponse);\\n           log('‚úÖ Successfully parsed direct JSON response');\\n           if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-        } catch {\\n-          log('üîç Direct parsing failed, trying to extract JSON from response...');\\n+        } catch (parseErr) {\\n+          const errMsg = parseErr instanceof Error ? parseErr.message : String(parseErr);\\n+          log(`üîç Direct JSON parsing failed: ${errMsg}`);\\n \\n           // If the response starts with \\\"I cannot\\\" or similar, it's likely a refusal\\n           if (\\n@@ -2210,66 +2284,16 @@ ${'='.repeat(60)}\\n             };\\n           }\\n \\n-          // Try to extract JSON using improved method with proper bracket matching\\n-          const jsonString = this.extractJsonFromResponse(response);\\n-\\n-          if (jsonString) {\\n-            try {\\n-              reviewData = JSON.parse(jsonString);\\n-              log('‚úÖ Successfully parsed extracted JSON');\\n-              if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-            } catch {\\n-              log('üîß Extracted JSON parsing failed, falling back to plain text handling...');\\n-\\n-              // Check if response is plain text and doesn't contain structured data\\n-              if (!response.includes('{') && !response.includes('}')) {\\n-                log('üîß Plain text response detected, creating structured fallback...');\\n-\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              } else {\\n-                // Fallback: treat the entire response as an issue\\n-                log('üîß Creating fallback response from non-JSON content...');\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              }\\n-            }\\n-          } else {\\n-            // No JSON found at all - treat as plain text response\\n-            log('üîß No JSON found in response, treating as plain text...');\\n-            reviewData = {\\n-              issues: [\\n-                {\\n-                  file: 'AI_RESPONSE',\\n-                  line: 1,\\n-                  ruleId: 'ai/raw_response',\\n-                  message: response,\\n-                  severity: 'info',\\n-                  category: 'documentation',\\n-                },\\n-              ],\\n-            };\\n-          }\\n+          // Not valid JSON - treat entire response as text output\\n+          // This allows Probe (or other AI providers) to handle JSON validation\\n+          // and avoids false positives from bracket-matching (e.g., mermaid diagrams)\\n+          log('üîß Treating response as plain text (no JSON extraction)');\\n+          const trimmed = response.trim();\\n+          return {\\n+            issues: [],\\n+            output: { text: trimmed },\\n+            debug: debugInfo,\\n+          };\\n         }\\n       }\\n \\n@@ -2457,80 +2481,6 @@ ${'='.repeat(60)}\\n     }\\n   }\\n \\n-  /**\\n-   * Extract JSON from a response that might contain surrounding text\\n-   * Uses proper bracket matching to find valid JSON objects or arrays\\n-   */\\n-  private extractJsonFromResponse(response: string): string | null {\\n-    const text = response.trim();\\n-\\n-    // Try to find JSON objects first (higher priority)\\n-    let bestJson = this.findJsonWithBracketMatching(text, '{', '}');\\n-\\n-    // If no object found, try arrays\\n-    if (!bestJson) {\\n-      bestJson = this.findJsonWithBracketMatching(text, '[', ']');\\n-    }\\n-\\n-    return bestJson;\\n-  }\\n-\\n-  /**\\n-   * Find JSON with proper bracket matching to avoid false positives\\n-   */\\n-  private findJsonWithBracketMatching(\\n-    text: string,\\n-    openChar: string,\\n-    closeChar: string\\n-  ): string | null {\\n-    const firstIndex = text.indexOf(openChar);\\n-    if (firstIndex === -1) return null;\\n-\\n-    let depth = 0;\\n-    let inString = false;\\n-    let escaping = false;\\n-\\n-    for (let i = firstIndex; i < text.length; i++) {\\n-      const char = text[i];\\n-\\n-      if (escaping) {\\n-        escaping = false;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\\\\\\' && inString) {\\n-        escaping = true;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\"' && !escaping) {\\n-        inString = !inString;\\n-        continue;\\n-      }\\n-\\n-      if (!inString) {\\n-        if (char === openChar) {\\n-          depth++;\\n-        } else if (char === closeChar) {\\n-          depth--;\\n-          if (depth === 0) {\\n-            // Found matching closing bracket\\n-            const candidate = text.substring(firstIndex, i + 1);\\n-            try {\\n-              JSON.parse(candidate); // Validate it's actually valid JSON\\n-              return candidate;\\n-            } catch {\\n-              // This wasn't valid JSON, keep looking\\n-              break;\\n-            }\\n-          }\\n-        }\\n-      }\\n-    }\\n-\\n-    return null;\\n-  }\\n-\\n   /**\\n    * Generate mock response for testing\\n    */\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli-main.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/src/cli-main.ts b/src/cli-main.ts\\nindex 7dbef501..e671fbd8 100644\\n--- a/src/cli-main.ts\\n+++ b/src/cli-main.ts\\n@@ -158,6 +158,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n   }\\n   const only = getArg('--only');\\n   const bail = hasFlag('--bail');\\n+  const noMocks = hasFlag('--no-mocks');\\n   const listOnly = hasFlag('--list');\\n   const validateOnly = hasFlag('--validate');\\n   const progress = (getArg('--progress') as 'compact' | 'detailed' | undefined) || 'compact';\\n@@ -224,6 +225,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       const agg = await runSuites(multiFiles, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallelSuites: maxParallelSuites || Math.max(1, require('os').cpus()?.length || 2),\\n         maxParallel,\\n         promptMaxChars,\\n@@ -340,6 +342,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       runRes = await runner.runCases(tpath, suite, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallel,\\n         promptMaxChars,\\n         engineMode: 'state-machine',\\n@@ -609,6 +612,9 @@ export async function main(): Promise<void> {\\n     // Set environment variables early for proper logging in all modules\\n     process.env.VISOR_OUTPUT_FORMAT = options.output;\\n     process.env.VISOR_DEBUG = options.debug ? 'true' : 'false';\\n+    if (options.keepWorkspace) {\\n+      process.env.VISOR_KEEP_WORKSPACE = 'true';\\n+    }\\n     // Configure centralized logger\\n     configureLoggerFromCli({\\n       output: options.output,\\n@@ -716,6 +722,31 @@ export async function main(): Promise<void> {\\n       const threads = slackAny.threads || 'any';\\n       const allow = Array.isArray(slackAny.channel_allowlist) ? slackAny.channel_allowlist : [];\\n       const appToken = slackAny.app_token || process.env.SLACK_APP_TOKEN;\\n+\\n+      // Initialize telemetry for Slack mode (normally done later for CLI runs).\\n+      if ((config as any)?.telemetry) {\\n+        const t = (config as any).telemetry as {\\n+          enabled?: boolean;\\n+          sink?: 'otlp' | 'file' | 'console';\\n+          file?: { dir?: string; ndjson?: boolean };\\n+          tracing?: { auto_instrumentations?: boolean; trace_report?: { enabled?: boolean } };\\n+        };\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true' || !!t?.enabled,\\n+          sink:\\n+            (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || t?.sink || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR || t?.file?.dir, ndjson: !!t?.file?.ndjson },\\n+          autoInstrument: !!t?.tracing?.auto_instrumentations,\\n+          traceReport: !!t?.tracing?.trace_report?.enabled,\\n+        });\\n+      } else {\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true',\\n+          sink: (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR },\\n+        });\\n+      }\\n+\\n       const runner = new SlackSocketRunner(engine, config, {\\n         appToken,\\n         endpoint,\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli.ts\",\"additions\":5,\"deletions\":1,\"changes\":6,\"patch\":\"diff --git a/src/cli.ts b/src/cli.ts\\nindex 134429b2..b1c46213 100644\\n--- a/src/cli.ts\\n+++ b/src/cli.ts\\n@@ -76,6 +76,7 @@ export class CLI {\\n         parseInt(value, 10)\\n       )\\n       .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText())\\n       .exitOverride(); // Prevent automatic process.exit for better error handling\\n \\n@@ -154,6 +155,7 @@ export class CLI {\\n           parseInt(value, 10)\\n         )\\n         .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+        .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n         .allowUnknownOption(false)\\n         .allowExcessArguments(false) // Don't allow positional arguments\\n         .addHelpText('after', this.getExamplesText())\\n@@ -224,6 +226,7 @@ export class CLI {\\n         message: options.message,\\n         githubV2: false,\\n         slack: Boolean(options.slack),\\n+        keepWorkspace: Boolean(options.keepWorkspace),\\n       };\\n     } catch (error: unknown) {\\n       // Handle commander.js exit overrides for help/version ONLY\\n@@ -348,6 +351,7 @@ export class CLI {\\n       .option('--debug-port <port>', 'Port for debug server (default: 3456)', value =>\\n         parseInt(value, 10)\\n       )\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText());\\n \\n     // Get the basic help and append examples manually if addHelpText doesn't work\\n\",\"status\":\"modified\"},{\"filename\":\"src/config.ts\",\"additions\":67,\"deletions\":17,\"changes\":84,\"patch\":\"diff --git a/src/config.ts b/src/config.ts\\nindex 407efa2d..02abe9df 100644\\n--- a/src/config.ts\\n+++ b/src/config.ts\\n@@ -20,6 +20,7 @@ import { ConfigLoader, ConfigLoaderOptions } from './utils/config-loader';\\n import { ConfigMerger } from './utils/config-merger';\\n import Ajv from 'ajv';\\n import addFormats from 'ajv-formats';\\n+import { validateJsSyntax } from './utils/sandbox';\\n \\n /**\\n  * Valid event triggers for checks\\n@@ -477,16 +478,29 @@ export class ConfigManager {\\n \\n     logger.info(`Registered workflow '${workflowId}' for standalone execution`);\\n \\n-    // Create a COMPLETELY NEW visor config with ONLY the test checks\\n-    // This prevents any workflow fields from leaking into the config\\n-    const visorConfig: Partial<VisorConfig> = {\\n+    // For standalone workflow testing, use the workflow's steps as checks\\n+    // This allows the test runner to execute the workflow steps directly\\n+    const workflowSteps = workflowData.steps || {};\\n+\\n+    // Create a config that includes the workflow steps as checks,\\n+    // plus the tests section and workflow metadata for output computation\\n+    const visorConfig: Partial<VisorConfig> & { tests?: any; outputs?: any; inputs?: any } = {\\n       version: '1.0',\\n-      steps: tests,\\n-      checks: tests, // Backward compatibility\\n+      steps: workflowSteps,\\n+      checks: workflowSteps,\\n+      tests: tests, // Preserve test harness config (may be empty if stripped by test runner)\\n     };\\n \\n-    logger.debug(`Standalone workflow config has ${Object.keys(tests).length} test checks`);\\n-    logger.debug(`Test check names: ${Object.keys(tests).join(', ')}`);\\n+    // Preserve workflow metadata for output computation during tests\\n+    if (workflowData.outputs) {\\n+      visorConfig.outputs = workflowData.outputs;\\n+    }\\n+    if (workflowData.inputs) {\\n+      visorConfig.inputs = workflowData.inputs;\\n+    }\\n+\\n+    logger.debug(`Standalone workflow config has ${Object.keys(workflowSteps).length} workflow steps as checks`);\\n+    logger.debug(`Workflow step names: ${Object.keys(workflowSteps).join(', ')}`);\\n     logger.debug(`Config keys after conversion: ${Object.keys(visorConfig).join(', ')}`);\\n \\n     return visorConfig;\\n@@ -982,26 +996,29 @@ export class ConfigManager {\\n \\n     // Validate reuse_ai_session configuration\\n     if (checkConfig.reuse_ai_session !== undefined) {\\n-      const isString = typeof checkConfig.reuse_ai_session === 'string';\\n-      const isBoolean = typeof checkConfig.reuse_ai_session === 'boolean';\\n+      const reuseValue = checkConfig.reuse_ai_session as unknown;\\n+      const isString = typeof reuseValue === 'string';\\n+      const isBoolean = typeof reuseValue === 'boolean';\\n+      const isSelf = reuseValue === 'self';\\n \\n       if (!isString && !isBoolean) {\\n         errors.push({\\n           field: `checks.${checkName}.reuse_ai_session`,\\n           message: `Invalid reuse_ai_session value for \\\"${checkName}\\\": must be string (check name) or boolean`,\\n-          value: checkConfig.reuse_ai_session,\\n+          value: reuseValue,\\n         });\\n-      } else if (isString) {\\n-        // When reuse_ai_session is a string, it must refer to a valid check\\n-        const targetCheckName = checkConfig.reuse_ai_session as string;\\n+      } else if (isString && !isSelf) {\\n+        // When reuse_ai_session is a string (other than the special 'self' value),\\n+        // it must refer to a valid check name\\n+        const targetCheckName = reuseValue as string;\\n         if (!config?.checks || !config.checks[targetCheckName]) {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" references non-existent check \\\"${targetCheckName}\\\" for session reuse`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n-      } else if (checkConfig.reuse_ai_session === true) {\\n+      } else if (reuseValue === true) {\\n         // When reuse_ai_session is true, depends_on must be specified and non-empty\\n         if (\\n           !checkConfig.depends_on ||\\n@@ -1011,7 +1028,7 @@ export class ConfigManager {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" has reuse_ai_session=true but missing or empty depends_on. Session reuse requires dependency on another check.`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n       }\\n@@ -1076,6 +1093,39 @@ export class ConfigManager {\\n         });\\n       }\\n     }\\n+\\n+    // Validate JavaScript syntax in transform_js and script content\\n+    try {\\n+      // Validate transform_js if present\\n+      const transformJs = (checkConfig as any).transform_js;\\n+      if (typeof transformJs === 'string' && transformJs.trim().length > 0) {\\n+        const result = validateJsSyntax(transformJs);\\n+        if (!result.valid) {\\n+          errors.push({\\n+            field: `checks.${checkName}.transform_js`,\\n+            message: `JavaScript syntax error in \\\"${checkName}\\\" transform_js: ${result.error}`,\\n+            value: transformJs.slice(0, 100) + (transformJs.length > 100 ? '...' : ''),\\n+          });\\n+        }\\n+      }\\n+\\n+      // Validate script content if type is 'script'\\n+      if (checkConfig.type === 'script') {\\n+        const content = (checkConfig as any).content;\\n+        if (typeof content === 'string' && content.trim().length > 0) {\\n+          const result = validateJsSyntax(content);\\n+          if (!result.valid) {\\n+            errors.push({\\n+              field: `checks.${checkName}.content`,\\n+              message: `JavaScript syntax error in \\\"${checkName}\\\" script: ${result.error}`,\\n+              value: content.slice(0, 100) + (content.length > 100 ? '...' : ''),\\n+            });\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Syntax validation is best-effort; don't fail the whole config on validation errors\\n+    }\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/failure-condition-evaluator.ts\",\"additions\":25,\"deletions\":9,\"changes\":34,\"patch\":\"diff --git a/src/failure-condition-evaluator.ts b/src/failure-condition-evaluator.ts\\nindex 3b54a015..5800df0a 100644\\n--- a/src/failure-condition-evaluator.ts\\n+++ b/src/failure-condition-evaluator.ts\\n@@ -137,6 +137,8 @@ export class FailureConditionEvaluator {\\n       previousResults?: Map<string, ReviewSummary>;\\n       authorAssociation?: string;\\n       workflowInputs?: Record<string, unknown>;\\n+      /** Current step's output for guarantee evaluation */\\n+      output?: unknown;\\n     }\\n   ): Promise<boolean> {\\n     // Build context for if evaluation\\n@@ -177,10 +179,10 @@ export class FailureConditionEvaluator {\\n       // Workflow inputs (for workflows)\\n       inputs: contextData?.workflowInputs || {},\\n \\n-      // Required output property (empty for if conditions)\\n-      output: {\\n-        issues: [],\\n-      },\\n+      // Output property: use provided output for guarantee evaluation, or empty for if conditions\\n+      output: contextData?.output !== undefined && contextData.output !== null && typeof contextData.output === 'object'\\n+        ? (contextData.output as Record<string, unknown>)\\n+        : { issues: [] },\\n       // Author association (used by permission helpers)\\n       authorAssociation: contextData?.authorAssociation,\\n \\n@@ -213,10 +215,10 @@ export class FailureConditionEvaluator {\\n               m && typeof m.get === 'function' ? m.get('all_valid', 'fact-validation') : undefined;\\n             memStr = ` mem.fact-validation.all_valid=${String(v)}`;\\n           } catch {}\\n+          // Debug if-eval output (only when VISOR_DEBUG enabled)\\n+          const outputKeys = Object.keys(context.outputs || {});\\n           console.error(\\n-            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" env.ENABLE_FACT_VALIDATION=${String(\\n-              (envMap as any).ENABLE_FACT_VALIDATION\\n-            )} event=${context.event?.event_name} result=${String(res)}${memStr}`\\n+            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" result=${String(res)} outputKeys=[${outputKeys.join(',')}]`\\n           );\\n         }\\n       } catch {}\\n@@ -480,7 +482,18 @@ export class FailureConditionEvaluator {\\n \\n       // Extract context variables\\n       const output = context.output || {};\\n-      const issues = output.issues || [];\\n+      // Ensure issues is an array - it might be a JSON string from workflow outputs\\n+      let issues = output.issues || [];\\n+      if (typeof issues === 'string') {\\n+        try {\\n+          issues = JSON.parse(issues);\\n+        } catch {\\n+          issues = [];\\n+        }\\n+      }\\n+      if (!Array.isArray(issues)) {\\n+        issues = [];\\n+      }\\n \\n       // Backward compatibility: provide metadata for transition period\\n       // TODO: Remove after all configurations are updated\\n@@ -518,6 +531,7 @@ export class FailureConditionEvaluator {\\n       const event = context.event || 'manual';\\n       const env = context.env || {};\\n       const outputs = context.outputs || {};\\n+      const inputs = context.inputs || {};\\n       const debugData = context.debug || null;\\n \\n       // Get memory store and create accessor for fail_if expressions\\n@@ -555,6 +569,7 @@ export class FailureConditionEvaluator {\\n         filesCount,\\n         event,\\n         env,\\n+        inputs,\\n         // Helper functions\\n         contains,\\n         startsWith,\\n@@ -621,6 +636,7 @@ export class FailureConditionEvaluator {\\n             filesCount,\\n             event,\\n             env,\\n+            inputs,\\n             // Helpers\\n             contains,\\n             startsWith,\\n\",\"status\":\"modified\"},{\"filename\":\"src/frontends/slack-frontend.ts\",\"additions\":107,\"deletions\":6,\"changes\":113,\"patch\":\"diff --git a/src/frontends/slack-frontend.ts b/src/frontends/slack-frontend.ts\\nindex 97161a7d..41735aa2 100644\\n--- a/src/frontends/slack-frontend.ts\\n+++ b/src/frontends/slack-frontend.ts\\n@@ -1,6 +1,32 @@\\n+/**\\n+ * Slack Frontend for Visor workflows.\\n+ *\\n+ * Features:\\n+ * - Posts AI replies to Slack threads\\n+ * - Converts Markdown to Slack mrkdwn format\\n+ * - Renders mermaid diagrams to PNG and uploads as images\\n+ * - Manages üëÄ/üëç reactions for acknowledgement\\n+ * - Handles human input prompts via prompt-state\\n+ *\\n+ * Mermaid Diagram Rendering:\\n+ * - Detects ```mermaid code blocks in AI responses\\n+ * - Renders to PNG using @mermaid-js/mermaid-cli (mmdc)\\n+ * - Uploads rendered images to Slack thread\\n+ * - Replaces mermaid blocks with \\\"_(See diagram above)_\\\" placeholder\\n+ *\\n+ * Requirements for mermaid rendering:\\n+ * - Node.js and npx in PATH\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ * - On Linux: apt-get install chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ */\\n import type { Frontend, FrontendContext } from './host';\\n import { SlackClient } from '../slack/client';\\n-import { formatSlackText } from '../slack/markdown';\\n+import {\\n+  formatSlackText,\\n+  extractMermaidDiagrams,\\n+  renderMermaidToPng,\\n+  replaceMermaidBlocks,\\n+} from '../slack/markdown';\\n \\n type SlackFrontendConfig = {\\n   defaultChannel?: string;\\n@@ -284,6 +310,9 @@ export class SlackFrontend implements Frontend {\\n       const isLogChat = providerType === 'log' && checkCfg.group === 'chat';\\n       if (!isAi && !isLogChat) return;\\n \\n+      // Skip internal steps - they're intermediate processing and shouldn't post to Slack\\n+      if (checkCfg.criticality === 'internal') return;\\n+\\n       // For AI checks, only post when using simple/unstructured schemas (or none).\\n       if (isAi) {\\n         const schema = checkCfg.schema;\\n@@ -316,6 +345,13 @@ export class SlackFrontend implements Frontend {\\n         ) {\\n           text = (result as any).content.trim();\\n         }\\n+      } else if (isLogChat && typeof (result as any)?.logOutput === 'string') {\\n+        // For log-based chat checks, render the formatted log output as the\\n+        // Slack message when no structured text field is present.\\n+        const raw = (result as any).logOutput;\\n+        if (raw.trim().length > 0) {\\n+          text = raw.trim();\\n+        }\\n       } else if (isAi && showRawOutput && out !== undefined) {\\n         try {\\n           text = JSON.stringify(out, null, 2);\\n@@ -325,13 +361,78 @@ export class SlackFrontend implements Frontend {\\n       }\\n       if (!text) return;\\n \\n-      const formattedText = formatSlackText(text);\\n+      // Extract and render mermaid diagrams before posting\\n+      const diagrams = extractMermaidDiagrams(text);\\n+      let processedText = text;\\n+\\n+      if (diagrams.length > 0) {\\n+        try {\\n+          ctx.logger.info(\\n+            `[slack-frontend] found ${diagrams.length} mermaid diagram(s) to render for ${checkId}`\\n+          );\\n+        } catch {}\\n+\\n+        // Render and upload each diagram\\n+        const uploadedCount: number[] = [];\\n+        for (let i = 0; i < diagrams.length; i++) {\\n+          const diagram = diagrams[i];\\n+          try {\\n+            ctx.logger.info(`[slack-frontend] rendering mermaid diagram ${i + 1}...`);\\n+            const pngBuffer = await renderMermaidToPng(diagram.code);\\n+            if (pngBuffer) {\\n+              ctx.logger.info(\\n+                `[slack-frontend] rendered diagram ${i + 1}, size=${pngBuffer.length} bytes, uploading...`\\n+              );\\n+              const filename = `diagram-${i + 1}.png`;\\n+              const uploadResult = await slack.files.uploadV2({\\n+                content: pngBuffer,\\n+                filename,\\n+                channel,\\n+                thread_ts: threadTs,\\n+                title: `Diagram ${i + 1}`,\\n+              });\\n+              if (uploadResult.ok) {\\n+                uploadedCount.push(i);\\n+                ctx.logger.info(`[slack-frontend] uploaded mermaid diagram ${i + 1} to ${channel}`);\\n+              } else {\\n+                ctx.logger.warn(`[slack-frontend] upload failed for diagram ${i + 1}`);\\n+              }\\n+            } else {\\n+              ctx.logger.warn(\\n+                `[slack-frontend] mermaid rendering returned null for diagram ${i + 1} (mmdc failed or not installed)`\\n+              );\\n+            }\\n+          } catch (e) {\\n+            ctx.logger.warn(\\n+              `[slack-frontend] failed to render/upload mermaid diagram ${i + 1}: ${\\n+                e instanceof Error ? e.message : String(e)\\n+              }`\\n+            );\\n+          }\\n+        }\\n+\\n+        // Replace mermaid blocks with placeholder text\\n+        if (uploadedCount.length > 0) {\\n+          processedText = replaceMermaidBlocks(text, diagrams, (idx) =>\\n+            uploadedCount.includes(idx) ? '_(See diagram above)_' : '_(Diagram rendering failed)_'\\n+          );\\n+        }\\n+      }\\n+\\n+      const formattedText = formatSlackText(processedText);\\n       await slack.chat.postMessage({ channel, text: formattedText, thread_ts: threadTs });\\n+      ctx.logger.info(\\n+        `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+      );\\n+    } catch (outerErr) {\\n+      // Log errors instead of silently swallowing them\\n       try {\\n-        ctx.logger.info(\\n-          `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+        ctx.logger.warn(\\n+          `[slack-frontend] maybePostDirectReply failed for ${checkId}: ${\\n+            outerErr instanceof Error ? outerErr.message : String(outerErr)\\n+          }`\\n         );\\n       } catch {}\\n-    } catch {}\\n+    }\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/generated/config-schema.ts\",\"additions\":233,\"deletions\":13,\"changes\":246,\"patch\":\"diff --git a/src/generated/config-schema.ts b/src/generated/config-schema.ts\\nindex b3caca4f..ecab390d 100644\\n--- a/src/generated/config-schema.ts\\n+++ b/src/generated/config-schema.ts\\n@@ -56,6 +56,20 @@ export const configSchema = {\\n           },\\n           description: 'Import workflow definitions from external files or URLs',\\n         },\\n+        inputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowInput',\\n+          },\\n+          description: 'Workflow inputs (for standalone reusable workflows)',\\n+        },\\n+        outputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowOutput',\\n+          },\\n+          description: 'Workflow outputs (for standalone reusable workflows)',\\n+        },\\n         steps: {\\n           $ref: '#/definitions/Record%3Cstring%2CCheckConfig%3E',\\n           description: 'Step configurations (recommended)',\\n@@ -67,7 +81,7 @@ export const configSchema = {\\n         },\\n         output: {\\n           $ref: '#/definitions/OutputConfig',\\n-          description: 'Output configuration',\\n+          description: 'Output configuration (optional - defaults provided)',\\n         },\\n         http_server: {\\n           $ref: '#/definitions/HttpServerConfig',\\n@@ -139,8 +153,16 @@ export const configSchema = {\\n           },\\n           description: 'Optional integrations: event-driven frontends (e.g., ndjson-sink, github)',\\n         },\\n+        workspace: {\\n+          $ref: '#/definitions/WorkspaceConfig',\\n+          description: 'Workspace isolation configuration for sandboxed execution',\\n+        },\\n+        slack: {\\n+          $ref: '#/definitions/SlackConfig',\\n+          description: 'Slack configuration',\\n+        },\\n       },\\n-      required: ['output', 'version'],\\n+      required: ['version'],\\n       patternProperties: {\\n         '^x-': {},\\n       },\\n@@ -243,6 +265,63 @@ export const configSchema = {\\n         type: 'string',\\n       },\\n     },\\n+    WorkflowInput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Input parameter name',\\n+        },\\n+        schema: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'JSON Schema for the input',\\n+        },\\n+        required: {\\n+          type: 'boolean',\\n+          description: 'Whether this input is required',\\n+        },\\n+        default: {\\n+          description: 'Default value if not provided',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow input definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    WorkflowOutput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Output name',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+        value: {\\n+          type: 'string',\\n+          description: 'Value using Liquid template syntax (references step outputs)',\\n+        },\\n+        value_js: {\\n+          type: 'string',\\n+          description: 'Value using JavaScript expression (alternative to value)',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow output definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n     'Record<string,CheckConfig>': {\\n       type: 'object',\\n       additionalProperties: {\\n@@ -390,11 +469,19 @@ export const configSchema = {\\n           description: 'Timeout in seconds for command execution (default: 60)',\\n         },\\n         depends_on: {\\n-          type: 'array',\\n-          items: {\\n-            type: 'string',\\n-          },\\n-          description: 'Check IDs that this check depends on (optional)',\\n+          anyOf: [\\n+            {\\n+              type: 'string',\\n+            },\\n+            {\\n+              type: 'array',\\n+              items: {\\n+                type: 'string',\\n+              },\\n+            },\\n+          ],\\n+          description:\\n+            'Check IDs that this check depends on (optional). Accepts single string or array.',\\n         },\\n         group: {\\n           type: 'string',\\n@@ -653,13 +740,93 @@ export const configSchema = {\\n           description: 'Arguments/inputs for the workflow',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Override specific step configurations in the workflow',\\n         },\\n         output_mapping: {\\n           $ref: '#/definitions/Record%3Cstring%2Cstring%3E',\\n           description: 'Map workflow outputs to check outputs',\\n         },\\n+        workflow_inputs: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'Alias for args - workflow inputs (backward compatibility)',\\n+        },\\n+        config: {\\n+          type: 'string',\\n+          description:\\n+            'Config file path - alternative to workflow ID (loads a Visor config file as workflow)',\\n+        },\\n+        workflow_overrides: {\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n+          description: 'Alias for overrides - workflow step overrides (backward compatibility)',\\n+        },\\n+        ref: {\\n+          type: 'string',\\n+          description: 'Git reference to checkout (branch, tag, commit SHA) - supports templates',\\n+        },\\n+        repository: {\\n+          type: 'string',\\n+          description: 'Repository URL or owner/repo format (defaults to current repository)',\\n+        },\\n+        token: {\\n+          type: 'string',\\n+          description: 'GitHub token for private repositories (defaults to GITHUB_TOKEN env)',\\n+        },\\n+        fetch_depth: {\\n+          type: 'number',\\n+          description: 'Number of commits to fetch (0 for full history, default: 1)',\\n+        },\\n+        fetch_tags: {\\n+          type: 'boolean',\\n+          description: 'Whether to fetch tags (default: false)',\\n+        },\\n+        submodules: {\\n+          anyOf: [\\n+            {\\n+              type: 'boolean',\\n+            },\\n+            {\\n+              type: 'string',\\n+              const: 'recursive',\\n+            },\\n+          ],\\n+          description: \\\"Checkout submodules: false, true, or 'recursive'\\\",\\n+        },\\n+        working_directory: {\\n+          type: 'string',\\n+          description: 'Working directory for the checkout (defaults to temp directory)',\\n+        },\\n+        use_worktree: {\\n+          type: 'boolean',\\n+          description: 'Use git worktree for efficient parallel checkouts (default: true)',\\n+        },\\n+        clean: {\\n+          type: 'boolean',\\n+          description: 'Clean the working directory before checkout (default: true)',\\n+        },\\n+        sparse_checkout: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description: 'Sparse checkout paths - only checkout specific directories/files',\\n+        },\\n+        lfs: {\\n+          type: 'boolean',\\n+          description: 'Enable Git LFS (Large File Storage)',\\n+        },\\n+        clone_timeout_ms: {\\n+          type: 'number',\\n+          description: 'Timeout in ms for cloning the bare repository (default: 300000 = 5 min)',\\n+        },\\n+        cleanup_on_failure: {\\n+          type: 'boolean',\\n+          description: 'Clean up worktree on failure (default: true)',\\n+        },\\n+        persist_worktree: {\\n+          type: 'boolean',\\n+          description: 'Keep worktree after workflow completion (default: false)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'Configuration for a single check',\\n@@ -794,6 +961,11 @@ export const configSchema = {\\n           $ref: '#/definitions/BashConfig',\\n           description: 'Advanced bash command execution configuration',\\n         },\\n+        completion_prompt: {\\n+          type: 'string',\\n+          description:\\n+            'Completion prompt for post-completion validation/review (runs after attempt_completion)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'AI provider configuration',\\n@@ -1223,7 +1395,7 @@ export const configSchema = {\\n           description: 'Custom output name (defaults to workflow name)',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Step overrides',\\n         },\\n         output_mapping: {\\n@@ -1238,14 +1410,14 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n-    'Record<string,Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>>':\\n+    'Record<string,Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>>':\\n       {\\n         type: 'object',\\n         additionalProperties: {\\n-          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E',\\n+          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E',\\n         },\\n       },\\n-    'Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>': {\\n+    'Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>': {\\n       type: 'object',\\n       additionalProperties: false,\\n     },\\n@@ -1811,6 +1983,54 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n+    WorkspaceConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        enabled: {\\n+          type: 'boolean',\\n+          description: 'Enable workspace isolation (default: true when config present)',\\n+        },\\n+        base_path: {\\n+          type: 'string',\\n+          description: 'Base path for workspaces (default: /tmp/visor-workspaces)',\\n+        },\\n+        cleanup_on_exit: {\\n+          type: 'boolean',\\n+          description: 'Clean up workspace on exit (default: true)',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Workspace isolation configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    SlackConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        version: {\\n+          type: 'string',\\n+          description: 'Slack API version',\\n+        },\\n+        mentions: {\\n+          type: 'string',\\n+          description: \\\"Mention handling: 'all', 'direct', etc.\\\",\\n+        },\\n+        threads: {\\n+          type: 'string',\\n+          description: \\\"Thread handling: 'required', 'optional', etc.\\\",\\n+        },\\n+        show_raw_output: {\\n+          type: 'boolean',\\n+          description: 'Show raw output in Slack responses',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Slack configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n   },\\n } as const;\\n export default configSchema;\\n\",\"status\":\"modified\"},{\"filename\":\"src/index.ts\",\"additions\":5,\"deletions\":2,\"changes\":7,\"patch\":\"diff --git a/src/index.ts b/src/index.ts\\nindex 50b89df3..9621e733 100644\\n--- a/src/index.ts\\n+++ b/src/index.ts\\n@@ -628,8 +628,11 @@ function resolveDependencies(\\n \\n     // Get dependencies for this check\\n     const checkConfig = config?.checks?.[checkId];\\n+    // Normalize depends_on to array (supports string | string[])\\n+    const rawDeps = checkConfig?.depends_on;\\n+    const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     // Expand OR groups (pipe syntax) for dependency closure discovery\\n-    const dependencies = (checkConfig?.depends_on || []).flatMap(d =>\\n+    const dependencies = depsArray.flatMap((d: string) =>\\n       typeof d === 'string' && d.includes('|')\\n         ? d\\n             .split('|')\\n\",\"status\":\"modified\"},{\"filename\":\"src/liquid-extensions.ts\",\"additions\":68,\"deletions\":1,\"changes\":69,\"patch\":\"diff --git a/src/liquid-extensions.ts b/src/liquid-extensions.ts\\nindex e9efaf68..dbf9c6ed 100644\\n--- a/src/liquid-extensions.ts\\n+++ b/src/liquid-extensions.ts\\n@@ -124,6 +124,26 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     }\\n   });\\n \\n+  // Register base64 filter for encoding strings\\n+  // Usage: {{ \\\"user:password\\\" | base64 }}\\n+  liquid.registerFilter('base64', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    return Buffer.from(str).toString('base64');\\n+  });\\n+\\n+  // Register base64_decode filter for decoding base64 strings\\n+  // Usage: {{ encoded_value | base64_decode }}\\n+  liquid.registerFilter('base64_decode', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    try {\\n+      return Buffer.from(str, 'base64').toString('utf-8');\\n+    } catch {\\n+      return '[Error: Invalid base64 string]';\\n+    }\\n+  });\\n+\\n   // Sanitize a label to allowed characters only: [A-Za-z0-9:/]\\n   liquid.registerFilter('safe_label', (value: unknown) => sanitizeLabel(value));\\n \\n@@ -137,6 +157,53 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     return s.replace(/\\\\\\\\n/g, '\\\\n').replace(/\\\\\\\\r/g, '\\\\r').replace(/\\\\\\\\t/g, '\\\\t');\\n   });\\n \\n+  // JSON escape filter - escapes a string for use inside a JSON string value\\n+  // This escapes special characters like quotes, backslashes, and control characters\\n+  // Usage: \\\"jql\\\": \\\"{{ myValue | json_escape }}\\\"\\n+  liquid.registerFilter('json_escape', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const s = String(value);\\n+    // Use JSON.stringify which handles all escaping, then strip the surrounding quotes\\n+    const jsonStr = JSON.stringify(s);\\n+    // Remove the first and last character (the quotes added by JSON.stringify)\\n+    return jsonStr.slice(1, -1);\\n+  });\\n+\\n+  // Shell escape filter - wraps value in single quotes with proper escaping\\n+  // Usage: {{ value | shell_escape }}\\n+  // Example: \\\"hello'world\\\" becomes \\\"'hello'\\\\''world'\\\"\\n+  // This is POSIX-compliant and safe for arbitrary text including mermaid diagrams\\n+  liquid.registerFilter('shell_escape', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    // Replace single quotes with: end quote, escaped quote, start quote\\n+    // Then wrap the entire thing in single quotes\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Alias for shell_escape\\n+  liquid.registerFilter('escape_shell', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Shell escape for double quotes (less safe but sometimes needed)\\n+  // Usage: {{ value | shell_escape_double }}\\n+  // Escapes: $, `, \\\\, \\\", and !\\n+  liquid.registerFilter('shell_escape_double', (value: unknown) => {\\n+    if (value == null) return '\\\"\\\"';\\n+    const s = String(value);\\n+    // Escape characters that have special meaning inside double quotes\\n+    const escaped = s\\n+      .replace(/\\\\\\\\/g, '\\\\\\\\\\\\\\\\')  // backslash first\\n+      .replace(/\\\\$/g, '\\\\\\\\$')   // dollar sign\\n+      .replace(/`/g, '\\\\\\\\`')    // backticks\\n+      .replace(/\\\"/g, '\\\\\\\\\\\"')    // double quotes\\n+      .replace(/!/g, '\\\\\\\\!');   // history expansion\\n+    return '\\\"' + escaped + '\\\"';\\n+  });\\n+\\n   // Register author permission filters (from main)\\n   // These filters check the author's permission level; detect local mode for tests\\n   const isLocal = detectLocalMode();\\n\",\"status\":\"modified\"},{\"filename\":\"src/logger.ts\",\"additions\":31,\"deletions\":2,\"changes\":33,\"patch\":\"diff --git a/src/logger.ts b/src/logger.ts\\nindex 6148b77c..aee7136e 100644\\n--- a/src/logger.ts\\n+++ b/src/logger.ts\\n@@ -85,7 +85,36 @@ class Logger {\\n       if (this.showTimestamps) {\\n         const ts = new Date().toISOString();\\n         const lvl = level ? level : undefined;\\n-        const prefix = lvl ? `[${ts}] [${lvl}]` : `[${ts}]`;\\n+\\n+        let tsToken = `[${ts}]`;\\n+        let lvlToken = lvl ? `[${lvl}]` : '';\\n+\\n+        // Add simple ANSI colour when running in a TTY and not emitting\\n+        // JSON/SARIF. Colours are intentionally minimal and only applied\\n+        // to the prefix markers, not the full line.\\n+        if (this.isTTY && !this.isJsonLike) {\\n+          const reset = '\\\\x1b[0m';\\n+          const dim = '\\\\x1b[2m';\\n+          const colours: Record<LogLevel, string> = {\\n+            silent: '',\\n+            error: '\\\\x1b[31m', // red\\n+            warn: '\\\\x1b[33m', // yellow\\n+            info: '\\\\x1b[36m', // cyan\\n+            verbose: '\\\\x1b[35m', // magenta\\n+            debug: '\\\\x1b[90m', // bright black / gray\\n+          };\\n+\\n+          tsToken = `${dim}${tsToken}${reset}`;\\n+\\n+          if (lvl) {\\n+            const colour = colours[lvl] || '';\\n+            if (colour) {\\n+              lvlToken = `${colour}${lvlToken}${reset}`;\\n+            }\\n+          }\\n+        }\\n+\\n+        const prefix = lvl ? `${tsToken} ${lvlToken}` : tsToken;\\n         process.stderr.write(`${prefix} ${msg}\\\\n`);\\n       } else {\\n         process.stderr.write(msg + '\\\\n');\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/ai-check-provider.ts\",\"additions\":100,\"deletions\":6,\"changes\":106,\"patch\":\"diff --git a/src/providers/ai-check-provider.ts b/src/providers/ai-check-provider.ts\\nindex 53f1a350..b41d6af1 100644\\n--- a/src/providers/ai-check-provider.ts\\n+++ b/src/providers/ai-check-provider.ts\\n@@ -186,7 +186,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     let promptContent: string;\\n \\n@@ -204,7 +205,8 @@ export class AICheckProvider extends CheckProvider {\\n       eventContext,\\n       dependencyResults,\\n       outputHistory,\\n-      args\\n+      args,\\n+      workflowInputs\\n     );\\n   }\\n \\n@@ -335,7 +337,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     // Build outputs_raw from -raw keys (aggregate parent values)\\n     const outputsRaw: Record<string, unknown> = {};\\n@@ -535,6 +538,8 @@ export class AICheckProvider extends CheckProvider {\\n       outputs_raw: outputsRaw,\\n       // Custom arguments from on_init 'with' directive\\n       args: args || {},\\n+      // Workflow inputs (for nested workflow steps to access parent inputs like {{ inputs.context }})\\n+      inputs: workflowInputs || {},\\n     };\\n \\n     try {\\n@@ -621,7 +626,8 @@ export class AICheckProvider extends CheckProvider {\\n         console.error(`[ai-exec] step=${String((config as any).checkName || 'unknown')}`);\\n       }\\n     } catch {}\\n-    // Extract AI configuration - only set properties that are explicitly provided\\n+    // Extract AI configuration - only set properties that are explicitly provided.\\n+    // Workspace / allowedFolders will be derived below from the execution context.\\n     const aiConfig: AIReviewConfig = {};\\n \\n     // Check-level AI configuration (ai object)\\n@@ -671,6 +677,9 @@ export class AICheckProvider extends CheckProvider {\\n       if (aiAny.bashConfig !== undefined) {\\n         aiConfig.bashConfig = aiAny.bashConfig as import('../types/config').BashConfig;\\n       }\\n+      if (aiAny.completion_prompt !== undefined) {\\n+        aiConfig.completionPrompt = aiAny.completion_prompt as string;\\n+      }\\n       if (aiAny.skip_code_context !== undefined) {\\n         // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n         (aiConfig as any).skip_code_context = aiAny.skip_code_context as boolean;\\n@@ -691,6 +700,90 @@ export class AICheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Derive workspace-aware allowedFolders for ProbeAgent when workspace\\n+    // isolation is enabled. This ensures tools like search/query operate\\n+    // inside the isolated workspace (and its project symlinks) instead of\\n+    // the Visor repository root.\\n+    // Folder names are human-readable (tyk-docs, visor2) thanks to WorkspaceManager.\\n+    try {\\n+      const ctxAny: any = sessionInfo as any;\\n+      const parentCtx = ctxAny?._parentContext;\\n+      const workspace = parentCtx?.workspace;\\n+\\n+      // Enhanced debug logging for workspace propagation diagnosis\\n+      logger.debug(`[AI Provider] Workspace detection for check '${(config as any).checkName || 'unknown'}':`);\\n+      logger.debug(`[AI Provider]   sessionInfo exists: ${!!sessionInfo}`);\\n+      logger.debug(`[AI Provider]   _parentContext exists: ${!!parentCtx}`);\\n+      logger.debug(`[AI Provider]   workspace exists: ${!!workspace}`);\\n+      if (workspace) {\\n+        logger.debug(`[AI Provider]   workspace.isEnabled exists: ${typeof workspace.isEnabled === 'function'}`);\\n+        logger.debug(`[AI Provider]   workspace.isEnabled(): ${typeof workspace.isEnabled === 'function' ? workspace.isEnabled() : 'N/A'}`);\\n+        const projectCount = typeof workspace.listProjects === 'function' ? workspace.listProjects()?.length : 'N/A';\\n+        logger.debug(`[AI Provider]   workspace.listProjects() count: ${projectCount}`);\\n+      }\\n+\\n+      if (workspace && typeof workspace.isEnabled === 'function' && workspace.isEnabled()) {\\n+        const folders: string[] = [];\\n+        let workspaceRoot: string | undefined;\\n+        let mainProjectPath: string | undefined;\\n+        try {\\n+          const info = workspace.getWorkspaceInfo?.();\\n+          if (info && typeof info.workspacePath === 'string') {\\n+            workspaceRoot = info.workspacePath;\\n+            mainProjectPath = info.mainProjectPath;\\n+            // Add workspace root\\n+            folders.push(info.workspacePath);\\n+            // Include main project path (has human-readable name like \\\"visor2\\\")\\n+            if (mainProjectPath) {\\n+              folders.push(mainProjectPath);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore workspace info errors\\n+        }\\n+        try {\\n+          const projects = workspace.listProjects?.() || [];\\n+          for (const proj of projects as any[]) {\\n+            if (proj && typeof proj.path === 'string') {\\n+              // Project paths have human-readable names (tyk-docs, not checkout-tyk-docs)\\n+              folders.push(proj.path);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore project listing errors\\n+        }\\n+        const unique = Array.from(new Set(folders.filter(p => typeof p === 'string' && p)));\\n+        if (unique.length > 0 && workspaceRoot) {\\n+          (aiConfig as any).allowedFolders = unique;\\n+          // Set path and cwd to workspace root - AI will run with cwd set to workspace\\n+          // Both are set for compatibility: path for older probe versions, cwd for rc175+\\n+          (aiConfig as any).path = workspaceRoot;\\n+          (aiConfig as any).cwd = workspaceRoot;\\n+          // Also set workspacePath for the AI to know the root\\n+          (aiConfig as any).workspacePath = workspaceRoot;\\n+          logger.debug(`[AI Provider] Workspace isolation enabled:`);\\n+          logger.debug(`[AI Provider]   workspaceRoot (cwd): ${workspaceRoot}`);\\n+          logger.debug(`[AI Provider]   allowedFolders: ${JSON.stringify(unique)}`);\\n+        }\\n+      } else if (parentCtx && typeof parentCtx.workingDirectory === 'string') {\\n+        // Fallback: when workspace is not available (or disabled), still\\n+        // constrain tools to the engine's working directory so ProbeAgent\\n+        // operates inside the same logical root as the state machine. This\\n+        // also ensures nested workflows (e.g. code-question-helper) use the\\n+        // workspace main project path once initializeWorkspace has updated\\n+        // the parent context.\\n+        if (!(aiConfig as any).allowedFolders) {\\n+          (aiConfig as any).allowedFolders = [parentCtx.workingDirectory];\\n+        }\\n+        if (!(aiConfig as any).path) {\\n+          (aiConfig as any).path = parentCtx.workingDirectory;\\n+          (aiConfig as any).cwd = parentCtx.workingDirectory;\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only; fall back to defaults on error.\\n+    }\\n+\\n     // Check-level AI model and provider (top-level properties)\\n     if (config.ai_model !== undefined) {\\n       aiConfig.model = config.ai_model as string;\\n@@ -876,7 +969,8 @@ export class AICheckProvider extends CheckProvider {\\n       ctxWithStage,\\n       _dependencyResults,\\n       (config as any).__outputHistory as Map<string, unknown[]> | undefined,\\n-      (sessionInfo as any)?.args\\n+      (sessionInfo as any)?.args,\\n+      (config as any).workflowInputs as Record<string, unknown> | undefined\\n     );\\n \\n     // Optional persona (vendor extension): ai.ai_persona or ai_persona.\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/command-check-provider.ts\",\"additions\":50,\"deletions\":16,\"changes\":66,\"patch\":\"diff --git a/src/providers/command-check-provider.ts b/src/providers/command-check-provider.ts\\nindex e0c9d634..bf7ffd43 100644\\n--- a/src/providers/command-check-provider.ts\\n+++ b/src/providers/command-check-provider.ts\\n@@ -131,7 +131,8 @@ export class CommandCheckProvider extends CheckProvider {\\n       // New: outputs_raw exposes aggregate values (e.g., full arrays for forEach parents)\\n       outputs_raw: outputsRaw,\\n       // Workflow inputs (when executing within a workflow)\\n-      inputs: context?.workflowInputs || {},\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any).workflowInputs || context?.workflowInputs || {},\\n       // Custom arguments from on_init 'with' directive\\n       args: context?.args || {},\\n       env: this.getSafeEnvironmentVariables(),\\n@@ -178,17 +179,31 @@ export class CommandCheckProvider extends CheckProvider {\\n           mock = rawMock as Record<string, unknown>;\\n         }\\n         const m = mock as { stdout?: string; stderr?: string; exit_code?: number; exit?: number };\\n-        let out: unknown = m.stdout ?? '';\\n-        try {\\n-          if (\\n-            typeof out === 'string' &&\\n-            (out.trim().startsWith('{') || out.trim().startsWith('['))\\n-          ) {\\n-            out = JSON.parse(out);\\n-          }\\n-        } catch {}\\n-        const code =\\n-          typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0;\\n+\\n+        // Check if this looks like a command mock (has stdout/stderr/exit_code) or direct data output\\n+        const isCommandMock = m.stdout !== undefined || m.stderr !== undefined ||\\n+          m.exit_code !== undefined || m.exit !== undefined;\\n+\\n+        let out: unknown;\\n+        if (isCommandMock) {\\n+          // Traditional command mock format: { stdout: \\\"...\\\", exit_code: 0 }\\n+          out = m.stdout ?? '';\\n+          try {\\n+            if (\\n+              typeof out === 'string' &&\\n+              (out.trim().startsWith('{') || out.trim().startsWith('['))\\n+            ) {\\n+              out = JSON.parse(out);\\n+            }\\n+          } catch {}\\n+        } else {\\n+          // Direct data mock format: { data: [...], count: 1 } - use as-is\\n+          out = mock;\\n+        }\\n+\\n+        const code = isCommandMock\\n+          ? (typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0)\\n+          : 0;\\n         if (code !== 0) {\\n           return {\\n             issues: [\\n@@ -231,9 +246,8 @@ export class CommandCheckProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Get timeout from config (in seconds) or use default (60 seconds)\\n-      const timeoutSeconds = (config.timeout as number) || 60;\\n-      const timeoutMs = timeoutSeconds * 1000;\\n+      // Get timeout from config (in milliseconds) or use default (60 seconds = 60000ms)\\n+      const timeoutMs = (config.timeout as number) || 60000;\\n \\n       // Normalize only the eval payload for `node -e|--eval` invocations that may contain\\n       // literal newlines due to YAML processing (\\\"\\\\n\\\" -> newline). We re-escape newlines\\n@@ -1533,6 +1547,26 @@ ${bodyWithReturn}\\n       return null;\\n     }\\n \\n+    // Require at least one issue-specific field beyond just a message-like field.\\n+    // This prevents objects like {text: \\\"...\\\"} from being treated as issues.\\n+    const hasIssueField =\\n+      data.file !== undefined ||\\n+      data.path !== undefined ||\\n+      data.filename !== undefined ||\\n+      data.line !== undefined ||\\n+      data.startLine !== undefined ||\\n+      data.lineNumber !== undefined ||\\n+      data.severity !== undefined ||\\n+      data.level !== undefined ||\\n+      data.priority !== undefined ||\\n+      data.ruleId !== undefined ||\\n+      data.rule !== undefined ||\\n+      data.category !== undefined ||\\n+      data.type !== undefined;\\n+    if (!hasIssueField) {\\n+      return null;\\n+    }\\n+\\n     const allowedSeverities = new Set(['info', 'warning', 'error', 'critical']);\\n     const severityRaw = this.toTrimmedString(data.severity || data.level || data.priority);\\n     let severity: ReviewIssue['severity'] = 'warning';\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/git-checkout-provider.ts\",\"additions\":41,\"deletions\":6,\"changes\":47,\"patch\":\"diff --git a/src/providers/git-checkout-provider.ts b/src/providers/git-checkout-provider.ts\\nindex baac0001..6d3a2852 100644\\n--- a/src/providers/git-checkout-provider.ts\\n+++ b/src/providers/git-checkout-provider.ts\\n@@ -69,6 +69,16 @@ export class GitCheckoutProvider extends CheckProvider {\\n       return false;\\n     }\\n \\n+    if (checkoutConfig.clone_timeout_ms !== undefined) {\\n+      if (\\n+        typeof checkoutConfig.clone_timeout_ms !== 'number' ||\\n+        checkoutConfig.clone_timeout_ms <= 0\\n+      ) {\\n+        logger.error('Invalid config: clone_timeout_ms must be a positive number (milliseconds)');\\n+        return false;\\n+      }\\n+    }\\n+\\n     return true;\\n   }\\n \\n@@ -91,7 +101,13 @@ export class GitCheckoutProvider extends CheckProvider {\\n       );\\n \\n       // Resolve dynamic variables\\n-      const resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      let resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      // If ref resolves to an empty string, fall back to HEAD so we rely on\\n+      // the repository's default branch. This keeps downstream git commands\\n+      // well-defined and matches existing expectations/tests.\\n+      if (!resolvedRef || resolvedRef.trim().length === 0) {\\n+        resolvedRef = 'HEAD';\\n+      }\\n       const resolvedRepository = checkoutConfig.repository\\n         ? await this.liquid.parseAndRender(checkoutConfig.repository, templateContext)\\n         : process.env.GITHUB_REPOSITORY || 'unknown/unknown';\\n@@ -118,6 +134,7 @@ export class GitCheckoutProvider extends CheckProvider {\\n           clean: checkoutConfig.clean !== false, // Default: true\\n           workflowId: (context as any)?.workflowId,\\n           fetchDepth: checkoutConfig.fetch_depth,\\n+          cloneTimeoutMs: checkoutConfig.clone_timeout_ms,\\n         }\\n       );\\n \\n@@ -134,18 +151,35 @@ export class GitCheckoutProvider extends CheckProvider {\\n \\n       // Add project to workspace if workspace isolation is enabled\\n       const workspace = (context as any)?._parentContext?.workspace;\\n+\\n+      // Enhanced debug logging for workspace project addition diagnosis\\n+      const checkName = (config as any)?.checkName || 'unknown';\\n+      logger.info(`[GitCheckout] Workspace check for '${checkName}':`);\\n+      logger.info(`[GitCheckout]   _parentContext exists: ${!!(context as any)?._parentContext}`);\\n+      logger.info(`[GitCheckout]   workspace exists: ${!!workspace}`);\\n+      logger.info(`[GitCheckout]   workspace.isEnabled(): ${workspace?.isEnabled?.() ?? 'N/A'}`);\\n+      if (workspace) {\\n+        const projectCountBefore = workspace.listProjects?.()?.length ?? 'N/A';\\n+        logger.debug(`[GitCheckout]   projects before addProject: ${projectCountBefore}`);\\n+      }\\n+\\n       if (workspace?.isEnabled()) {\\n         try {\\n+          // Don't pass checkName as description - let workspace use repo name\\n+          // This results in human-readable names like \\\"tyk-docs\\\" instead of \\\"checkout-tyk-docs\\\"\\n           const workspacePath = await workspace.addProject(\\n             resolvedRepository,\\n-            worktree.path,\\n-            checkoutConfig.checkName\\n+            worktree.path\\n           );\\n           output.workspace_path = workspacePath;\\n-          logger.debug(`Added project to workspace: ${workspacePath}`);\\n+          const projectCountAfter = workspace.listProjects?.()?.length ?? 'N/A';\\n+          logger.debug(`[GitCheckout] Added project to workspace: ${workspacePath}`);\\n+          logger.debug(`[GitCheckout]   projects after addProject: ${projectCountAfter}`);\\n         } catch (error) {\\n           logger.warn(`Failed to add project to workspace: ${error}`);\\n         }\\n+      } else {\\n+        logger.debug(`[GitCheckout] Workspace not enabled, skipping addProject`);\\n       }\\n \\n       logger.info(\\n@@ -228,7 +262,8 @@ export class GitCheckoutProvider extends CheckProvider {\\n       outputs: outputsObj,\\n       outputs_history: historyObj,\\n       env: safeEnv,\\n-      inputs: context?.workflowInputs,\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any)?.workflowInputs || context?.workflowInputs,\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/http-client-provider.ts\",\"additions\":279,\"deletions\":35,\"changes\":314,\"patch\":\"diff --git a/src/providers/http-client-provider.ts b/src/providers/http-client-provider.ts\\nindex 4d8c41be..6db81591 100644\\n--- a/src/providers/http-client-provider.ts\\n+++ b/src/providers/http-client-provider.ts\\n@@ -4,18 +4,29 @@ import { ReviewSummary } from '../reviewer';\\n import { Liquid } from 'liquidjs';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n import { EnvironmentResolver } from '../utils/env-resolver';\\n+import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n+import { buildProviderTemplateContext } from '../utils/template-context';\\n+import Sandbox from '@nyariv/sandboxjs';\\n+import { logger } from '../logger';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n \\n /**\\n  * Check provider that fetches data from HTTP endpoints\\n  */\\n export class HttpClientProvider extends CheckProvider {\\n   private liquid: Liquid;\\n+  private sandbox?: Sandbox;\\n \\n   constructor() {\\n     super();\\n     this.liquid = createExtendedLiquid();\\n   }\\n \\n+  private createSecureSandbox(): Sandbox {\\n+    return createSecureSandbox();\\n+  }\\n+\\n   getName(): string {\\n     return 'http_client';\\n   }\\n@@ -61,50 +72,124 @@ export class HttpClientProvider extends CheckProvider {\\n     const headers = (config.headers as Record<string, string>) || {};\\n     const timeout = (config.timeout as number) || 30000;\\n     const transform = config.transform as string | undefined;\\n+    const transformJs = config.transform_js as string | undefined;\\n     const bodyTemplate = config.body as string | undefined;\\n+    const outputFileTemplate = config.output_file as string | undefined;\\n+    const skipIfExists = config.skip_if_exists !== false; // Default true for caching\\n \\n-    try {\\n-      // Prepare template context for URL and body\\n-      const templateContext = {\\n-        pr: {\\n-          number: prInfo.number,\\n-          title: prInfo.title,\\n-          body: prInfo.body,\\n-          author: prInfo.author,\\n-          base: prInfo.base,\\n-          head: prInfo.head,\\n-          totalAdditions: prInfo.totalAdditions,\\n-          totalDeletions: prInfo.totalDeletions,\\n-        },\\n-        outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-        env: process.env,\\n-      };\\n+    // Track resolved URL for error messages\\n+    let resolvedUrlForErrors = url;\\n \\n-      // Render URL with template if it contains liquid syntax\\n-      let renderedUrl = url;\\n-      if (url.includes('{{') || url.includes('{%')) {\\n-        renderedUrl = await this.liquid.parseAndRender(url, templateContext);\\n+    try {\\n+      // Use shared template context builder for consistent output extraction\\n+      const templateContext = buildProviderTemplateContext(\\n+        prInfo,\\n+        dependencyResults,\\n+        undefined, // memoryStore\\n+        undefined, // outputHistory\\n+        undefined, // stageHistoryBase\\n+        { attachMemoryReadHelpers: false }\\n+      );\\n+      // Add env to context for shell-style variable resolution\\n+      (templateContext as Record<string, unknown>).env = process.env;\\n+\\n+      // First resolve shell-style environment variables (${VAR}, $VAR, ${{ env.VAR }})\\n+      let renderedUrl = String(EnvironmentResolver.resolveValue(url));\\n+      resolvedUrlForErrors = renderedUrl; // Track for error messages\\n+\\n+      // Then render Liquid templates if present\\n+      if (renderedUrl.includes('{{') || renderedUrl.includes('{%')) {\\n+        renderedUrl = await this.liquid.parseAndRender(renderedUrl, templateContext);\\n+        resolvedUrlForErrors = renderedUrl; // Update after Liquid rendering\\n       }\\n \\n       // Prepare request body if provided\\n       let requestBody: string | undefined;\\n       if (bodyTemplate) {\\n-        const renderedBody = await this.liquid.parseAndRender(bodyTemplate, templateContext);\\n-        requestBody = renderedBody;\\n+        // First resolve shell-style environment variables\\n+        let resolvedBody = String(EnvironmentResolver.resolveValue(bodyTemplate));\\n+        // Then render Liquid templates if present\\n+        if (resolvedBody.includes('{{') || resolvedBody.includes('{%')) {\\n+          resolvedBody = await this.liquid.parseAndRender(resolvedBody, templateContext);\\n+        }\\n+        requestBody = resolvedBody;\\n       }\\n \\n-      // Resolve environment variables in headers\\n-      const resolvedHeaders = EnvironmentResolver.resolveHeaders(headers);\\n+      // Resolve environment variables and Liquid templates in headers\\n+      const resolvedHeaders: Record<string, string> = {};\\n+      for (const [key, value] of Object.entries(headers)) {\\n+        let resolvedValue = String(EnvironmentResolver.resolveValue(value));\\n+        // Render Liquid templates if present\\n+        if (resolvedValue.includes('{{') || resolvedValue.includes('{%')) {\\n+          resolvedValue = await this.liquid.parseAndRender(resolvedValue, templateContext);\\n+        }\\n+        resolvedHeaders[key] = resolvedValue;\\n+        // Debug auth header (mask most of the value for security)\\n+        if (key.toLowerCase() === 'authorization') {\\n+          const maskedValue = resolvedValue.length > 20\\n+            ? `${resolvedValue.substring(0, 15)}...${resolvedValue.substring(resolvedValue.length - 5)}`\\n+            : resolvedValue;\\n+          logger.verbose(`[http_client] ${key}: ${maskedValue}`);\\n+        }\\n+      }\\n+\\n+      // Resolve output_file path if specified\\n+      let resolvedOutputFile: string | undefined;\\n+      if (outputFileTemplate) {\\n+        let outputPath = String(EnvironmentResolver.resolveValue(outputFileTemplate));\\n+        if (outputPath.includes('{{') || outputPath.includes('{%')) {\\n+          outputPath = await this.liquid.parseAndRender(outputPath, templateContext);\\n+        }\\n+        resolvedOutputFile = outputPath.trim();\\n+\\n+        // Check if file already exists (caching)\\n+        if (skipIfExists && fs.existsSync(resolvedOutputFile)) {\\n+          const stats = fs.statSync(resolvedOutputFile);\\n+          logger.verbose(`[http_client] File cached: ${resolvedOutputFile} (${stats.size} bytes)`);\\n+          return {\\n+            issues: [],\\n+            file_path: resolvedOutputFile,\\n+            size: stats.size,\\n+            cached: true,\\n+          } as unknown as ReviewSummary;\\n+        }\\n+      }\\n \\n       // Test hook: mock HTTP response for this step\\n       const stepName = (config as any).checkName || 'unknown';\\n       const mock = context?.hooks?.mockForStep?.(String(stepName));\\n-      const data =\\n-        mock !== undefined\\n-          ? mock\\n-          : await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n \\n-      // Apply transformation if specified\\n+      // If mock is provided, return it directly as the step output (with issues: [])\\n+      if (mock !== undefined) {\\n+        const mockObj = typeof mock === 'object' && mock !== null ? mock : { data: mock };\\n+        return {\\n+          issues: [],\\n+          ...mockObj,\\n+        } as unknown as ReviewSummary & { data: unknown };\\n+      }\\n+\\n+      // Debug log the request for troubleshooting\\n+      logger.verbose(`[http_client] ${method} ${renderedUrl}`);\\n+      if (requestBody) {\\n+        logger.verbose(`[http_client] Body: ${requestBody.substring(0, 500)}${requestBody.length > 500 ? '...' : ''}`);\\n+      }\\n+\\n+      // If output_file is specified, download to file instead of returning data\\n+      if (resolvedOutputFile) {\\n+        const fileResult = await this.downloadToFile(\\n+          renderedUrl,\\n+          method,\\n+          resolvedHeaders,\\n+          requestBody,\\n+          timeout,\\n+          resolvedOutputFile\\n+        );\\n+        return fileResult;\\n+      }\\n+\\n+      const data = await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n+\\n+      // Apply Liquid transformation if specified\\n       let processedData = data;\\n       if (transform) {\\n         try {\\n@@ -136,12 +221,53 @@ export class HttpClientProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Return the fetched data as a custom field for dependent checks to access\\n+      // Apply JavaScript transformation if specified\\n+      if (transformJs) {\\n+        try {\\n+          if (!this.sandbox) {\\n+            this.sandbox = this.createSecureSandbox();\\n+          }\\n+\\n+          // Create scope for JavaScript transform (scope, not context)\\n+          const jsScope: Record<string, unknown> = {\\n+            output: data,\\n+            pr: templateContext.pr,\\n+            outputs: templateContext.outputs,\\n+            env: process.env,\\n+          };\\n+\\n+          const result = compileAndRun(this.sandbox, transformJs, jsScope, {\\n+            injectLog: true,\\n+            logPrefix: 'üîç [transform_js]',\\n+            wrapFunction: true,\\n+          });\\n+          processedData = result;\\n+          logger.verbose(`‚úì Applied JavaScript transform successfully`);\\n+        } catch (error) {\\n+          logger.error(\\n+            `‚úó Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`\\n+          );\\n+          return {\\n+            issues: [\\n+              {\\n+                file: 'http_client',\\n+                line: 0,\\n+                ruleId: 'http_client/transform_js_error',\\n+                message: `Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              },\\n+            ],\\n+          };\\n+        }\\n+      }\\n+\\n+      // Return the fetched data in the standard `output` property for guarantee evaluation\\n+      // This is consistent with other providers (script, command, ai, etc.)\\n       return {\\n         issues: [],\\n-        // Add custom data field that will be passed through to dependent checks\\n-        data: processedData,\\n-      } as ReviewSummary & { data: unknown };\\n+        output: processedData,\\n+      } as unknown as ReviewSummary;\\n     } catch (error) {\\n       return {\\n         issues: [\\n@@ -149,7 +275,7 @@ export class HttpClientProvider extends CheckProvider {\\n             file: 'http_client',\\n             line: 0,\\n             ruleId: 'http_client/fetch_error',\\n-            message: `Failed to fetch from ${url}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            message: `Failed to fetch from ${resolvedUrlForErrors}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n             severity: 'error',\\n             category: 'logic',\\n           },\\n@@ -198,7 +324,14 @@ export class HttpClientProvider extends CheckProvider {\\n \\n       clearTimeout(timeoutId);\\n \\n+      logger.verbose(`[http_client] Response: ${response.status} ${response.statusText}`);\\n+\\n       if (!response.ok) {\\n+        // Log response body for debugging auth failures\\n+        try {\\n+          const errorBody = await response.text();\\n+          logger.warn(`[http_client] Error body: ${errorBody.substring(0, 500)}`);\\n+        } catch {}\\n         throw new Error(`HTTP ${response.status}: ${response.statusText}`);\\n       }\\n \\n@@ -233,6 +366,112 @@ export class HttpClientProvider extends CheckProvider {\\n     }\\n   }\\n \\n+  private async downloadToFile(\\n+    url: string,\\n+    method: string,\\n+    headers: Record<string, string>,\\n+    body: string | undefined,\\n+    timeout: number,\\n+    outputFile: string\\n+  ): Promise<ReviewSummary> {\\n+    // Check if fetch is available (Node 18+)\\n+    if (typeof fetch === 'undefined') {\\n+      throw new Error('HTTP client provider requires Node.js 18+ or node-fetch package');\\n+    }\\n+\\n+    const controller = new AbortController();\\n+    const timeoutId = setTimeout(() => controller.abort(), timeout);\\n+\\n+    try {\\n+      const requestOptions: RequestInit = {\\n+        method,\\n+        headers: { ...headers },\\n+        signal: controller.signal,\\n+      };\\n+\\n+      // Add body for non-GET requests\\n+      if (method !== 'GET' && body) {\\n+        requestOptions.body = body;\\n+        if (!headers['Content-Type'] && !headers['content-type']) {\\n+          requestOptions.headers = {\\n+            ...requestOptions.headers,\\n+            'Content-Type': 'application/json',\\n+          };\\n+        }\\n+      }\\n+\\n+      const response = await fetch(url, requestOptions);\\n+      clearTimeout(timeoutId);\\n+\\n+      if (!response.ok) {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_error',\\n+              message: `Failed to download file: HTTP ${response.status}: ${response.statusText}`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      // Create parent directory if it doesn't exist\\n+      const parentDir = path.dirname(outputFile);\\n+      if (parentDir && !fs.existsSync(parentDir)) {\\n+        fs.mkdirSync(parentDir, { recursive: true });\\n+      }\\n+\\n+      // Get the response as an ArrayBuffer and write to file\\n+      const arrayBuffer = await response.arrayBuffer();\\n+      const buffer = Buffer.from(arrayBuffer);\\n+      fs.writeFileSync(outputFile, buffer);\\n+\\n+      const contentType = response.headers.get('content-type') || 'application/octet-stream';\\n+      logger.verbose(`[http_client] Downloaded: ${outputFile} (${buffer.length} bytes)`);\\n+\\n+      return {\\n+        issues: [],\\n+        file_path: outputFile,\\n+        size: buffer.length,\\n+        content_type: contentType,\\n+        cached: false,\\n+      } as unknown as ReviewSummary;\\n+    } catch (error: unknown) {\\n+      clearTimeout(timeoutId);\\n+\\n+      if (error instanceof Error && error.name === 'AbortError') {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_timeout',\\n+              message: `Download timed out after ${timeout}ms`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      return {\\n+        issues: [\\n+          {\\n+            file: 'http_client',\\n+            line: 0,\\n+            ruleId: 'http_client/download_error',\\n+            message: `Failed to download file: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            severity: 'error',\\n+            category: 'logic',\\n+          },\\n+        ],\\n+      };\\n+    }\\n+  }\\n+\\n   getSupportedConfigKeys(): string[] {\\n     return [\\n       'type',\\n@@ -241,7 +480,10 @@ export class HttpClientProvider extends CheckProvider {\\n       'headers',\\n       'body',\\n       'transform',\\n+      'transform_js',\\n       'timeout',\\n+      'output_file',\\n+      'skip_if_exists',\\n       'depends_on',\\n       'on',\\n       'if',\\n@@ -261,6 +503,8 @@ export class HttpClientProvider extends CheckProvider {\\n       'Network access to the endpoint',\\n       'Optional: Transform template for processing response data',\\n       'Optional: Body template for POST/PUT requests',\\n+      'Optional: output_file path to download response to a file',\\n+      'Optional: skip_if_exists (default: true) to enable caching for file downloads',\\n     ];\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/log-check-provider.ts\",\"additions\":19,\"deletions\":8,\"changes\":27,\"patch\":\"diff --git a/src/providers/log-check-provider.ts b/src/providers/log-check-provider.ts\\nindex 9c6332d1..b57f632c 100644\\n--- a/src/providers/log-check-provider.ts\\n+++ b/src/providers/log-check-provider.ts\\n@@ -78,7 +78,8 @@ export class LogCheckProvider extends CheckProvider {\\n       includeDependencies,\\n       includeMetadata,\\n       config.__outputHistory as Map<string, unknown[]> | undefined,\\n-      context\\n+      context,\\n+      config\\n     );\\n \\n     // Render the log message template\\n@@ -100,12 +101,20 @@ export class LogCheckProvider extends CheckProvider {\\n     else if (level === 'debug') logger.debug(logOutput);\\n     else logger.info(logOutput);\\n \\n-    // Return with the log content as custom data for dependent checks\\n-    return {\\n+    // Return with the log content as custom data for dependent checks.\\n+    // For chat-style logs (group: chat), also expose a structured\\n+    // `output.text` field so frontends like Slack can post a clean\\n+    // human-facing message without the level prefix.\\n+    const summary: ReviewSummary & { logOutput: string; output?: unknown } = {\\n       issues: [],\\n-      // Add log output as custom field\\n       logOutput,\\n-    } as ReviewSummary & { logOutput: string };\\n+    };\\n+\\n+    if ((config as any).group === 'chat') {\\n+      (summary as any).output = { text: renderedMessage };\\n+    }\\n+\\n+    return summary;\\n   }\\n \\n   private buildTemplateContext(\\n@@ -115,7 +124,8 @@ export class LogCheckProvider extends CheckProvider {\\n     _includeDependencies: boolean = true,\\n     includeMetadata: boolean = true,\\n     outputHistory?: Map<string, unknown[]>,\\n-    executionContext?: ExecutionContext\\n+    executionContext?: ExecutionContext,\\n+    config?: CheckProviderConfig\\n   ): Record<string, unknown> {\\n     const context: Record<string, unknown> = {};\\n \\n@@ -197,7 +207,8 @@ export class LogCheckProvider extends CheckProvider {\\n     }\\n \\n     // Add workflow inputs if available\\n-    const workflowInputs = executionContext?.workflowInputs || {};\\n+    // Check config first (set by projectWorkflowToGraph), then fall back to executionContext\\n+    const workflowInputs = (config as any)?.workflowInputs || executionContext?.workflowInputs || {};\\n     logger.debug(\\n       `[LogProvider] Adding ${Object.keys(workflowInputs).length} workflow inputs to context`\\n     );\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/script-check-provider.ts\",\"additions\":37,\"deletions\":1,\"changes\":38,\"patch\":\"diff --git a/src/providers/script-check-provider.ts b/src/providers/script-check-provider.ts\\nindex 9825bb69..947dbd73 100644\\n--- a/src/providers/script-check-provider.ts\\n+++ b/src/providers/script-check-provider.ts\\n@@ -59,6 +59,16 @@ export class ScriptCheckProvider extends CheckProvider {\\n       reuseSession?: boolean;\\n     } & import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n+    // Test hook: mock output for this step (short-circuit execution)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = _sessionInfo?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        // Return mock directly as step output\\n+        return { issues: [], output: mock } as ReviewSummary & { output: unknown };\\n+      }\\n+    } catch {}\\n+\\n     const script = String(config.content || '');\\n     const memoryStore = MemoryStore.getInstance();\\n     const ctx = buildProviderTemplateContext(\\n@@ -72,10 +82,36 @@ export class ScriptCheckProvider extends CheckProvider {\\n     // Keep provider quiet by default; no step-specific debug\\n     // (historical ad-hoc logs removed to avoid hardcoding step names).\\n \\n+    // Add workflow inputs to the context\\n+    const inputs = (config as any).workflowInputs || _sessionInfo?.workflowInputs || {};\\n+    (ctx as any).inputs = inputs;\\n+\\n+    // Add environment variables to context (consistent with http-client-provider)\\n+    (ctx as any).env = process.env;\\n+\\n     // Attach synchronous memory ops consistent with memory provider\\n     const { ops, needsSave } = createSyncMemoryOps(memoryStore);\\n     (ctx as any).memory = ops as unknown as Record<string, unknown>;\\n \\n+    // Add helper functions to the context\\n+    (ctx as any).escapeXml = (str: unknown): string => {\\n+      if (str == null) return '';\\n+      return String(str)\\n+        .replace(/&/g, '&amp;')\\n+        .replace(/</g, '&lt;')\\n+        .replace(/>/g, '&gt;')\\n+        .replace(/\\\"/g, '&quot;')\\n+        .replace(/'/g, '&apos;');\\n+    };\\n+\\n+    // Add btoa/atob for base64 encoding/decoding (browser API polyfill)\\n+    (ctx as any).btoa = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'binary').toString('base64');\\n+    };\\n+    (ctx as any).atob = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'base64').toString('binary');\\n+    };\\n+\\n     // Evaluate the script in a secure sandbox (per-execution instance)\\n     const sandbox = this.createSecureSandbox();\\n     let result: unknown;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/workflow-check-provider.ts\",\"additions\":164,\"deletions\":43,\"changes\":207,\"patch\":\"diff --git a/src/providers/workflow-check-provider.ts b/src/providers/workflow-check-provider.ts\\nindex 116d4dff..681703ce 100644\\n--- a/src/providers/workflow-check-provider.ts\\n+++ b/src/providers/workflow-check-provider.ts\\n@@ -67,34 +67,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n   ): Promise<ReviewSummary> {\\n     const cfg = config as CheckProviderConfig & { workflow?: string; config?: string };\\n     const isConfigPathMode = !!cfg.config && !cfg.workflow;\\n+    const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n \\n-    // Test harness support: allow mocking workflow checks as a black box.\\n-    // If a mock is provided for this step, short-circuit nested execution and\\n-    // return a ReviewSummary with optional output field.\\n-    try {\\n-      const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n-      // test-runner passes hooks on execution context\\n-      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n-      if (mock !== undefined) {\\n-        const ms = mock as any;\\n-        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n-        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n-        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n-        const summary: ReviewSummary & { output?: unknown } = {\\n-          issues: issuesArr,\\n-          output: out,\\n-          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n-        } as any;\\n-        return summary;\\n-      }\\n-    } catch {}\\n-\\n-    // Resolve workflow definition\\n+    // Resolve workflow definition FIRST (needed for input preparation and validation)\\n     let workflow: WorkflowDefinition | undefined;\\n     let workflowId = cfg.workflow as string | undefined;\\n \\n     if (isConfigPathMode) {\\n-      const parentCwd = ((context as any)?._parentContext?.workingDirectory ||\\n+      // Use originalWorkingDirectory for config file resolution - workflow configs\\n+      // should be loaded from the original project path, not the sandbox\\n+      const parentCwd = ((context as any)?._parentContext?.originalWorkingDirectory ||\\n+        (context as any)?._parentContext?.workingDirectory ||\\n+        (context as any)?.originalWorkingDirectory ||\\n         (context as any)?.workingDirectory ||\\n         process.cwd()) as string;\\n       workflow = await this.loadWorkflowFromConfigPath(String(cfg.config), parentCwd);\\n@@ -109,9 +93,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       logger.info(`Executing workflow '${workflowId}'`);\\n     }\\n \\n-    // Prepare inputs\\n+    // Prepare inputs - do this BEFORE mock check so we can validate inputs even when mocked\\n+    // This allows tests to assert that the correct inputs were passed to workflow steps\\n     const inputs = await this.prepareInputs(workflow, config, prInfo, dependencyResults);\\n \\n+    // Capture resolved workflow inputs for testing assertions (reuse prompt capture infrastructure)\\n+    // This allows using `prompts` assertions with `provider: 'workflow'` to verify inputs\\n+    try {\\n+      // Serialize inputs to capture for assertions - specifically the context field\\n+      // which contains the rendered template with dependency outputs\\n+      const inputsCapture = Object.entries(inputs)\\n+        .map(([k, v]) => `${k}: ${typeof v === 'string' ? v : JSON.stringify(v)}`)\\n+        .join('\\\\n\\\\n');\\n+      (context as any)?.hooks?.onPromptCaptured?.({\\n+        step: String(stepName),\\n+        provider: 'workflow',\\n+        prompt: inputsCapture,\\n+      });\\n+    } catch {\\n+      // Ignore capture errors - this is only for testing\\n+    }\\n+\\n     // Validate inputs\\n     const validation = this.registry.validateInputs(workflow, inputs);\\n     if (!validation.valid) {\\n@@ -119,6 +121,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       throw new Error(`Invalid workflow inputs: ${errors}`);\\n     }\\n \\n+    // Test harness support: allow mocking workflow checks as a black box.\\n+    // If a mock is provided for this step, short-circuit nested execution and\\n+    // return a ReviewSummary with optional output field.\\n+    // NOTE: This happens AFTER input preparation/validation so tests can assert inputs are correct\\n+    try {\\n+      // test-runner passes hooks on execution context\\n+      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        const ms = mock as any;\\n+        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n+        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n+        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n+        const summary: ReviewSummary & { output?: unknown } = {\\n+          issues: issuesArr,\\n+          output: out,\\n+          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n+        } as any;\\n+        return summary;\\n+      }\\n+    } catch {}\\n+\\n     // Apply overrides to workflow steps if specified\\n     const modifiedWorkflow = this.applyOverrides(workflow, config);\\n \\n@@ -211,6 +234,80 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Extract eventContext for slack/conversation\\n+    const eventContext = config.eventContext || {};\\n+\\n+    // Debug logging for conversation context\\n+    logger.debug(`[WorkflowProvider] prepareInputs for ${workflow.id}`);\\n+    logger.debug(`[WorkflowProvider] eventContext keys: ${Object.keys(eventContext).join(', ') || 'none'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.slack: ${eventContext.slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.conversation: ${(eventContext as any).conversation ? 'present' : 'absent'}`);\\n+\\n+    // Extract slack context (if provided via eventContext.slack)\\n+    const slack = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        const slackCtx = anyCtx?.slack;\\n+        if (slackCtx && typeof slackCtx === 'object') return slackCtx;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Extract unified conversation context across transports (Slack & GitHub)\\n+    const conversation = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        if (anyCtx?.slack?.conversation) return anyCtx.slack.conversation;\\n+        if (anyCtx?.github?.conversation) return anyCtx.github.conversation;\\n+        if (anyCtx?.conversation) return anyCtx.conversation;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Debug logging for extracted context\\n+    logger.debug(`[WorkflowProvider] slack extracted: ${slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] conversation extracted: ${conversation ? 'present' : 'absent'}`);\\n+    if (conversation) {\\n+      logger.debug(`[WorkflowProvider] conversation.messages count: ${Array.isArray((conversation as any).messages) ? (conversation as any).messages.length : 0}`);\\n+    }\\n+\\n+    // Extract output history from config (passed via __outputHistory)\\n+    const outputHistory = (config as any).__outputHistory as Map<string, unknown[]> | undefined;\\n+    const outputs_history: Record<string, unknown[]> = {};\\n+    if (outputHistory) {\\n+      for (const [k, v] of outputHistory.entries()) {\\n+        outputs_history[k] = v;\\n+      }\\n+    }\\n+\\n+    // Build template context with all available data\\n+    // Extract .output from each dependency result so that outputs['step-name'].field works naturally\\n+    // (not outputs['step-name'].output.field)\\n+    const outputsMap: Record<string, unknown> = {};\\n+    logger.debug(`[WorkflowProvider] dependencyResults: ${dependencyResults ? dependencyResults.size : 'undefined'} entries`);\\n+    if (dependencyResults) {\\n+      for (const [key, result] of dependencyResults.entries()) {\\n+        // Extract the output property, or use the whole result if output is undefined\\n+        const extracted = (result as any).output ?? result;\\n+        outputsMap[key] = extracted;\\n+        // Debug: log what we extracted for each dependency\\n+        const extractedKeys = extracted && typeof extracted === 'object' ? Object.keys(extracted).join(', ') : 'not-object';\\n+        logger.debug(`[WorkflowProvider] outputs['${key}']: keys=[${extractedKeys}]`);\\n+      }\\n+    }\\n+    const templateContext = {\\n+      pr: prInfo,\\n+      outputs: outputsMap,\\n+      env: process.env,\\n+      slack,\\n+      conversation,\\n+      outputs_history,\\n+    };\\n+\\n     // Apply user-provided inputs (args)\\n     const userInputs = config.args || config.workflow_inputs; // Support both for compatibility\\n     if (userInputs) {\\n@@ -219,11 +316,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         if (typeof value === 'string') {\\n           // Check if it's a Liquid template\\n           if (value.includes('{{') || value.includes('{%')) {\\n-            inputs[key] = await this.liquid.parseAndRender(value, {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            });\\n+            inputs[key] = await this.liquid.parseAndRender(value, templateContext);\\n+            // Debug: log rendered template value for important inputs\\n+            if (key === 'text' || key === 'question' || key === 'context') {\\n+              const rendered = String(inputs[key]);\\n+              logger.info(`[WorkflowProvider] Rendered '${key}' input (${rendered.length} chars): ${rendered.substring(0, 500)}${rendered.length > 500 ? '...' : ''}`);\\n+            }\\n           } else {\\n             inputs[key] = value;\\n           }\\n@@ -234,11 +332,7 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           inputs[key] = compileAndRun(\\n             sandbox,\\n             exprValue.expression,\\n-            {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            },\\n+            templateContext,\\n             { injectLog: true, logPrefix: `workflow.input.${key}` }\\n           );\\n         } else {\\n@@ -367,6 +461,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n     try {\\n       await childMemory.initialize();\\n     } catch {}\\n+    // Enhanced debug logging for workspace propagation diagnosis\\n+    const parentWorkspace = parentContext?.workspace;\\n+    logger.info(`[WorkflowProvider] Workspace propagation for nested workflow '${workflow.id}':`);\\n+    logger.info(`[WorkflowProvider]   parentContext exists: ${!!parentContext}`);\\n+    logger.info(`[WorkflowProvider]   parentContext.workspace exists: ${!!parentWorkspace}`);\\n+    if (parentWorkspace) {\\n+      logger.info(`[WorkflowProvider]   parentWorkspace.isEnabled(): ${parentWorkspace.isEnabled?.() ?? 'N/A'}`);\\n+      const projectCount = parentWorkspace.listProjects?.()?.length ?? 'N/A';\\n+      logger.info(`[WorkflowProvider]   parentWorkspace project count: ${projectCount}`);\\n+    } else {\\n+      logger.warn(`[WorkflowProvider]   NO WORKSPACE from parent - nested checkouts won't be added to workspace!`);\\n+    }\\n \\n     const childContext = {\\n       mode: 'state-machine' as const,\\n@@ -374,7 +480,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       checks: checksMetadata,\\n       journal: childJournal,\\n       memory: childMemory,\\n+      // For nested workflows we continue to execute inside the same logical\\n+      // working directory as the parent run. When workspace isolation is\\n+      // enabled on the parent engine, its WorkspaceManager is also propagated\\n+      // so that nested checks (AI, git-checkout, etc.) see the same isolated\\n+      // workspace and project symlinks instead of falling back to the Visor\\n+      // repository root.\\n       workingDirectory: parentContext?.workingDirectory || process.cwd(),\\n+      originalWorkingDirectory:\\n+        parentContext?.originalWorkingDirectory ||\\n+        parentContext?.workingDirectory ||\\n+        process.cwd(),\\n+      workspace: parentWorkspace,\\n       // Always use a fresh session for nested workflows to isolate history\\n       sessionId: uuidv4(),\\n       event: parentContext?.event || prInfo.eventType,\\n@@ -500,6 +617,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     } catch {}\\n \\n+    // Create outputs map that directly exposes .output values (not wrapped in { output, issues })\\n+    // This makes {{ outputs['check-name'] }} work naturally without needing .output\\n+    const outputsMap = Object.fromEntries(\\n+      Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n+    );\\n+\\n     for (const output of workflow.outputs) {\\n       if (output.value_js) {\\n         // JavaScript expression\\n@@ -508,10 +631,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           output.value_js,\\n           {\\n             inputs,\\n-            steps: Object.fromEntries(\\n-              Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-            ),\\n-            outputs: flat,\\n+            outputs: outputsMap,\\n+            // Keep 'steps' as alias for backwards compatibility\\n+            steps: outputsMap,\\n             pr: prInfo,\\n           },\\n           { injectLog: true, logPrefix: `workflow.output.${output.name}` }\\n@@ -520,10 +642,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         // Liquid template\\n         outputs[output.name] = await this.liquid.parseAndRender(output.value, {\\n           inputs,\\n-          steps: Object.fromEntries(\\n-            Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-          ),\\n-          outputs: flat,\\n+          outputs: outputsMap,\\n+          // Keep 'steps' as alias for backwards compatibility\\n+          steps: outputsMap,\\n           pr: prInfo,\\n         });\\n       }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/client.ts\",\"additions\":65,\"deletions\":1,\"changes\":66,\"patch\":\"diff --git a/src/slack/client.ts b/src/slack/client.ts\\nindex 292cc358..435f8b0d 100644\\n--- a/src/slack/client.ts\\n+++ b/src/slack/client.ts\\n@@ -142,6 +142,70 @@ export class SlackClient {\\n     }\\n   }\\n \\n+  public readonly files = {\\n+    /**\\n+     * Upload a file to Slack using files.uploadV2 API\\n+     * @param options Upload options including file content, filename, channel, and thread_ts\\n+     */\\n+    uploadV2: async ({\\n+      content,\\n+      filename,\\n+      channel,\\n+      thread_ts,\\n+      title,\\n+      initial_comment,\\n+    }: {\\n+      content: Buffer;\\n+      filename: string;\\n+      channel: string;\\n+      thread_ts?: string;\\n+      title?: string;\\n+      initial_comment?: string;\\n+    }): Promise<{ ok: boolean; file?: { id: string; permalink?: string } }> => {\\n+      try {\\n+        // Step 1: Get upload URL\\n+        const getUrlResp: any = await this.api('files.getUploadURLExternal', {\\n+          filename,\\n+          length: content.length,\\n+        });\\n+        if (!getUrlResp || getUrlResp.ok !== true || !getUrlResp.upload_url) {\\n+          console.warn(`Slack files.getUploadURLExternal failed: ${getUrlResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 2: Upload file content to the URL\\n+        const uploadResp = await fetch(getUrlResp.upload_url, {\\n+          method: 'POST',\\n+          body: content,\\n+        });\\n+        if (!uploadResp.ok) {\\n+          console.warn(`Slack file upload to URL failed: ${uploadResp.status}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 3: Complete the upload and share to channel\\n+        const completeResp: any = await this.api('files.completeUploadExternal', {\\n+          files: [{ id: getUrlResp.file_id, title: title || filename }],\\n+          channel_id: channel,\\n+          thread_ts,\\n+          initial_comment,\\n+        });\\n+        if (!completeResp || completeResp.ok !== true) {\\n+          console.warn(`Slack files.completeUploadExternal failed: ${completeResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        return {\\n+          ok: true,\\n+          file: completeResp.files?.[0] || { id: getUrlResp.file_id },\\n+        };\\n+      } catch (e) {\\n+        console.warn(`Slack file upload failed: ${e instanceof Error ? e.message : String(e)}`);\\n+        return { ok: false };\\n+      }\\n+    },\\n+  };\\n+\\n   getWebClient(): any {\\n     return {\\n       conversations: {\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/markdown.ts\",\"additions\":204,\"deletions\":5,\"changes\":209,\"patch\":\"diff --git a/src/slack/markdown.ts b/src/slack/markdown.ts\\nindex cc63fb5f..9f91c25f 100644\\n--- a/src/slack/markdown.ts\\n+++ b/src/slack/markdown.ts\\n@@ -3,14 +3,191 @@\\n // without pulling in a full Markdown parser.\\n //\\n // Supported conversions:\\n+// - # Header / ## Header  ‚Üí *Header* (bold with visual separation)\\n // - **bold** / __bold__   ‚Üí *bold*\\n // - [label](url)          ‚Üí <url|label>\\n // - ![alt](url)           ‚Üí <url|alt>\\n // - *italic* (inline)     ‚Üí _italic_\\n+// - ```mermaid blocks     ‚Üí rendered to PNG and uploaded to Slack\\n //\\n // Everything else is passed through unchanged; Slack will still render many\\n // Markdown-like constructs (lists, code fences, etc.) natively.\\n \\n+import { spawn } from 'child_process';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n+import * as os from 'os';\\n+\\n+/**\\n+ * Represents an extracted mermaid diagram\\n+ */\\n+export interface MermaidDiagram {\\n+  /** The full match including ```mermaid and ``` */\\n+  fullMatch: string;\\n+  /** The mermaid code content */\\n+  code: string;\\n+  /** Start index in the original text */\\n+  startIndex: number;\\n+  /** End index in the original text */\\n+  endIndex: number;\\n+}\\n+\\n+/**\\n+ * Extract all mermaid code blocks from text\\n+ */\\n+export function extractMermaidDiagrams(text: string): MermaidDiagram[] {\\n+  const diagrams: MermaidDiagram[] = [];\\n+  // Match ```mermaid followed by newline, content, and closing ```\\n+  const regex = /```mermaid\\\\s*\\\\n([\\\\s\\\\S]*?)```/g;\\n+  let match;\\n+  while ((match = regex.exec(text)) !== null) {\\n+    diagrams.push({\\n+      fullMatch: match[0],\\n+      code: match[1].trim(),\\n+      startIndex: match.index,\\n+      endIndex: match.index + match[0].length,\\n+    });\\n+  }\\n+  return diagrams;\\n+}\\n+\\n+/**\\n+ * Render a mermaid diagram to PNG using mmdc CLI (@mermaid-js/mermaid-cli).\\n+ *\\n+ * Requirements:\\n+ * - Node.js and npx must be available in PATH\\n+ * - Network access on first run (npx downloads the package)\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ *\\n+ * On Linux, you may need to install chromium dependencies:\\n+ *   apt-get install -y chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ *\\n+ * On Docker/CI, consider using a base image with puppeteer support or\\n+ * pre-installing @mermaid-js/mermaid-cli globally.\\n+ *\\n+ * @param mermaidCode The mermaid diagram code\\n+ * @returns Buffer containing PNG data, or null if rendering failed\\n+ */\\n+export async function renderMermaidToPng(mermaidCode: string): Promise<Buffer | null> {\\n+  // Create temp files for input and output\\n+  const tmpDir = os.tmpdir();\\n+  const inputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.mmd`);\\n+  const outputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.png`);\\n+\\n+  try {\\n+    // Write mermaid code to temp file\\n+    fs.writeFileSync(inputFile, mermaidCode, 'utf-8');\\n+\\n+    // Detect system chromium for puppeteer (mermaid-cli dependency)\\n+    // Without this, puppeteer may hang trying to download its own chromium\\n+    const chromiumPaths = [\\n+      '/usr/bin/chromium',\\n+      '/usr/bin/chromium-browser',\\n+      '/usr/bin/google-chrome',\\n+      '/usr/bin/chrome',\\n+    ];\\n+    let chromiumPath: string | undefined;\\n+    for (const p of chromiumPaths) {\\n+      if (fs.existsSync(p)) {\\n+        chromiumPath = p;\\n+        break;\\n+      }\\n+    }\\n+\\n+    // Build environment with chromium path if found\\n+    const env = { ...process.env };\\n+    if (chromiumPath) {\\n+      env.PUPPETEER_EXECUTABLE_PATH = chromiumPath;\\n+    }\\n+\\n+    // Run mmdc to render PNG\\n+    const result = await new Promise<{ success: boolean; error?: string }>((resolve) => {\\n+      const proc = spawn('npx', [\\n+        '--yes',\\n+        '@mermaid-js/mermaid-cli',\\n+        '-i', inputFile,\\n+        '-o', outputFile,\\n+        '-e', 'png',\\n+        '-b', 'white',\\n+        '-w', '1200',\\n+      ], {\\n+        timeout: 60000, // 60 second timeout (first run may download packages)\\n+        stdio: ['pipe', 'pipe', 'pipe'],\\n+        env,\\n+      });\\n+\\n+      let stderr = '';\\n+      proc.stderr?.on('data', (data) => {\\n+        stderr += data.toString();\\n+      });\\n+\\n+      proc.on('close', (code) => {\\n+        if (code === 0) {\\n+          resolve({ success: true });\\n+        } else {\\n+          resolve({ success: false, error: stderr || `Exit code ${code}` });\\n+        }\\n+      });\\n+\\n+      proc.on('error', (err) => {\\n+        resolve({ success: false, error: err.message });\\n+      });\\n+    });\\n+\\n+    if (!result.success) {\\n+      console.warn(`Mermaid rendering failed: ${result.error}`);\\n+      return null;\\n+    }\\n+\\n+    // Read the output PNG\\n+    if (!fs.existsSync(outputFile)) {\\n+      console.warn('Mermaid output file not created');\\n+      return null;\\n+    }\\n+\\n+    const pngBuffer = fs.readFileSync(outputFile);\\n+    return pngBuffer;\\n+  } catch (e) {\\n+    console.warn(`Mermaid rendering error: ${e instanceof Error ? e.message : String(e)}`);\\n+    return null;\\n+  } finally {\\n+    // Cleanup temp files\\n+    try {\\n+      if (fs.existsSync(inputFile)) fs.unlinkSync(inputFile);\\n+      if (fs.existsSync(outputFile)) fs.unlinkSync(outputFile);\\n+    } catch {\\n+      // Ignore cleanup errors\\n+    }\\n+  }\\n+}\\n+\\n+/**\\n+ * Replace mermaid blocks in text with a placeholder message\\n+ * @param text Original text\\n+ * @param diagrams Extracted diagrams\\n+ * @param replacement Text to replace each diagram with (or a function that returns replacement for each index)\\n+ */\\n+export function replaceMermaidBlocks(\\n+  text: string,\\n+  diagrams: MermaidDiagram[],\\n+  replacement: string | ((index: number) => string) = '_(See diagram above)_'\\n+): string {\\n+  if (diagrams.length === 0) return text;\\n+\\n+  // Sort by start index descending to replace from end to start (preserves indices)\\n+  const sorted = [...diagrams].sort((a, b) => b.startIndex - a.startIndex);\\n+\\n+  let result = text;\\n+  sorted.forEach((diagram, sortedIndex) => {\\n+    // Calculate original index (since we sorted in reverse)\\n+    const originalIndex = diagrams.length - 1 - sortedIndex;\\n+    const rep = typeof replacement === 'function' ? replacement(originalIndex) : replacement;\\n+    result = result.slice(0, diagram.startIndex) + rep + result.slice(diagram.endIndex);\\n+  });\\n+\\n+  return result;\\n+}\\n+\\n export function markdownToSlack(text: string): string {\\n   if (!text || typeof text !== 'string') return '';\\n \\n@@ -33,7 +210,7 @@ export function markdownToSlack(text: string): string {\\n   out = out.replace(/\\\\*\\\\*([^*]+)\\\\*\\\\*/g, (_m, inner: string) => `*${inner}*`);\\n   out = out.replace(/__([^_]+)__/g, (_m, inner: string) => `*${inner}*`);\\n \\n-  // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation).\\n+  // Process lines for headers and bullet lists.\\n   // Slack's mrkdwn handles \\\"‚Ä¢\\\" bullets more naturally than raw \\\"-\\\" Markdown.\\n   const lines = out.split(/\\\\r?\\\\n/);\\n   let inCodeBlock = false;\\n@@ -46,9 +223,31 @@ export function markdownToSlack(text: string): string {\\n       continue;\\n     }\\n     if (inCodeBlock) continue;\\n-    const match = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n-    if (match) {\\n-      const [, indent, , rest] = match;\\n+\\n+    // Headers: # Header ‚Üí *Header* (Slack doesn't have native headers)\\n+    // Match 1-6 # at start of line, followed by space and text\\n+    const headerMatch = /^(#{1,6})\\\\s+(.+)$/.exec(trimmed);\\n+    if (headerMatch) {\\n+      const [, hashes, headerText] = headerMatch;\\n+      // For h1/h2, add extra emphasis with newline before (if not first line\\n+      // and previous line is not empty/header/code-fence)\\n+      const prevLine = i > 0 ? lines[i - 1].trim() : '';\\n+      const prevIsHeaderOrFence =\\n+        /^#{1,6}\\\\s+/.test(prevLine) ||\\n+        /^\\\\*[^*]+\\\\*$/.test(prevLine) ||\\n+        /^```/.test(prevLine);\\n+      if (hashes.length <= 2 && i > 0 && prevLine !== '' && !prevIsHeaderOrFence) {\\n+        lines[i] = `\\\\n*${headerText.trim()}*`;\\n+      } else {\\n+        lines[i] = `*${headerText.trim()}*`;\\n+      }\\n+      continue;\\n+    }\\n+\\n+    // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation)\\n+    const bulletMatch = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n+    if (bulletMatch) {\\n+      const [, indent, , rest] = bulletMatch;\\n       lines[i] = `${indent}‚Ä¢ ${rest}`;\\n     }\\n   }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/prompt-state.ts\",\"additions\":19,\"deletions\":2,\"changes\":21,\"patch\":\"diff --git a/src/slack/prompt-state.ts b/src/slack/prompt-state.ts\\nindex a13233b2..544777c6 100644\\n--- a/src/slack/prompt-state.ts\\n+++ b/src/slack/prompt-state.ts\\n@@ -95,7 +95,10 @@ export class PromptStateManager {\\n   setFirstMessage(channel: string, threadTs: string, text: string): void {\\n     const key = this.key(channel, threadTs);\\n     if (!text || !text.trim()) return;\\n-    if (!this.firstMessage.has(key)) {\\n+    const existing = this.firstMessage.get(key);\\n+    // Only set if: no entry exists OR the existing entry was already consumed\\n+    // This allows new messages to be captured after a resume cycle\\n+    if (!existing || existing.consumed) {\\n       this.firstMessage.set(key, { text, consumed: false });\\n     }\\n   }\\n@@ -130,6 +133,20 @@ export class PromptStateManager {\\n         removed++;\\n       }\\n     }\\n+    // Also clean up stale firstMessage entries (consumed entries older than TTL)\\n+    // Keep unconsumed entries to avoid losing user messages\\n+    for (const [key] of this.firstMessage.entries()) {\\n+      const waitingInfo = this.waiting.get(key);\\n+      // If no corresponding waiting entry exists and the firstMessage was consumed,\\n+      // the conversation is likely complete - safe to remove\\n+      if (!waitingInfo) {\\n+        const entry = this.firstMessage.get(key);\\n+        if (entry?.consumed) {\\n+          this.firstMessage.delete(key);\\n+          removed++;\\n+        }\\n+      }\\n+    }\\n     if (removed) {\\n       try {\\n         logger.info(`[prompt-state] cleanup removed ${removed} entries`);\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/socket-runner.ts\",\"additions\":56,\"deletions\":32,\"changes\":88,\"patch\":\"diff --git a/src/slack/socket-runner.ts b/src/slack/socket-runner.ts\\nindex d1681a03..5b461d22 100644\\n--- a/src/slack/socket-runner.ts\\n+++ b/src/slack/socket-runner.ts\\n@@ -7,6 +7,7 @@ import { SlackAdapter } from './adapter';\\n import { CachePrewarmer } from './cache-prewarmer';\\n import { RateLimiter, type RateLimitConfig } from './rate-limiter';\\n import type { SlackBotConfig } from '../types/bot';\\n+import { withActiveSpan } from '../telemetry/trace-helpers';\\n \\n type SlackSocketConfig = {\\n   appToken?: string; // xapp- token\\n@@ -30,6 +31,7 @@ export class SlackSocketRunner {\\n   private botUserId?: string;\\n   private processedKeys: Map<string, number> = new Map();\\n   private adapter?: SlackAdapter;\\n+  private retryCount = 0;\\n \\n   constructor(engine: StateMachineExecutionEngine, cfg: VisorConfig, opts: SlackSocketConfig) {\\n     const app = opts.appToken || process.env.SLACK_APP_TOKEN || '';\\n@@ -107,7 +109,10 @@ export class SlackSocketRunner {\\n \\n   private async connect(url: string): Promise<void> {\\n     this.ws = new WebSocket(url);\\n-    this.ws.on('open', () => logger.info('[SlackSocket] WebSocket connected'));\\n+    this.ws.on('open', () => {\\n+      this.retryCount = 0; // Reset on successful connection\\n+      logger.info('[SlackSocket] WebSocket connected');\\n+    });\\n     this.ws.on('close', (code, reason) => {\\n       logger.warn(`[SlackSocket] WebSocket closed: ${code} ${reason}`);\\n       setTimeout(() => this.restart().catch(() => {}), 1000);\\n@@ -123,7 +128,13 @@ export class SlackSocketRunner {\\n       const url = await this.openConnection();\\n       await this.connect(url);\\n     } catch (e) {\\n-      logger.error(`[SlackSocket] Restart failed: ${e instanceof Error ? e.message : e}`);\\n+      this.retryCount++;\\n+      // Exponential backoff: 2s, 4s, 8s, 16s, 32s, capped at 60s\\n+      const delay = Math.min(2000 * Math.pow(2, this.retryCount - 1), 60000);\\n+      logger.error(\\n+        `[SlackSocket] Restart failed (attempt ${this.retryCount}), retrying in ${Math.round(delay / 1000)}s: ${e instanceof Error ? e.message : e}`\\n+      );\\n+      setTimeout(() => this.restart().catch(() => {}), delay);\\n     }\\n   }\\n \\n@@ -296,36 +307,49 @@ export class SlackSocketRunner {\\n           });\\n         }\\n       } catch {}\\n-      if (path) {\\n-        try {\\n-          const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n-          const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n-          logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n-          await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n-            webhookContext: { webhookData: map, eventType: 'manual' },\\n-            debug: process.env.VISOR_DEBUG === 'true',\\n-          });\\n-          if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n-          return; // resume path handled\\n-        } catch (e) {\\n-          logger.warn(\\n-            `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n-              e instanceof Error ? e.message : String(e)\\n-            }`\\n-          );\\n-        }\\n-      }\\n+      try {\\n+        await withActiveSpan(\\n+          'visor.run',\\n+          {\\n+            'visor.run.source': 'slack',\\n+            'slack.event.type': String(type || ''),\\n+            'slack.channel': channelId,\\n+            'slack.thread_ts': threadTs,\\n+          },\\n+          async () => {\\n+            if (path) {\\n+              try {\\n+                const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n+                const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n+                logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n+                await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n+                  webhookContext: { webhookData: map, eventType: 'manual' },\\n+                  debug: process.env.VISOR_DEBUG === 'true',\\n+                });\\n+                return;\\n+              } catch (e) {\\n+                logger.warn(\\n+                  `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n+                    e instanceof Error ? e.message : String(e)\\n+                  }`\\n+                );\\n+              }\\n+            }\\n \\n-      // Cold run (no snapshot)\\n-      await runEngine.executeChecks({\\n-        checks: allChecks,\\n-        showDetails: true,\\n-        outputFormat: 'json',\\n-        config: cfgForRun,\\n-        webhookContext: { webhookData: map, eventType: 'manual' },\\n-        debug: process.env.VISOR_DEBUG === 'true',\\n-      } as any);\\n-      if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+            // Cold run (no snapshot)\\n+            await runEngine.executeChecks({\\n+              checks: allChecks,\\n+              showDetails: true,\\n+              outputFormat: 'json',\\n+              config: cfgForRun,\\n+              webhookContext: { webhookData: map, eventType: 'manual' },\\n+              debug: process.env.VISOR_DEBUG === 'true',\\n+            } as any);\\n+          }\\n+        );\\n+      } finally {\\n+        if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+      }\\n     } catch (e) {\\n       logger.error(\\n         `[SlackSocket] Engine execution failed: ${e instanceof Error ? e.message : String(e)}`\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/context/build-engine-context.ts\",\"additions\":11,\"deletions\":4,\"changes\":15,\"patch\":\"diff --git a/src/state-machine/context/build-engine-context.ts b/src/state-machine/context/build-engine-context.ts\\nindex 149c73bf..45e63b6a 100644\\n--- a/src/state-machine/context/build-engine-context.ts\\n+++ b/src/state-machine/context/build-engine-context.ts\\n@@ -57,7 +57,8 @@ export function buildEngineContextForRun(\\n       ) as EventTrigger[],\\n       group: checkConfig.group,\\n       providerType: checkConfig.type || 'ai',\\n-      dependencies: checkConfig.depends_on || [],\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: Array.isArray(checkConfig.depends_on) ? checkConfig.depends_on : checkConfig.depends_on ? [checkConfig.depends_on] : [],\\n     };\\n   }\\n \\n@@ -134,11 +135,14 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n   const originalPath = context.workingDirectory || process.cwd();\\n \\n   try {\\n+    // Check if workspace should be kept (for debugging)\\n+    const keepWorkspace = process.env.VISOR_KEEP_WORKSPACE === 'true';\\n+\\n     // Create workspace manager\\n     const workspace = WorkspaceManager.getInstance(context.sessionId, originalPath, {\\n       enabled: true,\\n-      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH,\\n-      cleanupOnExit: workspaceConfig?.cleanup_on_exit !== false,\\n+      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH || '/tmp/visor-workspaces',\\n+      cleanupOnExit: keepWorkspace ? false : workspaceConfig?.cleanup_on_exit !== false,\\n     });\\n \\n     // Initialize workspace (creates main project worktree)\\n@@ -151,6 +155,9 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n \\n     logger.info(`[Workspace] Initialized workspace: ${info.workspacePath}`);\\n     logger.debug(`[Workspace] Main project at: ${info.mainProjectPath}`);\\n+    if (keepWorkspace) {\\n+      logger.info(`[Workspace] Keeping workspace after execution (--keep-workspace)`);\\n+    }\\n \\n     return context;\\n   } catch (error) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/execution-invoker.ts\",\"additions\":84,\"deletions\":4,\"changes\":88,\"patch\":\"diff --git a/src/state-machine/dispatch/execution-invoker.ts b/src/state-machine/dispatch/execution-invoker.ts\\nindex eb7587d3..798ca8e4 100644\\n--- a/src/state-machine/dispatch/execution-invoker.ts\\n+++ b/src/state-machine/dispatch/execution-invoker.ts\\n@@ -356,10 +356,17 @@ export async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -413,6 +420,31 @@ export async function executeSingleCheck(\\n     }\\n   }\\n \\n+  // Banner-style log when a check actually starts executing. This is emitted\\n+  // after all gating (if/depends_on) has passed so it only appears for real\\n+  // runs, and gives a clear visual separator in the logs between checks.\\n+  try {\\n+    const wave = state.wave;\\n+    const level = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${level}) ‚îÅ‚îÅ‚îÅ`;\\n+\\n+    // When running in a TTY, colour the entire banner line for extra\\n+    // visibility; keep plain text for JSON/SARIF or non-TTY environments.\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   let forEachParent: string | undefined;\\n   let forEachItems: unknown[] | undefined;\\n   for (const depId of depList) {\\n@@ -563,13 +595,56 @@ export async function executeSingleCheck(\\n       files: [],\\n       commits: [],\\n     };\\n+    // Derive AI session reuse context for this check (self-mode only for now).\\n+    // When reuse_ai_session: 'self' is configured, look up the last root-scope\\n+    // journal entry for this check in the current engine session and expose its\\n+    // sessionId via ExecutionContext so ai-check-provider can call\\n+    // executeReviewWithSessionReuse() against the same ProbeAgent session.\\n+    let parentSessionId: string | undefined;\\n+    let reuseSession = false;\\n+    try {\\n+      const reuseCfg: unknown = (checkConfig as any).reuse_ai_session;\\n+      if (reuseCfg === 'self') {\\n+        const snapshotId = context.journal.beginSnapshot();\\n+        const visible = context.journal.readVisible(\\n+          context.sessionId,\\n+          snapshotId,\\n+          context.event as any\\n+        );\\n+        // Prefer the most recent root-scope result for this check\\n+        const prior = visible.filter(\\n+          e => e.checkId === checkId && (!e.scope || e.scope.length === 0)\\n+        );\\n+        if (prior.length > 0) {\\n+          const last = prior[prior.length - 1];\\n+          const sess = (last.result as any)?.sessionId;\\n+          if (typeof sess === 'string' && sess.length > 0) {\\n+            parentSessionId = sess;\\n+            reuseSession = true;\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only ‚Äì fall back to normal (non-reuse) execution on error.\\n+      parentSessionId = undefined;\\n+      reuseSession = false;\\n+    }\\n+\\n     const executionContext = {\\n       ...context.executionContext,\\n       _engineMode: context.mode,\\n       _parentContext: context,\\n       _parentState: state,\\n+      // Explicitly propagate workspace reference for nested workflows\\n+      workspace: context.workspace,\\n     };\\n \\n+    // Attach session reuse hints for providers that support them (AI, Claude Code, etc).\\n+    if (reuseSession && parentSessionId) {\\n+      (executionContext as any).parentSessionId = parentSessionId;\\n+      (executionContext as any).reuseSession = true;\\n+    }\\n+\\n     // Handle on_init lifecycle hook BEFORE main execution\\n     if (checkConfig.on_init) {\\n       try {\\n@@ -613,7 +688,12 @@ export async function executeSingleCheck(\\n \\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/foreach-processor.ts\",\"additions\":27,\"deletions\":2,\"changes\":29,\"patch\":\"diff --git a/src/state-machine/dispatch/foreach-processor.ts b/src/state-machine/dispatch/foreach-processor.ts\\nindex 336bf9df..ab630986 100644\\n--- a/src/state-machine/dispatch/foreach-processor.ts\\n+++ b/src/state-machine/dispatch/foreach-processor.ts\\n@@ -77,6 +77,26 @@ export async function executeCheckWithForEachItems(\\n   const allContents: string[] = [];\\n   const perIterationDurations: number[] = [];\\n \\n+  // Emit a banner for the forEach parent so logs clearly show when we are\\n+  // entering the aggregated execution for that step.\\n+  try {\\n+    const wave = state.wave;\\n+    const lvl = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${lvl}, forEach parent) ‚îÅ‚îÅ‚îÅ`;\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   for (let itemIndex = 0; itemIndex < forEachItems.length; itemIndex++) {\\n     const iterationStartMs = Date.now();\\n     const scope: Array<{ check: string; index: number }> = [\\n@@ -248,7 +268,12 @@ export async function executeCheckWithForEachItems(\\n \\n       const result = await withActiveSpan(\\n         `visor.check.${checkId}`,\\n-        { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+        {\\n+          'visor.check.id': checkId,\\n+          'visor.check.type': providerType,\\n+          session_id: context.sessionId,\\n+          wave: state.wave,\\n+        },\\n         async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n       );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/stats-manager.ts\",\"additions\":21,\"deletions\":1,\"changes\":22,\"patch\":\"diff --git a/src/state-machine/dispatch/stats-manager.ts b/src/state-machine/dispatch/stats-manager.ts\\nindex 8d04436e..2f317b59 100644\\n--- a/src/state-machine/dispatch/stats-manager.ts\\n+++ b/src/state-machine/dispatch/stats-manager.ts\\n@@ -44,6 +44,26 @@ export function updateStats(\\n       issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n     };\\n \\n+    // Respect explicit skip markers on the result (set by LevelDispatch).\\n+    const skippedMarker = (result as any).__skipped;\\n+    if (skippedMarker) {\\n+      stats.skipped = true;\\n+      stats.skipReason =\\n+        typeof skippedMarker === 'string'\\n+          ? (skippedMarker as any)\\n+          : (stats.skipReason as any) || 'if_condition';\\n+      // A skipped result should not contribute to run counters; reset\\n+      // them explicitly to guard against any prior values lingering\\n+      // from earlier runs of the same check.\\n+      stats.totalRuns = 0;\\n+      stats.successfulRuns = 0;\\n+      stats.failedRuns = 0;\\n+      stats.skippedRuns++;\\n+      // Do not count this as a run; keep totalRuns/failed/success unchanged.\\n+      state.stats.set(checkId, stats);\\n+      continue;\\n+    }\\n+\\n     // If this check was previously marked as skipped and now executed,\\n     // clear the skipped flag and any skip counters to ensure the run is visible.\\n     if (stats.skipped) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/runner.ts\",\"additions\":10,\"deletions\":7,\"changes\":17,\"patch\":\"diff --git a/src/state-machine/runner.ts b/src/state-machine/runner.ts\\nindex ee86b876..a13a5efd 100644\\n--- a/src/state-machine/runner.ts\\n+++ b/src/state-machine/runner.ts\\n@@ -124,14 +124,17 @@ export class StateMachineRunner {\\n    */\\n   private async executeState(state: EngineState): Promise<void> {\\n     // M4: Wrap state execution in OTEL span\\n+    const attrs: Record<string, unknown> = {\\n+      state: state,\\n+      engine_mode: this.context.mode,\\n+      wave: this.state.wave,\\n+      session_id: this.context.sessionId,\\n+    };\\n+    const waveKind = (this.state as any)?.flags?.waveKind;\\n+    if (waveKind) attrs.wave_kind = waveKind;\\n     return withActiveSpan(\\n       `engine.state.${state.toLowerCase()}`,\\n-      {\\n-        state: state,\\n-        engine_mode: this.context.mode,\\n-        wave: this.state.wave,\\n-        session_id: this.context.sessionId,\\n-      },\\n+      attrs,\\n       async () => {\\n         try {\\n           switch (state) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/level-dispatch.ts\",\"additions\":100,\"deletions\":15,\"changes\":115,\"patch\":\"diff --git a/src/state-machine/states/level-dispatch.ts b/src/state-machine/states/level-dispatch.ts\\nindex 5588b0f5..42ee383e 100644\\n--- a/src/state-machine/states/level-dispatch.ts\\n+++ b/src/state-machine/states/level-dispatch.ts\\n@@ -26,7 +26,7 @@ import type { CheckExecutionStats } from '../../types/execution';\\n import type { CheckProviderConfig } from '../../providers/check-provider.interface';\\n import type { CheckConfig } from '../../types/config';\\n import { handleRouting, checkLoopBudget } from './routing';\\n-import { withActiveSpan } from '../../telemetry/trace-helpers';\\n+import { withActiveSpan, setSpanAttributes } from '../../telemetry/trace-helpers';\\n import { emitMermaidFromMarkdown } from '../../utils/mermaid-telemetry';\\n import { emitNdjsonSpanWithEvents, emitNdjsonFallback } from '../../telemetry/fallback-ndjson';\\n import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n@@ -121,12 +121,13 @@ async function evaluateIfCondition(\\n         return false;\\n       }\\n     })();\\n+    // Steps with dependencies should always see outputs from all completed steps.\\n     // In forward-run waves (from on_success/on_fail goto), guards should see the\\n     // latest global outputs even if the check has no explicit dependencies.\\n     // In wave-retry (from on_finish), restrict to checks with dependencies to\\n     // avoid wrongly skipping top-level prompts like 'ask'.\\n     const useGlobalOutputs =\\n-      (useGlobalOutputsFlag && waveKind === 'forward') || (useGlobalOutputsFlag && hasDeps);\\n+      hasDeps || (useGlobalOutputsFlag && waveKind === 'forward');\\n \\n     if (useGlobalOutputs) {\\n       // Forward-run wave: allow guards to consult latest outputs from the entire\\n@@ -199,6 +200,7 @@ async function evaluateIfCondition(\\n       baseBranch: (context.prInfo as any)?.baseBranch,\\n       filesChanged: context.prInfo?.files?.map(f => f.filename),\\n       environment: envSnapshot,\\n+      workflowInputs: (context.config as any).workflow_inputs || {},\\n     };\\n \\n     const shouldRun = await evaluator.evaluateIfCondition(checkId, ifExpression, contextData);\\n@@ -238,6 +240,7 @@ export async function handleLevelDispatch(\\n   // Update current level tracking\\n   state.currentLevel = level.level;\\n   state.currentLevelChecks = new Set(level.parallel);\\n+  setSpanAttributes({ level_size: level.parallel.length });\\n \\n   // Emit level ready event\\n   emitEvent({ type: 'LevelReady', level, wave: state.wave });\\n@@ -689,6 +692,39 @@ async function executeCheckWithForEachItems(\\n         }\\n       } catch {}\\n \\n+      // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+      // The socket-runner stores conversation data in webhookData under the endpoint key\\n+      try {\\n+        const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+        const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+        if (context.debug) {\\n+          logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+        }\\n+        if (webhookData && webhookData.size > 0) {\\n+          // Find the payload with slack_conversation\\n+          for (const payload of webhookData.values()) {\\n+            const slackConv = (payload as any)?.slack_conversation;\\n+            if (slackConv) {\\n+              // Build slack context with event and conversation\\n+              const event = (payload as any)?.event;\\n+              const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+              if (context.debug) {\\n+                logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+              }\\n+              (providerConfig as any).eventContext = {\\n+                ...(providerConfig as any).eventContext,\\n+                slack: {\\n+                  event: event || {},\\n+                  conversation: slackConv,\\n+                },\\n+                conversation: slackConv, // Also expose at top level for convenience\\n+              };\\n+              break;\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n+\\n       // Build dependency results with scope\\n       const dependencyResults = buildDependencyResultsWithScope(\\n         checkId,\\n@@ -820,15 +856,17 @@ async function executeCheckWithForEachItems(\\n       } catch {}\\n \\n       // Execute provider with telemetry\\n-      const itemResult = await withActiveSpan(\\n-        `visor.check.${checkId}`,\\n-        {\\n-          'visor.check.id': checkId,\\n-          'visor.check.type': providerType,\\n-          'visor.foreach.index': itemIndex,\\n-        },\\n-        async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n-      );\\n+    const itemResult = await withActiveSpan(\\n+      `visor.check.${checkId}`,\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        'visor.foreach.index': itemIndex,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n+      async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n+    );\\n \\n       // Enrich issues\\n       const enrichedIssues = (itemResult.issues || []).map((issue: ReviewIssue) => ({\\n@@ -942,6 +980,7 @@ async function executeCheckWithForEachItems(\\n             const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n               previousResults: dependencyResults as any,\\n               event: context.event || 'manual',\\n+              output: output, // Pass the iteration output for guarantee evaluation\\n             } as any);\\n             if (!holds) {\\n               const issue: ReviewIssue = {\\n@@ -1618,10 +1657,17 @@ async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -1923,6 +1969,39 @@ async function executeSingleCheck(\\n       }\\n     } catch {}\\n \\n+    // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+    // The socket-runner stores conversation data in webhookData under the endpoint key\\n+    try {\\n+      const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+      const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+      if (context.debug) {\\n+        logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+      }\\n+      if (webhookData && webhookData.size > 0) {\\n+        // Find the payload with slack_conversation\\n+        for (const payload of webhookData.values()) {\\n+          const slackConv = (payload as any)?.slack_conversation;\\n+          if (slackConv) {\\n+            // Build slack context with event and conversation\\n+            const event = (payload as any)?.event;\\n+            const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+            if (context.debug) {\\n+              logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+            }\\n+            (providerConfig as any).eventContext = {\\n+              ...(providerConfig as any).eventContext,\\n+              slack: {\\n+                event: event || {},\\n+                conversation: slackConv,\\n+              },\\n+              conversation: slackConv, // Also expose at top level for convenience\\n+            };\\n+            break;\\n+          }\\n+        }\\n+      }\\n+    } catch {}\\n+\\n     // Build dependency results\\n     const dependencyResults = buildDependencyResults(checkId, checkConfig, context, state);\\n \\n@@ -2016,7 +2095,12 @@ async function executeSingleCheck(\\n     // Execute provider with telemetry\\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n@@ -2098,6 +2182,7 @@ async function executeSingleCheck(\\n           const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n             previousResults: dependencyResults as any,\\n             event: context.event || 'manual',\\n+            output: enrichedResult.output,\\n           } as any);\\n           if (!holds) {\\n             const issue: ReviewIssue = {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/wave-planning.ts\",\"additions\":10,\"deletions\":3,\"changes\":13,\"patch\":\"diff --git a/src/state-machine/states/wave-planning.ts b/src/state-machine/states/wave-planning.ts\\nindex 7551d55c..7b42f043 100644\\n--- a/src/state-machine/states/wave-planning.ts\\n+++ b/src/state-machine/states/wave-planning.ts\\n@@ -351,8 +351,11 @@ export async function handleWavePlanning(\\n     for (const id of checksToRun) {\\n       // Only include dependencies that are within the same subset (often none).\\n       const cfg = context.config.checks?.[id];\\n-      const deps = (cfg?.depends_on || []).filter((d: string) => checksToRun.includes(d));\\n-      subDeps[id] = deps as string[];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = cfg?.depends_on;\\n+      const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n+      const deps = depsArray.filter((d: string) => checksToRun.includes(d));\\n+      subDeps[id] = deps;\\n     }\\n \\n     const subGraph = DependencyResolver.buildDependencyGraph(subDeps);\\n@@ -410,6 +413,10 @@ export async function handleWavePlanning(\\n     // Initialize current wave state\\n     (state as any).currentWaveCompletions = new Set<string>();\\n     (state as any).failedChecks = new Set<string>();\\n+    try {\\n+      (state as any).flags = (state as any).flags || {};\\n+      (state as any).flags.waveKind = 'initial';\\n+    } catch {}\\n   }\\n \\n   // Check if there are levels to execute\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/workflow-projection.ts\",\"additions\":3,\"deletions\":2,\"changes\":5,\"patch\":\"diff --git a/src/state-machine/workflow-projection.ts b/src/state-machine/workflow-projection.ts\\nindex b2d5eb24..2bcfd858 100644\\n--- a/src/state-machine/workflow-projection.ts\\n+++ b/src/state-machine/workflow-projection.ts\\n@@ -53,7 +53,8 @@ export function projectWorkflowToGraph(\\n       triggers: step.on || workflow.on || [],\\n       group: step.group,\\n       providerType: step.type || 'ai',\\n-      dependencies: (step.depends_on || []).map(dep => dep),\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: (Array.isArray(step.depends_on) ? step.depends_on : step.depends_on ? [step.depends_on] : []).map((dep: string) => dep),\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/flow-stage.ts\",\"additions\":37,\"deletions\":4,\"changes\":41,\"patch\":\"diff --git a/src/test-runner/core/flow-stage.ts b/src/test-runner/core/flow-stage.ts\\nindex a14256c2..a587f9f1 100644\\n--- a/src/test-runner/core/flow-stage.ts\\n+++ b/src/test-runner/core/flow-stage.ts\\n@@ -352,10 +352,30 @@ export class FlowStage {\\n       } catch {}\\n       // Prefer authoritative counts from res.statistics when available, then fall back to deltas/inferred\\n       const fromResStats: Record<string, number> = {};\\n+      const resSkipped: Record<string, boolean> = {};\\n       try {\\n         for (const s of (res.statistics?.checks || []) as any[]) {\\n           const n = (s && s.checkName) || '';\\n-          if (typeof n === 'string' && n) fromResStats[n] = (s.totalRuns || 0) as number;\\n+          if (typeof n === 'string' && n) {\\n+            fromResStats[n] = (s.totalRuns || 0) as number;\\n+            resSkipped[n] = !!(s as any).skipped || !!(s as any).skipReason;\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      // Steps that have explicit expectations in this stage. We will use\\n+      // this to avoid inflating run counts (via parent-alignment heuristics)\\n+      // for checks that are not even expected in this stage.\\n+      const expectedSteps = new Set<string>();\\n+      const expectedZero = new Set<string>();\\n+      try {\\n+        const expCalls = ((stage.expect || {}).calls || []) as Array<{ step?: string }>;\\n+        for (const c of expCalls) {\\n+          if (c && typeof c.step === 'string' && c.step) expectedSteps.add(c.step);\\n+          try {\\n+            if (c && typeof c.step === 'string' && c.step && (c as any).exactly === 0)\\n+              expectedZero.add(c.step);\\n+          } catch {}\\n         }\\n       } catch {}\\n \\n@@ -410,10 +430,15 @@ export class FlowStage {\\n           const base = stageOnly ? 0 : statBase[name] || 0;\\n           const d = Math.max(0, resTotal - base);\\n           runs = d > 0 ? d : 0;\\n+          // If the engine marked this check as skipped, force runs to 0\\n+          // so parent-alignment heuristics below do not inflate it.\\n+          if (resSkipped[name]) {\\n+            runs = 0;\\n+          }\\n         } else {\\n           runs = deltaMap[name] !== undefined ? deltaMap[name] : inferred;\\n         }\\n-        if (runs === 0 && presentInResults.has(name)) runs = 1;\\n+        if (runs === 0 && presentInResults.has(name) && !resSkipped[name]) runs = 1;\\n         if (!isForEachLike && histRuns > 0) runs = histRuns;\\n         // Only use per-item history counts for non-forEach checks. For forEach parents,\\n         // use aggregated totals from res.statistics to reflect number of parent executions.\\n@@ -442,7 +467,15 @@ export class FlowStage {\\n             parentMax = Math.max(parentMax, dP);\\n           }\\n           // Apply only for non-forEach checks with no observable history in this stage\\n-          if (!isForEachLike && histRuns === 0 && histPerItemRuns === 0 && parentMax > 0) {\\n+          if (\\n+            !isForEachLike &&\\n+            histRuns === 0 &&\\n+            histPerItemRuns === 0 &&\\n+            parentMax > 0 &&\\n+            !resSkipped[name] &&\\n+            (expectedSteps.size === 0 || expectedSteps.has(name)) &&\\n+            !expectedZero.has(name)\\n+          ) {\\n             runs = Math.max(runs, parentMax);\\n           }\\n         } catch {}\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/test-execution-wrapper.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/test-runner/core/test-execution-wrapper.ts b/src/test-runner/core/test-execution-wrapper.ts\\nindex 254d0970..d0333152 100644\\n--- a/src/test-runner/core/test-execution-wrapper.ts\\n+++ b/src/test-runner/core/test-execution-wrapper.ts\\n@@ -31,6 +31,8 @@ export class TestExecutionWrapper {\\n       const prev: any = (this.engine as any).executionContext || {};\\n       const merged = {\\n         ...prev,\\n+        // Inject workflow inputs for template access via {{ inputs.* }}\\n+        workflowInputs: cfg.workflow_inputs || prev.workflowInputs || {},\\n         mode: {\\n           ...(prev.mode || {}),\\n           test: true,\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/evaluators.ts\",\"additions\":67,\"deletions\":2,\"changes\":69,\"patch\":\"diff --git a/src/test-runner/evaluators.ts b/src/test-runner/evaluators.ts\\nindex 4843163a..75e9c174 100644\\n--- a/src/test-runner/evaluators.ts\\n+++ b/src/test-runner/evaluators.ts\\n@@ -252,6 +252,69 @@ export function evaluatePrompts(\\n   }\\n }\\n \\n+/**\\n+ * Evaluate workflow_output assertions against computed workflow outputs.\\n+ * Similar to evaluateOutputs but tests workflow-level outputs (defined in outputs: section)\\n+ * rather than step outputs.\\n+ */\\n+export function evaluateWorkflowOutputs(\\n+  errors: string[],\\n+  expect: ExpectBlock,\\n+  workflowOutputs: Record<string, unknown> | undefined\\n+): void {\\n+  const expectations = (expect as any).workflow_output;\\n+  if (!Array.isArray(expectations) || expectations.length === 0) return;\\n+  if (!workflowOutputs) {\\n+    errors.push('workflow_output assertions present but no workflow outputs computed');\\n+    return;\\n+  }\\n+\\n+  for (const o of expectations) {\\n+    const path = o.path as string;\\n+    if (!path) {\\n+      errors.push('workflow_output assertion missing path');\\n+      continue;\\n+    }\\n+    const v = deepGet(workflowOutputs, path);\\n+    if (o.equals !== undefined && !deepEqual(v, o.equals)) {\\n+      errors.push(\\n+        `Workflow output ${path} expected ${JSON.stringify(o.equals)} but got ${JSON.stringify(v)}`\\n+      );\\n+    }\\n+    if (o.equalsDeep !== undefined && !deepEqual(v, o.equalsDeep)) {\\n+      errors.push(`Workflow output ${path} deepEquals failed`);\\n+    }\\n+    if (o.matches && !parseRegex(o.matches).test(String(v))) {\\n+      errors.push(`Workflow output ${path} does not match ${o.matches}`);\\n+    }\\n+    if (o.contains) {\\n+      const contents = Array.isArray(o.contains) ? o.contains : [o.contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (!strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} expected to contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.not_contains) {\\n+      const contents = Array.isArray(o.not_contains) ? o.not_contains : [o.not_contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} should not contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.contains_unordered) {\\n+      if (!Array.isArray(v)) {\\n+        errors.push(`Workflow output ${path} not an array for contains_unordered`);\\n+      } else if (!containsUnordered(v as unknown[], o.contains_unordered)) {\\n+        errors.push(`Workflow output ${path} missing elements (unordered)`);\\n+      }\\n+    }\\n+  }\\n+}\\n+\\n export function evaluateOutputs(\\n   errors: string[],\\n   expect: ExpectBlock,\\n@@ -332,7 +395,8 @@ export function evaluateCase(\\n   strict: boolean,\\n   promptsByStep: Record<string, string[]>,\\n   _results: GroupedResults,\\n-  outputHistory: Record<string, unknown[]>\\n+  outputHistory: Record<string, unknown[]>,\\n+  workflowOutputs?: Record<string, unknown>\\n ): string[] {\\n   const errors: string[] = [];\\n   const executed = buildExecutedMap(stats);\\n@@ -350,5 +414,6 @@ export function evaluateCase(\\n   evaluateNoCalls(errors, expect, executed, recorder, slackRecorder);\\n   evaluatePrompts(errors, expect, promptsByStep);\\n   evaluateOutputs(errors, expect, outputHistory);\\n+  evaluateWorkflowOutputs(errors, expect, workflowOutputs);\\n   return errors;\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/index.ts\",\"additions\":112,\"deletions\":7,\"changes\":119,\"patch\":\"diff --git a/src/test-runner/index.ts b/src/test-runner/index.ts\\nindex 29916a16..4dcc3331 100644\\n--- a/src/test-runner/index.ts\\n+++ b/src/test-runner/index.ts\\n@@ -159,6 +159,7 @@ export async function runSuites(\\n   options: {\\n     only?: string;\\n     bail?: boolean;\\n+    noMocks?: boolean;\\n     maxParallelSuites?: number;\\n     maxParallel?: number;\\n     promptMaxChars?: number;\\n@@ -225,6 +226,7 @@ export async function runSuites(\\n       const r = await runner.runCases(fp, suite as TestSuite, {\\n         only: options.only,\\n         bail: options.bail,\\n+        noMocks: options.noMocks,\\n         maxParallel: options.maxParallel,\\n         promptMaxChars: options.promptMaxChars,\\n       });\\n@@ -285,7 +287,8 @@ export class VisorTestRunner {\\n     defaultPromptCap?: number,\\n     ghRec?: { error_code?: number; timeout_ms?: number },\\n     defaultIncludeTags?: string[] | undefined,\\n-    defaultExcludeTags?: string[] | undefined\\n+    defaultExcludeTags?: string[] | undefined,\\n+    noMocks?: boolean\\n   ): {\\n     name: string;\\n     strict: boolean;\\n@@ -399,9 +402,11 @@ export class VisorTestRunner {\\n               : info.prompt;\\n           prompts[k].push(p);\\n         },\\n-        mockForStep: (step: string) => mockMgr.get(step),\\n+        // In noMocks mode, always return undefined to let real providers execute\\n+        mockForStep: (step: string) => noMocks ? undefined : mockMgr.get(step),\\n         // Ensure human-input never blocks tests: prefer case mock, then default value\\n         onHumanInput: async (req: { checkId: string; default?: string }) => {\\n+          if (noMocks) return (req.default ?? '').toString();\\n           const m = mockMgr.get(req.checkId);\\n           if (m !== undefined && m !== null) return String(m);\\n           return (req.default ?? '').toString();\\n@@ -481,6 +486,60 @@ export class VisorTestRunner {\\n     if (!checks || checks.length === 0) return;\\n     console.log(`  checks: ${checks.join(', ')}`);\\n   }\\n+  /**\\n+   * Compute workflow outputs from output definitions and step results.\\n+   * Evaluates value_js and value (Liquid) expressions to produce output values.\\n+   */\\n+  private computeWorkflowOutputs(\\n+    outputDefs: Array<{ name: string; value?: string; value_js?: string }>,\\n+    outputHistory: Record<string, unknown[]>,\\n+    results: any[]\\n+  ): Record<string, unknown> {\\n+    const computed: Record<string, unknown> = {};\\n+    // Build a steps map from outputHistory (last output for each step)\\n+    const steps: Record<string, unknown> = {};\\n+    for (const [stepId, hist] of Object.entries(outputHistory)) {\\n+      if (Array.isArray(hist) && hist.length > 0) {\\n+        steps[stepId] = hist[hist.length - 1];\\n+      }\\n+    }\\n+    // Build outputs map from history (for {{ outputs[\\\"step\\\"] }} syntax)\\n+    const outputs: Record<string, unknown> = { ...steps };\\n+\\n+    for (const outputDef of outputDefs) {\\n+      if (!outputDef.name) continue;\\n+      try {\\n+        if (outputDef.value_js) {\\n+          // Evaluate JavaScript expression\\n+          const fn = new Function('steps', 'outputs', 'results', `\\n+            try {\\n+              ${outputDef.value_js}\\n+            } catch (e) {\\n+              return undefined;\\n+            }\\n+          `);\\n+          computed[outputDef.name] = fn(steps, outputs, results);\\n+        } else if (outputDef.value) {\\n+          // Evaluate Liquid template\\n+          const { Liquid } = require('liquidjs');\\n+          const engine = new Liquid();\\n+          const rendered = engine.parseAndRenderSync(outputDef.value, { steps, outputs, results });\\n+          // Try to parse as JSON, otherwise keep as string\\n+          try {\\n+            computed[outputDef.name] = JSON.parse(rendered);\\n+          } catch {\\n+            computed[outputDef.name] = rendered.trim();\\n+          }\\n+        }\\n+      } catch (e) {\\n+        // Skip outputs that fail to compute\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.log(`  ‚ö†Ô∏è Failed to compute output '${outputDef.name}': ${e}`);\\n+        }\\n+      }\\n+    }\\n+    return computed;\\n+  }\\n   /**\\n    * Locate a tests file: explicit path > ./.visor.tests.yaml > defaults/visor.tests.yaml\\n    */\\n@@ -606,7 +665,7 @@ export class VisorTestRunner {\\n   public async runCases(\\n     testsPath: string,\\n     suite: TestSuite,\\n-    options: { only?: string; bail?: boolean; maxParallel?: number; promptMaxChars?: number }\\n+    options: { only?: string; bail?: boolean; noMocks?: boolean; maxParallel?: number; promptMaxChars?: number }\\n   ): Promise<{\\n     failures: number;\\n     results: Array<{\\n@@ -748,9 +807,14 @@ export class VisorTestRunner {\\n     const __keepAlive = setInterval(() => {}, 1000);\\n     // Header: show suite path for clarity\\n     let __suiteRel = testsPath;\\n+    const noMocksMode = options.noMocks || false;\\n     try {\\n       __suiteRel = path.relative(this.cwd, testsPath) || testsPath;\\n       console.log(`Suite: ${__suiteRel}`);\\n+      if (noMocksMode) {\\n+        console.log(this.color('üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)', '33'));\\n+        console.log(this.gray('   Step outputs will be captured and printed as suggested mocks\\\\n'));\\n+      }\\n     } catch {}\\n     const runOne = async (_case: any): Promise<{ name: string; failed: number }> => {\\n       // Case header for clarity\\n@@ -805,6 +869,11 @@ export class VisorTestRunner {\\n           cfgLocal.checks[name] = chk;\\n         }\\n       }\\n+      // Workflow testing: inject workflow_input into config for template access\\n+      const workflowInput = (_case as any).workflow_input;\\n+      if (workflowInput && typeof workflowInput === 'object') {\\n+        (cfgLocal as any).workflow_inputs = workflowInput;\\n+      }\\n       const setup = this.setupTestCase(\\n         _case,\\n         cfgLocal,\\n@@ -812,7 +881,8 @@ export class VisorTestRunner {\\n         defaultPromptCap,\\n         ghRec,\\n         defaultIncludeTags,\\n-        defaultExcludeTags\\n+        defaultExcludeTags,\\n+        noMocksMode\\n       );\\n       try {\\n         this.printSelectedChecks(setup.checksToRun);\\n@@ -826,6 +896,24 @@ export class VisorTestRunner {\\n         } catch {}\\n         const exec = await this.executeTestCase(setup, cfgLocal);\\n         const res = exec.res;\\n+\\n+        // In no-mocks mode, print captured outputs as suggested mocks\\n+        if (noMocksMode && Object.keys(exec.outHistory).length > 0) {\\n+          console.log(this.color('\\\\nüìã Suggested mocks (copy to your test case):', '36'));\\n+          console.log(this.gray('mocks:'));\\n+          for (const [stepName, outputs] of Object.entries(exec.outHistory)) {\\n+            if (!Array.isArray(outputs) || outputs.length === 0) continue;\\n+            // Use the last output for the step\\n+            const lastOutput = outputs[outputs.length - 1];\\n+            // Format as YAML with proper indentation\\n+            const yamlOutput = yaml.dump({ [stepName]: lastOutput }, { indent: 2, lineWidth: 120, noRefs: true });\\n+            // Indent the YAML output for proper nesting under 'mocks:'\\n+            const indented = yamlOutput.split('\\\\n').map(line => '  ' + line).join('\\\\n');\\n+            console.log(indented);\\n+          }\\n+          console.log('');\\n+        }\\n+\\n         if (process.env.VISOR_DEBUG === 'true') {\\n           try {\\n             const names = (res.statistics.checks || []).map(\\n@@ -836,6 +924,21 @@ export class VisorTestRunner {\\n         }\\n         // avoid printing raw history keys each case\\n         // (fallback for on_finish static targets handled inside executeTestCase)\\n+        // Workflow testing: compute workflow outputs if workflow has outputs defined\\n+        let workflowOutputs: Record<string, unknown> | undefined;\\n+        if (Array.isArray((cfgLocal as any).outputs)) {\\n+          try {\\n+            workflowOutputs = this.computeWorkflowOutputs(\\n+              (cfgLocal as any).outputs,\\n+              exec.outHistory,\\n+              res.results\\n+            );\\n+          } catch (e) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ‚ö†Ô∏è Error computing workflow outputs: ${e}`);\\n+            }\\n+          }\\n+        }\\n         const caseFailures = require('./evaluators').evaluateCase(\\n           _case.name,\\n           res.statistics,\\n@@ -845,7 +948,8 @@ export class VisorTestRunner {\\n           setup.strict,\\n           setup.prompts,\\n           res.results,\\n-          exec.outHistory\\n+          exec.outHistory,\\n+          workflowOutputs\\n         );\\n         // Warn about unmocked AI/command steps that executed\\n         try {\\n@@ -1280,7 +1384,8 @@ export class VisorTestRunner {\\n   ): void {\\n     const executed: Record<string, number> = {};\\n     for (const s of stats.checks) {\\n-      if (!s.skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n+      const skipped = (s as any).skipped === true || !!(s as any).skipReason;\\n+      if (!skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n     }\\n     const expCalls = (expect.calls || []).filter(c => c.step);\\n     const expectedSteps = new Map<\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/validator.ts\",\"additions\":48,\"deletions\":2,\"changes\":50,\"patch\":\"diff --git a/src/test-runner/validator.ts b/src/test-runner/validator.ts\\nindex dbb890c5..5ffff5ee 100644\\n--- a/src/test-runner/validator.ts\\n+++ b/src/test-runner/validator.ts\\n@@ -25,6 +25,16 @@ const schema: any = {\\n     hooks: { type: 'object' },\\n     slack: { type: 'object' },\\n     frontends: { type: 'array' },\\n+    workspace: { type: 'object' },\\n+    // Workflow definition fields (for workflow files with co-located tests)\\n+    id: { type: 'string' },\\n+    name: { type: 'string' },\\n+    description: { type: 'string' },\\n+    inputs: { type: 'array' },\\n+    outputs: { type: 'array' },\\n+    ai_mcp_servers: { type: 'object' },\\n+    ai_provider: { type: 'string' },\\n+    ai_model: { type: 'string' },\\n     tests: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -136,6 +146,8 @@ const schema: any = {\\n             oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n           },\\n         },\\n+        // Workflow testing: input values to pass to the workflow\\n+        workflow_input: { type: 'object' },\\n         expect: { $ref: '#/$defs/expectBlock' },\\n         // Flow cases\\n         flow: {\\n@@ -270,6 +282,35 @@ const schema: any = {\\n       },\\n       required: ['step', 'path'],\\n     },\\n+    // Workflow output expectation: like outputsExpectation but without step (tests workflow-level outputs)\\n+    workflowOutputExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        path: { type: 'string' },\\n+        equals: {},\\n+        equalsDeep: {},\\n+        matches: { type: 'string' },\\n+        contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        not_contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            path: { type: 'string' },\\n+            equals: {},\\n+            matches: { type: 'string' },\\n+          },\\n+          required: ['path'],\\n+        },\\n+        contains_unordered: { type: 'array' },\\n+      },\\n+      required: ['path'],\\n+    },\\n     expectBlock: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -278,6 +319,8 @@ const schema: any = {\\n         calls: { type: 'array', items: { $ref: '#/$defs/callsExpectation' } },\\n         prompts: { type: 'array', items: { $ref: '#/$defs/promptsExpectation' } },\\n         outputs: { type: 'array', items: { $ref: '#/$defs/outputsExpectation' } },\\n+        // Workflow testing: assert on workflow-level outputs (defined in workflow's outputs: section)\\n+        workflow_output: { type: 'array', items: { $ref: '#/$defs/workflowOutputExpectation' } },\\n         no_calls: {\\n           type: 'array',\\n           items: {\\n@@ -343,6 +386,7 @@ const knownKeys = new Set([\\n   'version',\\n   'extends',\\n   'tests',\\n+  'workspace',\\n   // tests\\n   'tests.defaults',\\n   'tests.fixtures',\\n@@ -363,6 +407,7 @@ const knownKeys = new Set([\\n   'env',\\n   'routing',\\n   'mocks',\\n+  'workflow_input',\\n   'expect',\\n   'flow',\\n   // expect\\n@@ -370,6 +415,7 @@ const knownKeys = new Set([\\n   'expect.calls',\\n   'expect.prompts',\\n   'expect.outputs',\\n+  'expect.workflow_output',\\n   'expect.no_calls',\\n   'expect.fail',\\n   'expect.strict_violation',\\n@@ -424,7 +470,7 @@ function formatError(e: ErrorObject): string {\\n       if (hint) msg += ` (${hint})`;\\n       // Small curated allow-list for frequent nodes to reduce guesswork\\n       if (path.endsWith('expect')) {\\n-        msg += ` (allowed: use, calls, prompts, outputs, no_calls, fail, strict_violation)`;\\n+        msg += ` (allowed: use, calls, prompts, outputs, workflow_output, no_calls, fail, strict_violation)`;\\n       } else if (path.endsWith('env')) {\\n         msg += ` (values must be strings)`;\\n       } else if (path.endsWith('tests')) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/cli.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/cli.ts b/src/types/cli.ts\\nindex a7d9ca30..f06f93d5 100644\\n--- a/src/types/cli.ts\\n+++ b/src/types/cli.ts\\n@@ -61,6 +61,8 @@ export interface CliOptions {\\n   githubV2?: boolean;\\n   /** Enable Slack Socket Mode runner */\\n   slack?: boolean;\\n+  /** Keep workspace folders after execution (for debugging) */\\n+  keepWorkspace?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/config.ts\",\"additions\":114,\"deletions\":5,\"changes\":119,\"patch\":\"diff --git a/src/types/config.ts b/src/types/config.ts\\nindex ed239959..dd363de1 100644\\n--- a/src/types/config.ts\\n+++ b/src/types/config.ts\\n@@ -122,6 +122,9 @@ export interface FailureConditionContext {\\n   /** Environment variables */\\n   env?: Record<string, string>;\\n \\n+  /** Workflow inputs - values passed to the workflow */\\n+  inputs?: Record<string, unknown>;\\n+\\n   /** Memory accessor for accessing memory store in expressions */\\n   memory?: {\\n     get: (key: string, namespace?: string) => unknown;\\n@@ -356,6 +359,8 @@ export interface AIProviderConfig {\\n   allowBash?: boolean;\\n   /** Advanced bash command execution configuration */\\n   bashConfig?: BashConfig;\\n+  /** Completion prompt for post-completion validation/review (runs after attempt_completion) */\\n+  completion_prompt?: string;\\n }\\n \\n /**\\n@@ -461,8 +466,8 @@ export interface CheckConfig {\\n   env?: EnvConfig;\\n   /** Timeout in seconds for command execution (default: 60) */\\n   timeout?: number;\\n-  /** Check IDs that this check depends on (optional) */\\n-  depends_on?: string[];\\n+  /** Check IDs that this check depends on (optional). Accepts single string or array. */\\n+  depends_on?: string | string[];\\n   /** Group name for comment separation (e.g., \\\"code-review\\\", \\\"pr-overview\\\") - optional */\\n   group?: string;\\n   /** Schema type for template rendering (e.g., \\\"code-review\\\", \\\"markdown\\\") or inline JSON schema object - optional */\\n@@ -621,6 +626,46 @@ export interface CheckConfig {\\n   overrides?: Record<string, Partial<CheckConfig>>;\\n   /** Map workflow outputs to check outputs */\\n   output_mapping?: Record<string, string>;\\n+\\n+  // Workflow aliases for backward compatibility\\n+  /** Alias for args - workflow inputs (backward compatibility) */\\n+  workflow_inputs?: Record<string, unknown>;\\n+  /** Config file path - alternative to workflow ID (loads a Visor config file as workflow) */\\n+  config?: string;\\n+  /** Alias for overrides - workflow step overrides (backward compatibility) */\\n+  workflow_overrides?: Record<string, Partial<CheckConfig>>;\\n+\\n+  /**\\n+   * Git-checkout provider specific options (optional, only used when type === 'git-checkout').\\n+   */\\n+  /** Git reference to checkout (branch, tag, commit SHA) - supports templates */\\n+  ref?: string;\\n+  /** Repository URL or owner/repo format (defaults to current repository) */\\n+  repository?: string;\\n+  /** GitHub token for private repositories (defaults to GITHUB_TOKEN env) */\\n+  token?: string;\\n+  /** Number of commits to fetch (0 for full history, default: 1) */\\n+  fetch_depth?: number;\\n+  /** Whether to fetch tags (default: false) */\\n+  fetch_tags?: boolean;\\n+  /** Checkout submodules: false, true, or 'recursive' */\\n+  submodules?: boolean | 'recursive';\\n+  /** Working directory for the checkout (defaults to temp directory) */\\n+  working_directory?: string;\\n+  /** Use git worktree for efficient parallel checkouts (default: true) */\\n+  use_worktree?: boolean;\\n+  /** Clean the working directory before checkout (default: true) */\\n+  clean?: boolean;\\n+  /** Sparse checkout paths - only checkout specific directories/files */\\n+  sparse_checkout?: string[];\\n+  /** Enable Git LFS (Large File Storage) */\\n+  lfs?: boolean;\\n+  /** Timeout in ms for cloning the bare repository (default: 300000 = 5 min) */\\n+  clone_timeout_ms?: number;\\n+  /** Clean up worktree on failure (default: true) */\\n+  cleanup_on_failure?: boolean;\\n+  /** Keep worktree after workflow completion (default: false) */\\n+  persist_worktree?: boolean;\\n }\\n \\n /**\\n@@ -1037,6 +1082,36 @@ export interface CustomToolDefinition {\\n   outputSchema?: Record<string, unknown>;\\n }\\n \\n+/**\\n+ * Workflow input definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowInput {\\n+  /** Input parameter name */\\n+  name: string;\\n+  /** JSON Schema for the input */\\n+  schema?: Record<string, unknown>;\\n+  /** Whether this input is required */\\n+  required?: boolean;\\n+  /** Default value if not provided */\\n+  default?: unknown;\\n+  /** Human-readable description */\\n+  description?: string;\\n+}\\n+\\n+/**\\n+ * Workflow output definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowOutput {\\n+  /** Output name */\\n+  name: string;\\n+  /** Human-readable description */\\n+  description?: string;\\n+  /** Value using Liquid template syntax (references step outputs) */\\n+  value?: string;\\n+  /** Value using JavaScript expression (alternative to value) */\\n+  value_js?: string;\\n+}\\n+\\n /**\\n  * Main Visor configuration\\n  */\\n@@ -1051,12 +1126,16 @@ export interface VisorConfig {\\n   tools?: Record<string, CustomToolDefinition>;\\n   /** Import workflow definitions from external files or URLs */\\n   imports?: string[];\\n+  /** Workflow inputs (for standalone reusable workflows) */\\n+  inputs?: WorkflowInput[];\\n+  /** Workflow outputs (for standalone reusable workflows) */\\n+  outputs?: WorkflowOutput[];\\n   /** Step configurations (recommended) */\\n   steps?: Record<string, CheckConfig>;\\n   /** Check configurations (legacy, use 'steps' instead) - always populated after normalization */\\n   checks?: Record<string, CheckConfig>;\\n-  /** Output configuration */\\n-  output: OutputConfig;\\n+  /** Output configuration (optional - defaults provided) */\\n+  output?: OutputConfig;\\n   /** HTTP server configuration for receiving webhooks */\\n   http_server?: HttpServerConfig;\\n   /** Memory storage configuration */\\n@@ -1092,6 +1171,36 @@ export interface VisorConfig {\\n     /** Frontend-specific configuration */\\n     config?: unknown;\\n   }>;\\n+  /** Workspace isolation configuration for sandboxed execution */\\n+  workspace?: WorkspaceConfig;\\n+  /** Slack configuration */\\n+  slack?: SlackConfig;\\n+}\\n+\\n+/**\\n+ * Workspace isolation configuration\\n+ */\\n+export interface WorkspaceConfig {\\n+  /** Enable workspace isolation (default: true when config present) */\\n+  enabled?: boolean;\\n+  /** Base path for workspaces (default: /tmp/visor-workspaces) */\\n+  base_path?: string;\\n+  /** Clean up workspace on exit (default: true) */\\n+  cleanup_on_exit?: boolean;\\n+}\\n+\\n+/**\\n+ * Slack configuration\\n+ */\\n+export interface SlackConfig {\\n+  /** Slack API version */\\n+  version?: string;\\n+  /** Mention handling: 'all', 'direct', etc. */\\n+  mentions?: string;\\n+  /** Thread handling: 'required', 'optional', etc. */\\n+  threads?: string;\\n+  /** Show raw output in Slack responses */\\n+  show_raw_output?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/git-checkout.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/git-checkout.ts b/src/types/git-checkout.ts\\nindex eb1c5ebc..4d27855d 100644\\n--- a/src/types/git-checkout.ts\\n+++ b/src/types/git-checkout.ts\\n@@ -16,6 +16,8 @@ export interface GitCheckoutConfig {\\n   fetch_depth?: number;\\n   fetch_tags?: boolean;\\n   submodules?: boolean | 'recursive';\\n+  /** Timeout (ms) for cloning the bare repository; defaults to 300000 (5 minutes) */\\n+  clone_timeout_ms?: number;\\n \\n   // Worktree configuration\\n   working_directory?: string;\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/sandbox.ts\",\"additions\":46,\"deletions\":1,\"changes\":47,\"patch\":\"diff --git a/src/utils/sandbox.ts b/src/utils/sandbox.ts\\nindex 9914a3a5..12e27c30 100644\\n--- a/src/utils/sandbox.ts\\n+++ b/src/utils/sandbox.ts\\n@@ -14,6 +14,51 @@ export interface CompileOptions {\\n   wrapFunction?: boolean;\\n }\\n \\n+export interface JsSyntaxValidationResult {\\n+  valid: boolean;\\n+  error?: string;\\n+}\\n+\\n+/**\\n+ * Validate JavaScript syntax without executing it.\\n+ * Uses the sandbox's compile method to check for syntax errors.\\n+ * Returns validation result with error message if invalid.\\n+ */\\n+export function validateJsSyntax(code: string): JsSyntaxValidationResult {\\n+  if (!code || typeof code !== 'string') {\\n+    return { valid: false, error: 'Code must be a non-empty string' };\\n+  }\\n+\\n+  const trimmed = code.trim();\\n+  if (trimmed.length === 0) {\\n+    return { valid: false, error: 'Code cannot be empty' };\\n+  }\\n+\\n+  // Create a minimal sandbox instance for syntax checking\\n+  const sandbox = createSecureSandbox();\\n+\\n+  // Wrap code similar to compileAndRun to catch the same syntax issues\\n+  const looksLikeBlock = /\\\\breturn\\\\b/.test(trimmed) || /;/.test(trimmed) || /\\\\n/.test(trimmed);\\n+  const looksLikeIife = /\\\\)\\\\s*\\\\(\\\\s*\\\\)\\\\s*;?$/.test(trimmed);\\n+  const body = looksLikeBlock\\n+    ? looksLikeIife\\n+      ? `return (\\\\n${trimmed}\\\\n);\\\\n`\\n+      : `return (() => {\\\\n${trimmed}\\\\n})();\\\\n`\\n+    : `return (\\\\n${trimmed}\\\\n);\\\\n`;\\n+\\n+  // Add log injection header (same as compileAndRun)\\n+  const header = `const __lp = \\\"[syntax-check]\\\"; const log = (...a) => { try { console.log(__lp, ...a); } catch {} };\\\\n`;\\n+  const fullCode = `${header}${body}`;\\n+\\n+  try {\\n+    sandbox.compile(fullCode);\\n+    return { valid: true };\\n+  } catch (e) {\\n+    const msg = e instanceof Error ? e.message : String(e);\\n+    return { valid: false, error: msg };\\n+  }\\n+}\\n+\\n /**\\n  * Create a hardened Sandbox with a consistent set of globals and prototype\\n  * whitelists. This is a superset of the sets previously used by individual\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/tracer-init.ts\",\"additions\":19,\"deletions\":1,\"changes\":20,\"patch\":\"diff --git a/src/utils/tracer-init.ts b/src/utils/tracer-init.ts\\nindex b42995d9..4edf1ffc 100644\\n--- a/src/utils/tracer-init.ts\\n+++ b/src/utils/tracer-init.ts\\n@@ -76,6 +76,24 @@ export async function initializeTracer(\\n \\n       const tracer = new SimpleAppTracer(telemetry, sessionId);\\n \\n+      // WORKAROUND: Add missing recordEvent method for completionPrompt feature (probe #321)\\n+      // SimpleAppTracer doesn't have recordEvent but completionPrompt requires it\\n+      if (typeof (tracer as any).recordEvent !== 'function') {\\n+        (tracer as any).recordEvent = (\\n+          name: string,\\n+          attributes?: Record<string, unknown>\\n+        ) => {\\n+          // Log completion events to telemetry for debugging\\n+          try {\\n+            if ((telemetry as any).record) {\\n+              (telemetry as any).record({ event: name, ...attributes });\\n+            }\\n+          } catch {\\n+            // Best-effort only\\n+          }\\n+        };\\n+      }\\n+\\n       console.error(`üìä Simple tracing enabled, will save to: ${traceFilePath}`);\\n \\n       // If in GitHub Actions, log the path for artifact upload\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/worktree-manager.ts\",\"additions\":142,\"deletions\":27,\"changes\":169,\"patch\":\"diff --git a/src/utils/worktree-manager.ts b/src/utils/worktree-manager.ts\\nindex f3cebf3d..2de7a9f4 100644\\n--- a/src/utils/worktree-manager.ts\\n+++ b/src/utils/worktree-manager.ts\\n@@ -6,6 +6,7 @@\\n  */\\n \\n import * as fs from 'fs';\\n+import * as fsp from 'fs/promises';\\n import * as path from 'path';\\n import * as crypto from 'crypto';\\n import { commandExecutor } from './command-executor';\\n@@ -119,7 +120,8 @@ export class WorktreeManager {\\n     repository: string,\\n     repoUrl: string,\\n     token?: string,\\n-    fetchDepth?: number\\n+    fetchDepth?: number,\\n+    cloneTimeoutMs?: number\\n   ): Promise<string> {\\n     const reposDir = this.getReposDir();\\n     const repoName = repository.replace(/\\\\//g, '-');\\n@@ -129,9 +131,21 @@ export class WorktreeManager {\\n     if (fs.existsSync(bareRepoPath)) {\\n       logger.debug(`Bare repository already exists: ${bareRepoPath}`);\\n \\n-      // Update remote refs\\n-      await this.updateBareRepo(bareRepoPath);\\n-      return bareRepoPath;\\n+      // Verify the bare repo has the correct remote URL to prevent using corrupted repos\\n+      const verifyResult = await this.verifyBareRepoRemote(bareRepoPath, repoUrl);\\n+      if (verifyResult === 'timeout') {\\n+        // Timeout during verification - use stale cache to avoid hanging on re-clone\\n+        logger.info(`Using stale bare repository (verification timed out): ${bareRepoPath}`);\\n+        return bareRepoPath;\\n+      } else if (verifyResult === false) {\\n+        logger.warn(`Bare repository at ${bareRepoPath} has incorrect remote, removing and re-cloning`);\\n+        await fsp.rm(bareRepoPath, { recursive: true, force: true });\\n+        // Fall through to clone below\\n+      } else {\\n+        // Update remote refs\\n+        await this.updateBareRepo(bareRepoPath);\\n+        return bareRepoPath;\\n+      }\\n     }\\n \\n     // Clone as bare repository\\n@@ -153,7 +167,9 @@ export class WorktreeManager {\\n     }\\n     cloneCmd += ` ${this.escapeShellArg(cloneUrl)} ${this.escapeShellArg(bareRepoPath)}`;\\n \\n-    const result = await this.executeGitCommand(cloneCmd, { timeout: 300000 }); // 5 minute timeout\\n+    const result = await this.executeGitCommand(cloneCmd, {\\n+      timeout: cloneTimeoutMs || 300000, // default 5 minutes\\n+    });\\n \\n     if (result.exitCode !== 0) {\\n       // Redact tokens from error messages\\n@@ -171,19 +187,98 @@ export class WorktreeManager {\\n   private async updateBareRepo(bareRepoPath: string): Promise<void> {\\n     logger.debug(`Updating bare repository: ${bareRepoPath}`);\\n \\n-    const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n-    const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+    try {\\n+      const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n+      const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n+        // Don't throw - we can continue with stale refs\\n+      } else {\\n+        logger.debug(`Successfully updated bare repository`);\\n+      }\\n+    } catch (error) {\\n+      // Handle timeout or other errors gracefully - we can continue with stale refs\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      logger.warn(`Failed to update bare repository (will use stale refs): ${errorMessage}`);\\n+    }\\n+  }\\n \\n-    if (result.exitCode !== 0) {\\n-      logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n-      // Don't throw - we can continue with stale refs\\n-    } else {\\n-      logger.debug(`Successfully updated bare repository`);\\n+  /**\\n+   * Verify that a bare repository has the correct remote URL.\\n+   * This prevents reusing corrupted repos that were cloned from a different repository.\\n+   * Returns: true (valid), false (invalid - should re-clone), or 'timeout' (use stale cache)\\n+   */\\n+  private async verifyBareRepoRemote(\\n+    bareRepoPath: string,\\n+    expectedUrl: string\\n+  ): Promise<boolean | 'timeout'> {\\n+    try {\\n+      const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote get-url origin`;\\n+      const result = await this.executeGitCommand(cmd, { timeout: 10000 });\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to get remote URL for ${bareRepoPath}: ${result.stderr}`);\\n+        return false;\\n+      }\\n+\\n+      const actualUrl = result.stdout.trim();\\n+\\n+      // Normalize URLs for comparison:\\n+      // - remove credentials (tokens/username)\\n+      // - remove .git suffix if present\\n+      // - trim trailing slash\\n+      // - lowercase for case‚Äëinsensitive match\\n+      const normalizeUrl = (url: string): string => {\\n+        // Convert common SSH form to https for comparison\\n+        if (url.startsWith('git@github.com:')) {\\n+          url = url.replace('git@github.com:', 'https://github.com/');\\n+        }\\n+        return url\\n+          // strip userinfo part to avoid mismatches when the cached bare\\n+          // repo was cloned with an access token but the expected URL is\\n+          // tokenless (or vice‚Äëversa)\\n+          .replace(/:\\\\/\\\\/[^@]+@/, '://')\\n+          .replace(/\\\\.git$/, '')\\n+          .replace(/\\\\/$/, '')\\n+          .toLowerCase();\\n+      };\\n+\\n+      const normalizedExpected = normalizeUrl(expectedUrl);\\n+      const normalizedActual = normalizeUrl(actualUrl);\\n+\\n+      if (normalizedExpected !== normalizedActual) {\\n+        logger.warn(\\n+          `Bare repo remote mismatch: expected ${expectedUrl}, got ${actualUrl}`\\n+        );\\n+        return false;\\n+      }\\n+\\n+      logger.debug(`Bare repo remote verified: ${actualUrl}`);\\n+      return true;\\n+    } catch (error) {\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      // If it's a timeout, return 'timeout' so caller can use stale cache instead of re-cloning\\n+      if (errorMessage.includes('timed out')) {\\n+        logger.warn(`Timeout verifying bare repo remote (will use stale cache): ${errorMessage}`);\\n+        return 'timeout';\\n+      }\\n+      logger.warn(`Error verifying bare repo remote: ${error}`);\\n+      return false;\\n     }\\n   }\\n \\n   /**\\n-   * Create a new worktree\\n+   * Create a new worktree for the given repository/ref.\\n+   *\\n+   * Important: we always create worktrees in a detached HEAD state pinned\\n+   * to a specific commit SHA rather than a named branch like \\\"main\\\". Git\\n+   * only allows a branch to be checked out in a single worktree at a time;\\n+   * using the raw commit (plus --detach) lets multiple workflows safely\\n+   * create independent worktrees for the same branch without hitting\\n+   * errors like:\\n+   *\\n+   *   fatal: 'main' is already used by worktree at '.../TykTechnologies-tyk-docs-main-XXXX'\\n    */\\n   async createWorktree(\\n     repository: string,\\n@@ -195,6 +290,7 @@ export class WorktreeManager {\\n       clean?: boolean;\\n       workflowId?: string;\\n       fetchDepth?: number;\\n+      cloneTimeoutMs?: number;\\n     } = {}\\n   ): Promise<WorktreeInfo> {\\n     // Validate ref to prevent command injection\\n@@ -205,7 +301,8 @@ export class WorktreeManager {\\n       repository,\\n       repoUrl,\\n       options.token,\\n-      options.fetchDepth\\n+      options.fetchDepth,\\n+      options.cloneTimeoutMs\\n     );\\n \\n     // Generate worktree ID and path\\n@@ -241,21 +338,24 @@ export class WorktreeManager {\\n       }\\n     }\\n \\n-    // Fetch the ref if needed\\n+    // Fetch the ref if needed, then resolve it to a concrete commit SHA.\\n+    // We use the commit (detached HEAD) instead of the branch name so we\\n+    // can have multiple worktrees for the same branch without git refusing\\n+    // with \\\"branch X is already checked out\\\".\\n     await this.fetchRef(bareRepoPath, ref);\\n+    const commit = await this.getCommitShaForRef(bareRepoPath, ref);\\n \\n-    // Create worktree\\n-    logger.info(`Creating worktree for ${repository}@${ref}`);\\n-    const createCmd = `git -C ${this.escapeShellArg(bareRepoPath)} worktree add ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(ref)}`;\\n+    // Create worktree in detached HEAD state at the resolved commit\\n+    logger.info(`Creating worktree for ${repository}@${ref} (${commit})`);\\n+    const createCmd = `git -C ${this.escapeShellArg(\\n+      bareRepoPath\\n+    )} worktree add --detach ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(commit)}`;\\n     const result = await this.executeGitCommand(createCmd, { timeout: 60000 });\\n \\n     if (result.exitCode !== 0) {\\n       throw new Error(`Failed to create worktree: ${result.stderr}`);\\n     }\\n \\n-    // Get commit SHA\\n-    const commit = await this.getCommitSha(worktreePath);\\n-\\n     // Create metadata\\n     const metadata: WorktreeMetadata = {\\n       worktree_id: worktreeId,\\n@@ -316,14 +416,17 @@ export class WorktreeManager {\\n   }\\n \\n   /**\\n-   * Get commit SHA for worktree\\n+   * Get commit SHA for a given ref inside a bare repository.\\n+   *\\n+   * This runs after fetchRef so that <ref> should resolve to either a\\n+   * local branch, tag, or remote-tracking ref.\\n    */\\n-  private async getCommitSha(worktreePath: string): Promise<string> {\\n-    const cmd = `git -C ${this.escapeShellArg(worktreePath)} rev-parse HEAD`;\\n+  private async getCommitShaForRef(bareRepoPath: string, ref: string): Promise<string> {\\n+    const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} rev-parse ${this.escapeShellArg(ref)}`;\\n     const result = await this.executeGitCommand(cmd);\\n \\n     if (result.exitCode !== 0) {\\n-      throw new Error(`Failed to get commit SHA: ${result.stderr}`);\\n+      throw new Error(`Failed to get commit SHA for ref ${ref}: ${result.stderr}`);\\n     }\\n \\n     return result.stdout.trim();\\n@@ -621,9 +724,21 @@ export class WorktreeManager {\\n     command: string,\\n     options: { timeout?: number; env?: Record<string, string> } = {}\\n   ): Promise<GitCommandResult> {\\n+    // Merge provided env with process.env and add git-specific settings\\n+    // These settings prevent git from hanging on interactive prompts while\\n+    // still allowing OS-level credential helpers to work:\\n+    // - GIT_TERMINAL_PROMPT=0: Prevents terminal credential prompts\\n+    // - GIT_SSH_COMMAND: Disables SSH password prompts (BatchMode)\\n+    const gitEnv = {\\n+      ...process.env,\\n+      ...options.env,\\n+      GIT_TERMINAL_PROMPT: '0',\\n+      GIT_SSH_COMMAND: 'ssh -o BatchMode=yes -o StrictHostKeyChecking=no',\\n+    };\\n+\\n     const result = await commandExecutor.execute(command, {\\n       timeout: options.timeout || 30000,\\n-      env: options.env || process.env,\\n+      env: gitEnv,\\n     } as any);\\n \\n     return {\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-executor.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-executor.ts b/src/workflow-executor.ts\\nindex 751e7f16..df76762f 100644\\n--- a/src/workflow-executor.ts\\n+++ b/src/workflow-executor.ts\\n@@ -219,7 +219,9 @@ export class WorkflowExecutor {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps)) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     // Use static DependencyResolver\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-registry.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-registry.ts b/src/workflow-registry.ts\\nindex 2cbe3a89..6bd855fb 100644\\n--- a/src/workflow-registry.ts\\n+++ b/src/workflow-registry.ts\\n@@ -410,7 +410,9 @@ export class WorkflowRegistry {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps || {})) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     try {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/ai-review-service.test.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/tests/unit/ai-review-service.test.ts b/tests/unit/ai-review-service.test.ts\\nindex ec2b998a..2c616c99 100644\\n--- a/tests/unit/ai-review-service.test.ts\\n+++ b/tests/unit/ai-review-service.test.ts\\n@@ -482,6 +482,37 @@ describe('AIReviewService', () => {\\n       expect(context).toContain('<additions>10</additions>');\\n       expect(context).toContain('<deletions>5</deletions>');\\n     });\\n+\\n+    it('should exclude diffs by default when in Slack mode', async () => {\\n+      // Slack mode is detected by presence of slackConversation property\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // In Slack mode, diffs should be excluded by default\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+      // Should still include file summary\\n+      expect(context).toContain('<files_summary>');\\n+    });\\n+\\n+    it('should include diffs in Slack mode when explicitly requested', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      (mockPRInfo as any).includeCodeContext = true;\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // With explicit includeCodeContext: true, diffs should be included\\n+      expect(context).toContain('<full_diff>');\\n+      expect(context).toContain('--- test.ts');\\n+    });\\n+\\n+    it('should exclude diffs in Slack mode even when includeCodeContext is undefined', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      // Deliberately not setting includeCodeContext to test default behavior\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+    });\\n   });\\n \\n   describe('Comment Filtering for Code Review', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config-extends.test.ts\",\"additions\":8,\"deletions\":8,\"changes\":16,\"patch\":\"diff --git a/tests/unit/config-extends.test.ts b/tests/unit/config-extends.test.ts\\nindex b112a869..7ee69ce6 100644\\n--- a/tests/unit/config-extends.test.ts\\n+++ b/tests/unit/config-extends.test.ts\\n@@ -564,7 +564,7 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.security).toBeDefined();\\n       expect(config.checks!.performance).toBeDefined();\\n       expect(config.checks!.custom).toBeDefined();\\n-      expect(config.output.pr_comment.format).toBe('json');\\n+      expect(config.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should extend from default configuration', async () => {\\n@@ -1002,7 +1002,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.base).toBeDefined(); // From base\\n       expect(loaded.checks!.middle).toBeDefined(); // From middle\\n       expect(loaded.checks!.top).toBeDefined(); // From top\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From top\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From top\\n     });\\n \\n     it('should handle mixed local and remote extends', async () => {\\n@@ -1079,7 +1079,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.local).toBeDefined();\\n       expect(loaded.checks!.remote).toBeDefined();\\n       expect(loaded.checks!.child).toBeDefined();\\n-      expect(loaded.output.pr_comment.format).toBe('json');\\n+      expect(loaded.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should handle extends with relative path resolution', async () => {\\n@@ -1593,9 +1593,9 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.performance).toBeDefined(); // Added in child\\n \\n       // Verify output\\n-      expect(config.output.pr_comment.format).toBe('json'); // Overridden\\n-      expect(config.output.pr_comment.group_by).toBe('file'); // Overridden\\n-      expect(config.output.pr_comment.collapse).toBe(false); // Overridden\\n+      expect(config.output!.pr_comment.format).toBe('json'); // Overridden\\n+      expect(config.output!.pr_comment.group_by).toBe('file'); // Overridden\\n+      expect(config.output!.pr_comment.collapse).toBe(false); // Overridden\\n     });\\n \\n     it('should handle configuration with disabled checks removal', async () => {\\n@@ -1814,7 +1814,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.root).toBeDefined(); // From root\\n       expect(loaded.checks!.level1).toBeDefined(); // From level1\\n       expect(loaded.checks!.level2).toBeDefined(); // From level2\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From level2\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From level2\\n     });\\n \\n     it('should handle mixed absolute and relative paths in extends array', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config.test.ts\",\"additions\":7,\"deletions\":7,\"changes\":14,\"patch\":\"diff --git a/tests/unit/config.test.ts b/tests/unit/config.test.ts\\nindex 67ce61fd..0677a3d4 100644\\n--- a/tests/unit/config.test.ts\\n+++ b/tests/unit/config.test.ts\\n@@ -48,7 +48,7 @@ output:\\n       expect(config.version).toBe('1.0');\\n       expect(config.checks).toHaveProperty('performance');\\n       expect(config.checks).toHaveProperty('security');\\n-      expect(config.output.pr_comment.format).toBe('table');\\n+      expect(config.output!.pr_comment.format).toBe('table');\\n     });\\n \\n     it('should handle missing config file gracefully', async () => {\\n@@ -264,9 +264,9 @@ checks:\\n       const config = await configManager.loadConfig('/path/to/minimal.yaml');\\n \\n       // Should have default output configuration\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.group_by).toBe('check');\\n-      expect(config.output.pr_comment.collapse).toBe(true);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.group_by).toBe('check');\\n+      expect(config.output!.pr_comment.collapse).toBe(true);\\n     });\\n   });\\n \\n@@ -483,8 +483,8 @@ output:\\n       expect(config.checks!.performance.prompt).toContain('N+1 database queries');\\n       expect(config.checks!.security.prompt).toContain('SQL injection');\\n       expect(config.checks!.architecture.prompt).toContain('SOLID principles');\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.collapse).toBe(false);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.collapse).toBe(false);\\n     });\\n   });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/foreach-empty-skip.test.ts\",\"additions\":5,\"deletions\":3,\"changes\":8,\"patch\":\"diff --git a/tests/unit/foreach-empty-skip.test.ts b/tests/unit/foreach-empty-skip.test.ts\\nindex 1ea8e1e5..2640ed14 100644\\n--- a/tests/unit/foreach-empty-skip.test.ts\\n+++ b/tests/unit/foreach-empty-skip.test.ts\\n@@ -90,11 +90,13 @@ describe('forEach Empty Array Skip', () => {\\n     );\\n     expect(processStats?.totalRuns).toBe(0);\\n \\n-    // notify-ticket should also NOT run because process-ticket returned empty\\n+    // notify-ticket SHOULD run even though process-ticket had 0 runs from empty forEach\\n+    // This is the expected behavior: downstream steps that depend on forEach children\\n+    // should still execute (with empty/null inputs) to handle the \\\"no items\\\" case\\n     const notifyStats = result.executionStatistics?.checks!.find(\\n       c => c.checkName === 'notify-ticket'\\n     );\\n-    expect(notifyStats?.totalRuns).toBe(0);\\n+    expect(notifyStats?.totalRuns).toBe(1);\\n   });\\n \\n   it('should handle empty array with transform_js', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/ai-check-provider.test.ts\",\"additions\":61,\"deletions\":1,\"changes\":62,\"patch\":\"diff --git a/tests/unit/providers/ai-check-provider.test.ts b/tests/unit/providers/ai-check-provider.test.ts\\nindex d0b94086..a858ddae 100644\\n--- a/tests/unit/providers/ai-check-provider.test.ts\\n+++ b/tests/unit/providers/ai-check-provider.test.ts\\n@@ -399,6 +399,66 @@ describe('AICheckProvider', () => {\\n         },\\n       });\\n     });\\n+\\n+    it('derives allowedFolders and path from workspace when present on parent context', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const workspace = {\\n+        isEnabled: () => true,\\n+        getWorkspaceInfo: () => ({\\n+          workspacePath: '/tmp/ws-123',\\n+          mainProjectPath: '/tmp/ws-123/main-project',\\n+        }),\\n+        listProjects: () => [\\n+          { name: 'tyk', path: '/tmp/ws-123/tyk' },\\n+          { name: 'tyk-docs', path: '/tmp/ws-123/tyk-docs' },\\n+        ],\\n+      };\\n+\\n+      const execContext: any = {\\n+        _parentContext: {\\n+          workspace,\\n+        },\\n+      };\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'code_help',\\n+        ai: {\\n+          provider: 'google',\\n+          model: 'gemini-2.5-pro',\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, execContext);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'google',\\n+        model: 'gemini-2.5-pro',\\n+        // Workspace root should be used as primary working directory for tools\\n+        path: '/tmp/ws-123',\\n+      });\\n+\\n+      // allowedFolders should contain the workspace plus all project paths, de-duped\\n+      expect(capturedConfig.allowedFolders).toEqual(\\n+        expect.arrayContaining(['/tmp/ws-123', '/tmp/ws-123/tyk', '/tmp/ws-123/tyk-docs'])\\n+      );\\n+    });\\n   });\\n \\n   describe('getSupportedConfigKeys', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/command-check-provider.test.ts\",\"additions\":105,\"deletions\":1,\"changes\":106,\"patch\":\"diff --git a/tests/unit/providers/command-check-provider.test.ts b/tests/unit/providers/command-check-provider.test.ts\\nindex 3bf11a84..0a821bbd 100644\\n--- a/tests/unit/providers/command-check-provider.test.ts\\n+++ b/tests/unit/providers/command-check-provider.test.ts\\n@@ -666,4 +666,108 @@ describe('CommandCheckProvider', () => {\\n       expect((result as any).output).toBe('content'); // CommandCheckProvider trims whitespace\\n     });\\n   });\\n+\\n+  describe('Workflow Inputs', () => {\\n+    it('should use workflowInputs from config when available', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Hello from config.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from config.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with inputs from config.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from config.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should fall back to context.workflowInputs when config.workflowInputs is not set', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Hello from context.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from context.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify the command was rendered with inputs from context.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from context.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should prefer config.workflowInputs over context.workflowInputs', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Config takes precedence',\\n+        },\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Context should be ignored',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Config takes precedence\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify config.workflowInputs takes precedence\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Config takes precedence\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should provide empty inputs when neither config nor context has workflowInputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with empty inputs (undefined renders as empty)\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+  });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/git-checkout-workspace.test.ts\",\"additions\":4,\"deletions\":5,\"changes\":9,\"patch\":\"diff --git a/tests/unit/providers/git-checkout-workspace.test.ts b/tests/unit/providers/git-checkout-workspace.test.ts\\nindex 4eb625b7..4a0fbb6e 100644\\n--- a/tests/unit/providers/git-checkout-workspace.test.ts\\n+++ b/tests/unit/providers/git-checkout-workspace.test.ts\\n@@ -129,13 +129,12 @@ describe('GitCheckoutProvider Workspace Integration', () => {\\n         const output = (result as any).output as GitCheckoutOutput;\\n         expect(output.success).toBe(true);\\n         expect(output.workspace_path).toBeDefined();\\n-        // When checkName is provided, it's used as the project name\\n-        expect(output.workspace_path).toContain('test-checkout');\\n \\n-        // Verify project was added to workspace\\n+        // Verify project was added to workspace and the name is derived\\n+        // from the repository/description, not the checkName.\\n         const projects = workspace.listProjects();\\n         expect(projects.length).toBe(1);\\n-        expect(projects[0].name).toBe('test-checkout');\\n+        expect(projects[0].name).toBe('external-repo');\\n       } finally {\\n         await workspace.cleanup();\\n         fs.rmSync(mainProjectDir, { recursive: true, force: true });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/http-client-provider.test.ts\",\"additions\":15,\"deletions\":10,\"changes\":25,\"patch\":\"diff --git a/tests/unit/providers/http-client-provider.test.ts b/tests/unit/providers/http-client-provider.test.ts\\nindex b759ad75..caa73417 100644\\n--- a/tests/unit/providers/http-client-provider.test.ts\\n+++ b/tests/unit/providers/http-client-provider.test.ts\\n@@ -120,15 +120,17 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect(result).toEqual<ReviewSummary & { data: unknown }>({\\n+      // The provider returns data in the 'output' property (consistent with other providers)\\n+      expect(result).toEqual({\\n         issues: [],\\n-        data: responseData,\\n+        output: { status: 'ok', data: { value: 123 } },\\n       });\\n     });\\n \\n     it('should handle POST request with body', async () => {\\n       mockConfig.method = 'POST';\\n-      mockConfig.body = '{\\\"request\\\": \\\"data\\\"}';\\n+      // Use a body with Liquid template to trigger parseAndRender\\n+      mockConfig.body = '{\\\"request\\\": \\\"{{ pr.title }}\\\"}';\\n \\n       const responseData = { result: 'success' };\\n       const mockResponse = {\\n@@ -143,15 +145,16 @@ describe('HttpClientProvider', () => {\\n       };\\n \\n       mockFetch.mockResolvedValue(mockResponse);\\n-      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"data\\\"}');\\n+      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"Test PR\\\"}');\\n \\n       const result = await provider.execute(mockPRInfo, mockConfig, new Map());\\n \\n+      // Verify the body template was parsed by Liquid with the correct template\\n       expect(mockLiquid.parseAndRender).toHaveBeenCalledWith(\\n-        '{\\\"request\\\": \\\"data\\\"}',\\n+        '{\\\"request\\\": \\\"{{ pr.title }}\\\"}',\\n         expect.objectContaining({\\n           pr: expect.any(Object),\\n-          outputs: {},\\n+          outputs: expect.any(Object),\\n         })\\n       );\\n \\n@@ -159,7 +162,7 @@ describe('HttpClientProvider', () => {\\n         'https://api.example.com/data',\\n         expect.objectContaining({\\n           method: 'POST',\\n-          body: '{\\\"request\\\": \\\"data\\\"}',\\n+          body: '{\\\"request\\\": \\\"Test PR\\\"}',\\n           headers: expect.objectContaining({\\n             Authorization: 'Bearer token',\\n             'Content-Type': 'application/json',\\n@@ -167,7 +170,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(responseData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { result: string } }).output.result).toEqual('success');\\n     });\\n \\n     it('should handle HTTP errors', async () => {\\n@@ -235,7 +239,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(transformedData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { transformed: string } }).output.transformed).toEqual('result');\\n     });\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/session-reuse-config.test.ts\",\"additions\":20,\"deletions\":1,\"changes\":21,\"patch\":\"diff --git a/tests/unit/session-reuse-config.test.ts b/tests/unit/session-reuse-config.test.ts\\nindex a2e2ee26..18fc42e6 100644\\n--- a/tests/unit/session-reuse-config.test.ts\\n+++ b/tests/unit/session-reuse-config.test.ts\\n@@ -152,6 +152,25 @@ describe('Session Reuse Configuration Validation', () => {\\n       }).not.toThrow();\\n     });\\n \\n+    it('should accept reuse_ai_session=\\\\\\\"self\\\\\\\" without depends_on', () => {\\n+      const config: Partial<VisorConfig> = {\\n+        version: '1.0',\\n+        checks: {\\n+          'loop-check': {\\n+            type: 'ai',\\n+            prompt: 'Chat-like loop that reuses its own session',\\n+            on: ['pr_opened'],\\n+            // Special self-reuse mode does not require depends_on\\n+            reuse_ai_session: 'self',\\n+          },\\n+        },\\n+      };\\n+\\n+      expect(() => {\\n+        (configManager as any).validateConfig(config);\\n+      }).not.toThrow();\\n+    });\\n+\\n     it('should accept reuse_ai_session=true with multiple dependencies', () => {\\n       const config: Partial<VisorConfig> = {\\n         version: '1.0',\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/slack-markdown-format.test.ts\",\"additions\":25,\"deletions\":1,\"changes\":26,\"patch\":\"diff --git a/tests/unit/slack-markdown-format.test.ts b/tests/unit/slack-markdown-format.test.ts\\nindex 5e57ccae..39d0baf3 100644\\n--- a/tests/unit/slack-markdown-format.test.ts\\n+++ b/tests/unit/slack-markdown-format.test.ts\\n@@ -39,4 +39,28 @@ describe('markdownToSlack', () => {\\n       ['‚Ä¢ parent', '  ‚Ä¢ child', '```', '- not-a-bullet inside code', '```', '‚Ä¢ after'].join('\\\\n')\\n     );\\n   });\\n+\\n+  it('converts markdown headers to bold text', () => {\\n+    const input = '# Main Title\\\\n## Subtitle\\\\n### Section';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*Main Title*\\\\n*Subtitle*\\\\n*Section*');\\n+  });\\n+\\n+  it('adds newline before h1/h2 headers when preceded by content', () => {\\n+    const input = 'Some content\\\\n## New Section\\\\nMore content';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('Some content\\\\n\\\\n*New Section*\\\\nMore content');\\n+  });\\n+\\n+  it('does not add newline before h1 if it is the first line', () => {\\n+    const input = '# First Header\\\\nContent here';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*First Header*\\\\nContent here');\\n+  });\\n+\\n+  it('ignores headers inside code blocks', () => {\\n+    const input = '```\\\\n# This is a comment\\\\n```\\\\n# Real Header';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('```\\\\n# This is a comment\\\\n```\\\\n*Real Header*');\\n+  });\\n });\\n\",\"status\":\"modified\"}],\"outputs\":{\"ask\":{\"text\":\"hi!\",\"ts\":1767891905091},\"jira-context\":{\"jira_context_xml\":\"\",\"issues\":[],\"issue_count\":0,\"ts\":1767891905244},\"zendesk-context\":{\"zendesk_context_xml\":\"\",\"tickets\":\"\",\"ticket_count\":0,\"attachments\":\"\",\"ts\":1767891905257},\"log-request\":{\"issues\":[],\"logOutput\":\"‚ÑπÔ∏è **INFO**: üì• Request from user: unknown\\nChannel: unknown\\nThread: unknown\\nMessage: hi!\\n\"},\"code-help-error\":{\"issues\":[]},\"customer-insights-error\":{\"issues\":[]},\"release-notes-error\":{\"issues\":[]}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.output":"{\"intent\":\"chat\",\"topic\":\"User said hi!\",\"text\":\"{\\n  \\\"intent\\\": \\\"chat\\\",\\n  \\\"topic\\\": \\\"User said hi!\\\"\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":2,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"chat-answer","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"chat-answer","visor.check.input.context":"{\"pr\":{\"number\":0,\"title\":\"Local Analysis: feat/slack-frontend-v2 (70 modified)\",\"author\":\"Leonid Bugaev\",\"branch\":\"feat/slack-frontend-v2\",\"base\":\"main\"},\"files\":[{\"filename\":\"README.md\",\"additions\":7,\"deletions\":1,\"changes\":8,\"patch\":\"diff --git a/README.md b/README.md\\nindex 41131861..d92f02cc 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -534,6 +534,12 @@ steps:\\n     session_mode: append    # Shares history for full conversation\\n ```\\n \\n+You can also reuse the **same check‚Äôs** session when it loops back to itself, using:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` with `session_mode: append`\\n+\\n+See the standalone example at `examples/session-reuse-self.yaml` and the detailed guide in [docs/advanced-ai.md](docs/advanced-ai.md).\\n+\\n Learn more: [docs/advanced-ai.md](docs/advanced-ai.md)\\n \\n ## üìã Schema-Template System\\n\",\"status\":\"modified\"},{\"filename\":\"docs/advanced-ai.md\",\"additions\":61,\"deletions\":2,\"changes\":63,\"patch\":\"diff --git a/docs/advanced-ai.md b/docs/advanced-ai.md\\nindex bb8e684a..879285ed 100644\\n--- a/docs/advanced-ai.md\\n+++ b/docs/advanced-ai.md\\n@@ -1,7 +1,7 @@\\n ## üß† Advanced AI Features\\n \\n ### AI Session Reuse\\n-Use `reuse_ai_session: true` on dependent checks to continue conversation context with the AI across checks. This improves follow‚Äëups and consistency.\\n+Use `reuse_ai_session` on checks to continue conversation context with the AI across steps. This improves follow‚Äëups and consistency for follow‚Äëon analysis and chat‚Äëstyle flows.\\n \\n **Session Modes:**\\n - **`clone` (default)**: Creates a copy of the conversation history. Each check gets an independent session with the same starting context. Changes made by one check don't affect others.\\n@@ -26,6 +26,65 @@ steps:\\n     depends_on: [security-remediation]\\n     reuse_ai_session: true\\n     session_mode: append  # Share history - sees full conversation\\n+\\n+#### Reusing your own session: `reuse_ai_session: self`\\n+\\n+Sometimes the step you want to loop back into is the AI step itself (e.g. Slack assistants or multi‚Äëturn internal tools). For that case you can use:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` ‚Äì the step reuses its **own** Probe session when it runs again in the same engine run.\\n+- `session_mode: append` ‚Äì makes the follow‚Äëup behave like a normal conversation turn.\\n+\\n+On the first run of the step, Visor creates a new ProbeAgent session and registers it. If routing (`on_success.goto`, `goto_js`, etc.) later jumps back to the same step within the same run, the engine:\\n+\\n+- Finds the last result for that step in the current run.\\n+- Reads the `sessionId` stored in the result.\\n+- Calls the AI provider again using `executeReviewWithSessionReuse` with that session id.\\n+\\n+Simple example (no transport wiring, just CLI/tests):\\n+\\n+```yaml\\n+version: \\\"2.0\\\"\\n+\\n+steps:\\n+  seed:\\n+    type: script\\n+    content: |\\n+      return { text: \\\"hello from seed\\\" };\\n+\\n+  convo:\\n+    type: ai\\n+    depends_on: [seed]\\n+    reuse_ai_session: self\\n+    session_mode: append\\n+    ai:\\n+      provider: mock\\n+      model: mock\\n+      disableTools: true\\n+      allowedTools: []\\n+      system_prompt: \\\"You are a tiny echo assistant.\\\"\\n+    prompt: |\\n+      Seed message: {{ outputs['seed'].text }}\\n+\\n+      Past convo outputs in this run:\\n+      {% assign hist = outputs_history['convo'] | default: empty %}\\n+      {% if hist and hist.size > 0 %}\\n+      {% for h in hist %}\\n+      - Previous reply {{ forloop.index }}.\\n+      {% endfor %}\\n+      {% else %}\\n+      - No previous replies yet.\\n+      {% endif %}\\n+    on_success:\\n+      goto_js: |\\n+        // Example: re‚Äëenter this step up to 3 times in a single run\\n+        return attempt < 3 ? 'convo' : null;\\n+```\\n+\\n+The corresponding testable example lives at:\\n+\\n+- `examples/session-reuse-self.yaml`\\n+\\n+This keeps the configuration small but shows how to wire `reuse_ai_session: self` and `session_mode: append` without touching higher‚Äëlevel workflows like `tyk-assistant`.\\n ```\\n \\n **When to use each mode:**\\n\",\"status\":\"modified\"},{\"filename\":\"docs/recipes.md\",\"additions\":149,\"deletions\":3,\"changes\":152,\"patch\":\"diff --git a/docs/recipes.md b/docs/recipes.md\\nindex 1015bc6c..1089e9a5 100644\\n--- a/docs/recipes.md\\n+++ b/docs/recipes.md\\n@@ -321,8 +321,154 @@ Tip: When you define a JSON Schema, you generally do **not** need to tell the mo\\n - Avoid noisy fallbacks like `(outputs['x']?.kind ?? '') === 'status'` when `outputs['x']?.kind === 'status'` is equivalent.\\n - These conventions apply uniformly to any provider (`ai`, `command`, `script`, `github`, `http_client`, etc).\\n \\n+### Command step best practices\\n+\\n+When using `type: command` steps:\\n+\\n+**Avoid external tool dependencies** like `jq`, `yq`, `python`, etc.:\\n+- They may not be installed in all environments (GitHub Actions, Docker, CI)\\n+- Use `transform_js` to parse and transform output instead\\n+- Keep shell commands simple: `grep`, `sed`, `awk`, `sort`, `head` are universally available\\n+\\n+```yaml\\n+# Bad - requires jq\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | jq -R -s 'split(\\\"\\\\n\\\")'\\n+  parseJson: true\\n+\\n+# Good - use transform_js for parsing\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | sort -u\\n+  transform_js: |\\n+    const lines = (output || '').trim().split('\\\\n').filter(Boolean);\\n+    return { data: lines, count: lines.length };\\n+```\\n+\\n+**Prefer line-separated output** over JSON from shell:\\n+- Simple to parse with `transform_js`\\n+- No need for `parseJson: true`\\n+- More robust across different shells/environments\\n+\\n+**Use transform_js for structured output**:\\n+- The sandbox provides `output` (command stdout as string)\\n+- Return an object with the fields you need\\n+- Works consistently across all environments\\n+\\n+### Testing workflows with `--no-mocks`\\n+\\n+The `--no-mocks` flag runs your test cases with real providers instead of injecting mock responses. This is essential for:\\n+\\n+1. **Debugging integration issues** - See actual API responses and errors\\n+2. **Capturing realistic mock data** - Get real output to copy into your test cases\\n+3. **Validating credentials** - Verify environment variables are set correctly\\n+4. **Developing new workflows** - Build tests incrementally with real data\\n+\\n+#### Basic usage\\n+\\n+```bash\\n+# Run all test cases with real providers\\n+visor test --config my-workflow.yaml --no-mocks\\n+\\n+# Run a specific test case with real providers\\n+visor test --config my-workflow.yaml --no-mocks --only \\\"my-test-case\\\"\\n+```\\n+\\n+#### Suggested mocks output\\n+\\n+When running with `--no-mocks`, Visor captures each step's output and prints it as YAML you can copy directly into your test case:\\n+\\n+```\\n+üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)\\n+   Step outputs will be captured and printed as suggested mocks\\n+\\n+... test execution ...\\n+\\n+üìã Suggested mocks (copy to your test case):\\n+mocks:\\n+  extract-keys:\\n+    data:\\n+      - PROJ-123\\n+      - DEV-456\\n+    count: 2\\n+  fetch-issues:\\n+    data:\\n+      - key: PROJ-123\\n+        summary: Fix authentication bug\\n+        status: In Progress\\n+```\\n+\\n+Copy the YAML under `mocks:` into your test case's `mocks:` section.\\n+\\n+#### Workflow for building tests\\n+\\n+1. **Start with a minimal test case** (no mocks):\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+   ```\\n+\\n+2. **Run with `--no-mocks`** to capture real outputs:\\n+   ```bash\\n+   visor test --config workflow.yaml --no-mocks --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+3. **Copy the suggested mocks** into your test case:\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+         mocks:\\n+           extract-keys:\\n+             data: [\\\"PROJ-123\\\"]\\n+             count: 1\\n+           # ... rest of captured mocks\\n+   ```\\n+\\n+4. **Add assertions** based on the real data:\\n+   ```yaml\\n+         expect:\\n+           workflow_output:\\n+             - path: issue_count\\n+               equals: 1\\n+   ```\\n+\\n+5. **Run normally** to verify mocks work:\\n+   ```bash\\n+   visor test --config workflow.yaml --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+#### Debugging with `--no-mocks`\\n+\\n+When a test fails with mocks, use `--no-mocks` to see what's actually happening:\\n+\\n+```bash\\n+# See real API responses and errors\\n+visor test --config workflow.yaml --no-mocks --only \\\"failing-test\\\"\\n+\\n+# Common issues revealed:\\n+# - Missing or expired credentials\\n+# - API endpoint changes\\n+# - Unexpected response formats\\n+# - Network/timeout issues\\n+```\\n+\\n+The real error messages and responses help identify whether the issue is with your mocks or the actual integration.\\n+\\n ### More examples\\n \\n-- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags  \\n-- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration  \\n+- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags\\n+- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration\\n - `examples/` ‚Äì MCP, Jira, and advanced configs\\n\",\"status\":\"modified\"},{\"filename\":\"package-lock.json\",\"additions\":252,\"deletions\":9,\"changes\":261,\"patch\":\"diff --git a/package-lock.json b/package-lock.json\\nindex 5fb3ad89..e5b10d27 100644\\n--- a/package-lock.json\\n+++ b/package-lock.json\\n@@ -7,6 +7,7 @@\\n     \\\"\\\": {\\n       \\\"name\\\": \\\"@probelabs/visor\\\",\\n       \\\"version\\\": \\\"0.1.42\\\",\\n+      \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"MIT\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@actions/core\\\": \\\"^1.11.1\\\",\\n@@ -16,7 +17,7 @@\\n         \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n         \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n         \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n         \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n         \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -54,6 +55,7 @@\\n         \\\"husky\\\": \\\"^9.1.7\\\",\\n         \\\"jest\\\": \\\"^30.1.3\\\",\\n         \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+        \\\"patch-package\\\": \\\"^8.0.1\\\",\\n         \\\"prettier\\\": \\\"^3.6.2\\\",\\n         \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n         \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n@@ -5942,9 +5944,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/maid\\\": {\\n-      \\\"version\\\": \\\"0.0.21\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.21.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-H3EA2tDjWsAPgXZAwfHrNoEXnHmQFS9+JGDK/YpeiF3g3FxPgnWBgtgYPgtNuETQsEXxR1RzDgQVJL2JHWRRHw==\\\",\\n+      \\\"version\\\": \\\"0.0.22\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.22.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-FYcyyP4BlVrHQmBznXFPpMbvUqFI6wbecHUUe4S+yIeDvJ8W4bP0ndZ0cMoc2p7hekWYAG4RtlohQBr8e4TKOg==\\\",\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@types/dagre\\\": \\\"^0.7.53\\\",\\n@@ -5960,9 +5962,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/probe\\\": {\\n-      \\\"version\\\": \\\"0.6.0-rc169\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc169.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-/laV3+1TSjASru8gGSvGwyjQ7jalMkFuZETl35uv37J1Hb4eG95CbKGU5NihcCrLXR44q1lKJiQGHXcAav2mBA==\\\",\\n+      \\\"version\\\": \\\"0.6.0-rc176\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc176.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-w/pAvnz2/yXb4FQXZV2QazxuW02xsI/wRg36yxZoLSG0O/hIncjF4rquZe3O+emceI2Ks5zbrA7cgQEdpwT8vA==\\\",\\n       \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n@@ -5972,7 +5974,7 @@\\n         \\\"@ai-sdk/openai\\\": \\\"^2.0.10\\\",\\n         \\\"@anthropic-ai/claude-agent-sdk\\\": \\\"^0.1.46\\\",\\n         \\\"@modelcontextprotocol/sdk\\\": \\\"^1.0.0\\\",\\n-        \\\"@probelabs/maid\\\": \\\"^0.0.21\\\",\\n+        \\\"@probelabs/maid\\\": \\\"^0.0.22\\\",\\n         \\\"adm-zip\\\": \\\"^0.5.16\\\",\\n         \\\"ai\\\": \\\"^5.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -8192,6 +8194,13 @@\\n         \\\"node\\\": \\\">= 20\\\"\\n       }\\n     },\\n+    \\\"node_modules/@yarnpkg/lockfile\\\": {\\n+      \\\"version\\\": \\\"1.1.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@yarnpkg/lockfile/-/lockfile-1.1.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-GpSwvyXOcOOlV70vbnzjj4fW5xW/FdUF6nQEt1ENy7m4ZCczi1+/buVUPAqmGfqznsORNFzUMjctTIp8a9tuCQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"BSD-2-Clause\\\"\\n+    },\\n     \\\"node_modules/accepts\\\": {\\n       \\\"version\\\": \\\"1.3.8\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/accepts/-/accepts-1.3.8.tgz\\\",\\n@@ -9180,6 +9189,25 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/call-bind\\\": {\\n+      \\\"version\\\": \\\"1.0.8\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind/-/call-bind-1.0.8.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-oKlSFMcMwpUg2ednkhQ454wfWiU/ul3CkJe/PEHcTKuiX6RpbehUiFMXu13HalGZxfUwCQzZG747YXBn1im9ww==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind-apply-helpers\\\": \\\"^1.0.0\\\",\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"set-function-length\\\": \\\"^1.2.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/call-bind-apply-helpers\\\": {\\n       \\\"version\\\": \\\"1.0.2\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind-apply-helpers/-/call-bind-apply-helpers-1.0.2.tgz\\\",\\n@@ -10613,6 +10641,24 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/define-data-property\\\": {\\n+      \\\"version\\\": \\\"1.1.4\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/define-data-property/-/define-data-property-1.1.4.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-rBMvIzlpA8v6E+SJZoo++HAYqsLrkg7MSfIinMPFhmkorw7X+dOXVJQs+QT69zGkzMyfDnIMN2Wid1+NbL3T+A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/define-lazy-prop\\\": {\\n       \\\"version\\\": \\\"3.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/define-lazy-prop/-/define-lazy-prop-3.0.0.tgz\\\",\\n@@ -11794,6 +11840,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/find-yarn-workspace-root\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/find-yarn-workspace-root/-/find-yarn-workspace-root-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-1IMnbjt4KzsQfnhnzNd8wUEgXZ44IzZaZmnLYx7D5FZlaHt2gW20Cri8Q+E/t5tIj4+epTBub+2Zxu/vNILzqQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Apache-2.0\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"micromatch\\\": \\\"^4.0.2\\\"\\n+      }\\n+    },\\n     \\\"node_modules/fix-dts-default-cjs-exports\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/fix-dts-default-cjs-exports/-/fix-dts-default-cjs-exports-1.0.1.tgz\\\",\\n@@ -12267,6 +12323,19 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/has-property-descriptors\\\": {\\n+      \\\"version\\\": \\\"1.0.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/has-property-descriptors/-/has-property-descriptors-1.0.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-55JNKuIW+vq4Ke1BjOTjM2YctQIvCT7GFzHwmfZPGo5wnrgkid0YQtnAleFSqumZm4az3n2BS+erby5ipJdgrg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/has-symbols\\\": {\\n       \\\"version\\\": \\\"1.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/has-symbols/-/has-symbols-1.1.0.tgz\\\",\\n@@ -12784,6 +12853,13 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/isarray\\\": {\\n+      \\\"version\\\": \\\"2.0.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/isarray/-/isarray-2.0.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-xHjhDr3cNBK0BzdUJSPXZntQUx/mwMS5Rw4A7lPJ90XGAO6ISP/ePDNuo0vhqOZU+UD5JoodwCAAoZQd3FeAKw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\"\\n+    },\\n     \\\"node_modules/isexe\\\": {\\n       \\\"version\\\": \\\"2.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz\\\",\\n@@ -13594,6 +13670,26 @@\\n       \\\"integrity\\\": \\\"sha512-NM8/P9n3XjXhIZn1lLhkFaACTOURQXjWhV4BA/RnOv8xvgqtqpAX9IO4mRQxSx1Rlo4tqzeqb0sOlruaOy3dug==\\\",\\n       \\\"license\\\": \\\"MIT\\\"\\n     },\\n+    \\\"node_modules/json-stable-stringify\\\": {\\n+      \\\"version\\\": \\\"1.3.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify/-/json-stable-stringify-1.3.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-qtYiSSFlwot9XHtF9bD9c7rwKjr+RecWT//ZnPvSmEjpV5mmPOCN4j8UjY5hbjNkOwZ/jQv3J6R1/pL7RwgMsg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind\\\": \\\"^1.0.8\\\",\\n+        \\\"call-bound\\\": \\\"^1.0.4\\\",\\n+        \\\"isarray\\\": \\\"^2.0.5\\\",\\n+        \\\"jsonify\\\": \\\"^0.0.1\\\",\\n+        \\\"object-keys\\\": \\\"^1.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/json-stable-stringify-without-jsonify\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify-without-jsonify/-/json-stable-stringify-without-jsonify-1.0.1.tgz\\\",\\n@@ -13633,6 +13729,16 @@\\n         \\\"graceful-fs\\\": \\\"^4.1.6\\\"\\n       }\\n     },\\n+    \\\"node_modules/jsonify\\\": {\\n+      \\\"version\\\": \\\"0.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/jsonify/-/jsonify-0.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-2/Ki0GcmuqSrgFyelQq9M05y7PS0mEwuIzrf3f1fPqkVDVRvZrPZtVSMHxdgo8Aq0sxAOb/cr2aqqA3LeWHVPg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Public Domain\\\",\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/katex\\\": {\\n       \\\"version\\\": \\\"0.16.25\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/katex/-/katex-0.16.25.tgz\\\",\\n@@ -13676,6 +13782,16 @@\\n       \\\"integrity\\\": \\\"sha512-Ls993zuzfayK269Svk9hzpeGUKob/sIgZzyHYdjQoAdQetRKpOLj+k/QQQ/6Qi0Yz65mlROrfd+Ev+1+7dz9Kw==\\\",\\n       \\\"dev\\\": true\\n     },\\n+    \\\"node_modules/klaw-sync\\\": {\\n+      \\\"version\\\": \\\"6.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/klaw-sync/-/klaw-sync-6.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-nIeuVSzdCCs6TDPTqI8w1Yre34sSq7AkZ4B3sfOBbI2CgVSB4Du4aLQijFU2+lhAFCwt9+42Hel6lQNIv6AntQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"graceful-fs\\\": \\\"^4.1.11\\\"\\n+      }\\n+    },\\n     \\\"node_modules/kolorist\\\": {\\n       \\\"version\\\": \\\"1.8.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/kolorist/-/kolorist-1.8.0.tgz\\\",\\n@@ -14785,6 +14901,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n       }\\n     },\\n+    \\\"node_modules/object-keys\\\": {\\n+      \\\"version\\\": \\\"1.1.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/object-keys/-/object-keys-1.1.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NuAESUOUMrlIXOfHKzD6bpPu3tYt3xvjNdRIQ+FeT0lNb4K8WR70CaDxhuNguS2XG+GjkyMwOzsN5ZktImfhLA==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/ohash\\\": {\\n       \\\"version\\\": \\\"2.0.11\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/ohash/-/ohash-2.0.11.tgz\\\",\\n@@ -15022,6 +15148,95 @@\\n         \\\"node\\\": \\\">= 0.8\\\"\\n       }\\n     },\\n+    \\\"node_modules/patch-package\\\": {\\n+      \\\"version\\\": \\\"8.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/patch-package/-/patch-package-8.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-VsKRIA8f5uqHQ7NGhwIna6Bx6D9s/1iXlA1hthBVBEbkq+t4kXD0HHt+rJhf/Z+Ci0F/HCB2hvn0qLdLG+Qxlw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"@yarnpkg/lockfile\\\": \\\"^1.1.0\\\",\\n+        \\\"chalk\\\": \\\"^4.1.2\\\",\\n+        \\\"ci-info\\\": \\\"^3.7.0\\\",\\n+        \\\"cross-spawn\\\": \\\"^7.0.3\\\",\\n+        \\\"find-yarn-workspace-root\\\": \\\"^2.0.0\\\",\\n+        \\\"fs-extra\\\": \\\"^10.0.0\\\",\\n+        \\\"json-stable-stringify\\\": \\\"^1.0.2\\\",\\n+        \\\"klaw-sync\\\": \\\"^6.0.0\\\",\\n+        \\\"minimist\\\": \\\"^1.2.6\\\",\\n+        \\\"open\\\": \\\"^7.4.2\\\",\\n+        \\\"semver\\\": \\\"^7.5.3\\\",\\n+        \\\"slash\\\": \\\"^2.0.0\\\",\\n+        \\\"tmp\\\": \\\"^0.2.4\\\",\\n+        \\\"yaml\\\": \\\"^2.2.2\\\"\\n+      },\\n+      \\\"bin\\\": {\\n+        \\\"patch-package\\\": \\\"index.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14\\\",\\n+        \\\"npm\\\": \\\">5\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/ci-info\\\": {\\n+      \\\"version\\\": \\\"3.9.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/ci-info/-/ci-info-3.9.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NIxF55hv4nSqQswkAeiOi1r83xy8JldOFDTWiug55KBu9Jnblncd2U6ViHmYgHf01TPZS77NJBhBMKdWj9HQMQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"funding\\\": [\\n+        {\\n+          \\\"type\\\": \\\"github\\\",\\n+          \\\"url\\\": \\\"https://github.com/sponsors/sibiraj-s\\\"\\n+        }\\n+      ],\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/is-docker\\\": {\\n+      \\\"version\\\": \\\"2.2.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/is-docker/-/is-docker-2.2.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-F+i2BKsFrH66iaUFc0woD8sLy8getkwTwtOBjvs56Cx4CgJDeKQeqfz8wAYiSb8JOprWhHH5p77PbmYCvvUuXQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"bin\\\": {\\n+        \\\"is-docker\\\": \\\"cli.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/open\\\": {\\n+      \\\"version\\\": \\\"7.4.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/open/-/open-7.4.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-MVHddDVweXZF3awtlAS+6pgKLlm/JgxZ90+/NBurBoQctVOOB/zDdVjcyPzQ+0laDGbsWgrRkflI65sQeOgT9Q==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"is-docker\\\": \\\"^2.0.0\\\",\\n+        \\\"is-wsl\\\": \\\"^2.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/slash\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/slash/-/slash-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-ZYKh3Wh2z1PpEXWr0MpSBZ0V6mZHAQfYevttO11c51CaWjGTaadiKZ+wVt1PbMlDV5qhMFslpZCemhwOK7C89A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=6\\\"\\n+      }\\n+    },\\n     \\\"node_modules/path-data-parser\\\": {\\n       \\\"version\\\": \\\"0.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/path-data-parser/-/path-data-parser-0.1.0.tgz\\\",\\n@@ -16473,6 +16688,24 @@\\n         \\\"node\\\": \\\">= 0.8.0\\\"\\n       }\\n     },\\n+    \\\"node_modules/set-function-length\\\": {\\n+      \\\"version\\\": \\\"1.2.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/set-function-length/-/set-function-length-1.2.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-pgRc4hJ4/sNjWCSS9AmnS40x3bNMDTknHgL5UaMBTMyJnU90EgWh1Rz+MC9eFu4BuN/UwZjKQuY/1v3rM7HMfg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"define-data-property\\\": \\\"^1.1.4\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"function-bind\\\": \\\"^1.1.2\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\",\\n+        \\\"has-property-descriptors\\\": \\\"^1.0.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/setprototypeof\\\": {\\n       \\\"version\\\": \\\"1.2.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/setprototypeof/-/setprototypeof-1.2.0.tgz\\\",\\n@@ -17384,6 +17617,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/tmp\\\": {\\n+      \\\"version\\\": \\\"0.2.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/tmp/-/tmp-0.2.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-voyz6MApa1rQGUxT3E+BK7/ROe8itEx7vD8/HEvt4xwXucvQ5G5oeEiHkmHZJuBO21RpOf+YYm9MOivj709jow==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14.14\\\"\\n+      }\\n+    },\\n     \\\"node_modules/tmpl\\\": {\\n       \\\"version\\\": \\\"1.0.5\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/tmpl/-/tmpl-1.0.5.tgz\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"package.json\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/package.json b/package.json\\nindex f9ccbb6c..302d728f 100644\\n--- a/package.json\\n+++ b/package.json\\n@@ -49,6 +49,7 @@\\n     \\\"test:with-build\\\": \\\"npm run build:cli && jest\\\",\\n     \\\"test:yaml\\\": \\\"node dist/index.js test --progress compact\\\",\\n     \\\"prepare\\\": \\\"husky\\\",\\n+    \\\"postinstall\\\": \\\"patch-package\\\",\\n     \\\"pre-commit\\\": \\\"lint-staged\\\",\\n     \\\"deploy:site\\\": \\\"cd site && npx wrangler pages deploy . --project-name=visor-site --commit-dirty=true\\\",\\n     \\\"deploy:worker\\\": \\\"npx wrangler deploy\\\",\\n@@ -96,7 +97,7 @@\\n     \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n     \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n     \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n     \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n     \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n     \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -143,6 +144,7 @@\\n     \\\"husky\\\": \\\"^9.1.7\\\",\\n     \\\"jest\\\": \\\"^30.1.3\\\",\\n     \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+    \\\"patch-package\\\": \\\"^8.0.1\\\",\\n     \\\"prettier\\\": \\\"^3.6.2\\\",\\n     \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n     \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"src/ai-review-service.ts\",\"additions\":101,\"deletions\":151,\"changes\":252,\"patch\":\"diff --git a/src/ai-review-service.ts b/src/ai-review-service.ts\\nindex 6b9c5b2e..f18accea 100644\\n--- a/src/ai-review-service.ts\\n+++ b/src/ai-review-service.ts\\n@@ -65,6 +65,13 @@ export interface AIReviewConfig {\\n   allowBash?: boolean;\\n   // Advanced bash command execution configuration\\n   bashConfig?: import('./types/config').BashConfig;\\n+  // Optional workspace root and allowed folders for ProbeAgent.\\n+  // When provided, these are forwarded to ProbeAgent so tools like search/query\\n+  // operate inside the isolated workspace/projects instead of the Visor repo root.\\n+  path?: string;\\n+  allowedFolders?: string[];\\n+  // Completion prompt for post-completion validation/review (runs after attempt_completion)\\n+  completionPrompt?: string;\\n }\\n \\n export interface AIDebugInfo {\\n@@ -278,7 +285,7 @@ export class AIReviewService {\\n     try {\\n       const call = this.callProbeAgent(prompt, schema, debugInfo, checkName, sessionId);\\n       const timeoutMs = Math.max(0, this.config.timeout || 0);\\n-      const { response, effectiveSchema } =\\n+      const { response, effectiveSchema, sessionId: usedSessionId } =\\n         timeoutMs > 0 ? await this.withTimeout(call, timeoutMs, 'AI review') : await call;\\n       const processingTime = Date.now() - startTime;\\n \\n@@ -290,6 +297,11 @@ export class AIReviewService {\\n \\n       const result = this.parseAIResponse(response, debugInfo, effectiveSchema);\\n \\n+      // Expose the session ID used for this call so the engine can reuse it later\\n+      try {\\n+        (result as any).sessionId = usedSessionId;\\n+      } catch {}\\n+\\n       if (debugInfo) {\\n         result.debug = debugInfo;\\n       }\\n@@ -651,17 +663,34 @@ ${prContext}${slackContextXml}\\n     const prContextInfo = prInfo as PRInfo & {\\n       isPRContext?: boolean;\\n       includeCodeContext?: boolean;\\n+      slackConversation?: unknown;\\n     };\\n     const isIssue = prContextInfo.isIssue === true;\\n \\n     // Check if we should include code context (diffs)\\n     const isPRContext = prContextInfo.isPRContext === true;\\n-    // In PR context, always include diffs. Otherwise check the flag.\\n-    const includeCodeContext = isPRContext || prContextInfo.includeCodeContext !== false;\\n+    const isSlackMode = prContextInfo.slackConversation !== undefined;\\n+\\n+    // Determine whether to include code context:\\n+    // - In explicit PR context (GitHub PR events), always include diffs\\n+    // - In Slack mode, default to NO code context unless explicitly requested\\n+    // - Otherwise, include code context unless explicitly disabled\\n+    let includeCodeContext: boolean;\\n+    if (isPRContext) {\\n+      includeCodeContext = true;\\n+    } else if (isSlackMode) {\\n+      // In Slack mode, only include code context if explicitly set to true\\n+      includeCodeContext = prContextInfo.includeCodeContext === true;\\n+    } else {\\n+      // Default: include unless explicitly disabled\\n+      includeCodeContext = prContextInfo.includeCodeContext !== false;\\n+    }\\n \\n     // Log the decision for transparency (debug level)\\n     if (isPRContext) {\\n       log('üîç Including full code diffs in AI context (PR mode)');\\n+    } else if (isSlackMode && !includeCodeContext) {\\n+      log('üí¨ Slack mode: excluding code diffs (use includeCodeContext: true to enable)');\\n     } else if (!includeCodeContext) {\\n       log('üìä Including only file summary in AI context (no diffs)');\\n     } else {\\n@@ -1537,7 +1566,15 @@ ${'='.repeat(60)}\\n     debugInfo?: AIDebugInfo,\\n     _checkName?: string,\\n     providedSessionId?: string\\n-  ): Promise<{ response: string; effectiveSchema?: string }> {\\n+  ): Promise<{ response: string; effectiveSchema?: string; sessionId: string }> {\\n+    // Derive a stable session ID for this call so the engine can reuse it later\\n+    const sessionId =\\n+      providedSessionId ||\\n+      (() => {\\n+        const timestamp = new Date().toISOString();\\n+        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n+      })();\\n+\\n     // Handle mock model/provider\\n     if (this.config.model === 'mock' || this.config.provider === 'mock') {\\n       const inJest = !!process.env.JEST_WORKER_ID;\\n@@ -1545,19 +1582,17 @@ ${'='.repeat(60)}\\n       if (!inJest) {\\n         // Fast path for CLI/integration: synthesize a mock response without invoking ProbeAgent\\n         const response = await this.generateMockResponse(prompt, _checkName, schema);\\n-        return { response, effectiveSchema: typeof schema === 'object' ? 'custom' : schema };\\n+        return {\\n+          response,\\n+          effectiveSchema: typeof schema === 'object' ? 'custom' : schema,\\n+          sessionId,\\n+        };\\n       }\\n       // In unit tests, still invoke ProbeAgent so tests can assert on options (schema) passed in\\n       // Fall through to normal flow below\\n     }\\n \\n     // Create ProbeAgent instance with proper options\\n-    const sessionId =\\n-      providedSessionId ||\\n-      (() => {\\n-        const timestamp = new Date().toISOString();\\n-        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n-      })();\\n \\n     log('ü§ñ Creating ProbeAgent for AI review...');\\n     log(`üÜî Session ID: ${sessionId}`);\\n@@ -1674,6 +1709,35 @@ ${'='.repeat(60)}\\n         (options as any).bashConfig = this.config.bashConfig;\\n       }\\n \\n+      // Pass completion prompt for post-completion validation/review\\n+      if (this.config.completionPrompt !== undefined) {\\n+        (options as any).completionPrompt = this.config.completionPrompt;\\n+      }\\n+\\n+      // Propagate workspace / allowed folders to ProbeAgent so that tools\\n+      // operate inside the isolated workspace and project checkouts instead\\n+      // of the Visor repository root.\\n+      try {\\n+        const cfgAny: any = this.config as any;\\n+        const allowedFolders = cfgAny.allowedFolders as string[] | undefined;\\n+        const workspacePath =\\n+          cfgAny.workspacePath || cfgAny.path || (Array.isArray(allowedFolders) && allowedFolders[0]);\\n+        if (Array.isArray(allowedFolders) && allowedFolders.length > 0) {\\n+          (options as any).allowedFolders = allowedFolders;\\n+          if (!options.path && workspacePath) {\\n+            (options as any).path = workspacePath;\\n+          }\\n+          log(`üóÇÔ∏è ProbeAgent workspace config:`);\\n+          log(`   path (cwd): ${(options as any).path}`);\\n+          log(`   allowedFolders[0]: ${allowedFolders[0]}`);\\n+        } else if (workspacePath) {\\n+          (options as any).path = workspacePath;\\n+          log(`üóÇÔ∏è ProbeAgent path: ${workspacePath} (no allowedFolders)`);\\n+        }\\n+      } catch {\\n+        // Best-effort only; fall back to ProbeAgent defaults on error.\\n+      }\\n+\\n       // Add provider-specific options if configured\\n       if (this.config.provider) {\\n         // Map claude-code to anthropic for ProbeAgent compatibility\\n@@ -2037,7 +2101,7 @@ ${'='.repeat(60)}\\n         log(`üîß Debug: Registered AI session for potential reuse: ${sessionId}`);\\n       }\\n \\n-      return { response, effectiveSchema };\\n+      return { response, effectiveSchema, sessionId };\\n     } catch (error) {\\n       console.error('‚ùå ProbeAgent failed:', error);\\n       throw new Error(\\n@@ -2191,13 +2255,23 @@ ${'='.repeat(60)}\\n         // For other schemas (code-review, etc.), extract and parse JSON with boundary detection\\n         log('üîç Extracting JSON from AI response...');\\n \\n-        // Try direct parsing first - if AI returned pure JSON\\n+        // Sanitize response: strip BOM, zero-width chars, and other invisible characters\\n+        // that can cause JSON parsing to fail even when the text looks valid\\n+        const sanitizedResponse = response\\n+          .replace(/^\\\\uFEFF/, '') // BOM\\n+          .replace(/[\\\\u200B-\\\\u200D\\\\uFEFF\\\\u00A0]/g, '') // Zero-width chars, NBSP\\n+          .replace(/[\\\\x00-\\\\x08\\\\x0B\\\\x0C\\\\x0E-\\\\x1F\\\\x7F]/g, '') // Control chars (except \\\\t \\\\n \\\\r)\\n+          .trim();\\n+\\n+        // Try direct JSON parsing - no bracket-matching extraction\\n+        // JSON validation is offloaded to Probe agent when schema is provided\\n         try {\\n-          reviewData = JSON.parse(response.trim());\\n+          reviewData = JSON.parse(sanitizedResponse);\\n           log('‚úÖ Successfully parsed direct JSON response');\\n           if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-        } catch {\\n-          log('üîç Direct parsing failed, trying to extract JSON from response...');\\n+        } catch (parseErr) {\\n+          const errMsg = parseErr instanceof Error ? parseErr.message : String(parseErr);\\n+          log(`üîç Direct JSON parsing failed: ${errMsg}`);\\n \\n           // If the response starts with \\\"I cannot\\\" or similar, it's likely a refusal\\n           if (\\n@@ -2210,66 +2284,16 @@ ${'='.repeat(60)}\\n             };\\n           }\\n \\n-          // Try to extract JSON using improved method with proper bracket matching\\n-          const jsonString = this.extractJsonFromResponse(response);\\n-\\n-          if (jsonString) {\\n-            try {\\n-              reviewData = JSON.parse(jsonString);\\n-              log('‚úÖ Successfully parsed extracted JSON');\\n-              if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-            } catch {\\n-              log('üîß Extracted JSON parsing failed, falling back to plain text handling...');\\n-\\n-              // Check if response is plain text and doesn't contain structured data\\n-              if (!response.includes('{') && !response.includes('}')) {\\n-                log('üîß Plain text response detected, creating structured fallback...');\\n-\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              } else {\\n-                // Fallback: treat the entire response as an issue\\n-                log('üîß Creating fallback response from non-JSON content...');\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              }\\n-            }\\n-          } else {\\n-            // No JSON found at all - treat as plain text response\\n-            log('üîß No JSON found in response, treating as plain text...');\\n-            reviewData = {\\n-              issues: [\\n-                {\\n-                  file: 'AI_RESPONSE',\\n-                  line: 1,\\n-                  ruleId: 'ai/raw_response',\\n-                  message: response,\\n-                  severity: 'info',\\n-                  category: 'documentation',\\n-                },\\n-              ],\\n-            };\\n-          }\\n+          // Not valid JSON - treat entire response as text output\\n+          // This allows Probe (or other AI providers) to handle JSON validation\\n+          // and avoids false positives from bracket-matching (e.g., mermaid diagrams)\\n+          log('üîß Treating response as plain text (no JSON extraction)');\\n+          const trimmed = response.trim();\\n+          return {\\n+            issues: [],\\n+            output: { text: trimmed },\\n+            debug: debugInfo,\\n+          };\\n         }\\n       }\\n \\n@@ -2457,80 +2481,6 @@ ${'='.repeat(60)}\\n     }\\n   }\\n \\n-  /**\\n-   * Extract JSON from a response that might contain surrounding text\\n-   * Uses proper bracket matching to find valid JSON objects or arrays\\n-   */\\n-  private extractJsonFromResponse(response: string): string | null {\\n-    const text = response.trim();\\n-\\n-    // Try to find JSON objects first (higher priority)\\n-    let bestJson = this.findJsonWithBracketMatching(text, '{', '}');\\n-\\n-    // If no object found, try arrays\\n-    if (!bestJson) {\\n-      bestJson = this.findJsonWithBracketMatching(text, '[', ']');\\n-    }\\n-\\n-    return bestJson;\\n-  }\\n-\\n-  /**\\n-   * Find JSON with proper bracket matching to avoid false positives\\n-   */\\n-  private findJsonWithBracketMatching(\\n-    text: string,\\n-    openChar: string,\\n-    closeChar: string\\n-  ): string | null {\\n-    const firstIndex = text.indexOf(openChar);\\n-    if (firstIndex === -1) return null;\\n-\\n-    let depth = 0;\\n-    let inString = false;\\n-    let escaping = false;\\n-\\n-    for (let i = firstIndex; i < text.length; i++) {\\n-      const char = text[i];\\n-\\n-      if (escaping) {\\n-        escaping = false;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\\\\\\' && inString) {\\n-        escaping = true;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\"' && !escaping) {\\n-        inString = !inString;\\n-        continue;\\n-      }\\n-\\n-      if (!inString) {\\n-        if (char === openChar) {\\n-          depth++;\\n-        } else if (char === closeChar) {\\n-          depth--;\\n-          if (depth === 0) {\\n-            // Found matching closing bracket\\n-            const candidate = text.substring(firstIndex, i + 1);\\n-            try {\\n-              JSON.parse(candidate); // Validate it's actually valid JSON\\n-              return candidate;\\n-            } catch {\\n-              // This wasn't valid JSON, keep looking\\n-              break;\\n-            }\\n-          }\\n-        }\\n-      }\\n-    }\\n-\\n-    return null;\\n-  }\\n-\\n   /**\\n    * Generate mock response for testing\\n    */\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli-main.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/src/cli-main.ts b/src/cli-main.ts\\nindex 7dbef501..e671fbd8 100644\\n--- a/src/cli-main.ts\\n+++ b/src/cli-main.ts\\n@@ -158,6 +158,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n   }\\n   const only = getArg('--only');\\n   const bail = hasFlag('--bail');\\n+  const noMocks = hasFlag('--no-mocks');\\n   const listOnly = hasFlag('--list');\\n   const validateOnly = hasFlag('--validate');\\n   const progress = (getArg('--progress') as 'compact' | 'detailed' | undefined) || 'compact';\\n@@ -224,6 +225,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       const agg = await runSuites(multiFiles, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallelSuites: maxParallelSuites || Math.max(1, require('os').cpus()?.length || 2),\\n         maxParallel,\\n         promptMaxChars,\\n@@ -340,6 +342,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       runRes = await runner.runCases(tpath, suite, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallel,\\n         promptMaxChars,\\n         engineMode: 'state-machine',\\n@@ -609,6 +612,9 @@ export async function main(): Promise<void> {\\n     // Set environment variables early for proper logging in all modules\\n     process.env.VISOR_OUTPUT_FORMAT = options.output;\\n     process.env.VISOR_DEBUG = options.debug ? 'true' : 'false';\\n+    if (options.keepWorkspace) {\\n+      process.env.VISOR_KEEP_WORKSPACE = 'true';\\n+    }\\n     // Configure centralized logger\\n     configureLoggerFromCli({\\n       output: options.output,\\n@@ -716,6 +722,31 @@ export async function main(): Promise<void> {\\n       const threads = slackAny.threads || 'any';\\n       const allow = Array.isArray(slackAny.channel_allowlist) ? slackAny.channel_allowlist : [];\\n       const appToken = slackAny.app_token || process.env.SLACK_APP_TOKEN;\\n+\\n+      // Initialize telemetry for Slack mode (normally done later for CLI runs).\\n+      if ((config as any)?.telemetry) {\\n+        const t = (config as any).telemetry as {\\n+          enabled?: boolean;\\n+          sink?: 'otlp' | 'file' | 'console';\\n+          file?: { dir?: string; ndjson?: boolean };\\n+          tracing?: { auto_instrumentations?: boolean; trace_report?: { enabled?: boolean } };\\n+        };\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true' || !!t?.enabled,\\n+          sink:\\n+            (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || t?.sink || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR || t?.file?.dir, ndjson: !!t?.file?.ndjson },\\n+          autoInstrument: !!t?.tracing?.auto_instrumentations,\\n+          traceReport: !!t?.tracing?.trace_report?.enabled,\\n+        });\\n+      } else {\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true',\\n+          sink: (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR },\\n+        });\\n+      }\\n+\\n       const runner = new SlackSocketRunner(engine, config, {\\n         appToken,\\n         endpoint,\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli.ts\",\"additions\":5,\"deletions\":1,\"changes\":6,\"patch\":\"diff --git a/src/cli.ts b/src/cli.ts\\nindex 134429b2..b1c46213 100644\\n--- a/src/cli.ts\\n+++ b/src/cli.ts\\n@@ -76,6 +76,7 @@ export class CLI {\\n         parseInt(value, 10)\\n       )\\n       .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText())\\n       .exitOverride(); // Prevent automatic process.exit for better error handling\\n \\n@@ -154,6 +155,7 @@ export class CLI {\\n           parseInt(value, 10)\\n         )\\n         .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+        .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n         .allowUnknownOption(false)\\n         .allowExcessArguments(false) // Don't allow positional arguments\\n         .addHelpText('after', this.getExamplesText())\\n@@ -224,6 +226,7 @@ export class CLI {\\n         message: options.message,\\n         githubV2: false,\\n         slack: Boolean(options.slack),\\n+        keepWorkspace: Boolean(options.keepWorkspace),\\n       };\\n     } catch (error: unknown) {\\n       // Handle commander.js exit overrides for help/version ONLY\\n@@ -348,6 +351,7 @@ export class CLI {\\n       .option('--debug-port <port>', 'Port for debug server (default: 3456)', value =>\\n         parseInt(value, 10)\\n       )\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText());\\n \\n     // Get the basic help and append examples manually if addHelpText doesn't work\\n\",\"status\":\"modified\"},{\"filename\":\"src/config.ts\",\"additions\":67,\"deletions\":17,\"changes\":84,\"patch\":\"diff --git a/src/config.ts b/src/config.ts\\nindex 407efa2d..02abe9df 100644\\n--- a/src/config.ts\\n+++ b/src/config.ts\\n@@ -20,6 +20,7 @@ import { ConfigLoader, ConfigLoaderOptions } from './utils/config-loader';\\n import { ConfigMerger } from './utils/config-merger';\\n import Ajv from 'ajv';\\n import addFormats from 'ajv-formats';\\n+import { validateJsSyntax } from './utils/sandbox';\\n \\n /**\\n  * Valid event triggers for checks\\n@@ -477,16 +478,29 @@ export class ConfigManager {\\n \\n     logger.info(`Registered workflow '${workflowId}' for standalone execution`);\\n \\n-    // Create a COMPLETELY NEW visor config with ONLY the test checks\\n-    // This prevents any workflow fields from leaking into the config\\n-    const visorConfig: Partial<VisorConfig> = {\\n+    // For standalone workflow testing, use the workflow's steps as checks\\n+    // This allows the test runner to execute the workflow steps directly\\n+    const workflowSteps = workflowData.steps || {};\\n+\\n+    // Create a config that includes the workflow steps as checks,\\n+    // plus the tests section and workflow metadata for output computation\\n+    const visorConfig: Partial<VisorConfig> & { tests?: any; outputs?: any; inputs?: any } = {\\n       version: '1.0',\\n-      steps: tests,\\n-      checks: tests, // Backward compatibility\\n+      steps: workflowSteps,\\n+      checks: workflowSteps,\\n+      tests: tests, // Preserve test harness config (may be empty if stripped by test runner)\\n     };\\n \\n-    logger.debug(`Standalone workflow config has ${Object.keys(tests).length} test checks`);\\n-    logger.debug(`Test check names: ${Object.keys(tests).join(', ')}`);\\n+    // Preserve workflow metadata for output computation during tests\\n+    if (workflowData.outputs) {\\n+      visorConfig.outputs = workflowData.outputs;\\n+    }\\n+    if (workflowData.inputs) {\\n+      visorConfig.inputs = workflowData.inputs;\\n+    }\\n+\\n+    logger.debug(`Standalone workflow config has ${Object.keys(workflowSteps).length} workflow steps as checks`);\\n+    logger.debug(`Workflow step names: ${Object.keys(workflowSteps).join(', ')}`);\\n     logger.debug(`Config keys after conversion: ${Object.keys(visorConfig).join(', ')}`);\\n \\n     return visorConfig;\\n@@ -982,26 +996,29 @@ export class ConfigManager {\\n \\n     // Validate reuse_ai_session configuration\\n     if (checkConfig.reuse_ai_session !== undefined) {\\n-      const isString = typeof checkConfig.reuse_ai_session === 'string';\\n-      const isBoolean = typeof checkConfig.reuse_ai_session === 'boolean';\\n+      const reuseValue = checkConfig.reuse_ai_session as unknown;\\n+      const isString = typeof reuseValue === 'string';\\n+      const isBoolean = typeof reuseValue === 'boolean';\\n+      const isSelf = reuseValue === 'self';\\n \\n       if (!isString && !isBoolean) {\\n         errors.push({\\n           field: `checks.${checkName}.reuse_ai_session`,\\n           message: `Invalid reuse_ai_session value for \\\"${checkName}\\\": must be string (check name) or boolean`,\\n-          value: checkConfig.reuse_ai_session,\\n+          value: reuseValue,\\n         });\\n-      } else if (isString) {\\n-        // When reuse_ai_session is a string, it must refer to a valid check\\n-        const targetCheckName = checkConfig.reuse_ai_session as string;\\n+      } else if (isString && !isSelf) {\\n+        // When reuse_ai_session is a string (other than the special 'self' value),\\n+        // it must refer to a valid check name\\n+        const targetCheckName = reuseValue as string;\\n         if (!config?.checks || !config.checks[targetCheckName]) {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" references non-existent check \\\"${targetCheckName}\\\" for session reuse`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n-      } else if (checkConfig.reuse_ai_session === true) {\\n+      } else if (reuseValue === true) {\\n         // When reuse_ai_session is true, depends_on must be specified and non-empty\\n         if (\\n           !checkConfig.depends_on ||\\n@@ -1011,7 +1028,7 @@ export class ConfigManager {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" has reuse_ai_session=true but missing or empty depends_on. Session reuse requires dependency on another check.`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n       }\\n@@ -1076,6 +1093,39 @@ export class ConfigManager {\\n         });\\n       }\\n     }\\n+\\n+    // Validate JavaScript syntax in transform_js and script content\\n+    try {\\n+      // Validate transform_js if present\\n+      const transformJs = (checkConfig as any).transform_js;\\n+      if (typeof transformJs === 'string' && transformJs.trim().length > 0) {\\n+        const result = validateJsSyntax(transformJs);\\n+        if (!result.valid) {\\n+          errors.push({\\n+            field: `checks.${checkName}.transform_js`,\\n+            message: `JavaScript syntax error in \\\"${checkName}\\\" transform_js: ${result.error}`,\\n+            value: transformJs.slice(0, 100) + (transformJs.length > 100 ? '...' : ''),\\n+          });\\n+        }\\n+      }\\n+\\n+      // Validate script content if type is 'script'\\n+      if (checkConfig.type === 'script') {\\n+        const content = (checkConfig as any).content;\\n+        if (typeof content === 'string' && content.trim().length > 0) {\\n+          const result = validateJsSyntax(content);\\n+          if (!result.valid) {\\n+            errors.push({\\n+              field: `checks.${checkName}.content`,\\n+              message: `JavaScript syntax error in \\\"${checkName}\\\" script: ${result.error}`,\\n+              value: content.slice(0, 100) + (content.length > 100 ? '...' : ''),\\n+            });\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Syntax validation is best-effort; don't fail the whole config on validation errors\\n+    }\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/failure-condition-evaluator.ts\",\"additions\":25,\"deletions\":9,\"changes\":34,\"patch\":\"diff --git a/src/failure-condition-evaluator.ts b/src/failure-condition-evaluator.ts\\nindex 3b54a015..5800df0a 100644\\n--- a/src/failure-condition-evaluator.ts\\n+++ b/src/failure-condition-evaluator.ts\\n@@ -137,6 +137,8 @@ export class FailureConditionEvaluator {\\n       previousResults?: Map<string, ReviewSummary>;\\n       authorAssociation?: string;\\n       workflowInputs?: Record<string, unknown>;\\n+      /** Current step's output for guarantee evaluation */\\n+      output?: unknown;\\n     }\\n   ): Promise<boolean> {\\n     // Build context for if evaluation\\n@@ -177,10 +179,10 @@ export class FailureConditionEvaluator {\\n       // Workflow inputs (for workflows)\\n       inputs: contextData?.workflowInputs || {},\\n \\n-      // Required output property (empty for if conditions)\\n-      output: {\\n-        issues: [],\\n-      },\\n+      // Output property: use provided output for guarantee evaluation, or empty for if conditions\\n+      output: contextData?.output !== undefined && contextData.output !== null && typeof contextData.output === 'object'\\n+        ? (contextData.output as Record<string, unknown>)\\n+        : { issues: [] },\\n       // Author association (used by permission helpers)\\n       authorAssociation: contextData?.authorAssociation,\\n \\n@@ -213,10 +215,10 @@ export class FailureConditionEvaluator {\\n               m && typeof m.get === 'function' ? m.get('all_valid', 'fact-validation') : undefined;\\n             memStr = ` mem.fact-validation.all_valid=${String(v)}`;\\n           } catch {}\\n+          // Debug if-eval output (only when VISOR_DEBUG enabled)\\n+          const outputKeys = Object.keys(context.outputs || {});\\n           console.error(\\n-            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" env.ENABLE_FACT_VALIDATION=${String(\\n-              (envMap as any).ENABLE_FACT_VALIDATION\\n-            )} event=${context.event?.event_name} result=${String(res)}${memStr}`\\n+            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" result=${String(res)} outputKeys=[${outputKeys.join(',')}]`\\n           );\\n         }\\n       } catch {}\\n@@ -480,7 +482,18 @@ export class FailureConditionEvaluator {\\n \\n       // Extract context variables\\n       const output = context.output || {};\\n-      const issues = output.issues || [];\\n+      // Ensure issues is an array - it might be a JSON string from workflow outputs\\n+      let issues = output.issues || [];\\n+      if (typeof issues === 'string') {\\n+        try {\\n+          issues = JSON.parse(issues);\\n+        } catch {\\n+          issues = [];\\n+        }\\n+      }\\n+      if (!Array.isArray(issues)) {\\n+        issues = [];\\n+      }\\n \\n       // Backward compatibility: provide metadata for transition period\\n       // TODO: Remove after all configurations are updated\\n@@ -518,6 +531,7 @@ export class FailureConditionEvaluator {\\n       const event = context.event || 'manual';\\n       const env = context.env || {};\\n       const outputs = context.outputs || {};\\n+      const inputs = context.inputs || {};\\n       const debugData = context.debug || null;\\n \\n       // Get memory store and create accessor for fail_if expressions\\n@@ -555,6 +569,7 @@ export class FailureConditionEvaluator {\\n         filesCount,\\n         event,\\n         env,\\n+        inputs,\\n         // Helper functions\\n         contains,\\n         startsWith,\\n@@ -621,6 +636,7 @@ export class FailureConditionEvaluator {\\n             filesCount,\\n             event,\\n             env,\\n+            inputs,\\n             // Helpers\\n             contains,\\n             startsWith,\\n\",\"status\":\"modified\"},{\"filename\":\"src/frontends/slack-frontend.ts\",\"additions\":107,\"deletions\":6,\"changes\":113,\"patch\":\"diff --git a/src/frontends/slack-frontend.ts b/src/frontends/slack-frontend.ts\\nindex 97161a7d..41735aa2 100644\\n--- a/src/frontends/slack-frontend.ts\\n+++ b/src/frontends/slack-frontend.ts\\n@@ -1,6 +1,32 @@\\n+/**\\n+ * Slack Frontend for Visor workflows.\\n+ *\\n+ * Features:\\n+ * - Posts AI replies to Slack threads\\n+ * - Converts Markdown to Slack mrkdwn format\\n+ * - Renders mermaid diagrams to PNG and uploads as images\\n+ * - Manages üëÄ/üëç reactions for acknowledgement\\n+ * - Handles human input prompts via prompt-state\\n+ *\\n+ * Mermaid Diagram Rendering:\\n+ * - Detects ```mermaid code blocks in AI responses\\n+ * - Renders to PNG using @mermaid-js/mermaid-cli (mmdc)\\n+ * - Uploads rendered images to Slack thread\\n+ * - Replaces mermaid blocks with \\\"_(See diagram above)_\\\" placeholder\\n+ *\\n+ * Requirements for mermaid rendering:\\n+ * - Node.js and npx in PATH\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ * - On Linux: apt-get install chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ */\\n import type { Frontend, FrontendContext } from './host';\\n import { SlackClient } from '../slack/client';\\n-import { formatSlackText } from '../slack/markdown';\\n+import {\\n+  formatSlackText,\\n+  extractMermaidDiagrams,\\n+  renderMermaidToPng,\\n+  replaceMermaidBlocks,\\n+} from '../slack/markdown';\\n \\n type SlackFrontendConfig = {\\n   defaultChannel?: string;\\n@@ -284,6 +310,9 @@ export class SlackFrontend implements Frontend {\\n       const isLogChat = providerType === 'log' && checkCfg.group === 'chat';\\n       if (!isAi && !isLogChat) return;\\n \\n+      // Skip internal steps - they're intermediate processing and shouldn't post to Slack\\n+      if (checkCfg.criticality === 'internal') return;\\n+\\n       // For AI checks, only post when using simple/unstructured schemas (or none).\\n       if (isAi) {\\n         const schema = checkCfg.schema;\\n@@ -316,6 +345,13 @@ export class SlackFrontend implements Frontend {\\n         ) {\\n           text = (result as any).content.trim();\\n         }\\n+      } else if (isLogChat && typeof (result as any)?.logOutput === 'string') {\\n+        // For log-based chat checks, render the formatted log output as the\\n+        // Slack message when no structured text field is present.\\n+        const raw = (result as any).logOutput;\\n+        if (raw.trim().length > 0) {\\n+          text = raw.trim();\\n+        }\\n       } else if (isAi && showRawOutput && out !== undefined) {\\n         try {\\n           text = JSON.stringify(out, null, 2);\\n@@ -325,13 +361,78 @@ export class SlackFrontend implements Frontend {\\n       }\\n       if (!text) return;\\n \\n-      const formattedText = formatSlackText(text);\\n+      // Extract and render mermaid diagrams before posting\\n+      const diagrams = extractMermaidDiagrams(text);\\n+      let processedText = text;\\n+\\n+      if (diagrams.length > 0) {\\n+        try {\\n+          ctx.logger.info(\\n+            `[slack-frontend] found ${diagrams.length} mermaid diagram(s) to render for ${checkId}`\\n+          );\\n+        } catch {}\\n+\\n+        // Render and upload each diagram\\n+        const uploadedCount: number[] = [];\\n+        for (let i = 0; i < diagrams.length; i++) {\\n+          const diagram = diagrams[i];\\n+          try {\\n+            ctx.logger.info(`[slack-frontend] rendering mermaid diagram ${i + 1}...`);\\n+            const pngBuffer = await renderMermaidToPng(diagram.code);\\n+            if (pngBuffer) {\\n+              ctx.logger.info(\\n+                `[slack-frontend] rendered diagram ${i + 1}, size=${pngBuffer.length} bytes, uploading...`\\n+              );\\n+              const filename = `diagram-${i + 1}.png`;\\n+              const uploadResult = await slack.files.uploadV2({\\n+                content: pngBuffer,\\n+                filename,\\n+                channel,\\n+                thread_ts: threadTs,\\n+                title: `Diagram ${i + 1}`,\\n+              });\\n+              if (uploadResult.ok) {\\n+                uploadedCount.push(i);\\n+                ctx.logger.info(`[slack-frontend] uploaded mermaid diagram ${i + 1} to ${channel}`);\\n+              } else {\\n+                ctx.logger.warn(`[slack-frontend] upload failed for diagram ${i + 1}`);\\n+              }\\n+            } else {\\n+              ctx.logger.warn(\\n+                `[slack-frontend] mermaid rendering returned null for diagram ${i + 1} (mmdc failed or not installed)`\\n+              );\\n+            }\\n+          } catch (e) {\\n+            ctx.logger.warn(\\n+              `[slack-frontend] failed to render/upload mermaid diagram ${i + 1}: ${\\n+                e instanceof Error ? e.message : String(e)\\n+              }`\\n+            );\\n+          }\\n+        }\\n+\\n+        // Replace mermaid blocks with placeholder text\\n+        if (uploadedCount.length > 0) {\\n+          processedText = replaceMermaidBlocks(text, diagrams, (idx) =>\\n+            uploadedCount.includes(idx) ? '_(See diagram above)_' : '_(Diagram rendering failed)_'\\n+          );\\n+        }\\n+      }\\n+\\n+      const formattedText = formatSlackText(processedText);\\n       await slack.chat.postMessage({ channel, text: formattedText, thread_ts: threadTs });\\n+      ctx.logger.info(\\n+        `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+      );\\n+    } catch (outerErr) {\\n+      // Log errors instead of silently swallowing them\\n       try {\\n-        ctx.logger.info(\\n-          `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+        ctx.logger.warn(\\n+          `[slack-frontend] maybePostDirectReply failed for ${checkId}: ${\\n+            outerErr instanceof Error ? outerErr.message : String(outerErr)\\n+          }`\\n         );\\n       } catch {}\\n-    } catch {}\\n+    }\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/generated/config-schema.ts\",\"additions\":233,\"deletions\":13,\"changes\":246,\"patch\":\"diff --git a/src/generated/config-schema.ts b/src/generated/config-schema.ts\\nindex b3caca4f..ecab390d 100644\\n--- a/src/generated/config-schema.ts\\n+++ b/src/generated/config-schema.ts\\n@@ -56,6 +56,20 @@ export const configSchema = {\\n           },\\n           description: 'Import workflow definitions from external files or URLs',\\n         },\\n+        inputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowInput',\\n+          },\\n+          description: 'Workflow inputs (for standalone reusable workflows)',\\n+        },\\n+        outputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowOutput',\\n+          },\\n+          description: 'Workflow outputs (for standalone reusable workflows)',\\n+        },\\n         steps: {\\n           $ref: '#/definitions/Record%3Cstring%2CCheckConfig%3E',\\n           description: 'Step configurations (recommended)',\\n@@ -67,7 +81,7 @@ export const configSchema = {\\n         },\\n         output: {\\n           $ref: '#/definitions/OutputConfig',\\n-          description: 'Output configuration',\\n+          description: 'Output configuration (optional - defaults provided)',\\n         },\\n         http_server: {\\n           $ref: '#/definitions/HttpServerConfig',\\n@@ -139,8 +153,16 @@ export const configSchema = {\\n           },\\n           description: 'Optional integrations: event-driven frontends (e.g., ndjson-sink, github)',\\n         },\\n+        workspace: {\\n+          $ref: '#/definitions/WorkspaceConfig',\\n+          description: 'Workspace isolation configuration for sandboxed execution',\\n+        },\\n+        slack: {\\n+          $ref: '#/definitions/SlackConfig',\\n+          description: 'Slack configuration',\\n+        },\\n       },\\n-      required: ['output', 'version'],\\n+      required: ['version'],\\n       patternProperties: {\\n         '^x-': {},\\n       },\\n@@ -243,6 +265,63 @@ export const configSchema = {\\n         type: 'string',\\n       },\\n     },\\n+    WorkflowInput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Input parameter name',\\n+        },\\n+        schema: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'JSON Schema for the input',\\n+        },\\n+        required: {\\n+          type: 'boolean',\\n+          description: 'Whether this input is required',\\n+        },\\n+        default: {\\n+          description: 'Default value if not provided',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow input definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    WorkflowOutput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Output name',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+        value: {\\n+          type: 'string',\\n+          description: 'Value using Liquid template syntax (references step outputs)',\\n+        },\\n+        value_js: {\\n+          type: 'string',\\n+          description: 'Value using JavaScript expression (alternative to value)',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow output definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n     'Record<string,CheckConfig>': {\\n       type: 'object',\\n       additionalProperties: {\\n@@ -390,11 +469,19 @@ export const configSchema = {\\n           description: 'Timeout in seconds for command execution (default: 60)',\\n         },\\n         depends_on: {\\n-          type: 'array',\\n-          items: {\\n-            type: 'string',\\n-          },\\n-          description: 'Check IDs that this check depends on (optional)',\\n+          anyOf: [\\n+            {\\n+              type: 'string',\\n+            },\\n+            {\\n+              type: 'array',\\n+              items: {\\n+                type: 'string',\\n+              },\\n+            },\\n+          ],\\n+          description:\\n+            'Check IDs that this check depends on (optional). Accepts single string or array.',\\n         },\\n         group: {\\n           type: 'string',\\n@@ -653,13 +740,93 @@ export const configSchema = {\\n           description: 'Arguments/inputs for the workflow',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Override specific step configurations in the workflow',\\n         },\\n         output_mapping: {\\n           $ref: '#/definitions/Record%3Cstring%2Cstring%3E',\\n           description: 'Map workflow outputs to check outputs',\\n         },\\n+        workflow_inputs: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'Alias for args - workflow inputs (backward compatibility)',\\n+        },\\n+        config: {\\n+          type: 'string',\\n+          description:\\n+            'Config file path - alternative to workflow ID (loads a Visor config file as workflow)',\\n+        },\\n+        workflow_overrides: {\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n+          description: 'Alias for overrides - workflow step overrides (backward compatibility)',\\n+        },\\n+        ref: {\\n+          type: 'string',\\n+          description: 'Git reference to checkout (branch, tag, commit SHA) - supports templates',\\n+        },\\n+        repository: {\\n+          type: 'string',\\n+          description: 'Repository URL or owner/repo format (defaults to current repository)',\\n+        },\\n+        token: {\\n+          type: 'string',\\n+          description: 'GitHub token for private repositories (defaults to GITHUB_TOKEN env)',\\n+        },\\n+        fetch_depth: {\\n+          type: 'number',\\n+          description: 'Number of commits to fetch (0 for full history, default: 1)',\\n+        },\\n+        fetch_tags: {\\n+          type: 'boolean',\\n+          description: 'Whether to fetch tags (default: false)',\\n+        },\\n+        submodules: {\\n+          anyOf: [\\n+            {\\n+              type: 'boolean',\\n+            },\\n+            {\\n+              type: 'string',\\n+              const: 'recursive',\\n+            },\\n+          ],\\n+          description: \\\"Checkout submodules: false, true, or 'recursive'\\\",\\n+        },\\n+        working_directory: {\\n+          type: 'string',\\n+          description: 'Working directory for the checkout (defaults to temp directory)',\\n+        },\\n+        use_worktree: {\\n+          type: 'boolean',\\n+          description: 'Use git worktree for efficient parallel checkouts (default: true)',\\n+        },\\n+        clean: {\\n+          type: 'boolean',\\n+          description: 'Clean the working directory before checkout (default: true)',\\n+        },\\n+        sparse_checkout: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description: 'Sparse checkout paths - only checkout specific directories/files',\\n+        },\\n+        lfs: {\\n+          type: 'boolean',\\n+          description: 'Enable Git LFS (Large File Storage)',\\n+        },\\n+        clone_timeout_ms: {\\n+          type: 'number',\\n+          description: 'Timeout in ms for cloning the bare repository (default: 300000 = 5 min)',\\n+        },\\n+        cleanup_on_failure: {\\n+          type: 'boolean',\\n+          description: 'Clean up worktree on failure (default: true)',\\n+        },\\n+        persist_worktree: {\\n+          type: 'boolean',\\n+          description: 'Keep worktree after workflow completion (default: false)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'Configuration for a single check',\\n@@ -794,6 +961,11 @@ export const configSchema = {\\n           $ref: '#/definitions/BashConfig',\\n           description: 'Advanced bash command execution configuration',\\n         },\\n+        completion_prompt: {\\n+          type: 'string',\\n+          description:\\n+            'Completion prompt for post-completion validation/review (runs after attempt_completion)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'AI provider configuration',\\n@@ -1223,7 +1395,7 @@ export const configSchema = {\\n           description: 'Custom output name (defaults to workflow name)',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Step overrides',\\n         },\\n         output_mapping: {\\n@@ -1238,14 +1410,14 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n-    'Record<string,Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>>':\\n+    'Record<string,Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>>':\\n       {\\n         type: 'object',\\n         additionalProperties: {\\n-          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E',\\n+          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E',\\n         },\\n       },\\n-    'Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>': {\\n+    'Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>': {\\n       type: 'object',\\n       additionalProperties: false,\\n     },\\n@@ -1811,6 +1983,54 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n+    WorkspaceConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        enabled: {\\n+          type: 'boolean',\\n+          description: 'Enable workspace isolation (default: true when config present)',\\n+        },\\n+        base_path: {\\n+          type: 'string',\\n+          description: 'Base path for workspaces (default: /tmp/visor-workspaces)',\\n+        },\\n+        cleanup_on_exit: {\\n+          type: 'boolean',\\n+          description: 'Clean up workspace on exit (default: true)',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Workspace isolation configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    SlackConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        version: {\\n+          type: 'string',\\n+          description: 'Slack API version',\\n+        },\\n+        mentions: {\\n+          type: 'string',\\n+          description: \\\"Mention handling: 'all', 'direct', etc.\\\",\\n+        },\\n+        threads: {\\n+          type: 'string',\\n+          description: \\\"Thread handling: 'required', 'optional', etc.\\\",\\n+        },\\n+        show_raw_output: {\\n+          type: 'boolean',\\n+          description: 'Show raw output in Slack responses',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Slack configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n   },\\n } as const;\\n export default configSchema;\\n\",\"status\":\"modified\"},{\"filename\":\"src/index.ts\",\"additions\":5,\"deletions\":2,\"changes\":7,\"patch\":\"diff --git a/src/index.ts b/src/index.ts\\nindex 50b89df3..9621e733 100644\\n--- a/src/index.ts\\n+++ b/src/index.ts\\n@@ -628,8 +628,11 @@ function resolveDependencies(\\n \\n     // Get dependencies for this check\\n     const checkConfig = config?.checks?.[checkId];\\n+    // Normalize depends_on to array (supports string | string[])\\n+    const rawDeps = checkConfig?.depends_on;\\n+    const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     // Expand OR groups (pipe syntax) for dependency closure discovery\\n-    const dependencies = (checkConfig?.depends_on || []).flatMap(d =>\\n+    const dependencies = depsArray.flatMap((d: string) =>\\n       typeof d === 'string' && d.includes('|')\\n         ? d\\n             .split('|')\\n\",\"status\":\"modified\"},{\"filename\":\"src/liquid-extensions.ts\",\"additions\":68,\"deletions\":1,\"changes\":69,\"patch\":\"diff --git a/src/liquid-extensions.ts b/src/liquid-extensions.ts\\nindex e9efaf68..dbf9c6ed 100644\\n--- a/src/liquid-extensions.ts\\n+++ b/src/liquid-extensions.ts\\n@@ -124,6 +124,26 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     }\\n   });\\n \\n+  // Register base64 filter for encoding strings\\n+  // Usage: {{ \\\"user:password\\\" | base64 }}\\n+  liquid.registerFilter('base64', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    return Buffer.from(str).toString('base64');\\n+  });\\n+\\n+  // Register base64_decode filter for decoding base64 strings\\n+  // Usage: {{ encoded_value | base64_decode }}\\n+  liquid.registerFilter('base64_decode', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    try {\\n+      return Buffer.from(str, 'base64').toString('utf-8');\\n+    } catch {\\n+      return '[Error: Invalid base64 string]';\\n+    }\\n+  });\\n+\\n   // Sanitize a label to allowed characters only: [A-Za-z0-9:/]\\n   liquid.registerFilter('safe_label', (value: unknown) => sanitizeLabel(value));\\n \\n@@ -137,6 +157,53 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     return s.replace(/\\\\\\\\n/g, '\\\\n').replace(/\\\\\\\\r/g, '\\\\r').replace(/\\\\\\\\t/g, '\\\\t');\\n   });\\n \\n+  // JSON escape filter - escapes a string for use inside a JSON string value\\n+  // This escapes special characters like quotes, backslashes, and control characters\\n+  // Usage: \\\"jql\\\": \\\"{{ myValue | json_escape }}\\\"\\n+  liquid.registerFilter('json_escape', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const s = String(value);\\n+    // Use JSON.stringify which handles all escaping, then strip the surrounding quotes\\n+    const jsonStr = JSON.stringify(s);\\n+    // Remove the first and last character (the quotes added by JSON.stringify)\\n+    return jsonStr.slice(1, -1);\\n+  });\\n+\\n+  // Shell escape filter - wraps value in single quotes with proper escaping\\n+  // Usage: {{ value | shell_escape }}\\n+  // Example: \\\"hello'world\\\" becomes \\\"'hello'\\\\''world'\\\"\\n+  // This is POSIX-compliant and safe for arbitrary text including mermaid diagrams\\n+  liquid.registerFilter('shell_escape', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    // Replace single quotes with: end quote, escaped quote, start quote\\n+    // Then wrap the entire thing in single quotes\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Alias for shell_escape\\n+  liquid.registerFilter('escape_shell', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Shell escape for double quotes (less safe but sometimes needed)\\n+  // Usage: {{ value | shell_escape_double }}\\n+  // Escapes: $, `, \\\\, \\\", and !\\n+  liquid.registerFilter('shell_escape_double', (value: unknown) => {\\n+    if (value == null) return '\\\"\\\"';\\n+    const s = String(value);\\n+    // Escape characters that have special meaning inside double quotes\\n+    const escaped = s\\n+      .replace(/\\\\\\\\/g, '\\\\\\\\\\\\\\\\')  // backslash first\\n+      .replace(/\\\\$/g, '\\\\\\\\$')   // dollar sign\\n+      .replace(/`/g, '\\\\\\\\`')    // backticks\\n+      .replace(/\\\"/g, '\\\\\\\\\\\"')    // double quotes\\n+      .replace(/!/g, '\\\\\\\\!');   // history expansion\\n+    return '\\\"' + escaped + '\\\"';\\n+  });\\n+\\n   // Register author permission filters (from main)\\n   // These filters check the author's permission level; detect local mode for tests\\n   const isLocal = detectLocalMode();\\n\",\"status\":\"modified\"},{\"filename\":\"src/logger.ts\",\"additions\":31,\"deletions\":2,\"changes\":33,\"patch\":\"diff --git a/src/logger.ts b/src/logger.ts\\nindex 6148b77c..aee7136e 100644\\n--- a/src/logger.ts\\n+++ b/src/logger.ts\\n@@ -85,7 +85,36 @@ class Logger {\\n       if (this.showTimestamps) {\\n         const ts = new Date().toISOString();\\n         const lvl = level ? level : undefined;\\n-        const prefix = lvl ? `[${ts}] [${lvl}]` : `[${ts}]`;\\n+\\n+        let tsToken = `[${ts}]`;\\n+        let lvlToken = lvl ? `[${lvl}]` : '';\\n+\\n+        // Add simple ANSI colour when running in a TTY and not emitting\\n+        // JSON/SARIF. Colours are intentionally minimal and only applied\\n+        // to the prefix markers, not the full line.\\n+        if (this.isTTY && !this.isJsonLike) {\\n+          const reset = '\\\\x1b[0m';\\n+          const dim = '\\\\x1b[2m';\\n+          const colours: Record<LogLevel, string> = {\\n+            silent: '',\\n+            error: '\\\\x1b[31m', // red\\n+            warn: '\\\\x1b[33m', // yellow\\n+            info: '\\\\x1b[36m', // cyan\\n+            verbose: '\\\\x1b[35m', // magenta\\n+            debug: '\\\\x1b[90m', // bright black / gray\\n+          };\\n+\\n+          tsToken = `${dim}${tsToken}${reset}`;\\n+\\n+          if (lvl) {\\n+            const colour = colours[lvl] || '';\\n+            if (colour) {\\n+              lvlToken = `${colour}${lvlToken}${reset}`;\\n+            }\\n+          }\\n+        }\\n+\\n+        const prefix = lvl ? `${tsToken} ${lvlToken}` : tsToken;\\n         process.stderr.write(`${prefix} ${msg}\\\\n`);\\n       } else {\\n         process.stderr.write(msg + '\\\\n');\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/ai-check-provider.ts\",\"additions\":100,\"deletions\":6,\"changes\":106,\"patch\":\"diff --git a/src/providers/ai-check-provider.ts b/src/providers/ai-check-provider.ts\\nindex 53f1a350..b41d6af1 100644\\n--- a/src/providers/ai-check-provider.ts\\n+++ b/src/providers/ai-check-provider.ts\\n@@ -186,7 +186,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     let promptContent: string;\\n \\n@@ -204,7 +205,8 @@ export class AICheckProvider extends CheckProvider {\\n       eventContext,\\n       dependencyResults,\\n       outputHistory,\\n-      args\\n+      args,\\n+      workflowInputs\\n     );\\n   }\\n \\n@@ -335,7 +337,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     // Build outputs_raw from -raw keys (aggregate parent values)\\n     const outputsRaw: Record<string, unknown> = {};\\n@@ -535,6 +538,8 @@ export class AICheckProvider extends CheckProvider {\\n       outputs_raw: outputsRaw,\\n       // Custom arguments from on_init 'with' directive\\n       args: args || {},\\n+      // Workflow inputs (for nested workflow steps to access parent inputs like {{ inputs.context }})\\n+      inputs: workflowInputs || {},\\n     };\\n \\n     try {\\n@@ -621,7 +626,8 @@ export class AICheckProvider extends CheckProvider {\\n         console.error(`[ai-exec] step=${String((config as any).checkName || 'unknown')}`);\\n       }\\n     } catch {}\\n-    // Extract AI configuration - only set properties that are explicitly provided\\n+    // Extract AI configuration - only set properties that are explicitly provided.\\n+    // Workspace / allowedFolders will be derived below from the execution context.\\n     const aiConfig: AIReviewConfig = {};\\n \\n     // Check-level AI configuration (ai object)\\n@@ -671,6 +677,9 @@ export class AICheckProvider extends CheckProvider {\\n       if (aiAny.bashConfig !== undefined) {\\n         aiConfig.bashConfig = aiAny.bashConfig as import('../types/config').BashConfig;\\n       }\\n+      if (aiAny.completion_prompt !== undefined) {\\n+        aiConfig.completionPrompt = aiAny.completion_prompt as string;\\n+      }\\n       if (aiAny.skip_code_context !== undefined) {\\n         // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n         (aiConfig as any).skip_code_context = aiAny.skip_code_context as boolean;\\n@@ -691,6 +700,90 @@ export class AICheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Derive workspace-aware allowedFolders for ProbeAgent when workspace\\n+    // isolation is enabled. This ensures tools like search/query operate\\n+    // inside the isolated workspace (and its project symlinks) instead of\\n+    // the Visor repository root.\\n+    // Folder names are human-readable (tyk-docs, visor2) thanks to WorkspaceManager.\\n+    try {\\n+      const ctxAny: any = sessionInfo as any;\\n+      const parentCtx = ctxAny?._parentContext;\\n+      const workspace = parentCtx?.workspace;\\n+\\n+      // Enhanced debug logging for workspace propagation diagnosis\\n+      logger.debug(`[AI Provider] Workspace detection for check '${(config as any).checkName || 'unknown'}':`);\\n+      logger.debug(`[AI Provider]   sessionInfo exists: ${!!sessionInfo}`);\\n+      logger.debug(`[AI Provider]   _parentContext exists: ${!!parentCtx}`);\\n+      logger.debug(`[AI Provider]   workspace exists: ${!!workspace}`);\\n+      if (workspace) {\\n+        logger.debug(`[AI Provider]   workspace.isEnabled exists: ${typeof workspace.isEnabled === 'function'}`);\\n+        logger.debug(`[AI Provider]   workspace.isEnabled(): ${typeof workspace.isEnabled === 'function' ? workspace.isEnabled() : 'N/A'}`);\\n+        const projectCount = typeof workspace.listProjects === 'function' ? workspace.listProjects()?.length : 'N/A';\\n+        logger.debug(`[AI Provider]   workspace.listProjects() count: ${projectCount}`);\\n+      }\\n+\\n+      if (workspace && typeof workspace.isEnabled === 'function' && workspace.isEnabled()) {\\n+        const folders: string[] = [];\\n+        let workspaceRoot: string | undefined;\\n+        let mainProjectPath: string | undefined;\\n+        try {\\n+          const info = workspace.getWorkspaceInfo?.();\\n+          if (info && typeof info.workspacePath === 'string') {\\n+            workspaceRoot = info.workspacePath;\\n+            mainProjectPath = info.mainProjectPath;\\n+            // Add workspace root\\n+            folders.push(info.workspacePath);\\n+            // Include main project path (has human-readable name like \\\"visor2\\\")\\n+            if (mainProjectPath) {\\n+              folders.push(mainProjectPath);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore workspace info errors\\n+        }\\n+        try {\\n+          const projects = workspace.listProjects?.() || [];\\n+          for (const proj of projects as any[]) {\\n+            if (proj && typeof proj.path === 'string') {\\n+              // Project paths have human-readable names (tyk-docs, not checkout-tyk-docs)\\n+              folders.push(proj.path);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore project listing errors\\n+        }\\n+        const unique = Array.from(new Set(folders.filter(p => typeof p === 'string' && p)));\\n+        if (unique.length > 0 && workspaceRoot) {\\n+          (aiConfig as any).allowedFolders = unique;\\n+          // Set path and cwd to workspace root - AI will run with cwd set to workspace\\n+          // Both are set for compatibility: path for older probe versions, cwd for rc175+\\n+          (aiConfig as any).path = workspaceRoot;\\n+          (aiConfig as any).cwd = workspaceRoot;\\n+          // Also set workspacePath for the AI to know the root\\n+          (aiConfig as any).workspacePath = workspaceRoot;\\n+          logger.debug(`[AI Provider] Workspace isolation enabled:`);\\n+          logger.debug(`[AI Provider]   workspaceRoot (cwd): ${workspaceRoot}`);\\n+          logger.debug(`[AI Provider]   allowedFolders: ${JSON.stringify(unique)}`);\\n+        }\\n+      } else if (parentCtx && typeof parentCtx.workingDirectory === 'string') {\\n+        // Fallback: when workspace is not available (or disabled), still\\n+        // constrain tools to the engine's working directory so ProbeAgent\\n+        // operates inside the same logical root as the state machine. This\\n+        // also ensures nested workflows (e.g. code-question-helper) use the\\n+        // workspace main project path once initializeWorkspace has updated\\n+        // the parent context.\\n+        if (!(aiConfig as any).allowedFolders) {\\n+          (aiConfig as any).allowedFolders = [parentCtx.workingDirectory];\\n+        }\\n+        if (!(aiConfig as any).path) {\\n+          (aiConfig as any).path = parentCtx.workingDirectory;\\n+          (aiConfig as any).cwd = parentCtx.workingDirectory;\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only; fall back to defaults on error.\\n+    }\\n+\\n     // Check-level AI model and provider (top-level properties)\\n     if (config.ai_model !== undefined) {\\n       aiConfig.model = config.ai_model as string;\\n@@ -876,7 +969,8 @@ export class AICheckProvider extends CheckProvider {\\n       ctxWithStage,\\n       _dependencyResults,\\n       (config as any).__outputHistory as Map<string, unknown[]> | undefined,\\n-      (sessionInfo as any)?.args\\n+      (sessionInfo as any)?.args,\\n+      (config as any).workflowInputs as Record<string, unknown> | undefined\\n     );\\n \\n     // Optional persona (vendor extension): ai.ai_persona or ai_persona.\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/command-check-provider.ts\",\"additions\":50,\"deletions\":16,\"changes\":66,\"patch\":\"diff --git a/src/providers/command-check-provider.ts b/src/providers/command-check-provider.ts\\nindex e0c9d634..bf7ffd43 100644\\n--- a/src/providers/command-check-provider.ts\\n+++ b/src/providers/command-check-provider.ts\\n@@ -131,7 +131,8 @@ export class CommandCheckProvider extends CheckProvider {\\n       // New: outputs_raw exposes aggregate values (e.g., full arrays for forEach parents)\\n       outputs_raw: outputsRaw,\\n       // Workflow inputs (when executing within a workflow)\\n-      inputs: context?.workflowInputs || {},\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any).workflowInputs || context?.workflowInputs || {},\\n       // Custom arguments from on_init 'with' directive\\n       args: context?.args || {},\\n       env: this.getSafeEnvironmentVariables(),\\n@@ -178,17 +179,31 @@ export class CommandCheckProvider extends CheckProvider {\\n           mock = rawMock as Record<string, unknown>;\\n         }\\n         const m = mock as { stdout?: string; stderr?: string; exit_code?: number; exit?: number };\\n-        let out: unknown = m.stdout ?? '';\\n-        try {\\n-          if (\\n-            typeof out === 'string' &&\\n-            (out.trim().startsWith('{') || out.trim().startsWith('['))\\n-          ) {\\n-            out = JSON.parse(out);\\n-          }\\n-        } catch {}\\n-        const code =\\n-          typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0;\\n+\\n+        // Check if this looks like a command mock (has stdout/stderr/exit_code) or direct data output\\n+        const isCommandMock = m.stdout !== undefined || m.stderr !== undefined ||\\n+          m.exit_code !== undefined || m.exit !== undefined;\\n+\\n+        let out: unknown;\\n+        if (isCommandMock) {\\n+          // Traditional command mock format: { stdout: \\\"...\\\", exit_code: 0 }\\n+          out = m.stdout ?? '';\\n+          try {\\n+            if (\\n+              typeof out === 'string' &&\\n+              (out.trim().startsWith('{') || out.trim().startsWith('['))\\n+            ) {\\n+              out = JSON.parse(out);\\n+            }\\n+          } catch {}\\n+        } else {\\n+          // Direct data mock format: { data: [...], count: 1 } - use as-is\\n+          out = mock;\\n+        }\\n+\\n+        const code = isCommandMock\\n+          ? (typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0)\\n+          : 0;\\n         if (code !== 0) {\\n           return {\\n             issues: [\\n@@ -231,9 +246,8 @@ export class CommandCheckProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Get timeout from config (in seconds) or use default (60 seconds)\\n-      const timeoutSeconds = (config.timeout as number) || 60;\\n-      const timeoutMs = timeoutSeconds * 1000;\\n+      // Get timeout from config (in milliseconds) or use default (60 seconds = 60000ms)\\n+      const timeoutMs = (config.timeout as number) || 60000;\\n \\n       // Normalize only the eval payload for `node -e|--eval` invocations that may contain\\n       // literal newlines due to YAML processing (\\\"\\\\n\\\" -> newline). We re-escape newlines\\n@@ -1533,6 +1547,26 @@ ${bodyWithReturn}\\n       return null;\\n     }\\n \\n+    // Require at least one issue-specific field beyond just a message-like field.\\n+    // This prevents objects like {text: \\\"...\\\"} from being treated as issues.\\n+    const hasIssueField =\\n+      data.file !== undefined ||\\n+      data.path !== undefined ||\\n+      data.filename !== undefined ||\\n+      data.line !== undefined ||\\n+      data.startLine !== undefined ||\\n+      data.lineNumber !== undefined ||\\n+      data.severity !== undefined ||\\n+      data.level !== undefined ||\\n+      data.priority !== undefined ||\\n+      data.ruleId !== undefined ||\\n+      data.rule !== undefined ||\\n+      data.category !== undefined ||\\n+      data.type !== undefined;\\n+    if (!hasIssueField) {\\n+      return null;\\n+    }\\n+\\n     const allowedSeverities = new Set(['info', 'warning', 'error', 'critical']);\\n     const severityRaw = this.toTrimmedString(data.severity || data.level || data.priority);\\n     let severity: ReviewIssue['severity'] = 'warning';\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/git-checkout-provider.ts\",\"additions\":41,\"deletions\":6,\"changes\":47,\"patch\":\"diff --git a/src/providers/git-checkout-provider.ts b/src/providers/git-checkout-provider.ts\\nindex baac0001..6d3a2852 100644\\n--- a/src/providers/git-checkout-provider.ts\\n+++ b/src/providers/git-checkout-provider.ts\\n@@ -69,6 +69,16 @@ export class GitCheckoutProvider extends CheckProvider {\\n       return false;\\n     }\\n \\n+    if (checkoutConfig.clone_timeout_ms !== undefined) {\\n+      if (\\n+        typeof checkoutConfig.clone_timeout_ms !== 'number' ||\\n+        checkoutConfig.clone_timeout_ms <= 0\\n+      ) {\\n+        logger.error('Invalid config: clone_timeout_ms must be a positive number (milliseconds)');\\n+        return false;\\n+      }\\n+    }\\n+\\n     return true;\\n   }\\n \\n@@ -91,7 +101,13 @@ export class GitCheckoutProvider extends CheckProvider {\\n       );\\n \\n       // Resolve dynamic variables\\n-      const resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      let resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      // If ref resolves to an empty string, fall back to HEAD so we rely on\\n+      // the repository's default branch. This keeps downstream git commands\\n+      // well-defined and matches existing expectations/tests.\\n+      if (!resolvedRef || resolvedRef.trim().length === 0) {\\n+        resolvedRef = 'HEAD';\\n+      }\\n       const resolvedRepository = checkoutConfig.repository\\n         ? await this.liquid.parseAndRender(checkoutConfig.repository, templateContext)\\n         : process.env.GITHUB_REPOSITORY || 'unknown/unknown';\\n@@ -118,6 +134,7 @@ export class GitCheckoutProvider extends CheckProvider {\\n           clean: checkoutConfig.clean !== false, // Default: true\\n           workflowId: (context as any)?.workflowId,\\n           fetchDepth: checkoutConfig.fetch_depth,\\n+          cloneTimeoutMs: checkoutConfig.clone_timeout_ms,\\n         }\\n       );\\n \\n@@ -134,18 +151,35 @@ export class GitCheckoutProvider extends CheckProvider {\\n \\n       // Add project to workspace if workspace isolation is enabled\\n       const workspace = (context as any)?._parentContext?.workspace;\\n+\\n+      // Enhanced debug logging for workspace project addition diagnosis\\n+      const checkName = (config as any)?.checkName || 'unknown';\\n+      logger.info(`[GitCheckout] Workspace check for '${checkName}':`);\\n+      logger.info(`[GitCheckout]   _parentContext exists: ${!!(context as any)?._parentContext}`);\\n+      logger.info(`[GitCheckout]   workspace exists: ${!!workspace}`);\\n+      logger.info(`[GitCheckout]   workspace.isEnabled(): ${workspace?.isEnabled?.() ?? 'N/A'}`);\\n+      if (workspace) {\\n+        const projectCountBefore = workspace.listProjects?.()?.length ?? 'N/A';\\n+        logger.debug(`[GitCheckout]   projects before addProject: ${projectCountBefore}`);\\n+      }\\n+\\n       if (workspace?.isEnabled()) {\\n         try {\\n+          // Don't pass checkName as description - let workspace use repo name\\n+          // This results in human-readable names like \\\"tyk-docs\\\" instead of \\\"checkout-tyk-docs\\\"\\n           const workspacePath = await workspace.addProject(\\n             resolvedRepository,\\n-            worktree.path,\\n-            checkoutConfig.checkName\\n+            worktree.path\\n           );\\n           output.workspace_path = workspacePath;\\n-          logger.debug(`Added project to workspace: ${workspacePath}`);\\n+          const projectCountAfter = workspace.listProjects?.()?.length ?? 'N/A';\\n+          logger.debug(`[GitCheckout] Added project to workspace: ${workspacePath}`);\\n+          logger.debug(`[GitCheckout]   projects after addProject: ${projectCountAfter}`);\\n         } catch (error) {\\n           logger.warn(`Failed to add project to workspace: ${error}`);\\n         }\\n+      } else {\\n+        logger.debug(`[GitCheckout] Workspace not enabled, skipping addProject`);\\n       }\\n \\n       logger.info(\\n@@ -228,7 +262,8 @@ export class GitCheckoutProvider extends CheckProvider {\\n       outputs: outputsObj,\\n       outputs_history: historyObj,\\n       env: safeEnv,\\n-      inputs: context?.workflowInputs,\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any)?.workflowInputs || context?.workflowInputs,\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/http-client-provider.ts\",\"additions\":279,\"deletions\":35,\"changes\":314,\"patch\":\"diff --git a/src/providers/http-client-provider.ts b/src/providers/http-client-provider.ts\\nindex 4d8c41be..6db81591 100644\\n--- a/src/providers/http-client-provider.ts\\n+++ b/src/providers/http-client-provider.ts\\n@@ -4,18 +4,29 @@ import { ReviewSummary } from '../reviewer';\\n import { Liquid } from 'liquidjs';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n import { EnvironmentResolver } from '../utils/env-resolver';\\n+import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n+import { buildProviderTemplateContext } from '../utils/template-context';\\n+import Sandbox from '@nyariv/sandboxjs';\\n+import { logger } from '../logger';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n \\n /**\\n  * Check provider that fetches data from HTTP endpoints\\n  */\\n export class HttpClientProvider extends CheckProvider {\\n   private liquid: Liquid;\\n+  private sandbox?: Sandbox;\\n \\n   constructor() {\\n     super();\\n     this.liquid = createExtendedLiquid();\\n   }\\n \\n+  private createSecureSandbox(): Sandbox {\\n+    return createSecureSandbox();\\n+  }\\n+\\n   getName(): string {\\n     return 'http_client';\\n   }\\n@@ -61,50 +72,124 @@ export class HttpClientProvider extends CheckProvider {\\n     const headers = (config.headers as Record<string, string>) || {};\\n     const timeout = (config.timeout as number) || 30000;\\n     const transform = config.transform as string | undefined;\\n+    const transformJs = config.transform_js as string | undefined;\\n     const bodyTemplate = config.body as string | undefined;\\n+    const outputFileTemplate = config.output_file as string | undefined;\\n+    const skipIfExists = config.skip_if_exists !== false; // Default true for caching\\n \\n-    try {\\n-      // Prepare template context for URL and body\\n-      const templateContext = {\\n-        pr: {\\n-          number: prInfo.number,\\n-          title: prInfo.title,\\n-          body: prInfo.body,\\n-          author: prInfo.author,\\n-          base: prInfo.base,\\n-          head: prInfo.head,\\n-          totalAdditions: prInfo.totalAdditions,\\n-          totalDeletions: prInfo.totalDeletions,\\n-        },\\n-        outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-        env: process.env,\\n-      };\\n+    // Track resolved URL for error messages\\n+    let resolvedUrlForErrors = url;\\n \\n-      // Render URL with template if it contains liquid syntax\\n-      let renderedUrl = url;\\n-      if (url.includes('{{') || url.includes('{%')) {\\n-        renderedUrl = await this.liquid.parseAndRender(url, templateContext);\\n+    try {\\n+      // Use shared template context builder for consistent output extraction\\n+      const templateContext = buildProviderTemplateContext(\\n+        prInfo,\\n+        dependencyResults,\\n+        undefined, // memoryStore\\n+        undefined, // outputHistory\\n+        undefined, // stageHistoryBase\\n+        { attachMemoryReadHelpers: false }\\n+      );\\n+      // Add env to context for shell-style variable resolution\\n+      (templateContext as Record<string, unknown>).env = process.env;\\n+\\n+      // First resolve shell-style environment variables (${VAR}, $VAR, ${{ env.VAR }})\\n+      let renderedUrl = String(EnvironmentResolver.resolveValue(url));\\n+      resolvedUrlForErrors = renderedUrl; // Track for error messages\\n+\\n+      // Then render Liquid templates if present\\n+      if (renderedUrl.includes('{{') || renderedUrl.includes('{%')) {\\n+        renderedUrl = await this.liquid.parseAndRender(renderedUrl, templateContext);\\n+        resolvedUrlForErrors = renderedUrl; // Update after Liquid rendering\\n       }\\n \\n       // Prepare request body if provided\\n       let requestBody: string | undefined;\\n       if (bodyTemplate) {\\n-        const renderedBody = await this.liquid.parseAndRender(bodyTemplate, templateContext);\\n-        requestBody = renderedBody;\\n+        // First resolve shell-style environment variables\\n+        let resolvedBody = String(EnvironmentResolver.resolveValue(bodyTemplate));\\n+        // Then render Liquid templates if present\\n+        if (resolvedBody.includes('{{') || resolvedBody.includes('{%')) {\\n+          resolvedBody = await this.liquid.parseAndRender(resolvedBody, templateContext);\\n+        }\\n+        requestBody = resolvedBody;\\n       }\\n \\n-      // Resolve environment variables in headers\\n-      const resolvedHeaders = EnvironmentResolver.resolveHeaders(headers);\\n+      // Resolve environment variables and Liquid templates in headers\\n+      const resolvedHeaders: Record<string, string> = {};\\n+      for (const [key, value] of Object.entries(headers)) {\\n+        let resolvedValue = String(EnvironmentResolver.resolveValue(value));\\n+        // Render Liquid templates if present\\n+        if (resolvedValue.includes('{{') || resolvedValue.includes('{%')) {\\n+          resolvedValue = await this.liquid.parseAndRender(resolvedValue, templateContext);\\n+        }\\n+        resolvedHeaders[key] = resolvedValue;\\n+        // Debug auth header (mask most of the value for security)\\n+        if (key.toLowerCase() === 'authorization') {\\n+          const maskedValue = resolvedValue.length > 20\\n+            ? `${resolvedValue.substring(0, 15)}...${resolvedValue.substring(resolvedValue.length - 5)}`\\n+            : resolvedValue;\\n+          logger.verbose(`[http_client] ${key}: ${maskedValue}`);\\n+        }\\n+      }\\n+\\n+      // Resolve output_file path if specified\\n+      let resolvedOutputFile: string | undefined;\\n+      if (outputFileTemplate) {\\n+        let outputPath = String(EnvironmentResolver.resolveValue(outputFileTemplate));\\n+        if (outputPath.includes('{{') || outputPath.includes('{%')) {\\n+          outputPath = await this.liquid.parseAndRender(outputPath, templateContext);\\n+        }\\n+        resolvedOutputFile = outputPath.trim();\\n+\\n+        // Check if file already exists (caching)\\n+        if (skipIfExists && fs.existsSync(resolvedOutputFile)) {\\n+          const stats = fs.statSync(resolvedOutputFile);\\n+          logger.verbose(`[http_client] File cached: ${resolvedOutputFile} (${stats.size} bytes)`);\\n+          return {\\n+            issues: [],\\n+            file_path: resolvedOutputFile,\\n+            size: stats.size,\\n+            cached: true,\\n+          } as unknown as ReviewSummary;\\n+        }\\n+      }\\n \\n       // Test hook: mock HTTP response for this step\\n       const stepName = (config as any).checkName || 'unknown';\\n       const mock = context?.hooks?.mockForStep?.(String(stepName));\\n-      const data =\\n-        mock !== undefined\\n-          ? mock\\n-          : await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n \\n-      // Apply transformation if specified\\n+      // If mock is provided, return it directly as the step output (with issues: [])\\n+      if (mock !== undefined) {\\n+        const mockObj = typeof mock === 'object' && mock !== null ? mock : { data: mock };\\n+        return {\\n+          issues: [],\\n+          ...mockObj,\\n+        } as unknown as ReviewSummary & { data: unknown };\\n+      }\\n+\\n+      // Debug log the request for troubleshooting\\n+      logger.verbose(`[http_client] ${method} ${renderedUrl}`);\\n+      if (requestBody) {\\n+        logger.verbose(`[http_client] Body: ${requestBody.substring(0, 500)}${requestBody.length > 500 ? '...' : ''}`);\\n+      }\\n+\\n+      // If output_file is specified, download to file instead of returning data\\n+      if (resolvedOutputFile) {\\n+        const fileResult = await this.downloadToFile(\\n+          renderedUrl,\\n+          method,\\n+          resolvedHeaders,\\n+          requestBody,\\n+          timeout,\\n+          resolvedOutputFile\\n+        );\\n+        return fileResult;\\n+      }\\n+\\n+      const data = await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n+\\n+      // Apply Liquid transformation if specified\\n       let processedData = data;\\n       if (transform) {\\n         try {\\n@@ -136,12 +221,53 @@ export class HttpClientProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Return the fetched data as a custom field for dependent checks to access\\n+      // Apply JavaScript transformation if specified\\n+      if (transformJs) {\\n+        try {\\n+          if (!this.sandbox) {\\n+            this.sandbox = this.createSecureSandbox();\\n+          }\\n+\\n+          // Create scope for JavaScript transform (scope, not context)\\n+          const jsScope: Record<string, unknown> = {\\n+            output: data,\\n+            pr: templateContext.pr,\\n+            outputs: templateContext.outputs,\\n+            env: process.env,\\n+          };\\n+\\n+          const result = compileAndRun(this.sandbox, transformJs, jsScope, {\\n+            injectLog: true,\\n+            logPrefix: 'üîç [transform_js]',\\n+            wrapFunction: true,\\n+          });\\n+          processedData = result;\\n+          logger.verbose(`‚úì Applied JavaScript transform successfully`);\\n+        } catch (error) {\\n+          logger.error(\\n+            `‚úó Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`\\n+          );\\n+          return {\\n+            issues: [\\n+              {\\n+                file: 'http_client',\\n+                line: 0,\\n+                ruleId: 'http_client/transform_js_error',\\n+                message: `Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              },\\n+            ],\\n+          };\\n+        }\\n+      }\\n+\\n+      // Return the fetched data in the standard `output` property for guarantee evaluation\\n+      // This is consistent with other providers (script, command, ai, etc.)\\n       return {\\n         issues: [],\\n-        // Add custom data field that will be passed through to dependent checks\\n-        data: processedData,\\n-      } as ReviewSummary & { data: unknown };\\n+        output: processedData,\\n+      } as unknown as ReviewSummary;\\n     } catch (error) {\\n       return {\\n         issues: [\\n@@ -149,7 +275,7 @@ export class HttpClientProvider extends CheckProvider {\\n             file: 'http_client',\\n             line: 0,\\n             ruleId: 'http_client/fetch_error',\\n-            message: `Failed to fetch from ${url}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            message: `Failed to fetch from ${resolvedUrlForErrors}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n             severity: 'error',\\n             category: 'logic',\\n           },\\n@@ -198,7 +324,14 @@ export class HttpClientProvider extends CheckProvider {\\n \\n       clearTimeout(timeoutId);\\n \\n+      logger.verbose(`[http_client] Response: ${response.status} ${response.statusText}`);\\n+\\n       if (!response.ok) {\\n+        // Log response body for debugging auth failures\\n+        try {\\n+          const errorBody = await response.text();\\n+          logger.warn(`[http_client] Error body: ${errorBody.substring(0, 500)}`);\\n+        } catch {}\\n         throw new Error(`HTTP ${response.status}: ${response.statusText}`);\\n       }\\n \\n@@ -233,6 +366,112 @@ export class HttpClientProvider extends CheckProvider {\\n     }\\n   }\\n \\n+  private async downloadToFile(\\n+    url: string,\\n+    method: string,\\n+    headers: Record<string, string>,\\n+    body: string | undefined,\\n+    timeout: number,\\n+    outputFile: string\\n+  ): Promise<ReviewSummary> {\\n+    // Check if fetch is available (Node 18+)\\n+    if (typeof fetch === 'undefined') {\\n+      throw new Error('HTTP client provider requires Node.js 18+ or node-fetch package');\\n+    }\\n+\\n+    const controller = new AbortController();\\n+    const timeoutId = setTimeout(() => controller.abort(), timeout);\\n+\\n+    try {\\n+      const requestOptions: RequestInit = {\\n+        method,\\n+        headers: { ...headers },\\n+        signal: controller.signal,\\n+      };\\n+\\n+      // Add body for non-GET requests\\n+      if (method !== 'GET' && body) {\\n+        requestOptions.body = body;\\n+        if (!headers['Content-Type'] && !headers['content-type']) {\\n+          requestOptions.headers = {\\n+            ...requestOptions.headers,\\n+            'Content-Type': 'application/json',\\n+          };\\n+        }\\n+      }\\n+\\n+      const response = await fetch(url, requestOptions);\\n+      clearTimeout(timeoutId);\\n+\\n+      if (!response.ok) {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_error',\\n+              message: `Failed to download file: HTTP ${response.status}: ${response.statusText}`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      // Create parent directory if it doesn't exist\\n+      const parentDir = path.dirname(outputFile);\\n+      if (parentDir && !fs.existsSync(parentDir)) {\\n+        fs.mkdirSync(parentDir, { recursive: true });\\n+      }\\n+\\n+      // Get the response as an ArrayBuffer and write to file\\n+      const arrayBuffer = await response.arrayBuffer();\\n+      const buffer = Buffer.from(arrayBuffer);\\n+      fs.writeFileSync(outputFile, buffer);\\n+\\n+      const contentType = response.headers.get('content-type') || 'application/octet-stream';\\n+      logger.verbose(`[http_client] Downloaded: ${outputFile} (${buffer.length} bytes)`);\\n+\\n+      return {\\n+        issues: [],\\n+        file_path: outputFile,\\n+        size: buffer.length,\\n+        content_type: contentType,\\n+        cached: false,\\n+      } as unknown as ReviewSummary;\\n+    } catch (error: unknown) {\\n+      clearTimeout(timeoutId);\\n+\\n+      if (error instanceof Error && error.name === 'AbortError') {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_timeout',\\n+              message: `Download timed out after ${timeout}ms`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      return {\\n+        issues: [\\n+          {\\n+            file: 'http_client',\\n+            line: 0,\\n+            ruleId: 'http_client/download_error',\\n+            message: `Failed to download file: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            severity: 'error',\\n+            category: 'logic',\\n+          },\\n+        ],\\n+      };\\n+    }\\n+  }\\n+\\n   getSupportedConfigKeys(): string[] {\\n     return [\\n       'type',\\n@@ -241,7 +480,10 @@ export class HttpClientProvider extends CheckProvider {\\n       'headers',\\n       'body',\\n       'transform',\\n+      'transform_js',\\n       'timeout',\\n+      'output_file',\\n+      'skip_if_exists',\\n       'depends_on',\\n       'on',\\n       'if',\\n@@ -261,6 +503,8 @@ export class HttpClientProvider extends CheckProvider {\\n       'Network access to the endpoint',\\n       'Optional: Transform template for processing response data',\\n       'Optional: Body template for POST/PUT requests',\\n+      'Optional: output_file path to download response to a file',\\n+      'Optional: skip_if_exists (default: true) to enable caching for file downloads',\\n     ];\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/log-check-provider.ts\",\"additions\":19,\"deletions\":8,\"changes\":27,\"patch\":\"diff --git a/src/providers/log-check-provider.ts b/src/providers/log-check-provider.ts\\nindex 9c6332d1..b57f632c 100644\\n--- a/src/providers/log-check-provider.ts\\n+++ b/src/providers/log-check-provider.ts\\n@@ -78,7 +78,8 @@ export class LogCheckProvider extends CheckProvider {\\n       includeDependencies,\\n       includeMetadata,\\n       config.__outputHistory as Map<string, unknown[]> | undefined,\\n-      context\\n+      context,\\n+      config\\n     );\\n \\n     // Render the log message template\\n@@ -100,12 +101,20 @@ export class LogCheckProvider extends CheckProvider {\\n     else if (level === 'debug') logger.debug(logOutput);\\n     else logger.info(logOutput);\\n \\n-    // Return with the log content as custom data for dependent checks\\n-    return {\\n+    // Return with the log content as custom data for dependent checks.\\n+    // For chat-style logs (group: chat), also expose a structured\\n+    // `output.text` field so frontends like Slack can post a clean\\n+    // human-facing message without the level prefix.\\n+    const summary: ReviewSummary & { logOutput: string; output?: unknown } = {\\n       issues: [],\\n-      // Add log output as custom field\\n       logOutput,\\n-    } as ReviewSummary & { logOutput: string };\\n+    };\\n+\\n+    if ((config as any).group === 'chat') {\\n+      (summary as any).output = { text: renderedMessage };\\n+    }\\n+\\n+    return summary;\\n   }\\n \\n   private buildTemplateContext(\\n@@ -115,7 +124,8 @@ export class LogCheckProvider extends CheckProvider {\\n     _includeDependencies: boolean = true,\\n     includeMetadata: boolean = true,\\n     outputHistory?: Map<string, unknown[]>,\\n-    executionContext?: ExecutionContext\\n+    executionContext?: ExecutionContext,\\n+    config?: CheckProviderConfig\\n   ): Record<string, unknown> {\\n     const context: Record<string, unknown> = {};\\n \\n@@ -197,7 +207,8 @@ export class LogCheckProvider extends CheckProvider {\\n     }\\n \\n     // Add workflow inputs if available\\n-    const workflowInputs = executionContext?.workflowInputs || {};\\n+    // Check config first (set by projectWorkflowToGraph), then fall back to executionContext\\n+    const workflowInputs = (config as any)?.workflowInputs || executionContext?.workflowInputs || {};\\n     logger.debug(\\n       `[LogProvider] Adding ${Object.keys(workflowInputs).length} workflow inputs to context`\\n     );\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/script-check-provider.ts\",\"additions\":37,\"deletions\":1,\"changes\":38,\"patch\":\"diff --git a/src/providers/script-check-provider.ts b/src/providers/script-check-provider.ts\\nindex 9825bb69..947dbd73 100644\\n--- a/src/providers/script-check-provider.ts\\n+++ b/src/providers/script-check-provider.ts\\n@@ -59,6 +59,16 @@ export class ScriptCheckProvider extends CheckProvider {\\n       reuseSession?: boolean;\\n     } & import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n+    // Test hook: mock output for this step (short-circuit execution)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = _sessionInfo?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        // Return mock directly as step output\\n+        return { issues: [], output: mock } as ReviewSummary & { output: unknown };\\n+      }\\n+    } catch {}\\n+\\n     const script = String(config.content || '');\\n     const memoryStore = MemoryStore.getInstance();\\n     const ctx = buildProviderTemplateContext(\\n@@ -72,10 +82,36 @@ export class ScriptCheckProvider extends CheckProvider {\\n     // Keep provider quiet by default; no step-specific debug\\n     // (historical ad-hoc logs removed to avoid hardcoding step names).\\n \\n+    // Add workflow inputs to the context\\n+    const inputs = (config as any).workflowInputs || _sessionInfo?.workflowInputs || {};\\n+    (ctx as any).inputs = inputs;\\n+\\n+    // Add environment variables to context (consistent with http-client-provider)\\n+    (ctx as any).env = process.env;\\n+\\n     // Attach synchronous memory ops consistent with memory provider\\n     const { ops, needsSave } = createSyncMemoryOps(memoryStore);\\n     (ctx as any).memory = ops as unknown as Record<string, unknown>;\\n \\n+    // Add helper functions to the context\\n+    (ctx as any).escapeXml = (str: unknown): string => {\\n+      if (str == null) return '';\\n+      return String(str)\\n+        .replace(/&/g, '&amp;')\\n+        .replace(/</g, '&lt;')\\n+        .replace(/>/g, '&gt;')\\n+        .replace(/\\\"/g, '&quot;')\\n+        .replace(/'/g, '&apos;');\\n+    };\\n+\\n+    // Add btoa/atob for base64 encoding/decoding (browser API polyfill)\\n+    (ctx as any).btoa = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'binary').toString('base64');\\n+    };\\n+    (ctx as any).atob = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'base64').toString('binary');\\n+    };\\n+\\n     // Evaluate the script in a secure sandbox (per-execution instance)\\n     const sandbox = this.createSecureSandbox();\\n     let result: unknown;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/workflow-check-provider.ts\",\"additions\":164,\"deletions\":43,\"changes\":207,\"patch\":\"diff --git a/src/providers/workflow-check-provider.ts b/src/providers/workflow-check-provider.ts\\nindex 116d4dff..681703ce 100644\\n--- a/src/providers/workflow-check-provider.ts\\n+++ b/src/providers/workflow-check-provider.ts\\n@@ -67,34 +67,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n   ): Promise<ReviewSummary> {\\n     const cfg = config as CheckProviderConfig & { workflow?: string; config?: string };\\n     const isConfigPathMode = !!cfg.config && !cfg.workflow;\\n+    const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n \\n-    // Test harness support: allow mocking workflow checks as a black box.\\n-    // If a mock is provided for this step, short-circuit nested execution and\\n-    // return a ReviewSummary with optional output field.\\n-    try {\\n-      const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n-      // test-runner passes hooks on execution context\\n-      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n-      if (mock !== undefined) {\\n-        const ms = mock as any;\\n-        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n-        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n-        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n-        const summary: ReviewSummary & { output?: unknown } = {\\n-          issues: issuesArr,\\n-          output: out,\\n-          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n-        } as any;\\n-        return summary;\\n-      }\\n-    } catch {}\\n-\\n-    // Resolve workflow definition\\n+    // Resolve workflow definition FIRST (needed for input preparation and validation)\\n     let workflow: WorkflowDefinition | undefined;\\n     let workflowId = cfg.workflow as string | undefined;\\n \\n     if (isConfigPathMode) {\\n-      const parentCwd = ((context as any)?._parentContext?.workingDirectory ||\\n+      // Use originalWorkingDirectory for config file resolution - workflow configs\\n+      // should be loaded from the original project path, not the sandbox\\n+      const parentCwd = ((context as any)?._parentContext?.originalWorkingDirectory ||\\n+        (context as any)?._parentContext?.workingDirectory ||\\n+        (context as any)?.originalWorkingDirectory ||\\n         (context as any)?.workingDirectory ||\\n         process.cwd()) as string;\\n       workflow = await this.loadWorkflowFromConfigPath(String(cfg.config), parentCwd);\\n@@ -109,9 +93,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       logger.info(`Executing workflow '${workflowId}'`);\\n     }\\n \\n-    // Prepare inputs\\n+    // Prepare inputs - do this BEFORE mock check so we can validate inputs even when mocked\\n+    // This allows tests to assert that the correct inputs were passed to workflow steps\\n     const inputs = await this.prepareInputs(workflow, config, prInfo, dependencyResults);\\n \\n+    // Capture resolved workflow inputs for testing assertions (reuse prompt capture infrastructure)\\n+    // This allows using `prompts` assertions with `provider: 'workflow'` to verify inputs\\n+    try {\\n+      // Serialize inputs to capture for assertions - specifically the context field\\n+      // which contains the rendered template with dependency outputs\\n+      const inputsCapture = Object.entries(inputs)\\n+        .map(([k, v]) => `${k}: ${typeof v === 'string' ? v : JSON.stringify(v)}`)\\n+        .join('\\\\n\\\\n');\\n+      (context as any)?.hooks?.onPromptCaptured?.({\\n+        step: String(stepName),\\n+        provider: 'workflow',\\n+        prompt: inputsCapture,\\n+      });\\n+    } catch {\\n+      // Ignore capture errors - this is only for testing\\n+    }\\n+\\n     // Validate inputs\\n     const validation = this.registry.validateInputs(workflow, inputs);\\n     if (!validation.valid) {\\n@@ -119,6 +121,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       throw new Error(`Invalid workflow inputs: ${errors}`);\\n     }\\n \\n+    // Test harness support: allow mocking workflow checks as a black box.\\n+    // If a mock is provided for this step, short-circuit nested execution and\\n+    // return a ReviewSummary with optional output field.\\n+    // NOTE: This happens AFTER input preparation/validation so tests can assert inputs are correct\\n+    try {\\n+      // test-runner passes hooks on execution context\\n+      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        const ms = mock as any;\\n+        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n+        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n+        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n+        const summary: ReviewSummary & { output?: unknown } = {\\n+          issues: issuesArr,\\n+          output: out,\\n+          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n+        } as any;\\n+        return summary;\\n+      }\\n+    } catch {}\\n+\\n     // Apply overrides to workflow steps if specified\\n     const modifiedWorkflow = this.applyOverrides(workflow, config);\\n \\n@@ -211,6 +234,80 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Extract eventContext for slack/conversation\\n+    const eventContext = config.eventContext || {};\\n+\\n+    // Debug logging for conversation context\\n+    logger.debug(`[WorkflowProvider] prepareInputs for ${workflow.id}`);\\n+    logger.debug(`[WorkflowProvider] eventContext keys: ${Object.keys(eventContext).join(', ') || 'none'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.slack: ${eventContext.slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.conversation: ${(eventContext as any).conversation ? 'present' : 'absent'}`);\\n+\\n+    // Extract slack context (if provided via eventContext.slack)\\n+    const slack = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        const slackCtx = anyCtx?.slack;\\n+        if (slackCtx && typeof slackCtx === 'object') return slackCtx;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Extract unified conversation context across transports (Slack & GitHub)\\n+    const conversation = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        if (anyCtx?.slack?.conversation) return anyCtx.slack.conversation;\\n+        if (anyCtx?.github?.conversation) return anyCtx.github.conversation;\\n+        if (anyCtx?.conversation) return anyCtx.conversation;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Debug logging for extracted context\\n+    logger.debug(`[WorkflowProvider] slack extracted: ${slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] conversation extracted: ${conversation ? 'present' : 'absent'}`);\\n+    if (conversation) {\\n+      logger.debug(`[WorkflowProvider] conversation.messages count: ${Array.isArray((conversation as any).messages) ? (conversation as any).messages.length : 0}`);\\n+    }\\n+\\n+    // Extract output history from config (passed via __outputHistory)\\n+    const outputHistory = (config as any).__outputHistory as Map<string, unknown[]> | undefined;\\n+    const outputs_history: Record<string, unknown[]> = {};\\n+    if (outputHistory) {\\n+      for (const [k, v] of outputHistory.entries()) {\\n+        outputs_history[k] = v;\\n+      }\\n+    }\\n+\\n+    // Build template context with all available data\\n+    // Extract .output from each dependency result so that outputs['step-name'].field works naturally\\n+    // (not outputs['step-name'].output.field)\\n+    const outputsMap: Record<string, unknown> = {};\\n+    logger.debug(`[WorkflowProvider] dependencyResults: ${dependencyResults ? dependencyResults.size : 'undefined'} entries`);\\n+    if (dependencyResults) {\\n+      for (const [key, result] of dependencyResults.entries()) {\\n+        // Extract the output property, or use the whole result if output is undefined\\n+        const extracted = (result as any).output ?? result;\\n+        outputsMap[key] = extracted;\\n+        // Debug: log what we extracted for each dependency\\n+        const extractedKeys = extracted && typeof extracted === 'object' ? Object.keys(extracted).join(', ') : 'not-object';\\n+        logger.debug(`[WorkflowProvider] outputs['${key}']: keys=[${extractedKeys}]`);\\n+      }\\n+    }\\n+    const templateContext = {\\n+      pr: prInfo,\\n+      outputs: outputsMap,\\n+      env: process.env,\\n+      slack,\\n+      conversation,\\n+      outputs_history,\\n+    };\\n+\\n     // Apply user-provided inputs (args)\\n     const userInputs = config.args || config.workflow_inputs; // Support both for compatibility\\n     if (userInputs) {\\n@@ -219,11 +316,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         if (typeof value === 'string') {\\n           // Check if it's a Liquid template\\n           if (value.includes('{{') || value.includes('{%')) {\\n-            inputs[key] = await this.liquid.parseAndRender(value, {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            });\\n+            inputs[key] = await this.liquid.parseAndRender(value, templateContext);\\n+            // Debug: log rendered template value for important inputs\\n+            if (key === 'text' || key === 'question' || key === 'context') {\\n+              const rendered = String(inputs[key]);\\n+              logger.info(`[WorkflowProvider] Rendered '${key}' input (${rendered.length} chars): ${rendered.substring(0, 500)}${rendered.length > 500 ? '...' : ''}`);\\n+            }\\n           } else {\\n             inputs[key] = value;\\n           }\\n@@ -234,11 +332,7 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           inputs[key] = compileAndRun(\\n             sandbox,\\n             exprValue.expression,\\n-            {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            },\\n+            templateContext,\\n             { injectLog: true, logPrefix: `workflow.input.${key}` }\\n           );\\n         } else {\\n@@ -367,6 +461,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n     try {\\n       await childMemory.initialize();\\n     } catch {}\\n+    // Enhanced debug logging for workspace propagation diagnosis\\n+    const parentWorkspace = parentContext?.workspace;\\n+    logger.info(`[WorkflowProvider] Workspace propagation for nested workflow '${workflow.id}':`);\\n+    logger.info(`[WorkflowProvider]   parentContext exists: ${!!parentContext}`);\\n+    logger.info(`[WorkflowProvider]   parentContext.workspace exists: ${!!parentWorkspace}`);\\n+    if (parentWorkspace) {\\n+      logger.info(`[WorkflowProvider]   parentWorkspace.isEnabled(): ${parentWorkspace.isEnabled?.() ?? 'N/A'}`);\\n+      const projectCount = parentWorkspace.listProjects?.()?.length ?? 'N/A';\\n+      logger.info(`[WorkflowProvider]   parentWorkspace project count: ${projectCount}`);\\n+    } else {\\n+      logger.warn(`[WorkflowProvider]   NO WORKSPACE from parent - nested checkouts won't be added to workspace!`);\\n+    }\\n \\n     const childContext = {\\n       mode: 'state-machine' as const,\\n@@ -374,7 +480,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       checks: checksMetadata,\\n       journal: childJournal,\\n       memory: childMemory,\\n+      // For nested workflows we continue to execute inside the same logical\\n+      // working directory as the parent run. When workspace isolation is\\n+      // enabled on the parent engine, its WorkspaceManager is also propagated\\n+      // so that nested checks (AI, git-checkout, etc.) see the same isolated\\n+      // workspace and project symlinks instead of falling back to the Visor\\n+      // repository root.\\n       workingDirectory: parentContext?.workingDirectory || process.cwd(),\\n+      originalWorkingDirectory:\\n+        parentContext?.originalWorkingDirectory ||\\n+        parentContext?.workingDirectory ||\\n+        process.cwd(),\\n+      workspace: parentWorkspace,\\n       // Always use a fresh session for nested workflows to isolate history\\n       sessionId: uuidv4(),\\n       event: parentContext?.event || prInfo.eventType,\\n@@ -500,6 +617,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     } catch {}\\n \\n+    // Create outputs map that directly exposes .output values (not wrapped in { output, issues })\\n+    // This makes {{ outputs['check-name'] }} work naturally without needing .output\\n+    const outputsMap = Object.fromEntries(\\n+      Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n+    );\\n+\\n     for (const output of workflow.outputs) {\\n       if (output.value_js) {\\n         // JavaScript expression\\n@@ -508,10 +631,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           output.value_js,\\n           {\\n             inputs,\\n-            steps: Object.fromEntries(\\n-              Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-            ),\\n-            outputs: flat,\\n+            outputs: outputsMap,\\n+            // Keep 'steps' as alias for backwards compatibility\\n+            steps: outputsMap,\\n             pr: prInfo,\\n           },\\n           { injectLog: true, logPrefix: `workflow.output.${output.name}` }\\n@@ -520,10 +642,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         // Liquid template\\n         outputs[output.name] = await this.liquid.parseAndRender(output.value, {\\n           inputs,\\n-          steps: Object.fromEntries(\\n-            Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-          ),\\n-          outputs: flat,\\n+          outputs: outputsMap,\\n+          // Keep 'steps' as alias for backwards compatibility\\n+          steps: outputsMap,\\n           pr: prInfo,\\n         });\\n       }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/client.ts\",\"additions\":65,\"deletions\":1,\"changes\":66,\"patch\":\"diff --git a/src/slack/client.ts b/src/slack/client.ts\\nindex 292cc358..435f8b0d 100644\\n--- a/src/slack/client.ts\\n+++ b/src/slack/client.ts\\n@@ -142,6 +142,70 @@ export class SlackClient {\\n     }\\n   }\\n \\n+  public readonly files = {\\n+    /**\\n+     * Upload a file to Slack using files.uploadV2 API\\n+     * @param options Upload options including file content, filename, channel, and thread_ts\\n+     */\\n+    uploadV2: async ({\\n+      content,\\n+      filename,\\n+      channel,\\n+      thread_ts,\\n+      title,\\n+      initial_comment,\\n+    }: {\\n+      content: Buffer;\\n+      filename: string;\\n+      channel: string;\\n+      thread_ts?: string;\\n+      title?: string;\\n+      initial_comment?: string;\\n+    }): Promise<{ ok: boolean; file?: { id: string; permalink?: string } }> => {\\n+      try {\\n+        // Step 1: Get upload URL\\n+        const getUrlResp: any = await this.api('files.getUploadURLExternal', {\\n+          filename,\\n+          length: content.length,\\n+        });\\n+        if (!getUrlResp || getUrlResp.ok !== true || !getUrlResp.upload_url) {\\n+          console.warn(`Slack files.getUploadURLExternal failed: ${getUrlResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 2: Upload file content to the URL\\n+        const uploadResp = await fetch(getUrlResp.upload_url, {\\n+          method: 'POST',\\n+          body: content,\\n+        });\\n+        if (!uploadResp.ok) {\\n+          console.warn(`Slack file upload to URL failed: ${uploadResp.status}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 3: Complete the upload and share to channel\\n+        const completeResp: any = await this.api('files.completeUploadExternal', {\\n+          files: [{ id: getUrlResp.file_id, title: title || filename }],\\n+          channel_id: channel,\\n+          thread_ts,\\n+          initial_comment,\\n+        });\\n+        if (!completeResp || completeResp.ok !== true) {\\n+          console.warn(`Slack files.completeUploadExternal failed: ${completeResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        return {\\n+          ok: true,\\n+          file: completeResp.files?.[0] || { id: getUrlResp.file_id },\\n+        };\\n+      } catch (e) {\\n+        console.warn(`Slack file upload failed: ${e instanceof Error ? e.message : String(e)}`);\\n+        return { ok: false };\\n+      }\\n+    },\\n+  };\\n+\\n   getWebClient(): any {\\n     return {\\n       conversations: {\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/markdown.ts\",\"additions\":204,\"deletions\":5,\"changes\":209,\"patch\":\"diff --git a/src/slack/markdown.ts b/src/slack/markdown.ts\\nindex cc63fb5f..9f91c25f 100644\\n--- a/src/slack/markdown.ts\\n+++ b/src/slack/markdown.ts\\n@@ -3,14 +3,191 @@\\n // without pulling in a full Markdown parser.\\n //\\n // Supported conversions:\\n+// - # Header / ## Header  ‚Üí *Header* (bold with visual separation)\\n // - **bold** / __bold__   ‚Üí *bold*\\n // - [label](url)          ‚Üí <url|label>\\n // - ![alt](url)           ‚Üí <url|alt>\\n // - *italic* (inline)     ‚Üí _italic_\\n+// - ```mermaid blocks     ‚Üí rendered to PNG and uploaded to Slack\\n //\\n // Everything else is passed through unchanged; Slack will still render many\\n // Markdown-like constructs (lists, code fences, etc.) natively.\\n \\n+import { spawn } from 'child_process';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n+import * as os from 'os';\\n+\\n+/**\\n+ * Represents an extracted mermaid diagram\\n+ */\\n+export interface MermaidDiagram {\\n+  /** The full match including ```mermaid and ``` */\\n+  fullMatch: string;\\n+  /** The mermaid code content */\\n+  code: string;\\n+  /** Start index in the original text */\\n+  startIndex: number;\\n+  /** End index in the original text */\\n+  endIndex: number;\\n+}\\n+\\n+/**\\n+ * Extract all mermaid code blocks from text\\n+ */\\n+export function extractMermaidDiagrams(text: string): MermaidDiagram[] {\\n+  const diagrams: MermaidDiagram[] = [];\\n+  // Match ```mermaid followed by newline, content, and closing ```\\n+  const regex = /```mermaid\\\\s*\\\\n([\\\\s\\\\S]*?)```/g;\\n+  let match;\\n+  while ((match = regex.exec(text)) !== null) {\\n+    diagrams.push({\\n+      fullMatch: match[0],\\n+      code: match[1].trim(),\\n+      startIndex: match.index,\\n+      endIndex: match.index + match[0].length,\\n+    });\\n+  }\\n+  return diagrams;\\n+}\\n+\\n+/**\\n+ * Render a mermaid diagram to PNG using mmdc CLI (@mermaid-js/mermaid-cli).\\n+ *\\n+ * Requirements:\\n+ * - Node.js and npx must be available in PATH\\n+ * - Network access on first run (npx downloads the package)\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ *\\n+ * On Linux, you may need to install chromium dependencies:\\n+ *   apt-get install -y chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ *\\n+ * On Docker/CI, consider using a base image with puppeteer support or\\n+ * pre-installing @mermaid-js/mermaid-cli globally.\\n+ *\\n+ * @param mermaidCode The mermaid diagram code\\n+ * @returns Buffer containing PNG data, or null if rendering failed\\n+ */\\n+export async function renderMermaidToPng(mermaidCode: string): Promise<Buffer | null> {\\n+  // Create temp files for input and output\\n+  const tmpDir = os.tmpdir();\\n+  const inputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.mmd`);\\n+  const outputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.png`);\\n+\\n+  try {\\n+    // Write mermaid code to temp file\\n+    fs.writeFileSync(inputFile, mermaidCode, 'utf-8');\\n+\\n+    // Detect system chromium for puppeteer (mermaid-cli dependency)\\n+    // Without this, puppeteer may hang trying to download its own chromium\\n+    const chromiumPaths = [\\n+      '/usr/bin/chromium',\\n+      '/usr/bin/chromium-browser',\\n+      '/usr/bin/google-chrome',\\n+      '/usr/bin/chrome',\\n+    ];\\n+    let chromiumPath: string | undefined;\\n+    for (const p of chromiumPaths) {\\n+      if (fs.existsSync(p)) {\\n+        chromiumPath = p;\\n+        break;\\n+      }\\n+    }\\n+\\n+    // Build environment with chromium path if found\\n+    const env = { ...process.env };\\n+    if (chromiumPath) {\\n+      env.PUPPETEER_EXECUTABLE_PATH = chromiumPath;\\n+    }\\n+\\n+    // Run mmdc to render PNG\\n+    const result = await new Promise<{ success: boolean; error?: string }>((resolve) => {\\n+      const proc = spawn('npx', [\\n+        '--yes',\\n+        '@mermaid-js/mermaid-cli',\\n+        '-i', inputFile,\\n+        '-o', outputFile,\\n+        '-e', 'png',\\n+        '-b', 'white',\\n+        '-w', '1200',\\n+      ], {\\n+        timeout: 60000, // 60 second timeout (first run may download packages)\\n+        stdio: ['pipe', 'pipe', 'pipe'],\\n+        env,\\n+      });\\n+\\n+      let stderr = '';\\n+      proc.stderr?.on('data', (data) => {\\n+        stderr += data.toString();\\n+      });\\n+\\n+      proc.on('close', (code) => {\\n+        if (code === 0) {\\n+          resolve({ success: true });\\n+        } else {\\n+          resolve({ success: false, error: stderr || `Exit code ${code}` });\\n+        }\\n+      });\\n+\\n+      proc.on('error', (err) => {\\n+        resolve({ success: false, error: err.message });\\n+      });\\n+    });\\n+\\n+    if (!result.success) {\\n+      console.warn(`Mermaid rendering failed: ${result.error}`);\\n+      return null;\\n+    }\\n+\\n+    // Read the output PNG\\n+    if (!fs.existsSync(outputFile)) {\\n+      console.warn('Mermaid output file not created');\\n+      return null;\\n+    }\\n+\\n+    const pngBuffer = fs.readFileSync(outputFile);\\n+    return pngBuffer;\\n+  } catch (e) {\\n+    console.warn(`Mermaid rendering error: ${e instanceof Error ? e.message : String(e)}`);\\n+    return null;\\n+  } finally {\\n+    // Cleanup temp files\\n+    try {\\n+      if (fs.existsSync(inputFile)) fs.unlinkSync(inputFile);\\n+      if (fs.existsSync(outputFile)) fs.unlinkSync(outputFile);\\n+    } catch {\\n+      // Ignore cleanup errors\\n+    }\\n+  }\\n+}\\n+\\n+/**\\n+ * Replace mermaid blocks in text with a placeholder message\\n+ * @param text Original text\\n+ * @param diagrams Extracted diagrams\\n+ * @param replacement Text to replace each diagram with (or a function that returns replacement for each index)\\n+ */\\n+export function replaceMermaidBlocks(\\n+  text: string,\\n+  diagrams: MermaidDiagram[],\\n+  replacement: string | ((index: number) => string) = '_(See diagram above)_'\\n+): string {\\n+  if (diagrams.length === 0) return text;\\n+\\n+  // Sort by start index descending to replace from end to start (preserves indices)\\n+  const sorted = [...diagrams].sort((a, b) => b.startIndex - a.startIndex);\\n+\\n+  let result = text;\\n+  sorted.forEach((diagram, sortedIndex) => {\\n+    // Calculate original index (since we sorted in reverse)\\n+    const originalIndex = diagrams.length - 1 - sortedIndex;\\n+    const rep = typeof replacement === 'function' ? replacement(originalIndex) : replacement;\\n+    result = result.slice(0, diagram.startIndex) + rep + result.slice(diagram.endIndex);\\n+  });\\n+\\n+  return result;\\n+}\\n+\\n export function markdownToSlack(text: string): string {\\n   if (!text || typeof text !== 'string') return '';\\n \\n@@ -33,7 +210,7 @@ export function markdownToSlack(text: string): string {\\n   out = out.replace(/\\\\*\\\\*([^*]+)\\\\*\\\\*/g, (_m, inner: string) => `*${inner}*`);\\n   out = out.replace(/__([^_]+)__/g, (_m, inner: string) => `*${inner}*`);\\n \\n-  // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation).\\n+  // Process lines for headers and bullet lists.\\n   // Slack's mrkdwn handles \\\"‚Ä¢\\\" bullets more naturally than raw \\\"-\\\" Markdown.\\n   const lines = out.split(/\\\\r?\\\\n/);\\n   let inCodeBlock = false;\\n@@ -46,9 +223,31 @@ export function markdownToSlack(text: string): string {\\n       continue;\\n     }\\n     if (inCodeBlock) continue;\\n-    const match = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n-    if (match) {\\n-      const [, indent, , rest] = match;\\n+\\n+    // Headers: # Header ‚Üí *Header* (Slack doesn't have native headers)\\n+    // Match 1-6 # at start of line, followed by space and text\\n+    const headerMatch = /^(#{1,6})\\\\s+(.+)$/.exec(trimmed);\\n+    if (headerMatch) {\\n+      const [, hashes, headerText] = headerMatch;\\n+      // For h1/h2, add extra emphasis with newline before (if not first line\\n+      // and previous line is not empty/header/code-fence)\\n+      const prevLine = i > 0 ? lines[i - 1].trim() : '';\\n+      const prevIsHeaderOrFence =\\n+        /^#{1,6}\\\\s+/.test(prevLine) ||\\n+        /^\\\\*[^*]+\\\\*$/.test(prevLine) ||\\n+        /^```/.test(prevLine);\\n+      if (hashes.length <= 2 && i > 0 && prevLine !== '' && !prevIsHeaderOrFence) {\\n+        lines[i] = `\\\\n*${headerText.trim()}*`;\\n+      } else {\\n+        lines[i] = `*${headerText.trim()}*`;\\n+      }\\n+      continue;\\n+    }\\n+\\n+    // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation)\\n+    const bulletMatch = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n+    if (bulletMatch) {\\n+      const [, indent, , rest] = bulletMatch;\\n       lines[i] = `${indent}‚Ä¢ ${rest}`;\\n     }\\n   }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/prompt-state.ts\",\"additions\":19,\"deletions\":2,\"changes\":21,\"patch\":\"diff --git a/src/slack/prompt-state.ts b/src/slack/prompt-state.ts\\nindex a13233b2..544777c6 100644\\n--- a/src/slack/prompt-state.ts\\n+++ b/src/slack/prompt-state.ts\\n@@ -95,7 +95,10 @@ export class PromptStateManager {\\n   setFirstMessage(channel: string, threadTs: string, text: string): void {\\n     const key = this.key(channel, threadTs);\\n     if (!text || !text.trim()) return;\\n-    if (!this.firstMessage.has(key)) {\\n+    const existing = this.firstMessage.get(key);\\n+    // Only set if: no entry exists OR the existing entry was already consumed\\n+    // This allows new messages to be captured after a resume cycle\\n+    if (!existing || existing.consumed) {\\n       this.firstMessage.set(key, { text, consumed: false });\\n     }\\n   }\\n@@ -130,6 +133,20 @@ export class PromptStateManager {\\n         removed++;\\n       }\\n     }\\n+    // Also clean up stale firstMessage entries (consumed entries older than TTL)\\n+    // Keep unconsumed entries to avoid losing user messages\\n+    for (const [key] of this.firstMessage.entries()) {\\n+      const waitingInfo = this.waiting.get(key);\\n+      // If no corresponding waiting entry exists and the firstMessage was consumed,\\n+      // the conversation is likely complete - safe to remove\\n+      if (!waitingInfo) {\\n+        const entry = this.firstMessage.get(key);\\n+        if (entry?.consumed) {\\n+          this.firstMessage.delete(key);\\n+          removed++;\\n+        }\\n+      }\\n+    }\\n     if (removed) {\\n       try {\\n         logger.info(`[prompt-state] cleanup removed ${removed} entries`);\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/socket-runner.ts\",\"additions\":56,\"deletions\":32,\"changes\":88,\"patch\":\"diff --git a/src/slack/socket-runner.ts b/src/slack/socket-runner.ts\\nindex d1681a03..5b461d22 100644\\n--- a/src/slack/socket-runner.ts\\n+++ b/src/slack/socket-runner.ts\\n@@ -7,6 +7,7 @@ import { SlackAdapter } from './adapter';\\n import { CachePrewarmer } from './cache-prewarmer';\\n import { RateLimiter, type RateLimitConfig } from './rate-limiter';\\n import type { SlackBotConfig } from '../types/bot';\\n+import { withActiveSpan } from '../telemetry/trace-helpers';\\n \\n type SlackSocketConfig = {\\n   appToken?: string; // xapp- token\\n@@ -30,6 +31,7 @@ export class SlackSocketRunner {\\n   private botUserId?: string;\\n   private processedKeys: Map<string, number> = new Map();\\n   private adapter?: SlackAdapter;\\n+  private retryCount = 0;\\n \\n   constructor(engine: StateMachineExecutionEngine, cfg: VisorConfig, opts: SlackSocketConfig) {\\n     const app = opts.appToken || process.env.SLACK_APP_TOKEN || '';\\n@@ -107,7 +109,10 @@ export class SlackSocketRunner {\\n \\n   private async connect(url: string): Promise<void> {\\n     this.ws = new WebSocket(url);\\n-    this.ws.on('open', () => logger.info('[SlackSocket] WebSocket connected'));\\n+    this.ws.on('open', () => {\\n+      this.retryCount = 0; // Reset on successful connection\\n+      logger.info('[SlackSocket] WebSocket connected');\\n+    });\\n     this.ws.on('close', (code, reason) => {\\n       logger.warn(`[SlackSocket] WebSocket closed: ${code} ${reason}`);\\n       setTimeout(() => this.restart().catch(() => {}), 1000);\\n@@ -123,7 +128,13 @@ export class SlackSocketRunner {\\n       const url = await this.openConnection();\\n       await this.connect(url);\\n     } catch (e) {\\n-      logger.error(`[SlackSocket] Restart failed: ${e instanceof Error ? e.message : e}`);\\n+      this.retryCount++;\\n+      // Exponential backoff: 2s, 4s, 8s, 16s, 32s, capped at 60s\\n+      const delay = Math.min(2000 * Math.pow(2, this.retryCount - 1), 60000);\\n+      logger.error(\\n+        `[SlackSocket] Restart failed (attempt ${this.retryCount}), retrying in ${Math.round(delay / 1000)}s: ${e instanceof Error ? e.message : e}`\\n+      );\\n+      setTimeout(() => this.restart().catch(() => {}), delay);\\n     }\\n   }\\n \\n@@ -296,36 +307,49 @@ export class SlackSocketRunner {\\n           });\\n         }\\n       } catch {}\\n-      if (path) {\\n-        try {\\n-          const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n-          const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n-          logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n-          await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n-            webhookContext: { webhookData: map, eventType: 'manual' },\\n-            debug: process.env.VISOR_DEBUG === 'true',\\n-          });\\n-          if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n-          return; // resume path handled\\n-        } catch (e) {\\n-          logger.warn(\\n-            `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n-              e instanceof Error ? e.message : String(e)\\n-            }`\\n-          );\\n-        }\\n-      }\\n+      try {\\n+        await withActiveSpan(\\n+          'visor.run',\\n+          {\\n+            'visor.run.source': 'slack',\\n+            'slack.event.type': String(type || ''),\\n+            'slack.channel': channelId,\\n+            'slack.thread_ts': threadTs,\\n+          },\\n+          async () => {\\n+            if (path) {\\n+              try {\\n+                const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n+                const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n+                logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n+                await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n+                  webhookContext: { webhookData: map, eventType: 'manual' },\\n+                  debug: process.env.VISOR_DEBUG === 'true',\\n+                });\\n+                return;\\n+              } catch (e) {\\n+                logger.warn(\\n+                  `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n+                    e instanceof Error ? e.message : String(e)\\n+                  }`\\n+                );\\n+              }\\n+            }\\n \\n-      // Cold run (no snapshot)\\n-      await runEngine.executeChecks({\\n-        checks: allChecks,\\n-        showDetails: true,\\n-        outputFormat: 'json',\\n-        config: cfgForRun,\\n-        webhookContext: { webhookData: map, eventType: 'manual' },\\n-        debug: process.env.VISOR_DEBUG === 'true',\\n-      } as any);\\n-      if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+            // Cold run (no snapshot)\\n+            await runEngine.executeChecks({\\n+              checks: allChecks,\\n+              showDetails: true,\\n+              outputFormat: 'json',\\n+              config: cfgForRun,\\n+              webhookContext: { webhookData: map, eventType: 'manual' },\\n+              debug: process.env.VISOR_DEBUG === 'true',\\n+            } as any);\\n+          }\\n+        );\\n+      } finally {\\n+        if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+      }\\n     } catch (e) {\\n       logger.error(\\n         `[SlackSocket] Engine execution failed: ${e instanceof Error ? e.message : String(e)}`\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/context/build-engine-context.ts\",\"additions\":11,\"deletions\":4,\"changes\":15,\"patch\":\"diff --git a/src/state-machine/context/build-engine-context.ts b/src/state-machine/context/build-engine-context.ts\\nindex 149c73bf..45e63b6a 100644\\n--- a/src/state-machine/context/build-engine-context.ts\\n+++ b/src/state-machine/context/build-engine-context.ts\\n@@ -57,7 +57,8 @@ export function buildEngineContextForRun(\\n       ) as EventTrigger[],\\n       group: checkConfig.group,\\n       providerType: checkConfig.type || 'ai',\\n-      dependencies: checkConfig.depends_on || [],\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: Array.isArray(checkConfig.depends_on) ? checkConfig.depends_on : checkConfig.depends_on ? [checkConfig.depends_on] : [],\\n     };\\n   }\\n \\n@@ -134,11 +135,14 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n   const originalPath = context.workingDirectory || process.cwd();\\n \\n   try {\\n+    // Check if workspace should be kept (for debugging)\\n+    const keepWorkspace = process.env.VISOR_KEEP_WORKSPACE === 'true';\\n+\\n     // Create workspace manager\\n     const workspace = WorkspaceManager.getInstance(context.sessionId, originalPath, {\\n       enabled: true,\\n-      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH,\\n-      cleanupOnExit: workspaceConfig?.cleanup_on_exit !== false,\\n+      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH || '/tmp/visor-workspaces',\\n+      cleanupOnExit: keepWorkspace ? false : workspaceConfig?.cleanup_on_exit !== false,\\n     });\\n \\n     // Initialize workspace (creates main project worktree)\\n@@ -151,6 +155,9 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n \\n     logger.info(`[Workspace] Initialized workspace: ${info.workspacePath}`);\\n     logger.debug(`[Workspace] Main project at: ${info.mainProjectPath}`);\\n+    if (keepWorkspace) {\\n+      logger.info(`[Workspace] Keeping workspace after execution (--keep-workspace)`);\\n+    }\\n \\n     return context;\\n   } catch (error) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/execution-invoker.ts\",\"additions\":84,\"deletions\":4,\"changes\":88,\"patch\":\"diff --git a/src/state-machine/dispatch/execution-invoker.ts b/src/state-machine/dispatch/execution-invoker.ts\\nindex eb7587d3..798ca8e4 100644\\n--- a/src/state-machine/dispatch/execution-invoker.ts\\n+++ b/src/state-machine/dispatch/execution-invoker.ts\\n@@ -356,10 +356,17 @@ export async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -413,6 +420,31 @@ export async function executeSingleCheck(\\n     }\\n   }\\n \\n+  // Banner-style log when a check actually starts executing. This is emitted\\n+  // after all gating (if/depends_on) has passed so it only appears for real\\n+  // runs, and gives a clear visual separator in the logs between checks.\\n+  try {\\n+    const wave = state.wave;\\n+    const level = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${level}) ‚îÅ‚îÅ‚îÅ`;\\n+\\n+    // When running in a TTY, colour the entire banner line for extra\\n+    // visibility; keep plain text for JSON/SARIF or non-TTY environments.\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   let forEachParent: string | undefined;\\n   let forEachItems: unknown[] | undefined;\\n   for (const depId of depList) {\\n@@ -563,13 +595,56 @@ export async function executeSingleCheck(\\n       files: [],\\n       commits: [],\\n     };\\n+    // Derive AI session reuse context for this check (self-mode only for now).\\n+    // When reuse_ai_session: 'self' is configured, look up the last root-scope\\n+    // journal entry for this check in the current engine session and expose its\\n+    // sessionId via ExecutionContext so ai-check-provider can call\\n+    // executeReviewWithSessionReuse() against the same ProbeAgent session.\\n+    let parentSessionId: string | undefined;\\n+    let reuseSession = false;\\n+    try {\\n+      const reuseCfg: unknown = (checkConfig as any).reuse_ai_session;\\n+      if (reuseCfg === 'self') {\\n+        const snapshotId = context.journal.beginSnapshot();\\n+        const visible = context.journal.readVisible(\\n+          context.sessionId,\\n+          snapshotId,\\n+          context.event as any\\n+        );\\n+        // Prefer the most recent root-scope result for this check\\n+        const prior = visible.filter(\\n+          e => e.checkId === checkId && (!e.scope || e.scope.length === 0)\\n+        );\\n+        if (prior.length > 0) {\\n+          const last = prior[prior.length - 1];\\n+          const sess = (last.result as any)?.sessionId;\\n+          if (typeof sess === 'string' && sess.length > 0) {\\n+            parentSessionId = sess;\\n+            reuseSession = true;\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only ‚Äì fall back to normal (non-reuse) execution on error.\\n+      parentSessionId = undefined;\\n+      reuseSession = false;\\n+    }\\n+\\n     const executionContext = {\\n       ...context.executionContext,\\n       _engineMode: context.mode,\\n       _parentContext: context,\\n       _parentState: state,\\n+      // Explicitly propagate workspace reference for nested workflows\\n+      workspace: context.workspace,\\n     };\\n \\n+    // Attach session reuse hints for providers that support them (AI, Claude Code, etc).\\n+    if (reuseSession && parentSessionId) {\\n+      (executionContext as any).parentSessionId = parentSessionId;\\n+      (executionContext as any).reuseSession = true;\\n+    }\\n+\\n     // Handle on_init lifecycle hook BEFORE main execution\\n     if (checkConfig.on_init) {\\n       try {\\n@@ -613,7 +688,12 @@ export async function executeSingleCheck(\\n \\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/foreach-processor.ts\",\"additions\":27,\"deletions\":2,\"changes\":29,\"patch\":\"diff --git a/src/state-machine/dispatch/foreach-processor.ts b/src/state-machine/dispatch/foreach-processor.ts\\nindex 336bf9df..ab630986 100644\\n--- a/src/state-machine/dispatch/foreach-processor.ts\\n+++ b/src/state-machine/dispatch/foreach-processor.ts\\n@@ -77,6 +77,26 @@ export async function executeCheckWithForEachItems(\\n   const allContents: string[] = [];\\n   const perIterationDurations: number[] = [];\\n \\n+  // Emit a banner for the forEach parent so logs clearly show when we are\\n+  // entering the aggregated execution for that step.\\n+  try {\\n+    const wave = state.wave;\\n+    const lvl = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${lvl}, forEach parent) ‚îÅ‚îÅ‚îÅ`;\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   for (let itemIndex = 0; itemIndex < forEachItems.length; itemIndex++) {\\n     const iterationStartMs = Date.now();\\n     const scope: Array<{ check: string; index: number }> = [\\n@@ -248,7 +268,12 @@ export async function executeCheckWithForEachItems(\\n \\n       const result = await withActiveSpan(\\n         `visor.check.${checkId}`,\\n-        { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+        {\\n+          'visor.check.id': checkId,\\n+          'visor.check.type': providerType,\\n+          session_id: context.sessionId,\\n+          wave: state.wave,\\n+        },\\n         async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n       );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/stats-manager.ts\",\"additions\":21,\"deletions\":1,\"changes\":22,\"patch\":\"diff --git a/src/state-machine/dispatch/stats-manager.ts b/src/state-machine/dispatch/stats-manager.ts\\nindex 8d04436e..2f317b59 100644\\n--- a/src/state-machine/dispatch/stats-manager.ts\\n+++ b/src/state-machine/dispatch/stats-manager.ts\\n@@ -44,6 +44,26 @@ export function updateStats(\\n       issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n     };\\n \\n+    // Respect explicit skip markers on the result (set by LevelDispatch).\\n+    const skippedMarker = (result as any).__skipped;\\n+    if (skippedMarker) {\\n+      stats.skipped = true;\\n+      stats.skipReason =\\n+        typeof skippedMarker === 'string'\\n+          ? (skippedMarker as any)\\n+          : (stats.skipReason as any) || 'if_condition';\\n+      // A skipped result should not contribute to run counters; reset\\n+      // them explicitly to guard against any prior values lingering\\n+      // from earlier runs of the same check.\\n+      stats.totalRuns = 0;\\n+      stats.successfulRuns = 0;\\n+      stats.failedRuns = 0;\\n+      stats.skippedRuns++;\\n+      // Do not count this as a run; keep totalRuns/failed/success unchanged.\\n+      state.stats.set(checkId, stats);\\n+      continue;\\n+    }\\n+\\n     // If this check was previously marked as skipped and now executed,\\n     // clear the skipped flag and any skip counters to ensure the run is visible.\\n     if (stats.skipped) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/runner.ts\",\"additions\":10,\"deletions\":7,\"changes\":17,\"patch\":\"diff --git a/src/state-machine/runner.ts b/src/state-machine/runner.ts\\nindex ee86b876..a13a5efd 100644\\n--- a/src/state-machine/runner.ts\\n+++ b/src/state-machine/runner.ts\\n@@ -124,14 +124,17 @@ export class StateMachineRunner {\\n    */\\n   private async executeState(state: EngineState): Promise<void> {\\n     // M4: Wrap state execution in OTEL span\\n+    const attrs: Record<string, unknown> = {\\n+      state: state,\\n+      engine_mode: this.context.mode,\\n+      wave: this.state.wave,\\n+      session_id: this.context.sessionId,\\n+    };\\n+    const waveKind = (this.state as any)?.flags?.waveKind;\\n+    if (waveKind) attrs.wave_kind = waveKind;\\n     return withActiveSpan(\\n       `engine.state.${state.toLowerCase()}`,\\n-      {\\n-        state: state,\\n-        engine_mode: this.context.mode,\\n-        wave: this.state.wave,\\n-        session_id: this.context.sessionId,\\n-      },\\n+      attrs,\\n       async () => {\\n         try {\\n           switch (state) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/level-dispatch.ts\",\"additions\":100,\"deletions\":15,\"changes\":115,\"patch\":\"diff --git a/src/state-machine/states/level-dispatch.ts b/src/state-machine/states/level-dispatch.ts\\nindex 5588b0f5..42ee383e 100644\\n--- a/src/state-machine/states/level-dispatch.ts\\n+++ b/src/state-machine/states/level-dispatch.ts\\n@@ -26,7 +26,7 @@ import type { CheckExecutionStats } from '../../types/execution';\\n import type { CheckProviderConfig } from '../../providers/check-provider.interface';\\n import type { CheckConfig } from '../../types/config';\\n import { handleRouting, checkLoopBudget } from './routing';\\n-import { withActiveSpan } from '../../telemetry/trace-helpers';\\n+import { withActiveSpan, setSpanAttributes } from '../../telemetry/trace-helpers';\\n import { emitMermaidFromMarkdown } from '../../utils/mermaid-telemetry';\\n import { emitNdjsonSpanWithEvents, emitNdjsonFallback } from '../../telemetry/fallback-ndjson';\\n import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n@@ -121,12 +121,13 @@ async function evaluateIfCondition(\\n         return false;\\n       }\\n     })();\\n+    // Steps with dependencies should always see outputs from all completed steps.\\n     // In forward-run waves (from on_success/on_fail goto), guards should see the\\n     // latest global outputs even if the check has no explicit dependencies.\\n     // In wave-retry (from on_finish), restrict to checks with dependencies to\\n     // avoid wrongly skipping top-level prompts like 'ask'.\\n     const useGlobalOutputs =\\n-      (useGlobalOutputsFlag && waveKind === 'forward') || (useGlobalOutputsFlag && hasDeps);\\n+      hasDeps || (useGlobalOutputsFlag && waveKind === 'forward');\\n \\n     if (useGlobalOutputs) {\\n       // Forward-run wave: allow guards to consult latest outputs from the entire\\n@@ -199,6 +200,7 @@ async function evaluateIfCondition(\\n       baseBranch: (context.prInfo as any)?.baseBranch,\\n       filesChanged: context.prInfo?.files?.map(f => f.filename),\\n       environment: envSnapshot,\\n+      workflowInputs: (context.config as any).workflow_inputs || {},\\n     };\\n \\n     const shouldRun = await evaluator.evaluateIfCondition(checkId, ifExpression, contextData);\\n@@ -238,6 +240,7 @@ export async function handleLevelDispatch(\\n   // Update current level tracking\\n   state.currentLevel = level.level;\\n   state.currentLevelChecks = new Set(level.parallel);\\n+  setSpanAttributes({ level_size: level.parallel.length });\\n \\n   // Emit level ready event\\n   emitEvent({ type: 'LevelReady', level, wave: state.wave });\\n@@ -689,6 +692,39 @@ async function executeCheckWithForEachItems(\\n         }\\n       } catch {}\\n \\n+      // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+      // The socket-runner stores conversation data in webhookData under the endpoint key\\n+      try {\\n+        const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+        const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+        if (context.debug) {\\n+          logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+        }\\n+        if (webhookData && webhookData.size > 0) {\\n+          // Find the payload with slack_conversation\\n+          for (const payload of webhookData.values()) {\\n+            const slackConv = (payload as any)?.slack_conversation;\\n+            if (slackConv) {\\n+              // Build slack context with event and conversation\\n+              const event = (payload as any)?.event;\\n+              const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+              if (context.debug) {\\n+                logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+              }\\n+              (providerConfig as any).eventContext = {\\n+                ...(providerConfig as any).eventContext,\\n+                slack: {\\n+                  event: event || {},\\n+                  conversation: slackConv,\\n+                },\\n+                conversation: slackConv, // Also expose at top level for convenience\\n+              };\\n+              break;\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n+\\n       // Build dependency results with scope\\n       const dependencyResults = buildDependencyResultsWithScope(\\n         checkId,\\n@@ -820,15 +856,17 @@ async function executeCheckWithForEachItems(\\n       } catch {}\\n \\n       // Execute provider with telemetry\\n-      const itemResult = await withActiveSpan(\\n-        `visor.check.${checkId}`,\\n-        {\\n-          'visor.check.id': checkId,\\n-          'visor.check.type': providerType,\\n-          'visor.foreach.index': itemIndex,\\n-        },\\n-        async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n-      );\\n+    const itemResult = await withActiveSpan(\\n+      `visor.check.${checkId}`,\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        'visor.foreach.index': itemIndex,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n+      async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n+    );\\n \\n       // Enrich issues\\n       const enrichedIssues = (itemResult.issues || []).map((issue: ReviewIssue) => ({\\n@@ -942,6 +980,7 @@ async function executeCheckWithForEachItems(\\n             const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n               previousResults: dependencyResults as any,\\n               event: context.event || 'manual',\\n+              output: output, // Pass the iteration output for guarantee evaluation\\n             } as any);\\n             if (!holds) {\\n               const issue: ReviewIssue = {\\n@@ -1618,10 +1657,17 @@ async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -1923,6 +1969,39 @@ async function executeSingleCheck(\\n       }\\n     } catch {}\\n \\n+    // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+    // The socket-runner stores conversation data in webhookData under the endpoint key\\n+    try {\\n+      const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+      const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+      if (context.debug) {\\n+        logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+      }\\n+      if (webhookData && webhookData.size > 0) {\\n+        // Find the payload with slack_conversation\\n+        for (const payload of webhookData.values()) {\\n+          const slackConv = (payload as any)?.slack_conversation;\\n+          if (slackConv) {\\n+            // Build slack context with event and conversation\\n+            const event = (payload as any)?.event;\\n+            const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+            if (context.debug) {\\n+              logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+            }\\n+            (providerConfig as any).eventContext = {\\n+              ...(providerConfig as any).eventContext,\\n+              slack: {\\n+                event: event || {},\\n+                conversation: slackConv,\\n+              },\\n+              conversation: slackConv, // Also expose at top level for convenience\\n+            };\\n+            break;\\n+          }\\n+        }\\n+      }\\n+    } catch {}\\n+\\n     // Build dependency results\\n     const dependencyResults = buildDependencyResults(checkId, checkConfig, context, state);\\n \\n@@ -2016,7 +2095,12 @@ async function executeSingleCheck(\\n     // Execute provider with telemetry\\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n@@ -2098,6 +2182,7 @@ async function executeSingleCheck(\\n           const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n             previousResults: dependencyResults as any,\\n             event: context.event || 'manual',\\n+            output: enrichedResult.output,\\n           } as any);\\n           if (!holds) {\\n             const issue: ReviewIssue = {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/wave-planning.ts\",\"additions\":10,\"deletions\":3,\"changes\":13,\"patch\":\"diff --git a/src/state-machine/states/wave-planning.ts b/src/state-machine/states/wave-planning.ts\\nindex 7551d55c..7b42f043 100644\\n--- a/src/state-machine/states/wave-planning.ts\\n+++ b/src/state-machine/states/wave-planning.ts\\n@@ -351,8 +351,11 @@ export async function handleWavePlanning(\\n     for (const id of checksToRun) {\\n       // Only include dependencies that are within the same subset (often none).\\n       const cfg = context.config.checks?.[id];\\n-      const deps = (cfg?.depends_on || []).filter((d: string) => checksToRun.includes(d));\\n-      subDeps[id] = deps as string[];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = cfg?.depends_on;\\n+      const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n+      const deps = depsArray.filter((d: string) => checksToRun.includes(d));\\n+      subDeps[id] = deps;\\n     }\\n \\n     const subGraph = DependencyResolver.buildDependencyGraph(subDeps);\\n@@ -410,6 +413,10 @@ export async function handleWavePlanning(\\n     // Initialize current wave state\\n     (state as any).currentWaveCompletions = new Set<string>();\\n     (state as any).failedChecks = new Set<string>();\\n+    try {\\n+      (state as any).flags = (state as any).flags || {};\\n+      (state as any).flags.waveKind = 'initial';\\n+    } catch {}\\n   }\\n \\n   // Check if there are levels to execute\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/workflow-projection.ts\",\"additions\":3,\"deletions\":2,\"changes\":5,\"patch\":\"diff --git a/src/state-machine/workflow-projection.ts b/src/state-machine/workflow-projection.ts\\nindex b2d5eb24..2bcfd858 100644\\n--- a/src/state-machine/workflow-projection.ts\\n+++ b/src/state-machine/workflow-projection.ts\\n@@ -53,7 +53,8 @@ export function projectWorkflowToGraph(\\n       triggers: step.on || workflow.on || [],\\n       group: step.group,\\n       providerType: step.type || 'ai',\\n-      dependencies: (step.depends_on || []).map(dep => dep),\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: (Array.isArray(step.depends_on) ? step.depends_on : step.depends_on ? [step.depends_on] : []).map((dep: string) => dep),\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/flow-stage.ts\",\"additions\":37,\"deletions\":4,\"changes\":41,\"patch\":\"diff --git a/src/test-runner/core/flow-stage.ts b/src/test-runner/core/flow-stage.ts\\nindex a14256c2..a587f9f1 100644\\n--- a/src/test-runner/core/flow-stage.ts\\n+++ b/src/test-runner/core/flow-stage.ts\\n@@ -352,10 +352,30 @@ export class FlowStage {\\n       } catch {}\\n       // Prefer authoritative counts from res.statistics when available, then fall back to deltas/inferred\\n       const fromResStats: Record<string, number> = {};\\n+      const resSkipped: Record<string, boolean> = {};\\n       try {\\n         for (const s of (res.statistics?.checks || []) as any[]) {\\n           const n = (s && s.checkName) || '';\\n-          if (typeof n === 'string' && n) fromResStats[n] = (s.totalRuns || 0) as number;\\n+          if (typeof n === 'string' && n) {\\n+            fromResStats[n] = (s.totalRuns || 0) as number;\\n+            resSkipped[n] = !!(s as any).skipped || !!(s as any).skipReason;\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      // Steps that have explicit expectations in this stage. We will use\\n+      // this to avoid inflating run counts (via parent-alignment heuristics)\\n+      // for checks that are not even expected in this stage.\\n+      const expectedSteps = new Set<string>();\\n+      const expectedZero = new Set<string>();\\n+      try {\\n+        const expCalls = ((stage.expect || {}).calls || []) as Array<{ step?: string }>;\\n+        for (const c of expCalls) {\\n+          if (c && typeof c.step === 'string' && c.step) expectedSteps.add(c.step);\\n+          try {\\n+            if (c && typeof c.step === 'string' && c.step && (c as any).exactly === 0)\\n+              expectedZero.add(c.step);\\n+          } catch {}\\n         }\\n       } catch {}\\n \\n@@ -410,10 +430,15 @@ export class FlowStage {\\n           const base = stageOnly ? 0 : statBase[name] || 0;\\n           const d = Math.max(0, resTotal - base);\\n           runs = d > 0 ? d : 0;\\n+          // If the engine marked this check as skipped, force runs to 0\\n+          // so parent-alignment heuristics below do not inflate it.\\n+          if (resSkipped[name]) {\\n+            runs = 0;\\n+          }\\n         } else {\\n           runs = deltaMap[name] !== undefined ? deltaMap[name] : inferred;\\n         }\\n-        if (runs === 0 && presentInResults.has(name)) runs = 1;\\n+        if (runs === 0 && presentInResults.has(name) && !resSkipped[name]) runs = 1;\\n         if (!isForEachLike && histRuns > 0) runs = histRuns;\\n         // Only use per-item history counts for non-forEach checks. For forEach parents,\\n         // use aggregated totals from res.statistics to reflect number of parent executions.\\n@@ -442,7 +467,15 @@ export class FlowStage {\\n             parentMax = Math.max(parentMax, dP);\\n           }\\n           // Apply only for non-forEach checks with no observable history in this stage\\n-          if (!isForEachLike && histRuns === 0 && histPerItemRuns === 0 && parentMax > 0) {\\n+          if (\\n+            !isForEachLike &&\\n+            histRuns === 0 &&\\n+            histPerItemRuns === 0 &&\\n+            parentMax > 0 &&\\n+            !resSkipped[name] &&\\n+            (expectedSteps.size === 0 || expectedSteps.has(name)) &&\\n+            !expectedZero.has(name)\\n+          ) {\\n             runs = Math.max(runs, parentMax);\\n           }\\n         } catch {}\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/test-execution-wrapper.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/test-runner/core/test-execution-wrapper.ts b/src/test-runner/core/test-execution-wrapper.ts\\nindex 254d0970..d0333152 100644\\n--- a/src/test-runner/core/test-execution-wrapper.ts\\n+++ b/src/test-runner/core/test-execution-wrapper.ts\\n@@ -31,6 +31,8 @@ export class TestExecutionWrapper {\\n       const prev: any = (this.engine as any).executionContext || {};\\n       const merged = {\\n         ...prev,\\n+        // Inject workflow inputs for template access via {{ inputs.* }}\\n+        workflowInputs: cfg.workflow_inputs || prev.workflowInputs || {},\\n         mode: {\\n           ...(prev.mode || {}),\\n           test: true,\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/evaluators.ts\",\"additions\":67,\"deletions\":2,\"changes\":69,\"patch\":\"diff --git a/src/test-runner/evaluators.ts b/src/test-runner/evaluators.ts\\nindex 4843163a..75e9c174 100644\\n--- a/src/test-runner/evaluators.ts\\n+++ b/src/test-runner/evaluators.ts\\n@@ -252,6 +252,69 @@ export function evaluatePrompts(\\n   }\\n }\\n \\n+/**\\n+ * Evaluate workflow_output assertions against computed workflow outputs.\\n+ * Similar to evaluateOutputs but tests workflow-level outputs (defined in outputs: section)\\n+ * rather than step outputs.\\n+ */\\n+export function evaluateWorkflowOutputs(\\n+  errors: string[],\\n+  expect: ExpectBlock,\\n+  workflowOutputs: Record<string, unknown> | undefined\\n+): void {\\n+  const expectations = (expect as any).workflow_output;\\n+  if (!Array.isArray(expectations) || expectations.length === 0) return;\\n+  if (!workflowOutputs) {\\n+    errors.push('workflow_output assertions present but no workflow outputs computed');\\n+    return;\\n+  }\\n+\\n+  for (const o of expectations) {\\n+    const path = o.path as string;\\n+    if (!path) {\\n+      errors.push('workflow_output assertion missing path');\\n+      continue;\\n+    }\\n+    const v = deepGet(workflowOutputs, path);\\n+    if (o.equals !== undefined && !deepEqual(v, o.equals)) {\\n+      errors.push(\\n+        `Workflow output ${path} expected ${JSON.stringify(o.equals)} but got ${JSON.stringify(v)}`\\n+      );\\n+    }\\n+    if (o.equalsDeep !== undefined && !deepEqual(v, o.equalsDeep)) {\\n+      errors.push(`Workflow output ${path} deepEquals failed`);\\n+    }\\n+    if (o.matches && !parseRegex(o.matches).test(String(v))) {\\n+      errors.push(`Workflow output ${path} does not match ${o.matches}`);\\n+    }\\n+    if (o.contains) {\\n+      const contents = Array.isArray(o.contains) ? o.contains : [o.contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (!strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} expected to contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.not_contains) {\\n+      const contents = Array.isArray(o.not_contains) ? o.not_contains : [o.not_contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} should not contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.contains_unordered) {\\n+      if (!Array.isArray(v)) {\\n+        errors.push(`Workflow output ${path} not an array for contains_unordered`);\\n+      } else if (!containsUnordered(v as unknown[], o.contains_unordered)) {\\n+        errors.push(`Workflow output ${path} missing elements (unordered)`);\\n+      }\\n+    }\\n+  }\\n+}\\n+\\n export function evaluateOutputs(\\n   errors: string[],\\n   expect: ExpectBlock,\\n@@ -332,7 +395,8 @@ export function evaluateCase(\\n   strict: boolean,\\n   promptsByStep: Record<string, string[]>,\\n   _results: GroupedResults,\\n-  outputHistory: Record<string, unknown[]>\\n+  outputHistory: Record<string, unknown[]>,\\n+  workflowOutputs?: Record<string, unknown>\\n ): string[] {\\n   const errors: string[] = [];\\n   const executed = buildExecutedMap(stats);\\n@@ -350,5 +414,6 @@ export function evaluateCase(\\n   evaluateNoCalls(errors, expect, executed, recorder, slackRecorder);\\n   evaluatePrompts(errors, expect, promptsByStep);\\n   evaluateOutputs(errors, expect, outputHistory);\\n+  evaluateWorkflowOutputs(errors, expect, workflowOutputs);\\n   return errors;\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/index.ts\",\"additions\":112,\"deletions\":7,\"changes\":119,\"patch\":\"diff --git a/src/test-runner/index.ts b/src/test-runner/index.ts\\nindex 29916a16..4dcc3331 100644\\n--- a/src/test-runner/index.ts\\n+++ b/src/test-runner/index.ts\\n@@ -159,6 +159,7 @@ export async function runSuites(\\n   options: {\\n     only?: string;\\n     bail?: boolean;\\n+    noMocks?: boolean;\\n     maxParallelSuites?: number;\\n     maxParallel?: number;\\n     promptMaxChars?: number;\\n@@ -225,6 +226,7 @@ export async function runSuites(\\n       const r = await runner.runCases(fp, suite as TestSuite, {\\n         only: options.only,\\n         bail: options.bail,\\n+        noMocks: options.noMocks,\\n         maxParallel: options.maxParallel,\\n         promptMaxChars: options.promptMaxChars,\\n       });\\n@@ -285,7 +287,8 @@ export class VisorTestRunner {\\n     defaultPromptCap?: number,\\n     ghRec?: { error_code?: number; timeout_ms?: number },\\n     defaultIncludeTags?: string[] | undefined,\\n-    defaultExcludeTags?: string[] | undefined\\n+    defaultExcludeTags?: string[] | undefined,\\n+    noMocks?: boolean\\n   ): {\\n     name: string;\\n     strict: boolean;\\n@@ -399,9 +402,11 @@ export class VisorTestRunner {\\n               : info.prompt;\\n           prompts[k].push(p);\\n         },\\n-        mockForStep: (step: string) => mockMgr.get(step),\\n+        // In noMocks mode, always return undefined to let real providers execute\\n+        mockForStep: (step: string) => noMocks ? undefined : mockMgr.get(step),\\n         // Ensure human-input never blocks tests: prefer case mock, then default value\\n         onHumanInput: async (req: { checkId: string; default?: string }) => {\\n+          if (noMocks) return (req.default ?? '').toString();\\n           const m = mockMgr.get(req.checkId);\\n           if (m !== undefined && m !== null) return String(m);\\n           return (req.default ?? '').toString();\\n@@ -481,6 +486,60 @@ export class VisorTestRunner {\\n     if (!checks || checks.length === 0) return;\\n     console.log(`  checks: ${checks.join(', ')}`);\\n   }\\n+  /**\\n+   * Compute workflow outputs from output definitions and step results.\\n+   * Evaluates value_js and value (Liquid) expressions to produce output values.\\n+   */\\n+  private computeWorkflowOutputs(\\n+    outputDefs: Array<{ name: string; value?: string; value_js?: string }>,\\n+    outputHistory: Record<string, unknown[]>,\\n+    results: any[]\\n+  ): Record<string, unknown> {\\n+    const computed: Record<string, unknown> = {};\\n+    // Build a steps map from outputHistory (last output for each step)\\n+    const steps: Record<string, unknown> = {};\\n+    for (const [stepId, hist] of Object.entries(outputHistory)) {\\n+      if (Array.isArray(hist) && hist.length > 0) {\\n+        steps[stepId] = hist[hist.length - 1];\\n+      }\\n+    }\\n+    // Build outputs map from history (for {{ outputs[\\\"step\\\"] }} syntax)\\n+    const outputs: Record<string, unknown> = { ...steps };\\n+\\n+    for (const outputDef of outputDefs) {\\n+      if (!outputDef.name) continue;\\n+      try {\\n+        if (outputDef.value_js) {\\n+          // Evaluate JavaScript expression\\n+          const fn = new Function('steps', 'outputs', 'results', `\\n+            try {\\n+              ${outputDef.value_js}\\n+            } catch (e) {\\n+              return undefined;\\n+            }\\n+          `);\\n+          computed[outputDef.name] = fn(steps, outputs, results);\\n+        } else if (outputDef.value) {\\n+          // Evaluate Liquid template\\n+          const { Liquid } = require('liquidjs');\\n+          const engine = new Liquid();\\n+          const rendered = engine.parseAndRenderSync(outputDef.value, { steps, outputs, results });\\n+          // Try to parse as JSON, otherwise keep as string\\n+          try {\\n+            computed[outputDef.name] = JSON.parse(rendered);\\n+          } catch {\\n+            computed[outputDef.name] = rendered.trim();\\n+          }\\n+        }\\n+      } catch (e) {\\n+        // Skip outputs that fail to compute\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.log(`  ‚ö†Ô∏è Failed to compute output '${outputDef.name}': ${e}`);\\n+        }\\n+      }\\n+    }\\n+    return computed;\\n+  }\\n   /**\\n    * Locate a tests file: explicit path > ./.visor.tests.yaml > defaults/visor.tests.yaml\\n    */\\n@@ -606,7 +665,7 @@ export class VisorTestRunner {\\n   public async runCases(\\n     testsPath: string,\\n     suite: TestSuite,\\n-    options: { only?: string; bail?: boolean; maxParallel?: number; promptMaxChars?: number }\\n+    options: { only?: string; bail?: boolean; noMocks?: boolean; maxParallel?: number; promptMaxChars?: number }\\n   ): Promise<{\\n     failures: number;\\n     results: Array<{\\n@@ -748,9 +807,14 @@ export class VisorTestRunner {\\n     const __keepAlive = setInterval(() => {}, 1000);\\n     // Header: show suite path for clarity\\n     let __suiteRel = testsPath;\\n+    const noMocksMode = options.noMocks || false;\\n     try {\\n       __suiteRel = path.relative(this.cwd, testsPath) || testsPath;\\n       console.log(`Suite: ${__suiteRel}`);\\n+      if (noMocksMode) {\\n+        console.log(this.color('üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)', '33'));\\n+        console.log(this.gray('   Step outputs will be captured and printed as suggested mocks\\\\n'));\\n+      }\\n     } catch {}\\n     const runOne = async (_case: any): Promise<{ name: string; failed: number }> => {\\n       // Case header for clarity\\n@@ -805,6 +869,11 @@ export class VisorTestRunner {\\n           cfgLocal.checks[name] = chk;\\n         }\\n       }\\n+      // Workflow testing: inject workflow_input into config for template access\\n+      const workflowInput = (_case as any).workflow_input;\\n+      if (workflowInput && typeof workflowInput === 'object') {\\n+        (cfgLocal as any).workflow_inputs = workflowInput;\\n+      }\\n       const setup = this.setupTestCase(\\n         _case,\\n         cfgLocal,\\n@@ -812,7 +881,8 @@ export class VisorTestRunner {\\n         defaultPromptCap,\\n         ghRec,\\n         defaultIncludeTags,\\n-        defaultExcludeTags\\n+        defaultExcludeTags,\\n+        noMocksMode\\n       );\\n       try {\\n         this.printSelectedChecks(setup.checksToRun);\\n@@ -826,6 +896,24 @@ export class VisorTestRunner {\\n         } catch {}\\n         const exec = await this.executeTestCase(setup, cfgLocal);\\n         const res = exec.res;\\n+\\n+        // In no-mocks mode, print captured outputs as suggested mocks\\n+        if (noMocksMode && Object.keys(exec.outHistory).length > 0) {\\n+          console.log(this.color('\\\\nüìã Suggested mocks (copy to your test case):', '36'));\\n+          console.log(this.gray('mocks:'));\\n+          for (const [stepName, outputs] of Object.entries(exec.outHistory)) {\\n+            if (!Array.isArray(outputs) || outputs.length === 0) continue;\\n+            // Use the last output for the step\\n+            const lastOutput = outputs[outputs.length - 1];\\n+            // Format as YAML with proper indentation\\n+            const yamlOutput = yaml.dump({ [stepName]: lastOutput }, { indent: 2, lineWidth: 120, noRefs: true });\\n+            // Indent the YAML output for proper nesting under 'mocks:'\\n+            const indented = yamlOutput.split('\\\\n').map(line => '  ' + line).join('\\\\n');\\n+            console.log(indented);\\n+          }\\n+          console.log('');\\n+        }\\n+\\n         if (process.env.VISOR_DEBUG === 'true') {\\n           try {\\n             const names = (res.statistics.checks || []).map(\\n@@ -836,6 +924,21 @@ export class VisorTestRunner {\\n         }\\n         // avoid printing raw history keys each case\\n         // (fallback for on_finish static targets handled inside executeTestCase)\\n+        // Workflow testing: compute workflow outputs if workflow has outputs defined\\n+        let workflowOutputs: Record<string, unknown> | undefined;\\n+        if (Array.isArray((cfgLocal as any).outputs)) {\\n+          try {\\n+            workflowOutputs = this.computeWorkflowOutputs(\\n+              (cfgLocal as any).outputs,\\n+              exec.outHistory,\\n+              res.results\\n+            );\\n+          } catch (e) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ‚ö†Ô∏è Error computing workflow outputs: ${e}`);\\n+            }\\n+          }\\n+        }\\n         const caseFailures = require('./evaluators').evaluateCase(\\n           _case.name,\\n           res.statistics,\\n@@ -845,7 +948,8 @@ export class VisorTestRunner {\\n           setup.strict,\\n           setup.prompts,\\n           res.results,\\n-          exec.outHistory\\n+          exec.outHistory,\\n+          workflowOutputs\\n         );\\n         // Warn about unmocked AI/command steps that executed\\n         try {\\n@@ -1280,7 +1384,8 @@ export class VisorTestRunner {\\n   ): void {\\n     const executed: Record<string, number> = {};\\n     for (const s of stats.checks) {\\n-      if (!s.skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n+      const skipped = (s as any).skipped === true || !!(s as any).skipReason;\\n+      if (!skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n     }\\n     const expCalls = (expect.calls || []).filter(c => c.step);\\n     const expectedSteps = new Map<\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/validator.ts\",\"additions\":48,\"deletions\":2,\"changes\":50,\"patch\":\"diff --git a/src/test-runner/validator.ts b/src/test-runner/validator.ts\\nindex dbb890c5..5ffff5ee 100644\\n--- a/src/test-runner/validator.ts\\n+++ b/src/test-runner/validator.ts\\n@@ -25,6 +25,16 @@ const schema: any = {\\n     hooks: { type: 'object' },\\n     slack: { type: 'object' },\\n     frontends: { type: 'array' },\\n+    workspace: { type: 'object' },\\n+    // Workflow definition fields (for workflow files with co-located tests)\\n+    id: { type: 'string' },\\n+    name: { type: 'string' },\\n+    description: { type: 'string' },\\n+    inputs: { type: 'array' },\\n+    outputs: { type: 'array' },\\n+    ai_mcp_servers: { type: 'object' },\\n+    ai_provider: { type: 'string' },\\n+    ai_model: { type: 'string' },\\n     tests: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -136,6 +146,8 @@ const schema: any = {\\n             oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n           },\\n         },\\n+        // Workflow testing: input values to pass to the workflow\\n+        workflow_input: { type: 'object' },\\n         expect: { $ref: '#/$defs/expectBlock' },\\n         // Flow cases\\n         flow: {\\n@@ -270,6 +282,35 @@ const schema: any = {\\n       },\\n       required: ['step', 'path'],\\n     },\\n+    // Workflow output expectation: like outputsExpectation but without step (tests workflow-level outputs)\\n+    workflowOutputExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        path: { type: 'string' },\\n+        equals: {},\\n+        equalsDeep: {},\\n+        matches: { type: 'string' },\\n+        contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        not_contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            path: { type: 'string' },\\n+            equals: {},\\n+            matches: { type: 'string' },\\n+          },\\n+          required: ['path'],\\n+        },\\n+        contains_unordered: { type: 'array' },\\n+      },\\n+      required: ['path'],\\n+    },\\n     expectBlock: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -278,6 +319,8 @@ const schema: any = {\\n         calls: { type: 'array', items: { $ref: '#/$defs/callsExpectation' } },\\n         prompts: { type: 'array', items: { $ref: '#/$defs/promptsExpectation' } },\\n         outputs: { type: 'array', items: { $ref: '#/$defs/outputsExpectation' } },\\n+        // Workflow testing: assert on workflow-level outputs (defined in workflow's outputs: section)\\n+        workflow_output: { type: 'array', items: { $ref: '#/$defs/workflowOutputExpectation' } },\\n         no_calls: {\\n           type: 'array',\\n           items: {\\n@@ -343,6 +386,7 @@ const knownKeys = new Set([\\n   'version',\\n   'extends',\\n   'tests',\\n+  'workspace',\\n   // tests\\n   'tests.defaults',\\n   'tests.fixtures',\\n@@ -363,6 +407,7 @@ const knownKeys = new Set([\\n   'env',\\n   'routing',\\n   'mocks',\\n+  'workflow_input',\\n   'expect',\\n   'flow',\\n   // expect\\n@@ -370,6 +415,7 @@ const knownKeys = new Set([\\n   'expect.calls',\\n   'expect.prompts',\\n   'expect.outputs',\\n+  'expect.workflow_output',\\n   'expect.no_calls',\\n   'expect.fail',\\n   'expect.strict_violation',\\n@@ -424,7 +470,7 @@ function formatError(e: ErrorObject): string {\\n       if (hint) msg += ` (${hint})`;\\n       // Small curated allow-list for frequent nodes to reduce guesswork\\n       if (path.endsWith('expect')) {\\n-        msg += ` (allowed: use, calls, prompts, outputs, no_calls, fail, strict_violation)`;\\n+        msg += ` (allowed: use, calls, prompts, outputs, workflow_output, no_calls, fail, strict_violation)`;\\n       } else if (path.endsWith('env')) {\\n         msg += ` (values must be strings)`;\\n       } else if (path.endsWith('tests')) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/cli.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/cli.ts b/src/types/cli.ts\\nindex a7d9ca30..f06f93d5 100644\\n--- a/src/types/cli.ts\\n+++ b/src/types/cli.ts\\n@@ -61,6 +61,8 @@ export interface CliOptions {\\n   githubV2?: boolean;\\n   /** Enable Slack Socket Mode runner */\\n   slack?: boolean;\\n+  /** Keep workspace folders after execution (for debugging) */\\n+  keepWorkspace?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/config.ts\",\"additions\":114,\"deletions\":5,\"changes\":119,\"patch\":\"diff --git a/src/types/config.ts b/src/types/config.ts\\nindex ed239959..dd363de1 100644\\n--- a/src/types/config.ts\\n+++ b/src/types/config.ts\\n@@ -122,6 +122,9 @@ export interface FailureConditionContext {\\n   /** Environment variables */\\n   env?: Record<string, string>;\\n \\n+  /** Workflow inputs - values passed to the workflow */\\n+  inputs?: Record<string, unknown>;\\n+\\n   /** Memory accessor for accessing memory store in expressions */\\n   memory?: {\\n     get: (key: string, namespace?: string) => unknown;\\n@@ -356,6 +359,8 @@ export interface AIProviderConfig {\\n   allowBash?: boolean;\\n   /** Advanced bash command execution configuration */\\n   bashConfig?: BashConfig;\\n+  /** Completion prompt for post-completion validation/review (runs after attempt_completion) */\\n+  completion_prompt?: string;\\n }\\n \\n /**\\n@@ -461,8 +466,8 @@ export interface CheckConfig {\\n   env?: EnvConfig;\\n   /** Timeout in seconds for command execution (default: 60) */\\n   timeout?: number;\\n-  /** Check IDs that this check depends on (optional) */\\n-  depends_on?: string[];\\n+  /** Check IDs that this check depends on (optional). Accepts single string or array. */\\n+  depends_on?: string | string[];\\n   /** Group name for comment separation (e.g., \\\"code-review\\\", \\\"pr-overview\\\") - optional */\\n   group?: string;\\n   /** Schema type for template rendering (e.g., \\\"code-review\\\", \\\"markdown\\\") or inline JSON schema object - optional */\\n@@ -621,6 +626,46 @@ export interface CheckConfig {\\n   overrides?: Record<string, Partial<CheckConfig>>;\\n   /** Map workflow outputs to check outputs */\\n   output_mapping?: Record<string, string>;\\n+\\n+  // Workflow aliases for backward compatibility\\n+  /** Alias for args - workflow inputs (backward compatibility) */\\n+  workflow_inputs?: Record<string, unknown>;\\n+  /** Config file path - alternative to workflow ID (loads a Visor config file as workflow) */\\n+  config?: string;\\n+  /** Alias for overrides - workflow step overrides (backward compatibility) */\\n+  workflow_overrides?: Record<string, Partial<CheckConfig>>;\\n+\\n+  /**\\n+   * Git-checkout provider specific options (optional, only used when type === 'git-checkout').\\n+   */\\n+  /** Git reference to checkout (branch, tag, commit SHA) - supports templates */\\n+  ref?: string;\\n+  /** Repository URL or owner/repo format (defaults to current repository) */\\n+  repository?: string;\\n+  /** GitHub token for private repositories (defaults to GITHUB_TOKEN env) */\\n+  token?: string;\\n+  /** Number of commits to fetch (0 for full history, default: 1) */\\n+  fetch_depth?: number;\\n+  /** Whether to fetch tags (default: false) */\\n+  fetch_tags?: boolean;\\n+  /** Checkout submodules: false, true, or 'recursive' */\\n+  submodules?: boolean | 'recursive';\\n+  /** Working directory for the checkout (defaults to temp directory) */\\n+  working_directory?: string;\\n+  /** Use git worktree for efficient parallel checkouts (default: true) */\\n+  use_worktree?: boolean;\\n+  /** Clean the working directory before checkout (default: true) */\\n+  clean?: boolean;\\n+  /** Sparse checkout paths - only checkout specific directories/files */\\n+  sparse_checkout?: string[];\\n+  /** Enable Git LFS (Large File Storage) */\\n+  lfs?: boolean;\\n+  /** Timeout in ms for cloning the bare repository (default: 300000 = 5 min) */\\n+  clone_timeout_ms?: number;\\n+  /** Clean up worktree on failure (default: true) */\\n+  cleanup_on_failure?: boolean;\\n+  /** Keep worktree after workflow completion (default: false) */\\n+  persist_worktree?: boolean;\\n }\\n \\n /**\\n@@ -1037,6 +1082,36 @@ export interface CustomToolDefinition {\\n   outputSchema?: Record<string, unknown>;\\n }\\n \\n+/**\\n+ * Workflow input definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowInput {\\n+  /** Input parameter name */\\n+  name: string;\\n+  /** JSON Schema for the input */\\n+  schema?: Record<string, unknown>;\\n+  /** Whether this input is required */\\n+  required?: boolean;\\n+  /** Default value if not provided */\\n+  default?: unknown;\\n+  /** Human-readable description */\\n+  description?: string;\\n+}\\n+\\n+/**\\n+ * Workflow output definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowOutput {\\n+  /** Output name */\\n+  name: string;\\n+  /** Human-readable description */\\n+  description?: string;\\n+  /** Value using Liquid template syntax (references step outputs) */\\n+  value?: string;\\n+  /** Value using JavaScript expression (alternative to value) */\\n+  value_js?: string;\\n+}\\n+\\n /**\\n  * Main Visor configuration\\n  */\\n@@ -1051,12 +1126,16 @@ export interface VisorConfig {\\n   tools?: Record<string, CustomToolDefinition>;\\n   /** Import workflow definitions from external files or URLs */\\n   imports?: string[];\\n+  /** Workflow inputs (for standalone reusable workflows) */\\n+  inputs?: WorkflowInput[];\\n+  /** Workflow outputs (for standalone reusable workflows) */\\n+  outputs?: WorkflowOutput[];\\n   /** Step configurations (recommended) */\\n   steps?: Record<string, CheckConfig>;\\n   /** Check configurations (legacy, use 'steps' instead) - always populated after normalization */\\n   checks?: Record<string, CheckConfig>;\\n-  /** Output configuration */\\n-  output: OutputConfig;\\n+  /** Output configuration (optional - defaults provided) */\\n+  output?: OutputConfig;\\n   /** HTTP server configuration for receiving webhooks */\\n   http_server?: HttpServerConfig;\\n   /** Memory storage configuration */\\n@@ -1092,6 +1171,36 @@ export interface VisorConfig {\\n     /** Frontend-specific configuration */\\n     config?: unknown;\\n   }>;\\n+  /** Workspace isolation configuration for sandboxed execution */\\n+  workspace?: WorkspaceConfig;\\n+  /** Slack configuration */\\n+  slack?: SlackConfig;\\n+}\\n+\\n+/**\\n+ * Workspace isolation configuration\\n+ */\\n+export interface WorkspaceConfig {\\n+  /** Enable workspace isolation (default: true when config present) */\\n+  enabled?: boolean;\\n+  /** Base path for workspaces (default: /tmp/visor-workspaces) */\\n+  base_path?: string;\\n+  /** Clean up workspace on exit (default: true) */\\n+  cleanup_on_exit?: boolean;\\n+}\\n+\\n+/**\\n+ * Slack configuration\\n+ */\\n+export interface SlackConfig {\\n+  /** Slack API version */\\n+  version?: string;\\n+  /** Mention handling: 'all', 'direct', etc. */\\n+  mentions?: string;\\n+  /** Thread handling: 'required', 'optional', etc. */\\n+  threads?: string;\\n+  /** Show raw output in Slack responses */\\n+  show_raw_output?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/git-checkout.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/git-checkout.ts b/src/types/git-checkout.ts\\nindex eb1c5ebc..4d27855d 100644\\n--- a/src/types/git-checkout.ts\\n+++ b/src/types/git-checkout.ts\\n@@ -16,6 +16,8 @@ export interface GitCheckoutConfig {\\n   fetch_depth?: number;\\n   fetch_tags?: boolean;\\n   submodules?: boolean | 'recursive';\\n+  /** Timeout (ms) for cloning the bare repository; defaults to 300000 (5 minutes) */\\n+  clone_timeout_ms?: number;\\n \\n   // Worktree configuration\\n   working_directory?: string;\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/sandbox.ts\",\"additions\":46,\"deletions\":1,\"changes\":47,\"patch\":\"diff --git a/src/utils/sandbox.ts b/src/utils/sandbox.ts\\nindex 9914a3a5..12e27c30 100644\\n--- a/src/utils/sandbox.ts\\n+++ b/src/utils/sandbox.ts\\n@@ -14,6 +14,51 @@ export interface CompileOptions {\\n   wrapFunction?: boolean;\\n }\\n \\n+export interface JsSyntaxValidationResult {\\n+  valid: boolean;\\n+  error?: string;\\n+}\\n+\\n+/**\\n+ * Validate JavaScript syntax without executing it.\\n+ * Uses the sandbox's compile method to check for syntax errors.\\n+ * Returns validation result with error message if invalid.\\n+ */\\n+export function validateJsSyntax(code: string): JsSyntaxValidationResult {\\n+  if (!code || typeof code !== 'string') {\\n+    return { valid: false, error: 'Code must be a non-empty string' };\\n+  }\\n+\\n+  const trimmed = code.trim();\\n+  if (trimmed.length === 0) {\\n+    return { valid: false, error: 'Code cannot be empty' };\\n+  }\\n+\\n+  // Create a minimal sandbox instance for syntax checking\\n+  const sandbox = createSecureSandbox();\\n+\\n+  // Wrap code similar to compileAndRun to catch the same syntax issues\\n+  const looksLikeBlock = /\\\\breturn\\\\b/.test(trimmed) || /;/.test(trimmed) || /\\\\n/.test(trimmed);\\n+  const looksLikeIife = /\\\\)\\\\s*\\\\(\\\\s*\\\\)\\\\s*;?$/.test(trimmed);\\n+  const body = looksLikeBlock\\n+    ? looksLikeIife\\n+      ? `return (\\\\n${trimmed}\\\\n);\\\\n`\\n+      : `return (() => {\\\\n${trimmed}\\\\n})();\\\\n`\\n+    : `return (\\\\n${trimmed}\\\\n);\\\\n`;\\n+\\n+  // Add log injection header (same as compileAndRun)\\n+  const header = `const __lp = \\\"[syntax-check]\\\"; const log = (...a) => { try { console.log(__lp, ...a); } catch {} };\\\\n`;\\n+  const fullCode = `${header}${body}`;\\n+\\n+  try {\\n+    sandbox.compile(fullCode);\\n+    return { valid: true };\\n+  } catch (e) {\\n+    const msg = e instanceof Error ? e.message : String(e);\\n+    return { valid: false, error: msg };\\n+  }\\n+}\\n+\\n /**\\n  * Create a hardened Sandbox with a consistent set of globals and prototype\\n  * whitelists. This is a superset of the sets previously used by individual\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/tracer-init.ts\",\"additions\":19,\"deletions\":1,\"changes\":20,\"patch\":\"diff --git a/src/utils/tracer-init.ts b/src/utils/tracer-init.ts\\nindex b42995d9..4edf1ffc 100644\\n--- a/src/utils/tracer-init.ts\\n+++ b/src/utils/tracer-init.ts\\n@@ -76,6 +76,24 @@ export async function initializeTracer(\\n \\n       const tracer = new SimpleAppTracer(telemetry, sessionId);\\n \\n+      // WORKAROUND: Add missing recordEvent method for completionPrompt feature (probe #321)\\n+      // SimpleAppTracer doesn't have recordEvent but completionPrompt requires it\\n+      if (typeof (tracer as any).recordEvent !== 'function') {\\n+        (tracer as any).recordEvent = (\\n+          name: string,\\n+          attributes?: Record<string, unknown>\\n+        ) => {\\n+          // Log completion events to telemetry for debugging\\n+          try {\\n+            if ((telemetry as any).record) {\\n+              (telemetry as any).record({ event: name, ...attributes });\\n+            }\\n+          } catch {\\n+            // Best-effort only\\n+          }\\n+        };\\n+      }\\n+\\n       console.error(`üìä Simple tracing enabled, will save to: ${traceFilePath}`);\\n \\n       // If in GitHub Actions, log the path for artifact upload\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/worktree-manager.ts\",\"additions\":142,\"deletions\":27,\"changes\":169,\"patch\":\"diff --git a/src/utils/worktree-manager.ts b/src/utils/worktree-manager.ts\\nindex f3cebf3d..2de7a9f4 100644\\n--- a/src/utils/worktree-manager.ts\\n+++ b/src/utils/worktree-manager.ts\\n@@ -6,6 +6,7 @@\\n  */\\n \\n import * as fs from 'fs';\\n+import * as fsp from 'fs/promises';\\n import * as path from 'path';\\n import * as crypto from 'crypto';\\n import { commandExecutor } from './command-executor';\\n@@ -119,7 +120,8 @@ export class WorktreeManager {\\n     repository: string,\\n     repoUrl: string,\\n     token?: string,\\n-    fetchDepth?: number\\n+    fetchDepth?: number,\\n+    cloneTimeoutMs?: number\\n   ): Promise<string> {\\n     const reposDir = this.getReposDir();\\n     const repoName = repository.replace(/\\\\//g, '-');\\n@@ -129,9 +131,21 @@ export class WorktreeManager {\\n     if (fs.existsSync(bareRepoPath)) {\\n       logger.debug(`Bare repository already exists: ${bareRepoPath}`);\\n \\n-      // Update remote refs\\n-      await this.updateBareRepo(bareRepoPath);\\n-      return bareRepoPath;\\n+      // Verify the bare repo has the correct remote URL to prevent using corrupted repos\\n+      const verifyResult = await this.verifyBareRepoRemote(bareRepoPath, repoUrl);\\n+      if (verifyResult === 'timeout') {\\n+        // Timeout during verification - use stale cache to avoid hanging on re-clone\\n+        logger.info(`Using stale bare repository (verification timed out): ${bareRepoPath}`);\\n+        return bareRepoPath;\\n+      } else if (verifyResult === false) {\\n+        logger.warn(`Bare repository at ${bareRepoPath} has incorrect remote, removing and re-cloning`);\\n+        await fsp.rm(bareRepoPath, { recursive: true, force: true });\\n+        // Fall through to clone below\\n+      } else {\\n+        // Update remote refs\\n+        await this.updateBareRepo(bareRepoPath);\\n+        return bareRepoPath;\\n+      }\\n     }\\n \\n     // Clone as bare repository\\n@@ -153,7 +167,9 @@ export class WorktreeManager {\\n     }\\n     cloneCmd += ` ${this.escapeShellArg(cloneUrl)} ${this.escapeShellArg(bareRepoPath)}`;\\n \\n-    const result = await this.executeGitCommand(cloneCmd, { timeout: 300000 }); // 5 minute timeout\\n+    const result = await this.executeGitCommand(cloneCmd, {\\n+      timeout: cloneTimeoutMs || 300000, // default 5 minutes\\n+    });\\n \\n     if (result.exitCode !== 0) {\\n       // Redact tokens from error messages\\n@@ -171,19 +187,98 @@ export class WorktreeManager {\\n   private async updateBareRepo(bareRepoPath: string): Promise<void> {\\n     logger.debug(`Updating bare repository: ${bareRepoPath}`);\\n \\n-    const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n-    const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+    try {\\n+      const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n+      const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n+        // Don't throw - we can continue with stale refs\\n+      } else {\\n+        logger.debug(`Successfully updated bare repository`);\\n+      }\\n+    } catch (error) {\\n+      // Handle timeout or other errors gracefully - we can continue with stale refs\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      logger.warn(`Failed to update bare repository (will use stale refs): ${errorMessage}`);\\n+    }\\n+  }\\n \\n-    if (result.exitCode !== 0) {\\n-      logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n-      // Don't throw - we can continue with stale refs\\n-    } else {\\n-      logger.debug(`Successfully updated bare repository`);\\n+  /**\\n+   * Verify that a bare repository has the correct remote URL.\\n+   * This prevents reusing corrupted repos that were cloned from a different repository.\\n+   * Returns: true (valid), false (invalid - should re-clone), or 'timeout' (use stale cache)\\n+   */\\n+  private async verifyBareRepoRemote(\\n+    bareRepoPath: string,\\n+    expectedUrl: string\\n+  ): Promise<boolean | 'timeout'> {\\n+    try {\\n+      const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote get-url origin`;\\n+      const result = await this.executeGitCommand(cmd, { timeout: 10000 });\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to get remote URL for ${bareRepoPath}: ${result.stderr}`);\\n+        return false;\\n+      }\\n+\\n+      const actualUrl = result.stdout.trim();\\n+\\n+      // Normalize URLs for comparison:\\n+      // - remove credentials (tokens/username)\\n+      // - remove .git suffix if present\\n+      // - trim trailing slash\\n+      // - lowercase for case‚Äëinsensitive match\\n+      const normalizeUrl = (url: string): string => {\\n+        // Convert common SSH form to https for comparison\\n+        if (url.startsWith('git@github.com:')) {\\n+          url = url.replace('git@github.com:', 'https://github.com/');\\n+        }\\n+        return url\\n+          // strip userinfo part to avoid mismatches when the cached bare\\n+          // repo was cloned with an access token but the expected URL is\\n+          // tokenless (or vice‚Äëversa)\\n+          .replace(/:\\\\/\\\\/[^@]+@/, '://')\\n+          .replace(/\\\\.git$/, '')\\n+          .replace(/\\\\/$/, '')\\n+          .toLowerCase();\\n+      };\\n+\\n+      const normalizedExpected = normalizeUrl(expectedUrl);\\n+      const normalizedActual = normalizeUrl(actualUrl);\\n+\\n+      if (normalizedExpected !== normalizedActual) {\\n+        logger.warn(\\n+          `Bare repo remote mismatch: expected ${expectedUrl}, got ${actualUrl}`\\n+        );\\n+        return false;\\n+      }\\n+\\n+      logger.debug(`Bare repo remote verified: ${actualUrl}`);\\n+      return true;\\n+    } catch (error) {\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      // If it's a timeout, return 'timeout' so caller can use stale cache instead of re-cloning\\n+      if (errorMessage.includes('timed out')) {\\n+        logger.warn(`Timeout verifying bare repo remote (will use stale cache): ${errorMessage}`);\\n+        return 'timeout';\\n+      }\\n+      logger.warn(`Error verifying bare repo remote: ${error}`);\\n+      return false;\\n     }\\n   }\\n \\n   /**\\n-   * Create a new worktree\\n+   * Create a new worktree for the given repository/ref.\\n+   *\\n+   * Important: we always create worktrees in a detached HEAD state pinned\\n+   * to a specific commit SHA rather than a named branch like \\\"main\\\". Git\\n+   * only allows a branch to be checked out in a single worktree at a time;\\n+   * using the raw commit (plus --detach) lets multiple workflows safely\\n+   * create independent worktrees for the same branch without hitting\\n+   * errors like:\\n+   *\\n+   *   fatal: 'main' is already used by worktree at '.../TykTechnologies-tyk-docs-main-XXXX'\\n    */\\n   async createWorktree(\\n     repository: string,\\n@@ -195,6 +290,7 @@ export class WorktreeManager {\\n       clean?: boolean;\\n       workflowId?: string;\\n       fetchDepth?: number;\\n+      cloneTimeoutMs?: number;\\n     } = {}\\n   ): Promise<WorktreeInfo> {\\n     // Validate ref to prevent command injection\\n@@ -205,7 +301,8 @@ export class WorktreeManager {\\n       repository,\\n       repoUrl,\\n       options.token,\\n-      options.fetchDepth\\n+      options.fetchDepth,\\n+      options.cloneTimeoutMs\\n     );\\n \\n     // Generate worktree ID and path\\n@@ -241,21 +338,24 @@ export class WorktreeManager {\\n       }\\n     }\\n \\n-    // Fetch the ref if needed\\n+    // Fetch the ref if needed, then resolve it to a concrete commit SHA.\\n+    // We use the commit (detached HEAD) instead of the branch name so we\\n+    // can have multiple worktrees for the same branch without git refusing\\n+    // with \\\"branch X is already checked out\\\".\\n     await this.fetchRef(bareRepoPath, ref);\\n+    const commit = await this.getCommitShaForRef(bareRepoPath, ref);\\n \\n-    // Create worktree\\n-    logger.info(`Creating worktree for ${repository}@${ref}`);\\n-    const createCmd = `git -C ${this.escapeShellArg(bareRepoPath)} worktree add ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(ref)}`;\\n+    // Create worktree in detached HEAD state at the resolved commit\\n+    logger.info(`Creating worktree for ${repository}@${ref} (${commit})`);\\n+    const createCmd = `git -C ${this.escapeShellArg(\\n+      bareRepoPath\\n+    )} worktree add --detach ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(commit)}`;\\n     const result = await this.executeGitCommand(createCmd, { timeout: 60000 });\\n \\n     if (result.exitCode !== 0) {\\n       throw new Error(`Failed to create worktree: ${result.stderr}`);\\n     }\\n \\n-    // Get commit SHA\\n-    const commit = await this.getCommitSha(worktreePath);\\n-\\n     // Create metadata\\n     const metadata: WorktreeMetadata = {\\n       worktree_id: worktreeId,\\n@@ -316,14 +416,17 @@ export class WorktreeManager {\\n   }\\n \\n   /**\\n-   * Get commit SHA for worktree\\n+   * Get commit SHA for a given ref inside a bare repository.\\n+   *\\n+   * This runs after fetchRef so that <ref> should resolve to either a\\n+   * local branch, tag, or remote-tracking ref.\\n    */\\n-  private async getCommitSha(worktreePath: string): Promise<string> {\\n-    const cmd = `git -C ${this.escapeShellArg(worktreePath)} rev-parse HEAD`;\\n+  private async getCommitShaForRef(bareRepoPath: string, ref: string): Promise<string> {\\n+    const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} rev-parse ${this.escapeShellArg(ref)}`;\\n     const result = await this.executeGitCommand(cmd);\\n \\n     if (result.exitCode !== 0) {\\n-      throw new Error(`Failed to get commit SHA: ${result.stderr}`);\\n+      throw new Error(`Failed to get commit SHA for ref ${ref}: ${result.stderr}`);\\n     }\\n \\n     return result.stdout.trim();\\n@@ -621,9 +724,21 @@ export class WorktreeManager {\\n     command: string,\\n     options: { timeout?: number; env?: Record<string, string> } = {}\\n   ): Promise<GitCommandResult> {\\n+    // Merge provided env with process.env and add git-specific settings\\n+    // These settings prevent git from hanging on interactive prompts while\\n+    // still allowing OS-level credential helpers to work:\\n+    // - GIT_TERMINAL_PROMPT=0: Prevents terminal credential prompts\\n+    // - GIT_SSH_COMMAND: Disables SSH password prompts (BatchMode)\\n+    const gitEnv = {\\n+      ...process.env,\\n+      ...options.env,\\n+      GIT_TERMINAL_PROMPT: '0',\\n+      GIT_SSH_COMMAND: 'ssh -o BatchMode=yes -o StrictHostKeyChecking=no',\\n+    };\\n+\\n     const result = await commandExecutor.execute(command, {\\n       timeout: options.timeout || 30000,\\n-      env: options.env || process.env,\\n+      env: gitEnv,\\n     } as any);\\n \\n     return {\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-executor.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-executor.ts b/src/workflow-executor.ts\\nindex 751e7f16..df76762f 100644\\n--- a/src/workflow-executor.ts\\n+++ b/src/workflow-executor.ts\\n@@ -219,7 +219,9 @@ export class WorkflowExecutor {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps)) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     // Use static DependencyResolver\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-registry.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-registry.ts b/src/workflow-registry.ts\\nindex 2cbe3a89..6bd855fb 100644\\n--- a/src/workflow-registry.ts\\n+++ b/src/workflow-registry.ts\\n@@ -410,7 +410,9 @@ export class WorkflowRegistry {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps || {})) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     try {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/ai-review-service.test.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/tests/unit/ai-review-service.test.ts b/tests/unit/ai-review-service.test.ts\\nindex ec2b998a..2c616c99 100644\\n--- a/tests/unit/ai-review-service.test.ts\\n+++ b/tests/unit/ai-review-service.test.ts\\n@@ -482,6 +482,37 @@ describe('AIReviewService', () => {\\n       expect(context).toContain('<additions>10</additions>');\\n       expect(context).toContain('<deletions>5</deletions>');\\n     });\\n+\\n+    it('should exclude diffs by default when in Slack mode', async () => {\\n+      // Slack mode is detected by presence of slackConversation property\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // In Slack mode, diffs should be excluded by default\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+      // Should still include file summary\\n+      expect(context).toContain('<files_summary>');\\n+    });\\n+\\n+    it('should include diffs in Slack mode when explicitly requested', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      (mockPRInfo as any).includeCodeContext = true;\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // With explicit includeCodeContext: true, diffs should be included\\n+      expect(context).toContain('<full_diff>');\\n+      expect(context).toContain('--- test.ts');\\n+    });\\n+\\n+    it('should exclude diffs in Slack mode even when includeCodeContext is undefined', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      // Deliberately not setting includeCodeContext to test default behavior\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+    });\\n   });\\n \\n   describe('Comment Filtering for Code Review', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config-extends.test.ts\",\"additions\":8,\"deletions\":8,\"changes\":16,\"patch\":\"diff --git a/tests/unit/config-extends.test.ts b/tests/unit/config-extends.test.ts\\nindex b112a869..7ee69ce6 100644\\n--- a/tests/unit/config-extends.test.ts\\n+++ b/tests/unit/config-extends.test.ts\\n@@ -564,7 +564,7 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.security).toBeDefined();\\n       expect(config.checks!.performance).toBeDefined();\\n       expect(config.checks!.custom).toBeDefined();\\n-      expect(config.output.pr_comment.format).toBe('json');\\n+      expect(config.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should extend from default configuration', async () => {\\n@@ -1002,7 +1002,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.base).toBeDefined(); // From base\\n       expect(loaded.checks!.middle).toBeDefined(); // From middle\\n       expect(loaded.checks!.top).toBeDefined(); // From top\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From top\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From top\\n     });\\n \\n     it('should handle mixed local and remote extends', async () => {\\n@@ -1079,7 +1079,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.local).toBeDefined();\\n       expect(loaded.checks!.remote).toBeDefined();\\n       expect(loaded.checks!.child).toBeDefined();\\n-      expect(loaded.output.pr_comment.format).toBe('json');\\n+      expect(loaded.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should handle extends with relative path resolution', async () => {\\n@@ -1593,9 +1593,9 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.performance).toBeDefined(); // Added in child\\n \\n       // Verify output\\n-      expect(config.output.pr_comment.format).toBe('json'); // Overridden\\n-      expect(config.output.pr_comment.group_by).toBe('file'); // Overridden\\n-      expect(config.output.pr_comment.collapse).toBe(false); // Overridden\\n+      expect(config.output!.pr_comment.format).toBe('json'); // Overridden\\n+      expect(config.output!.pr_comment.group_by).toBe('file'); // Overridden\\n+      expect(config.output!.pr_comment.collapse).toBe(false); // Overridden\\n     });\\n \\n     it('should handle configuration with disabled checks removal', async () => {\\n@@ -1814,7 +1814,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.root).toBeDefined(); // From root\\n       expect(loaded.checks!.level1).toBeDefined(); // From level1\\n       expect(loaded.checks!.level2).toBeDefined(); // From level2\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From level2\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From level2\\n     });\\n \\n     it('should handle mixed absolute and relative paths in extends array', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config.test.ts\",\"additions\":7,\"deletions\":7,\"changes\":14,\"patch\":\"diff --git a/tests/unit/config.test.ts b/tests/unit/config.test.ts\\nindex 67ce61fd..0677a3d4 100644\\n--- a/tests/unit/config.test.ts\\n+++ b/tests/unit/config.test.ts\\n@@ -48,7 +48,7 @@ output:\\n       expect(config.version).toBe('1.0');\\n       expect(config.checks).toHaveProperty('performance');\\n       expect(config.checks).toHaveProperty('security');\\n-      expect(config.output.pr_comment.format).toBe('table');\\n+      expect(config.output!.pr_comment.format).toBe('table');\\n     });\\n \\n     it('should handle missing config file gracefully', async () => {\\n@@ -264,9 +264,9 @@ checks:\\n       const config = await configManager.loadConfig('/path/to/minimal.yaml');\\n \\n       // Should have default output configuration\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.group_by).toBe('check');\\n-      expect(config.output.pr_comment.collapse).toBe(true);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.group_by).toBe('check');\\n+      expect(config.output!.pr_comment.collapse).toBe(true);\\n     });\\n   });\\n \\n@@ -483,8 +483,8 @@ output:\\n       expect(config.checks!.performance.prompt).toContain('N+1 database queries');\\n       expect(config.checks!.security.prompt).toContain('SQL injection');\\n       expect(config.checks!.architecture.prompt).toContain('SOLID principles');\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.collapse).toBe(false);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.collapse).toBe(false);\\n     });\\n   });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/foreach-empty-skip.test.ts\",\"additions\":5,\"deletions\":3,\"changes\":8,\"patch\":\"diff --git a/tests/unit/foreach-empty-skip.test.ts b/tests/unit/foreach-empty-skip.test.ts\\nindex 1ea8e1e5..2640ed14 100644\\n--- a/tests/unit/foreach-empty-skip.test.ts\\n+++ b/tests/unit/foreach-empty-skip.test.ts\\n@@ -90,11 +90,13 @@ describe('forEach Empty Array Skip', () => {\\n     );\\n     expect(processStats?.totalRuns).toBe(0);\\n \\n-    // notify-ticket should also NOT run because process-ticket returned empty\\n+    // notify-ticket SHOULD run even though process-ticket had 0 runs from empty forEach\\n+    // This is the expected behavior: downstream steps that depend on forEach children\\n+    // should still execute (with empty/null inputs) to handle the \\\"no items\\\" case\\n     const notifyStats = result.executionStatistics?.checks!.find(\\n       c => c.checkName === 'notify-ticket'\\n     );\\n-    expect(notifyStats?.totalRuns).toBe(0);\\n+    expect(notifyStats?.totalRuns).toBe(1);\\n   });\\n \\n   it('should handle empty array with transform_js', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/ai-check-provider.test.ts\",\"additions\":61,\"deletions\":1,\"changes\":62,\"patch\":\"diff --git a/tests/unit/providers/ai-check-provider.test.ts b/tests/unit/providers/ai-check-provider.test.ts\\nindex d0b94086..a858ddae 100644\\n--- a/tests/unit/providers/ai-check-provider.test.ts\\n+++ b/tests/unit/providers/ai-check-provider.test.ts\\n@@ -399,6 +399,66 @@ describe('AICheckProvider', () => {\\n         },\\n       });\\n     });\\n+\\n+    it('derives allowedFolders and path from workspace when present on parent context', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const workspace = {\\n+        isEnabled: () => true,\\n+        getWorkspaceInfo: () => ({\\n+          workspacePath: '/tmp/ws-123',\\n+          mainProjectPath: '/tmp/ws-123/main-project',\\n+        }),\\n+        listProjects: () => [\\n+          { name: 'tyk', path: '/tmp/ws-123/tyk' },\\n+          { name: 'tyk-docs', path: '/tmp/ws-123/tyk-docs' },\\n+        ],\\n+      };\\n+\\n+      const execContext: any = {\\n+        _parentContext: {\\n+          workspace,\\n+        },\\n+      };\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'code_help',\\n+        ai: {\\n+          provider: 'google',\\n+          model: 'gemini-2.5-pro',\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, execContext);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'google',\\n+        model: 'gemini-2.5-pro',\\n+        // Workspace root should be used as primary working directory for tools\\n+        path: '/tmp/ws-123',\\n+      });\\n+\\n+      // allowedFolders should contain the workspace plus all project paths, de-duped\\n+      expect(capturedConfig.allowedFolders).toEqual(\\n+        expect.arrayContaining(['/tmp/ws-123', '/tmp/ws-123/tyk', '/tmp/ws-123/tyk-docs'])\\n+      );\\n+    });\\n   });\\n \\n   describe('getSupportedConfigKeys', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/command-check-provider.test.ts\",\"additions\":105,\"deletions\":1,\"changes\":106,\"patch\":\"diff --git a/tests/unit/providers/command-check-provider.test.ts b/tests/unit/providers/command-check-provider.test.ts\\nindex 3bf11a84..0a821bbd 100644\\n--- a/tests/unit/providers/command-check-provider.test.ts\\n+++ b/tests/unit/providers/command-check-provider.test.ts\\n@@ -666,4 +666,108 @@ describe('CommandCheckProvider', () => {\\n       expect((result as any).output).toBe('content'); // CommandCheckProvider trims whitespace\\n     });\\n   });\\n+\\n+  describe('Workflow Inputs', () => {\\n+    it('should use workflowInputs from config when available', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Hello from config.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from config.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with inputs from config.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from config.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should fall back to context.workflowInputs when config.workflowInputs is not set', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Hello from context.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from context.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify the command was rendered with inputs from context.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from context.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should prefer config.workflowInputs over context.workflowInputs', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Config takes precedence',\\n+        },\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Context should be ignored',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Config takes precedence\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify config.workflowInputs takes precedence\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Config takes precedence\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should provide empty inputs when neither config nor context has workflowInputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with empty inputs (undefined renders as empty)\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+  });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/git-checkout-workspace.test.ts\",\"additions\":4,\"deletions\":5,\"changes\":9,\"patch\":\"diff --git a/tests/unit/providers/git-checkout-workspace.test.ts b/tests/unit/providers/git-checkout-workspace.test.ts\\nindex 4eb625b7..4a0fbb6e 100644\\n--- a/tests/unit/providers/git-checkout-workspace.test.ts\\n+++ b/tests/unit/providers/git-checkout-workspace.test.ts\\n@@ -129,13 +129,12 @@ describe('GitCheckoutProvider Workspace Integration', () => {\\n         const output = (result as any).output as GitCheckoutOutput;\\n         expect(output.success).toBe(true);\\n         expect(output.workspace_path).toBeDefined();\\n-        // When checkName is provided, it's used as the project name\\n-        expect(output.workspace_path).toContain('test-checkout');\\n \\n-        // Verify project was added to workspace\\n+        // Verify project was added to workspace and the name is derived\\n+        // from the repository/description, not the checkName.\\n         const projects = workspace.listProjects();\\n         expect(projects.length).toBe(1);\\n-        expect(projects[0].name).toBe('test-checkout');\\n+        expect(projects[0].name).toBe('external-repo');\\n       } finally {\\n         await workspace.cleanup();\\n         fs.rmSync(mainProjectDir, { recursive: true, force: true });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/http-client-provider.test.ts\",\"additions\":15,\"deletions\":10,\"changes\":25,\"patch\":\"diff --git a/tests/unit/providers/http-client-provider.test.ts b/tests/unit/providers/http-client-provider.test.ts\\nindex b759ad75..caa73417 100644\\n--- a/tests/unit/providers/http-client-provider.test.ts\\n+++ b/tests/unit/providers/http-client-provider.test.ts\\n@@ -120,15 +120,17 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect(result).toEqual<ReviewSummary & { data: unknown }>({\\n+      // The provider returns data in the 'output' property (consistent with other providers)\\n+      expect(result).toEqual({\\n         issues: [],\\n-        data: responseData,\\n+        output: { status: 'ok', data: { value: 123 } },\\n       });\\n     });\\n \\n     it('should handle POST request with body', async () => {\\n       mockConfig.method = 'POST';\\n-      mockConfig.body = '{\\\"request\\\": \\\"data\\\"}';\\n+      // Use a body with Liquid template to trigger parseAndRender\\n+      mockConfig.body = '{\\\"request\\\": \\\"{{ pr.title }}\\\"}';\\n \\n       const responseData = { result: 'success' };\\n       const mockResponse = {\\n@@ -143,15 +145,16 @@ describe('HttpClientProvider', () => {\\n       };\\n \\n       mockFetch.mockResolvedValue(mockResponse);\\n-      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"data\\\"}');\\n+      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"Test PR\\\"}');\\n \\n       const result = await provider.execute(mockPRInfo, mockConfig, new Map());\\n \\n+      // Verify the body template was parsed by Liquid with the correct template\\n       expect(mockLiquid.parseAndRender).toHaveBeenCalledWith(\\n-        '{\\\"request\\\": \\\"data\\\"}',\\n+        '{\\\"request\\\": \\\"{{ pr.title }}\\\"}',\\n         expect.objectContaining({\\n           pr: expect.any(Object),\\n-          outputs: {},\\n+          outputs: expect.any(Object),\\n         })\\n       );\\n \\n@@ -159,7 +162,7 @@ describe('HttpClientProvider', () => {\\n         'https://api.example.com/data',\\n         expect.objectContaining({\\n           method: 'POST',\\n-          body: '{\\\"request\\\": \\\"data\\\"}',\\n+          body: '{\\\"request\\\": \\\"Test PR\\\"}',\\n           headers: expect.objectContaining({\\n             Authorization: 'Bearer token',\\n             'Content-Type': 'application/json',\\n@@ -167,7 +170,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(responseData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { result: string } }).output.result).toEqual('success');\\n     });\\n \\n     it('should handle HTTP errors', async () => {\\n@@ -235,7 +239,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(transformedData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { transformed: string } }).output.transformed).toEqual('result');\\n     });\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/session-reuse-config.test.ts\",\"additions\":20,\"deletions\":1,\"changes\":21,\"patch\":\"diff --git a/tests/unit/session-reuse-config.test.ts b/tests/unit/session-reuse-config.test.ts\\nindex a2e2ee26..18fc42e6 100644\\n--- a/tests/unit/session-reuse-config.test.ts\\n+++ b/tests/unit/session-reuse-config.test.ts\\n@@ -152,6 +152,25 @@ describe('Session Reuse Configuration Validation', () => {\\n       }).not.toThrow();\\n     });\\n \\n+    it('should accept reuse_ai_session=\\\\\\\"self\\\\\\\" without depends_on', () => {\\n+      const config: Partial<VisorConfig> = {\\n+        version: '1.0',\\n+        checks: {\\n+          'loop-check': {\\n+            type: 'ai',\\n+            prompt: 'Chat-like loop that reuses its own session',\\n+            on: ['pr_opened'],\\n+            // Special self-reuse mode does not require depends_on\\n+            reuse_ai_session: 'self',\\n+          },\\n+        },\\n+      };\\n+\\n+      expect(() => {\\n+        (configManager as any).validateConfig(config);\\n+      }).not.toThrow();\\n+    });\\n+\\n     it('should accept reuse_ai_session=true with multiple dependencies', () => {\\n       const config: Partial<VisorConfig> = {\\n         version: '1.0',\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/slack-markdown-format.test.ts\",\"additions\":25,\"deletions\":1,\"changes\":26,\"patch\":\"diff --git a/tests/unit/slack-markdown-format.test.ts b/tests/unit/slack-markdown-format.test.ts\\nindex 5e57ccae..39d0baf3 100644\\n--- a/tests/unit/slack-markdown-format.test.ts\\n+++ b/tests/unit/slack-markdown-format.test.ts\\n@@ -39,4 +39,28 @@ describe('markdownToSlack', () => {\\n       ['‚Ä¢ parent', '  ‚Ä¢ child', '```', '- not-a-bullet inside code', '```', '‚Ä¢ after'].join('\\\\n')\\n     );\\n   });\\n+\\n+  it('converts markdown headers to bold text', () => {\\n+    const input = '# Main Title\\\\n## Subtitle\\\\n### Section';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*Main Title*\\\\n*Subtitle*\\\\n*Section*');\\n+  });\\n+\\n+  it('adds newline before h1/h2 headers when preceded by content', () => {\\n+    const input = 'Some content\\\\n## New Section\\\\nMore content';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('Some content\\\\n\\\\n*New Section*\\\\nMore content');\\n+  });\\n+\\n+  it('does not add newline before h1 if it is the first line', () => {\\n+    const input = '# First Header\\\\nContent here';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*First Header*\\\\nContent here');\\n+  });\\n+\\n+  it('ignores headers inside code blocks', () => {\\n+    const input = '```\\\\n# This is a comment\\\\n```\\\\n# Real Header';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('```\\\\n# This is a comment\\\\n```\\\\n*Real Header*');\\n+  });\\n });\\n\",\"status\":\"modified\"}],\"outputs\":{\"route-intent\":{\"intent\":\"chat\",\"topic\":\"User said hi!\",\"text\":\"{\\n  \\\"intent\\\": \\\"chat\\\",\\n  \\\"topic\\\": \\\"User said hi!\\\"\\n}\",\"ts\":1767891909379},\"ask\":{\"text\":\"hi!\",\"ts\":1767891905091},\"log-request\":{\"issues\":[],\"logOutput\":\"‚ÑπÔ∏è **INFO**: üì• Request from user: unknown\\nChannel: unknown\\nThread: unknown\\nMessage: hi!\\n\"},\"jira-context\":{\"jira_context_xml\":\"\",\"issues\":[],\"issue_count\":0,\"ts\":1767891905244},\"zendesk-context\":{\"zendesk_context_xml\":\"\",\"tickets\":\"\",\"ticket_count\":0,\"attachments\":\"\",\"ts\":1767891905257},\"code-help-error\":{\"issues\":[]},\"customer-insights-error\":{\"issues\":[]},\"release-notes-error\":{\"issues\":[]}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"chat-answer","visor.check.output":"{\"text\":\"Hello! I'm a Tyk assistant. How can I help you today?\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":3,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":3,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":3,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":3,"session_id":"81f93225-da70-490e-bded-b4a4296e52bf"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"jira-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"zendesk-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"log-request","visor.provider.type":"log"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"f82aab11-248e-4ae4-b41b-944255158998"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"f82aab11-248e-4ae4-b41b-944255158998"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"f82aab11-248e-4ae4-b41b-944255158998"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-keys","visor.provider.type":"script"}}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-ids","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"history\":{}},\"outputs_history\":{},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"include_comments\":true,\"max_tickets\":5,\"download_assets\":true,\"cache_dir\":\".cache/zendesk\",\"text\":\"<@U09T5KRLMPU>\\n\\nThis is a spike for an observability subject. There is a lot of information in it, but given your knowledge of Tyk, I'd like you to review the  content of this and suggest areas of the spike where you can add more detail, or areas of the spike where you think we have missed something.\\n<https://tyktech.atlassian.net/wiki/spaces/EN/pages/3230957684/SPIKE+Technical+Investigation+for+Exporting+Application+Logs>\\nThis is a spike for an observability subject. There is a lot of information in it, but given your knowledge of Tyk, I'd like you to review the  content of this and suggest areas of the spike where you can add more detail, or areas of the spike where you think we have missed something.\\n<https://tyktech.atlassian.net/wiki/spaces/EN/pages/3230957684/SPIKE+Technical+Investigation+for+Exporting+Application+Logs>\\n\"},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"f82aab11-248e-4ae4-b41b-944255158998"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"f82aab11-248e-4ae4-b41b-944255158998"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"f82aab11-248e-4ae4-b41b-944255158998"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"no-issues-fallback","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"no-issues-fallback","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"extract-keys\":{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767892784241},\"fetch-issues\":{\"issues\":[]},\"history\":{\"extract-keys\":[{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767892784241}],\"fetch-issues\":[{\"issues\":[]}]}},\"outputs_history\":{\"extract-keys\":[{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767892784241}],\"fetch-issues\":[{\"issues\":[]}]},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"text\":\"<@U09T5KRLMPU>\\n\\nThis is a spike for an observability subject. There is a lot of information in it, but given your knowledge of Tyk, I'd like you to review the  content of this and suggest areas of the spike where you can add more detail, or areas of the spike where you think we have missed something.\\n<https://tyktech.atlassian.net/wiki/spaces/EN/pages/3230957684/SPIKE+Technical+Investigation+for+Exporting+Application+Logs>\\nThis is a spike for an observability subject. There is a lot of information in it, but given your knowledge of Tyk, I'd like you to review the  content of this and suggest areas of the spike where you can add more detail, or areas of the spike where you think we have missed something.\\n<https://tyktech.atlassian.net/wiki/spaces/EN/pages/3230957684/SPIKE+Technical+Investigation+for+Exporting+Application+Logs>\\n\",\"jql\":\"\",\"custom_field_aliases\":{},\"include_comments\":true,\"max_issues\":3,\"ticket_prefixes\":[\"TT\",\"DX\"]},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.output":"{\"data\":[\"3230957684\"],\"count\":1}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.check","attributes":{"visor.check.id":"no-issues-fallback","visor.check.output":"\"<jira_context><issue_count>0</issue_count><message>No Jira issue keys found in the provided text</message></jira_context>\""},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"f82aab11-248e-4ae4-b41b-944255158998"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"f82aab11-248e-4ae4-b41b-944255158998"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"f82aab11-248e-4ae4-b41b-944255158998"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"f82aab11-248e-4ae4-b41b-944255158998"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"f82aab11-248e-4ae4-b41b-944255158998"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"fetch-tickets","visor.provider.type":"http_client"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"prepare-attachments","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"aggregate-downloads","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"format-output","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"f658918e-69f1-4924-8304-e22efdf2d955"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"route-intent","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.input.context":"{\"pr\":{\"number\":0,\"title\":\"Local Analysis: feat/slack-frontend-v2 (70 modified)\",\"author\":\"Leonid Bugaev\",\"branch\":\"feat/slack-frontend-v2\",\"base\":\"main\"},\"files\":[{\"filename\":\"README.md\",\"additions\":7,\"deletions\":1,\"changes\":8,\"patch\":\"diff --git a/README.md b/README.md\\nindex 41131861..d92f02cc 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -534,6 +534,12 @@ steps:\\n     session_mode: append    # Shares history for full conversation\\n ```\\n \\n+You can also reuse the **same check‚Äôs** session when it loops back to itself, using:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` with `session_mode: append`\\n+\\n+See the standalone example at `examples/session-reuse-self.yaml` and the detailed guide in [docs/advanced-ai.md](docs/advanced-ai.md).\\n+\\n Learn more: [docs/advanced-ai.md](docs/advanced-ai.md)\\n \\n ## üìã Schema-Template System\\n\",\"status\":\"modified\"},{\"filename\":\"docs/advanced-ai.md\",\"additions\":61,\"deletions\":2,\"changes\":63,\"patch\":\"diff --git a/docs/advanced-ai.md b/docs/advanced-ai.md\\nindex bb8e684a..879285ed 100644\\n--- a/docs/advanced-ai.md\\n+++ b/docs/advanced-ai.md\\n@@ -1,7 +1,7 @@\\n ## üß† Advanced AI Features\\n \\n ### AI Session Reuse\\n-Use `reuse_ai_session: true` on dependent checks to continue conversation context with the AI across checks. This improves follow‚Äëups and consistency.\\n+Use `reuse_ai_session` on checks to continue conversation context with the AI across steps. This improves follow‚Äëups and consistency for follow‚Äëon analysis and chat‚Äëstyle flows.\\n \\n **Session Modes:**\\n - **`clone` (default)**: Creates a copy of the conversation history. Each check gets an independent session with the same starting context. Changes made by one check don't affect others.\\n@@ -26,6 +26,65 @@ steps:\\n     depends_on: [security-remediation]\\n     reuse_ai_session: true\\n     session_mode: append  # Share history - sees full conversation\\n+\\n+#### Reusing your own session: `reuse_ai_session: self`\\n+\\n+Sometimes the step you want to loop back into is the AI step itself (e.g. Slack assistants or multi‚Äëturn internal tools). For that case you can use:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` ‚Äì the step reuses its **own** Probe session when it runs again in the same engine run.\\n+- `session_mode: append` ‚Äì makes the follow‚Äëup behave like a normal conversation turn.\\n+\\n+On the first run of the step, Visor creates a new ProbeAgent session and registers it. If routing (`on_success.goto`, `goto_js`, etc.) later jumps back to the same step within the same run, the engine:\\n+\\n+- Finds the last result for that step in the current run.\\n+- Reads the `sessionId` stored in the result.\\n+- Calls the AI provider again using `executeReviewWithSessionReuse` with that session id.\\n+\\n+Simple example (no transport wiring, just CLI/tests):\\n+\\n+```yaml\\n+version: \\\"2.0\\\"\\n+\\n+steps:\\n+  seed:\\n+    type: script\\n+    content: |\\n+      return { text: \\\"hello from seed\\\" };\\n+\\n+  convo:\\n+    type: ai\\n+    depends_on: [seed]\\n+    reuse_ai_session: self\\n+    session_mode: append\\n+    ai:\\n+      provider: mock\\n+      model: mock\\n+      disableTools: true\\n+      allowedTools: []\\n+      system_prompt: \\\"You are a tiny echo assistant.\\\"\\n+    prompt: |\\n+      Seed message: {{ outputs['seed'].text }}\\n+\\n+      Past convo outputs in this run:\\n+      {% assign hist = outputs_history['convo'] | default: empty %}\\n+      {% if hist and hist.size > 0 %}\\n+      {% for h in hist %}\\n+      - Previous reply {{ forloop.index }}.\\n+      {% endfor %}\\n+      {% else %}\\n+      - No previous replies yet.\\n+      {% endif %}\\n+    on_success:\\n+      goto_js: |\\n+        // Example: re‚Äëenter this step up to 3 times in a single run\\n+        return attempt < 3 ? 'convo' : null;\\n+```\\n+\\n+The corresponding testable example lives at:\\n+\\n+- `examples/session-reuse-self.yaml`\\n+\\n+This keeps the configuration small but shows how to wire `reuse_ai_session: self` and `session_mode: append` without touching higher‚Äëlevel workflows like `tyk-assistant`.\\n ```\\n \\n **When to use each mode:**\\n\",\"status\":\"modified\"},{\"filename\":\"docs/recipes.md\",\"additions\":149,\"deletions\":3,\"changes\":152,\"patch\":\"diff --git a/docs/recipes.md b/docs/recipes.md\\nindex 1015bc6c..1089e9a5 100644\\n--- a/docs/recipes.md\\n+++ b/docs/recipes.md\\n@@ -321,8 +321,154 @@ Tip: When you define a JSON Schema, you generally do **not** need to tell the mo\\n - Avoid noisy fallbacks like `(outputs['x']?.kind ?? '') === 'status'` when `outputs['x']?.kind === 'status'` is equivalent.\\n - These conventions apply uniformly to any provider (`ai`, `command`, `script`, `github`, `http_client`, etc).\\n \\n+### Command step best practices\\n+\\n+When using `type: command` steps:\\n+\\n+**Avoid external tool dependencies** like `jq`, `yq`, `python`, etc.:\\n+- They may not be installed in all environments (GitHub Actions, Docker, CI)\\n+- Use `transform_js` to parse and transform output instead\\n+- Keep shell commands simple: `grep`, `sed`, `awk`, `sort`, `head` are universally available\\n+\\n+```yaml\\n+# Bad - requires jq\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | jq -R -s 'split(\\\"\\\\n\\\")'\\n+  parseJson: true\\n+\\n+# Good - use transform_js for parsing\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | sort -u\\n+  transform_js: |\\n+    const lines = (output || '').trim().split('\\\\n').filter(Boolean);\\n+    return { data: lines, count: lines.length };\\n+```\\n+\\n+**Prefer line-separated output** over JSON from shell:\\n+- Simple to parse with `transform_js`\\n+- No need for `parseJson: true`\\n+- More robust across different shells/environments\\n+\\n+**Use transform_js for structured output**:\\n+- The sandbox provides `output` (command stdout as string)\\n+- Return an object with the fields you need\\n+- Works consistently across all environments\\n+\\n+### Testing workflows with `--no-mocks`\\n+\\n+The `--no-mocks` flag runs your test cases with real providers instead of injecting mock responses. This is essential for:\\n+\\n+1. **Debugging integration issues** - See actual API responses and errors\\n+2. **Capturing realistic mock data** - Get real output to copy into your test cases\\n+3. **Validating credentials** - Verify environment variables are set correctly\\n+4. **Developing new workflows** - Build tests incrementally with real data\\n+\\n+#### Basic usage\\n+\\n+```bash\\n+# Run all test cases with real providers\\n+visor test --config my-workflow.yaml --no-mocks\\n+\\n+# Run a specific test case with real providers\\n+visor test --config my-workflow.yaml --no-mocks --only \\\"my-test-case\\\"\\n+```\\n+\\n+#### Suggested mocks output\\n+\\n+When running with `--no-mocks`, Visor captures each step's output and prints it as YAML you can copy directly into your test case:\\n+\\n+```\\n+üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)\\n+   Step outputs will be captured and printed as suggested mocks\\n+\\n+... test execution ...\\n+\\n+üìã Suggested mocks (copy to your test case):\\n+mocks:\\n+  extract-keys:\\n+    data:\\n+      - PROJ-123\\n+      - DEV-456\\n+    count: 2\\n+  fetch-issues:\\n+    data:\\n+      - key: PROJ-123\\n+        summary: Fix authentication bug\\n+        status: In Progress\\n+```\\n+\\n+Copy the YAML under `mocks:` into your test case's `mocks:` section.\\n+\\n+#### Workflow for building tests\\n+\\n+1. **Start with a minimal test case** (no mocks):\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+   ```\\n+\\n+2. **Run with `--no-mocks`** to capture real outputs:\\n+   ```bash\\n+   visor test --config workflow.yaml --no-mocks --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+3. **Copy the suggested mocks** into your test case:\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+         mocks:\\n+           extract-keys:\\n+             data: [\\\"PROJ-123\\\"]\\n+             count: 1\\n+           # ... rest of captured mocks\\n+   ```\\n+\\n+4. **Add assertions** based on the real data:\\n+   ```yaml\\n+         expect:\\n+           workflow_output:\\n+             - path: issue_count\\n+               equals: 1\\n+   ```\\n+\\n+5. **Run normally** to verify mocks work:\\n+   ```bash\\n+   visor test --config workflow.yaml --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+#### Debugging with `--no-mocks`\\n+\\n+When a test fails with mocks, use `--no-mocks` to see what's actually happening:\\n+\\n+```bash\\n+# See real API responses and errors\\n+visor test --config workflow.yaml --no-mocks --only \\\"failing-test\\\"\\n+\\n+# Common issues revealed:\\n+# - Missing or expired credentials\\n+# - API endpoint changes\\n+# - Unexpected response formats\\n+# - Network/timeout issues\\n+```\\n+\\n+The real error messages and responses help identify whether the issue is with your mocks or the actual integration.\\n+\\n ### More examples\\n \\n-- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags  \\n-- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration  \\n+- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags\\n+- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration\\n - `examples/` ‚Äì MCP, Jira, and advanced configs\\n\",\"status\":\"modified\"},{\"filename\":\"package-lock.json\",\"additions\":252,\"deletions\":9,\"changes\":261,\"patch\":\"diff --git a/package-lock.json b/package-lock.json\\nindex 5fb3ad89..e5b10d27 100644\\n--- a/package-lock.json\\n+++ b/package-lock.json\\n@@ -7,6 +7,7 @@\\n     \\\"\\\": {\\n       \\\"name\\\": \\\"@probelabs/visor\\\",\\n       \\\"version\\\": \\\"0.1.42\\\",\\n+      \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"MIT\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@actions/core\\\": \\\"^1.11.1\\\",\\n@@ -16,7 +17,7 @@\\n         \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n         \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n         \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n         \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n         \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -54,6 +55,7 @@\\n         \\\"husky\\\": \\\"^9.1.7\\\",\\n         \\\"jest\\\": \\\"^30.1.3\\\",\\n         \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+        \\\"patch-package\\\": \\\"^8.0.1\\\",\\n         \\\"prettier\\\": \\\"^3.6.2\\\",\\n         \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n         \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n@@ -5942,9 +5944,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/maid\\\": {\\n-      \\\"version\\\": \\\"0.0.21\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.21.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-H3EA2tDjWsAPgXZAwfHrNoEXnHmQFS9+JGDK/YpeiF3g3FxPgnWBgtgYPgtNuETQsEXxR1RzDgQVJL2JHWRRHw==\\\",\\n+      \\\"version\\\": \\\"0.0.22\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.22.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-FYcyyP4BlVrHQmBznXFPpMbvUqFI6wbecHUUe4S+yIeDvJ8W4bP0ndZ0cMoc2p7hekWYAG4RtlohQBr8e4TKOg==\\\",\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@types/dagre\\\": \\\"^0.7.53\\\",\\n@@ -5960,9 +5962,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/probe\\\": {\\n-      \\\"version\\\": \\\"0.6.0-rc169\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc169.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-/laV3+1TSjASru8gGSvGwyjQ7jalMkFuZETl35uv37J1Hb4eG95CbKGU5NihcCrLXR44q1lKJiQGHXcAav2mBA==\\\",\\n+      \\\"version\\\": \\\"0.6.0-rc176\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc176.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-w/pAvnz2/yXb4FQXZV2QazxuW02xsI/wRg36yxZoLSG0O/hIncjF4rquZe3O+emceI2Ks5zbrA7cgQEdpwT8vA==\\\",\\n       \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n@@ -5972,7 +5974,7 @@\\n         \\\"@ai-sdk/openai\\\": \\\"^2.0.10\\\",\\n         \\\"@anthropic-ai/claude-agent-sdk\\\": \\\"^0.1.46\\\",\\n         \\\"@modelcontextprotocol/sdk\\\": \\\"^1.0.0\\\",\\n-        \\\"@probelabs/maid\\\": \\\"^0.0.21\\\",\\n+        \\\"@probelabs/maid\\\": \\\"^0.0.22\\\",\\n         \\\"adm-zip\\\": \\\"^0.5.16\\\",\\n         \\\"ai\\\": \\\"^5.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -8192,6 +8194,13 @@\\n         \\\"node\\\": \\\">= 20\\\"\\n       }\\n     },\\n+    \\\"node_modules/@yarnpkg/lockfile\\\": {\\n+      \\\"version\\\": \\\"1.1.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@yarnpkg/lockfile/-/lockfile-1.1.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-GpSwvyXOcOOlV70vbnzjj4fW5xW/FdUF6nQEt1ENy7m4ZCczi1+/buVUPAqmGfqznsORNFzUMjctTIp8a9tuCQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"BSD-2-Clause\\\"\\n+    },\\n     \\\"node_modules/accepts\\\": {\\n       \\\"version\\\": \\\"1.3.8\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/accepts/-/accepts-1.3.8.tgz\\\",\\n@@ -9180,6 +9189,25 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/call-bind\\\": {\\n+      \\\"version\\\": \\\"1.0.8\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind/-/call-bind-1.0.8.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-oKlSFMcMwpUg2ednkhQ454wfWiU/ul3CkJe/PEHcTKuiX6RpbehUiFMXu13HalGZxfUwCQzZG747YXBn1im9ww==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind-apply-helpers\\\": \\\"^1.0.0\\\",\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"set-function-length\\\": \\\"^1.2.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/call-bind-apply-helpers\\\": {\\n       \\\"version\\\": \\\"1.0.2\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind-apply-helpers/-/call-bind-apply-helpers-1.0.2.tgz\\\",\\n@@ -10613,6 +10641,24 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/define-data-property\\\": {\\n+      \\\"version\\\": \\\"1.1.4\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/define-data-property/-/define-data-property-1.1.4.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-rBMvIzlpA8v6E+SJZoo++HAYqsLrkg7MSfIinMPFhmkorw7X+dOXVJQs+QT69zGkzMyfDnIMN2Wid1+NbL3T+A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/define-lazy-prop\\\": {\\n       \\\"version\\\": \\\"3.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/define-lazy-prop/-/define-lazy-prop-3.0.0.tgz\\\",\\n@@ -11794,6 +11840,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/find-yarn-workspace-root\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/find-yarn-workspace-root/-/find-yarn-workspace-root-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-1IMnbjt4KzsQfnhnzNd8wUEgXZ44IzZaZmnLYx7D5FZlaHt2gW20Cri8Q+E/t5tIj4+epTBub+2Zxu/vNILzqQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Apache-2.0\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"micromatch\\\": \\\"^4.0.2\\\"\\n+      }\\n+    },\\n     \\\"node_modules/fix-dts-default-cjs-exports\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/fix-dts-default-cjs-exports/-/fix-dts-default-cjs-exports-1.0.1.tgz\\\",\\n@@ -12267,6 +12323,19 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/has-property-descriptors\\\": {\\n+      \\\"version\\\": \\\"1.0.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/has-property-descriptors/-/has-property-descriptors-1.0.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-55JNKuIW+vq4Ke1BjOTjM2YctQIvCT7GFzHwmfZPGo5wnrgkid0YQtnAleFSqumZm4az3n2BS+erby5ipJdgrg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/has-symbols\\\": {\\n       \\\"version\\\": \\\"1.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/has-symbols/-/has-symbols-1.1.0.tgz\\\",\\n@@ -12784,6 +12853,13 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/isarray\\\": {\\n+      \\\"version\\\": \\\"2.0.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/isarray/-/isarray-2.0.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-xHjhDr3cNBK0BzdUJSPXZntQUx/mwMS5Rw4A7lPJ90XGAO6ISP/ePDNuo0vhqOZU+UD5JoodwCAAoZQd3FeAKw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\"\\n+    },\\n     \\\"node_modules/isexe\\\": {\\n       \\\"version\\\": \\\"2.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz\\\",\\n@@ -13594,6 +13670,26 @@\\n       \\\"integrity\\\": \\\"sha512-NM8/P9n3XjXhIZn1lLhkFaACTOURQXjWhV4BA/RnOv8xvgqtqpAX9IO4mRQxSx1Rlo4tqzeqb0sOlruaOy3dug==\\\",\\n       \\\"license\\\": \\\"MIT\\\"\\n     },\\n+    \\\"node_modules/json-stable-stringify\\\": {\\n+      \\\"version\\\": \\\"1.3.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify/-/json-stable-stringify-1.3.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-qtYiSSFlwot9XHtF9bD9c7rwKjr+RecWT//ZnPvSmEjpV5mmPOCN4j8UjY5hbjNkOwZ/jQv3J6R1/pL7RwgMsg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind\\\": \\\"^1.0.8\\\",\\n+        \\\"call-bound\\\": \\\"^1.0.4\\\",\\n+        \\\"isarray\\\": \\\"^2.0.5\\\",\\n+        \\\"jsonify\\\": \\\"^0.0.1\\\",\\n+        \\\"object-keys\\\": \\\"^1.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/json-stable-stringify-without-jsonify\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify-without-jsonify/-/json-stable-stringify-without-jsonify-1.0.1.tgz\\\",\\n@@ -13633,6 +13729,16 @@\\n         \\\"graceful-fs\\\": \\\"^4.1.6\\\"\\n       }\\n     },\\n+    \\\"node_modules/jsonify\\\": {\\n+      \\\"version\\\": \\\"0.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/jsonify/-/jsonify-0.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-2/Ki0GcmuqSrgFyelQq9M05y7PS0mEwuIzrf3f1fPqkVDVRvZrPZtVSMHxdgo8Aq0sxAOb/cr2aqqA3LeWHVPg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Public Domain\\\",\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/katex\\\": {\\n       \\\"version\\\": \\\"0.16.25\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/katex/-/katex-0.16.25.tgz\\\",\\n@@ -13676,6 +13782,16 @@\\n       \\\"integrity\\\": \\\"sha512-Ls993zuzfayK269Svk9hzpeGUKob/sIgZzyHYdjQoAdQetRKpOLj+k/QQQ/6Qi0Yz65mlROrfd+Ev+1+7dz9Kw==\\\",\\n       \\\"dev\\\": true\\n     },\\n+    \\\"node_modules/klaw-sync\\\": {\\n+      \\\"version\\\": \\\"6.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/klaw-sync/-/klaw-sync-6.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-nIeuVSzdCCs6TDPTqI8w1Yre34sSq7AkZ4B3sfOBbI2CgVSB4Du4aLQijFU2+lhAFCwt9+42Hel6lQNIv6AntQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"graceful-fs\\\": \\\"^4.1.11\\\"\\n+      }\\n+    },\\n     \\\"node_modules/kolorist\\\": {\\n       \\\"version\\\": \\\"1.8.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/kolorist/-/kolorist-1.8.0.tgz\\\",\\n@@ -14785,6 +14901,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n       }\\n     },\\n+    \\\"node_modules/object-keys\\\": {\\n+      \\\"version\\\": \\\"1.1.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/object-keys/-/object-keys-1.1.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NuAESUOUMrlIXOfHKzD6bpPu3tYt3xvjNdRIQ+FeT0lNb4K8WR70CaDxhuNguS2XG+GjkyMwOzsN5ZktImfhLA==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/ohash\\\": {\\n       \\\"version\\\": \\\"2.0.11\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/ohash/-/ohash-2.0.11.tgz\\\",\\n@@ -15022,6 +15148,95 @@\\n         \\\"node\\\": \\\">= 0.8\\\"\\n       }\\n     },\\n+    \\\"node_modules/patch-package\\\": {\\n+      \\\"version\\\": \\\"8.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/patch-package/-/patch-package-8.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-VsKRIA8f5uqHQ7NGhwIna6Bx6D9s/1iXlA1hthBVBEbkq+t4kXD0HHt+rJhf/Z+Ci0F/HCB2hvn0qLdLG+Qxlw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"@yarnpkg/lockfile\\\": \\\"^1.1.0\\\",\\n+        \\\"chalk\\\": \\\"^4.1.2\\\",\\n+        \\\"ci-info\\\": \\\"^3.7.0\\\",\\n+        \\\"cross-spawn\\\": \\\"^7.0.3\\\",\\n+        \\\"find-yarn-workspace-root\\\": \\\"^2.0.0\\\",\\n+        \\\"fs-extra\\\": \\\"^10.0.0\\\",\\n+        \\\"json-stable-stringify\\\": \\\"^1.0.2\\\",\\n+        \\\"klaw-sync\\\": \\\"^6.0.0\\\",\\n+        \\\"minimist\\\": \\\"^1.2.6\\\",\\n+        \\\"open\\\": \\\"^7.4.2\\\",\\n+        \\\"semver\\\": \\\"^7.5.3\\\",\\n+        \\\"slash\\\": \\\"^2.0.0\\\",\\n+        \\\"tmp\\\": \\\"^0.2.4\\\",\\n+        \\\"yaml\\\": \\\"^2.2.2\\\"\\n+      },\\n+      \\\"bin\\\": {\\n+        \\\"patch-package\\\": \\\"index.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14\\\",\\n+        \\\"npm\\\": \\\">5\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/ci-info\\\": {\\n+      \\\"version\\\": \\\"3.9.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/ci-info/-/ci-info-3.9.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NIxF55hv4nSqQswkAeiOi1r83xy8JldOFDTWiug55KBu9Jnblncd2U6ViHmYgHf01TPZS77NJBhBMKdWj9HQMQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"funding\\\": [\\n+        {\\n+          \\\"type\\\": \\\"github\\\",\\n+          \\\"url\\\": \\\"https://github.com/sponsors/sibiraj-s\\\"\\n+        }\\n+      ],\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/is-docker\\\": {\\n+      \\\"version\\\": \\\"2.2.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/is-docker/-/is-docker-2.2.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-F+i2BKsFrH66iaUFc0woD8sLy8getkwTwtOBjvs56Cx4CgJDeKQeqfz8wAYiSb8JOprWhHH5p77PbmYCvvUuXQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"bin\\\": {\\n+        \\\"is-docker\\\": \\\"cli.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/open\\\": {\\n+      \\\"version\\\": \\\"7.4.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/open/-/open-7.4.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-MVHddDVweXZF3awtlAS+6pgKLlm/JgxZ90+/NBurBoQctVOOB/zDdVjcyPzQ+0laDGbsWgrRkflI65sQeOgT9Q==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"is-docker\\\": \\\"^2.0.0\\\",\\n+        \\\"is-wsl\\\": \\\"^2.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/slash\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/slash/-/slash-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-ZYKh3Wh2z1PpEXWr0MpSBZ0V6mZHAQfYevttO11c51CaWjGTaadiKZ+wVt1PbMlDV5qhMFslpZCemhwOK7C89A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=6\\\"\\n+      }\\n+    },\\n     \\\"node_modules/path-data-parser\\\": {\\n       \\\"version\\\": \\\"0.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/path-data-parser/-/path-data-parser-0.1.0.tgz\\\",\\n@@ -16473,6 +16688,24 @@\\n         \\\"node\\\": \\\">= 0.8.0\\\"\\n       }\\n     },\\n+    \\\"node_modules/set-function-length\\\": {\\n+      \\\"version\\\": \\\"1.2.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/set-function-length/-/set-function-length-1.2.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-pgRc4hJ4/sNjWCSS9AmnS40x3bNMDTknHgL5UaMBTMyJnU90EgWh1Rz+MC9eFu4BuN/UwZjKQuY/1v3rM7HMfg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"define-data-property\\\": \\\"^1.1.4\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"function-bind\\\": \\\"^1.1.2\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\",\\n+        \\\"has-property-descriptors\\\": \\\"^1.0.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/setprototypeof\\\": {\\n       \\\"version\\\": \\\"1.2.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/setprototypeof/-/setprototypeof-1.2.0.tgz\\\",\\n@@ -17384,6 +17617,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/tmp\\\": {\\n+      \\\"version\\\": \\\"0.2.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/tmp/-/tmp-0.2.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-voyz6MApa1rQGUxT3E+BK7/ROe8itEx7vD8/HEvt4xwXucvQ5G5oeEiHkmHZJuBO21RpOf+YYm9MOivj709jow==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14.14\\\"\\n+      }\\n+    },\\n     \\\"node_modules/tmpl\\\": {\\n       \\\"version\\\": \\\"1.0.5\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/tmpl/-/tmpl-1.0.5.tgz\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"package.json\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/package.json b/package.json\\nindex f9ccbb6c..302d728f 100644\\n--- a/package.json\\n+++ b/package.json\\n@@ -49,6 +49,7 @@\\n     \\\"test:with-build\\\": \\\"npm run build:cli && jest\\\",\\n     \\\"test:yaml\\\": \\\"node dist/index.js test --progress compact\\\",\\n     \\\"prepare\\\": \\\"husky\\\",\\n+    \\\"postinstall\\\": \\\"patch-package\\\",\\n     \\\"pre-commit\\\": \\\"lint-staged\\\",\\n     \\\"deploy:site\\\": \\\"cd site && npx wrangler pages deploy . --project-name=visor-site --commit-dirty=true\\\",\\n     \\\"deploy:worker\\\": \\\"npx wrangler deploy\\\",\\n@@ -96,7 +97,7 @@\\n     \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n     \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n     \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n     \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n     \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n     \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -143,6 +144,7 @@\\n     \\\"husky\\\": \\\"^9.1.7\\\",\\n     \\\"jest\\\": \\\"^30.1.3\\\",\\n     \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+    \\\"patch-package\\\": \\\"^8.0.1\\\",\\n     \\\"prettier\\\": \\\"^3.6.2\\\",\\n     \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n     \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"src/ai-review-service.ts\",\"additions\":101,\"deletions\":151,\"changes\":252,\"patch\":\"diff --git a/src/ai-review-service.ts b/src/ai-review-service.ts\\nindex 6b9c5b2e..f18accea 100644\\n--- a/src/ai-review-service.ts\\n+++ b/src/ai-review-service.ts\\n@@ -65,6 +65,13 @@ export interface AIReviewConfig {\\n   allowBash?: boolean;\\n   // Advanced bash command execution configuration\\n   bashConfig?: import('./types/config').BashConfig;\\n+  // Optional workspace root and allowed folders for ProbeAgent.\\n+  // When provided, these are forwarded to ProbeAgent so tools like search/query\\n+  // operate inside the isolated workspace/projects instead of the Visor repo root.\\n+  path?: string;\\n+  allowedFolders?: string[];\\n+  // Completion prompt for post-completion validation/review (runs after attempt_completion)\\n+  completionPrompt?: string;\\n }\\n \\n export interface AIDebugInfo {\\n@@ -278,7 +285,7 @@ export class AIReviewService {\\n     try {\\n       const call = this.callProbeAgent(prompt, schema, debugInfo, checkName, sessionId);\\n       const timeoutMs = Math.max(0, this.config.timeout || 0);\\n-      const { response, effectiveSchema } =\\n+      const { response, effectiveSchema, sessionId: usedSessionId } =\\n         timeoutMs > 0 ? await this.withTimeout(call, timeoutMs, 'AI review') : await call;\\n       const processingTime = Date.now() - startTime;\\n \\n@@ -290,6 +297,11 @@ export class AIReviewService {\\n \\n       const result = this.parseAIResponse(response, debugInfo, effectiveSchema);\\n \\n+      // Expose the session ID used for this call so the engine can reuse it later\\n+      try {\\n+        (result as any).sessionId = usedSessionId;\\n+      } catch {}\\n+\\n       if (debugInfo) {\\n         result.debug = debugInfo;\\n       }\\n@@ -651,17 +663,34 @@ ${prContext}${slackContextXml}\\n     const prContextInfo = prInfo as PRInfo & {\\n       isPRContext?: boolean;\\n       includeCodeContext?: boolean;\\n+      slackConversation?: unknown;\\n     };\\n     const isIssue = prContextInfo.isIssue === true;\\n \\n     // Check if we should include code context (diffs)\\n     const isPRContext = prContextInfo.isPRContext === true;\\n-    // In PR context, always include diffs. Otherwise check the flag.\\n-    const includeCodeContext = isPRContext || prContextInfo.includeCodeContext !== false;\\n+    const isSlackMode = prContextInfo.slackConversation !== undefined;\\n+\\n+    // Determine whether to include code context:\\n+    // - In explicit PR context (GitHub PR events), always include diffs\\n+    // - In Slack mode, default to NO code context unless explicitly requested\\n+    // - Otherwise, include code context unless explicitly disabled\\n+    let includeCodeContext: boolean;\\n+    if (isPRContext) {\\n+      includeCodeContext = true;\\n+    } else if (isSlackMode) {\\n+      // In Slack mode, only include code context if explicitly set to true\\n+      includeCodeContext = prContextInfo.includeCodeContext === true;\\n+    } else {\\n+      // Default: include unless explicitly disabled\\n+      includeCodeContext = prContextInfo.includeCodeContext !== false;\\n+    }\\n \\n     // Log the decision for transparency (debug level)\\n     if (isPRContext) {\\n       log('üîç Including full code diffs in AI context (PR mode)');\\n+    } else if (isSlackMode && !includeCodeContext) {\\n+      log('üí¨ Slack mode: excluding code diffs (use includeCodeContext: true to enable)');\\n     } else if (!includeCodeContext) {\\n       log('üìä Including only file summary in AI context (no diffs)');\\n     } else {\\n@@ -1537,7 +1566,15 @@ ${'='.repeat(60)}\\n     debugInfo?: AIDebugInfo,\\n     _checkName?: string,\\n     providedSessionId?: string\\n-  ): Promise<{ response: string; effectiveSchema?: string }> {\\n+  ): Promise<{ response: string; effectiveSchema?: string; sessionId: string }> {\\n+    // Derive a stable session ID for this call so the engine can reuse it later\\n+    const sessionId =\\n+      providedSessionId ||\\n+      (() => {\\n+        const timestamp = new Date().toISOString();\\n+        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n+      })();\\n+\\n     // Handle mock model/provider\\n     if (this.config.model === 'mock' || this.config.provider === 'mock') {\\n       const inJest = !!process.env.JEST_WORKER_ID;\\n@@ -1545,19 +1582,17 @@ ${'='.repeat(60)}\\n       if (!inJest) {\\n         // Fast path for CLI/integration: synthesize a mock response without invoking ProbeAgent\\n         const response = await this.generateMockResponse(prompt, _checkName, schema);\\n-        return { response, effectiveSchema: typeof schema === 'object' ? 'custom' : schema };\\n+        return {\\n+          response,\\n+          effectiveSchema: typeof schema === 'object' ? 'custom' : schema,\\n+          sessionId,\\n+        };\\n       }\\n       // In unit tests, still invoke ProbeAgent so tests can assert on options (schema) passed in\\n       // Fall through to normal flow below\\n     }\\n \\n     // Create ProbeAgent instance with proper options\\n-    const sessionId =\\n-      providedSessionId ||\\n-      (() => {\\n-        const timestamp = new Date().toISOString();\\n-        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n-      })();\\n \\n     log('ü§ñ Creating ProbeAgent for AI review...');\\n     log(`üÜî Session ID: ${sessionId}`);\\n@@ -1674,6 +1709,35 @@ ${'='.repeat(60)}\\n         (options as any).bashConfig = this.config.bashConfig;\\n       }\\n \\n+      // Pass completion prompt for post-completion validation/review\\n+      if (this.config.completionPrompt !== undefined) {\\n+        (options as any).completionPrompt = this.config.completionPrompt;\\n+      }\\n+\\n+      // Propagate workspace / allowed folders to ProbeAgent so that tools\\n+      // operate inside the isolated workspace and project checkouts instead\\n+      // of the Visor repository root.\\n+      try {\\n+        const cfgAny: any = this.config as any;\\n+        const allowedFolders = cfgAny.allowedFolders as string[] | undefined;\\n+        const workspacePath =\\n+          cfgAny.workspacePath || cfgAny.path || (Array.isArray(allowedFolders) && allowedFolders[0]);\\n+        if (Array.isArray(allowedFolders) && allowedFolders.length > 0) {\\n+          (options as any).allowedFolders = allowedFolders;\\n+          if (!options.path && workspacePath) {\\n+            (options as any).path = workspacePath;\\n+          }\\n+          log(`üóÇÔ∏è ProbeAgent workspace config:`);\\n+          log(`   path (cwd): ${(options as any).path}`);\\n+          log(`   allowedFolders[0]: ${allowedFolders[0]}`);\\n+        } else if (workspacePath) {\\n+          (options as any).path = workspacePath;\\n+          log(`üóÇÔ∏è ProbeAgent path: ${workspacePath} (no allowedFolders)`);\\n+        }\\n+      } catch {\\n+        // Best-effort only; fall back to ProbeAgent defaults on error.\\n+      }\\n+\\n       // Add provider-specific options if configured\\n       if (this.config.provider) {\\n         // Map claude-code to anthropic for ProbeAgent compatibility\\n@@ -2037,7 +2101,7 @@ ${'='.repeat(60)}\\n         log(`üîß Debug: Registered AI session for potential reuse: ${sessionId}`);\\n       }\\n \\n-      return { response, effectiveSchema };\\n+      return { response, effectiveSchema, sessionId };\\n     } catch (error) {\\n       console.error('‚ùå ProbeAgent failed:', error);\\n       throw new Error(\\n@@ -2191,13 +2255,23 @@ ${'='.repeat(60)}\\n         // For other schemas (code-review, etc.), extract and parse JSON with boundary detection\\n         log('üîç Extracting JSON from AI response...');\\n \\n-        // Try direct parsing first - if AI returned pure JSON\\n+        // Sanitize response: strip BOM, zero-width chars, and other invisible characters\\n+        // that can cause JSON parsing to fail even when the text looks valid\\n+        const sanitizedResponse = response\\n+          .replace(/^\\\\uFEFF/, '') // BOM\\n+          .replace(/[\\\\u200B-\\\\u200D\\\\uFEFF\\\\u00A0]/g, '') // Zero-width chars, NBSP\\n+          .replace(/[\\\\x00-\\\\x08\\\\x0B\\\\x0C\\\\x0E-\\\\x1F\\\\x7F]/g, '') // Control chars (except \\\\t \\\\n \\\\r)\\n+          .trim();\\n+\\n+        // Try direct JSON parsing - no bracket-matching extraction\\n+        // JSON validation is offloaded to Probe agent when schema is provided\\n         try {\\n-          reviewData = JSON.parse(response.trim());\\n+          reviewData = JSON.parse(sanitizedResponse);\\n           log('‚úÖ Successfully parsed direct JSON response');\\n           if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-        } catch {\\n-          log('üîç Direct parsing failed, trying to extract JSON from response...');\\n+        } catch (parseErr) {\\n+          const errMsg = parseErr instanceof Error ? parseErr.message : String(parseErr);\\n+          log(`üîç Direct JSON parsing failed: ${errMsg}`);\\n \\n           // If the response starts with \\\"I cannot\\\" or similar, it's likely a refusal\\n           if (\\n@@ -2210,66 +2284,16 @@ ${'='.repeat(60)}\\n             };\\n           }\\n \\n-          // Try to extract JSON using improved method with proper bracket matching\\n-          const jsonString = this.extractJsonFromResponse(response);\\n-\\n-          if (jsonString) {\\n-            try {\\n-              reviewData = JSON.parse(jsonString);\\n-              log('‚úÖ Successfully parsed extracted JSON');\\n-              if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-            } catch {\\n-              log('üîß Extracted JSON parsing failed, falling back to plain text handling...');\\n-\\n-              // Check if response is plain text and doesn't contain structured data\\n-              if (!response.includes('{') && !response.includes('}')) {\\n-                log('üîß Plain text response detected, creating structured fallback...');\\n-\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              } else {\\n-                // Fallback: treat the entire response as an issue\\n-                log('üîß Creating fallback response from non-JSON content...');\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              }\\n-            }\\n-          } else {\\n-            // No JSON found at all - treat as plain text response\\n-            log('üîß No JSON found in response, treating as plain text...');\\n-            reviewData = {\\n-              issues: [\\n-                {\\n-                  file: 'AI_RESPONSE',\\n-                  line: 1,\\n-                  ruleId: 'ai/raw_response',\\n-                  message: response,\\n-                  severity: 'info',\\n-                  category: 'documentation',\\n-                },\\n-              ],\\n-            };\\n-          }\\n+          // Not valid JSON - treat entire response as text output\\n+          // This allows Probe (or other AI providers) to handle JSON validation\\n+          // and avoids false positives from bracket-matching (e.g., mermaid diagrams)\\n+          log('üîß Treating response as plain text (no JSON extraction)');\\n+          const trimmed = response.trim();\\n+          return {\\n+            issues: [],\\n+            output: { text: trimmed },\\n+            debug: debugInfo,\\n+          };\\n         }\\n       }\\n \\n@@ -2457,80 +2481,6 @@ ${'='.repeat(60)}\\n     }\\n   }\\n \\n-  /**\\n-   * Extract JSON from a response that might contain surrounding text\\n-   * Uses proper bracket matching to find valid JSON objects or arrays\\n-   */\\n-  private extractJsonFromResponse(response: string): string | null {\\n-    const text = response.trim();\\n-\\n-    // Try to find JSON objects first (higher priority)\\n-    let bestJson = this.findJsonWithBracketMatching(text, '{', '}');\\n-\\n-    // If no object found, try arrays\\n-    if (!bestJson) {\\n-      bestJson = this.findJsonWithBracketMatching(text, '[', ']');\\n-    }\\n-\\n-    return bestJson;\\n-  }\\n-\\n-  /**\\n-   * Find JSON with proper bracket matching to avoid false positives\\n-   */\\n-  private findJsonWithBracketMatching(\\n-    text: string,\\n-    openChar: string,\\n-    closeChar: string\\n-  ): string | null {\\n-    const firstIndex = text.indexOf(openChar);\\n-    if (firstIndex === -1) return null;\\n-\\n-    let depth = 0;\\n-    let inString = false;\\n-    let escaping = false;\\n-\\n-    for (let i = firstIndex; i < text.length; i++) {\\n-      const char = text[i];\\n-\\n-      if (escaping) {\\n-        escaping = false;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\\\\\\' && inString) {\\n-        escaping = true;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\"' && !escaping) {\\n-        inString = !inString;\\n-        continue;\\n-      }\\n-\\n-      if (!inString) {\\n-        if (char === openChar) {\\n-          depth++;\\n-        } else if (char === closeChar) {\\n-          depth--;\\n-          if (depth === 0) {\\n-            // Found matching closing bracket\\n-            const candidate = text.substring(firstIndex, i + 1);\\n-            try {\\n-              JSON.parse(candidate); // Validate it's actually valid JSON\\n-              return candidate;\\n-            } catch {\\n-              // This wasn't valid JSON, keep looking\\n-              break;\\n-            }\\n-          }\\n-        }\\n-      }\\n-    }\\n-\\n-    return null;\\n-  }\\n-\\n   /**\\n    * Generate mock response for testing\\n    */\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli-main.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/src/cli-main.ts b/src/cli-main.ts\\nindex 7dbef501..e671fbd8 100644\\n--- a/src/cli-main.ts\\n+++ b/src/cli-main.ts\\n@@ -158,6 +158,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n   }\\n   const only = getArg('--only');\\n   const bail = hasFlag('--bail');\\n+  const noMocks = hasFlag('--no-mocks');\\n   const listOnly = hasFlag('--list');\\n   const validateOnly = hasFlag('--validate');\\n   const progress = (getArg('--progress') as 'compact' | 'detailed' | undefined) || 'compact';\\n@@ -224,6 +225,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       const agg = await runSuites(multiFiles, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallelSuites: maxParallelSuites || Math.max(1, require('os').cpus()?.length || 2),\\n         maxParallel,\\n         promptMaxChars,\\n@@ -340,6 +342,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       runRes = await runner.runCases(tpath, suite, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallel,\\n         promptMaxChars,\\n         engineMode: 'state-machine',\\n@@ -609,6 +612,9 @@ export async function main(): Promise<void> {\\n     // Set environment variables early for proper logging in all modules\\n     process.env.VISOR_OUTPUT_FORMAT = options.output;\\n     process.env.VISOR_DEBUG = options.debug ? 'true' : 'false';\\n+    if (options.keepWorkspace) {\\n+      process.env.VISOR_KEEP_WORKSPACE = 'true';\\n+    }\\n     // Configure centralized logger\\n     configureLoggerFromCli({\\n       output: options.output,\\n@@ -716,6 +722,31 @@ export async function main(): Promise<void> {\\n       const threads = slackAny.threads || 'any';\\n       const allow = Array.isArray(slackAny.channel_allowlist) ? slackAny.channel_allowlist : [];\\n       const appToken = slackAny.app_token || process.env.SLACK_APP_TOKEN;\\n+\\n+      // Initialize telemetry for Slack mode (normally done later for CLI runs).\\n+      if ((config as any)?.telemetry) {\\n+        const t = (config as any).telemetry as {\\n+          enabled?: boolean;\\n+          sink?: 'otlp' | 'file' | 'console';\\n+          file?: { dir?: string; ndjson?: boolean };\\n+          tracing?: { auto_instrumentations?: boolean; trace_report?: { enabled?: boolean } };\\n+        };\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true' || !!t?.enabled,\\n+          sink:\\n+            (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || t?.sink || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR || t?.file?.dir, ndjson: !!t?.file?.ndjson },\\n+          autoInstrument: !!t?.tracing?.auto_instrumentations,\\n+          traceReport: !!t?.tracing?.trace_report?.enabled,\\n+        });\\n+      } else {\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true',\\n+          sink: (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR },\\n+        });\\n+      }\\n+\\n       const runner = new SlackSocketRunner(engine, config, {\\n         appToken,\\n         endpoint,\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli.ts\",\"additions\":5,\"deletions\":1,\"changes\":6,\"patch\":\"diff --git a/src/cli.ts b/src/cli.ts\\nindex 134429b2..b1c46213 100644\\n--- a/src/cli.ts\\n+++ b/src/cli.ts\\n@@ -76,6 +76,7 @@ export class CLI {\\n         parseInt(value, 10)\\n       )\\n       .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText())\\n       .exitOverride(); // Prevent automatic process.exit for better error handling\\n \\n@@ -154,6 +155,7 @@ export class CLI {\\n           parseInt(value, 10)\\n         )\\n         .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+        .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n         .allowUnknownOption(false)\\n         .allowExcessArguments(false) // Don't allow positional arguments\\n         .addHelpText('after', this.getExamplesText())\\n@@ -224,6 +226,7 @@ export class CLI {\\n         message: options.message,\\n         githubV2: false,\\n         slack: Boolean(options.slack),\\n+        keepWorkspace: Boolean(options.keepWorkspace),\\n       };\\n     } catch (error: unknown) {\\n       // Handle commander.js exit overrides for help/version ONLY\\n@@ -348,6 +351,7 @@ export class CLI {\\n       .option('--debug-port <port>', 'Port for debug server (default: 3456)', value =>\\n         parseInt(value, 10)\\n       )\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText());\\n \\n     // Get the basic help and append examples manually if addHelpText doesn't work\\n\",\"status\":\"modified\"},{\"filename\":\"src/config.ts\",\"additions\":67,\"deletions\":17,\"changes\":84,\"patch\":\"diff --git a/src/config.ts b/src/config.ts\\nindex 407efa2d..02abe9df 100644\\n--- a/src/config.ts\\n+++ b/src/config.ts\\n@@ -20,6 +20,7 @@ import { ConfigLoader, ConfigLoaderOptions } from './utils/config-loader';\\n import { ConfigMerger } from './utils/config-merger';\\n import Ajv from 'ajv';\\n import addFormats from 'ajv-formats';\\n+import { validateJsSyntax } from './utils/sandbox';\\n \\n /**\\n  * Valid event triggers for checks\\n@@ -477,16 +478,29 @@ export class ConfigManager {\\n \\n     logger.info(`Registered workflow '${workflowId}' for standalone execution`);\\n \\n-    // Create a COMPLETELY NEW visor config with ONLY the test checks\\n-    // This prevents any workflow fields from leaking into the config\\n-    const visorConfig: Partial<VisorConfig> = {\\n+    // For standalone workflow testing, use the workflow's steps as checks\\n+    // This allows the test runner to execute the workflow steps directly\\n+    const workflowSteps = workflowData.steps || {};\\n+\\n+    // Create a config that includes the workflow steps as checks,\\n+    // plus the tests section and workflow metadata for output computation\\n+    const visorConfig: Partial<VisorConfig> & { tests?: any; outputs?: any; inputs?: any } = {\\n       version: '1.0',\\n-      steps: tests,\\n-      checks: tests, // Backward compatibility\\n+      steps: workflowSteps,\\n+      checks: workflowSteps,\\n+      tests: tests, // Preserve test harness config (may be empty if stripped by test runner)\\n     };\\n \\n-    logger.debug(`Standalone workflow config has ${Object.keys(tests).length} test checks`);\\n-    logger.debug(`Test check names: ${Object.keys(tests).join(', ')}`);\\n+    // Preserve workflow metadata for output computation during tests\\n+    if (workflowData.outputs) {\\n+      visorConfig.outputs = workflowData.outputs;\\n+    }\\n+    if (workflowData.inputs) {\\n+      visorConfig.inputs = workflowData.inputs;\\n+    }\\n+\\n+    logger.debug(`Standalone workflow config has ${Object.keys(workflowSteps).length} workflow steps as checks`);\\n+    logger.debug(`Workflow step names: ${Object.keys(workflowSteps).join(', ')}`);\\n     logger.debug(`Config keys after conversion: ${Object.keys(visorConfig).join(', ')}`);\\n \\n     return visorConfig;\\n@@ -982,26 +996,29 @@ export class ConfigManager {\\n \\n     // Validate reuse_ai_session configuration\\n     if (checkConfig.reuse_ai_session !== undefined) {\\n-      const isString = typeof checkConfig.reuse_ai_session === 'string';\\n-      const isBoolean = typeof checkConfig.reuse_ai_session === 'boolean';\\n+      const reuseValue = checkConfig.reuse_ai_session as unknown;\\n+      const isString = typeof reuseValue === 'string';\\n+      const isBoolean = typeof reuseValue === 'boolean';\\n+      const isSelf = reuseValue === 'self';\\n \\n       if (!isString && !isBoolean) {\\n         errors.push({\\n           field: `checks.${checkName}.reuse_ai_session`,\\n           message: `Invalid reuse_ai_session value for \\\"${checkName}\\\": must be string (check name) or boolean`,\\n-          value: checkConfig.reuse_ai_session,\\n+          value: reuseValue,\\n         });\\n-      } else if (isString) {\\n-        // When reuse_ai_session is a string, it must refer to a valid check\\n-        const targetCheckName = checkConfig.reuse_ai_session as string;\\n+      } else if (isString && !isSelf) {\\n+        // When reuse_ai_session is a string (other than the special 'self' value),\\n+        // it must refer to a valid check name\\n+        const targetCheckName = reuseValue as string;\\n         if (!config?.checks || !config.checks[targetCheckName]) {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" references non-existent check \\\"${targetCheckName}\\\" for session reuse`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n-      } else if (checkConfig.reuse_ai_session === true) {\\n+      } else if (reuseValue === true) {\\n         // When reuse_ai_session is true, depends_on must be specified and non-empty\\n         if (\\n           !checkConfig.depends_on ||\\n@@ -1011,7 +1028,7 @@ export class ConfigManager {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" has reuse_ai_session=true but missing or empty depends_on. Session reuse requires dependency on another check.`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n       }\\n@@ -1076,6 +1093,39 @@ export class ConfigManager {\\n         });\\n       }\\n     }\\n+\\n+    // Validate JavaScript syntax in transform_js and script content\\n+    try {\\n+      // Validate transform_js if present\\n+      const transformJs = (checkConfig as any).transform_js;\\n+      if (typeof transformJs === 'string' && transformJs.trim().length > 0) {\\n+        const result = validateJsSyntax(transformJs);\\n+        if (!result.valid) {\\n+          errors.push({\\n+            field: `checks.${checkName}.transform_js`,\\n+            message: `JavaScript syntax error in \\\"${checkName}\\\" transform_js: ${result.error}`,\\n+            value: transformJs.slice(0, 100) + (transformJs.length > 100 ? '...' : ''),\\n+          });\\n+        }\\n+      }\\n+\\n+      // Validate script content if type is 'script'\\n+      if (checkConfig.type === 'script') {\\n+        const content = (checkConfig as any).content;\\n+        if (typeof content === 'string' && content.trim().length > 0) {\\n+          const result = validateJsSyntax(content);\\n+          if (!result.valid) {\\n+            errors.push({\\n+              field: `checks.${checkName}.content`,\\n+              message: `JavaScript syntax error in \\\"${checkName}\\\" script: ${result.error}`,\\n+              value: content.slice(0, 100) + (content.length > 100 ? '...' : ''),\\n+            });\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Syntax validation is best-effort; don't fail the whole config on validation errors\\n+    }\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/failure-condition-evaluator.ts\",\"additions\":25,\"deletions\":9,\"changes\":34,\"patch\":\"diff --git a/src/failure-condition-evaluator.ts b/src/failure-condition-evaluator.ts\\nindex 3b54a015..5800df0a 100644\\n--- a/src/failure-condition-evaluator.ts\\n+++ b/src/failure-condition-evaluator.ts\\n@@ -137,6 +137,8 @@ export class FailureConditionEvaluator {\\n       previousResults?: Map<string, ReviewSummary>;\\n       authorAssociation?: string;\\n       workflowInputs?: Record<string, unknown>;\\n+      /** Current step's output for guarantee evaluation */\\n+      output?: unknown;\\n     }\\n   ): Promise<boolean> {\\n     // Build context for if evaluation\\n@@ -177,10 +179,10 @@ export class FailureConditionEvaluator {\\n       // Workflow inputs (for workflows)\\n       inputs: contextData?.workflowInputs || {},\\n \\n-      // Required output property (empty for if conditions)\\n-      output: {\\n-        issues: [],\\n-      },\\n+      // Output property: use provided output for guarantee evaluation, or empty for if conditions\\n+      output: contextData?.output !== undefined && contextData.output !== null && typeof contextData.output === 'object'\\n+        ? (contextData.output as Record<string, unknown>)\\n+        : { issues: [] },\\n       // Author association (used by permission helpers)\\n       authorAssociation: contextData?.authorAssociation,\\n \\n@@ -213,10 +215,10 @@ export class FailureConditionEvaluator {\\n               m && typeof m.get === 'function' ? m.get('all_valid', 'fact-validation') : undefined;\\n             memStr = ` mem.fact-validation.all_valid=${String(v)}`;\\n           } catch {}\\n+          // Debug if-eval output (only when VISOR_DEBUG enabled)\\n+          const outputKeys = Object.keys(context.outputs || {});\\n           console.error(\\n-            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" env.ENABLE_FACT_VALIDATION=${String(\\n-              (envMap as any).ENABLE_FACT_VALIDATION\\n-            )} event=${context.event?.event_name} result=${String(res)}${memStr}`\\n+            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" result=${String(res)} outputKeys=[${outputKeys.join(',')}]`\\n           );\\n         }\\n       } catch {}\\n@@ -480,7 +482,18 @@ export class FailureConditionEvaluator {\\n \\n       // Extract context variables\\n       const output = context.output || {};\\n-      const issues = output.issues || [];\\n+      // Ensure issues is an array - it might be a JSON string from workflow outputs\\n+      let issues = output.issues || [];\\n+      if (typeof issues === 'string') {\\n+        try {\\n+          issues = JSON.parse(issues);\\n+        } catch {\\n+          issues = [];\\n+        }\\n+      }\\n+      if (!Array.isArray(issues)) {\\n+        issues = [];\\n+      }\\n \\n       // Backward compatibility: provide metadata for transition period\\n       // TODO: Remove after all configurations are updated\\n@@ -518,6 +531,7 @@ export class FailureConditionEvaluator {\\n       const event = context.event || 'manual';\\n       const env = context.env || {};\\n       const outputs = context.outputs || {};\\n+      const inputs = context.inputs || {};\\n       const debugData = context.debug || null;\\n \\n       // Get memory store and create accessor for fail_if expressions\\n@@ -555,6 +569,7 @@ export class FailureConditionEvaluator {\\n         filesCount,\\n         event,\\n         env,\\n+        inputs,\\n         // Helper functions\\n         contains,\\n         startsWith,\\n@@ -621,6 +636,7 @@ export class FailureConditionEvaluator {\\n             filesCount,\\n             event,\\n             env,\\n+            inputs,\\n             // Helpers\\n             contains,\\n             startsWith,\\n\",\"status\":\"modified\"},{\"filename\":\"src/frontends/slack-frontend.ts\",\"additions\":107,\"deletions\":6,\"changes\":113,\"patch\":\"diff --git a/src/frontends/slack-frontend.ts b/src/frontends/slack-frontend.ts\\nindex 97161a7d..41735aa2 100644\\n--- a/src/frontends/slack-frontend.ts\\n+++ b/src/frontends/slack-frontend.ts\\n@@ -1,6 +1,32 @@\\n+/**\\n+ * Slack Frontend for Visor workflows.\\n+ *\\n+ * Features:\\n+ * - Posts AI replies to Slack threads\\n+ * - Converts Markdown to Slack mrkdwn format\\n+ * - Renders mermaid diagrams to PNG and uploads as images\\n+ * - Manages üëÄ/üëç reactions for acknowledgement\\n+ * - Handles human input prompts via prompt-state\\n+ *\\n+ * Mermaid Diagram Rendering:\\n+ * - Detects ```mermaid code blocks in AI responses\\n+ * - Renders to PNG using @mermaid-js/mermaid-cli (mmdc)\\n+ * - Uploads rendered images to Slack thread\\n+ * - Replaces mermaid blocks with \\\"_(See diagram above)_\\\" placeholder\\n+ *\\n+ * Requirements for mermaid rendering:\\n+ * - Node.js and npx in PATH\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ * - On Linux: apt-get install chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ */\\n import type { Frontend, FrontendContext } from './host';\\n import { SlackClient } from '../slack/client';\\n-import { formatSlackText } from '../slack/markdown';\\n+import {\\n+  formatSlackText,\\n+  extractMermaidDiagrams,\\n+  renderMermaidToPng,\\n+  replaceMermaidBlocks,\\n+} from '../slack/markdown';\\n \\n type SlackFrontendConfig = {\\n   defaultChannel?: string;\\n@@ -284,6 +310,9 @@ export class SlackFrontend implements Frontend {\\n       const isLogChat = providerType === 'log' && checkCfg.group === 'chat';\\n       if (!isAi && !isLogChat) return;\\n \\n+      // Skip internal steps - they're intermediate processing and shouldn't post to Slack\\n+      if (checkCfg.criticality === 'internal') return;\\n+\\n       // For AI checks, only post when using simple/unstructured schemas (or none).\\n       if (isAi) {\\n         const schema = checkCfg.schema;\\n@@ -316,6 +345,13 @@ export class SlackFrontend implements Frontend {\\n         ) {\\n           text = (result as any).content.trim();\\n         }\\n+      } else if (isLogChat && typeof (result as any)?.logOutput === 'string') {\\n+        // For log-based chat checks, render the formatted log output as the\\n+        // Slack message when no structured text field is present.\\n+        const raw = (result as any).logOutput;\\n+        if (raw.trim().length > 0) {\\n+          text = raw.trim();\\n+        }\\n       } else if (isAi && showRawOutput && out !== undefined) {\\n         try {\\n           text = JSON.stringify(out, null, 2);\\n@@ -325,13 +361,78 @@ export class SlackFrontend implements Frontend {\\n       }\\n       if (!text) return;\\n \\n-      const formattedText = formatSlackText(text);\\n+      // Extract and render mermaid diagrams before posting\\n+      const diagrams = extractMermaidDiagrams(text);\\n+      let processedText = text;\\n+\\n+      if (diagrams.length > 0) {\\n+        try {\\n+          ctx.logger.info(\\n+            `[slack-frontend] found ${diagrams.length} mermaid diagram(s) to render for ${checkId}`\\n+          );\\n+        } catch {}\\n+\\n+        // Render and upload each diagram\\n+        const uploadedCount: number[] = [];\\n+        for (let i = 0; i < diagrams.length; i++) {\\n+          const diagram = diagrams[i];\\n+          try {\\n+            ctx.logger.info(`[slack-frontend] rendering mermaid diagram ${i + 1}...`);\\n+            const pngBuffer = await renderMermaidToPng(diagram.code);\\n+            if (pngBuffer) {\\n+              ctx.logger.info(\\n+                `[slack-frontend] rendered diagram ${i + 1}, size=${pngBuffer.length} bytes, uploading...`\\n+              );\\n+              const filename = `diagram-${i + 1}.png`;\\n+              const uploadResult = await slack.files.uploadV2({\\n+                content: pngBuffer,\\n+                filename,\\n+                channel,\\n+                thread_ts: threadTs,\\n+                title: `Diagram ${i + 1}`,\\n+              });\\n+              if (uploadResult.ok) {\\n+                uploadedCount.push(i);\\n+                ctx.logger.info(`[slack-frontend] uploaded mermaid diagram ${i + 1} to ${channel}`);\\n+              } else {\\n+                ctx.logger.warn(`[slack-frontend] upload failed for diagram ${i + 1}`);\\n+              }\\n+            } else {\\n+              ctx.logger.warn(\\n+                `[slack-frontend] mermaid rendering returned null for diagram ${i + 1} (mmdc failed or not installed)`\\n+              );\\n+            }\\n+          } catch (e) {\\n+            ctx.logger.warn(\\n+              `[slack-frontend] failed to render/upload mermaid diagram ${i + 1}: ${\\n+                e instanceof Error ? e.message : String(e)\\n+              }`\\n+            );\\n+          }\\n+        }\\n+\\n+        // Replace mermaid blocks with placeholder text\\n+        if (uploadedCount.length > 0) {\\n+          processedText = replaceMermaidBlocks(text, diagrams, (idx) =>\\n+            uploadedCount.includes(idx) ? '_(See diagram above)_' : '_(Diagram rendering failed)_'\\n+          );\\n+        }\\n+      }\\n+\\n+      const formattedText = formatSlackText(processedText);\\n       await slack.chat.postMessage({ channel, text: formattedText, thread_ts: threadTs });\\n+      ctx.logger.info(\\n+        `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+      );\\n+    } catch (outerErr) {\\n+      // Log errors instead of silently swallowing them\\n       try {\\n-        ctx.logger.info(\\n-          `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+        ctx.logger.warn(\\n+          `[slack-frontend] maybePostDirectReply failed for ${checkId}: ${\\n+            outerErr instanceof Error ? outerErr.message : String(outerErr)\\n+          }`\\n         );\\n       } catch {}\\n-    } catch {}\\n+    }\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/generated/config-schema.ts\",\"additions\":233,\"deletions\":13,\"changes\":246,\"patch\":\"diff --git a/src/generated/config-schema.ts b/src/generated/config-schema.ts\\nindex b3caca4f..ecab390d 100644\\n--- a/src/generated/config-schema.ts\\n+++ b/src/generated/config-schema.ts\\n@@ -56,6 +56,20 @@ export const configSchema = {\\n           },\\n           description: 'Import workflow definitions from external files or URLs',\\n         },\\n+        inputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowInput',\\n+          },\\n+          description: 'Workflow inputs (for standalone reusable workflows)',\\n+        },\\n+        outputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowOutput',\\n+          },\\n+          description: 'Workflow outputs (for standalone reusable workflows)',\\n+        },\\n         steps: {\\n           $ref: '#/definitions/Record%3Cstring%2CCheckConfig%3E',\\n           description: 'Step configurations (recommended)',\\n@@ -67,7 +81,7 @@ export const configSchema = {\\n         },\\n         output: {\\n           $ref: '#/definitions/OutputConfig',\\n-          description: 'Output configuration',\\n+          description: 'Output configuration (optional - defaults provided)',\\n         },\\n         http_server: {\\n           $ref: '#/definitions/HttpServerConfig',\\n@@ -139,8 +153,16 @@ export const configSchema = {\\n           },\\n           description: 'Optional integrations: event-driven frontends (e.g., ndjson-sink, github)',\\n         },\\n+        workspace: {\\n+          $ref: '#/definitions/WorkspaceConfig',\\n+          description: 'Workspace isolation configuration for sandboxed execution',\\n+        },\\n+        slack: {\\n+          $ref: '#/definitions/SlackConfig',\\n+          description: 'Slack configuration',\\n+        },\\n       },\\n-      required: ['output', 'version'],\\n+      required: ['version'],\\n       patternProperties: {\\n         '^x-': {},\\n       },\\n@@ -243,6 +265,63 @@ export const configSchema = {\\n         type: 'string',\\n       },\\n     },\\n+    WorkflowInput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Input parameter name',\\n+        },\\n+        schema: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'JSON Schema for the input',\\n+        },\\n+        required: {\\n+          type: 'boolean',\\n+          description: 'Whether this input is required',\\n+        },\\n+        default: {\\n+          description: 'Default value if not provided',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow input definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    WorkflowOutput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Output name',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+        value: {\\n+          type: 'string',\\n+          description: 'Value using Liquid template syntax (references step outputs)',\\n+        },\\n+        value_js: {\\n+          type: 'string',\\n+          description: 'Value using JavaScript expression (alternative to value)',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow output definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n     'Record<string,CheckConfig>': {\\n       type: 'object',\\n       additionalProperties: {\\n@@ -390,11 +469,19 @@ export const configSchema = {\\n           description: 'Timeout in seconds for command execution (default: 60)',\\n         },\\n         depends_on: {\\n-          type: 'array',\\n-          items: {\\n-            type: 'string',\\n-          },\\n-          description: 'Check IDs that this check depends on (optional)',\\n+          anyOf: [\\n+            {\\n+              type: 'string',\\n+            },\\n+            {\\n+              type: 'array',\\n+              items: {\\n+                type: 'string',\\n+              },\\n+            },\\n+          ],\\n+          description:\\n+            'Check IDs that this check depends on (optional). Accepts single string or array.',\\n         },\\n         group: {\\n           type: 'string',\\n@@ -653,13 +740,93 @@ export const configSchema = {\\n           description: 'Arguments/inputs for the workflow',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Override specific step configurations in the workflow',\\n         },\\n         output_mapping: {\\n           $ref: '#/definitions/Record%3Cstring%2Cstring%3E',\\n           description: 'Map workflow outputs to check outputs',\\n         },\\n+        workflow_inputs: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'Alias for args - workflow inputs (backward compatibility)',\\n+        },\\n+        config: {\\n+          type: 'string',\\n+          description:\\n+            'Config file path - alternative to workflow ID (loads a Visor config file as workflow)',\\n+        },\\n+        workflow_overrides: {\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n+          description: 'Alias for overrides - workflow step overrides (backward compatibility)',\\n+        },\\n+        ref: {\\n+          type: 'string',\\n+          description: 'Git reference to checkout (branch, tag, commit SHA) - supports templates',\\n+        },\\n+        repository: {\\n+          type: 'string',\\n+          description: 'Repository URL or owner/repo format (defaults to current repository)',\\n+        },\\n+        token: {\\n+          type: 'string',\\n+          description: 'GitHub token for private repositories (defaults to GITHUB_TOKEN env)',\\n+        },\\n+        fetch_depth: {\\n+          type: 'number',\\n+          description: 'Number of commits to fetch (0 for full history, default: 1)',\\n+        },\\n+        fetch_tags: {\\n+          type: 'boolean',\\n+          description: 'Whether to fetch tags (default: false)',\\n+        },\\n+        submodules: {\\n+          anyOf: [\\n+            {\\n+              type: 'boolean',\\n+            },\\n+            {\\n+              type: 'string',\\n+              const: 'recursive',\\n+            },\\n+          ],\\n+          description: \\\"Checkout submodules: false, true, or 'recursive'\\\",\\n+        },\\n+        working_directory: {\\n+          type: 'string',\\n+          description: 'Working directory for the checkout (defaults to temp directory)',\\n+        },\\n+        use_worktree: {\\n+          type: 'boolean',\\n+          description: 'Use git worktree for efficient parallel checkouts (default: true)',\\n+        },\\n+        clean: {\\n+          type: 'boolean',\\n+          description: 'Clean the working directory before checkout (default: true)',\\n+        },\\n+        sparse_checkout: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description: 'Sparse checkout paths - only checkout specific directories/files',\\n+        },\\n+        lfs: {\\n+          type: 'boolean',\\n+          description: 'Enable Git LFS (Large File Storage)',\\n+        },\\n+        clone_timeout_ms: {\\n+          type: 'number',\\n+          description: 'Timeout in ms for cloning the bare repository (default: 300000 = 5 min)',\\n+        },\\n+        cleanup_on_failure: {\\n+          type: 'boolean',\\n+          description: 'Clean up worktree on failure (default: true)',\\n+        },\\n+        persist_worktree: {\\n+          type: 'boolean',\\n+          description: 'Keep worktree after workflow completion (default: false)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'Configuration for a single check',\\n@@ -794,6 +961,11 @@ export const configSchema = {\\n           $ref: '#/definitions/BashConfig',\\n           description: 'Advanced bash command execution configuration',\\n         },\\n+        completion_prompt: {\\n+          type: 'string',\\n+          description:\\n+            'Completion prompt for post-completion validation/review (runs after attempt_completion)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'AI provider configuration',\\n@@ -1223,7 +1395,7 @@ export const configSchema = {\\n           description: 'Custom output name (defaults to workflow name)',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Step overrides',\\n         },\\n         output_mapping: {\\n@@ -1238,14 +1410,14 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n-    'Record<string,Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>>':\\n+    'Record<string,Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>>':\\n       {\\n         type: 'object',\\n         additionalProperties: {\\n-          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E',\\n+          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E',\\n         },\\n       },\\n-    'Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>': {\\n+    'Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>': {\\n       type: 'object',\\n       additionalProperties: false,\\n     },\\n@@ -1811,6 +1983,54 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n+    WorkspaceConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        enabled: {\\n+          type: 'boolean',\\n+          description: 'Enable workspace isolation (default: true when config present)',\\n+        },\\n+        base_path: {\\n+          type: 'string',\\n+          description: 'Base path for workspaces (default: /tmp/visor-workspaces)',\\n+        },\\n+        cleanup_on_exit: {\\n+          type: 'boolean',\\n+          description: 'Clean up workspace on exit (default: true)',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Workspace isolation configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    SlackConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        version: {\\n+          type: 'string',\\n+          description: 'Slack API version',\\n+        },\\n+        mentions: {\\n+          type: 'string',\\n+          description: \\\"Mention handling: 'all', 'direct', etc.\\\",\\n+        },\\n+        threads: {\\n+          type: 'string',\\n+          description: \\\"Thread handling: 'required', 'optional', etc.\\\",\\n+        },\\n+        show_raw_output: {\\n+          type: 'boolean',\\n+          description: 'Show raw output in Slack responses',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Slack configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n   },\\n } as const;\\n export default configSchema;\\n\",\"status\":\"modified\"},{\"filename\":\"src/index.ts\",\"additions\":5,\"deletions\":2,\"changes\":7,\"patch\":\"diff --git a/src/index.ts b/src/index.ts\\nindex 50b89df3..9621e733 100644\\n--- a/src/index.ts\\n+++ b/src/index.ts\\n@@ -628,8 +628,11 @@ function resolveDependencies(\\n \\n     // Get dependencies for this check\\n     const checkConfig = config?.checks?.[checkId];\\n+    // Normalize depends_on to array (supports string | string[])\\n+    const rawDeps = checkConfig?.depends_on;\\n+    const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     // Expand OR groups (pipe syntax) for dependency closure discovery\\n-    const dependencies = (checkConfig?.depends_on || []).flatMap(d =>\\n+    const dependencies = depsArray.flatMap((d: string) =>\\n       typeof d === 'string' && d.includes('|')\\n         ? d\\n             .split('|')\\n\",\"status\":\"modified\"},{\"filename\":\"src/liquid-extensions.ts\",\"additions\":68,\"deletions\":1,\"changes\":69,\"patch\":\"diff --git a/src/liquid-extensions.ts b/src/liquid-extensions.ts\\nindex e9efaf68..dbf9c6ed 100644\\n--- a/src/liquid-extensions.ts\\n+++ b/src/liquid-extensions.ts\\n@@ -124,6 +124,26 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     }\\n   });\\n \\n+  // Register base64 filter for encoding strings\\n+  // Usage: {{ \\\"user:password\\\" | base64 }}\\n+  liquid.registerFilter('base64', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    return Buffer.from(str).toString('base64');\\n+  });\\n+\\n+  // Register base64_decode filter for decoding base64 strings\\n+  // Usage: {{ encoded_value | base64_decode }}\\n+  liquid.registerFilter('base64_decode', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    try {\\n+      return Buffer.from(str, 'base64').toString('utf-8');\\n+    } catch {\\n+      return '[Error: Invalid base64 string]';\\n+    }\\n+  });\\n+\\n   // Sanitize a label to allowed characters only: [A-Za-z0-9:/]\\n   liquid.registerFilter('safe_label', (value: unknown) => sanitizeLabel(value));\\n \\n@@ -137,6 +157,53 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     return s.replace(/\\\\\\\\n/g, '\\\\n').replace(/\\\\\\\\r/g, '\\\\r').replace(/\\\\\\\\t/g, '\\\\t');\\n   });\\n \\n+  // JSON escape filter - escapes a string for use inside a JSON string value\\n+  // This escapes special characters like quotes, backslashes, and control characters\\n+  // Usage: \\\"jql\\\": \\\"{{ myValue | json_escape }}\\\"\\n+  liquid.registerFilter('json_escape', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const s = String(value);\\n+    // Use JSON.stringify which handles all escaping, then strip the surrounding quotes\\n+    const jsonStr = JSON.stringify(s);\\n+    // Remove the first and last character (the quotes added by JSON.stringify)\\n+    return jsonStr.slice(1, -1);\\n+  });\\n+\\n+  // Shell escape filter - wraps value in single quotes with proper escaping\\n+  // Usage: {{ value | shell_escape }}\\n+  // Example: \\\"hello'world\\\" becomes \\\"'hello'\\\\''world'\\\"\\n+  // This is POSIX-compliant and safe for arbitrary text including mermaid diagrams\\n+  liquid.registerFilter('shell_escape', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    // Replace single quotes with: end quote, escaped quote, start quote\\n+    // Then wrap the entire thing in single quotes\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Alias for shell_escape\\n+  liquid.registerFilter('escape_shell', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Shell escape for double quotes (less safe but sometimes needed)\\n+  // Usage: {{ value | shell_escape_double }}\\n+  // Escapes: $, `, \\\\, \\\", and !\\n+  liquid.registerFilter('shell_escape_double', (value: unknown) => {\\n+    if (value == null) return '\\\"\\\"';\\n+    const s = String(value);\\n+    // Escape characters that have special meaning inside double quotes\\n+    const escaped = s\\n+      .replace(/\\\\\\\\/g, '\\\\\\\\\\\\\\\\')  // backslash first\\n+      .replace(/\\\\$/g, '\\\\\\\\$')   // dollar sign\\n+      .replace(/`/g, '\\\\\\\\`')    // backticks\\n+      .replace(/\\\"/g, '\\\\\\\\\\\"')    // double quotes\\n+      .replace(/!/g, '\\\\\\\\!');   // history expansion\\n+    return '\\\"' + escaped + '\\\"';\\n+  });\\n+\\n   // Register author permission filters (from main)\\n   // These filters check the author's permission level; detect local mode for tests\\n   const isLocal = detectLocalMode();\\n\",\"status\":\"modified\"},{\"filename\":\"src/logger.ts\",\"additions\":31,\"deletions\":2,\"changes\":33,\"patch\":\"diff --git a/src/logger.ts b/src/logger.ts\\nindex 6148b77c..aee7136e 100644\\n--- a/src/logger.ts\\n+++ b/src/logger.ts\\n@@ -85,7 +85,36 @@ class Logger {\\n       if (this.showTimestamps) {\\n         const ts = new Date().toISOString();\\n         const lvl = level ? level : undefined;\\n-        const prefix = lvl ? `[${ts}] [${lvl}]` : `[${ts}]`;\\n+\\n+        let tsToken = `[${ts}]`;\\n+        let lvlToken = lvl ? `[${lvl}]` : '';\\n+\\n+        // Add simple ANSI colour when running in a TTY and not emitting\\n+        // JSON/SARIF. Colours are intentionally minimal and only applied\\n+        // to the prefix markers, not the full line.\\n+        if (this.isTTY && !this.isJsonLike) {\\n+          const reset = '\\\\x1b[0m';\\n+          const dim = '\\\\x1b[2m';\\n+          const colours: Record<LogLevel, string> = {\\n+            silent: '',\\n+            error: '\\\\x1b[31m', // red\\n+            warn: '\\\\x1b[33m', // yellow\\n+            info: '\\\\x1b[36m', // cyan\\n+            verbose: '\\\\x1b[35m', // magenta\\n+            debug: '\\\\x1b[90m', // bright black / gray\\n+          };\\n+\\n+          tsToken = `${dim}${tsToken}${reset}`;\\n+\\n+          if (lvl) {\\n+            const colour = colours[lvl] || '';\\n+            if (colour) {\\n+              lvlToken = `${colour}${lvlToken}${reset}`;\\n+            }\\n+          }\\n+        }\\n+\\n+        const prefix = lvl ? `${tsToken} ${lvlToken}` : tsToken;\\n         process.stderr.write(`${prefix} ${msg}\\\\n`);\\n       } else {\\n         process.stderr.write(msg + '\\\\n');\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/ai-check-provider.ts\",\"additions\":100,\"deletions\":6,\"changes\":106,\"patch\":\"diff --git a/src/providers/ai-check-provider.ts b/src/providers/ai-check-provider.ts\\nindex 53f1a350..b41d6af1 100644\\n--- a/src/providers/ai-check-provider.ts\\n+++ b/src/providers/ai-check-provider.ts\\n@@ -186,7 +186,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     let promptContent: string;\\n \\n@@ -204,7 +205,8 @@ export class AICheckProvider extends CheckProvider {\\n       eventContext,\\n       dependencyResults,\\n       outputHistory,\\n-      args\\n+      args,\\n+      workflowInputs\\n     );\\n   }\\n \\n@@ -335,7 +337,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     // Build outputs_raw from -raw keys (aggregate parent values)\\n     const outputsRaw: Record<string, unknown> = {};\\n@@ -535,6 +538,8 @@ export class AICheckProvider extends CheckProvider {\\n       outputs_raw: outputsRaw,\\n       // Custom arguments from on_init 'with' directive\\n       args: args || {},\\n+      // Workflow inputs (for nested workflow steps to access parent inputs like {{ inputs.context }})\\n+      inputs: workflowInputs || {},\\n     };\\n \\n     try {\\n@@ -621,7 +626,8 @@ export class AICheckProvider extends CheckProvider {\\n         console.error(`[ai-exec] step=${String((config as any).checkName || 'unknown')}`);\\n       }\\n     } catch {}\\n-    // Extract AI configuration - only set properties that are explicitly provided\\n+    // Extract AI configuration - only set properties that are explicitly provided.\\n+    // Workspace / allowedFolders will be derived below from the execution context.\\n     const aiConfig: AIReviewConfig = {};\\n \\n     // Check-level AI configuration (ai object)\\n@@ -671,6 +677,9 @@ export class AICheckProvider extends CheckProvider {\\n       if (aiAny.bashConfig !== undefined) {\\n         aiConfig.bashConfig = aiAny.bashConfig as import('../types/config').BashConfig;\\n       }\\n+      if (aiAny.completion_prompt !== undefined) {\\n+        aiConfig.completionPrompt = aiAny.completion_prompt as string;\\n+      }\\n       if (aiAny.skip_code_context !== undefined) {\\n         // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n         (aiConfig as any).skip_code_context = aiAny.skip_code_context as boolean;\\n@@ -691,6 +700,90 @@ export class AICheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Derive workspace-aware allowedFolders for ProbeAgent when workspace\\n+    // isolation is enabled. This ensures tools like search/query operate\\n+    // inside the isolated workspace (and its project symlinks) instead of\\n+    // the Visor repository root.\\n+    // Folder names are human-readable (tyk-docs, visor2) thanks to WorkspaceManager.\\n+    try {\\n+      const ctxAny: any = sessionInfo as any;\\n+      const parentCtx = ctxAny?._parentContext;\\n+      const workspace = parentCtx?.workspace;\\n+\\n+      // Enhanced debug logging for workspace propagation diagnosis\\n+      logger.debug(`[AI Provider] Workspace detection for check '${(config as any).checkName || 'unknown'}':`);\\n+      logger.debug(`[AI Provider]   sessionInfo exists: ${!!sessionInfo}`);\\n+      logger.debug(`[AI Provider]   _parentContext exists: ${!!parentCtx}`);\\n+      logger.debug(`[AI Provider]   workspace exists: ${!!workspace}`);\\n+      if (workspace) {\\n+        logger.debug(`[AI Provider]   workspace.isEnabled exists: ${typeof workspace.isEnabled === 'function'}`);\\n+        logger.debug(`[AI Provider]   workspace.isEnabled(): ${typeof workspace.isEnabled === 'function' ? workspace.isEnabled() : 'N/A'}`);\\n+        const projectCount = typeof workspace.listProjects === 'function' ? workspace.listProjects()?.length : 'N/A';\\n+        logger.debug(`[AI Provider]   workspace.listProjects() count: ${projectCount}`);\\n+      }\\n+\\n+      if (workspace && typeof workspace.isEnabled === 'function' && workspace.isEnabled()) {\\n+        const folders: string[] = [];\\n+        let workspaceRoot: string | undefined;\\n+        let mainProjectPath: string | undefined;\\n+        try {\\n+          const info = workspace.getWorkspaceInfo?.();\\n+          if (info && typeof info.workspacePath === 'string') {\\n+            workspaceRoot = info.workspacePath;\\n+            mainProjectPath = info.mainProjectPath;\\n+            // Add workspace root\\n+            folders.push(info.workspacePath);\\n+            // Include main project path (has human-readable name like \\\"visor2\\\")\\n+            if (mainProjectPath) {\\n+              folders.push(mainProjectPath);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore workspace info errors\\n+        }\\n+        try {\\n+          const projects = workspace.listProjects?.() || [];\\n+          for (const proj of projects as any[]) {\\n+            if (proj && typeof proj.path === 'string') {\\n+              // Project paths have human-readable names (tyk-docs, not checkout-tyk-docs)\\n+              folders.push(proj.path);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore project listing errors\\n+        }\\n+        const unique = Array.from(new Set(folders.filter(p => typeof p === 'string' && p)));\\n+        if (unique.length > 0 && workspaceRoot) {\\n+          (aiConfig as any).allowedFolders = unique;\\n+          // Set path and cwd to workspace root - AI will run with cwd set to workspace\\n+          // Both are set for compatibility: path for older probe versions, cwd for rc175+\\n+          (aiConfig as any).path = workspaceRoot;\\n+          (aiConfig as any).cwd = workspaceRoot;\\n+          // Also set workspacePath for the AI to know the root\\n+          (aiConfig as any).workspacePath = workspaceRoot;\\n+          logger.debug(`[AI Provider] Workspace isolation enabled:`);\\n+          logger.debug(`[AI Provider]   workspaceRoot (cwd): ${workspaceRoot}`);\\n+          logger.debug(`[AI Provider]   allowedFolders: ${JSON.stringify(unique)}`);\\n+        }\\n+      } else if (parentCtx && typeof parentCtx.workingDirectory === 'string') {\\n+        // Fallback: when workspace is not available (or disabled), still\\n+        // constrain tools to the engine's working directory so ProbeAgent\\n+        // operates inside the same logical root as the state machine. This\\n+        // also ensures nested workflows (e.g. code-question-helper) use the\\n+        // workspace main project path once initializeWorkspace has updated\\n+        // the parent context.\\n+        if (!(aiConfig as any).allowedFolders) {\\n+          (aiConfig as any).allowedFolders = [parentCtx.workingDirectory];\\n+        }\\n+        if (!(aiConfig as any).path) {\\n+          (aiConfig as any).path = parentCtx.workingDirectory;\\n+          (aiConfig as any).cwd = parentCtx.workingDirectory;\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only; fall back to defaults on error.\\n+    }\\n+\\n     // Check-level AI model and provider (top-level properties)\\n     if (config.ai_model !== undefined) {\\n       aiConfig.model = config.ai_model as string;\\n@@ -876,7 +969,8 @@ export class AICheckProvider extends CheckProvider {\\n       ctxWithStage,\\n       _dependencyResults,\\n       (config as any).__outputHistory as Map<string, unknown[]> | undefined,\\n-      (sessionInfo as any)?.args\\n+      (sessionInfo as any)?.args,\\n+      (config as any).workflowInputs as Record<string, unknown> | undefined\\n     );\\n \\n     // Optional persona (vendor extension): ai.ai_persona or ai_persona.\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/command-check-provider.ts\",\"additions\":50,\"deletions\":16,\"changes\":66,\"patch\":\"diff --git a/src/providers/command-check-provider.ts b/src/providers/command-check-provider.ts\\nindex e0c9d634..bf7ffd43 100644\\n--- a/src/providers/command-check-provider.ts\\n+++ b/src/providers/command-check-provider.ts\\n@@ -131,7 +131,8 @@ export class CommandCheckProvider extends CheckProvider {\\n       // New: outputs_raw exposes aggregate values (e.g., full arrays for forEach parents)\\n       outputs_raw: outputsRaw,\\n       // Workflow inputs (when executing within a workflow)\\n-      inputs: context?.workflowInputs || {},\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any).workflowInputs || context?.workflowInputs || {},\\n       // Custom arguments from on_init 'with' directive\\n       args: context?.args || {},\\n       env: this.getSafeEnvironmentVariables(),\\n@@ -178,17 +179,31 @@ export class CommandCheckProvider extends CheckProvider {\\n           mock = rawMock as Record<string, unknown>;\\n         }\\n         const m = mock as { stdout?: string; stderr?: string; exit_code?: number; exit?: number };\\n-        let out: unknown = m.stdout ?? '';\\n-        try {\\n-          if (\\n-            typeof out === 'string' &&\\n-            (out.trim().startsWith('{') || out.trim().startsWith('['))\\n-          ) {\\n-            out = JSON.parse(out);\\n-          }\\n-        } catch {}\\n-        const code =\\n-          typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0;\\n+\\n+        // Check if this looks like a command mock (has stdout/stderr/exit_code) or direct data output\\n+        const isCommandMock = m.stdout !== undefined || m.stderr !== undefined ||\\n+          m.exit_code !== undefined || m.exit !== undefined;\\n+\\n+        let out: unknown;\\n+        if (isCommandMock) {\\n+          // Traditional command mock format: { stdout: \\\"...\\\", exit_code: 0 }\\n+          out = m.stdout ?? '';\\n+          try {\\n+            if (\\n+              typeof out === 'string' &&\\n+              (out.trim().startsWith('{') || out.trim().startsWith('['))\\n+            ) {\\n+              out = JSON.parse(out);\\n+            }\\n+          } catch {}\\n+        } else {\\n+          // Direct data mock format: { data: [...], count: 1 } - use as-is\\n+          out = mock;\\n+        }\\n+\\n+        const code = isCommandMock\\n+          ? (typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0)\\n+          : 0;\\n         if (code !== 0) {\\n           return {\\n             issues: [\\n@@ -231,9 +246,8 @@ export class CommandCheckProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Get timeout from config (in seconds) or use default (60 seconds)\\n-      const timeoutSeconds = (config.timeout as number) || 60;\\n-      const timeoutMs = timeoutSeconds * 1000;\\n+      // Get timeout from config (in milliseconds) or use default (60 seconds = 60000ms)\\n+      const timeoutMs = (config.timeout as number) || 60000;\\n \\n       // Normalize only the eval payload for `node -e|--eval` invocations that may contain\\n       // literal newlines due to YAML processing (\\\"\\\\n\\\" -> newline). We re-escape newlines\\n@@ -1533,6 +1547,26 @@ ${bodyWithReturn}\\n       return null;\\n     }\\n \\n+    // Require at least one issue-specific field beyond just a message-like field.\\n+    // This prevents objects like {text: \\\"...\\\"} from being treated as issues.\\n+    const hasIssueField =\\n+      data.file !== undefined ||\\n+      data.path !== undefined ||\\n+      data.filename !== undefined ||\\n+      data.line !== undefined ||\\n+      data.startLine !== undefined ||\\n+      data.lineNumber !== undefined ||\\n+      data.severity !== undefined ||\\n+      data.level !== undefined ||\\n+      data.priority !== undefined ||\\n+      data.ruleId !== undefined ||\\n+      data.rule !== undefined ||\\n+      data.category !== undefined ||\\n+      data.type !== undefined;\\n+    if (!hasIssueField) {\\n+      return null;\\n+    }\\n+\\n     const allowedSeverities = new Set(['info', 'warning', 'error', 'critical']);\\n     const severityRaw = this.toTrimmedString(data.severity || data.level || data.priority);\\n     let severity: ReviewIssue['severity'] = 'warning';\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/git-checkout-provider.ts\",\"additions\":41,\"deletions\":6,\"changes\":47,\"patch\":\"diff --git a/src/providers/git-checkout-provider.ts b/src/providers/git-checkout-provider.ts\\nindex baac0001..6d3a2852 100644\\n--- a/src/providers/git-checkout-provider.ts\\n+++ b/src/providers/git-checkout-provider.ts\\n@@ -69,6 +69,16 @@ export class GitCheckoutProvider extends CheckProvider {\\n       return false;\\n     }\\n \\n+    if (checkoutConfig.clone_timeout_ms !== undefined) {\\n+      if (\\n+        typeof checkoutConfig.clone_timeout_ms !== 'number' ||\\n+        checkoutConfig.clone_timeout_ms <= 0\\n+      ) {\\n+        logger.error('Invalid config: clone_timeout_ms must be a positive number (milliseconds)');\\n+        return false;\\n+      }\\n+    }\\n+\\n     return true;\\n   }\\n \\n@@ -91,7 +101,13 @@ export class GitCheckoutProvider extends CheckProvider {\\n       );\\n \\n       // Resolve dynamic variables\\n-      const resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      let resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      // If ref resolves to an empty string, fall back to HEAD so we rely on\\n+      // the repository's default branch. This keeps downstream git commands\\n+      // well-defined and matches existing expectations/tests.\\n+      if (!resolvedRef || resolvedRef.trim().length === 0) {\\n+        resolvedRef = 'HEAD';\\n+      }\\n       const resolvedRepository = checkoutConfig.repository\\n         ? await this.liquid.parseAndRender(checkoutConfig.repository, templateContext)\\n         : process.env.GITHUB_REPOSITORY || 'unknown/unknown';\\n@@ -118,6 +134,7 @@ export class GitCheckoutProvider extends CheckProvider {\\n           clean: checkoutConfig.clean !== false, // Default: true\\n           workflowId: (context as any)?.workflowId,\\n           fetchDepth: checkoutConfig.fetch_depth,\\n+          cloneTimeoutMs: checkoutConfig.clone_timeout_ms,\\n         }\\n       );\\n \\n@@ -134,18 +151,35 @@ export class GitCheckoutProvider extends CheckProvider {\\n \\n       // Add project to workspace if workspace isolation is enabled\\n       const workspace = (context as any)?._parentContext?.workspace;\\n+\\n+      // Enhanced debug logging for workspace project addition diagnosis\\n+      const checkName = (config as any)?.checkName || 'unknown';\\n+      logger.info(`[GitCheckout] Workspace check for '${checkName}':`);\\n+      logger.info(`[GitCheckout]   _parentContext exists: ${!!(context as any)?._parentContext}`);\\n+      logger.info(`[GitCheckout]   workspace exists: ${!!workspace}`);\\n+      logger.info(`[GitCheckout]   workspace.isEnabled(): ${workspace?.isEnabled?.() ?? 'N/A'}`);\\n+      if (workspace) {\\n+        const projectCountBefore = workspace.listProjects?.()?.length ?? 'N/A';\\n+        logger.debug(`[GitCheckout]   projects before addProject: ${projectCountBefore}`);\\n+      }\\n+\\n       if (workspace?.isEnabled()) {\\n         try {\\n+          // Don't pass checkName as description - let workspace use repo name\\n+          // This results in human-readable names like \\\"tyk-docs\\\" instead of \\\"checkout-tyk-docs\\\"\\n           const workspacePath = await workspace.addProject(\\n             resolvedRepository,\\n-            worktree.path,\\n-            checkoutConfig.checkName\\n+            worktree.path\\n           );\\n           output.workspace_path = workspacePath;\\n-          logger.debug(`Added project to workspace: ${workspacePath}`);\\n+          const projectCountAfter = workspace.listProjects?.()?.length ?? 'N/A';\\n+          logger.debug(`[GitCheckout] Added project to workspace: ${workspacePath}`);\\n+          logger.debug(`[GitCheckout]   projects after addProject: ${projectCountAfter}`);\\n         } catch (error) {\\n           logger.warn(`Failed to add project to workspace: ${error}`);\\n         }\\n+      } else {\\n+        logger.debug(`[GitCheckout] Workspace not enabled, skipping addProject`);\\n       }\\n \\n       logger.info(\\n@@ -228,7 +262,8 @@ export class GitCheckoutProvider extends CheckProvider {\\n       outputs: outputsObj,\\n       outputs_history: historyObj,\\n       env: safeEnv,\\n-      inputs: context?.workflowInputs,\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any)?.workflowInputs || context?.workflowInputs,\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/http-client-provider.ts\",\"additions\":279,\"deletions\":35,\"changes\":314,\"patch\":\"diff --git a/src/providers/http-client-provider.ts b/src/providers/http-client-provider.ts\\nindex 4d8c41be..6db81591 100644\\n--- a/src/providers/http-client-provider.ts\\n+++ b/src/providers/http-client-provider.ts\\n@@ -4,18 +4,29 @@ import { ReviewSummary } from '../reviewer';\\n import { Liquid } from 'liquidjs';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n import { EnvironmentResolver } from '../utils/env-resolver';\\n+import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n+import { buildProviderTemplateContext } from '../utils/template-context';\\n+import Sandbox from '@nyariv/sandboxjs';\\n+import { logger } from '../logger';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n \\n /**\\n  * Check provider that fetches data from HTTP endpoints\\n  */\\n export class HttpClientProvider extends CheckProvider {\\n   private liquid: Liquid;\\n+  private sandbox?: Sandbox;\\n \\n   constructor() {\\n     super();\\n     this.liquid = createExtendedLiquid();\\n   }\\n \\n+  private createSecureSandbox(): Sandbox {\\n+    return createSecureSandbox();\\n+  }\\n+\\n   getName(): string {\\n     return 'http_client';\\n   }\\n@@ -61,50 +72,124 @@ export class HttpClientProvider extends CheckProvider {\\n     const headers = (config.headers as Record<string, string>) || {};\\n     const timeout = (config.timeout as number) || 30000;\\n     const transform = config.transform as string | undefined;\\n+    const transformJs = config.transform_js as string | undefined;\\n     const bodyTemplate = config.body as string | undefined;\\n+    const outputFileTemplate = config.output_file as string | undefined;\\n+    const skipIfExists = config.skip_if_exists !== false; // Default true for caching\\n \\n-    try {\\n-      // Prepare template context for URL and body\\n-      const templateContext = {\\n-        pr: {\\n-          number: prInfo.number,\\n-          title: prInfo.title,\\n-          body: prInfo.body,\\n-          author: prInfo.author,\\n-          base: prInfo.base,\\n-          head: prInfo.head,\\n-          totalAdditions: prInfo.totalAdditions,\\n-          totalDeletions: prInfo.totalDeletions,\\n-        },\\n-        outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-        env: process.env,\\n-      };\\n+    // Track resolved URL for error messages\\n+    let resolvedUrlForErrors = url;\\n \\n-      // Render URL with template if it contains liquid syntax\\n-      let renderedUrl = url;\\n-      if (url.includes('{{') || url.includes('{%')) {\\n-        renderedUrl = await this.liquid.parseAndRender(url, templateContext);\\n+    try {\\n+      // Use shared template context builder for consistent output extraction\\n+      const templateContext = buildProviderTemplateContext(\\n+        prInfo,\\n+        dependencyResults,\\n+        undefined, // memoryStore\\n+        undefined, // outputHistory\\n+        undefined, // stageHistoryBase\\n+        { attachMemoryReadHelpers: false }\\n+      );\\n+      // Add env to context for shell-style variable resolution\\n+      (templateContext as Record<string, unknown>).env = process.env;\\n+\\n+      // First resolve shell-style environment variables (${VAR}, $VAR, ${{ env.VAR }})\\n+      let renderedUrl = String(EnvironmentResolver.resolveValue(url));\\n+      resolvedUrlForErrors = renderedUrl; // Track for error messages\\n+\\n+      // Then render Liquid templates if present\\n+      if (renderedUrl.includes('{{') || renderedUrl.includes('{%')) {\\n+        renderedUrl = await this.liquid.parseAndRender(renderedUrl, templateContext);\\n+        resolvedUrlForErrors = renderedUrl; // Update after Liquid rendering\\n       }\\n \\n       // Prepare request body if provided\\n       let requestBody: string | undefined;\\n       if (bodyTemplate) {\\n-        const renderedBody = await this.liquid.parseAndRender(bodyTemplate, templateContext);\\n-        requestBody = renderedBody;\\n+        // First resolve shell-style environment variables\\n+        let resolvedBody = String(EnvironmentResolver.resolveValue(bodyTemplate));\\n+        // Then render Liquid templates if present\\n+        if (resolvedBody.includes('{{') || resolvedBody.includes('{%')) {\\n+          resolvedBody = await this.liquid.parseAndRender(resolvedBody, templateContext);\\n+        }\\n+        requestBody = resolvedBody;\\n       }\\n \\n-      // Resolve environment variables in headers\\n-      const resolvedHeaders = EnvironmentResolver.resolveHeaders(headers);\\n+      // Resolve environment variables and Liquid templates in headers\\n+      const resolvedHeaders: Record<string, string> = {};\\n+      for (const [key, value] of Object.entries(headers)) {\\n+        let resolvedValue = String(EnvironmentResolver.resolveValue(value));\\n+        // Render Liquid templates if present\\n+        if (resolvedValue.includes('{{') || resolvedValue.includes('{%')) {\\n+          resolvedValue = await this.liquid.parseAndRender(resolvedValue, templateContext);\\n+        }\\n+        resolvedHeaders[key] = resolvedValue;\\n+        // Debug auth header (mask most of the value for security)\\n+        if (key.toLowerCase() === 'authorization') {\\n+          const maskedValue = resolvedValue.length > 20\\n+            ? `${resolvedValue.substring(0, 15)}...${resolvedValue.substring(resolvedValue.length - 5)}`\\n+            : resolvedValue;\\n+          logger.verbose(`[http_client] ${key}: ${maskedValue}`);\\n+        }\\n+      }\\n+\\n+      // Resolve output_file path if specified\\n+      let resolvedOutputFile: string | undefined;\\n+      if (outputFileTemplate) {\\n+        let outputPath = String(EnvironmentResolver.resolveValue(outputFileTemplate));\\n+        if (outputPath.includes('{{') || outputPath.includes('{%')) {\\n+          outputPath = await this.liquid.parseAndRender(outputPath, templateContext);\\n+        }\\n+        resolvedOutputFile = outputPath.trim();\\n+\\n+        // Check if file already exists (caching)\\n+        if (skipIfExists && fs.existsSync(resolvedOutputFile)) {\\n+          const stats = fs.statSync(resolvedOutputFile);\\n+          logger.verbose(`[http_client] File cached: ${resolvedOutputFile} (${stats.size} bytes)`);\\n+          return {\\n+            issues: [],\\n+            file_path: resolvedOutputFile,\\n+            size: stats.size,\\n+            cached: true,\\n+          } as unknown as ReviewSummary;\\n+        }\\n+      }\\n \\n       // Test hook: mock HTTP response for this step\\n       const stepName = (config as any).checkName || 'unknown';\\n       const mock = context?.hooks?.mockForStep?.(String(stepName));\\n-      const data =\\n-        mock !== undefined\\n-          ? mock\\n-          : await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n \\n-      // Apply transformation if specified\\n+      // If mock is provided, return it directly as the step output (with issues: [])\\n+      if (mock !== undefined) {\\n+        const mockObj = typeof mock === 'object' && mock !== null ? mock : { data: mock };\\n+        return {\\n+          issues: [],\\n+          ...mockObj,\\n+        } as unknown as ReviewSummary & { data: unknown };\\n+      }\\n+\\n+      // Debug log the request for troubleshooting\\n+      logger.verbose(`[http_client] ${method} ${renderedUrl}`);\\n+      if (requestBody) {\\n+        logger.verbose(`[http_client] Body: ${requestBody.substring(0, 500)}${requestBody.length > 500 ? '...' : ''}`);\\n+      }\\n+\\n+      // If output_file is specified, download to file instead of returning data\\n+      if (resolvedOutputFile) {\\n+        const fileResult = await this.downloadToFile(\\n+          renderedUrl,\\n+          method,\\n+          resolvedHeaders,\\n+          requestBody,\\n+          timeout,\\n+          resolvedOutputFile\\n+        );\\n+        return fileResult;\\n+      }\\n+\\n+      const data = await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n+\\n+      // Apply Liquid transformation if specified\\n       let processedData = data;\\n       if (transform) {\\n         try {\\n@@ -136,12 +221,53 @@ export class HttpClientProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Return the fetched data as a custom field for dependent checks to access\\n+      // Apply JavaScript transformation if specified\\n+      if (transformJs) {\\n+        try {\\n+          if (!this.sandbox) {\\n+            this.sandbox = this.createSecureSandbox();\\n+          }\\n+\\n+          // Create scope for JavaScript transform (scope, not context)\\n+          const jsScope: Record<string, unknown> = {\\n+            output: data,\\n+            pr: templateContext.pr,\\n+            outputs: templateContext.outputs,\\n+            env: process.env,\\n+          };\\n+\\n+          const result = compileAndRun(this.sandbox, transformJs, jsScope, {\\n+            injectLog: true,\\n+            logPrefix: 'üîç [transform_js]',\\n+            wrapFunction: true,\\n+          });\\n+          processedData = result;\\n+          logger.verbose(`‚úì Applied JavaScript transform successfully`);\\n+        } catch (error) {\\n+          logger.error(\\n+            `‚úó Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`\\n+          );\\n+          return {\\n+            issues: [\\n+              {\\n+                file: 'http_client',\\n+                line: 0,\\n+                ruleId: 'http_client/transform_js_error',\\n+                message: `Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              },\\n+            ],\\n+          };\\n+        }\\n+      }\\n+\\n+      // Return the fetched data in the standard `output` property for guarantee evaluation\\n+      // This is consistent with other providers (script, command, ai, etc.)\\n       return {\\n         issues: [],\\n-        // Add custom data field that will be passed through to dependent checks\\n-        data: processedData,\\n-      } as ReviewSummary & { data: unknown };\\n+        output: processedData,\\n+      } as unknown as ReviewSummary;\\n     } catch (error) {\\n       return {\\n         issues: [\\n@@ -149,7 +275,7 @@ export class HttpClientProvider extends CheckProvider {\\n             file: 'http_client',\\n             line: 0,\\n             ruleId: 'http_client/fetch_error',\\n-            message: `Failed to fetch from ${url}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            message: `Failed to fetch from ${resolvedUrlForErrors}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n             severity: 'error',\\n             category: 'logic',\\n           },\\n@@ -198,7 +324,14 @@ export class HttpClientProvider extends CheckProvider {\\n \\n       clearTimeout(timeoutId);\\n \\n+      logger.verbose(`[http_client] Response: ${response.status} ${response.statusText}`);\\n+\\n       if (!response.ok) {\\n+        // Log response body for debugging auth failures\\n+        try {\\n+          const errorBody = await response.text();\\n+          logger.warn(`[http_client] Error body: ${errorBody.substring(0, 500)}`);\\n+        } catch {}\\n         throw new Error(`HTTP ${response.status}: ${response.statusText}`);\\n       }\\n \\n@@ -233,6 +366,112 @@ export class HttpClientProvider extends CheckProvider {\\n     }\\n   }\\n \\n+  private async downloadToFile(\\n+    url: string,\\n+    method: string,\\n+    headers: Record<string, string>,\\n+    body: string | undefined,\\n+    timeout: number,\\n+    outputFile: string\\n+  ): Promise<ReviewSummary> {\\n+    // Check if fetch is available (Node 18+)\\n+    if (typeof fetch === 'undefined') {\\n+      throw new Error('HTTP client provider requires Node.js 18+ or node-fetch package');\\n+    }\\n+\\n+    const controller = new AbortController();\\n+    const timeoutId = setTimeout(() => controller.abort(), timeout);\\n+\\n+    try {\\n+      const requestOptions: RequestInit = {\\n+        method,\\n+        headers: { ...headers },\\n+        signal: controller.signal,\\n+      };\\n+\\n+      // Add body for non-GET requests\\n+      if (method !== 'GET' && body) {\\n+        requestOptions.body = body;\\n+        if (!headers['Content-Type'] && !headers['content-type']) {\\n+          requestOptions.headers = {\\n+            ...requestOptions.headers,\\n+            'Content-Type': 'application/json',\\n+          };\\n+        }\\n+      }\\n+\\n+      const response = await fetch(url, requestOptions);\\n+      clearTimeout(timeoutId);\\n+\\n+      if (!response.ok) {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_error',\\n+              message: `Failed to download file: HTTP ${response.status}: ${response.statusText}`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      // Create parent directory if it doesn't exist\\n+      const parentDir = path.dirname(outputFile);\\n+      if (parentDir && !fs.existsSync(parentDir)) {\\n+        fs.mkdirSync(parentDir, { recursive: true });\\n+      }\\n+\\n+      // Get the response as an ArrayBuffer and write to file\\n+      const arrayBuffer = await response.arrayBuffer();\\n+      const buffer = Buffer.from(arrayBuffer);\\n+      fs.writeFileSync(outputFile, buffer);\\n+\\n+      const contentType = response.headers.get('content-type') || 'application/octet-stream';\\n+      logger.verbose(`[http_client] Downloaded: ${outputFile} (${buffer.length} bytes)`);\\n+\\n+      return {\\n+        issues: [],\\n+        file_path: outputFile,\\n+        size: buffer.length,\\n+        content_type: contentType,\\n+        cached: false,\\n+      } as unknown as ReviewSummary;\\n+    } catch (error: unknown) {\\n+      clearTimeout(timeoutId);\\n+\\n+      if (error instanceof Error && error.name === 'AbortError') {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_timeout',\\n+              message: `Download timed out after ${timeout}ms`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      return {\\n+        issues: [\\n+          {\\n+            file: 'http_client',\\n+            line: 0,\\n+            ruleId: 'http_client/download_error',\\n+            message: `Failed to download file: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            severity: 'error',\\n+            category: 'logic',\\n+          },\\n+        ],\\n+      };\\n+    }\\n+  }\\n+\\n   getSupportedConfigKeys(): string[] {\\n     return [\\n       'type',\\n@@ -241,7 +480,10 @@ export class HttpClientProvider extends CheckProvider {\\n       'headers',\\n       'body',\\n       'transform',\\n+      'transform_js',\\n       'timeout',\\n+      'output_file',\\n+      'skip_if_exists',\\n       'depends_on',\\n       'on',\\n       'if',\\n@@ -261,6 +503,8 @@ export class HttpClientProvider extends CheckProvider {\\n       'Network access to the endpoint',\\n       'Optional: Transform template for processing response data',\\n       'Optional: Body template for POST/PUT requests',\\n+      'Optional: output_file path to download response to a file',\\n+      'Optional: skip_if_exists (default: true) to enable caching for file downloads',\\n     ];\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/log-check-provider.ts\",\"additions\":19,\"deletions\":8,\"changes\":27,\"patch\":\"diff --git a/src/providers/log-check-provider.ts b/src/providers/log-check-provider.ts\\nindex 9c6332d1..b57f632c 100644\\n--- a/src/providers/log-check-provider.ts\\n+++ b/src/providers/log-check-provider.ts\\n@@ -78,7 +78,8 @@ export class LogCheckProvider extends CheckProvider {\\n       includeDependencies,\\n       includeMetadata,\\n       config.__outputHistory as Map<string, unknown[]> | undefined,\\n-      context\\n+      context,\\n+      config\\n     );\\n \\n     // Render the log message template\\n@@ -100,12 +101,20 @@ export class LogCheckProvider extends CheckProvider {\\n     else if (level === 'debug') logger.debug(logOutput);\\n     else logger.info(logOutput);\\n \\n-    // Return with the log content as custom data for dependent checks\\n-    return {\\n+    // Return with the log content as custom data for dependent checks.\\n+    // For chat-style logs (group: chat), also expose a structured\\n+    // `output.text` field so frontends like Slack can post a clean\\n+    // human-facing message without the level prefix.\\n+    const summary: ReviewSummary & { logOutput: string; output?: unknown } = {\\n       issues: [],\\n-      // Add log output as custom field\\n       logOutput,\\n-    } as ReviewSummary & { logOutput: string };\\n+    };\\n+\\n+    if ((config as any).group === 'chat') {\\n+      (summary as any).output = { text: renderedMessage };\\n+    }\\n+\\n+    return summary;\\n   }\\n \\n   private buildTemplateContext(\\n@@ -115,7 +124,8 @@ export class LogCheckProvider extends CheckProvider {\\n     _includeDependencies: boolean = true,\\n     includeMetadata: boolean = true,\\n     outputHistory?: Map<string, unknown[]>,\\n-    executionContext?: ExecutionContext\\n+    executionContext?: ExecutionContext,\\n+    config?: CheckProviderConfig\\n   ): Record<string, unknown> {\\n     const context: Record<string, unknown> = {};\\n \\n@@ -197,7 +207,8 @@ export class LogCheckProvider extends CheckProvider {\\n     }\\n \\n     // Add workflow inputs if available\\n-    const workflowInputs = executionContext?.workflowInputs || {};\\n+    // Check config first (set by projectWorkflowToGraph), then fall back to executionContext\\n+    const workflowInputs = (config as any)?.workflowInputs || executionContext?.workflowInputs || {};\\n     logger.debug(\\n       `[LogProvider] Adding ${Object.keys(workflowInputs).length} workflow inputs to context`\\n     );\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/script-check-provider.ts\",\"additions\":37,\"deletions\":1,\"changes\":38,\"patch\":\"diff --git a/src/providers/script-check-provider.ts b/src/providers/script-check-provider.ts\\nindex 9825bb69..947dbd73 100644\\n--- a/src/providers/script-check-provider.ts\\n+++ b/src/providers/script-check-provider.ts\\n@@ -59,6 +59,16 @@ export class ScriptCheckProvider extends CheckProvider {\\n       reuseSession?: boolean;\\n     } & import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n+    // Test hook: mock output for this step (short-circuit execution)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = _sessionInfo?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        // Return mock directly as step output\\n+        return { issues: [], output: mock } as ReviewSummary & { output: unknown };\\n+      }\\n+    } catch {}\\n+\\n     const script = String(config.content || '');\\n     const memoryStore = MemoryStore.getInstance();\\n     const ctx = buildProviderTemplateContext(\\n@@ -72,10 +82,36 @@ export class ScriptCheckProvider extends CheckProvider {\\n     // Keep provider quiet by default; no step-specific debug\\n     // (historical ad-hoc logs removed to avoid hardcoding step names).\\n \\n+    // Add workflow inputs to the context\\n+    const inputs = (config as any).workflowInputs || _sessionInfo?.workflowInputs || {};\\n+    (ctx as any).inputs = inputs;\\n+\\n+    // Add environment variables to context (consistent with http-client-provider)\\n+    (ctx as any).env = process.env;\\n+\\n     // Attach synchronous memory ops consistent with memory provider\\n     const { ops, needsSave } = createSyncMemoryOps(memoryStore);\\n     (ctx as any).memory = ops as unknown as Record<string, unknown>;\\n \\n+    // Add helper functions to the context\\n+    (ctx as any).escapeXml = (str: unknown): string => {\\n+      if (str == null) return '';\\n+      return String(str)\\n+        .replace(/&/g, '&amp;')\\n+        .replace(/</g, '&lt;')\\n+        .replace(/>/g, '&gt;')\\n+        .replace(/\\\"/g, '&quot;')\\n+        .replace(/'/g, '&apos;');\\n+    };\\n+\\n+    // Add btoa/atob for base64 encoding/decoding (browser API polyfill)\\n+    (ctx as any).btoa = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'binary').toString('base64');\\n+    };\\n+    (ctx as any).atob = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'base64').toString('binary');\\n+    };\\n+\\n     // Evaluate the script in a secure sandbox (per-execution instance)\\n     const sandbox = this.createSecureSandbox();\\n     let result: unknown;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/workflow-check-provider.ts\",\"additions\":164,\"deletions\":43,\"changes\":207,\"patch\":\"diff --git a/src/providers/workflow-check-provider.ts b/src/providers/workflow-check-provider.ts\\nindex 116d4dff..681703ce 100644\\n--- a/src/providers/workflow-check-provider.ts\\n+++ b/src/providers/workflow-check-provider.ts\\n@@ -67,34 +67,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n   ): Promise<ReviewSummary> {\\n     const cfg = config as CheckProviderConfig & { workflow?: string; config?: string };\\n     const isConfigPathMode = !!cfg.config && !cfg.workflow;\\n+    const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n \\n-    // Test harness support: allow mocking workflow checks as a black box.\\n-    // If a mock is provided for this step, short-circuit nested execution and\\n-    // return a ReviewSummary with optional output field.\\n-    try {\\n-      const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n-      // test-runner passes hooks on execution context\\n-      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n-      if (mock !== undefined) {\\n-        const ms = mock as any;\\n-        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n-        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n-        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n-        const summary: ReviewSummary & { output?: unknown } = {\\n-          issues: issuesArr,\\n-          output: out,\\n-          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n-        } as any;\\n-        return summary;\\n-      }\\n-    } catch {}\\n-\\n-    // Resolve workflow definition\\n+    // Resolve workflow definition FIRST (needed for input preparation and validation)\\n     let workflow: WorkflowDefinition | undefined;\\n     let workflowId = cfg.workflow as string | undefined;\\n \\n     if (isConfigPathMode) {\\n-      const parentCwd = ((context as any)?._parentContext?.workingDirectory ||\\n+      // Use originalWorkingDirectory for config file resolution - workflow configs\\n+      // should be loaded from the original project path, not the sandbox\\n+      const parentCwd = ((context as any)?._parentContext?.originalWorkingDirectory ||\\n+        (context as any)?._parentContext?.workingDirectory ||\\n+        (context as any)?.originalWorkingDirectory ||\\n         (context as any)?.workingDirectory ||\\n         process.cwd()) as string;\\n       workflow = await this.loadWorkflowFromConfigPath(String(cfg.config), parentCwd);\\n@@ -109,9 +93,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       logger.info(`Executing workflow '${workflowId}'`);\\n     }\\n \\n-    // Prepare inputs\\n+    // Prepare inputs - do this BEFORE mock check so we can validate inputs even when mocked\\n+    // This allows tests to assert that the correct inputs were passed to workflow steps\\n     const inputs = await this.prepareInputs(workflow, config, prInfo, dependencyResults);\\n \\n+    // Capture resolved workflow inputs for testing assertions (reuse prompt capture infrastructure)\\n+    // This allows using `prompts` assertions with `provider: 'workflow'` to verify inputs\\n+    try {\\n+      // Serialize inputs to capture for assertions - specifically the context field\\n+      // which contains the rendered template with dependency outputs\\n+      const inputsCapture = Object.entries(inputs)\\n+        .map(([k, v]) => `${k}: ${typeof v === 'string' ? v : JSON.stringify(v)}`)\\n+        .join('\\\\n\\\\n');\\n+      (context as any)?.hooks?.onPromptCaptured?.({\\n+        step: String(stepName),\\n+        provider: 'workflow',\\n+        prompt: inputsCapture,\\n+      });\\n+    } catch {\\n+      // Ignore capture errors - this is only for testing\\n+    }\\n+\\n     // Validate inputs\\n     const validation = this.registry.validateInputs(workflow, inputs);\\n     if (!validation.valid) {\\n@@ -119,6 +121,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       throw new Error(`Invalid workflow inputs: ${errors}`);\\n     }\\n \\n+    // Test harness support: allow mocking workflow checks as a black box.\\n+    // If a mock is provided for this step, short-circuit nested execution and\\n+    // return a ReviewSummary with optional output field.\\n+    // NOTE: This happens AFTER input preparation/validation so tests can assert inputs are correct\\n+    try {\\n+      // test-runner passes hooks on execution context\\n+      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        const ms = mock as any;\\n+        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n+        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n+        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n+        const summary: ReviewSummary & { output?: unknown } = {\\n+          issues: issuesArr,\\n+          output: out,\\n+          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n+        } as any;\\n+        return summary;\\n+      }\\n+    } catch {}\\n+\\n     // Apply overrides to workflow steps if specified\\n     const modifiedWorkflow = this.applyOverrides(workflow, config);\\n \\n@@ -211,6 +234,80 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Extract eventContext for slack/conversation\\n+    const eventContext = config.eventContext || {};\\n+\\n+    // Debug logging for conversation context\\n+    logger.debug(`[WorkflowProvider] prepareInputs for ${workflow.id}`);\\n+    logger.debug(`[WorkflowProvider] eventContext keys: ${Object.keys(eventContext).join(', ') || 'none'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.slack: ${eventContext.slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.conversation: ${(eventContext as any).conversation ? 'present' : 'absent'}`);\\n+\\n+    // Extract slack context (if provided via eventContext.slack)\\n+    const slack = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        const slackCtx = anyCtx?.slack;\\n+        if (slackCtx && typeof slackCtx === 'object') return slackCtx;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Extract unified conversation context across transports (Slack & GitHub)\\n+    const conversation = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        if (anyCtx?.slack?.conversation) return anyCtx.slack.conversation;\\n+        if (anyCtx?.github?.conversation) return anyCtx.github.conversation;\\n+        if (anyCtx?.conversation) return anyCtx.conversation;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Debug logging for extracted context\\n+    logger.debug(`[WorkflowProvider] slack extracted: ${slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] conversation extracted: ${conversation ? 'present' : 'absent'}`);\\n+    if (conversation) {\\n+      logger.debug(`[WorkflowProvider] conversation.messages count: ${Array.isArray((conversation as any).messages) ? (conversation as any).messages.length : 0}`);\\n+    }\\n+\\n+    // Extract output history from config (passed via __outputHistory)\\n+    const outputHistory = (config as any).__outputHistory as Map<string, unknown[]> | undefined;\\n+    const outputs_history: Record<string, unknown[]> = {};\\n+    if (outputHistory) {\\n+      for (const [k, v] of outputHistory.entries()) {\\n+        outputs_history[k] = v;\\n+      }\\n+    }\\n+\\n+    // Build template context with all available data\\n+    // Extract .output from each dependency result so that outputs['step-name'].field works naturally\\n+    // (not outputs['step-name'].output.field)\\n+    const outputsMap: Record<string, unknown> = {};\\n+    logger.debug(`[WorkflowProvider] dependencyResults: ${dependencyResults ? dependencyResults.size : 'undefined'} entries`);\\n+    if (dependencyResults) {\\n+      for (const [key, result] of dependencyResults.entries()) {\\n+        // Extract the output property, or use the whole result if output is undefined\\n+        const extracted = (result as any).output ?? result;\\n+        outputsMap[key] = extracted;\\n+        // Debug: log what we extracted for each dependency\\n+        const extractedKeys = extracted && typeof extracted === 'object' ? Object.keys(extracted).join(', ') : 'not-object';\\n+        logger.debug(`[WorkflowProvider] outputs['${key}']: keys=[${extractedKeys}]`);\\n+      }\\n+    }\\n+    const templateContext = {\\n+      pr: prInfo,\\n+      outputs: outputsMap,\\n+      env: process.env,\\n+      slack,\\n+      conversation,\\n+      outputs_history,\\n+    };\\n+\\n     // Apply user-provided inputs (args)\\n     const userInputs = config.args || config.workflow_inputs; // Support both for compatibility\\n     if (userInputs) {\\n@@ -219,11 +316,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         if (typeof value === 'string') {\\n           // Check if it's a Liquid template\\n           if (value.includes('{{') || value.includes('{%')) {\\n-            inputs[key] = await this.liquid.parseAndRender(value, {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            });\\n+            inputs[key] = await this.liquid.parseAndRender(value, templateContext);\\n+            // Debug: log rendered template value for important inputs\\n+            if (key === 'text' || key === 'question' || key === 'context') {\\n+              const rendered = String(inputs[key]);\\n+              logger.info(`[WorkflowProvider] Rendered '${key}' input (${rendered.length} chars): ${rendered.substring(0, 500)}${rendered.length > 500 ? '...' : ''}`);\\n+            }\\n           } else {\\n             inputs[key] = value;\\n           }\\n@@ -234,11 +332,7 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           inputs[key] = compileAndRun(\\n             sandbox,\\n             exprValue.expression,\\n-            {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            },\\n+            templateContext,\\n             { injectLog: true, logPrefix: `workflow.input.${key}` }\\n           );\\n         } else {\\n@@ -367,6 +461,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n     try {\\n       await childMemory.initialize();\\n     } catch {}\\n+    // Enhanced debug logging for workspace propagation diagnosis\\n+    const parentWorkspace = parentContext?.workspace;\\n+    logger.info(`[WorkflowProvider] Workspace propagation for nested workflow '${workflow.id}':`);\\n+    logger.info(`[WorkflowProvider]   parentContext exists: ${!!parentContext}`);\\n+    logger.info(`[WorkflowProvider]   parentContext.workspace exists: ${!!parentWorkspace}`);\\n+    if (parentWorkspace) {\\n+      logger.info(`[WorkflowProvider]   parentWorkspace.isEnabled(): ${parentWorkspace.isEnabled?.() ?? 'N/A'}`);\\n+      const projectCount = parentWorkspace.listProjects?.()?.length ?? 'N/A';\\n+      logger.info(`[WorkflowProvider]   parentWorkspace project count: ${projectCount}`);\\n+    } else {\\n+      logger.warn(`[WorkflowProvider]   NO WORKSPACE from parent - nested checkouts won't be added to workspace!`);\\n+    }\\n \\n     const childContext = {\\n       mode: 'state-machine' as const,\\n@@ -374,7 +480,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       checks: checksMetadata,\\n       journal: childJournal,\\n       memory: childMemory,\\n+      // For nested workflows we continue to execute inside the same logical\\n+      // working directory as the parent run. When workspace isolation is\\n+      // enabled on the parent engine, its WorkspaceManager is also propagated\\n+      // so that nested checks (AI, git-checkout, etc.) see the same isolated\\n+      // workspace and project symlinks instead of falling back to the Visor\\n+      // repository root.\\n       workingDirectory: parentContext?.workingDirectory || process.cwd(),\\n+      originalWorkingDirectory:\\n+        parentContext?.originalWorkingDirectory ||\\n+        parentContext?.workingDirectory ||\\n+        process.cwd(),\\n+      workspace: parentWorkspace,\\n       // Always use a fresh session for nested workflows to isolate history\\n       sessionId: uuidv4(),\\n       event: parentContext?.event || prInfo.eventType,\\n@@ -500,6 +617,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     } catch {}\\n \\n+    // Create outputs map that directly exposes .output values (not wrapped in { output, issues })\\n+    // This makes {{ outputs['check-name'] }} work naturally without needing .output\\n+    const outputsMap = Object.fromEntries(\\n+      Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n+    );\\n+\\n     for (const output of workflow.outputs) {\\n       if (output.value_js) {\\n         // JavaScript expression\\n@@ -508,10 +631,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           output.value_js,\\n           {\\n             inputs,\\n-            steps: Object.fromEntries(\\n-              Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-            ),\\n-            outputs: flat,\\n+            outputs: outputsMap,\\n+            // Keep 'steps' as alias for backwards compatibility\\n+            steps: outputsMap,\\n             pr: prInfo,\\n           },\\n           { injectLog: true, logPrefix: `workflow.output.${output.name}` }\\n@@ -520,10 +642,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         // Liquid template\\n         outputs[output.name] = await this.liquid.parseAndRender(output.value, {\\n           inputs,\\n-          steps: Object.fromEntries(\\n-            Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-          ),\\n-          outputs: flat,\\n+          outputs: outputsMap,\\n+          // Keep 'steps' as alias for backwards compatibility\\n+          steps: outputsMap,\\n           pr: prInfo,\\n         });\\n       }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/client.ts\",\"additions\":65,\"deletions\":1,\"changes\":66,\"patch\":\"diff --git a/src/slack/client.ts b/src/slack/client.ts\\nindex 292cc358..435f8b0d 100644\\n--- a/src/slack/client.ts\\n+++ b/src/slack/client.ts\\n@@ -142,6 +142,70 @@ export class SlackClient {\\n     }\\n   }\\n \\n+  public readonly files = {\\n+    /**\\n+     * Upload a file to Slack using files.uploadV2 API\\n+     * @param options Upload options including file content, filename, channel, and thread_ts\\n+     */\\n+    uploadV2: async ({\\n+      content,\\n+      filename,\\n+      channel,\\n+      thread_ts,\\n+      title,\\n+      initial_comment,\\n+    }: {\\n+      content: Buffer;\\n+      filename: string;\\n+      channel: string;\\n+      thread_ts?: string;\\n+      title?: string;\\n+      initial_comment?: string;\\n+    }): Promise<{ ok: boolean; file?: { id: string; permalink?: string } }> => {\\n+      try {\\n+        // Step 1: Get upload URL\\n+        const getUrlResp: any = await this.api('files.getUploadURLExternal', {\\n+          filename,\\n+          length: content.length,\\n+        });\\n+        if (!getUrlResp || getUrlResp.ok !== true || !getUrlResp.upload_url) {\\n+          console.warn(`Slack files.getUploadURLExternal failed: ${getUrlResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 2: Upload file content to the URL\\n+        const uploadResp = await fetch(getUrlResp.upload_url, {\\n+          method: 'POST',\\n+          body: content,\\n+        });\\n+        if (!uploadResp.ok) {\\n+          console.warn(`Slack file upload to URL failed: ${uploadResp.status}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 3: Complete the upload and share to channel\\n+        const completeResp: any = await this.api('files.completeUploadExternal', {\\n+          files: [{ id: getUrlResp.file_id, title: title || filename }],\\n+          channel_id: channel,\\n+          thread_ts,\\n+          initial_comment,\\n+        });\\n+        if (!completeResp || completeResp.ok !== true) {\\n+          console.warn(`Slack files.completeUploadExternal failed: ${completeResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        return {\\n+          ok: true,\\n+          file: completeResp.files?.[0] || { id: getUrlResp.file_id },\\n+        };\\n+      } catch (e) {\\n+        console.warn(`Slack file upload failed: ${e instanceof Error ? e.message : String(e)}`);\\n+        return { ok: false };\\n+      }\\n+    },\\n+  };\\n+\\n   getWebClient(): any {\\n     return {\\n       conversations: {\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/markdown.ts\",\"additions\":204,\"deletions\":5,\"changes\":209,\"patch\":\"diff --git a/src/slack/markdown.ts b/src/slack/markdown.ts\\nindex cc63fb5f..9f91c25f 100644\\n--- a/src/slack/markdown.ts\\n+++ b/src/slack/markdown.ts\\n@@ -3,14 +3,191 @@\\n // without pulling in a full Markdown parser.\\n //\\n // Supported conversions:\\n+// - # Header / ## Header  ‚Üí *Header* (bold with visual separation)\\n // - **bold** / __bold__   ‚Üí *bold*\\n // - [label](url)          ‚Üí <url|label>\\n // - ![alt](url)           ‚Üí <url|alt>\\n // - *italic* (inline)     ‚Üí _italic_\\n+// - ```mermaid blocks     ‚Üí rendered to PNG and uploaded to Slack\\n //\\n // Everything else is passed through unchanged; Slack will still render many\\n // Markdown-like constructs (lists, code fences, etc.) natively.\\n \\n+import { spawn } from 'child_process';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n+import * as os from 'os';\\n+\\n+/**\\n+ * Represents an extracted mermaid diagram\\n+ */\\n+export interface MermaidDiagram {\\n+  /** The full match including ```mermaid and ``` */\\n+  fullMatch: string;\\n+  /** The mermaid code content */\\n+  code: string;\\n+  /** Start index in the original text */\\n+  startIndex: number;\\n+  /** End index in the original text */\\n+  endIndex: number;\\n+}\\n+\\n+/**\\n+ * Extract all mermaid code blocks from text\\n+ */\\n+export function extractMermaidDiagrams(text: string): MermaidDiagram[] {\\n+  const diagrams: MermaidDiagram[] = [];\\n+  // Match ```mermaid followed by newline, content, and closing ```\\n+  const regex = /```mermaid\\\\s*\\\\n([\\\\s\\\\S]*?)```/g;\\n+  let match;\\n+  while ((match = regex.exec(text)) !== null) {\\n+    diagrams.push({\\n+      fullMatch: match[0],\\n+      code: match[1].trim(),\\n+      startIndex: match.index,\\n+      endIndex: match.index + match[0].length,\\n+    });\\n+  }\\n+  return diagrams;\\n+}\\n+\\n+/**\\n+ * Render a mermaid diagram to PNG using mmdc CLI (@mermaid-js/mermaid-cli).\\n+ *\\n+ * Requirements:\\n+ * - Node.js and npx must be available in PATH\\n+ * - Network access on first run (npx downloads the package)\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ *\\n+ * On Linux, you may need to install chromium dependencies:\\n+ *   apt-get install -y chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ *\\n+ * On Docker/CI, consider using a base image with puppeteer support or\\n+ * pre-installing @mermaid-js/mermaid-cli globally.\\n+ *\\n+ * @param mermaidCode The mermaid diagram code\\n+ * @returns Buffer containing PNG data, or null if rendering failed\\n+ */\\n+export async function renderMermaidToPng(mermaidCode: string): Promise<Buffer | null> {\\n+  // Create temp files for input and output\\n+  const tmpDir = os.tmpdir();\\n+  const inputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.mmd`);\\n+  const outputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.png`);\\n+\\n+  try {\\n+    // Write mermaid code to temp file\\n+    fs.writeFileSync(inputFile, mermaidCode, 'utf-8');\\n+\\n+    // Detect system chromium for puppeteer (mermaid-cli dependency)\\n+    // Without this, puppeteer may hang trying to download its own chromium\\n+    const chromiumPaths = [\\n+      '/usr/bin/chromium',\\n+      '/usr/bin/chromium-browser',\\n+      '/usr/bin/google-chrome',\\n+      '/usr/bin/chrome',\\n+    ];\\n+    let chromiumPath: string | undefined;\\n+    for (const p of chromiumPaths) {\\n+      if (fs.existsSync(p)) {\\n+        chromiumPath = p;\\n+        break;\\n+      }\\n+    }\\n+\\n+    // Build environment with chromium path if found\\n+    const env = { ...process.env };\\n+    if (chromiumPath) {\\n+      env.PUPPETEER_EXECUTABLE_PATH = chromiumPath;\\n+    }\\n+\\n+    // Run mmdc to render PNG\\n+    const result = await new Promise<{ success: boolean; error?: string }>((resolve) => {\\n+      const proc = spawn('npx', [\\n+        '--yes',\\n+        '@mermaid-js/mermaid-cli',\\n+        '-i', inputFile,\\n+        '-o', outputFile,\\n+        '-e', 'png',\\n+        '-b', 'white',\\n+        '-w', '1200',\\n+      ], {\\n+        timeout: 60000, // 60 second timeout (first run may download packages)\\n+        stdio: ['pipe', 'pipe', 'pipe'],\\n+        env,\\n+      });\\n+\\n+      let stderr = '';\\n+      proc.stderr?.on('data', (data) => {\\n+        stderr += data.toString();\\n+      });\\n+\\n+      proc.on('close', (code) => {\\n+        if (code === 0) {\\n+          resolve({ success: true });\\n+        } else {\\n+          resolve({ success: false, error: stderr || `Exit code ${code}` });\\n+        }\\n+      });\\n+\\n+      proc.on('error', (err) => {\\n+        resolve({ success: false, error: err.message });\\n+      });\\n+    });\\n+\\n+    if (!result.success) {\\n+      console.warn(`Mermaid rendering failed: ${result.error}`);\\n+      return null;\\n+    }\\n+\\n+    // Read the output PNG\\n+    if (!fs.existsSync(outputFile)) {\\n+      console.warn('Mermaid output file not created');\\n+      return null;\\n+    }\\n+\\n+    const pngBuffer = fs.readFileSync(outputFile);\\n+    return pngBuffer;\\n+  } catch (e) {\\n+    console.warn(`Mermaid rendering error: ${e instanceof Error ? e.message : String(e)}`);\\n+    return null;\\n+  } finally {\\n+    // Cleanup temp files\\n+    try {\\n+      if (fs.existsSync(inputFile)) fs.unlinkSync(inputFile);\\n+      if (fs.existsSync(outputFile)) fs.unlinkSync(outputFile);\\n+    } catch {\\n+      // Ignore cleanup errors\\n+    }\\n+  }\\n+}\\n+\\n+/**\\n+ * Replace mermaid blocks in text with a placeholder message\\n+ * @param text Original text\\n+ * @param diagrams Extracted diagrams\\n+ * @param replacement Text to replace each diagram with (or a function that returns replacement for each index)\\n+ */\\n+export function replaceMermaidBlocks(\\n+  text: string,\\n+  diagrams: MermaidDiagram[],\\n+  replacement: string | ((index: number) => string) = '_(See diagram above)_'\\n+): string {\\n+  if (diagrams.length === 0) return text;\\n+\\n+  // Sort by start index descending to replace from end to start (preserves indices)\\n+  const sorted = [...diagrams].sort((a, b) => b.startIndex - a.startIndex);\\n+\\n+  let result = text;\\n+  sorted.forEach((diagram, sortedIndex) => {\\n+    // Calculate original index (since we sorted in reverse)\\n+    const originalIndex = diagrams.length - 1 - sortedIndex;\\n+    const rep = typeof replacement === 'function' ? replacement(originalIndex) : replacement;\\n+    result = result.slice(0, diagram.startIndex) + rep + result.slice(diagram.endIndex);\\n+  });\\n+\\n+  return result;\\n+}\\n+\\n export function markdownToSlack(text: string): string {\\n   if (!text || typeof text !== 'string') return '';\\n \\n@@ -33,7 +210,7 @@ export function markdownToSlack(text: string): string {\\n   out = out.replace(/\\\\*\\\\*([^*]+)\\\\*\\\\*/g, (_m, inner: string) => `*${inner}*`);\\n   out = out.replace(/__([^_]+)__/g, (_m, inner: string) => `*${inner}*`);\\n \\n-  // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation).\\n+  // Process lines for headers and bullet lists.\\n   // Slack's mrkdwn handles \\\"‚Ä¢\\\" bullets more naturally than raw \\\"-\\\" Markdown.\\n   const lines = out.split(/\\\\r?\\\\n/);\\n   let inCodeBlock = false;\\n@@ -46,9 +223,31 @@ export function markdownToSlack(text: string): string {\\n       continue;\\n     }\\n     if (inCodeBlock) continue;\\n-    const match = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n-    if (match) {\\n-      const [, indent, , rest] = match;\\n+\\n+    // Headers: # Header ‚Üí *Header* (Slack doesn't have native headers)\\n+    // Match 1-6 # at start of line, followed by space and text\\n+    const headerMatch = /^(#{1,6})\\\\s+(.+)$/.exec(trimmed);\\n+    if (headerMatch) {\\n+      const [, hashes, headerText] = headerMatch;\\n+      // For h1/h2, add extra emphasis with newline before (if not first line\\n+      // and previous line is not empty/header/code-fence)\\n+      const prevLine = i > 0 ? lines[i - 1].trim() : '';\\n+      const prevIsHeaderOrFence =\\n+        /^#{1,6}\\\\s+/.test(prevLine) ||\\n+        /^\\\\*[^*]+\\\\*$/.test(prevLine) ||\\n+        /^```/.test(prevLine);\\n+      if (hashes.length <= 2 && i > 0 && prevLine !== '' && !prevIsHeaderOrFence) {\\n+        lines[i] = `\\\\n*${headerText.trim()}*`;\\n+      } else {\\n+        lines[i] = `*${headerText.trim()}*`;\\n+      }\\n+      continue;\\n+    }\\n+\\n+    // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation)\\n+    const bulletMatch = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n+    if (bulletMatch) {\\n+      const [, indent, , rest] = bulletMatch;\\n       lines[i] = `${indent}‚Ä¢ ${rest}`;\\n     }\\n   }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/prompt-state.ts\",\"additions\":19,\"deletions\":2,\"changes\":21,\"patch\":\"diff --git a/src/slack/prompt-state.ts b/src/slack/prompt-state.ts\\nindex a13233b2..544777c6 100644\\n--- a/src/slack/prompt-state.ts\\n+++ b/src/slack/prompt-state.ts\\n@@ -95,7 +95,10 @@ export class PromptStateManager {\\n   setFirstMessage(channel: string, threadTs: string, text: string): void {\\n     const key = this.key(channel, threadTs);\\n     if (!text || !text.trim()) return;\\n-    if (!this.firstMessage.has(key)) {\\n+    const existing = this.firstMessage.get(key);\\n+    // Only set if: no entry exists OR the existing entry was already consumed\\n+    // This allows new messages to be captured after a resume cycle\\n+    if (!existing || existing.consumed) {\\n       this.firstMessage.set(key, { text, consumed: false });\\n     }\\n   }\\n@@ -130,6 +133,20 @@ export class PromptStateManager {\\n         removed++;\\n       }\\n     }\\n+    // Also clean up stale firstMessage entries (consumed entries older than TTL)\\n+    // Keep unconsumed entries to avoid losing user messages\\n+    for (const [key] of this.firstMessage.entries()) {\\n+      const waitingInfo = this.waiting.get(key);\\n+      // If no corresponding waiting entry exists and the firstMessage was consumed,\\n+      // the conversation is likely complete - safe to remove\\n+      if (!waitingInfo) {\\n+        const entry = this.firstMessage.get(key);\\n+        if (entry?.consumed) {\\n+          this.firstMessage.delete(key);\\n+          removed++;\\n+        }\\n+      }\\n+    }\\n     if (removed) {\\n       try {\\n         logger.info(`[prompt-state] cleanup removed ${removed} entries`);\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/socket-runner.ts\",\"additions\":56,\"deletions\":32,\"changes\":88,\"patch\":\"diff --git a/src/slack/socket-runner.ts b/src/slack/socket-runner.ts\\nindex d1681a03..5b461d22 100644\\n--- a/src/slack/socket-runner.ts\\n+++ b/src/slack/socket-runner.ts\\n@@ -7,6 +7,7 @@ import { SlackAdapter } from './adapter';\\n import { CachePrewarmer } from './cache-prewarmer';\\n import { RateLimiter, type RateLimitConfig } from './rate-limiter';\\n import type { SlackBotConfig } from '../types/bot';\\n+import { withActiveSpan } from '../telemetry/trace-helpers';\\n \\n type SlackSocketConfig = {\\n   appToken?: string; // xapp- token\\n@@ -30,6 +31,7 @@ export class SlackSocketRunner {\\n   private botUserId?: string;\\n   private processedKeys: Map<string, number> = new Map();\\n   private adapter?: SlackAdapter;\\n+  private retryCount = 0;\\n \\n   constructor(engine: StateMachineExecutionEngine, cfg: VisorConfig, opts: SlackSocketConfig) {\\n     const app = opts.appToken || process.env.SLACK_APP_TOKEN || '';\\n@@ -107,7 +109,10 @@ export class SlackSocketRunner {\\n \\n   private async connect(url: string): Promise<void> {\\n     this.ws = new WebSocket(url);\\n-    this.ws.on('open', () => logger.info('[SlackSocket] WebSocket connected'));\\n+    this.ws.on('open', () => {\\n+      this.retryCount = 0; // Reset on successful connection\\n+      logger.info('[SlackSocket] WebSocket connected');\\n+    });\\n     this.ws.on('close', (code, reason) => {\\n       logger.warn(`[SlackSocket] WebSocket closed: ${code} ${reason}`);\\n       setTimeout(() => this.restart().catch(() => {}), 1000);\\n@@ -123,7 +128,13 @@ export class SlackSocketRunner {\\n       const url = await this.openConnection();\\n       await this.connect(url);\\n     } catch (e) {\\n-      logger.error(`[SlackSocket] Restart failed: ${e instanceof Error ? e.message : e}`);\\n+      this.retryCount++;\\n+      // Exponential backoff: 2s, 4s, 8s, 16s, 32s, capped at 60s\\n+      const delay = Math.min(2000 * Math.pow(2, this.retryCount - 1), 60000);\\n+      logger.error(\\n+        `[SlackSocket] Restart failed (attempt ${this.retryCount}), retrying in ${Math.round(delay / 1000)}s: ${e instanceof Error ? e.message : e}`\\n+      );\\n+      setTimeout(() => this.restart().catch(() => {}), delay);\\n     }\\n   }\\n \\n@@ -296,36 +307,49 @@ export class SlackSocketRunner {\\n           });\\n         }\\n       } catch {}\\n-      if (path) {\\n-        try {\\n-          const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n-          const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n-          logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n-          await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n-            webhookContext: { webhookData: map, eventType: 'manual' },\\n-            debug: process.env.VISOR_DEBUG === 'true',\\n-          });\\n-          if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n-          return; // resume path handled\\n-        } catch (e) {\\n-          logger.warn(\\n-            `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n-              e instanceof Error ? e.message : String(e)\\n-            }`\\n-          );\\n-        }\\n-      }\\n+      try {\\n+        await withActiveSpan(\\n+          'visor.run',\\n+          {\\n+            'visor.run.source': 'slack',\\n+            'slack.event.type': String(type || ''),\\n+            'slack.channel': channelId,\\n+            'slack.thread_ts': threadTs,\\n+          },\\n+          async () => {\\n+            if (path) {\\n+              try {\\n+                const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n+                const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n+                logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n+                await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n+                  webhookContext: { webhookData: map, eventType: 'manual' },\\n+                  debug: process.env.VISOR_DEBUG === 'true',\\n+                });\\n+                return;\\n+              } catch (e) {\\n+                logger.warn(\\n+                  `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n+                    e instanceof Error ? e.message : String(e)\\n+                  }`\\n+                );\\n+              }\\n+            }\\n \\n-      // Cold run (no snapshot)\\n-      await runEngine.executeChecks({\\n-        checks: allChecks,\\n-        showDetails: true,\\n-        outputFormat: 'json',\\n-        config: cfgForRun,\\n-        webhookContext: { webhookData: map, eventType: 'manual' },\\n-        debug: process.env.VISOR_DEBUG === 'true',\\n-      } as any);\\n-      if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+            // Cold run (no snapshot)\\n+            await runEngine.executeChecks({\\n+              checks: allChecks,\\n+              showDetails: true,\\n+              outputFormat: 'json',\\n+              config: cfgForRun,\\n+              webhookContext: { webhookData: map, eventType: 'manual' },\\n+              debug: process.env.VISOR_DEBUG === 'true',\\n+            } as any);\\n+          }\\n+        );\\n+      } finally {\\n+        if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+      }\\n     } catch (e) {\\n       logger.error(\\n         `[SlackSocket] Engine execution failed: ${e instanceof Error ? e.message : String(e)}`\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/context/build-engine-context.ts\",\"additions\":11,\"deletions\":4,\"changes\":15,\"patch\":\"diff --git a/src/state-machine/context/build-engine-context.ts b/src/state-machine/context/build-engine-context.ts\\nindex 149c73bf..45e63b6a 100644\\n--- a/src/state-machine/context/build-engine-context.ts\\n+++ b/src/state-machine/context/build-engine-context.ts\\n@@ -57,7 +57,8 @@ export function buildEngineContextForRun(\\n       ) as EventTrigger[],\\n       group: checkConfig.group,\\n       providerType: checkConfig.type || 'ai',\\n-      dependencies: checkConfig.depends_on || [],\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: Array.isArray(checkConfig.depends_on) ? checkConfig.depends_on : checkConfig.depends_on ? [checkConfig.depends_on] : [],\\n     };\\n   }\\n \\n@@ -134,11 +135,14 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n   const originalPath = context.workingDirectory || process.cwd();\\n \\n   try {\\n+    // Check if workspace should be kept (for debugging)\\n+    const keepWorkspace = process.env.VISOR_KEEP_WORKSPACE === 'true';\\n+\\n     // Create workspace manager\\n     const workspace = WorkspaceManager.getInstance(context.sessionId, originalPath, {\\n       enabled: true,\\n-      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH,\\n-      cleanupOnExit: workspaceConfig?.cleanup_on_exit !== false,\\n+      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH || '/tmp/visor-workspaces',\\n+      cleanupOnExit: keepWorkspace ? false : workspaceConfig?.cleanup_on_exit !== false,\\n     });\\n \\n     // Initialize workspace (creates main project worktree)\\n@@ -151,6 +155,9 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n \\n     logger.info(`[Workspace] Initialized workspace: ${info.workspacePath}`);\\n     logger.debug(`[Workspace] Main project at: ${info.mainProjectPath}`);\\n+    if (keepWorkspace) {\\n+      logger.info(`[Workspace] Keeping workspace after execution (--keep-workspace)`);\\n+    }\\n \\n     return context;\\n   } catch (error) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/execution-invoker.ts\",\"additions\":84,\"deletions\":4,\"changes\":88,\"patch\":\"diff --git a/src/state-machine/dispatch/execution-invoker.ts b/src/state-machine/dispatch/execution-invoker.ts\\nindex eb7587d3..798ca8e4 100644\\n--- a/src/state-machine/dispatch/execution-invoker.ts\\n+++ b/src/state-machine/dispatch/execution-invoker.ts\\n@@ -356,10 +356,17 @@ export async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -413,6 +420,31 @@ export async function executeSingleCheck(\\n     }\\n   }\\n \\n+  // Banner-style log when a check actually starts executing. This is emitted\\n+  // after all gating (if/depends_on) has passed so it only appears for real\\n+  // runs, and gives a clear visual separator in the logs between checks.\\n+  try {\\n+    const wave = state.wave;\\n+    const level = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${level}) ‚îÅ‚îÅ‚îÅ`;\\n+\\n+    // When running in a TTY, colour the entire banner line for extra\\n+    // visibility; keep plain text for JSON/SARIF or non-TTY environments.\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   let forEachParent: string | undefined;\\n   let forEachItems: unknown[] | undefined;\\n   for (const depId of depList) {\\n@@ -563,13 +595,56 @@ export async function executeSingleCheck(\\n       files: [],\\n       commits: [],\\n     };\\n+    // Derive AI session reuse context for this check (self-mode only for now).\\n+    // When reuse_ai_session: 'self' is configured, look up the last root-scope\\n+    // journal entry for this check in the current engine session and expose its\\n+    // sessionId via ExecutionContext so ai-check-provider can call\\n+    // executeReviewWithSessionReuse() against the same ProbeAgent session.\\n+    let parentSessionId: string | undefined;\\n+    let reuseSession = false;\\n+    try {\\n+      const reuseCfg: unknown = (checkConfig as any).reuse_ai_session;\\n+      if (reuseCfg === 'self') {\\n+        const snapshotId = context.journal.beginSnapshot();\\n+        const visible = context.journal.readVisible(\\n+          context.sessionId,\\n+          snapshotId,\\n+          context.event as any\\n+        );\\n+        // Prefer the most recent root-scope result for this check\\n+        const prior = visible.filter(\\n+          e => e.checkId === checkId && (!e.scope || e.scope.length === 0)\\n+        );\\n+        if (prior.length > 0) {\\n+          const last = prior[prior.length - 1];\\n+          const sess = (last.result as any)?.sessionId;\\n+          if (typeof sess === 'string' && sess.length > 0) {\\n+            parentSessionId = sess;\\n+            reuseSession = true;\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only ‚Äì fall back to normal (non-reuse) execution on error.\\n+      parentSessionId = undefined;\\n+      reuseSession = false;\\n+    }\\n+\\n     const executionContext = {\\n       ...context.executionContext,\\n       _engineMode: context.mode,\\n       _parentContext: context,\\n       _parentState: state,\\n+      // Explicitly propagate workspace reference for nested workflows\\n+      workspace: context.workspace,\\n     };\\n \\n+    // Attach session reuse hints for providers that support them (AI, Claude Code, etc).\\n+    if (reuseSession && parentSessionId) {\\n+      (executionContext as any).parentSessionId = parentSessionId;\\n+      (executionContext as any).reuseSession = true;\\n+    }\\n+\\n     // Handle on_init lifecycle hook BEFORE main execution\\n     if (checkConfig.on_init) {\\n       try {\\n@@ -613,7 +688,12 @@ export async function executeSingleCheck(\\n \\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/foreach-processor.ts\",\"additions\":27,\"deletions\":2,\"changes\":29,\"patch\":\"diff --git a/src/state-machine/dispatch/foreach-processor.ts b/src/state-machine/dispatch/foreach-processor.ts\\nindex 336bf9df..ab630986 100644\\n--- a/src/state-machine/dispatch/foreach-processor.ts\\n+++ b/src/state-machine/dispatch/foreach-processor.ts\\n@@ -77,6 +77,26 @@ export async function executeCheckWithForEachItems(\\n   const allContents: string[] = [];\\n   const perIterationDurations: number[] = [];\\n \\n+  // Emit a banner for the forEach parent so logs clearly show when we are\\n+  // entering the aggregated execution for that step.\\n+  try {\\n+    const wave = state.wave;\\n+    const lvl = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${lvl}, forEach parent) ‚îÅ‚îÅ‚îÅ`;\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   for (let itemIndex = 0; itemIndex < forEachItems.length; itemIndex++) {\\n     const iterationStartMs = Date.now();\\n     const scope: Array<{ check: string; index: number }> = [\\n@@ -248,7 +268,12 @@ export async function executeCheckWithForEachItems(\\n \\n       const result = await withActiveSpan(\\n         `visor.check.${checkId}`,\\n-        { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+        {\\n+          'visor.check.id': checkId,\\n+          'visor.check.type': providerType,\\n+          session_id: context.sessionId,\\n+          wave: state.wave,\\n+        },\\n         async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n       );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/stats-manager.ts\",\"additions\":21,\"deletions\":1,\"changes\":22,\"patch\":\"diff --git a/src/state-machine/dispatch/stats-manager.ts b/src/state-machine/dispatch/stats-manager.ts\\nindex 8d04436e..2f317b59 100644\\n--- a/src/state-machine/dispatch/stats-manager.ts\\n+++ b/src/state-machine/dispatch/stats-manager.ts\\n@@ -44,6 +44,26 @@ export function updateStats(\\n       issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n     };\\n \\n+    // Respect explicit skip markers on the result (set by LevelDispatch).\\n+    const skippedMarker = (result as any).__skipped;\\n+    if (skippedMarker) {\\n+      stats.skipped = true;\\n+      stats.skipReason =\\n+        typeof skippedMarker === 'string'\\n+          ? (skippedMarker as any)\\n+          : (stats.skipReason as any) || 'if_condition';\\n+      // A skipped result should not contribute to run counters; reset\\n+      // them explicitly to guard against any prior values lingering\\n+      // from earlier runs of the same check.\\n+      stats.totalRuns = 0;\\n+      stats.successfulRuns = 0;\\n+      stats.failedRuns = 0;\\n+      stats.skippedRuns++;\\n+      // Do not count this as a run; keep totalRuns/failed/success unchanged.\\n+      state.stats.set(checkId, stats);\\n+      continue;\\n+    }\\n+\\n     // If this check was previously marked as skipped and now executed,\\n     // clear the skipped flag and any skip counters to ensure the run is visible.\\n     if (stats.skipped) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/runner.ts\",\"additions\":10,\"deletions\":7,\"changes\":17,\"patch\":\"diff --git a/src/state-machine/runner.ts b/src/state-machine/runner.ts\\nindex ee86b876..a13a5efd 100644\\n--- a/src/state-machine/runner.ts\\n+++ b/src/state-machine/runner.ts\\n@@ -124,14 +124,17 @@ export class StateMachineRunner {\\n    */\\n   private async executeState(state: EngineState): Promise<void> {\\n     // M4: Wrap state execution in OTEL span\\n+    const attrs: Record<string, unknown> = {\\n+      state: state,\\n+      engine_mode: this.context.mode,\\n+      wave: this.state.wave,\\n+      session_id: this.context.sessionId,\\n+    };\\n+    const waveKind = (this.state as any)?.flags?.waveKind;\\n+    if (waveKind) attrs.wave_kind = waveKind;\\n     return withActiveSpan(\\n       `engine.state.${state.toLowerCase()}`,\\n-      {\\n-        state: state,\\n-        engine_mode: this.context.mode,\\n-        wave: this.state.wave,\\n-        session_id: this.context.sessionId,\\n-      },\\n+      attrs,\\n       async () => {\\n         try {\\n           switch (state) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/level-dispatch.ts\",\"additions\":100,\"deletions\":15,\"changes\":115,\"patch\":\"diff --git a/src/state-machine/states/level-dispatch.ts b/src/state-machine/states/level-dispatch.ts\\nindex 5588b0f5..42ee383e 100644\\n--- a/src/state-machine/states/level-dispatch.ts\\n+++ b/src/state-machine/states/level-dispatch.ts\\n@@ -26,7 +26,7 @@ import type { CheckExecutionStats } from '../../types/execution';\\n import type { CheckProviderConfig } from '../../providers/check-provider.interface';\\n import type { CheckConfig } from '../../types/config';\\n import { handleRouting, checkLoopBudget } from './routing';\\n-import { withActiveSpan } from '../../telemetry/trace-helpers';\\n+import { withActiveSpan, setSpanAttributes } from '../../telemetry/trace-helpers';\\n import { emitMermaidFromMarkdown } from '../../utils/mermaid-telemetry';\\n import { emitNdjsonSpanWithEvents, emitNdjsonFallback } from '../../telemetry/fallback-ndjson';\\n import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n@@ -121,12 +121,13 @@ async function evaluateIfCondition(\\n         return false;\\n       }\\n     })();\\n+    // Steps with dependencies should always see outputs from all completed steps.\\n     // In forward-run waves (from on_success/on_fail goto), guards should see the\\n     // latest global outputs even if the check has no explicit dependencies.\\n     // In wave-retry (from on_finish), restrict to checks with dependencies to\\n     // avoid wrongly skipping top-level prompts like 'ask'.\\n     const useGlobalOutputs =\\n-      (useGlobalOutputsFlag && waveKind === 'forward') || (useGlobalOutputsFlag && hasDeps);\\n+      hasDeps || (useGlobalOutputsFlag && waveKind === 'forward');\\n \\n     if (useGlobalOutputs) {\\n       // Forward-run wave: allow guards to consult latest outputs from the entire\\n@@ -199,6 +200,7 @@ async function evaluateIfCondition(\\n       baseBranch: (context.prInfo as any)?.baseBranch,\\n       filesChanged: context.prInfo?.files?.map(f => f.filename),\\n       environment: envSnapshot,\\n+      workflowInputs: (context.config as any).workflow_inputs || {},\\n     };\\n \\n     const shouldRun = await evaluator.evaluateIfCondition(checkId, ifExpression, contextData);\\n@@ -238,6 +240,7 @@ export async function handleLevelDispatch(\\n   // Update current level tracking\\n   state.currentLevel = level.level;\\n   state.currentLevelChecks = new Set(level.parallel);\\n+  setSpanAttributes({ level_size: level.parallel.length });\\n \\n   // Emit level ready event\\n   emitEvent({ type: 'LevelReady', level, wave: state.wave });\\n@@ -689,6 +692,39 @@ async function executeCheckWithForEachItems(\\n         }\\n       } catch {}\\n \\n+      // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+      // The socket-runner stores conversation data in webhookData under the endpoint key\\n+      try {\\n+        const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+        const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+        if (context.debug) {\\n+          logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+        }\\n+        if (webhookData && webhookData.size > 0) {\\n+          // Find the payload with slack_conversation\\n+          for (const payload of webhookData.values()) {\\n+            const slackConv = (payload as any)?.slack_conversation;\\n+            if (slackConv) {\\n+              // Build slack context with event and conversation\\n+              const event = (payload as any)?.event;\\n+              const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+              if (context.debug) {\\n+                logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+              }\\n+              (providerConfig as any).eventContext = {\\n+                ...(providerConfig as any).eventContext,\\n+                slack: {\\n+                  event: event || {},\\n+                  conversation: slackConv,\\n+                },\\n+                conversation: slackConv, // Also expose at top level for convenience\\n+              };\\n+              break;\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n+\\n       // Build dependency results with scope\\n       const dependencyResults = buildDependencyResultsWithScope(\\n         checkId,\\n@@ -820,15 +856,17 @@ async function executeCheckWithForEachItems(\\n       } catch {}\\n \\n       // Execute provider with telemetry\\n-      const itemResult = await withActiveSpan(\\n-        `visor.check.${checkId}`,\\n-        {\\n-          'visor.check.id': checkId,\\n-          'visor.check.type': providerType,\\n-          'visor.foreach.index': itemIndex,\\n-        },\\n-        async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n-      );\\n+    const itemResult = await withActiveSpan(\\n+      `visor.check.${checkId}`,\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        'visor.foreach.index': itemIndex,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n+      async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n+    );\\n \\n       // Enrich issues\\n       const enrichedIssues = (itemResult.issues || []).map((issue: ReviewIssue) => ({\\n@@ -942,6 +980,7 @@ async function executeCheckWithForEachItems(\\n             const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n               previousResults: dependencyResults as any,\\n               event: context.event || 'manual',\\n+              output: output, // Pass the iteration output for guarantee evaluation\\n             } as any);\\n             if (!holds) {\\n               const issue: ReviewIssue = {\\n@@ -1618,10 +1657,17 @@ async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -1923,6 +1969,39 @@ async function executeSingleCheck(\\n       }\\n     } catch {}\\n \\n+    // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+    // The socket-runner stores conversation data in webhookData under the endpoint key\\n+    try {\\n+      const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+      const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+      if (context.debug) {\\n+        logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+      }\\n+      if (webhookData && webhookData.size > 0) {\\n+        // Find the payload with slack_conversation\\n+        for (const payload of webhookData.values()) {\\n+          const slackConv = (payload as any)?.slack_conversation;\\n+          if (slackConv) {\\n+            // Build slack context with event and conversation\\n+            const event = (payload as any)?.event;\\n+            const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+            if (context.debug) {\\n+              logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+            }\\n+            (providerConfig as any).eventContext = {\\n+              ...(providerConfig as any).eventContext,\\n+              slack: {\\n+                event: event || {},\\n+                conversation: slackConv,\\n+              },\\n+              conversation: slackConv, // Also expose at top level for convenience\\n+            };\\n+            break;\\n+          }\\n+        }\\n+      }\\n+    } catch {}\\n+\\n     // Build dependency results\\n     const dependencyResults = buildDependencyResults(checkId, checkConfig, context, state);\\n \\n@@ -2016,7 +2095,12 @@ async function executeSingleCheck(\\n     // Execute provider with telemetry\\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n@@ -2098,6 +2182,7 @@ async function executeSingleCheck(\\n           const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n             previousResults: dependencyResults as any,\\n             event: context.event || 'manual',\\n+            output: enrichedResult.output,\\n           } as any);\\n           if (!holds) {\\n             const issue: ReviewIssue = {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/wave-planning.ts\",\"additions\":10,\"deletions\":3,\"changes\":13,\"patch\":\"diff --git a/src/state-machine/states/wave-planning.ts b/src/state-machine/states/wave-planning.ts\\nindex 7551d55c..7b42f043 100644\\n--- a/src/state-machine/states/wave-planning.ts\\n+++ b/src/state-machine/states/wave-planning.ts\\n@@ -351,8 +351,11 @@ export async function handleWavePlanning(\\n     for (const id of checksToRun) {\\n       // Only include dependencies that are within the same subset (often none).\\n       const cfg = context.config.checks?.[id];\\n-      const deps = (cfg?.depends_on || []).filter((d: string) => checksToRun.includes(d));\\n-      subDeps[id] = deps as string[];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = cfg?.depends_on;\\n+      const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n+      const deps = depsArray.filter((d: string) => checksToRun.includes(d));\\n+      subDeps[id] = deps;\\n     }\\n \\n     const subGraph = DependencyResolver.buildDependencyGraph(subDeps);\\n@@ -410,6 +413,10 @@ export async function handleWavePlanning(\\n     // Initialize current wave state\\n     (state as any).currentWaveCompletions = new Set<string>();\\n     (state as any).failedChecks = new Set<string>();\\n+    try {\\n+      (state as any).flags = (state as any).flags || {};\\n+      (state as any).flags.waveKind = 'initial';\\n+    } catch {}\\n   }\\n \\n   // Check if there are levels to execute\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/workflow-projection.ts\",\"additions\":3,\"deletions\":2,\"changes\":5,\"patch\":\"diff --git a/src/state-machine/workflow-projection.ts b/src/state-machine/workflow-projection.ts\\nindex b2d5eb24..2bcfd858 100644\\n--- a/src/state-machine/workflow-projection.ts\\n+++ b/src/state-machine/workflow-projection.ts\\n@@ -53,7 +53,8 @@ export function projectWorkflowToGraph(\\n       triggers: step.on || workflow.on || [],\\n       group: step.group,\\n       providerType: step.type || 'ai',\\n-      dependencies: (step.depends_on || []).map(dep => dep),\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: (Array.isArray(step.depends_on) ? step.depends_on : step.depends_on ? [step.depends_on] : []).map((dep: string) => dep),\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/flow-stage.ts\",\"additions\":37,\"deletions\":4,\"changes\":41,\"patch\":\"diff --git a/src/test-runner/core/flow-stage.ts b/src/test-runner/core/flow-stage.ts\\nindex a14256c2..a587f9f1 100644\\n--- a/src/test-runner/core/flow-stage.ts\\n+++ b/src/test-runner/core/flow-stage.ts\\n@@ -352,10 +352,30 @@ export class FlowStage {\\n       } catch {}\\n       // Prefer authoritative counts from res.statistics when available, then fall back to deltas/inferred\\n       const fromResStats: Record<string, number> = {};\\n+      const resSkipped: Record<string, boolean> = {};\\n       try {\\n         for (const s of (res.statistics?.checks || []) as any[]) {\\n           const n = (s && s.checkName) || '';\\n-          if (typeof n === 'string' && n) fromResStats[n] = (s.totalRuns || 0) as number;\\n+          if (typeof n === 'string' && n) {\\n+            fromResStats[n] = (s.totalRuns || 0) as number;\\n+            resSkipped[n] = !!(s as any).skipped || !!(s as any).skipReason;\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      // Steps that have explicit expectations in this stage. We will use\\n+      // this to avoid inflating run counts (via parent-alignment heuristics)\\n+      // for checks that are not even expected in this stage.\\n+      const expectedSteps = new Set<string>();\\n+      const expectedZero = new Set<string>();\\n+      try {\\n+        const expCalls = ((stage.expect || {}).calls || []) as Array<{ step?: string }>;\\n+        for (const c of expCalls) {\\n+          if (c && typeof c.step === 'string' && c.step) expectedSteps.add(c.step);\\n+          try {\\n+            if (c && typeof c.step === 'string' && c.step && (c as any).exactly === 0)\\n+              expectedZero.add(c.step);\\n+          } catch {}\\n         }\\n       } catch {}\\n \\n@@ -410,10 +430,15 @@ export class FlowStage {\\n           const base = stageOnly ? 0 : statBase[name] || 0;\\n           const d = Math.max(0, resTotal - base);\\n           runs = d > 0 ? d : 0;\\n+          // If the engine marked this check as skipped, force runs to 0\\n+          // so parent-alignment heuristics below do not inflate it.\\n+          if (resSkipped[name]) {\\n+            runs = 0;\\n+          }\\n         } else {\\n           runs = deltaMap[name] !== undefined ? deltaMap[name] : inferred;\\n         }\\n-        if (runs === 0 && presentInResults.has(name)) runs = 1;\\n+        if (runs === 0 && presentInResults.has(name) && !resSkipped[name]) runs = 1;\\n         if (!isForEachLike && histRuns > 0) runs = histRuns;\\n         // Only use per-item history counts for non-forEach checks. For forEach parents,\\n         // use aggregated totals from res.statistics to reflect number of parent executions.\\n@@ -442,7 +467,15 @@ export class FlowStage {\\n             parentMax = Math.max(parentMax, dP);\\n           }\\n           // Apply only for non-forEach checks with no observable history in this stage\\n-          if (!isForEachLike && histRuns === 0 && histPerItemRuns === 0 && parentMax > 0) {\\n+          if (\\n+            !isForEachLike &&\\n+            histRuns === 0 &&\\n+            histPerItemRuns === 0 &&\\n+            parentMax > 0 &&\\n+            !resSkipped[name] &&\\n+            (expectedSteps.size === 0 || expectedSteps.has(name)) &&\\n+            !expectedZero.has(name)\\n+          ) {\\n             runs = Math.max(runs, parentMax);\\n           }\\n         } catch {}\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/test-execution-wrapper.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/test-runner/core/test-execution-wrapper.ts b/src/test-runner/core/test-execution-wrapper.ts\\nindex 254d0970..d0333152 100644\\n--- a/src/test-runner/core/test-execution-wrapper.ts\\n+++ b/src/test-runner/core/test-execution-wrapper.ts\\n@@ -31,6 +31,8 @@ export class TestExecutionWrapper {\\n       const prev: any = (this.engine as any).executionContext || {};\\n       const merged = {\\n         ...prev,\\n+        // Inject workflow inputs for template access via {{ inputs.* }}\\n+        workflowInputs: cfg.workflow_inputs || prev.workflowInputs || {},\\n         mode: {\\n           ...(prev.mode || {}),\\n           test: true,\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/evaluators.ts\",\"additions\":67,\"deletions\":2,\"changes\":69,\"patch\":\"diff --git a/src/test-runner/evaluators.ts b/src/test-runner/evaluators.ts\\nindex 4843163a..75e9c174 100644\\n--- a/src/test-runner/evaluators.ts\\n+++ b/src/test-runner/evaluators.ts\\n@@ -252,6 +252,69 @@ export function evaluatePrompts(\\n   }\\n }\\n \\n+/**\\n+ * Evaluate workflow_output assertions against computed workflow outputs.\\n+ * Similar to evaluateOutputs but tests workflow-level outputs (defined in outputs: section)\\n+ * rather than step outputs.\\n+ */\\n+export function evaluateWorkflowOutputs(\\n+  errors: string[],\\n+  expect: ExpectBlock,\\n+  workflowOutputs: Record<string, unknown> | undefined\\n+): void {\\n+  const expectations = (expect as any).workflow_output;\\n+  if (!Array.isArray(expectations) || expectations.length === 0) return;\\n+  if (!workflowOutputs) {\\n+    errors.push('workflow_output assertions present but no workflow outputs computed');\\n+    return;\\n+  }\\n+\\n+  for (const o of expectations) {\\n+    const path = o.path as string;\\n+    if (!path) {\\n+      errors.push('workflow_output assertion missing path');\\n+      continue;\\n+    }\\n+    const v = deepGet(workflowOutputs, path);\\n+    if (o.equals !== undefined && !deepEqual(v, o.equals)) {\\n+      errors.push(\\n+        `Workflow output ${path} expected ${JSON.stringify(o.equals)} but got ${JSON.stringify(v)}`\\n+      );\\n+    }\\n+    if (o.equalsDeep !== undefined && !deepEqual(v, o.equalsDeep)) {\\n+      errors.push(`Workflow output ${path} deepEquals failed`);\\n+    }\\n+    if (o.matches && !parseRegex(o.matches).test(String(v))) {\\n+      errors.push(`Workflow output ${path} does not match ${o.matches}`);\\n+    }\\n+    if (o.contains) {\\n+      const contents = Array.isArray(o.contains) ? o.contains : [o.contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (!strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} expected to contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.not_contains) {\\n+      const contents = Array.isArray(o.not_contains) ? o.not_contains : [o.not_contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} should not contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.contains_unordered) {\\n+      if (!Array.isArray(v)) {\\n+        errors.push(`Workflow output ${path} not an array for contains_unordered`);\\n+      } else if (!containsUnordered(v as unknown[], o.contains_unordered)) {\\n+        errors.push(`Workflow output ${path} missing elements (unordered)`);\\n+      }\\n+    }\\n+  }\\n+}\\n+\\n export function evaluateOutputs(\\n   errors: string[],\\n   expect: ExpectBlock,\\n@@ -332,7 +395,8 @@ export function evaluateCase(\\n   strict: boolean,\\n   promptsByStep: Record<string, string[]>,\\n   _results: GroupedResults,\\n-  outputHistory: Record<string, unknown[]>\\n+  outputHistory: Record<string, unknown[]>,\\n+  workflowOutputs?: Record<string, unknown>\\n ): string[] {\\n   const errors: string[] = [];\\n   const executed = buildExecutedMap(stats);\\n@@ -350,5 +414,6 @@ export function evaluateCase(\\n   evaluateNoCalls(errors, expect, executed, recorder, slackRecorder);\\n   evaluatePrompts(errors, expect, promptsByStep);\\n   evaluateOutputs(errors, expect, outputHistory);\\n+  evaluateWorkflowOutputs(errors, expect, workflowOutputs);\\n   return errors;\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/index.ts\",\"additions\":112,\"deletions\":7,\"changes\":119,\"patch\":\"diff --git a/src/test-runner/index.ts b/src/test-runner/index.ts\\nindex 29916a16..4dcc3331 100644\\n--- a/src/test-runner/index.ts\\n+++ b/src/test-runner/index.ts\\n@@ -159,6 +159,7 @@ export async function runSuites(\\n   options: {\\n     only?: string;\\n     bail?: boolean;\\n+    noMocks?: boolean;\\n     maxParallelSuites?: number;\\n     maxParallel?: number;\\n     promptMaxChars?: number;\\n@@ -225,6 +226,7 @@ export async function runSuites(\\n       const r = await runner.runCases(fp, suite as TestSuite, {\\n         only: options.only,\\n         bail: options.bail,\\n+        noMocks: options.noMocks,\\n         maxParallel: options.maxParallel,\\n         promptMaxChars: options.promptMaxChars,\\n       });\\n@@ -285,7 +287,8 @@ export class VisorTestRunner {\\n     defaultPromptCap?: number,\\n     ghRec?: { error_code?: number; timeout_ms?: number },\\n     defaultIncludeTags?: string[] | undefined,\\n-    defaultExcludeTags?: string[] | undefined\\n+    defaultExcludeTags?: string[] | undefined,\\n+    noMocks?: boolean\\n   ): {\\n     name: string;\\n     strict: boolean;\\n@@ -399,9 +402,11 @@ export class VisorTestRunner {\\n               : info.prompt;\\n           prompts[k].push(p);\\n         },\\n-        mockForStep: (step: string) => mockMgr.get(step),\\n+        // In noMocks mode, always return undefined to let real providers execute\\n+        mockForStep: (step: string) => noMocks ? undefined : mockMgr.get(step),\\n         // Ensure human-input never blocks tests: prefer case mock, then default value\\n         onHumanInput: async (req: { checkId: string; default?: string }) => {\\n+          if (noMocks) return (req.default ?? '').toString();\\n           const m = mockMgr.get(req.checkId);\\n           if (m !== undefined && m !== null) return String(m);\\n           return (req.default ?? '').toString();\\n@@ -481,6 +486,60 @@ export class VisorTestRunner {\\n     if (!checks || checks.length === 0) return;\\n     console.log(`  checks: ${checks.join(', ')}`);\\n   }\\n+  /**\\n+   * Compute workflow outputs from output definitions and step results.\\n+   * Evaluates value_js and value (Liquid) expressions to produce output values.\\n+   */\\n+  private computeWorkflowOutputs(\\n+    outputDefs: Array<{ name: string; value?: string; value_js?: string }>,\\n+    outputHistory: Record<string, unknown[]>,\\n+    results: any[]\\n+  ): Record<string, unknown> {\\n+    const computed: Record<string, unknown> = {};\\n+    // Build a steps map from outputHistory (last output for each step)\\n+    const steps: Record<string, unknown> = {};\\n+    for (const [stepId, hist] of Object.entries(outputHistory)) {\\n+      if (Array.isArray(hist) && hist.length > 0) {\\n+        steps[stepId] = hist[hist.length - 1];\\n+      }\\n+    }\\n+    // Build outputs map from history (for {{ outputs[\\\"step\\\"] }} syntax)\\n+    const outputs: Record<string, unknown> = { ...steps };\\n+\\n+    for (const outputDef of outputDefs) {\\n+      if (!outputDef.name) continue;\\n+      try {\\n+        if (outputDef.value_js) {\\n+          // Evaluate JavaScript expression\\n+          const fn = new Function('steps', 'outputs', 'results', `\\n+            try {\\n+              ${outputDef.value_js}\\n+            } catch (e) {\\n+              return undefined;\\n+            }\\n+          `);\\n+          computed[outputDef.name] = fn(steps, outputs, results);\\n+        } else if (outputDef.value) {\\n+          // Evaluate Liquid template\\n+          const { Liquid } = require('liquidjs');\\n+          const engine = new Liquid();\\n+          const rendered = engine.parseAndRenderSync(outputDef.value, { steps, outputs, results });\\n+          // Try to parse as JSON, otherwise keep as string\\n+          try {\\n+            computed[outputDef.name] = JSON.parse(rendered);\\n+          } catch {\\n+            computed[outputDef.name] = rendered.trim();\\n+          }\\n+        }\\n+      } catch (e) {\\n+        // Skip outputs that fail to compute\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.log(`  ‚ö†Ô∏è Failed to compute output '${outputDef.name}': ${e}`);\\n+        }\\n+      }\\n+    }\\n+    return computed;\\n+  }\\n   /**\\n    * Locate a tests file: explicit path > ./.visor.tests.yaml > defaults/visor.tests.yaml\\n    */\\n@@ -606,7 +665,7 @@ export class VisorTestRunner {\\n   public async runCases(\\n     testsPath: string,\\n     suite: TestSuite,\\n-    options: { only?: string; bail?: boolean; maxParallel?: number; promptMaxChars?: number }\\n+    options: { only?: string; bail?: boolean; noMocks?: boolean; maxParallel?: number; promptMaxChars?: number }\\n   ): Promise<{\\n     failures: number;\\n     results: Array<{\\n@@ -748,9 +807,14 @@ export class VisorTestRunner {\\n     const __keepAlive = setInterval(() => {}, 1000);\\n     // Header: show suite path for clarity\\n     let __suiteRel = testsPath;\\n+    const noMocksMode = options.noMocks || false;\\n     try {\\n       __suiteRel = path.relative(this.cwd, testsPath) || testsPath;\\n       console.log(`Suite: ${__suiteRel}`);\\n+      if (noMocksMode) {\\n+        console.log(this.color('üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)', '33'));\\n+        console.log(this.gray('   Step outputs will be captured and printed as suggested mocks\\\\n'));\\n+      }\\n     } catch {}\\n     const runOne = async (_case: any): Promise<{ name: string; failed: number }> => {\\n       // Case header for clarity\\n@@ -805,6 +869,11 @@ export class VisorTestRunner {\\n           cfgLocal.checks[name] = chk;\\n         }\\n       }\\n+      // Workflow testing: inject workflow_input into config for template access\\n+      const workflowInput = (_case as any).workflow_input;\\n+      if (workflowInput && typeof workflowInput === 'object') {\\n+        (cfgLocal as any).workflow_inputs = workflowInput;\\n+      }\\n       const setup = this.setupTestCase(\\n         _case,\\n         cfgLocal,\\n@@ -812,7 +881,8 @@ export class VisorTestRunner {\\n         defaultPromptCap,\\n         ghRec,\\n         defaultIncludeTags,\\n-        defaultExcludeTags\\n+        defaultExcludeTags,\\n+        noMocksMode\\n       );\\n       try {\\n         this.printSelectedChecks(setup.checksToRun);\\n@@ -826,6 +896,24 @@ export class VisorTestRunner {\\n         } catch {}\\n         const exec = await this.executeTestCase(setup, cfgLocal);\\n         const res = exec.res;\\n+\\n+        // In no-mocks mode, print captured outputs as suggested mocks\\n+        if (noMocksMode && Object.keys(exec.outHistory).length > 0) {\\n+          console.log(this.color('\\\\nüìã Suggested mocks (copy to your test case):', '36'));\\n+          console.log(this.gray('mocks:'));\\n+          for (const [stepName, outputs] of Object.entries(exec.outHistory)) {\\n+            if (!Array.isArray(outputs) || outputs.length === 0) continue;\\n+            // Use the last output for the step\\n+            const lastOutput = outputs[outputs.length - 1];\\n+            // Format as YAML with proper indentation\\n+            const yamlOutput = yaml.dump({ [stepName]: lastOutput }, { indent: 2, lineWidth: 120, noRefs: true });\\n+            // Indent the YAML output for proper nesting under 'mocks:'\\n+            const indented = yamlOutput.split('\\\\n').map(line => '  ' + line).join('\\\\n');\\n+            console.log(indented);\\n+          }\\n+          console.log('');\\n+        }\\n+\\n         if (process.env.VISOR_DEBUG === 'true') {\\n           try {\\n             const names = (res.statistics.checks || []).map(\\n@@ -836,6 +924,21 @@ export class VisorTestRunner {\\n         }\\n         // avoid printing raw history keys each case\\n         // (fallback for on_finish static targets handled inside executeTestCase)\\n+        // Workflow testing: compute workflow outputs if workflow has outputs defined\\n+        let workflowOutputs: Record<string, unknown> | undefined;\\n+        if (Array.isArray((cfgLocal as any).outputs)) {\\n+          try {\\n+            workflowOutputs = this.computeWorkflowOutputs(\\n+              (cfgLocal as any).outputs,\\n+              exec.outHistory,\\n+              res.results\\n+            );\\n+          } catch (e) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ‚ö†Ô∏è Error computing workflow outputs: ${e}`);\\n+            }\\n+          }\\n+        }\\n         const caseFailures = require('./evaluators').evaluateCase(\\n           _case.name,\\n           res.statistics,\\n@@ -845,7 +948,8 @@ export class VisorTestRunner {\\n           setup.strict,\\n           setup.prompts,\\n           res.results,\\n-          exec.outHistory\\n+          exec.outHistory,\\n+          workflowOutputs\\n         );\\n         // Warn about unmocked AI/command steps that executed\\n         try {\\n@@ -1280,7 +1384,8 @@ export class VisorTestRunner {\\n   ): void {\\n     const executed: Record<string, number> = {};\\n     for (const s of stats.checks) {\\n-      if (!s.skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n+      const skipped = (s as any).skipped === true || !!(s as any).skipReason;\\n+      if (!skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n     }\\n     const expCalls = (expect.calls || []).filter(c => c.step);\\n     const expectedSteps = new Map<\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/validator.ts\",\"additions\":48,\"deletions\":2,\"changes\":50,\"patch\":\"diff --git a/src/test-runner/validator.ts b/src/test-runner/validator.ts\\nindex dbb890c5..5ffff5ee 100644\\n--- a/src/test-runner/validator.ts\\n+++ b/src/test-runner/validator.ts\\n@@ -25,6 +25,16 @@ const schema: any = {\\n     hooks: { type: 'object' },\\n     slack: { type: 'object' },\\n     frontends: { type: 'array' },\\n+    workspace: { type: 'object' },\\n+    // Workflow definition fields (for workflow files with co-located tests)\\n+    id: { type: 'string' },\\n+    name: { type: 'string' },\\n+    description: { type: 'string' },\\n+    inputs: { type: 'array' },\\n+    outputs: { type: 'array' },\\n+    ai_mcp_servers: { type: 'object' },\\n+    ai_provider: { type: 'string' },\\n+    ai_model: { type: 'string' },\\n     tests: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -136,6 +146,8 @@ const schema: any = {\\n             oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n           },\\n         },\\n+        // Workflow testing: input values to pass to the workflow\\n+        workflow_input: { type: 'object' },\\n         expect: { $ref: '#/$defs/expectBlock' },\\n         // Flow cases\\n         flow: {\\n@@ -270,6 +282,35 @@ const schema: any = {\\n       },\\n       required: ['step', 'path'],\\n     },\\n+    // Workflow output expectation: like outputsExpectation but without step (tests workflow-level outputs)\\n+    workflowOutputExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        path: { type: 'string' },\\n+        equals: {},\\n+        equalsDeep: {},\\n+        matches: { type: 'string' },\\n+        contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        not_contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            path: { type: 'string' },\\n+            equals: {},\\n+            matches: { type: 'string' },\\n+          },\\n+          required: ['path'],\\n+        },\\n+        contains_unordered: { type: 'array' },\\n+      },\\n+      required: ['path'],\\n+    },\\n     expectBlock: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -278,6 +319,8 @@ const schema: any = {\\n         calls: { type: 'array', items: { $ref: '#/$defs/callsExpectation' } },\\n         prompts: { type: 'array', items: { $ref: '#/$defs/promptsExpectation' } },\\n         outputs: { type: 'array', items: { $ref: '#/$defs/outputsExpectation' } },\\n+        // Workflow testing: assert on workflow-level outputs (defined in workflow's outputs: section)\\n+        workflow_output: { type: 'array', items: { $ref: '#/$defs/workflowOutputExpectation' } },\\n         no_calls: {\\n           type: 'array',\\n           items: {\\n@@ -343,6 +386,7 @@ const knownKeys = new Set([\\n   'version',\\n   'extends',\\n   'tests',\\n+  'workspace',\\n   // tests\\n   'tests.defaults',\\n   'tests.fixtures',\\n@@ -363,6 +407,7 @@ const knownKeys = new Set([\\n   'env',\\n   'routing',\\n   'mocks',\\n+  'workflow_input',\\n   'expect',\\n   'flow',\\n   // expect\\n@@ -370,6 +415,7 @@ const knownKeys = new Set([\\n   'expect.calls',\\n   'expect.prompts',\\n   'expect.outputs',\\n+  'expect.workflow_output',\\n   'expect.no_calls',\\n   'expect.fail',\\n   'expect.strict_violation',\\n@@ -424,7 +470,7 @@ function formatError(e: ErrorObject): string {\\n       if (hint) msg += ` (${hint})`;\\n       // Small curated allow-list for frequent nodes to reduce guesswork\\n       if (path.endsWith('expect')) {\\n-        msg += ` (allowed: use, calls, prompts, outputs, no_calls, fail, strict_violation)`;\\n+        msg += ` (allowed: use, calls, prompts, outputs, workflow_output, no_calls, fail, strict_violation)`;\\n       } else if (path.endsWith('env')) {\\n         msg += ` (values must be strings)`;\\n       } else if (path.endsWith('tests')) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/cli.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/cli.ts b/src/types/cli.ts\\nindex a7d9ca30..f06f93d5 100644\\n--- a/src/types/cli.ts\\n+++ b/src/types/cli.ts\\n@@ -61,6 +61,8 @@ export interface CliOptions {\\n   githubV2?: boolean;\\n   /** Enable Slack Socket Mode runner */\\n   slack?: boolean;\\n+  /** Keep workspace folders after execution (for debugging) */\\n+  keepWorkspace?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/config.ts\",\"additions\":114,\"deletions\":5,\"changes\":119,\"patch\":\"diff --git a/src/types/config.ts b/src/types/config.ts\\nindex ed239959..dd363de1 100644\\n--- a/src/types/config.ts\\n+++ b/src/types/config.ts\\n@@ -122,6 +122,9 @@ export interface FailureConditionContext {\\n   /** Environment variables */\\n   env?: Record<string, string>;\\n \\n+  /** Workflow inputs - values passed to the workflow */\\n+  inputs?: Record<string, unknown>;\\n+\\n   /** Memory accessor for accessing memory store in expressions */\\n   memory?: {\\n     get: (key: string, namespace?: string) => unknown;\\n@@ -356,6 +359,8 @@ export interface AIProviderConfig {\\n   allowBash?: boolean;\\n   /** Advanced bash command execution configuration */\\n   bashConfig?: BashConfig;\\n+  /** Completion prompt for post-completion validation/review (runs after attempt_completion) */\\n+  completion_prompt?: string;\\n }\\n \\n /**\\n@@ -461,8 +466,8 @@ export interface CheckConfig {\\n   env?: EnvConfig;\\n   /** Timeout in seconds for command execution (default: 60) */\\n   timeout?: number;\\n-  /** Check IDs that this check depends on (optional) */\\n-  depends_on?: string[];\\n+  /** Check IDs that this check depends on (optional). Accepts single string or array. */\\n+  depends_on?: string | string[];\\n   /** Group name for comment separation (e.g., \\\"code-review\\\", \\\"pr-overview\\\") - optional */\\n   group?: string;\\n   /** Schema type for template rendering (e.g., \\\"code-review\\\", \\\"markdown\\\") or inline JSON schema object - optional */\\n@@ -621,6 +626,46 @@ export interface CheckConfig {\\n   overrides?: Record<string, Partial<CheckConfig>>;\\n   /** Map workflow outputs to check outputs */\\n   output_mapping?: Record<string, string>;\\n+\\n+  // Workflow aliases for backward compatibility\\n+  /** Alias for args - workflow inputs (backward compatibility) */\\n+  workflow_inputs?: Record<string, unknown>;\\n+  /** Config file path - alternative to workflow ID (loads a Visor config file as workflow) */\\n+  config?: string;\\n+  /** Alias for overrides - workflow step overrides (backward compatibility) */\\n+  workflow_overrides?: Record<string, Partial<CheckConfig>>;\\n+\\n+  /**\\n+   * Git-checkout provider specific options (optional, only used when type === 'git-checkout').\\n+   */\\n+  /** Git reference to checkout (branch, tag, commit SHA) - supports templates */\\n+  ref?: string;\\n+  /** Repository URL or owner/repo format (defaults to current repository) */\\n+  repository?: string;\\n+  /** GitHub token for private repositories (defaults to GITHUB_TOKEN env) */\\n+  token?: string;\\n+  /** Number of commits to fetch (0 for full history, default: 1) */\\n+  fetch_depth?: number;\\n+  /** Whether to fetch tags (default: false) */\\n+  fetch_tags?: boolean;\\n+  /** Checkout submodules: false, true, or 'recursive' */\\n+  submodules?: boolean | 'recursive';\\n+  /** Working directory for the checkout (defaults to temp directory) */\\n+  working_directory?: string;\\n+  /** Use git worktree for efficient parallel checkouts (default: true) */\\n+  use_worktree?: boolean;\\n+  /** Clean the working directory before checkout (default: true) */\\n+  clean?: boolean;\\n+  /** Sparse checkout paths - only checkout specific directories/files */\\n+  sparse_checkout?: string[];\\n+  /** Enable Git LFS (Large File Storage) */\\n+  lfs?: boolean;\\n+  /** Timeout in ms for cloning the bare repository (default: 300000 = 5 min) */\\n+  clone_timeout_ms?: number;\\n+  /** Clean up worktree on failure (default: true) */\\n+  cleanup_on_failure?: boolean;\\n+  /** Keep worktree after workflow completion (default: false) */\\n+  persist_worktree?: boolean;\\n }\\n \\n /**\\n@@ -1037,6 +1082,36 @@ export interface CustomToolDefinition {\\n   outputSchema?: Record<string, unknown>;\\n }\\n \\n+/**\\n+ * Workflow input definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowInput {\\n+  /** Input parameter name */\\n+  name: string;\\n+  /** JSON Schema for the input */\\n+  schema?: Record<string, unknown>;\\n+  /** Whether this input is required */\\n+  required?: boolean;\\n+  /** Default value if not provided */\\n+  default?: unknown;\\n+  /** Human-readable description */\\n+  description?: string;\\n+}\\n+\\n+/**\\n+ * Workflow output definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowOutput {\\n+  /** Output name */\\n+  name: string;\\n+  /** Human-readable description */\\n+  description?: string;\\n+  /** Value using Liquid template syntax (references step outputs) */\\n+  value?: string;\\n+  /** Value using JavaScript expression (alternative to value) */\\n+  value_js?: string;\\n+}\\n+\\n /**\\n  * Main Visor configuration\\n  */\\n@@ -1051,12 +1126,16 @@ export interface VisorConfig {\\n   tools?: Record<string, CustomToolDefinition>;\\n   /** Import workflow definitions from external files or URLs */\\n   imports?: string[];\\n+  /** Workflow inputs (for standalone reusable workflows) */\\n+  inputs?: WorkflowInput[];\\n+  /** Workflow outputs (for standalone reusable workflows) */\\n+  outputs?: WorkflowOutput[];\\n   /** Step configurations (recommended) */\\n   steps?: Record<string, CheckConfig>;\\n   /** Check configurations (legacy, use 'steps' instead) - always populated after normalization */\\n   checks?: Record<string, CheckConfig>;\\n-  /** Output configuration */\\n-  output: OutputConfig;\\n+  /** Output configuration (optional - defaults provided) */\\n+  output?: OutputConfig;\\n   /** HTTP server configuration for receiving webhooks */\\n   http_server?: HttpServerConfig;\\n   /** Memory storage configuration */\\n@@ -1092,6 +1171,36 @@ export interface VisorConfig {\\n     /** Frontend-specific configuration */\\n     config?: unknown;\\n   }>;\\n+  /** Workspace isolation configuration for sandboxed execution */\\n+  workspace?: WorkspaceConfig;\\n+  /** Slack configuration */\\n+  slack?: SlackConfig;\\n+}\\n+\\n+/**\\n+ * Workspace isolation configuration\\n+ */\\n+export interface WorkspaceConfig {\\n+  /** Enable workspace isolation (default: true when config present) */\\n+  enabled?: boolean;\\n+  /** Base path for workspaces (default: /tmp/visor-workspaces) */\\n+  base_path?: string;\\n+  /** Clean up workspace on exit (default: true) */\\n+  cleanup_on_exit?: boolean;\\n+}\\n+\\n+/**\\n+ * Slack configuration\\n+ */\\n+export interface SlackConfig {\\n+  /** Slack API version */\\n+  version?: string;\\n+  /** Mention handling: 'all', 'direct', etc. */\\n+  mentions?: string;\\n+  /** Thread handling: 'required', 'optional', etc. */\\n+  threads?: string;\\n+  /** Show raw output in Slack responses */\\n+  show_raw_output?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/git-checkout.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/git-checkout.ts b/src/types/git-checkout.ts\\nindex eb1c5ebc..4d27855d 100644\\n--- a/src/types/git-checkout.ts\\n+++ b/src/types/git-checkout.ts\\n@@ -16,6 +16,8 @@ export interface GitCheckoutConfig {\\n   fetch_depth?: number;\\n   fetch_tags?: boolean;\\n   submodules?: boolean | 'recursive';\\n+  /** Timeout (ms) for cloning the bare repository; defaults to 300000 (5 minutes) */\\n+  clone_timeout_ms?: number;\\n \\n   // Worktree configuration\\n   working_directory?: string;\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/sandbox.ts\",\"additions\":46,\"deletions\":1,\"changes\":47,\"patch\":\"diff --git a/src/utils/sandbox.ts b/src/utils/sandbox.ts\\nindex 9914a3a5..12e27c30 100644\\n--- a/src/utils/sandbox.ts\\n+++ b/src/utils/sandbox.ts\\n@@ -14,6 +14,51 @@ export interface CompileOptions {\\n   wrapFunction?: boolean;\\n }\\n \\n+export interface JsSyntaxValidationResult {\\n+  valid: boolean;\\n+  error?: string;\\n+}\\n+\\n+/**\\n+ * Validate JavaScript syntax without executing it.\\n+ * Uses the sandbox's compile method to check for syntax errors.\\n+ * Returns validation result with error message if invalid.\\n+ */\\n+export function validateJsSyntax(code: string): JsSyntaxValidationResult {\\n+  if (!code || typeof code !== 'string') {\\n+    return { valid: false, error: 'Code must be a non-empty string' };\\n+  }\\n+\\n+  const trimmed = code.trim();\\n+  if (trimmed.length === 0) {\\n+    return { valid: false, error: 'Code cannot be empty' };\\n+  }\\n+\\n+  // Create a minimal sandbox instance for syntax checking\\n+  const sandbox = createSecureSandbox();\\n+\\n+  // Wrap code similar to compileAndRun to catch the same syntax issues\\n+  const looksLikeBlock = /\\\\breturn\\\\b/.test(trimmed) || /;/.test(trimmed) || /\\\\n/.test(trimmed);\\n+  const looksLikeIife = /\\\\)\\\\s*\\\\(\\\\s*\\\\)\\\\s*;?$/.test(trimmed);\\n+  const body = looksLikeBlock\\n+    ? looksLikeIife\\n+      ? `return (\\\\n${trimmed}\\\\n);\\\\n`\\n+      : `return (() => {\\\\n${trimmed}\\\\n})();\\\\n`\\n+    : `return (\\\\n${trimmed}\\\\n);\\\\n`;\\n+\\n+  // Add log injection header (same as compileAndRun)\\n+  const header = `const __lp = \\\"[syntax-check]\\\"; const log = (...a) => { try { console.log(__lp, ...a); } catch {} };\\\\n`;\\n+  const fullCode = `${header}${body}`;\\n+\\n+  try {\\n+    sandbox.compile(fullCode);\\n+    return { valid: true };\\n+  } catch (e) {\\n+    const msg = e instanceof Error ? e.message : String(e);\\n+    return { valid: false, error: msg };\\n+  }\\n+}\\n+\\n /**\\n  * Create a hardened Sandbox with a consistent set of globals and prototype\\n  * whitelists. This is a superset of the sets previously used by individual\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/tracer-init.ts\",\"additions\":19,\"deletions\":1,\"changes\":20,\"patch\":\"diff --git a/src/utils/tracer-init.ts b/src/utils/tracer-init.ts\\nindex b42995d9..4edf1ffc 100644\\n--- a/src/utils/tracer-init.ts\\n+++ b/src/utils/tracer-init.ts\\n@@ -76,6 +76,24 @@ export async function initializeTracer(\\n \\n       const tracer = new SimpleAppTracer(telemetry, sessionId);\\n \\n+      // WORKAROUND: Add missing recordEvent method for completionPrompt feature (probe #321)\\n+      // SimpleAppTracer doesn't have recordEvent but completionPrompt requires it\\n+      if (typeof (tracer as any).recordEvent !== 'function') {\\n+        (tracer as any).recordEvent = (\\n+          name: string,\\n+          attributes?: Record<string, unknown>\\n+        ) => {\\n+          // Log completion events to telemetry for debugging\\n+          try {\\n+            if ((telemetry as any).record) {\\n+              (telemetry as any).record({ event: name, ...attributes });\\n+            }\\n+          } catch {\\n+            // Best-effort only\\n+          }\\n+        };\\n+      }\\n+\\n       console.error(`üìä Simple tracing enabled, will save to: ${traceFilePath}`);\\n \\n       // If in GitHub Actions, log the path for artifact upload\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/worktree-manager.ts\",\"additions\":142,\"deletions\":27,\"changes\":169,\"patch\":\"diff --git a/src/utils/worktree-manager.ts b/src/utils/worktree-manager.ts\\nindex f3cebf3d..2de7a9f4 100644\\n--- a/src/utils/worktree-manager.ts\\n+++ b/src/utils/worktree-manager.ts\\n@@ -6,6 +6,7 @@\\n  */\\n \\n import * as fs from 'fs';\\n+import * as fsp from 'fs/promises';\\n import * as path from 'path';\\n import * as crypto from 'crypto';\\n import { commandExecutor } from './command-executor';\\n@@ -119,7 +120,8 @@ export class WorktreeManager {\\n     repository: string,\\n     repoUrl: string,\\n     token?: string,\\n-    fetchDepth?: number\\n+    fetchDepth?: number,\\n+    cloneTimeoutMs?: number\\n   ): Promise<string> {\\n     const reposDir = this.getReposDir();\\n     const repoName = repository.replace(/\\\\//g, '-');\\n@@ -129,9 +131,21 @@ export class WorktreeManager {\\n     if (fs.existsSync(bareRepoPath)) {\\n       logger.debug(`Bare repository already exists: ${bareRepoPath}`);\\n \\n-      // Update remote refs\\n-      await this.updateBareRepo(bareRepoPath);\\n-      return bareRepoPath;\\n+      // Verify the bare repo has the correct remote URL to prevent using corrupted repos\\n+      const verifyResult = await this.verifyBareRepoRemote(bareRepoPath, repoUrl);\\n+      if (verifyResult === 'timeout') {\\n+        // Timeout during verification - use stale cache to avoid hanging on re-clone\\n+        logger.info(`Using stale bare repository (verification timed out): ${bareRepoPath}`);\\n+        return bareRepoPath;\\n+      } else if (verifyResult === false) {\\n+        logger.warn(`Bare repository at ${bareRepoPath} has incorrect remote, removing and re-cloning`);\\n+        await fsp.rm(bareRepoPath, { recursive: true, force: true });\\n+        // Fall through to clone below\\n+      } else {\\n+        // Update remote refs\\n+        await this.updateBareRepo(bareRepoPath);\\n+        return bareRepoPath;\\n+      }\\n     }\\n \\n     // Clone as bare repository\\n@@ -153,7 +167,9 @@ export class WorktreeManager {\\n     }\\n     cloneCmd += ` ${this.escapeShellArg(cloneUrl)} ${this.escapeShellArg(bareRepoPath)}`;\\n \\n-    const result = await this.executeGitCommand(cloneCmd, { timeout: 300000 }); // 5 minute timeout\\n+    const result = await this.executeGitCommand(cloneCmd, {\\n+      timeout: cloneTimeoutMs || 300000, // default 5 minutes\\n+    });\\n \\n     if (result.exitCode !== 0) {\\n       // Redact tokens from error messages\\n@@ -171,19 +187,98 @@ export class WorktreeManager {\\n   private async updateBareRepo(bareRepoPath: string): Promise<void> {\\n     logger.debug(`Updating bare repository: ${bareRepoPath}`);\\n \\n-    const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n-    const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+    try {\\n+      const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n+      const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n+        // Don't throw - we can continue with stale refs\\n+      } else {\\n+        logger.debug(`Successfully updated bare repository`);\\n+      }\\n+    } catch (error) {\\n+      // Handle timeout or other errors gracefully - we can continue with stale refs\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      logger.warn(`Failed to update bare repository (will use stale refs): ${errorMessage}`);\\n+    }\\n+  }\\n \\n-    if (result.exitCode !== 0) {\\n-      logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n-      // Don't throw - we can continue with stale refs\\n-    } else {\\n-      logger.debug(`Successfully updated bare repository`);\\n+  /**\\n+   * Verify that a bare repository has the correct remote URL.\\n+   * This prevents reusing corrupted repos that were cloned from a different repository.\\n+   * Returns: true (valid), false (invalid - should re-clone), or 'timeout' (use stale cache)\\n+   */\\n+  private async verifyBareRepoRemote(\\n+    bareRepoPath: string,\\n+    expectedUrl: string\\n+  ): Promise<boolean | 'timeout'> {\\n+    try {\\n+      const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote get-url origin`;\\n+      const result = await this.executeGitCommand(cmd, { timeout: 10000 });\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to get remote URL for ${bareRepoPath}: ${result.stderr}`);\\n+        return false;\\n+      }\\n+\\n+      const actualUrl = result.stdout.trim();\\n+\\n+      // Normalize URLs for comparison:\\n+      // - remove credentials (tokens/username)\\n+      // - remove .git suffix if present\\n+      // - trim trailing slash\\n+      // - lowercase for case‚Äëinsensitive match\\n+      const normalizeUrl = (url: string): string => {\\n+        // Convert common SSH form to https for comparison\\n+        if (url.startsWith('git@github.com:')) {\\n+          url = url.replace('git@github.com:', 'https://github.com/');\\n+        }\\n+        return url\\n+          // strip userinfo part to avoid mismatches when the cached bare\\n+          // repo was cloned with an access token but the expected URL is\\n+          // tokenless (or vice‚Äëversa)\\n+          .replace(/:\\\\/\\\\/[^@]+@/, '://')\\n+          .replace(/\\\\.git$/, '')\\n+          .replace(/\\\\/$/, '')\\n+          .toLowerCase();\\n+      };\\n+\\n+      const normalizedExpected = normalizeUrl(expectedUrl);\\n+      const normalizedActual = normalizeUrl(actualUrl);\\n+\\n+      if (normalizedExpected !== normalizedActual) {\\n+        logger.warn(\\n+          `Bare repo remote mismatch: expected ${expectedUrl}, got ${actualUrl}`\\n+        );\\n+        return false;\\n+      }\\n+\\n+      logger.debug(`Bare repo remote verified: ${actualUrl}`);\\n+      return true;\\n+    } catch (error) {\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      // If it's a timeout, return 'timeout' so caller can use stale cache instead of re-cloning\\n+      if (errorMessage.includes('timed out')) {\\n+        logger.warn(`Timeout verifying bare repo remote (will use stale cache): ${errorMessage}`);\\n+        return 'timeout';\\n+      }\\n+      logger.warn(`Error verifying bare repo remote: ${error}`);\\n+      return false;\\n     }\\n   }\\n \\n   /**\\n-   * Create a new worktree\\n+   * Create a new worktree for the given repository/ref.\\n+   *\\n+   * Important: we always create worktrees in a detached HEAD state pinned\\n+   * to a specific commit SHA rather than a named branch like \\\"main\\\". Git\\n+   * only allows a branch to be checked out in a single worktree at a time;\\n+   * using the raw commit (plus --detach) lets multiple workflows safely\\n+   * create independent worktrees for the same branch without hitting\\n+   * errors like:\\n+   *\\n+   *   fatal: 'main' is already used by worktree at '.../TykTechnologies-tyk-docs-main-XXXX'\\n    */\\n   async createWorktree(\\n     repository: string,\\n@@ -195,6 +290,7 @@ export class WorktreeManager {\\n       clean?: boolean;\\n       workflowId?: string;\\n       fetchDepth?: number;\\n+      cloneTimeoutMs?: number;\\n     } = {}\\n   ): Promise<WorktreeInfo> {\\n     // Validate ref to prevent command injection\\n@@ -205,7 +301,8 @@ export class WorktreeManager {\\n       repository,\\n       repoUrl,\\n       options.token,\\n-      options.fetchDepth\\n+      options.fetchDepth,\\n+      options.cloneTimeoutMs\\n     );\\n \\n     // Generate worktree ID and path\\n@@ -241,21 +338,24 @@ export class WorktreeManager {\\n       }\\n     }\\n \\n-    // Fetch the ref if needed\\n+    // Fetch the ref if needed, then resolve it to a concrete commit SHA.\\n+    // We use the commit (detached HEAD) instead of the branch name so we\\n+    // can have multiple worktrees for the same branch without git refusing\\n+    // with \\\"branch X is already checked out\\\".\\n     await this.fetchRef(bareRepoPath, ref);\\n+    const commit = await this.getCommitShaForRef(bareRepoPath, ref);\\n \\n-    // Create worktree\\n-    logger.info(`Creating worktree for ${repository}@${ref}`);\\n-    const createCmd = `git -C ${this.escapeShellArg(bareRepoPath)} worktree add ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(ref)}`;\\n+    // Create worktree in detached HEAD state at the resolved commit\\n+    logger.info(`Creating worktree for ${repository}@${ref} (${commit})`);\\n+    const createCmd = `git -C ${this.escapeShellArg(\\n+      bareRepoPath\\n+    )} worktree add --detach ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(commit)}`;\\n     const result = await this.executeGitCommand(createCmd, { timeout: 60000 });\\n \\n     if (result.exitCode !== 0) {\\n       throw new Error(`Failed to create worktree: ${result.stderr}`);\\n     }\\n \\n-    // Get commit SHA\\n-    const commit = await this.getCommitSha(worktreePath);\\n-\\n     // Create metadata\\n     const metadata: WorktreeMetadata = {\\n       worktree_id: worktreeId,\\n@@ -316,14 +416,17 @@ export class WorktreeManager {\\n   }\\n \\n   /**\\n-   * Get commit SHA for worktree\\n+   * Get commit SHA for a given ref inside a bare repository.\\n+   *\\n+   * This runs after fetchRef so that <ref> should resolve to either a\\n+   * local branch, tag, or remote-tracking ref.\\n    */\\n-  private async getCommitSha(worktreePath: string): Promise<string> {\\n-    const cmd = `git -C ${this.escapeShellArg(worktreePath)} rev-parse HEAD`;\\n+  private async getCommitShaForRef(bareRepoPath: string, ref: string): Promise<string> {\\n+    const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} rev-parse ${this.escapeShellArg(ref)}`;\\n     const result = await this.executeGitCommand(cmd);\\n \\n     if (result.exitCode !== 0) {\\n-      throw new Error(`Failed to get commit SHA: ${result.stderr}`);\\n+      throw new Error(`Failed to get commit SHA for ref ${ref}: ${result.stderr}`);\\n     }\\n \\n     return result.stdout.trim();\\n@@ -621,9 +724,21 @@ export class WorktreeManager {\\n     command: string,\\n     options: { timeout?: number; env?: Record<string, string> } = {}\\n   ): Promise<GitCommandResult> {\\n+    // Merge provided env with process.env and add git-specific settings\\n+    // These settings prevent git from hanging on interactive prompts while\\n+    // still allowing OS-level credential helpers to work:\\n+    // - GIT_TERMINAL_PROMPT=0: Prevents terminal credential prompts\\n+    // - GIT_SSH_COMMAND: Disables SSH password prompts (BatchMode)\\n+    const gitEnv = {\\n+      ...process.env,\\n+      ...options.env,\\n+      GIT_TERMINAL_PROMPT: '0',\\n+      GIT_SSH_COMMAND: 'ssh -o BatchMode=yes -o StrictHostKeyChecking=no',\\n+    };\\n+\\n     const result = await commandExecutor.execute(command, {\\n       timeout: options.timeout || 30000,\\n-      env: options.env || process.env,\\n+      env: gitEnv,\\n     } as any);\\n \\n     return {\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-executor.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-executor.ts b/src/workflow-executor.ts\\nindex 751e7f16..df76762f 100644\\n--- a/src/workflow-executor.ts\\n+++ b/src/workflow-executor.ts\\n@@ -219,7 +219,9 @@ export class WorkflowExecutor {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps)) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     // Use static DependencyResolver\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-registry.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-registry.ts b/src/workflow-registry.ts\\nindex 2cbe3a89..6bd855fb 100644\\n--- a/src/workflow-registry.ts\\n+++ b/src/workflow-registry.ts\\n@@ -410,7 +410,9 @@ export class WorkflowRegistry {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps || {})) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     try {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/ai-review-service.test.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/tests/unit/ai-review-service.test.ts b/tests/unit/ai-review-service.test.ts\\nindex ec2b998a..2c616c99 100644\\n--- a/tests/unit/ai-review-service.test.ts\\n+++ b/tests/unit/ai-review-service.test.ts\\n@@ -482,6 +482,37 @@ describe('AIReviewService', () => {\\n       expect(context).toContain('<additions>10</additions>');\\n       expect(context).toContain('<deletions>5</deletions>');\\n     });\\n+\\n+    it('should exclude diffs by default when in Slack mode', async () => {\\n+      // Slack mode is detected by presence of slackConversation property\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // In Slack mode, diffs should be excluded by default\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+      // Should still include file summary\\n+      expect(context).toContain('<files_summary>');\\n+    });\\n+\\n+    it('should include diffs in Slack mode when explicitly requested', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      (mockPRInfo as any).includeCodeContext = true;\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // With explicit includeCodeContext: true, diffs should be included\\n+      expect(context).toContain('<full_diff>');\\n+      expect(context).toContain('--- test.ts');\\n+    });\\n+\\n+    it('should exclude diffs in Slack mode even when includeCodeContext is undefined', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      // Deliberately not setting includeCodeContext to test default behavior\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+    });\\n   });\\n \\n   describe('Comment Filtering for Code Review', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config-extends.test.ts\",\"additions\":8,\"deletions\":8,\"changes\":16,\"patch\":\"diff --git a/tests/unit/config-extends.test.ts b/tests/unit/config-extends.test.ts\\nindex b112a869..7ee69ce6 100644\\n--- a/tests/unit/config-extends.test.ts\\n+++ b/tests/unit/config-extends.test.ts\\n@@ -564,7 +564,7 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.security).toBeDefined();\\n       expect(config.checks!.performance).toBeDefined();\\n       expect(config.checks!.custom).toBeDefined();\\n-      expect(config.output.pr_comment.format).toBe('json');\\n+      expect(config.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should extend from default configuration', async () => {\\n@@ -1002,7 +1002,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.base).toBeDefined(); // From base\\n       expect(loaded.checks!.middle).toBeDefined(); // From middle\\n       expect(loaded.checks!.top).toBeDefined(); // From top\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From top\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From top\\n     });\\n \\n     it('should handle mixed local and remote extends', async () => {\\n@@ -1079,7 +1079,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.local).toBeDefined();\\n       expect(loaded.checks!.remote).toBeDefined();\\n       expect(loaded.checks!.child).toBeDefined();\\n-      expect(loaded.output.pr_comment.format).toBe('json');\\n+      expect(loaded.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should handle extends with relative path resolution', async () => {\\n@@ -1593,9 +1593,9 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.performance).toBeDefined(); // Added in child\\n \\n       // Verify output\\n-      expect(config.output.pr_comment.format).toBe('json'); // Overridden\\n-      expect(config.output.pr_comment.group_by).toBe('file'); // Overridden\\n-      expect(config.output.pr_comment.collapse).toBe(false); // Overridden\\n+      expect(config.output!.pr_comment.format).toBe('json'); // Overridden\\n+      expect(config.output!.pr_comment.group_by).toBe('file'); // Overridden\\n+      expect(config.output!.pr_comment.collapse).toBe(false); // Overridden\\n     });\\n \\n     it('should handle configuration with disabled checks removal', async () => {\\n@@ -1814,7 +1814,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.root).toBeDefined(); // From root\\n       expect(loaded.checks!.level1).toBeDefined(); // From level1\\n       expect(loaded.checks!.level2).toBeDefined(); // From level2\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From level2\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From level2\\n     });\\n \\n     it('should handle mixed absolute and relative paths in extends array', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config.test.ts\",\"additions\":7,\"deletions\":7,\"changes\":14,\"patch\":\"diff --git a/tests/unit/config.test.ts b/tests/unit/config.test.ts\\nindex 67ce61fd..0677a3d4 100644\\n--- a/tests/unit/config.test.ts\\n+++ b/tests/unit/config.test.ts\\n@@ -48,7 +48,7 @@ output:\\n       expect(config.version).toBe('1.0');\\n       expect(config.checks).toHaveProperty('performance');\\n       expect(config.checks).toHaveProperty('security');\\n-      expect(config.output.pr_comment.format).toBe('table');\\n+      expect(config.output!.pr_comment.format).toBe('table');\\n     });\\n \\n     it('should handle missing config file gracefully', async () => {\\n@@ -264,9 +264,9 @@ checks:\\n       const config = await configManager.loadConfig('/path/to/minimal.yaml');\\n \\n       // Should have default output configuration\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.group_by).toBe('check');\\n-      expect(config.output.pr_comment.collapse).toBe(true);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.group_by).toBe('check');\\n+      expect(config.output!.pr_comment.collapse).toBe(true);\\n     });\\n   });\\n \\n@@ -483,8 +483,8 @@ output:\\n       expect(config.checks!.performance.prompt).toContain('N+1 database queries');\\n       expect(config.checks!.security.prompt).toContain('SQL injection');\\n       expect(config.checks!.architecture.prompt).toContain('SOLID principles');\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.collapse).toBe(false);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.collapse).toBe(false);\\n     });\\n   });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/foreach-empty-skip.test.ts\",\"additions\":5,\"deletions\":3,\"changes\":8,\"patch\":\"diff --git a/tests/unit/foreach-empty-skip.test.ts b/tests/unit/foreach-empty-skip.test.ts\\nindex 1ea8e1e5..2640ed14 100644\\n--- a/tests/unit/foreach-empty-skip.test.ts\\n+++ b/tests/unit/foreach-empty-skip.test.ts\\n@@ -90,11 +90,13 @@ describe('forEach Empty Array Skip', () => {\\n     );\\n     expect(processStats?.totalRuns).toBe(0);\\n \\n-    // notify-ticket should also NOT run because process-ticket returned empty\\n+    // notify-ticket SHOULD run even though process-ticket had 0 runs from empty forEach\\n+    // This is the expected behavior: downstream steps that depend on forEach children\\n+    // should still execute (with empty/null inputs) to handle the \\\"no items\\\" case\\n     const notifyStats = result.executionStatistics?.checks!.find(\\n       c => c.checkName === 'notify-ticket'\\n     );\\n-    expect(notifyStats?.totalRuns).toBe(0);\\n+    expect(notifyStats?.totalRuns).toBe(1);\\n   });\\n \\n   it('should handle empty array with transform_js', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/ai-check-provider.test.ts\",\"additions\":61,\"deletions\":1,\"changes\":62,\"patch\":\"diff --git a/tests/unit/providers/ai-check-provider.test.ts b/tests/unit/providers/ai-check-provider.test.ts\\nindex d0b94086..a858ddae 100644\\n--- a/tests/unit/providers/ai-check-provider.test.ts\\n+++ b/tests/unit/providers/ai-check-provider.test.ts\\n@@ -399,6 +399,66 @@ describe('AICheckProvider', () => {\\n         },\\n       });\\n     });\\n+\\n+    it('derives allowedFolders and path from workspace when present on parent context', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const workspace = {\\n+        isEnabled: () => true,\\n+        getWorkspaceInfo: () => ({\\n+          workspacePath: '/tmp/ws-123',\\n+          mainProjectPath: '/tmp/ws-123/main-project',\\n+        }),\\n+        listProjects: () => [\\n+          { name: 'tyk', path: '/tmp/ws-123/tyk' },\\n+          { name: 'tyk-docs', path: '/tmp/ws-123/tyk-docs' },\\n+        ],\\n+      };\\n+\\n+      const execContext: any = {\\n+        _parentContext: {\\n+          workspace,\\n+        },\\n+      };\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'code_help',\\n+        ai: {\\n+          provider: 'google',\\n+          model: 'gemini-2.5-pro',\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, execContext);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'google',\\n+        model: 'gemini-2.5-pro',\\n+        // Workspace root should be used as primary working directory for tools\\n+        path: '/tmp/ws-123',\\n+      });\\n+\\n+      // allowedFolders should contain the workspace plus all project paths, de-duped\\n+      expect(capturedConfig.allowedFolders).toEqual(\\n+        expect.arrayContaining(['/tmp/ws-123', '/tmp/ws-123/tyk', '/tmp/ws-123/tyk-docs'])\\n+      );\\n+    });\\n   });\\n \\n   describe('getSupportedConfigKeys', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/command-check-provider.test.ts\",\"additions\":105,\"deletions\":1,\"changes\":106,\"patch\":\"diff --git a/tests/unit/providers/command-check-provider.test.ts b/tests/unit/providers/command-check-provider.test.ts\\nindex 3bf11a84..0a821bbd 100644\\n--- a/tests/unit/providers/command-check-provider.test.ts\\n+++ b/tests/unit/providers/command-check-provider.test.ts\\n@@ -666,4 +666,108 @@ describe('CommandCheckProvider', () => {\\n       expect((result as any).output).toBe('content'); // CommandCheckProvider trims whitespace\\n     });\\n   });\\n+\\n+  describe('Workflow Inputs', () => {\\n+    it('should use workflowInputs from config when available', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Hello from config.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from config.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with inputs from config.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from config.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should fall back to context.workflowInputs when config.workflowInputs is not set', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Hello from context.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from context.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify the command was rendered with inputs from context.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from context.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should prefer config.workflowInputs over context.workflowInputs', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Config takes precedence',\\n+        },\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Context should be ignored',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Config takes precedence\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify config.workflowInputs takes precedence\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Config takes precedence\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should provide empty inputs when neither config nor context has workflowInputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with empty inputs (undefined renders as empty)\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+  });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/git-checkout-workspace.test.ts\",\"additions\":4,\"deletions\":5,\"changes\":9,\"patch\":\"diff --git a/tests/unit/providers/git-checkout-workspace.test.ts b/tests/unit/providers/git-checkout-workspace.test.ts\\nindex 4eb625b7..4a0fbb6e 100644\\n--- a/tests/unit/providers/git-checkout-workspace.test.ts\\n+++ b/tests/unit/providers/git-checkout-workspace.test.ts\\n@@ -129,13 +129,12 @@ describe('GitCheckoutProvider Workspace Integration', () => {\\n         const output = (result as any).output as GitCheckoutOutput;\\n         expect(output.success).toBe(true);\\n         expect(output.workspace_path).toBeDefined();\\n-        // When checkName is provided, it's used as the project name\\n-        expect(output.workspace_path).toContain('test-checkout');\\n \\n-        // Verify project was added to workspace\\n+        // Verify project was added to workspace and the name is derived\\n+        // from the repository/description, not the checkName.\\n         const projects = workspace.listProjects();\\n         expect(projects.length).toBe(1);\\n-        expect(projects[0].name).toBe('test-checkout');\\n+        expect(projects[0].name).toBe('external-repo');\\n       } finally {\\n         await workspace.cleanup();\\n         fs.rmSync(mainProjectDir, { recursive: true, force: true });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/http-client-provider.test.ts\",\"additions\":15,\"deletions\":10,\"changes\":25,\"patch\":\"diff --git a/tests/unit/providers/http-client-provider.test.ts b/tests/unit/providers/http-client-provider.test.ts\\nindex b759ad75..caa73417 100644\\n--- a/tests/unit/providers/http-client-provider.test.ts\\n+++ b/tests/unit/providers/http-client-provider.test.ts\\n@@ -120,15 +120,17 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect(result).toEqual<ReviewSummary & { data: unknown }>({\\n+      // The provider returns data in the 'output' property (consistent with other providers)\\n+      expect(result).toEqual({\\n         issues: [],\\n-        data: responseData,\\n+        output: { status: 'ok', data: { value: 123 } },\\n       });\\n     });\\n \\n     it('should handle POST request with body', async () => {\\n       mockConfig.method = 'POST';\\n-      mockConfig.body = '{\\\"request\\\": \\\"data\\\"}';\\n+      // Use a body with Liquid template to trigger parseAndRender\\n+      mockConfig.body = '{\\\"request\\\": \\\"{{ pr.title }}\\\"}';\\n \\n       const responseData = { result: 'success' };\\n       const mockResponse = {\\n@@ -143,15 +145,16 @@ describe('HttpClientProvider', () => {\\n       };\\n \\n       mockFetch.mockResolvedValue(mockResponse);\\n-      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"data\\\"}');\\n+      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"Test PR\\\"}');\\n \\n       const result = await provider.execute(mockPRInfo, mockConfig, new Map());\\n \\n+      // Verify the body template was parsed by Liquid with the correct template\\n       expect(mockLiquid.parseAndRender).toHaveBeenCalledWith(\\n-        '{\\\"request\\\": \\\"data\\\"}',\\n+        '{\\\"request\\\": \\\"{{ pr.title }}\\\"}',\\n         expect.objectContaining({\\n           pr: expect.any(Object),\\n-          outputs: {},\\n+          outputs: expect.any(Object),\\n         })\\n       );\\n \\n@@ -159,7 +162,7 @@ describe('HttpClientProvider', () => {\\n         'https://api.example.com/data',\\n         expect.objectContaining({\\n           method: 'POST',\\n-          body: '{\\\"request\\\": \\\"data\\\"}',\\n+          body: '{\\\"request\\\": \\\"Test PR\\\"}',\\n           headers: expect.objectContaining({\\n             Authorization: 'Bearer token',\\n             'Content-Type': 'application/json',\\n@@ -167,7 +170,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(responseData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { result: string } }).output.result).toEqual('success');\\n     });\\n \\n     it('should handle HTTP errors', async () => {\\n@@ -235,7 +239,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(transformedData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { transformed: string } }).output.transformed).toEqual('result');\\n     });\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/session-reuse-config.test.ts\",\"additions\":20,\"deletions\":1,\"changes\":21,\"patch\":\"diff --git a/tests/unit/session-reuse-config.test.ts b/tests/unit/session-reuse-config.test.ts\\nindex a2e2ee26..18fc42e6 100644\\n--- a/tests/unit/session-reuse-config.test.ts\\n+++ b/tests/unit/session-reuse-config.test.ts\\n@@ -152,6 +152,25 @@ describe('Session Reuse Configuration Validation', () => {\\n       }).not.toThrow();\\n     });\\n \\n+    it('should accept reuse_ai_session=\\\\\\\"self\\\\\\\" without depends_on', () => {\\n+      const config: Partial<VisorConfig> = {\\n+        version: '1.0',\\n+        checks: {\\n+          'loop-check': {\\n+            type: 'ai',\\n+            prompt: 'Chat-like loop that reuses its own session',\\n+            on: ['pr_opened'],\\n+            // Special self-reuse mode does not require depends_on\\n+            reuse_ai_session: 'self',\\n+          },\\n+        },\\n+      };\\n+\\n+      expect(() => {\\n+        (configManager as any).validateConfig(config);\\n+      }).not.toThrow();\\n+    });\\n+\\n     it('should accept reuse_ai_session=true with multiple dependencies', () => {\\n       const config: Partial<VisorConfig> = {\\n         version: '1.0',\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/slack-markdown-format.test.ts\",\"additions\":25,\"deletions\":1,\"changes\":26,\"patch\":\"diff --git a/tests/unit/slack-markdown-format.test.ts b/tests/unit/slack-markdown-format.test.ts\\nindex 5e57ccae..39d0baf3 100644\\n--- a/tests/unit/slack-markdown-format.test.ts\\n+++ b/tests/unit/slack-markdown-format.test.ts\\n@@ -39,4 +39,28 @@ describe('markdownToSlack', () => {\\n       ['‚Ä¢ parent', '  ‚Ä¢ child', '```', '- not-a-bullet inside code', '```', '‚Ä¢ after'].join('\\\\n')\\n     );\\n   });\\n+\\n+  it('converts markdown headers to bold text', () => {\\n+    const input = '# Main Title\\\\n## Subtitle\\\\n### Section';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*Main Title*\\\\n*Subtitle*\\\\n*Section*');\\n+  });\\n+\\n+  it('adds newline before h1/h2 headers when preceded by content', () => {\\n+    const input = 'Some content\\\\n## New Section\\\\nMore content';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('Some content\\\\n\\\\n*New Section*\\\\nMore content');\\n+  });\\n+\\n+  it('does not add newline before h1 if it is the first line', () => {\\n+    const input = '# First Header\\\\nContent here';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*First Header*\\\\nContent here');\\n+  });\\n+\\n+  it('ignores headers inside code blocks', () => {\\n+    const input = '```\\\\n# This is a comment\\\\n```\\\\n# Real Header';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('```\\\\n# This is a comment\\\\n```\\\\n*Real Header*');\\n+  });\\n });\\n\",\"status\":\"modified\"}],\"outputs\":{\"ask\":{\"text\":\"This is a spike for an observability subject. There is a lot of information in it, but given your knowledge of Tyk, I'd like you to review the  content of this and suggest areas of the spike where you can add more detail, or areas of the spike where you think we have missed something.\\n<https://tyktech.atlassian.net/wiki/spaces/EN/pages/3230957684/SPIKE+Technical+Investigation+for+Exporting+Application+Logs>\",\"ts\":1767892784185},\"jira-context\":{\"jira_context_xml\":\"\",\"issues\":[],\"issue_count\":0,\"ts\":1767892784282},\"zendesk-context\":{\"zendesk_context_xml\":\"<zendesk_context>\\n  <ticket_count>0</ticket_count>\\n</zendesk_context>\",\"tickets\":\"[]\",\"ticket_count\":0,\"attachments\":\"{}\",\"ts\":1767892785436},\"log-request\":{\"issues\":[],\"logOutput\":\"‚ÑπÔ∏è **INFO**: üì• Request from user: unknown\\nChannel: unknown\\nThread: unknown\\nMessage: This is a spike for an observability subject. There is a lot of information in it, but given your knowledge of Tyk, I'd like you to review the  content of this and suggest areas of the spike where ...\\n\"},\"code-help-error\":{\"issues\":[]},\"customer-insights-error\":{\"issues\":[]},\"release-notes-error\":{\"issues\":[]}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.output":"{\"intent\":\"code_help\",\"topic\":\"Review the Confluence spike page on 'Technical Investigation for Exporting Application Logs' and suggest areas for adding more detail or where something might be missing.\",\"text\":\"{\\n  \\\"intent\\\": \\\"code_help\\\",\\n  \\\"topic\\\": \\\"Review the Confluence spike page on 'Technical Investigation for Exporting Application Logs' and suggest areas for adding more detail or where something might be missing.\\\"\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":2,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"code-help","visor.provider.type":"workflow"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"62ad7ac0-d2f5-416a-8e6d-75fdf292d95a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"62ad7ac0-d2f5-416a-8e6d-75fdf292d95a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"62ad7ac0-d2f5-416a-8e6d-75fdf292d95a"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-tyk-docs","visor.provider.type":"git-checkout"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"62ad7ac0-d2f5-416a-8e6d-75fdf292d95a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"62ad7ac0-d2f5-416a-8e6d-75fdf292d95a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"62ad7ac0-d2f5-416a-8e6d-75fdf292d95a"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ensure-code-plan","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"ensure-code-plan","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"outputs\":{\"checkout-tyk-docs\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-docs-main-66279114\",\"ref\":\"main\",\"commit\":\"a022bb7c96ca4f48bcdac27cae3f7c4a24171ee9\",\"worktree_id\":\"TykTechnologies-tyk-docs-main-66279114\",\"repository\":\"TykTechnologies/tyk-docs\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/fb082dda-74d2-49c1-a4ee-2069f3bc40e9/tyk-docs\",\"ts\":1767892834845}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"ensure-code-plan","visor.check.output":"{\"projects\":[{\"project_id\":\"tyk\",\"reason\":\"The Gateway is the primary source of API traffic logs (analytics records) and also generates its own operational logs. Understanding how it generates and outputs this data is fundamental to the spike.\"},{\"project_id\":\"tyk-pump\",\"reason\":\"Tyk Pump is the core component responsible for exporting analytics data from Redis to various sinks (e.g., Elasticsearch, Datadog, Splunk). Its configuration and internal workings are central to the topic of exporting logs.\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Dashboard backend consumes processed analytics for display and has its own application logs. It also provides the API for configuring the analytics pipeline, making it a key part of the overall observability story.\"},{\"project_id\":\"tyk-sink\",\"reason\":\"In Multi-Data Centre Bridge (MDCB) deployments, the sink can take on the role of processing and forwarding analytics, as noted in the documentation. Its logging behaviour is relevant for hybrid and edge deployments.\"}],\"notes\":\"The user's request is to review a technical spike on 'Exporting Application Logs'. This topic covers both API traffic analytics and the operational logs of the Tyk components themselves. The plan includes all components that either generate, process, or consume these logs to provide a comprehensive analysis of the entire data pipeline.\",\"text\":\"{\\n  \\\"projects\\\": [\\n    {\\n      \\\"project_id\\\": \\\"tyk\\\",\\n      \\\"reason\\\": \\\"The Gateway is the primary source of API traffic logs (analytics records) and also generates its own operational logs. Understanding how it generates and outputs this data is fundamental to the spike.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-pump\\\",\\n      \\\"reason\\\": \\\"Tyk Pump is the core component responsible for exporting analytics data from Redis to various sinks (e.g., Elasticsearch, Datadog, Splunk). Its configuration and internal workings are central to the topic of exporting logs.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-analytics\\\",\\n      \\\"reason\\\": \\\"The Dashboard backend consumes processed analytics for display and has its own application logs. It also provides the API for configuring the analytics pipeline, making it a key part of the overall observability story.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-sink\\\",\\n      \\\"reason\\\": \\\"In Multi-Data Centre Bridge (MDCB) deployments, the sink can take on the role of processing and forwarding analytics, as noted in the documentation. Its logging behaviour is relevant for hybrid and edge deployments.\\\"\\n    }\\n  ],\\n  \\\"notes\\\": \\\"The user's request is to review a technical spike on 'Exporting Application Logs'. This topic covers both API traffic analytics and the operational logs of the Tyk components themselves. The plan includes all components that either generate, process, or consume these logs to provide a comprehensive analysis of the entire data pipeline.\\\"\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"62ad7ac0-d2f5-416a-8e6d-75fdf292d95a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"62ad7ac0-d2f5-416a-8e6d-75fdf292d95a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"62ad7ac0-d2f5-416a-8e6d-75fdf292d95a"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"project-items","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"62ad7ac0-d2f5-416a-8e6d-75fdf292d95a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"62ad7ac0-d2f5-416a-8e6d-75fdf292d95a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"62ad7ac0-d2f5-416a-8e6d-75fdf292d95a"}}]}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":0,"visor.foreach.total":4},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":1,"visor.foreach.total":4},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":2,"visor.foreach.total":4},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":3,"visor.foreach.total":4},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"62ad7ac0-d2f5-416a-8e6d-75fdf292d95a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"62ad7ac0-d2f5-416a-8e6d-75fdf292d95a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"62ad7ac0-d2f5-416a-8e6d-75fdf292d95a"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"project-code-query","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"project-code-query","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"outputs\":{\"ensure-code-plan\":{\"projects\":[{\"project_id\":\"tyk\",\"reason\":\"The Gateway is the primary source of API traffic logs (analytics records) and also generates its own operational logs. Understanding how it generates and outputs this data is fundamental to the spike.\"},{\"project_id\":\"tyk-pump\",\"reason\":\"Tyk Pump is the core component responsible for exporting analytics data from Redis to various sinks (e.g., Elasticsearch, Datadog, Splunk). Its configuration and internal workings are central to the topic of exporting logs.\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Dashboard backend consumes processed analytics for display and has its own application logs. It also provides the API for configuring the analytics pipeline, making it a key part of the overall observability story.\"},{\"project_id\":\"tyk-sink\",\"reason\":\"In Multi-Data Centre Bridge (MDCB) deployments, the sink can take on the role of processing and forwarding analytics, as noted in the documentation. Its logging behaviour is relevant for hybrid and edge deployments.\"}],\"notes\":\"The user's request is to review a technical spike on 'Exporting Application Logs'. This topic covers both API traffic analytics and the operational logs of the Tyk components themselves. The plan includes all components that either generate, process, or consume these logs to provide a comprehensive analysis of the entire data pipeline.\",\"text\":\"{\\n  \\\"projects\\\": [\\n    {\\n      \\\"project_id\\\": \\\"tyk\\\",\\n      \\\"reason\\\": \\\"The Gateway is the primary source of API traffic logs (analytics records) and also generates its own operational logs. Understanding how it generates and outputs this data is fundamental to the spike.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-pump\\\",\\n      \\\"reason\\\": \\\"Tyk Pump is the core component responsible for exporting analytics data from Redis to various sinks (e.g., Elasticsearch, Datadog, Splunk). Its configuration and internal workings are central to the topic of exporting logs.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-analytics\\\",\\n      \\\"reason\\\": \\\"The Dashboard backend consumes processed analytics for display and has its own application logs. It also provides the API for configuring the analytics pipeline, making it a key part of the overall observability story.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-sink\\\",\\n      \\\"reason\\\": \\\"In Multi-Data Centre Bridge (MDCB) deployments, the sink can take on the role of processing and forwarding analytics, as noted in the documentation. Its logging behaviour is relevant for hybrid and edge deployments.\\\"\\n    }\\n  ],\\n  \\\"notes\\\": \\\"The user's request is to review a technical spike on 'Exporting Application Logs'. This topic covers both API traffic analytics and the operational logs of the Tyk components themselves. The plan includes all components that either generate, process, or consume these logs to provide a comprehensive analysis of the entire data pipeline.\\\"\\n}\",\"ts\":1767892863533},\"project-items\":[{\"project_id\":\"tyk\",\"reason\":\"The Gateway is the primary source of API traffic logs (analytics records) and also generates its own operational logs. Understanding how it generates and outputs this data is fundamental to the spike.\",\"repository\":\"TykTechnologies/tyk\",\"description\":\"Tyk Gateway\"},{\"project_id\":\"tyk-pump\",\"reason\":\"Tyk Pump is the core component responsible for exporting analytics data from Redis to various sinks (e.g., Elasticsearch, Datadog, Splunk). Its configuration and internal workings are central to the topic of exporting logs.\",\"repository\":\"TykTechnologies/tyk-pump\",\"description\":\"Tyk Pump\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Dashboard backend consumes processed analytics for display and has its own application logs. It also provides the API for configuring the analytics pipeline, making it a key part of the overall observability story.\",\"repository\":\"TykTechnologies/tyk-analytics\",\"description\":\"Tyk Dashboard backend\"},{\"project_id\":\"tyk-sink\",\"reason\":\"In Multi-Data Centre Bridge (MDCB) deployments, the sink can take on the role of processing and forwarding analytics, as noted in the documentation. Its logging behaviour is relevant for hybrid and edge deployments.\",\"repository\":\"TykTechnologies/tyk-sink\",\"description\":\"Tyk MDCB\"}],\"checkout-projects\":{\"issues\":[],\"isForEach\":true,\"forEachItems\":[{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-HEAD-fb30c78d\",\"ref\":\"HEAD\",\"commit\":\"bb67e9e7f9382a09ff152c6942d00023c463feef\",\"worktree_id\":\"TykTechnologies-tyk-HEAD-fb30c78d\",\"repository\":\"TykTechnologies/tyk\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/fb082dda-74d2-49c1-a4ee-2069f3bc40e9/tyk\"},{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-pump-HEAD-7525bd83\",\"ref\":\"HEAD\",\"commit\":\"3231dc3eac716fbd85f3a07e796e5bd1bab4d526\",\"worktree_id\":\"TykTechnologies-tyk-pump-HEAD-7525bd83\",\"repository\":\"TykTechnologies/tyk-pump\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/fb082dda-74d2-49c1-a4ee-2069f3bc40e9/tyk-pump\"},{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-analytics-HEAD-5a559e04\",\"ref\":\"HEAD\",\"commit\":\"b7ed8000a93e1284b28100938363a312d2d1fad1\",\"worktree_id\":\"TykTechnologies-tyk-analytics-HEAD-5a559e04\",\"repository\":\"TykTechnologies/tyk-analytics\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/fb082dda-74d2-49c1-a4ee-2069f3bc40e9/tyk-analytics\"},{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-sink-HEAD-83712304\",\"ref\":\"HEAD\",\"commit\":\"d015f96a8988b5cad4dfc51ff4b48fd77e9aec8b\",\"worktree_id\":\"TykTechnologies-tyk-sink-HEAD-83712304\",\"repository\":\"TykTechnologies/tyk-sink\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/fb082dda-74d2-49c1-a4ee-2069f3bc40e9/tyk-sink\"}],\"forEachItemResults\":[{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-HEAD-fb30c78d\",\"ref\":\"HEAD\",\"commit\":\"bb67e9e7f9382a09ff152c6942d00023c463feef\",\"worktree_id\":\"TykTechnologies-tyk-HEAD-fb30c78d\",\"repository\":\"TykTechnologies/tyk\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/fb082dda-74d2-49c1-a4ee-2069f3bc40e9/tyk\"}},{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-pump-HEAD-7525bd83\",\"ref\":\"HEAD\",\"commit\":\"3231dc3eac716fbd85f3a07e796e5bd1bab4d526\",\"worktree_id\":\"TykTechnologies-tyk-pump-HEAD-7525bd83\",\"repository\":\"TykTechnologies/tyk-pump\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/fb082dda-74d2-49c1-a4ee-2069f3bc40e9/tyk-pump\"}},{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-analytics-HEAD-5a559e04\",\"ref\":\"HEAD\",\"commit\":\"b7ed8000a93e1284b28100938363a312d2d1fad1\",\"worktree_id\":\"TykTechnologies-tyk-analytics-HEAD-5a559e04\",\"repository\":\"TykTechnologies/tyk-analytics\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/fb082dda-74d2-49c1-a4ee-2069f3bc40e9/tyk-analytics\"}},{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-sink-HEAD-83712304\",\"ref\":\"HEAD\",\"commit\":\"d015f96a8988b5cad4dfc51ff4b48fd77e9aec8b\",\"worktree_id\":\"TykTechnologies-tyk-sink-HEAD-83712304\",\"repository\":\"TykTechnologies/tyk-sink\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/fb082dda-74d2-49c1-a4ee-2069f3bc40e9/tyk-sink\"}}]},\"checkout-tyk-docs\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-docs-main-66279114\",\"ref\":\"main\",\"commit\":\"a022bb7c96ca4f48bcdac27cae3f7c4a24171ee9\",\"worktree_id\":\"TykTechnologies-tyk-docs-main-66279114\",\"repository\":\"TykTechnologies/tyk-docs\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/fb082dda-74d2-49c1-a4ee-2069f3bc40e9/tyk-docs\",\"ts\":1767892834845},\"project-items-raw\":[{\"project_id\":\"tyk\",\"reason\":\"The Gateway is the primary source of API traffic logs (analytics records) and also generates its own operational logs. Understanding how it generates and outputs this data is fundamental to the spike.\",\"repository\":\"TykTechnologies/tyk\",\"description\":\"Tyk Gateway\"},{\"project_id\":\"tyk-pump\",\"reason\":\"Tyk Pump is the core component responsible for exporting analytics data from Redis to various sinks (e.g., Elasticsearch, Datadog, Splunk). Its configuration and internal workings are central to the topic of exporting logs.\",\"repository\":\"TykTechnologies/tyk-pump\",\"description\":\"Tyk Pump\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Dashboard backend consumes processed analytics for display and has its own application logs. It also provides the API for configuring the analytics pipeline, making it a key part of the overall observability story.\",\"repository\":\"TykTechnologies/tyk-analytics\",\"description\":\"Tyk Dashboard backend\"},{\"project_id\":\"tyk-sink\",\"reason\":\"In Multi-Data Centre Bridge (MDCB) deployments, the sink can take on the role of processing and forwarding analytics, as noted in the documentation. Its logging behaviour is relevant for hybrid and edge deployments.\",\"repository\":\"TykTechnologies/tyk-sink\",\"description\":\"Tyk MDCB\"}]},\"args\":{}}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"jira-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"zendesk-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"log-request","visor.provider.type":"log"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"798c58ce-686c-45a8-95ba-ba1ed6c2eb25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"798c58ce-686c-45a8-95ba-ba1ed6c2eb25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"798c58ce-686c-45a8-95ba-ba1ed6c2eb25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-keys","visor.provider.type":"script"}}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-ids","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"history\":{}},\"outputs_history\":{},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"include_comments\":true,\"max_tickets\":5,\"download_assets\":true,\"cache_dir\":\".cache/zendesk\",\"text\":\"<@U09T5KRLMPU> what is the root cause of this bug:\\n<https://tyktech.atlassian.net/browse/TT-16304>\\nwhat is the root cause of this bug:\\n<https://tyktech.atlassian.net/browse/TT-16304>\\n\"},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"798c58ce-686c-45a8-95ba-ba1ed6c2eb25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"798c58ce-686c-45a8-95ba-ba1ed6c2eb25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"798c58ce-686c-45a8-95ba-ba1ed6c2eb25"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"fetch-issues","visor.provider.type":"http_client"}}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.output":"{\"data\":[\"16304\"],\"count\":1}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"fetch-tickets","visor.provider.type":"http_client"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"prepare-attachments","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"aggregate-downloads","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"format-output","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"df1955e7-3366-4eee-a9f8-3da5ce891260"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"798c58ce-686c-45a8-95ba-ba1ed6c2eb25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"798c58ce-686c-45a8-95ba-ba1ed6c2eb25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"798c58ce-686c-45a8-95ba-ba1ed6c2eb25"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"format-output","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"798c58ce-686c-45a8-95ba-ba1ed6c2eb25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"798c58ce-686c-45a8-95ba-ba1ed6c2eb25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"798c58ce-686c-45a8-95ba-ba1ed6c2eb25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"route-intent","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.input.context":"{\"pr\":{\"number\":0,\"title\":\"Local Analysis: feat/slack-frontend-v2 (70 modified)\",\"author\":\"Leonid Bugaev\",\"branch\":\"feat/slack-frontend-v2\",\"base\":\"main\"},\"files\":[{\"filename\":\"README.md\",\"additions\":7,\"deletions\":1,\"changes\":8,\"patch\":\"diff --git a/README.md b/README.md\\nindex 41131861..d92f02cc 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -534,6 +534,12 @@ steps:\\n     session_mode: append    # Shares history for full conversation\\n ```\\n \\n+You can also reuse the **same check‚Äôs** session when it loops back to itself, using:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` with `session_mode: append`\\n+\\n+See the standalone example at `examples/session-reuse-self.yaml` and the detailed guide in [docs/advanced-ai.md](docs/advanced-ai.md).\\n+\\n Learn more: [docs/advanced-ai.md](docs/advanced-ai.md)\\n \\n ## üìã Schema-Template System\\n\",\"status\":\"modified\"},{\"filename\":\"docs/advanced-ai.md\",\"additions\":61,\"deletions\":2,\"changes\":63,\"patch\":\"diff --git a/docs/advanced-ai.md b/docs/advanced-ai.md\\nindex bb8e684a..879285ed 100644\\n--- a/docs/advanced-ai.md\\n+++ b/docs/advanced-ai.md\\n@@ -1,7 +1,7 @@\\n ## üß† Advanced AI Features\\n \\n ### AI Session Reuse\\n-Use `reuse_ai_session: true` on dependent checks to continue conversation context with the AI across checks. This improves follow‚Äëups and consistency.\\n+Use `reuse_ai_session` on checks to continue conversation context with the AI across steps. This improves follow‚Äëups and consistency for follow‚Äëon analysis and chat‚Äëstyle flows.\\n \\n **Session Modes:**\\n - **`clone` (default)**: Creates a copy of the conversation history. Each check gets an independent session with the same starting context. Changes made by one check don't affect others.\\n@@ -26,6 +26,65 @@ steps:\\n     depends_on: [security-remediation]\\n     reuse_ai_session: true\\n     session_mode: append  # Share history - sees full conversation\\n+\\n+#### Reusing your own session: `reuse_ai_session: self`\\n+\\n+Sometimes the step you want to loop back into is the AI step itself (e.g. Slack assistants or multi‚Äëturn internal tools). For that case you can use:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` ‚Äì the step reuses its **own** Probe session when it runs again in the same engine run.\\n+- `session_mode: append` ‚Äì makes the follow‚Äëup behave like a normal conversation turn.\\n+\\n+On the first run of the step, Visor creates a new ProbeAgent session and registers it. If routing (`on_success.goto`, `goto_js`, etc.) later jumps back to the same step within the same run, the engine:\\n+\\n+- Finds the last result for that step in the current run.\\n+- Reads the `sessionId` stored in the result.\\n+- Calls the AI provider again using `executeReviewWithSessionReuse` with that session id.\\n+\\n+Simple example (no transport wiring, just CLI/tests):\\n+\\n+```yaml\\n+version: \\\"2.0\\\"\\n+\\n+steps:\\n+  seed:\\n+    type: script\\n+    content: |\\n+      return { text: \\\"hello from seed\\\" };\\n+\\n+  convo:\\n+    type: ai\\n+    depends_on: [seed]\\n+    reuse_ai_session: self\\n+    session_mode: append\\n+    ai:\\n+      provider: mock\\n+      model: mock\\n+      disableTools: true\\n+      allowedTools: []\\n+      system_prompt: \\\"You are a tiny echo assistant.\\\"\\n+    prompt: |\\n+      Seed message: {{ outputs['seed'].text }}\\n+\\n+      Past convo outputs in this run:\\n+      {% assign hist = outputs_history['convo'] | default: empty %}\\n+      {% if hist and hist.size > 0 %}\\n+      {% for h in hist %}\\n+      - Previous reply {{ forloop.index }}.\\n+      {% endfor %}\\n+      {% else %}\\n+      - No previous replies yet.\\n+      {% endif %}\\n+    on_success:\\n+      goto_js: |\\n+        // Example: re‚Äëenter this step up to 3 times in a single run\\n+        return attempt < 3 ? 'convo' : null;\\n+```\\n+\\n+The corresponding testable example lives at:\\n+\\n+- `examples/session-reuse-self.yaml`\\n+\\n+This keeps the configuration small but shows how to wire `reuse_ai_session: self` and `session_mode: append` without touching higher‚Äëlevel workflows like `tyk-assistant`.\\n ```\\n \\n **When to use each mode:**\\n\",\"status\":\"modified\"},{\"filename\":\"docs/recipes.md\",\"additions\":149,\"deletions\":3,\"changes\":152,\"patch\":\"diff --git a/docs/recipes.md b/docs/recipes.md\\nindex 1015bc6c..1089e9a5 100644\\n--- a/docs/recipes.md\\n+++ b/docs/recipes.md\\n@@ -321,8 +321,154 @@ Tip: When you define a JSON Schema, you generally do **not** need to tell the mo\\n - Avoid noisy fallbacks like `(outputs['x']?.kind ?? '') === 'status'` when `outputs['x']?.kind === 'status'` is equivalent.\\n - These conventions apply uniformly to any provider (`ai`, `command`, `script`, `github`, `http_client`, etc).\\n \\n+### Command step best practices\\n+\\n+When using `type: command` steps:\\n+\\n+**Avoid external tool dependencies** like `jq`, `yq`, `python`, etc.:\\n+- They may not be installed in all environments (GitHub Actions, Docker, CI)\\n+- Use `transform_js` to parse and transform output instead\\n+- Keep shell commands simple: `grep`, `sed`, `awk`, `sort`, `head` are universally available\\n+\\n+```yaml\\n+# Bad - requires jq\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | jq -R -s 'split(\\\"\\\\n\\\")'\\n+  parseJson: true\\n+\\n+# Good - use transform_js for parsing\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | sort -u\\n+  transform_js: |\\n+    const lines = (output || '').trim().split('\\\\n').filter(Boolean);\\n+    return { data: lines, count: lines.length };\\n+```\\n+\\n+**Prefer line-separated output** over JSON from shell:\\n+- Simple to parse with `transform_js`\\n+- No need for `parseJson: true`\\n+- More robust across different shells/environments\\n+\\n+**Use transform_js for structured output**:\\n+- The sandbox provides `output` (command stdout as string)\\n+- Return an object with the fields you need\\n+- Works consistently across all environments\\n+\\n+### Testing workflows with `--no-mocks`\\n+\\n+The `--no-mocks` flag runs your test cases with real providers instead of injecting mock responses. This is essential for:\\n+\\n+1. **Debugging integration issues** - See actual API responses and errors\\n+2. **Capturing realistic mock data** - Get real output to copy into your test cases\\n+3. **Validating credentials** - Verify environment variables are set correctly\\n+4. **Developing new workflows** - Build tests incrementally with real data\\n+\\n+#### Basic usage\\n+\\n+```bash\\n+# Run all test cases with real providers\\n+visor test --config my-workflow.yaml --no-mocks\\n+\\n+# Run a specific test case with real providers\\n+visor test --config my-workflow.yaml --no-mocks --only \\\"my-test-case\\\"\\n+```\\n+\\n+#### Suggested mocks output\\n+\\n+When running with `--no-mocks`, Visor captures each step's output and prints it as YAML you can copy directly into your test case:\\n+\\n+```\\n+üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)\\n+   Step outputs will be captured and printed as suggested mocks\\n+\\n+... test execution ...\\n+\\n+üìã Suggested mocks (copy to your test case):\\n+mocks:\\n+  extract-keys:\\n+    data:\\n+      - PROJ-123\\n+      - DEV-456\\n+    count: 2\\n+  fetch-issues:\\n+    data:\\n+      - key: PROJ-123\\n+        summary: Fix authentication bug\\n+        status: In Progress\\n+```\\n+\\n+Copy the YAML under `mocks:` into your test case's `mocks:` section.\\n+\\n+#### Workflow for building tests\\n+\\n+1. **Start with a minimal test case** (no mocks):\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+   ```\\n+\\n+2. **Run with `--no-mocks`** to capture real outputs:\\n+   ```bash\\n+   visor test --config workflow.yaml --no-mocks --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+3. **Copy the suggested mocks** into your test case:\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+         mocks:\\n+           extract-keys:\\n+             data: [\\\"PROJ-123\\\"]\\n+             count: 1\\n+           # ... rest of captured mocks\\n+   ```\\n+\\n+4. **Add assertions** based on the real data:\\n+   ```yaml\\n+         expect:\\n+           workflow_output:\\n+             - path: issue_count\\n+               equals: 1\\n+   ```\\n+\\n+5. **Run normally** to verify mocks work:\\n+   ```bash\\n+   visor test --config workflow.yaml --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+#### Debugging with `--no-mocks`\\n+\\n+When a test fails with mocks, use `--no-mocks` to see what's actually happening:\\n+\\n+```bash\\n+# See real API responses and errors\\n+visor test --config workflow.yaml --no-mocks --only \\\"failing-test\\\"\\n+\\n+# Common issues revealed:\\n+# - Missing or expired credentials\\n+# - API endpoint changes\\n+# - Unexpected response formats\\n+# - Network/timeout issues\\n+```\\n+\\n+The real error messages and responses help identify whether the issue is with your mocks or the actual integration.\\n+\\n ### More examples\\n \\n-- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags  \\n-- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration  \\n+- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags\\n+- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration\\n - `examples/` ‚Äì MCP, Jira, and advanced configs\\n\",\"status\":\"modified\"},{\"filename\":\"package-lock.json\",\"additions\":252,\"deletions\":9,\"changes\":261,\"patch\":\"diff --git a/package-lock.json b/package-lock.json\\nindex 5fb3ad89..e5b10d27 100644\\n--- a/package-lock.json\\n+++ b/package-lock.json\\n@@ -7,6 +7,7 @@\\n     \\\"\\\": {\\n       \\\"name\\\": \\\"@probelabs/visor\\\",\\n       \\\"version\\\": \\\"0.1.42\\\",\\n+      \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"MIT\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@actions/core\\\": \\\"^1.11.1\\\",\\n@@ -16,7 +17,7 @@\\n         \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n         \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n         \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n         \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n         \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -54,6 +55,7 @@\\n         \\\"husky\\\": \\\"^9.1.7\\\",\\n         \\\"jest\\\": \\\"^30.1.3\\\",\\n         \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+        \\\"patch-package\\\": \\\"^8.0.1\\\",\\n         \\\"prettier\\\": \\\"^3.6.2\\\",\\n         \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n         \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n@@ -5942,9 +5944,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/maid\\\": {\\n-      \\\"version\\\": \\\"0.0.21\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.21.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-H3EA2tDjWsAPgXZAwfHrNoEXnHmQFS9+JGDK/YpeiF3g3FxPgnWBgtgYPgtNuETQsEXxR1RzDgQVJL2JHWRRHw==\\\",\\n+      \\\"version\\\": \\\"0.0.22\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.22.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-FYcyyP4BlVrHQmBznXFPpMbvUqFI6wbecHUUe4S+yIeDvJ8W4bP0ndZ0cMoc2p7hekWYAG4RtlohQBr8e4TKOg==\\\",\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@types/dagre\\\": \\\"^0.7.53\\\",\\n@@ -5960,9 +5962,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/probe\\\": {\\n-      \\\"version\\\": \\\"0.6.0-rc169\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc169.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-/laV3+1TSjASru8gGSvGwyjQ7jalMkFuZETl35uv37J1Hb4eG95CbKGU5NihcCrLXR44q1lKJiQGHXcAav2mBA==\\\",\\n+      \\\"version\\\": \\\"0.6.0-rc176\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc176.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-w/pAvnz2/yXb4FQXZV2QazxuW02xsI/wRg36yxZoLSG0O/hIncjF4rquZe3O+emceI2Ks5zbrA7cgQEdpwT8vA==\\\",\\n       \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n@@ -5972,7 +5974,7 @@\\n         \\\"@ai-sdk/openai\\\": \\\"^2.0.10\\\",\\n         \\\"@anthropic-ai/claude-agent-sdk\\\": \\\"^0.1.46\\\",\\n         \\\"@modelcontextprotocol/sdk\\\": \\\"^1.0.0\\\",\\n-        \\\"@probelabs/maid\\\": \\\"^0.0.21\\\",\\n+        \\\"@probelabs/maid\\\": \\\"^0.0.22\\\",\\n         \\\"adm-zip\\\": \\\"^0.5.16\\\",\\n         \\\"ai\\\": \\\"^5.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -8192,6 +8194,13 @@\\n         \\\"node\\\": \\\">= 20\\\"\\n       }\\n     },\\n+    \\\"node_modules/@yarnpkg/lockfile\\\": {\\n+      \\\"version\\\": \\\"1.1.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@yarnpkg/lockfile/-/lockfile-1.1.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-GpSwvyXOcOOlV70vbnzjj4fW5xW/FdUF6nQEt1ENy7m4ZCczi1+/buVUPAqmGfqznsORNFzUMjctTIp8a9tuCQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"BSD-2-Clause\\\"\\n+    },\\n     \\\"node_modules/accepts\\\": {\\n       \\\"version\\\": \\\"1.3.8\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/accepts/-/accepts-1.3.8.tgz\\\",\\n@@ -9180,6 +9189,25 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/call-bind\\\": {\\n+      \\\"version\\\": \\\"1.0.8\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind/-/call-bind-1.0.8.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-oKlSFMcMwpUg2ednkhQ454wfWiU/ul3CkJe/PEHcTKuiX6RpbehUiFMXu13HalGZxfUwCQzZG747YXBn1im9ww==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind-apply-helpers\\\": \\\"^1.0.0\\\",\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"set-function-length\\\": \\\"^1.2.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/call-bind-apply-helpers\\\": {\\n       \\\"version\\\": \\\"1.0.2\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind-apply-helpers/-/call-bind-apply-helpers-1.0.2.tgz\\\",\\n@@ -10613,6 +10641,24 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/define-data-property\\\": {\\n+      \\\"version\\\": \\\"1.1.4\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/define-data-property/-/define-data-property-1.1.4.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-rBMvIzlpA8v6E+SJZoo++HAYqsLrkg7MSfIinMPFhmkorw7X+dOXVJQs+QT69zGkzMyfDnIMN2Wid1+NbL3T+A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/define-lazy-prop\\\": {\\n       \\\"version\\\": \\\"3.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/define-lazy-prop/-/define-lazy-prop-3.0.0.tgz\\\",\\n@@ -11794,6 +11840,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/find-yarn-workspace-root\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/find-yarn-workspace-root/-/find-yarn-workspace-root-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-1IMnbjt4KzsQfnhnzNd8wUEgXZ44IzZaZmnLYx7D5FZlaHt2gW20Cri8Q+E/t5tIj4+epTBub+2Zxu/vNILzqQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Apache-2.0\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"micromatch\\\": \\\"^4.0.2\\\"\\n+      }\\n+    },\\n     \\\"node_modules/fix-dts-default-cjs-exports\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/fix-dts-default-cjs-exports/-/fix-dts-default-cjs-exports-1.0.1.tgz\\\",\\n@@ -12267,6 +12323,19 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/has-property-descriptors\\\": {\\n+      \\\"version\\\": \\\"1.0.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/has-property-descriptors/-/has-property-descriptors-1.0.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-55JNKuIW+vq4Ke1BjOTjM2YctQIvCT7GFzHwmfZPGo5wnrgkid0YQtnAleFSqumZm4az3n2BS+erby5ipJdgrg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/has-symbols\\\": {\\n       \\\"version\\\": \\\"1.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/has-symbols/-/has-symbols-1.1.0.tgz\\\",\\n@@ -12784,6 +12853,13 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/isarray\\\": {\\n+      \\\"version\\\": \\\"2.0.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/isarray/-/isarray-2.0.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-xHjhDr3cNBK0BzdUJSPXZntQUx/mwMS5Rw4A7lPJ90XGAO6ISP/ePDNuo0vhqOZU+UD5JoodwCAAoZQd3FeAKw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\"\\n+    },\\n     \\\"node_modules/isexe\\\": {\\n       \\\"version\\\": \\\"2.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz\\\",\\n@@ -13594,6 +13670,26 @@\\n       \\\"integrity\\\": \\\"sha512-NM8/P9n3XjXhIZn1lLhkFaACTOURQXjWhV4BA/RnOv8xvgqtqpAX9IO4mRQxSx1Rlo4tqzeqb0sOlruaOy3dug==\\\",\\n       \\\"license\\\": \\\"MIT\\\"\\n     },\\n+    \\\"node_modules/json-stable-stringify\\\": {\\n+      \\\"version\\\": \\\"1.3.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify/-/json-stable-stringify-1.3.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-qtYiSSFlwot9XHtF9bD9c7rwKjr+RecWT//ZnPvSmEjpV5mmPOCN4j8UjY5hbjNkOwZ/jQv3J6R1/pL7RwgMsg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind\\\": \\\"^1.0.8\\\",\\n+        \\\"call-bound\\\": \\\"^1.0.4\\\",\\n+        \\\"isarray\\\": \\\"^2.0.5\\\",\\n+        \\\"jsonify\\\": \\\"^0.0.1\\\",\\n+        \\\"object-keys\\\": \\\"^1.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/json-stable-stringify-without-jsonify\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify-without-jsonify/-/json-stable-stringify-without-jsonify-1.0.1.tgz\\\",\\n@@ -13633,6 +13729,16 @@\\n         \\\"graceful-fs\\\": \\\"^4.1.6\\\"\\n       }\\n     },\\n+    \\\"node_modules/jsonify\\\": {\\n+      \\\"version\\\": \\\"0.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/jsonify/-/jsonify-0.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-2/Ki0GcmuqSrgFyelQq9M05y7PS0mEwuIzrf3f1fPqkVDVRvZrPZtVSMHxdgo8Aq0sxAOb/cr2aqqA3LeWHVPg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Public Domain\\\",\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/katex\\\": {\\n       \\\"version\\\": \\\"0.16.25\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/katex/-/katex-0.16.25.tgz\\\",\\n@@ -13676,6 +13782,16 @@\\n       \\\"integrity\\\": \\\"sha512-Ls993zuzfayK269Svk9hzpeGUKob/sIgZzyHYdjQoAdQetRKpOLj+k/QQQ/6Qi0Yz65mlROrfd+Ev+1+7dz9Kw==\\\",\\n       \\\"dev\\\": true\\n     },\\n+    \\\"node_modules/klaw-sync\\\": {\\n+      \\\"version\\\": \\\"6.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/klaw-sync/-/klaw-sync-6.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-nIeuVSzdCCs6TDPTqI8w1Yre34sSq7AkZ4B3sfOBbI2CgVSB4Du4aLQijFU2+lhAFCwt9+42Hel6lQNIv6AntQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"graceful-fs\\\": \\\"^4.1.11\\\"\\n+      }\\n+    },\\n     \\\"node_modules/kolorist\\\": {\\n       \\\"version\\\": \\\"1.8.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/kolorist/-/kolorist-1.8.0.tgz\\\",\\n@@ -14785,6 +14901,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n       }\\n     },\\n+    \\\"node_modules/object-keys\\\": {\\n+      \\\"version\\\": \\\"1.1.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/object-keys/-/object-keys-1.1.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NuAESUOUMrlIXOfHKzD6bpPu3tYt3xvjNdRIQ+FeT0lNb4K8WR70CaDxhuNguS2XG+GjkyMwOzsN5ZktImfhLA==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/ohash\\\": {\\n       \\\"version\\\": \\\"2.0.11\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/ohash/-/ohash-2.0.11.tgz\\\",\\n@@ -15022,6 +15148,95 @@\\n         \\\"node\\\": \\\">= 0.8\\\"\\n       }\\n     },\\n+    \\\"node_modules/patch-package\\\": {\\n+      \\\"version\\\": \\\"8.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/patch-package/-/patch-package-8.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-VsKRIA8f5uqHQ7NGhwIna6Bx6D9s/1iXlA1hthBVBEbkq+t4kXD0HHt+rJhf/Z+Ci0F/HCB2hvn0qLdLG+Qxlw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"@yarnpkg/lockfile\\\": \\\"^1.1.0\\\",\\n+        \\\"chalk\\\": \\\"^4.1.2\\\",\\n+        \\\"ci-info\\\": \\\"^3.7.0\\\",\\n+        \\\"cross-spawn\\\": \\\"^7.0.3\\\",\\n+        \\\"find-yarn-workspace-root\\\": \\\"^2.0.0\\\",\\n+        \\\"fs-extra\\\": \\\"^10.0.0\\\",\\n+        \\\"json-stable-stringify\\\": \\\"^1.0.2\\\",\\n+        \\\"klaw-sync\\\": \\\"^6.0.0\\\",\\n+        \\\"minimist\\\": \\\"^1.2.6\\\",\\n+        \\\"open\\\": \\\"^7.4.2\\\",\\n+        \\\"semver\\\": \\\"^7.5.3\\\",\\n+        \\\"slash\\\": \\\"^2.0.0\\\",\\n+        \\\"tmp\\\": \\\"^0.2.4\\\",\\n+        \\\"yaml\\\": \\\"^2.2.2\\\"\\n+      },\\n+      \\\"bin\\\": {\\n+        \\\"patch-package\\\": \\\"index.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14\\\",\\n+        \\\"npm\\\": \\\">5\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/ci-info\\\": {\\n+      \\\"version\\\": \\\"3.9.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/ci-info/-/ci-info-3.9.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NIxF55hv4nSqQswkAeiOi1r83xy8JldOFDTWiug55KBu9Jnblncd2U6ViHmYgHf01TPZS77NJBhBMKdWj9HQMQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"funding\\\": [\\n+        {\\n+          \\\"type\\\": \\\"github\\\",\\n+          \\\"url\\\": \\\"https://github.com/sponsors/sibiraj-s\\\"\\n+        }\\n+      ],\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/is-docker\\\": {\\n+      \\\"version\\\": \\\"2.2.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/is-docker/-/is-docker-2.2.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-F+i2BKsFrH66iaUFc0woD8sLy8getkwTwtOBjvs56Cx4CgJDeKQeqfz8wAYiSb8JOprWhHH5p77PbmYCvvUuXQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"bin\\\": {\\n+        \\\"is-docker\\\": \\\"cli.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/open\\\": {\\n+      \\\"version\\\": \\\"7.4.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/open/-/open-7.4.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-MVHddDVweXZF3awtlAS+6pgKLlm/JgxZ90+/NBurBoQctVOOB/zDdVjcyPzQ+0laDGbsWgrRkflI65sQeOgT9Q==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"is-docker\\\": \\\"^2.0.0\\\",\\n+        \\\"is-wsl\\\": \\\"^2.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/slash\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/slash/-/slash-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-ZYKh3Wh2z1PpEXWr0MpSBZ0V6mZHAQfYevttO11c51CaWjGTaadiKZ+wVt1PbMlDV5qhMFslpZCemhwOK7C89A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=6\\\"\\n+      }\\n+    },\\n     \\\"node_modules/path-data-parser\\\": {\\n       \\\"version\\\": \\\"0.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/path-data-parser/-/path-data-parser-0.1.0.tgz\\\",\\n@@ -16473,6 +16688,24 @@\\n         \\\"node\\\": \\\">= 0.8.0\\\"\\n       }\\n     },\\n+    \\\"node_modules/set-function-length\\\": {\\n+      \\\"version\\\": \\\"1.2.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/set-function-length/-/set-function-length-1.2.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-pgRc4hJ4/sNjWCSS9AmnS40x3bNMDTknHgL5UaMBTMyJnU90EgWh1Rz+MC9eFu4BuN/UwZjKQuY/1v3rM7HMfg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"define-data-property\\\": \\\"^1.1.4\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"function-bind\\\": \\\"^1.1.2\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\",\\n+        \\\"has-property-descriptors\\\": \\\"^1.0.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/setprototypeof\\\": {\\n       \\\"version\\\": \\\"1.2.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/setprototypeof/-/setprototypeof-1.2.0.tgz\\\",\\n@@ -17384,6 +17617,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/tmp\\\": {\\n+      \\\"version\\\": \\\"0.2.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/tmp/-/tmp-0.2.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-voyz6MApa1rQGUxT3E+BK7/ROe8itEx7vD8/HEvt4xwXucvQ5G5oeEiHkmHZJuBO21RpOf+YYm9MOivj709jow==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14.14\\\"\\n+      }\\n+    },\\n     \\\"node_modules/tmpl\\\": {\\n       \\\"version\\\": \\\"1.0.5\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/tmpl/-/tmpl-1.0.5.tgz\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"package.json\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/package.json b/package.json\\nindex f9ccbb6c..302d728f 100644\\n--- a/package.json\\n+++ b/package.json\\n@@ -49,6 +49,7 @@\\n     \\\"test:with-build\\\": \\\"npm run build:cli && jest\\\",\\n     \\\"test:yaml\\\": \\\"node dist/index.js test --progress compact\\\",\\n     \\\"prepare\\\": \\\"husky\\\",\\n+    \\\"postinstall\\\": \\\"patch-package\\\",\\n     \\\"pre-commit\\\": \\\"lint-staged\\\",\\n     \\\"deploy:site\\\": \\\"cd site && npx wrangler pages deploy . --project-name=visor-site --commit-dirty=true\\\",\\n     \\\"deploy:worker\\\": \\\"npx wrangler deploy\\\",\\n@@ -96,7 +97,7 @@\\n     \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n     \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n     \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n     \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n     \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n     \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -143,6 +144,7 @@\\n     \\\"husky\\\": \\\"^9.1.7\\\",\\n     \\\"jest\\\": \\\"^30.1.3\\\",\\n     \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+    \\\"patch-package\\\": \\\"^8.0.1\\\",\\n     \\\"prettier\\\": \\\"^3.6.2\\\",\\n     \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n     \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"src/ai-review-service.ts\",\"additions\":101,\"deletions\":151,\"changes\":252,\"patch\":\"diff --git a/src/ai-review-service.ts b/src/ai-review-service.ts\\nindex 6b9c5b2e..f18accea 100644\\n--- a/src/ai-review-service.ts\\n+++ b/src/ai-review-service.ts\\n@@ -65,6 +65,13 @@ export interface AIReviewConfig {\\n   allowBash?: boolean;\\n   // Advanced bash command execution configuration\\n   bashConfig?: import('./types/config').BashConfig;\\n+  // Optional workspace root and allowed folders for ProbeAgent.\\n+  // When provided, these are forwarded to ProbeAgent so tools like search/query\\n+  // operate inside the isolated workspace/projects instead of the Visor repo root.\\n+  path?: string;\\n+  allowedFolders?: string[];\\n+  // Completion prompt for post-completion validation/review (runs after attempt_completion)\\n+  completionPrompt?: string;\\n }\\n \\n export interface AIDebugInfo {\\n@@ -278,7 +285,7 @@ export class AIReviewService {\\n     try {\\n       const call = this.callProbeAgent(prompt, schema, debugInfo, checkName, sessionId);\\n       const timeoutMs = Math.max(0, this.config.timeout || 0);\\n-      const { response, effectiveSchema } =\\n+      const { response, effectiveSchema, sessionId: usedSessionId } =\\n         timeoutMs > 0 ? await this.withTimeout(call, timeoutMs, 'AI review') : await call;\\n       const processingTime = Date.now() - startTime;\\n \\n@@ -290,6 +297,11 @@ export class AIReviewService {\\n \\n       const result = this.parseAIResponse(response, debugInfo, effectiveSchema);\\n \\n+      // Expose the session ID used for this call so the engine can reuse it later\\n+      try {\\n+        (result as any).sessionId = usedSessionId;\\n+      } catch {}\\n+\\n       if (debugInfo) {\\n         result.debug = debugInfo;\\n       }\\n@@ -651,17 +663,34 @@ ${prContext}${slackContextXml}\\n     const prContextInfo = prInfo as PRInfo & {\\n       isPRContext?: boolean;\\n       includeCodeContext?: boolean;\\n+      slackConversation?: unknown;\\n     };\\n     const isIssue = prContextInfo.isIssue === true;\\n \\n     // Check if we should include code context (diffs)\\n     const isPRContext = prContextInfo.isPRContext === true;\\n-    // In PR context, always include diffs. Otherwise check the flag.\\n-    const includeCodeContext = isPRContext || prContextInfo.includeCodeContext !== false;\\n+    const isSlackMode = prContextInfo.slackConversation !== undefined;\\n+\\n+    // Determine whether to include code context:\\n+    // - In explicit PR context (GitHub PR events), always include diffs\\n+    // - In Slack mode, default to NO code context unless explicitly requested\\n+    // - Otherwise, include code context unless explicitly disabled\\n+    let includeCodeContext: boolean;\\n+    if (isPRContext) {\\n+      includeCodeContext = true;\\n+    } else if (isSlackMode) {\\n+      // In Slack mode, only include code context if explicitly set to true\\n+      includeCodeContext = prContextInfo.includeCodeContext === true;\\n+    } else {\\n+      // Default: include unless explicitly disabled\\n+      includeCodeContext = prContextInfo.includeCodeContext !== false;\\n+    }\\n \\n     // Log the decision for transparency (debug level)\\n     if (isPRContext) {\\n       log('üîç Including full code diffs in AI context (PR mode)');\\n+    } else if (isSlackMode && !includeCodeContext) {\\n+      log('üí¨ Slack mode: excluding code diffs (use includeCodeContext: true to enable)');\\n     } else if (!includeCodeContext) {\\n       log('üìä Including only file summary in AI context (no diffs)');\\n     } else {\\n@@ -1537,7 +1566,15 @@ ${'='.repeat(60)}\\n     debugInfo?: AIDebugInfo,\\n     _checkName?: string,\\n     providedSessionId?: string\\n-  ): Promise<{ response: string; effectiveSchema?: string }> {\\n+  ): Promise<{ response: string; effectiveSchema?: string; sessionId: string }> {\\n+    // Derive a stable session ID for this call so the engine can reuse it later\\n+    const sessionId =\\n+      providedSessionId ||\\n+      (() => {\\n+        const timestamp = new Date().toISOString();\\n+        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n+      })();\\n+\\n     // Handle mock model/provider\\n     if (this.config.model === 'mock' || this.config.provider === 'mock') {\\n       const inJest = !!process.env.JEST_WORKER_ID;\\n@@ -1545,19 +1582,17 @@ ${'='.repeat(60)}\\n       if (!inJest) {\\n         // Fast path for CLI/integration: synthesize a mock response without invoking ProbeAgent\\n         const response = await this.generateMockResponse(prompt, _checkName, schema);\\n-        return { response, effectiveSchema: typeof schema === 'object' ? 'custom' : schema };\\n+        return {\\n+          response,\\n+          effectiveSchema: typeof schema === 'object' ? 'custom' : schema,\\n+          sessionId,\\n+        };\\n       }\\n       // In unit tests, still invoke ProbeAgent so tests can assert on options (schema) passed in\\n       // Fall through to normal flow below\\n     }\\n \\n     // Create ProbeAgent instance with proper options\\n-    const sessionId =\\n-      providedSessionId ||\\n-      (() => {\\n-        const timestamp = new Date().toISOString();\\n-        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n-      })();\\n \\n     log('ü§ñ Creating ProbeAgent for AI review...');\\n     log(`üÜî Session ID: ${sessionId}`);\\n@@ -1674,6 +1709,35 @@ ${'='.repeat(60)}\\n         (options as any).bashConfig = this.config.bashConfig;\\n       }\\n \\n+      // Pass completion prompt for post-completion validation/review\\n+      if (this.config.completionPrompt !== undefined) {\\n+        (options as any).completionPrompt = this.config.completionPrompt;\\n+      }\\n+\\n+      // Propagate workspace / allowed folders to ProbeAgent so that tools\\n+      // operate inside the isolated workspace and project checkouts instead\\n+      // of the Visor repository root.\\n+      try {\\n+        const cfgAny: any = this.config as any;\\n+        const allowedFolders = cfgAny.allowedFolders as string[] | undefined;\\n+        const workspacePath =\\n+          cfgAny.workspacePath || cfgAny.path || (Array.isArray(allowedFolders) && allowedFolders[0]);\\n+        if (Array.isArray(allowedFolders) && allowedFolders.length > 0) {\\n+          (options as any).allowedFolders = allowedFolders;\\n+          if (!options.path && workspacePath) {\\n+            (options as any).path = workspacePath;\\n+          }\\n+          log(`üóÇÔ∏è ProbeAgent workspace config:`);\\n+          log(`   path (cwd): ${(options as any).path}`);\\n+          log(`   allowedFolders[0]: ${allowedFolders[0]}`);\\n+        } else if (workspacePath) {\\n+          (options as any).path = workspacePath;\\n+          log(`üóÇÔ∏è ProbeAgent path: ${workspacePath} (no allowedFolders)`);\\n+        }\\n+      } catch {\\n+        // Best-effort only; fall back to ProbeAgent defaults on error.\\n+      }\\n+\\n       // Add provider-specific options if configured\\n       if (this.config.provider) {\\n         // Map claude-code to anthropic for ProbeAgent compatibility\\n@@ -2037,7 +2101,7 @@ ${'='.repeat(60)}\\n         log(`üîß Debug: Registered AI session for potential reuse: ${sessionId}`);\\n       }\\n \\n-      return { response, effectiveSchema };\\n+      return { response, effectiveSchema, sessionId };\\n     } catch (error) {\\n       console.error('‚ùå ProbeAgent failed:', error);\\n       throw new Error(\\n@@ -2191,13 +2255,23 @@ ${'='.repeat(60)}\\n         // For other schemas (code-review, etc.), extract and parse JSON with boundary detection\\n         log('üîç Extracting JSON from AI response...');\\n \\n-        // Try direct parsing first - if AI returned pure JSON\\n+        // Sanitize response: strip BOM, zero-width chars, and other invisible characters\\n+        // that can cause JSON parsing to fail even when the text looks valid\\n+        const sanitizedResponse = response\\n+          .replace(/^\\\\uFEFF/, '') // BOM\\n+          .replace(/[\\\\u200B-\\\\u200D\\\\uFEFF\\\\u00A0]/g, '') // Zero-width chars, NBSP\\n+          .replace(/[\\\\x00-\\\\x08\\\\x0B\\\\x0C\\\\x0E-\\\\x1F\\\\x7F]/g, '') // Control chars (except \\\\t \\\\n \\\\r)\\n+          .trim();\\n+\\n+        // Try direct JSON parsing - no bracket-matching extraction\\n+        // JSON validation is offloaded to Probe agent when schema is provided\\n         try {\\n-          reviewData = JSON.parse(response.trim());\\n+          reviewData = JSON.parse(sanitizedResponse);\\n           log('‚úÖ Successfully parsed direct JSON response');\\n           if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-        } catch {\\n-          log('üîç Direct parsing failed, trying to extract JSON from response...');\\n+        } catch (parseErr) {\\n+          const errMsg = parseErr instanceof Error ? parseErr.message : String(parseErr);\\n+          log(`üîç Direct JSON parsing failed: ${errMsg}`);\\n \\n           // If the response starts with \\\"I cannot\\\" or similar, it's likely a refusal\\n           if (\\n@@ -2210,66 +2284,16 @@ ${'='.repeat(60)}\\n             };\\n           }\\n \\n-          // Try to extract JSON using improved method with proper bracket matching\\n-          const jsonString = this.extractJsonFromResponse(response);\\n-\\n-          if (jsonString) {\\n-            try {\\n-              reviewData = JSON.parse(jsonString);\\n-              log('‚úÖ Successfully parsed extracted JSON');\\n-              if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-            } catch {\\n-              log('üîß Extracted JSON parsing failed, falling back to plain text handling...');\\n-\\n-              // Check if response is plain text and doesn't contain structured data\\n-              if (!response.includes('{') && !response.includes('}')) {\\n-                log('üîß Plain text response detected, creating structured fallback...');\\n-\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              } else {\\n-                // Fallback: treat the entire response as an issue\\n-                log('üîß Creating fallback response from non-JSON content...');\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              }\\n-            }\\n-          } else {\\n-            // No JSON found at all - treat as plain text response\\n-            log('üîß No JSON found in response, treating as plain text...');\\n-            reviewData = {\\n-              issues: [\\n-                {\\n-                  file: 'AI_RESPONSE',\\n-                  line: 1,\\n-                  ruleId: 'ai/raw_response',\\n-                  message: response,\\n-                  severity: 'info',\\n-                  category: 'documentation',\\n-                },\\n-              ],\\n-            };\\n-          }\\n+          // Not valid JSON - treat entire response as text output\\n+          // This allows Probe (or other AI providers) to handle JSON validation\\n+          // and avoids false positives from bracket-matching (e.g., mermaid diagrams)\\n+          log('üîß Treating response as plain text (no JSON extraction)');\\n+          const trimmed = response.trim();\\n+          return {\\n+            issues: [],\\n+            output: { text: trimmed },\\n+            debug: debugInfo,\\n+          };\\n         }\\n       }\\n \\n@@ -2457,80 +2481,6 @@ ${'='.repeat(60)}\\n     }\\n   }\\n \\n-  /**\\n-   * Extract JSON from a response that might contain surrounding text\\n-   * Uses proper bracket matching to find valid JSON objects or arrays\\n-   */\\n-  private extractJsonFromResponse(response: string): string | null {\\n-    const text = response.trim();\\n-\\n-    // Try to find JSON objects first (higher priority)\\n-    let bestJson = this.findJsonWithBracketMatching(text, '{', '}');\\n-\\n-    // If no object found, try arrays\\n-    if (!bestJson) {\\n-      bestJson = this.findJsonWithBracketMatching(text, '[', ']');\\n-    }\\n-\\n-    return bestJson;\\n-  }\\n-\\n-  /**\\n-   * Find JSON with proper bracket matching to avoid false positives\\n-   */\\n-  private findJsonWithBracketMatching(\\n-    text: string,\\n-    openChar: string,\\n-    closeChar: string\\n-  ): string | null {\\n-    const firstIndex = text.indexOf(openChar);\\n-    if (firstIndex === -1) return null;\\n-\\n-    let depth = 0;\\n-    let inString = false;\\n-    let escaping = false;\\n-\\n-    for (let i = firstIndex; i < text.length; i++) {\\n-      const char = text[i];\\n-\\n-      if (escaping) {\\n-        escaping = false;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\\\\\\' && inString) {\\n-        escaping = true;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\"' && !escaping) {\\n-        inString = !inString;\\n-        continue;\\n-      }\\n-\\n-      if (!inString) {\\n-        if (char === openChar) {\\n-          depth++;\\n-        } else if (char === closeChar) {\\n-          depth--;\\n-          if (depth === 0) {\\n-            // Found matching closing bracket\\n-            const candidate = text.substring(firstIndex, i + 1);\\n-            try {\\n-              JSON.parse(candidate); // Validate it's actually valid JSON\\n-              return candidate;\\n-            } catch {\\n-              // This wasn't valid JSON, keep looking\\n-              break;\\n-            }\\n-          }\\n-        }\\n-      }\\n-    }\\n-\\n-    return null;\\n-  }\\n-\\n   /**\\n    * Generate mock response for testing\\n    */\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli-main.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/src/cli-main.ts b/src/cli-main.ts\\nindex 7dbef501..e671fbd8 100644\\n--- a/src/cli-main.ts\\n+++ b/src/cli-main.ts\\n@@ -158,6 +158,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n   }\\n   const only = getArg('--only');\\n   const bail = hasFlag('--bail');\\n+  const noMocks = hasFlag('--no-mocks');\\n   const listOnly = hasFlag('--list');\\n   const validateOnly = hasFlag('--validate');\\n   const progress = (getArg('--progress') as 'compact' | 'detailed' | undefined) || 'compact';\\n@@ -224,6 +225,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       const agg = await runSuites(multiFiles, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallelSuites: maxParallelSuites || Math.max(1, require('os').cpus()?.length || 2),\\n         maxParallel,\\n         promptMaxChars,\\n@@ -340,6 +342,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       runRes = await runner.runCases(tpath, suite, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallel,\\n         promptMaxChars,\\n         engineMode: 'state-machine',\\n@@ -609,6 +612,9 @@ export async function main(): Promise<void> {\\n     // Set environment variables early for proper logging in all modules\\n     process.env.VISOR_OUTPUT_FORMAT = options.output;\\n     process.env.VISOR_DEBUG = options.debug ? 'true' : 'false';\\n+    if (options.keepWorkspace) {\\n+      process.env.VISOR_KEEP_WORKSPACE = 'true';\\n+    }\\n     // Configure centralized logger\\n     configureLoggerFromCli({\\n       output: options.output,\\n@@ -716,6 +722,31 @@ export async function main(): Promise<void> {\\n       const threads = slackAny.threads || 'any';\\n       const allow = Array.isArray(slackAny.channel_allowlist) ? slackAny.channel_allowlist : [];\\n       const appToken = slackAny.app_token || process.env.SLACK_APP_TOKEN;\\n+\\n+      // Initialize telemetry for Slack mode (normally done later for CLI runs).\\n+      if ((config as any)?.telemetry) {\\n+        const t = (config as any).telemetry as {\\n+          enabled?: boolean;\\n+          sink?: 'otlp' | 'file' | 'console';\\n+          file?: { dir?: string; ndjson?: boolean };\\n+          tracing?: { auto_instrumentations?: boolean; trace_report?: { enabled?: boolean } };\\n+        };\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true' || !!t?.enabled,\\n+          sink:\\n+            (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || t?.sink || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR || t?.file?.dir, ndjson: !!t?.file?.ndjson },\\n+          autoInstrument: !!t?.tracing?.auto_instrumentations,\\n+          traceReport: !!t?.tracing?.trace_report?.enabled,\\n+        });\\n+      } else {\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true',\\n+          sink: (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR },\\n+        });\\n+      }\\n+\\n       const runner = new SlackSocketRunner(engine, config, {\\n         appToken,\\n         endpoint,\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli.ts\",\"additions\":5,\"deletions\":1,\"changes\":6,\"patch\":\"diff --git a/src/cli.ts b/src/cli.ts\\nindex 134429b2..b1c46213 100644\\n--- a/src/cli.ts\\n+++ b/src/cli.ts\\n@@ -76,6 +76,7 @@ export class CLI {\\n         parseInt(value, 10)\\n       )\\n       .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText())\\n       .exitOverride(); // Prevent automatic process.exit for better error handling\\n \\n@@ -154,6 +155,7 @@ export class CLI {\\n           parseInt(value, 10)\\n         )\\n         .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+        .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n         .allowUnknownOption(false)\\n         .allowExcessArguments(false) // Don't allow positional arguments\\n         .addHelpText('after', this.getExamplesText())\\n@@ -224,6 +226,7 @@ export class CLI {\\n         message: options.message,\\n         githubV2: false,\\n         slack: Boolean(options.slack),\\n+        keepWorkspace: Boolean(options.keepWorkspace),\\n       };\\n     } catch (error: unknown) {\\n       // Handle commander.js exit overrides for help/version ONLY\\n@@ -348,6 +351,7 @@ export class CLI {\\n       .option('--debug-port <port>', 'Port for debug server (default: 3456)', value =>\\n         parseInt(value, 10)\\n       )\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText());\\n \\n     // Get the basic help and append examples manually if addHelpText doesn't work\\n\",\"status\":\"modified\"},{\"filename\":\"src/config.ts\",\"additions\":67,\"deletions\":17,\"changes\":84,\"patch\":\"diff --git a/src/config.ts b/src/config.ts\\nindex 407efa2d..02abe9df 100644\\n--- a/src/config.ts\\n+++ b/src/config.ts\\n@@ -20,6 +20,7 @@ import { ConfigLoader, ConfigLoaderOptions } from './utils/config-loader';\\n import { ConfigMerger } from './utils/config-merger';\\n import Ajv from 'ajv';\\n import addFormats from 'ajv-formats';\\n+import { validateJsSyntax } from './utils/sandbox';\\n \\n /**\\n  * Valid event triggers for checks\\n@@ -477,16 +478,29 @@ export class ConfigManager {\\n \\n     logger.info(`Registered workflow '${workflowId}' for standalone execution`);\\n \\n-    // Create a COMPLETELY NEW visor config with ONLY the test checks\\n-    // This prevents any workflow fields from leaking into the config\\n-    const visorConfig: Partial<VisorConfig> = {\\n+    // For standalone workflow testing, use the workflow's steps as checks\\n+    // This allows the test runner to execute the workflow steps directly\\n+    const workflowSteps = workflowData.steps || {};\\n+\\n+    // Create a config that includes the workflow steps as checks,\\n+    // plus the tests section and workflow metadata for output computation\\n+    const visorConfig: Partial<VisorConfig> & { tests?: any; outputs?: any; inputs?: any } = {\\n       version: '1.0',\\n-      steps: tests,\\n-      checks: tests, // Backward compatibility\\n+      steps: workflowSteps,\\n+      checks: workflowSteps,\\n+      tests: tests, // Preserve test harness config (may be empty if stripped by test runner)\\n     };\\n \\n-    logger.debug(`Standalone workflow config has ${Object.keys(tests).length} test checks`);\\n-    logger.debug(`Test check names: ${Object.keys(tests).join(', ')}`);\\n+    // Preserve workflow metadata for output computation during tests\\n+    if (workflowData.outputs) {\\n+      visorConfig.outputs = workflowData.outputs;\\n+    }\\n+    if (workflowData.inputs) {\\n+      visorConfig.inputs = workflowData.inputs;\\n+    }\\n+\\n+    logger.debug(`Standalone workflow config has ${Object.keys(workflowSteps).length} workflow steps as checks`);\\n+    logger.debug(`Workflow step names: ${Object.keys(workflowSteps).join(', ')}`);\\n     logger.debug(`Config keys after conversion: ${Object.keys(visorConfig).join(', ')}`);\\n \\n     return visorConfig;\\n@@ -982,26 +996,29 @@ export class ConfigManager {\\n \\n     // Validate reuse_ai_session configuration\\n     if (checkConfig.reuse_ai_session !== undefined) {\\n-      const isString = typeof checkConfig.reuse_ai_session === 'string';\\n-      const isBoolean = typeof checkConfig.reuse_ai_session === 'boolean';\\n+      const reuseValue = checkConfig.reuse_ai_session as unknown;\\n+      const isString = typeof reuseValue === 'string';\\n+      const isBoolean = typeof reuseValue === 'boolean';\\n+      const isSelf = reuseValue === 'self';\\n \\n       if (!isString && !isBoolean) {\\n         errors.push({\\n           field: `checks.${checkName}.reuse_ai_session`,\\n           message: `Invalid reuse_ai_session value for \\\"${checkName}\\\": must be string (check name) or boolean`,\\n-          value: checkConfig.reuse_ai_session,\\n+          value: reuseValue,\\n         });\\n-      } else if (isString) {\\n-        // When reuse_ai_session is a string, it must refer to a valid check\\n-        const targetCheckName = checkConfig.reuse_ai_session as string;\\n+      } else if (isString && !isSelf) {\\n+        // When reuse_ai_session is a string (other than the special 'self' value),\\n+        // it must refer to a valid check name\\n+        const targetCheckName = reuseValue as string;\\n         if (!config?.checks || !config.checks[targetCheckName]) {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" references non-existent check \\\"${targetCheckName}\\\" for session reuse`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n-      } else if (checkConfig.reuse_ai_session === true) {\\n+      } else if (reuseValue === true) {\\n         // When reuse_ai_session is true, depends_on must be specified and non-empty\\n         if (\\n           !checkConfig.depends_on ||\\n@@ -1011,7 +1028,7 @@ export class ConfigManager {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" has reuse_ai_session=true but missing or empty depends_on. Session reuse requires dependency on another check.`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n       }\\n@@ -1076,6 +1093,39 @@ export class ConfigManager {\\n         });\\n       }\\n     }\\n+\\n+    // Validate JavaScript syntax in transform_js and script content\\n+    try {\\n+      // Validate transform_js if present\\n+      const transformJs = (checkConfig as any).transform_js;\\n+      if (typeof transformJs === 'string' && transformJs.trim().length > 0) {\\n+        const result = validateJsSyntax(transformJs);\\n+        if (!result.valid) {\\n+          errors.push({\\n+            field: `checks.${checkName}.transform_js`,\\n+            message: `JavaScript syntax error in \\\"${checkName}\\\" transform_js: ${result.error}`,\\n+            value: transformJs.slice(0, 100) + (transformJs.length > 100 ? '...' : ''),\\n+          });\\n+        }\\n+      }\\n+\\n+      // Validate script content if type is 'script'\\n+      if (checkConfig.type === 'script') {\\n+        const content = (checkConfig as any).content;\\n+        if (typeof content === 'string' && content.trim().length > 0) {\\n+          const result = validateJsSyntax(content);\\n+          if (!result.valid) {\\n+            errors.push({\\n+              field: `checks.${checkName}.content`,\\n+              message: `JavaScript syntax error in \\\"${checkName}\\\" script: ${result.error}`,\\n+              value: content.slice(0, 100) + (content.length > 100 ? '...' : ''),\\n+            });\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Syntax validation is best-effort; don't fail the whole config on validation errors\\n+    }\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/failure-condition-evaluator.ts\",\"additions\":25,\"deletions\":9,\"changes\":34,\"patch\":\"diff --git a/src/failure-condition-evaluator.ts b/src/failure-condition-evaluator.ts\\nindex 3b54a015..5800df0a 100644\\n--- a/src/failure-condition-evaluator.ts\\n+++ b/src/failure-condition-evaluator.ts\\n@@ -137,6 +137,8 @@ export class FailureConditionEvaluator {\\n       previousResults?: Map<string, ReviewSummary>;\\n       authorAssociation?: string;\\n       workflowInputs?: Record<string, unknown>;\\n+      /** Current step's output for guarantee evaluation */\\n+      output?: unknown;\\n     }\\n   ): Promise<boolean> {\\n     // Build context for if evaluation\\n@@ -177,10 +179,10 @@ export class FailureConditionEvaluator {\\n       // Workflow inputs (for workflows)\\n       inputs: contextData?.workflowInputs || {},\\n \\n-      // Required output property (empty for if conditions)\\n-      output: {\\n-        issues: [],\\n-      },\\n+      // Output property: use provided output for guarantee evaluation, or empty for if conditions\\n+      output: contextData?.output !== undefined && contextData.output !== null && typeof contextData.output === 'object'\\n+        ? (contextData.output as Record<string, unknown>)\\n+        : { issues: [] },\\n       // Author association (used by permission helpers)\\n       authorAssociation: contextData?.authorAssociation,\\n \\n@@ -213,10 +215,10 @@ export class FailureConditionEvaluator {\\n               m && typeof m.get === 'function' ? m.get('all_valid', 'fact-validation') : undefined;\\n             memStr = ` mem.fact-validation.all_valid=${String(v)}`;\\n           } catch {}\\n+          // Debug if-eval output (only when VISOR_DEBUG enabled)\\n+          const outputKeys = Object.keys(context.outputs || {});\\n           console.error(\\n-            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" env.ENABLE_FACT_VALIDATION=${String(\\n-              (envMap as any).ENABLE_FACT_VALIDATION\\n-            )} event=${context.event?.event_name} result=${String(res)}${memStr}`\\n+            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" result=${String(res)} outputKeys=[${outputKeys.join(',')}]`\\n           );\\n         }\\n       } catch {}\\n@@ -480,7 +482,18 @@ export class FailureConditionEvaluator {\\n \\n       // Extract context variables\\n       const output = context.output || {};\\n-      const issues = output.issues || [];\\n+      // Ensure issues is an array - it might be a JSON string from workflow outputs\\n+      let issues = output.issues || [];\\n+      if (typeof issues === 'string') {\\n+        try {\\n+          issues = JSON.parse(issues);\\n+        } catch {\\n+          issues = [];\\n+        }\\n+      }\\n+      if (!Array.isArray(issues)) {\\n+        issues = [];\\n+      }\\n \\n       // Backward compatibility: provide metadata for transition period\\n       // TODO: Remove after all configurations are updated\\n@@ -518,6 +531,7 @@ export class FailureConditionEvaluator {\\n       const event = context.event || 'manual';\\n       const env = context.env || {};\\n       const outputs = context.outputs || {};\\n+      const inputs = context.inputs || {};\\n       const debugData = context.debug || null;\\n \\n       // Get memory store and create accessor for fail_if expressions\\n@@ -555,6 +569,7 @@ export class FailureConditionEvaluator {\\n         filesCount,\\n         event,\\n         env,\\n+        inputs,\\n         // Helper functions\\n         contains,\\n         startsWith,\\n@@ -621,6 +636,7 @@ export class FailureConditionEvaluator {\\n             filesCount,\\n             event,\\n             env,\\n+            inputs,\\n             // Helpers\\n             contains,\\n             startsWith,\\n\",\"status\":\"modified\"},{\"filename\":\"src/frontends/slack-frontend.ts\",\"additions\":107,\"deletions\":6,\"changes\":113,\"patch\":\"diff --git a/src/frontends/slack-frontend.ts b/src/frontends/slack-frontend.ts\\nindex 97161a7d..41735aa2 100644\\n--- a/src/frontends/slack-frontend.ts\\n+++ b/src/frontends/slack-frontend.ts\\n@@ -1,6 +1,32 @@\\n+/**\\n+ * Slack Frontend for Visor workflows.\\n+ *\\n+ * Features:\\n+ * - Posts AI replies to Slack threads\\n+ * - Converts Markdown to Slack mrkdwn format\\n+ * - Renders mermaid diagrams to PNG and uploads as images\\n+ * - Manages üëÄ/üëç reactions for acknowledgement\\n+ * - Handles human input prompts via prompt-state\\n+ *\\n+ * Mermaid Diagram Rendering:\\n+ * - Detects ```mermaid code blocks in AI responses\\n+ * - Renders to PNG using @mermaid-js/mermaid-cli (mmdc)\\n+ * - Uploads rendered images to Slack thread\\n+ * - Replaces mermaid blocks with \\\"_(See diagram above)_\\\" placeholder\\n+ *\\n+ * Requirements for mermaid rendering:\\n+ * - Node.js and npx in PATH\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ * - On Linux: apt-get install chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ */\\n import type { Frontend, FrontendContext } from './host';\\n import { SlackClient } from '../slack/client';\\n-import { formatSlackText } from '../slack/markdown';\\n+import {\\n+  formatSlackText,\\n+  extractMermaidDiagrams,\\n+  renderMermaidToPng,\\n+  replaceMermaidBlocks,\\n+} from '../slack/markdown';\\n \\n type SlackFrontendConfig = {\\n   defaultChannel?: string;\\n@@ -284,6 +310,9 @@ export class SlackFrontend implements Frontend {\\n       const isLogChat = providerType === 'log' && checkCfg.group === 'chat';\\n       if (!isAi && !isLogChat) return;\\n \\n+      // Skip internal steps - they're intermediate processing and shouldn't post to Slack\\n+      if (checkCfg.criticality === 'internal') return;\\n+\\n       // For AI checks, only post when using simple/unstructured schemas (or none).\\n       if (isAi) {\\n         const schema = checkCfg.schema;\\n@@ -316,6 +345,13 @@ export class SlackFrontend implements Frontend {\\n         ) {\\n           text = (result as any).content.trim();\\n         }\\n+      } else if (isLogChat && typeof (result as any)?.logOutput === 'string') {\\n+        // For log-based chat checks, render the formatted log output as the\\n+        // Slack message when no structured text field is present.\\n+        const raw = (result as any).logOutput;\\n+        if (raw.trim().length > 0) {\\n+          text = raw.trim();\\n+        }\\n       } else if (isAi && showRawOutput && out !== undefined) {\\n         try {\\n           text = JSON.stringify(out, null, 2);\\n@@ -325,13 +361,78 @@ export class SlackFrontend implements Frontend {\\n       }\\n       if (!text) return;\\n \\n-      const formattedText = formatSlackText(text);\\n+      // Extract and render mermaid diagrams before posting\\n+      const diagrams = extractMermaidDiagrams(text);\\n+      let processedText = text;\\n+\\n+      if (diagrams.length > 0) {\\n+        try {\\n+          ctx.logger.info(\\n+            `[slack-frontend] found ${diagrams.length} mermaid diagram(s) to render for ${checkId}`\\n+          );\\n+        } catch {}\\n+\\n+        // Render and upload each diagram\\n+        const uploadedCount: number[] = [];\\n+        for (let i = 0; i < diagrams.length; i++) {\\n+          const diagram = diagrams[i];\\n+          try {\\n+            ctx.logger.info(`[slack-frontend] rendering mermaid diagram ${i + 1}...`);\\n+            const pngBuffer = await renderMermaidToPng(diagram.code);\\n+            if (pngBuffer) {\\n+              ctx.logger.info(\\n+                `[slack-frontend] rendered diagram ${i + 1}, size=${pngBuffer.length} bytes, uploading...`\\n+              );\\n+              const filename = `diagram-${i + 1}.png`;\\n+              const uploadResult = await slack.files.uploadV2({\\n+                content: pngBuffer,\\n+                filename,\\n+                channel,\\n+                thread_ts: threadTs,\\n+                title: `Diagram ${i + 1}`,\\n+              });\\n+              if (uploadResult.ok) {\\n+                uploadedCount.push(i);\\n+                ctx.logger.info(`[slack-frontend] uploaded mermaid diagram ${i + 1} to ${channel}`);\\n+              } else {\\n+                ctx.logger.warn(`[slack-frontend] upload failed for diagram ${i + 1}`);\\n+              }\\n+            } else {\\n+              ctx.logger.warn(\\n+                `[slack-frontend] mermaid rendering returned null for diagram ${i + 1} (mmdc failed or not installed)`\\n+              );\\n+            }\\n+          } catch (e) {\\n+            ctx.logger.warn(\\n+              `[slack-frontend] failed to render/upload mermaid diagram ${i + 1}: ${\\n+                e instanceof Error ? e.message : String(e)\\n+              }`\\n+            );\\n+          }\\n+        }\\n+\\n+        // Replace mermaid blocks with placeholder text\\n+        if (uploadedCount.length > 0) {\\n+          processedText = replaceMermaidBlocks(text, diagrams, (idx) =>\\n+            uploadedCount.includes(idx) ? '_(See diagram above)_' : '_(Diagram rendering failed)_'\\n+          );\\n+        }\\n+      }\\n+\\n+      const formattedText = formatSlackText(processedText);\\n       await slack.chat.postMessage({ channel, text: formattedText, thread_ts: threadTs });\\n+      ctx.logger.info(\\n+        `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+      );\\n+    } catch (outerErr) {\\n+      // Log errors instead of silently swallowing them\\n       try {\\n-        ctx.logger.info(\\n-          `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+        ctx.logger.warn(\\n+          `[slack-frontend] maybePostDirectReply failed for ${checkId}: ${\\n+            outerErr instanceof Error ? outerErr.message : String(outerErr)\\n+          }`\\n         );\\n       } catch {}\\n-    } catch {}\\n+    }\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/generated/config-schema.ts\",\"additions\":233,\"deletions\":13,\"changes\":246,\"patch\":\"diff --git a/src/generated/config-schema.ts b/src/generated/config-schema.ts\\nindex b3caca4f..ecab390d 100644\\n--- a/src/generated/config-schema.ts\\n+++ b/src/generated/config-schema.ts\\n@@ -56,6 +56,20 @@ export const configSchema = {\\n           },\\n           description: 'Import workflow definitions from external files or URLs',\\n         },\\n+        inputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowInput',\\n+          },\\n+          description: 'Workflow inputs (for standalone reusable workflows)',\\n+        },\\n+        outputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowOutput',\\n+          },\\n+          description: 'Workflow outputs (for standalone reusable workflows)',\\n+        },\\n         steps: {\\n           $ref: '#/definitions/Record%3Cstring%2CCheckConfig%3E',\\n           description: 'Step configurations (recommended)',\\n@@ -67,7 +81,7 @@ export const configSchema = {\\n         },\\n         output: {\\n           $ref: '#/definitions/OutputConfig',\\n-          description: 'Output configuration',\\n+          description: 'Output configuration (optional - defaults provided)',\\n         },\\n         http_server: {\\n           $ref: '#/definitions/HttpServerConfig',\\n@@ -139,8 +153,16 @@ export const configSchema = {\\n           },\\n           description: 'Optional integrations: event-driven frontends (e.g., ndjson-sink, github)',\\n         },\\n+        workspace: {\\n+          $ref: '#/definitions/WorkspaceConfig',\\n+          description: 'Workspace isolation configuration for sandboxed execution',\\n+        },\\n+        slack: {\\n+          $ref: '#/definitions/SlackConfig',\\n+          description: 'Slack configuration',\\n+        },\\n       },\\n-      required: ['output', 'version'],\\n+      required: ['version'],\\n       patternProperties: {\\n         '^x-': {},\\n       },\\n@@ -243,6 +265,63 @@ export const configSchema = {\\n         type: 'string',\\n       },\\n     },\\n+    WorkflowInput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Input parameter name',\\n+        },\\n+        schema: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'JSON Schema for the input',\\n+        },\\n+        required: {\\n+          type: 'boolean',\\n+          description: 'Whether this input is required',\\n+        },\\n+        default: {\\n+          description: 'Default value if not provided',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow input definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    WorkflowOutput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Output name',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+        value: {\\n+          type: 'string',\\n+          description: 'Value using Liquid template syntax (references step outputs)',\\n+        },\\n+        value_js: {\\n+          type: 'string',\\n+          description: 'Value using JavaScript expression (alternative to value)',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow output definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n     'Record<string,CheckConfig>': {\\n       type: 'object',\\n       additionalProperties: {\\n@@ -390,11 +469,19 @@ export const configSchema = {\\n           description: 'Timeout in seconds for command execution (default: 60)',\\n         },\\n         depends_on: {\\n-          type: 'array',\\n-          items: {\\n-            type: 'string',\\n-          },\\n-          description: 'Check IDs that this check depends on (optional)',\\n+          anyOf: [\\n+            {\\n+              type: 'string',\\n+            },\\n+            {\\n+              type: 'array',\\n+              items: {\\n+                type: 'string',\\n+              },\\n+            },\\n+          ],\\n+          description:\\n+            'Check IDs that this check depends on (optional). Accepts single string or array.',\\n         },\\n         group: {\\n           type: 'string',\\n@@ -653,13 +740,93 @@ export const configSchema = {\\n           description: 'Arguments/inputs for the workflow',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Override specific step configurations in the workflow',\\n         },\\n         output_mapping: {\\n           $ref: '#/definitions/Record%3Cstring%2Cstring%3E',\\n           description: 'Map workflow outputs to check outputs',\\n         },\\n+        workflow_inputs: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'Alias for args - workflow inputs (backward compatibility)',\\n+        },\\n+        config: {\\n+          type: 'string',\\n+          description:\\n+            'Config file path - alternative to workflow ID (loads a Visor config file as workflow)',\\n+        },\\n+        workflow_overrides: {\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n+          description: 'Alias for overrides - workflow step overrides (backward compatibility)',\\n+        },\\n+        ref: {\\n+          type: 'string',\\n+          description: 'Git reference to checkout (branch, tag, commit SHA) - supports templates',\\n+        },\\n+        repository: {\\n+          type: 'string',\\n+          description: 'Repository URL or owner/repo format (defaults to current repository)',\\n+        },\\n+        token: {\\n+          type: 'string',\\n+          description: 'GitHub token for private repositories (defaults to GITHUB_TOKEN env)',\\n+        },\\n+        fetch_depth: {\\n+          type: 'number',\\n+          description: 'Number of commits to fetch (0 for full history, default: 1)',\\n+        },\\n+        fetch_tags: {\\n+          type: 'boolean',\\n+          description: 'Whether to fetch tags (default: false)',\\n+        },\\n+        submodules: {\\n+          anyOf: [\\n+            {\\n+              type: 'boolean',\\n+            },\\n+            {\\n+              type: 'string',\\n+              const: 'recursive',\\n+            },\\n+          ],\\n+          description: \\\"Checkout submodules: false, true, or 'recursive'\\\",\\n+        },\\n+        working_directory: {\\n+          type: 'string',\\n+          description: 'Working directory for the checkout (defaults to temp directory)',\\n+        },\\n+        use_worktree: {\\n+          type: 'boolean',\\n+          description: 'Use git worktree for efficient parallel checkouts (default: true)',\\n+        },\\n+        clean: {\\n+          type: 'boolean',\\n+          description: 'Clean the working directory before checkout (default: true)',\\n+        },\\n+        sparse_checkout: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description: 'Sparse checkout paths - only checkout specific directories/files',\\n+        },\\n+        lfs: {\\n+          type: 'boolean',\\n+          description: 'Enable Git LFS (Large File Storage)',\\n+        },\\n+        clone_timeout_ms: {\\n+          type: 'number',\\n+          description: 'Timeout in ms for cloning the bare repository (default: 300000 = 5 min)',\\n+        },\\n+        cleanup_on_failure: {\\n+          type: 'boolean',\\n+          description: 'Clean up worktree on failure (default: true)',\\n+        },\\n+        persist_worktree: {\\n+          type: 'boolean',\\n+          description: 'Keep worktree after workflow completion (default: false)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'Configuration for a single check',\\n@@ -794,6 +961,11 @@ export const configSchema = {\\n           $ref: '#/definitions/BashConfig',\\n           description: 'Advanced bash command execution configuration',\\n         },\\n+        completion_prompt: {\\n+          type: 'string',\\n+          description:\\n+            'Completion prompt for post-completion validation/review (runs after attempt_completion)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'AI provider configuration',\\n@@ -1223,7 +1395,7 @@ export const configSchema = {\\n           description: 'Custom output name (defaults to workflow name)',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Step overrides',\\n         },\\n         output_mapping: {\\n@@ -1238,14 +1410,14 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n-    'Record<string,Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>>':\\n+    'Record<string,Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>>':\\n       {\\n         type: 'object',\\n         additionalProperties: {\\n-          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E',\\n+          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E',\\n         },\\n       },\\n-    'Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>': {\\n+    'Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>': {\\n       type: 'object',\\n       additionalProperties: false,\\n     },\\n@@ -1811,6 +1983,54 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n+    WorkspaceConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        enabled: {\\n+          type: 'boolean',\\n+          description: 'Enable workspace isolation (default: true when config present)',\\n+        },\\n+        base_path: {\\n+          type: 'string',\\n+          description: 'Base path for workspaces (default: /tmp/visor-workspaces)',\\n+        },\\n+        cleanup_on_exit: {\\n+          type: 'boolean',\\n+          description: 'Clean up workspace on exit (default: true)',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Workspace isolation configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    SlackConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        version: {\\n+          type: 'string',\\n+          description: 'Slack API version',\\n+        },\\n+        mentions: {\\n+          type: 'string',\\n+          description: \\\"Mention handling: 'all', 'direct', etc.\\\",\\n+        },\\n+        threads: {\\n+          type: 'string',\\n+          description: \\\"Thread handling: 'required', 'optional', etc.\\\",\\n+        },\\n+        show_raw_output: {\\n+          type: 'boolean',\\n+          description: 'Show raw output in Slack responses',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Slack configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n   },\\n } as const;\\n export default configSchema;\\n\",\"status\":\"modified\"},{\"filename\":\"src/index.ts\",\"additions\":5,\"deletions\":2,\"changes\":7,\"patch\":\"diff --git a/src/index.ts b/src/index.ts\\nindex 50b89df3..9621e733 100644\\n--- a/src/index.ts\\n+++ b/src/index.ts\\n@@ -628,8 +628,11 @@ function resolveDependencies(\\n \\n     // Get dependencies for this check\\n     const checkConfig = config?.checks?.[checkId];\\n+    // Normalize depends_on to array (supports string | string[])\\n+    const rawDeps = checkConfig?.depends_on;\\n+    const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     // Expand OR groups (pipe syntax) for dependency closure discovery\\n-    const dependencies = (checkConfig?.depends_on || []).flatMap(d =>\\n+    const dependencies = depsArray.flatMap((d: string) =>\\n       typeof d === 'string' && d.includes('|')\\n         ? d\\n             .split('|')\\n\",\"status\":\"modified\"},{\"filename\":\"src/liquid-extensions.ts\",\"additions\":68,\"deletions\":1,\"changes\":69,\"patch\":\"diff --git a/src/liquid-extensions.ts b/src/liquid-extensions.ts\\nindex e9efaf68..dbf9c6ed 100644\\n--- a/src/liquid-extensions.ts\\n+++ b/src/liquid-extensions.ts\\n@@ -124,6 +124,26 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     }\\n   });\\n \\n+  // Register base64 filter for encoding strings\\n+  // Usage: {{ \\\"user:password\\\" | base64 }}\\n+  liquid.registerFilter('base64', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    return Buffer.from(str).toString('base64');\\n+  });\\n+\\n+  // Register base64_decode filter for decoding base64 strings\\n+  // Usage: {{ encoded_value | base64_decode }}\\n+  liquid.registerFilter('base64_decode', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    try {\\n+      return Buffer.from(str, 'base64').toString('utf-8');\\n+    } catch {\\n+      return '[Error: Invalid base64 string]';\\n+    }\\n+  });\\n+\\n   // Sanitize a label to allowed characters only: [A-Za-z0-9:/]\\n   liquid.registerFilter('safe_label', (value: unknown) => sanitizeLabel(value));\\n \\n@@ -137,6 +157,53 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     return s.replace(/\\\\\\\\n/g, '\\\\n').replace(/\\\\\\\\r/g, '\\\\r').replace(/\\\\\\\\t/g, '\\\\t');\\n   });\\n \\n+  // JSON escape filter - escapes a string for use inside a JSON string value\\n+  // This escapes special characters like quotes, backslashes, and control characters\\n+  // Usage: \\\"jql\\\": \\\"{{ myValue | json_escape }}\\\"\\n+  liquid.registerFilter('json_escape', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const s = String(value);\\n+    // Use JSON.stringify which handles all escaping, then strip the surrounding quotes\\n+    const jsonStr = JSON.stringify(s);\\n+    // Remove the first and last character (the quotes added by JSON.stringify)\\n+    return jsonStr.slice(1, -1);\\n+  });\\n+\\n+  // Shell escape filter - wraps value in single quotes with proper escaping\\n+  // Usage: {{ value | shell_escape }}\\n+  // Example: \\\"hello'world\\\" becomes \\\"'hello'\\\\''world'\\\"\\n+  // This is POSIX-compliant and safe for arbitrary text including mermaid diagrams\\n+  liquid.registerFilter('shell_escape', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    // Replace single quotes with: end quote, escaped quote, start quote\\n+    // Then wrap the entire thing in single quotes\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Alias for shell_escape\\n+  liquid.registerFilter('escape_shell', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Shell escape for double quotes (less safe but sometimes needed)\\n+  // Usage: {{ value | shell_escape_double }}\\n+  // Escapes: $, `, \\\\, \\\", and !\\n+  liquid.registerFilter('shell_escape_double', (value: unknown) => {\\n+    if (value == null) return '\\\"\\\"';\\n+    const s = String(value);\\n+    // Escape characters that have special meaning inside double quotes\\n+    const escaped = s\\n+      .replace(/\\\\\\\\/g, '\\\\\\\\\\\\\\\\')  // backslash first\\n+      .replace(/\\\\$/g, '\\\\\\\\$')   // dollar sign\\n+      .replace(/`/g, '\\\\\\\\`')    // backticks\\n+      .replace(/\\\"/g, '\\\\\\\\\\\"')    // double quotes\\n+      .replace(/!/g, '\\\\\\\\!');   // history expansion\\n+    return '\\\"' + escaped + '\\\"';\\n+  });\\n+\\n   // Register author permission filters (from main)\\n   // These filters check the author's permission level; detect local mode for tests\\n   const isLocal = detectLocalMode();\\n\",\"status\":\"modified\"},{\"filename\":\"src/logger.ts\",\"additions\":31,\"deletions\":2,\"changes\":33,\"patch\":\"diff --git a/src/logger.ts b/src/logger.ts\\nindex 6148b77c..aee7136e 100644\\n--- a/src/logger.ts\\n+++ b/src/logger.ts\\n@@ -85,7 +85,36 @@ class Logger {\\n       if (this.showTimestamps) {\\n         const ts = new Date().toISOString();\\n         const lvl = level ? level : undefined;\\n-        const prefix = lvl ? `[${ts}] [${lvl}]` : `[${ts}]`;\\n+\\n+        let tsToken = `[${ts}]`;\\n+        let lvlToken = lvl ? `[${lvl}]` : '';\\n+\\n+        // Add simple ANSI colour when running in a TTY and not emitting\\n+        // JSON/SARIF. Colours are intentionally minimal and only applied\\n+        // to the prefix markers, not the full line.\\n+        if (this.isTTY && !this.isJsonLike) {\\n+          const reset = '\\\\x1b[0m';\\n+          const dim = '\\\\x1b[2m';\\n+          const colours: Record<LogLevel, string> = {\\n+            silent: '',\\n+            error: '\\\\x1b[31m', // red\\n+            warn: '\\\\x1b[33m', // yellow\\n+            info: '\\\\x1b[36m', // cyan\\n+            verbose: '\\\\x1b[35m', // magenta\\n+            debug: '\\\\x1b[90m', // bright black / gray\\n+          };\\n+\\n+          tsToken = `${dim}${tsToken}${reset}`;\\n+\\n+          if (lvl) {\\n+            const colour = colours[lvl] || '';\\n+            if (colour) {\\n+              lvlToken = `${colour}${lvlToken}${reset}`;\\n+            }\\n+          }\\n+        }\\n+\\n+        const prefix = lvl ? `${tsToken} ${lvlToken}` : tsToken;\\n         process.stderr.write(`${prefix} ${msg}\\\\n`);\\n       } else {\\n         process.stderr.write(msg + '\\\\n');\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/ai-check-provider.ts\",\"additions\":100,\"deletions\":6,\"changes\":106,\"patch\":\"diff --git a/src/providers/ai-check-provider.ts b/src/providers/ai-check-provider.ts\\nindex 53f1a350..b41d6af1 100644\\n--- a/src/providers/ai-check-provider.ts\\n+++ b/src/providers/ai-check-provider.ts\\n@@ -186,7 +186,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     let promptContent: string;\\n \\n@@ -204,7 +205,8 @@ export class AICheckProvider extends CheckProvider {\\n       eventContext,\\n       dependencyResults,\\n       outputHistory,\\n-      args\\n+      args,\\n+      workflowInputs\\n     );\\n   }\\n \\n@@ -335,7 +337,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     // Build outputs_raw from -raw keys (aggregate parent values)\\n     const outputsRaw: Record<string, unknown> = {};\\n@@ -535,6 +538,8 @@ export class AICheckProvider extends CheckProvider {\\n       outputs_raw: outputsRaw,\\n       // Custom arguments from on_init 'with' directive\\n       args: args || {},\\n+      // Workflow inputs (for nested workflow steps to access parent inputs like {{ inputs.context }})\\n+      inputs: workflowInputs || {},\\n     };\\n \\n     try {\\n@@ -621,7 +626,8 @@ export class AICheckProvider extends CheckProvider {\\n         console.error(`[ai-exec] step=${String((config as any).checkName || 'unknown')}`);\\n       }\\n     } catch {}\\n-    // Extract AI configuration - only set properties that are explicitly provided\\n+    // Extract AI configuration - only set properties that are explicitly provided.\\n+    // Workspace / allowedFolders will be derived below from the execution context.\\n     const aiConfig: AIReviewConfig = {};\\n \\n     // Check-level AI configuration (ai object)\\n@@ -671,6 +677,9 @@ export class AICheckProvider extends CheckProvider {\\n       if (aiAny.bashConfig !== undefined) {\\n         aiConfig.bashConfig = aiAny.bashConfig as import('../types/config').BashConfig;\\n       }\\n+      if (aiAny.completion_prompt !== undefined) {\\n+        aiConfig.completionPrompt = aiAny.completion_prompt as string;\\n+      }\\n       if (aiAny.skip_code_context !== undefined) {\\n         // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n         (aiConfig as any).skip_code_context = aiAny.skip_code_context as boolean;\\n@@ -691,6 +700,90 @@ export class AICheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Derive workspace-aware allowedFolders for ProbeAgent when workspace\\n+    // isolation is enabled. This ensures tools like search/query operate\\n+    // inside the isolated workspace (and its project symlinks) instead of\\n+    // the Visor repository root.\\n+    // Folder names are human-readable (tyk-docs, visor2) thanks to WorkspaceManager.\\n+    try {\\n+      const ctxAny: any = sessionInfo as any;\\n+      const parentCtx = ctxAny?._parentContext;\\n+      const workspace = parentCtx?.workspace;\\n+\\n+      // Enhanced debug logging for workspace propagation diagnosis\\n+      logger.debug(`[AI Provider] Workspace detection for check '${(config as any).checkName || 'unknown'}':`);\\n+      logger.debug(`[AI Provider]   sessionInfo exists: ${!!sessionInfo}`);\\n+      logger.debug(`[AI Provider]   _parentContext exists: ${!!parentCtx}`);\\n+      logger.debug(`[AI Provider]   workspace exists: ${!!workspace}`);\\n+      if (workspace) {\\n+        logger.debug(`[AI Provider]   workspace.isEnabled exists: ${typeof workspace.isEnabled === 'function'}`);\\n+        logger.debug(`[AI Provider]   workspace.isEnabled(): ${typeof workspace.isEnabled === 'function' ? workspace.isEnabled() : 'N/A'}`);\\n+        const projectCount = typeof workspace.listProjects === 'function' ? workspace.listProjects()?.length : 'N/A';\\n+        logger.debug(`[AI Provider]   workspace.listProjects() count: ${projectCount}`);\\n+      }\\n+\\n+      if (workspace && typeof workspace.isEnabled === 'function' && workspace.isEnabled()) {\\n+        const folders: string[] = [];\\n+        let workspaceRoot: string | undefined;\\n+        let mainProjectPath: string | undefined;\\n+        try {\\n+          const info = workspace.getWorkspaceInfo?.();\\n+          if (info && typeof info.workspacePath === 'string') {\\n+            workspaceRoot = info.workspacePath;\\n+            mainProjectPath = info.mainProjectPath;\\n+            // Add workspace root\\n+            folders.push(info.workspacePath);\\n+            // Include main project path (has human-readable name like \\\"visor2\\\")\\n+            if (mainProjectPath) {\\n+              folders.push(mainProjectPath);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore workspace info errors\\n+        }\\n+        try {\\n+          const projects = workspace.listProjects?.() || [];\\n+          for (const proj of projects as any[]) {\\n+            if (proj && typeof proj.path === 'string') {\\n+              // Project paths have human-readable names (tyk-docs, not checkout-tyk-docs)\\n+              folders.push(proj.path);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore project listing errors\\n+        }\\n+        const unique = Array.from(new Set(folders.filter(p => typeof p === 'string' && p)));\\n+        if (unique.length > 0 && workspaceRoot) {\\n+          (aiConfig as any).allowedFolders = unique;\\n+          // Set path and cwd to workspace root - AI will run with cwd set to workspace\\n+          // Both are set for compatibility: path for older probe versions, cwd for rc175+\\n+          (aiConfig as any).path = workspaceRoot;\\n+          (aiConfig as any).cwd = workspaceRoot;\\n+          // Also set workspacePath for the AI to know the root\\n+          (aiConfig as any).workspacePath = workspaceRoot;\\n+          logger.debug(`[AI Provider] Workspace isolation enabled:`);\\n+          logger.debug(`[AI Provider]   workspaceRoot (cwd): ${workspaceRoot}`);\\n+          logger.debug(`[AI Provider]   allowedFolders: ${JSON.stringify(unique)}`);\\n+        }\\n+      } else if (parentCtx && typeof parentCtx.workingDirectory === 'string') {\\n+        // Fallback: when workspace is not available (or disabled), still\\n+        // constrain tools to the engine's working directory so ProbeAgent\\n+        // operates inside the same logical root as the state machine. This\\n+        // also ensures nested workflows (e.g. code-question-helper) use the\\n+        // workspace main project path once initializeWorkspace has updated\\n+        // the parent context.\\n+        if (!(aiConfig as any).allowedFolders) {\\n+          (aiConfig as any).allowedFolders = [parentCtx.workingDirectory];\\n+        }\\n+        if (!(aiConfig as any).path) {\\n+          (aiConfig as any).path = parentCtx.workingDirectory;\\n+          (aiConfig as any).cwd = parentCtx.workingDirectory;\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only; fall back to defaults on error.\\n+    }\\n+\\n     // Check-level AI model and provider (top-level properties)\\n     if (config.ai_model !== undefined) {\\n       aiConfig.model = config.ai_model as string;\\n@@ -876,7 +969,8 @@ export class AICheckProvider extends CheckProvider {\\n       ctxWithStage,\\n       _dependencyResults,\\n       (config as any).__outputHistory as Map<string, unknown[]> | undefined,\\n-      (sessionInfo as any)?.args\\n+      (sessionInfo as any)?.args,\\n+      (config as any).workflowInputs as Record<string, unknown> | undefined\\n     );\\n \\n     // Optional persona (vendor extension): ai.ai_persona or ai_persona.\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/command-check-provider.ts\",\"additions\":50,\"deletions\":16,\"changes\":66,\"patch\":\"diff --git a/src/providers/command-check-provider.ts b/src/providers/command-check-provider.ts\\nindex e0c9d634..bf7ffd43 100644\\n--- a/src/providers/command-check-provider.ts\\n+++ b/src/providers/command-check-provider.ts\\n@@ -131,7 +131,8 @@ export class CommandCheckProvider extends CheckProvider {\\n       // New: outputs_raw exposes aggregate values (e.g., full arrays for forEach parents)\\n       outputs_raw: outputsRaw,\\n       // Workflow inputs (when executing within a workflow)\\n-      inputs: context?.workflowInputs || {},\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any).workflowInputs || context?.workflowInputs || {},\\n       // Custom arguments from on_init 'with' directive\\n       args: context?.args || {},\\n       env: this.getSafeEnvironmentVariables(),\\n@@ -178,17 +179,31 @@ export class CommandCheckProvider extends CheckProvider {\\n           mock = rawMock as Record<string, unknown>;\\n         }\\n         const m = mock as { stdout?: string; stderr?: string; exit_code?: number; exit?: number };\\n-        let out: unknown = m.stdout ?? '';\\n-        try {\\n-          if (\\n-            typeof out === 'string' &&\\n-            (out.trim().startsWith('{') || out.trim().startsWith('['))\\n-          ) {\\n-            out = JSON.parse(out);\\n-          }\\n-        } catch {}\\n-        const code =\\n-          typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0;\\n+\\n+        // Check if this looks like a command mock (has stdout/stderr/exit_code) or direct data output\\n+        const isCommandMock = m.stdout !== undefined || m.stderr !== undefined ||\\n+          m.exit_code !== undefined || m.exit !== undefined;\\n+\\n+        let out: unknown;\\n+        if (isCommandMock) {\\n+          // Traditional command mock format: { stdout: \\\"...\\\", exit_code: 0 }\\n+          out = m.stdout ?? '';\\n+          try {\\n+            if (\\n+              typeof out === 'string' &&\\n+              (out.trim().startsWith('{') || out.trim().startsWith('['))\\n+            ) {\\n+              out = JSON.parse(out);\\n+            }\\n+          } catch {}\\n+        } else {\\n+          // Direct data mock format: { data: [...], count: 1 } - use as-is\\n+          out = mock;\\n+        }\\n+\\n+        const code = isCommandMock\\n+          ? (typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0)\\n+          : 0;\\n         if (code !== 0) {\\n           return {\\n             issues: [\\n@@ -231,9 +246,8 @@ export class CommandCheckProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Get timeout from config (in seconds) or use default (60 seconds)\\n-      const timeoutSeconds = (config.timeout as number) || 60;\\n-      const timeoutMs = timeoutSeconds * 1000;\\n+      // Get timeout from config (in milliseconds) or use default (60 seconds = 60000ms)\\n+      const timeoutMs = (config.timeout as number) || 60000;\\n \\n       // Normalize only the eval payload for `node -e|--eval` invocations that may contain\\n       // literal newlines due to YAML processing (\\\"\\\\n\\\" -> newline). We re-escape newlines\\n@@ -1533,6 +1547,26 @@ ${bodyWithReturn}\\n       return null;\\n     }\\n \\n+    // Require at least one issue-specific field beyond just a message-like field.\\n+    // This prevents objects like {text: \\\"...\\\"} from being treated as issues.\\n+    const hasIssueField =\\n+      data.file !== undefined ||\\n+      data.path !== undefined ||\\n+      data.filename !== undefined ||\\n+      data.line !== undefined ||\\n+      data.startLine !== undefined ||\\n+      data.lineNumber !== undefined ||\\n+      data.severity !== undefined ||\\n+      data.level !== undefined ||\\n+      data.priority !== undefined ||\\n+      data.ruleId !== undefined ||\\n+      data.rule !== undefined ||\\n+      data.category !== undefined ||\\n+      data.type !== undefined;\\n+    if (!hasIssueField) {\\n+      return null;\\n+    }\\n+\\n     const allowedSeverities = new Set(['info', 'warning', 'error', 'critical']);\\n     const severityRaw = this.toTrimmedString(data.severity || data.level || data.priority);\\n     let severity: ReviewIssue['severity'] = 'warning';\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/git-checkout-provider.ts\",\"additions\":41,\"deletions\":6,\"changes\":47,\"patch\":\"diff --git a/src/providers/git-checkout-provider.ts b/src/providers/git-checkout-provider.ts\\nindex baac0001..6d3a2852 100644\\n--- a/src/providers/git-checkout-provider.ts\\n+++ b/src/providers/git-checkout-provider.ts\\n@@ -69,6 +69,16 @@ export class GitCheckoutProvider extends CheckProvider {\\n       return false;\\n     }\\n \\n+    if (checkoutConfig.clone_timeout_ms !== undefined) {\\n+      if (\\n+        typeof checkoutConfig.clone_timeout_ms !== 'number' ||\\n+        checkoutConfig.clone_timeout_ms <= 0\\n+      ) {\\n+        logger.error('Invalid config: clone_timeout_ms must be a positive number (milliseconds)');\\n+        return false;\\n+      }\\n+    }\\n+\\n     return true;\\n   }\\n \\n@@ -91,7 +101,13 @@ export class GitCheckoutProvider extends CheckProvider {\\n       );\\n \\n       // Resolve dynamic variables\\n-      const resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      let resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      // If ref resolves to an empty string, fall back to HEAD so we rely on\\n+      // the repository's default branch. This keeps downstream git commands\\n+      // well-defined and matches existing expectations/tests.\\n+      if (!resolvedRef || resolvedRef.trim().length === 0) {\\n+        resolvedRef = 'HEAD';\\n+      }\\n       const resolvedRepository = checkoutConfig.repository\\n         ? await this.liquid.parseAndRender(checkoutConfig.repository, templateContext)\\n         : process.env.GITHUB_REPOSITORY || 'unknown/unknown';\\n@@ -118,6 +134,7 @@ export class GitCheckoutProvider extends CheckProvider {\\n           clean: checkoutConfig.clean !== false, // Default: true\\n           workflowId: (context as any)?.workflowId,\\n           fetchDepth: checkoutConfig.fetch_depth,\\n+          cloneTimeoutMs: checkoutConfig.clone_timeout_ms,\\n         }\\n       );\\n \\n@@ -134,18 +151,35 @@ export class GitCheckoutProvider extends CheckProvider {\\n \\n       // Add project to workspace if workspace isolation is enabled\\n       const workspace = (context as any)?._parentContext?.workspace;\\n+\\n+      // Enhanced debug logging for workspace project addition diagnosis\\n+      const checkName = (config as any)?.checkName || 'unknown';\\n+      logger.info(`[GitCheckout] Workspace check for '${checkName}':`);\\n+      logger.info(`[GitCheckout]   _parentContext exists: ${!!(context as any)?._parentContext}`);\\n+      logger.info(`[GitCheckout]   workspace exists: ${!!workspace}`);\\n+      logger.info(`[GitCheckout]   workspace.isEnabled(): ${workspace?.isEnabled?.() ?? 'N/A'}`);\\n+      if (workspace) {\\n+        const projectCountBefore = workspace.listProjects?.()?.length ?? 'N/A';\\n+        logger.debug(`[GitCheckout]   projects before addProject: ${projectCountBefore}`);\\n+      }\\n+\\n       if (workspace?.isEnabled()) {\\n         try {\\n+          // Don't pass checkName as description - let workspace use repo name\\n+          // This results in human-readable names like \\\"tyk-docs\\\" instead of \\\"checkout-tyk-docs\\\"\\n           const workspacePath = await workspace.addProject(\\n             resolvedRepository,\\n-            worktree.path,\\n-            checkoutConfig.checkName\\n+            worktree.path\\n           );\\n           output.workspace_path = workspacePath;\\n-          logger.debug(`Added project to workspace: ${workspacePath}`);\\n+          const projectCountAfter = workspace.listProjects?.()?.length ?? 'N/A';\\n+          logger.debug(`[GitCheckout] Added project to workspace: ${workspacePath}`);\\n+          logger.debug(`[GitCheckout]   projects after addProject: ${projectCountAfter}`);\\n         } catch (error) {\\n           logger.warn(`Failed to add project to workspace: ${error}`);\\n         }\\n+      } else {\\n+        logger.debug(`[GitCheckout] Workspace not enabled, skipping addProject`);\\n       }\\n \\n       logger.info(\\n@@ -228,7 +262,8 @@ export class GitCheckoutProvider extends CheckProvider {\\n       outputs: outputsObj,\\n       outputs_history: historyObj,\\n       env: safeEnv,\\n-      inputs: context?.workflowInputs,\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any)?.workflowInputs || context?.workflowInputs,\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/http-client-provider.ts\",\"additions\":279,\"deletions\":35,\"changes\":314,\"patch\":\"diff --git a/src/providers/http-client-provider.ts b/src/providers/http-client-provider.ts\\nindex 4d8c41be..6db81591 100644\\n--- a/src/providers/http-client-provider.ts\\n+++ b/src/providers/http-client-provider.ts\\n@@ -4,18 +4,29 @@ import { ReviewSummary } from '../reviewer';\\n import { Liquid } from 'liquidjs';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n import { EnvironmentResolver } from '../utils/env-resolver';\\n+import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n+import { buildProviderTemplateContext } from '../utils/template-context';\\n+import Sandbox from '@nyariv/sandboxjs';\\n+import { logger } from '../logger';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n \\n /**\\n  * Check provider that fetches data from HTTP endpoints\\n  */\\n export class HttpClientProvider extends CheckProvider {\\n   private liquid: Liquid;\\n+  private sandbox?: Sandbox;\\n \\n   constructor() {\\n     super();\\n     this.liquid = createExtendedLiquid();\\n   }\\n \\n+  private createSecureSandbox(): Sandbox {\\n+    return createSecureSandbox();\\n+  }\\n+\\n   getName(): string {\\n     return 'http_client';\\n   }\\n@@ -61,50 +72,124 @@ export class HttpClientProvider extends CheckProvider {\\n     const headers = (config.headers as Record<string, string>) || {};\\n     const timeout = (config.timeout as number) || 30000;\\n     const transform = config.transform as string | undefined;\\n+    const transformJs = config.transform_js as string | undefined;\\n     const bodyTemplate = config.body as string | undefined;\\n+    const outputFileTemplate = config.output_file as string | undefined;\\n+    const skipIfExists = config.skip_if_exists !== false; // Default true for caching\\n \\n-    try {\\n-      // Prepare template context for URL and body\\n-      const templateContext = {\\n-        pr: {\\n-          number: prInfo.number,\\n-          title: prInfo.title,\\n-          body: prInfo.body,\\n-          author: prInfo.author,\\n-          base: prInfo.base,\\n-          head: prInfo.head,\\n-          totalAdditions: prInfo.totalAdditions,\\n-          totalDeletions: prInfo.totalDeletions,\\n-        },\\n-        outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-        env: process.env,\\n-      };\\n+    // Track resolved URL for error messages\\n+    let resolvedUrlForErrors = url;\\n \\n-      // Render URL with template if it contains liquid syntax\\n-      let renderedUrl = url;\\n-      if (url.includes('{{') || url.includes('{%')) {\\n-        renderedUrl = await this.liquid.parseAndRender(url, templateContext);\\n+    try {\\n+      // Use shared template context builder for consistent output extraction\\n+      const templateContext = buildProviderTemplateContext(\\n+        prInfo,\\n+        dependencyResults,\\n+        undefined, // memoryStore\\n+        undefined, // outputHistory\\n+        undefined, // stageHistoryBase\\n+        { attachMemoryReadHelpers: false }\\n+      );\\n+      // Add env to context for shell-style variable resolution\\n+      (templateContext as Record<string, unknown>).env = process.env;\\n+\\n+      // First resolve shell-style environment variables (${VAR}, $VAR, ${{ env.VAR }})\\n+      let renderedUrl = String(EnvironmentResolver.resolveValue(url));\\n+      resolvedUrlForErrors = renderedUrl; // Track for error messages\\n+\\n+      // Then render Liquid templates if present\\n+      if (renderedUrl.includes('{{') || renderedUrl.includes('{%')) {\\n+        renderedUrl = await this.liquid.parseAndRender(renderedUrl, templateContext);\\n+        resolvedUrlForErrors = renderedUrl; // Update after Liquid rendering\\n       }\\n \\n       // Prepare request body if provided\\n       let requestBody: string | undefined;\\n       if (bodyTemplate) {\\n-        const renderedBody = await this.liquid.parseAndRender(bodyTemplate, templateContext);\\n-        requestBody = renderedBody;\\n+        // First resolve shell-style environment variables\\n+        let resolvedBody = String(EnvironmentResolver.resolveValue(bodyTemplate));\\n+        // Then render Liquid templates if present\\n+        if (resolvedBody.includes('{{') || resolvedBody.includes('{%')) {\\n+          resolvedBody = await this.liquid.parseAndRender(resolvedBody, templateContext);\\n+        }\\n+        requestBody = resolvedBody;\\n       }\\n \\n-      // Resolve environment variables in headers\\n-      const resolvedHeaders = EnvironmentResolver.resolveHeaders(headers);\\n+      // Resolve environment variables and Liquid templates in headers\\n+      const resolvedHeaders: Record<string, string> = {};\\n+      for (const [key, value] of Object.entries(headers)) {\\n+        let resolvedValue = String(EnvironmentResolver.resolveValue(value));\\n+        // Render Liquid templates if present\\n+        if (resolvedValue.includes('{{') || resolvedValue.includes('{%')) {\\n+          resolvedValue = await this.liquid.parseAndRender(resolvedValue, templateContext);\\n+        }\\n+        resolvedHeaders[key] = resolvedValue;\\n+        // Debug auth header (mask most of the value for security)\\n+        if (key.toLowerCase() === 'authorization') {\\n+          const maskedValue = resolvedValue.length > 20\\n+            ? `${resolvedValue.substring(0, 15)}...${resolvedValue.substring(resolvedValue.length - 5)}`\\n+            : resolvedValue;\\n+          logger.verbose(`[http_client] ${key}: ${maskedValue}`);\\n+        }\\n+      }\\n+\\n+      // Resolve output_file path if specified\\n+      let resolvedOutputFile: string | undefined;\\n+      if (outputFileTemplate) {\\n+        let outputPath = String(EnvironmentResolver.resolveValue(outputFileTemplate));\\n+        if (outputPath.includes('{{') || outputPath.includes('{%')) {\\n+          outputPath = await this.liquid.parseAndRender(outputPath, templateContext);\\n+        }\\n+        resolvedOutputFile = outputPath.trim();\\n+\\n+        // Check if file already exists (caching)\\n+        if (skipIfExists && fs.existsSync(resolvedOutputFile)) {\\n+          const stats = fs.statSync(resolvedOutputFile);\\n+          logger.verbose(`[http_client] File cached: ${resolvedOutputFile} (${stats.size} bytes)`);\\n+          return {\\n+            issues: [],\\n+            file_path: resolvedOutputFile,\\n+            size: stats.size,\\n+            cached: true,\\n+          } as unknown as ReviewSummary;\\n+        }\\n+      }\\n \\n       // Test hook: mock HTTP response for this step\\n       const stepName = (config as any).checkName || 'unknown';\\n       const mock = context?.hooks?.mockForStep?.(String(stepName));\\n-      const data =\\n-        mock !== undefined\\n-          ? mock\\n-          : await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n \\n-      // Apply transformation if specified\\n+      // If mock is provided, return it directly as the step output (with issues: [])\\n+      if (mock !== undefined) {\\n+        const mockObj = typeof mock === 'object' && mock !== null ? mock : { data: mock };\\n+        return {\\n+          issues: [],\\n+          ...mockObj,\\n+        } as unknown as ReviewSummary & { data: unknown };\\n+      }\\n+\\n+      // Debug log the request for troubleshooting\\n+      logger.verbose(`[http_client] ${method} ${renderedUrl}`);\\n+      if (requestBody) {\\n+        logger.verbose(`[http_client] Body: ${requestBody.substring(0, 500)}${requestBody.length > 500 ? '...' : ''}`);\\n+      }\\n+\\n+      // If output_file is specified, download to file instead of returning data\\n+      if (resolvedOutputFile) {\\n+        const fileResult = await this.downloadToFile(\\n+          renderedUrl,\\n+          method,\\n+          resolvedHeaders,\\n+          requestBody,\\n+          timeout,\\n+          resolvedOutputFile\\n+        );\\n+        return fileResult;\\n+      }\\n+\\n+      const data = await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n+\\n+      // Apply Liquid transformation if specified\\n       let processedData = data;\\n       if (transform) {\\n         try {\\n@@ -136,12 +221,53 @@ export class HttpClientProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Return the fetched data as a custom field for dependent checks to access\\n+      // Apply JavaScript transformation if specified\\n+      if (transformJs) {\\n+        try {\\n+          if (!this.sandbox) {\\n+            this.sandbox = this.createSecureSandbox();\\n+          }\\n+\\n+          // Create scope for JavaScript transform (scope, not context)\\n+          const jsScope: Record<string, unknown> = {\\n+            output: data,\\n+            pr: templateContext.pr,\\n+            outputs: templateContext.outputs,\\n+            env: process.env,\\n+          };\\n+\\n+          const result = compileAndRun(this.sandbox, transformJs, jsScope, {\\n+            injectLog: true,\\n+            logPrefix: 'üîç [transform_js]',\\n+            wrapFunction: true,\\n+          });\\n+          processedData = result;\\n+          logger.verbose(`‚úì Applied JavaScript transform successfully`);\\n+        } catch (error) {\\n+          logger.error(\\n+            `‚úó Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`\\n+          );\\n+          return {\\n+            issues: [\\n+              {\\n+                file: 'http_client',\\n+                line: 0,\\n+                ruleId: 'http_client/transform_js_error',\\n+                message: `Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              },\\n+            ],\\n+          };\\n+        }\\n+      }\\n+\\n+      // Return the fetched data in the standard `output` property for guarantee evaluation\\n+      // This is consistent with other providers (script, command, ai, etc.)\\n       return {\\n         issues: [],\\n-        // Add custom data field that will be passed through to dependent checks\\n-        data: processedData,\\n-      } as ReviewSummary & { data: unknown };\\n+        output: processedData,\\n+      } as unknown as ReviewSummary;\\n     } catch (error) {\\n       return {\\n         issues: [\\n@@ -149,7 +275,7 @@ export class HttpClientProvider extends CheckProvider {\\n             file: 'http_client',\\n             line: 0,\\n             ruleId: 'http_client/fetch_error',\\n-            message: `Failed to fetch from ${url}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            message: `Failed to fetch from ${resolvedUrlForErrors}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n             severity: 'error',\\n             category: 'logic',\\n           },\\n@@ -198,7 +324,14 @@ export class HttpClientProvider extends CheckProvider {\\n \\n       clearTimeout(timeoutId);\\n \\n+      logger.verbose(`[http_client] Response: ${response.status} ${response.statusText}`);\\n+\\n       if (!response.ok) {\\n+        // Log response body for debugging auth failures\\n+        try {\\n+          const errorBody = await response.text();\\n+          logger.warn(`[http_client] Error body: ${errorBody.substring(0, 500)}`);\\n+        } catch {}\\n         throw new Error(`HTTP ${response.status}: ${response.statusText}`);\\n       }\\n \\n@@ -233,6 +366,112 @@ export class HttpClientProvider extends CheckProvider {\\n     }\\n   }\\n \\n+  private async downloadToFile(\\n+    url: string,\\n+    method: string,\\n+    headers: Record<string, string>,\\n+    body: string | undefined,\\n+    timeout: number,\\n+    outputFile: string\\n+  ): Promise<ReviewSummary> {\\n+    // Check if fetch is available (Node 18+)\\n+    if (typeof fetch === 'undefined') {\\n+      throw new Error('HTTP client provider requires Node.js 18+ or node-fetch package');\\n+    }\\n+\\n+    const controller = new AbortController();\\n+    const timeoutId = setTimeout(() => controller.abort(), timeout);\\n+\\n+    try {\\n+      const requestOptions: RequestInit = {\\n+        method,\\n+        headers: { ...headers },\\n+        signal: controller.signal,\\n+      };\\n+\\n+      // Add body for non-GET requests\\n+      if (method !== 'GET' && body) {\\n+        requestOptions.body = body;\\n+        if (!headers['Content-Type'] && !headers['content-type']) {\\n+          requestOptions.headers = {\\n+            ...requestOptions.headers,\\n+            'Content-Type': 'application/json',\\n+          };\\n+        }\\n+      }\\n+\\n+      const response = await fetch(url, requestOptions);\\n+      clearTimeout(timeoutId);\\n+\\n+      if (!response.ok) {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_error',\\n+              message: `Failed to download file: HTTP ${response.status}: ${response.statusText}`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      // Create parent directory if it doesn't exist\\n+      const parentDir = path.dirname(outputFile);\\n+      if (parentDir && !fs.existsSync(parentDir)) {\\n+        fs.mkdirSync(parentDir, { recursive: true });\\n+      }\\n+\\n+      // Get the response as an ArrayBuffer and write to file\\n+      const arrayBuffer = await response.arrayBuffer();\\n+      const buffer = Buffer.from(arrayBuffer);\\n+      fs.writeFileSync(outputFile, buffer);\\n+\\n+      const contentType = response.headers.get('content-type') || 'application/octet-stream';\\n+      logger.verbose(`[http_client] Downloaded: ${outputFile} (${buffer.length} bytes)`);\\n+\\n+      return {\\n+        issues: [],\\n+        file_path: outputFile,\\n+        size: buffer.length,\\n+        content_type: contentType,\\n+        cached: false,\\n+      } as unknown as ReviewSummary;\\n+    } catch (error: unknown) {\\n+      clearTimeout(timeoutId);\\n+\\n+      if (error instanceof Error && error.name === 'AbortError') {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_timeout',\\n+              message: `Download timed out after ${timeout}ms`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      return {\\n+        issues: [\\n+          {\\n+            file: 'http_client',\\n+            line: 0,\\n+            ruleId: 'http_client/download_error',\\n+            message: `Failed to download file: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            severity: 'error',\\n+            category: 'logic',\\n+          },\\n+        ],\\n+      };\\n+    }\\n+  }\\n+\\n   getSupportedConfigKeys(): string[] {\\n     return [\\n       'type',\\n@@ -241,7 +480,10 @@ export class HttpClientProvider extends CheckProvider {\\n       'headers',\\n       'body',\\n       'transform',\\n+      'transform_js',\\n       'timeout',\\n+      'output_file',\\n+      'skip_if_exists',\\n       'depends_on',\\n       'on',\\n       'if',\\n@@ -261,6 +503,8 @@ export class HttpClientProvider extends CheckProvider {\\n       'Network access to the endpoint',\\n       'Optional: Transform template for processing response data',\\n       'Optional: Body template for POST/PUT requests',\\n+      'Optional: output_file path to download response to a file',\\n+      'Optional: skip_if_exists (default: true) to enable caching for file downloads',\\n     ];\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/log-check-provider.ts\",\"additions\":19,\"deletions\":8,\"changes\":27,\"patch\":\"diff --git a/src/providers/log-check-provider.ts b/src/providers/log-check-provider.ts\\nindex 9c6332d1..b57f632c 100644\\n--- a/src/providers/log-check-provider.ts\\n+++ b/src/providers/log-check-provider.ts\\n@@ -78,7 +78,8 @@ export class LogCheckProvider extends CheckProvider {\\n       includeDependencies,\\n       includeMetadata,\\n       config.__outputHistory as Map<string, unknown[]> | undefined,\\n-      context\\n+      context,\\n+      config\\n     );\\n \\n     // Render the log message template\\n@@ -100,12 +101,20 @@ export class LogCheckProvider extends CheckProvider {\\n     else if (level === 'debug') logger.debug(logOutput);\\n     else logger.info(logOutput);\\n \\n-    // Return with the log content as custom data for dependent checks\\n-    return {\\n+    // Return with the log content as custom data for dependent checks.\\n+    // For chat-style logs (group: chat), also expose a structured\\n+    // `output.text` field so frontends like Slack can post a clean\\n+    // human-facing message without the level prefix.\\n+    const summary: ReviewSummary & { logOutput: string; output?: unknown } = {\\n       issues: [],\\n-      // Add log output as custom field\\n       logOutput,\\n-    } as ReviewSummary & { logOutput: string };\\n+    };\\n+\\n+    if ((config as any).group === 'chat') {\\n+      (summary as any).output = { text: renderedMessage };\\n+    }\\n+\\n+    return summary;\\n   }\\n \\n   private buildTemplateContext(\\n@@ -115,7 +124,8 @@ export class LogCheckProvider extends CheckProvider {\\n     _includeDependencies: boolean = true,\\n     includeMetadata: boolean = true,\\n     outputHistory?: Map<string, unknown[]>,\\n-    executionContext?: ExecutionContext\\n+    executionContext?: ExecutionContext,\\n+    config?: CheckProviderConfig\\n   ): Record<string, unknown> {\\n     const context: Record<string, unknown> = {};\\n \\n@@ -197,7 +207,8 @@ export class LogCheckProvider extends CheckProvider {\\n     }\\n \\n     // Add workflow inputs if available\\n-    const workflowInputs = executionContext?.workflowInputs || {};\\n+    // Check config first (set by projectWorkflowToGraph), then fall back to executionContext\\n+    const workflowInputs = (config as any)?.workflowInputs || executionContext?.workflowInputs || {};\\n     logger.debug(\\n       `[LogProvider] Adding ${Object.keys(workflowInputs).length} workflow inputs to context`\\n     );\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/script-check-provider.ts\",\"additions\":37,\"deletions\":1,\"changes\":38,\"patch\":\"diff --git a/src/providers/script-check-provider.ts b/src/providers/script-check-provider.ts\\nindex 9825bb69..947dbd73 100644\\n--- a/src/providers/script-check-provider.ts\\n+++ b/src/providers/script-check-provider.ts\\n@@ -59,6 +59,16 @@ export class ScriptCheckProvider extends CheckProvider {\\n       reuseSession?: boolean;\\n     } & import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n+    // Test hook: mock output for this step (short-circuit execution)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = _sessionInfo?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        // Return mock directly as step output\\n+        return { issues: [], output: mock } as ReviewSummary & { output: unknown };\\n+      }\\n+    } catch {}\\n+\\n     const script = String(config.content || '');\\n     const memoryStore = MemoryStore.getInstance();\\n     const ctx = buildProviderTemplateContext(\\n@@ -72,10 +82,36 @@ export class ScriptCheckProvider extends CheckProvider {\\n     // Keep provider quiet by default; no step-specific debug\\n     // (historical ad-hoc logs removed to avoid hardcoding step names).\\n \\n+    // Add workflow inputs to the context\\n+    const inputs = (config as any).workflowInputs || _sessionInfo?.workflowInputs || {};\\n+    (ctx as any).inputs = inputs;\\n+\\n+    // Add environment variables to context (consistent with http-client-provider)\\n+    (ctx as any).env = process.env;\\n+\\n     // Attach synchronous memory ops consistent with memory provider\\n     const { ops, needsSave } = createSyncMemoryOps(memoryStore);\\n     (ctx as any).memory = ops as unknown as Record<string, unknown>;\\n \\n+    // Add helper functions to the context\\n+    (ctx as any).escapeXml = (str: unknown): string => {\\n+      if (str == null) return '';\\n+      return String(str)\\n+        .replace(/&/g, '&amp;')\\n+        .replace(/</g, '&lt;')\\n+        .replace(/>/g, '&gt;')\\n+        .replace(/\\\"/g, '&quot;')\\n+        .replace(/'/g, '&apos;');\\n+    };\\n+\\n+    // Add btoa/atob for base64 encoding/decoding (browser API polyfill)\\n+    (ctx as any).btoa = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'binary').toString('base64');\\n+    };\\n+    (ctx as any).atob = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'base64').toString('binary');\\n+    };\\n+\\n     // Evaluate the script in a secure sandbox (per-execution instance)\\n     const sandbox = this.createSecureSandbox();\\n     let result: unknown;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/workflow-check-provider.ts\",\"additions\":164,\"deletions\":43,\"changes\":207,\"patch\":\"diff --git a/src/providers/workflow-check-provider.ts b/src/providers/workflow-check-provider.ts\\nindex 116d4dff..681703ce 100644\\n--- a/src/providers/workflow-check-provider.ts\\n+++ b/src/providers/workflow-check-provider.ts\\n@@ -67,34 +67,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n   ): Promise<ReviewSummary> {\\n     const cfg = config as CheckProviderConfig & { workflow?: string; config?: string };\\n     const isConfigPathMode = !!cfg.config && !cfg.workflow;\\n+    const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n \\n-    // Test harness support: allow mocking workflow checks as a black box.\\n-    // If a mock is provided for this step, short-circuit nested execution and\\n-    // return a ReviewSummary with optional output field.\\n-    try {\\n-      const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n-      // test-runner passes hooks on execution context\\n-      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n-      if (mock !== undefined) {\\n-        const ms = mock as any;\\n-        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n-        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n-        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n-        const summary: ReviewSummary & { output?: unknown } = {\\n-          issues: issuesArr,\\n-          output: out,\\n-          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n-        } as any;\\n-        return summary;\\n-      }\\n-    } catch {}\\n-\\n-    // Resolve workflow definition\\n+    // Resolve workflow definition FIRST (needed for input preparation and validation)\\n     let workflow: WorkflowDefinition | undefined;\\n     let workflowId = cfg.workflow as string | undefined;\\n \\n     if (isConfigPathMode) {\\n-      const parentCwd = ((context as any)?._parentContext?.workingDirectory ||\\n+      // Use originalWorkingDirectory for config file resolution - workflow configs\\n+      // should be loaded from the original project path, not the sandbox\\n+      const parentCwd = ((context as any)?._parentContext?.originalWorkingDirectory ||\\n+        (context as any)?._parentContext?.workingDirectory ||\\n+        (context as any)?.originalWorkingDirectory ||\\n         (context as any)?.workingDirectory ||\\n         process.cwd()) as string;\\n       workflow = await this.loadWorkflowFromConfigPath(String(cfg.config), parentCwd);\\n@@ -109,9 +93,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       logger.info(`Executing workflow '${workflowId}'`);\\n     }\\n \\n-    // Prepare inputs\\n+    // Prepare inputs - do this BEFORE mock check so we can validate inputs even when mocked\\n+    // This allows tests to assert that the correct inputs were passed to workflow steps\\n     const inputs = await this.prepareInputs(workflow, config, prInfo, dependencyResults);\\n \\n+    // Capture resolved workflow inputs for testing assertions (reuse prompt capture infrastructure)\\n+    // This allows using `prompts` assertions with `provider: 'workflow'` to verify inputs\\n+    try {\\n+      // Serialize inputs to capture for assertions - specifically the context field\\n+      // which contains the rendered template with dependency outputs\\n+      const inputsCapture = Object.entries(inputs)\\n+        .map(([k, v]) => `${k}: ${typeof v === 'string' ? v : JSON.stringify(v)}`)\\n+        .join('\\\\n\\\\n');\\n+      (context as any)?.hooks?.onPromptCaptured?.({\\n+        step: String(stepName),\\n+        provider: 'workflow',\\n+        prompt: inputsCapture,\\n+      });\\n+    } catch {\\n+      // Ignore capture errors - this is only for testing\\n+    }\\n+\\n     // Validate inputs\\n     const validation = this.registry.validateInputs(workflow, inputs);\\n     if (!validation.valid) {\\n@@ -119,6 +121,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       throw new Error(`Invalid workflow inputs: ${errors}`);\\n     }\\n \\n+    // Test harness support: allow mocking workflow checks as a black box.\\n+    // If a mock is provided for this step, short-circuit nested execution and\\n+    // return a ReviewSummary with optional output field.\\n+    // NOTE: This happens AFTER input preparation/validation so tests can assert inputs are correct\\n+    try {\\n+      // test-runner passes hooks on execution context\\n+      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        const ms = mock as any;\\n+        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n+        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n+        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n+        const summary: ReviewSummary & { output?: unknown } = {\\n+          issues: issuesArr,\\n+          output: out,\\n+          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n+        } as any;\\n+        return summary;\\n+      }\\n+    } catch {}\\n+\\n     // Apply overrides to workflow steps if specified\\n     const modifiedWorkflow = this.applyOverrides(workflow, config);\\n \\n@@ -211,6 +234,80 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Extract eventContext for slack/conversation\\n+    const eventContext = config.eventContext || {};\\n+\\n+    // Debug logging for conversation context\\n+    logger.debug(`[WorkflowProvider] prepareInputs for ${workflow.id}`);\\n+    logger.debug(`[WorkflowProvider] eventContext keys: ${Object.keys(eventContext).join(', ') || 'none'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.slack: ${eventContext.slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.conversation: ${(eventContext as any).conversation ? 'present' : 'absent'}`);\\n+\\n+    // Extract slack context (if provided via eventContext.slack)\\n+    const slack = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        const slackCtx = anyCtx?.slack;\\n+        if (slackCtx && typeof slackCtx === 'object') return slackCtx;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Extract unified conversation context across transports (Slack & GitHub)\\n+    const conversation = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        if (anyCtx?.slack?.conversation) return anyCtx.slack.conversation;\\n+        if (anyCtx?.github?.conversation) return anyCtx.github.conversation;\\n+        if (anyCtx?.conversation) return anyCtx.conversation;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Debug logging for extracted context\\n+    logger.debug(`[WorkflowProvider] slack extracted: ${slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] conversation extracted: ${conversation ? 'present' : 'absent'}`);\\n+    if (conversation) {\\n+      logger.debug(`[WorkflowProvider] conversation.messages count: ${Array.isArray((conversation as any).messages) ? (conversation as any).messages.length : 0}`);\\n+    }\\n+\\n+    // Extract output history from config (passed via __outputHistory)\\n+    const outputHistory = (config as any).__outputHistory as Map<string, unknown[]> | undefined;\\n+    const outputs_history: Record<string, unknown[]> = {};\\n+    if (outputHistory) {\\n+      for (const [k, v] of outputHistory.entries()) {\\n+        outputs_history[k] = v;\\n+      }\\n+    }\\n+\\n+    // Build template context with all available data\\n+    // Extract .output from each dependency result so that outputs['step-name'].field works naturally\\n+    // (not outputs['step-name'].output.field)\\n+    const outputsMap: Record<string, unknown> = {};\\n+    logger.debug(`[WorkflowProvider] dependencyResults: ${dependencyResults ? dependencyResults.size : 'undefined'} entries`);\\n+    if (dependencyResults) {\\n+      for (const [key, result] of dependencyResults.entries()) {\\n+        // Extract the output property, or use the whole result if output is undefined\\n+        const extracted = (result as any).output ?? result;\\n+        outputsMap[key] = extracted;\\n+        // Debug: log what we extracted for each dependency\\n+        const extractedKeys = extracted && typeof extracted === 'object' ? Object.keys(extracted).join(', ') : 'not-object';\\n+        logger.debug(`[WorkflowProvider] outputs['${key}']: keys=[${extractedKeys}]`);\\n+      }\\n+    }\\n+    const templateContext = {\\n+      pr: prInfo,\\n+      outputs: outputsMap,\\n+      env: process.env,\\n+      slack,\\n+      conversation,\\n+      outputs_history,\\n+    };\\n+\\n     // Apply user-provided inputs (args)\\n     const userInputs = config.args || config.workflow_inputs; // Support both for compatibility\\n     if (userInputs) {\\n@@ -219,11 +316,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         if (typeof value === 'string') {\\n           // Check if it's a Liquid template\\n           if (value.includes('{{') || value.includes('{%')) {\\n-            inputs[key] = await this.liquid.parseAndRender(value, {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            });\\n+            inputs[key] = await this.liquid.parseAndRender(value, templateContext);\\n+            // Debug: log rendered template value for important inputs\\n+            if (key === 'text' || key === 'question' || key === 'context') {\\n+              const rendered = String(inputs[key]);\\n+              logger.info(`[WorkflowProvider] Rendered '${key}' input (${rendered.length} chars): ${rendered.substring(0, 500)}${rendered.length > 500 ? '...' : ''}`);\\n+            }\\n           } else {\\n             inputs[key] = value;\\n           }\\n@@ -234,11 +332,7 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           inputs[key] = compileAndRun(\\n             sandbox,\\n             exprValue.expression,\\n-            {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            },\\n+            templateContext,\\n             { injectLog: true, logPrefix: `workflow.input.${key}` }\\n           );\\n         } else {\\n@@ -367,6 +461,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n     try {\\n       await childMemory.initialize();\\n     } catch {}\\n+    // Enhanced debug logging for workspace propagation diagnosis\\n+    const parentWorkspace = parentContext?.workspace;\\n+    logger.info(`[WorkflowProvider] Workspace propagation for nested workflow '${workflow.id}':`);\\n+    logger.info(`[WorkflowProvider]   parentContext exists: ${!!parentContext}`);\\n+    logger.info(`[WorkflowProvider]   parentContext.workspace exists: ${!!parentWorkspace}`);\\n+    if (parentWorkspace) {\\n+      logger.info(`[WorkflowProvider]   parentWorkspace.isEnabled(): ${parentWorkspace.isEnabled?.() ?? 'N/A'}`);\\n+      const projectCount = parentWorkspace.listProjects?.()?.length ?? 'N/A';\\n+      logger.info(`[WorkflowProvider]   parentWorkspace project count: ${projectCount}`);\\n+    } else {\\n+      logger.warn(`[WorkflowProvider]   NO WORKSPACE from parent - nested checkouts won't be added to workspace!`);\\n+    }\\n \\n     const childContext = {\\n       mode: 'state-machine' as const,\\n@@ -374,7 +480,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       checks: checksMetadata,\\n       journal: childJournal,\\n       memory: childMemory,\\n+      // For nested workflows we continue to execute inside the same logical\\n+      // working directory as the parent run. When workspace isolation is\\n+      // enabled on the parent engine, its WorkspaceManager is also propagated\\n+      // so that nested checks (AI, git-checkout, etc.) see the same isolated\\n+      // workspace and project symlinks instead of falling back to the Visor\\n+      // repository root.\\n       workingDirectory: parentContext?.workingDirectory || process.cwd(),\\n+      originalWorkingDirectory:\\n+        parentContext?.originalWorkingDirectory ||\\n+        parentContext?.workingDirectory ||\\n+        process.cwd(),\\n+      workspace: parentWorkspace,\\n       // Always use a fresh session for nested workflows to isolate history\\n       sessionId: uuidv4(),\\n       event: parentContext?.event || prInfo.eventType,\\n@@ -500,6 +617,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     } catch {}\\n \\n+    // Create outputs map that directly exposes .output values (not wrapped in { output, issues })\\n+    // This makes {{ outputs['check-name'] }} work naturally without needing .output\\n+    const outputsMap = Object.fromEntries(\\n+      Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n+    );\\n+\\n     for (const output of workflow.outputs) {\\n       if (output.value_js) {\\n         // JavaScript expression\\n@@ -508,10 +631,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           output.value_js,\\n           {\\n             inputs,\\n-            steps: Object.fromEntries(\\n-              Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-            ),\\n-            outputs: flat,\\n+            outputs: outputsMap,\\n+            // Keep 'steps' as alias for backwards compatibility\\n+            steps: outputsMap,\\n             pr: prInfo,\\n           },\\n           { injectLog: true, logPrefix: `workflow.output.${output.name}` }\\n@@ -520,10 +642,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         // Liquid template\\n         outputs[output.name] = await this.liquid.parseAndRender(output.value, {\\n           inputs,\\n-          steps: Object.fromEntries(\\n-            Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-          ),\\n-          outputs: flat,\\n+          outputs: outputsMap,\\n+          // Keep 'steps' as alias for backwards compatibility\\n+          steps: outputsMap,\\n           pr: prInfo,\\n         });\\n       }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/client.ts\",\"additions\":65,\"deletions\":1,\"changes\":66,\"patch\":\"diff --git a/src/slack/client.ts b/src/slack/client.ts\\nindex 292cc358..435f8b0d 100644\\n--- a/src/slack/client.ts\\n+++ b/src/slack/client.ts\\n@@ -142,6 +142,70 @@ export class SlackClient {\\n     }\\n   }\\n \\n+  public readonly files = {\\n+    /**\\n+     * Upload a file to Slack using files.uploadV2 API\\n+     * @param options Upload options including file content, filename, channel, and thread_ts\\n+     */\\n+    uploadV2: async ({\\n+      content,\\n+      filename,\\n+      channel,\\n+      thread_ts,\\n+      title,\\n+      initial_comment,\\n+    }: {\\n+      content: Buffer;\\n+      filename: string;\\n+      channel: string;\\n+      thread_ts?: string;\\n+      title?: string;\\n+      initial_comment?: string;\\n+    }): Promise<{ ok: boolean; file?: { id: string; permalink?: string } }> => {\\n+      try {\\n+        // Step 1: Get upload URL\\n+        const getUrlResp: any = await this.api('files.getUploadURLExternal', {\\n+          filename,\\n+          length: content.length,\\n+        });\\n+        if (!getUrlResp || getUrlResp.ok !== true || !getUrlResp.upload_url) {\\n+          console.warn(`Slack files.getUploadURLExternal failed: ${getUrlResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 2: Upload file content to the URL\\n+        const uploadResp = await fetch(getUrlResp.upload_url, {\\n+          method: 'POST',\\n+          body: content,\\n+        });\\n+        if (!uploadResp.ok) {\\n+          console.warn(`Slack file upload to URL failed: ${uploadResp.status}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 3: Complete the upload and share to channel\\n+        const completeResp: any = await this.api('files.completeUploadExternal', {\\n+          files: [{ id: getUrlResp.file_id, title: title || filename }],\\n+          channel_id: channel,\\n+          thread_ts,\\n+          initial_comment,\\n+        });\\n+        if (!completeResp || completeResp.ok !== true) {\\n+          console.warn(`Slack files.completeUploadExternal failed: ${completeResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        return {\\n+          ok: true,\\n+          file: completeResp.files?.[0] || { id: getUrlResp.file_id },\\n+        };\\n+      } catch (e) {\\n+        console.warn(`Slack file upload failed: ${e instanceof Error ? e.message : String(e)}`);\\n+        return { ok: false };\\n+      }\\n+    },\\n+  };\\n+\\n   getWebClient(): any {\\n     return {\\n       conversations: {\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/markdown.ts\",\"additions\":204,\"deletions\":5,\"changes\":209,\"patch\":\"diff --git a/src/slack/markdown.ts b/src/slack/markdown.ts\\nindex cc63fb5f..9f91c25f 100644\\n--- a/src/slack/markdown.ts\\n+++ b/src/slack/markdown.ts\\n@@ -3,14 +3,191 @@\\n // without pulling in a full Markdown parser.\\n //\\n // Supported conversions:\\n+// - # Header / ## Header  ‚Üí *Header* (bold with visual separation)\\n // - **bold** / __bold__   ‚Üí *bold*\\n // - [label](url)          ‚Üí <url|label>\\n // - ![alt](url)           ‚Üí <url|alt>\\n // - *italic* (inline)     ‚Üí _italic_\\n+// - ```mermaid blocks     ‚Üí rendered to PNG and uploaded to Slack\\n //\\n // Everything else is passed through unchanged; Slack will still render many\\n // Markdown-like constructs (lists, code fences, etc.) natively.\\n \\n+import { spawn } from 'child_process';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n+import * as os from 'os';\\n+\\n+/**\\n+ * Represents an extracted mermaid diagram\\n+ */\\n+export interface MermaidDiagram {\\n+  /** The full match including ```mermaid and ``` */\\n+  fullMatch: string;\\n+  /** The mermaid code content */\\n+  code: string;\\n+  /** Start index in the original text */\\n+  startIndex: number;\\n+  /** End index in the original text */\\n+  endIndex: number;\\n+}\\n+\\n+/**\\n+ * Extract all mermaid code blocks from text\\n+ */\\n+export function extractMermaidDiagrams(text: string): MermaidDiagram[] {\\n+  const diagrams: MermaidDiagram[] = [];\\n+  // Match ```mermaid followed by newline, content, and closing ```\\n+  const regex = /```mermaid\\\\s*\\\\n([\\\\s\\\\S]*?)```/g;\\n+  let match;\\n+  while ((match = regex.exec(text)) !== null) {\\n+    diagrams.push({\\n+      fullMatch: match[0],\\n+      code: match[1].trim(),\\n+      startIndex: match.index,\\n+      endIndex: match.index + match[0].length,\\n+    });\\n+  }\\n+  return diagrams;\\n+}\\n+\\n+/**\\n+ * Render a mermaid diagram to PNG using mmdc CLI (@mermaid-js/mermaid-cli).\\n+ *\\n+ * Requirements:\\n+ * - Node.js and npx must be available in PATH\\n+ * - Network access on first run (npx downloads the package)\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ *\\n+ * On Linux, you may need to install chromium dependencies:\\n+ *   apt-get install -y chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ *\\n+ * On Docker/CI, consider using a base image with puppeteer support or\\n+ * pre-installing @mermaid-js/mermaid-cli globally.\\n+ *\\n+ * @param mermaidCode The mermaid diagram code\\n+ * @returns Buffer containing PNG data, or null if rendering failed\\n+ */\\n+export async function renderMermaidToPng(mermaidCode: string): Promise<Buffer | null> {\\n+  // Create temp files for input and output\\n+  const tmpDir = os.tmpdir();\\n+  const inputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.mmd`);\\n+  const outputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.png`);\\n+\\n+  try {\\n+    // Write mermaid code to temp file\\n+    fs.writeFileSync(inputFile, mermaidCode, 'utf-8');\\n+\\n+    // Detect system chromium for puppeteer (mermaid-cli dependency)\\n+    // Without this, puppeteer may hang trying to download its own chromium\\n+    const chromiumPaths = [\\n+      '/usr/bin/chromium',\\n+      '/usr/bin/chromium-browser',\\n+      '/usr/bin/google-chrome',\\n+      '/usr/bin/chrome',\\n+    ];\\n+    let chromiumPath: string | undefined;\\n+    for (const p of chromiumPaths) {\\n+      if (fs.existsSync(p)) {\\n+        chromiumPath = p;\\n+        break;\\n+      }\\n+    }\\n+\\n+    // Build environment with chromium path if found\\n+    const env = { ...process.env };\\n+    if (chromiumPath) {\\n+      env.PUPPETEER_EXECUTABLE_PATH = chromiumPath;\\n+    }\\n+\\n+    // Run mmdc to render PNG\\n+    const result = await new Promise<{ success: boolean; error?: string }>((resolve) => {\\n+      const proc = spawn('npx', [\\n+        '--yes',\\n+        '@mermaid-js/mermaid-cli',\\n+        '-i', inputFile,\\n+        '-o', outputFile,\\n+        '-e', 'png',\\n+        '-b', 'white',\\n+        '-w', '1200',\\n+      ], {\\n+        timeout: 60000, // 60 second timeout (first run may download packages)\\n+        stdio: ['pipe', 'pipe', 'pipe'],\\n+        env,\\n+      });\\n+\\n+      let stderr = '';\\n+      proc.stderr?.on('data', (data) => {\\n+        stderr += data.toString();\\n+      });\\n+\\n+      proc.on('close', (code) => {\\n+        if (code === 0) {\\n+          resolve({ success: true });\\n+        } else {\\n+          resolve({ success: false, error: stderr || `Exit code ${code}` });\\n+        }\\n+      });\\n+\\n+      proc.on('error', (err) => {\\n+        resolve({ success: false, error: err.message });\\n+      });\\n+    });\\n+\\n+    if (!result.success) {\\n+      console.warn(`Mermaid rendering failed: ${result.error}`);\\n+      return null;\\n+    }\\n+\\n+    // Read the output PNG\\n+    if (!fs.existsSync(outputFile)) {\\n+      console.warn('Mermaid output file not created');\\n+      return null;\\n+    }\\n+\\n+    const pngBuffer = fs.readFileSync(outputFile);\\n+    return pngBuffer;\\n+  } catch (e) {\\n+    console.warn(`Mermaid rendering error: ${e instanceof Error ? e.message : String(e)}`);\\n+    return null;\\n+  } finally {\\n+    // Cleanup temp files\\n+    try {\\n+      if (fs.existsSync(inputFile)) fs.unlinkSync(inputFile);\\n+      if (fs.existsSync(outputFile)) fs.unlinkSync(outputFile);\\n+    } catch {\\n+      // Ignore cleanup errors\\n+    }\\n+  }\\n+}\\n+\\n+/**\\n+ * Replace mermaid blocks in text with a placeholder message\\n+ * @param text Original text\\n+ * @param diagrams Extracted diagrams\\n+ * @param replacement Text to replace each diagram with (or a function that returns replacement for each index)\\n+ */\\n+export function replaceMermaidBlocks(\\n+  text: string,\\n+  diagrams: MermaidDiagram[],\\n+  replacement: string | ((index: number) => string) = '_(See diagram above)_'\\n+): string {\\n+  if (diagrams.length === 0) return text;\\n+\\n+  // Sort by start index descending to replace from end to start (preserves indices)\\n+  const sorted = [...diagrams].sort((a, b) => b.startIndex - a.startIndex);\\n+\\n+  let result = text;\\n+  sorted.forEach((diagram, sortedIndex) => {\\n+    // Calculate original index (since we sorted in reverse)\\n+    const originalIndex = diagrams.length - 1 - sortedIndex;\\n+    const rep = typeof replacement === 'function' ? replacement(originalIndex) : replacement;\\n+    result = result.slice(0, diagram.startIndex) + rep + result.slice(diagram.endIndex);\\n+  });\\n+\\n+  return result;\\n+}\\n+\\n export function markdownToSlack(text: string): string {\\n   if (!text || typeof text !== 'string') return '';\\n \\n@@ -33,7 +210,7 @@ export function markdownToSlack(text: string): string {\\n   out = out.replace(/\\\\*\\\\*([^*]+)\\\\*\\\\*/g, (_m, inner: string) => `*${inner}*`);\\n   out = out.replace(/__([^_]+)__/g, (_m, inner: string) => `*${inner}*`);\\n \\n-  // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation).\\n+  // Process lines for headers and bullet lists.\\n   // Slack's mrkdwn handles \\\"‚Ä¢\\\" bullets more naturally than raw \\\"-\\\" Markdown.\\n   const lines = out.split(/\\\\r?\\\\n/);\\n   let inCodeBlock = false;\\n@@ -46,9 +223,31 @@ export function markdownToSlack(text: string): string {\\n       continue;\\n     }\\n     if (inCodeBlock) continue;\\n-    const match = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n-    if (match) {\\n-      const [, indent, , rest] = match;\\n+\\n+    // Headers: # Header ‚Üí *Header* (Slack doesn't have native headers)\\n+    // Match 1-6 # at start of line, followed by space and text\\n+    const headerMatch = /^(#{1,6})\\\\s+(.+)$/.exec(trimmed);\\n+    if (headerMatch) {\\n+      const [, hashes, headerText] = headerMatch;\\n+      // For h1/h2, add extra emphasis with newline before (if not first line\\n+      // and previous line is not empty/header/code-fence)\\n+      const prevLine = i > 0 ? lines[i - 1].trim() : '';\\n+      const prevIsHeaderOrFence =\\n+        /^#{1,6}\\\\s+/.test(prevLine) ||\\n+        /^\\\\*[^*]+\\\\*$/.test(prevLine) ||\\n+        /^```/.test(prevLine);\\n+      if (hashes.length <= 2 && i > 0 && prevLine !== '' && !prevIsHeaderOrFence) {\\n+        lines[i] = `\\\\n*${headerText.trim()}*`;\\n+      } else {\\n+        lines[i] = `*${headerText.trim()}*`;\\n+      }\\n+      continue;\\n+    }\\n+\\n+    // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation)\\n+    const bulletMatch = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n+    if (bulletMatch) {\\n+      const [, indent, , rest] = bulletMatch;\\n       lines[i] = `${indent}‚Ä¢ ${rest}`;\\n     }\\n   }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/prompt-state.ts\",\"additions\":19,\"deletions\":2,\"changes\":21,\"patch\":\"diff --git a/src/slack/prompt-state.ts b/src/slack/prompt-state.ts\\nindex a13233b2..544777c6 100644\\n--- a/src/slack/prompt-state.ts\\n+++ b/src/slack/prompt-state.ts\\n@@ -95,7 +95,10 @@ export class PromptStateManager {\\n   setFirstMessage(channel: string, threadTs: string, text: string): void {\\n     const key = this.key(channel, threadTs);\\n     if (!text || !text.trim()) return;\\n-    if (!this.firstMessage.has(key)) {\\n+    const existing = this.firstMessage.get(key);\\n+    // Only set if: no entry exists OR the existing entry was already consumed\\n+    // This allows new messages to be captured after a resume cycle\\n+    if (!existing || existing.consumed) {\\n       this.firstMessage.set(key, { text, consumed: false });\\n     }\\n   }\\n@@ -130,6 +133,20 @@ export class PromptStateManager {\\n         removed++;\\n       }\\n     }\\n+    // Also clean up stale firstMessage entries (consumed entries older than TTL)\\n+    // Keep unconsumed entries to avoid losing user messages\\n+    for (const [key] of this.firstMessage.entries()) {\\n+      const waitingInfo = this.waiting.get(key);\\n+      // If no corresponding waiting entry exists and the firstMessage was consumed,\\n+      // the conversation is likely complete - safe to remove\\n+      if (!waitingInfo) {\\n+        const entry = this.firstMessage.get(key);\\n+        if (entry?.consumed) {\\n+          this.firstMessage.delete(key);\\n+          removed++;\\n+        }\\n+      }\\n+    }\\n     if (removed) {\\n       try {\\n         logger.info(`[prompt-state] cleanup removed ${removed} entries`);\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/socket-runner.ts\",\"additions\":56,\"deletions\":32,\"changes\":88,\"patch\":\"diff --git a/src/slack/socket-runner.ts b/src/slack/socket-runner.ts\\nindex d1681a03..5b461d22 100644\\n--- a/src/slack/socket-runner.ts\\n+++ b/src/slack/socket-runner.ts\\n@@ -7,6 +7,7 @@ import { SlackAdapter } from './adapter';\\n import { CachePrewarmer } from './cache-prewarmer';\\n import { RateLimiter, type RateLimitConfig } from './rate-limiter';\\n import type { SlackBotConfig } from '../types/bot';\\n+import { withActiveSpan } from '../telemetry/trace-helpers';\\n \\n type SlackSocketConfig = {\\n   appToken?: string; // xapp- token\\n@@ -30,6 +31,7 @@ export class SlackSocketRunner {\\n   private botUserId?: string;\\n   private processedKeys: Map<string, number> = new Map();\\n   private adapter?: SlackAdapter;\\n+  private retryCount = 0;\\n \\n   constructor(engine: StateMachineExecutionEngine, cfg: VisorConfig, opts: SlackSocketConfig) {\\n     const app = opts.appToken || process.env.SLACK_APP_TOKEN || '';\\n@@ -107,7 +109,10 @@ export class SlackSocketRunner {\\n \\n   private async connect(url: string): Promise<void> {\\n     this.ws = new WebSocket(url);\\n-    this.ws.on('open', () => logger.info('[SlackSocket] WebSocket connected'));\\n+    this.ws.on('open', () => {\\n+      this.retryCount = 0; // Reset on successful connection\\n+      logger.info('[SlackSocket] WebSocket connected');\\n+    });\\n     this.ws.on('close', (code, reason) => {\\n       logger.warn(`[SlackSocket] WebSocket closed: ${code} ${reason}`);\\n       setTimeout(() => this.restart().catch(() => {}), 1000);\\n@@ -123,7 +128,13 @@ export class SlackSocketRunner {\\n       const url = await this.openConnection();\\n       await this.connect(url);\\n     } catch (e) {\\n-      logger.error(`[SlackSocket] Restart failed: ${e instanceof Error ? e.message : e}`);\\n+      this.retryCount++;\\n+      // Exponential backoff: 2s, 4s, 8s, 16s, 32s, capped at 60s\\n+      const delay = Math.min(2000 * Math.pow(2, this.retryCount - 1), 60000);\\n+      logger.error(\\n+        `[SlackSocket] Restart failed (attempt ${this.retryCount}), retrying in ${Math.round(delay / 1000)}s: ${e instanceof Error ? e.message : e}`\\n+      );\\n+      setTimeout(() => this.restart().catch(() => {}), delay);\\n     }\\n   }\\n \\n@@ -296,36 +307,49 @@ export class SlackSocketRunner {\\n           });\\n         }\\n       } catch {}\\n-      if (path) {\\n-        try {\\n-          const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n-          const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n-          logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n-          await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n-            webhookContext: { webhookData: map, eventType: 'manual' },\\n-            debug: process.env.VISOR_DEBUG === 'true',\\n-          });\\n-          if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n-          return; // resume path handled\\n-        } catch (e) {\\n-          logger.warn(\\n-            `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n-              e instanceof Error ? e.message : String(e)\\n-            }`\\n-          );\\n-        }\\n-      }\\n+      try {\\n+        await withActiveSpan(\\n+          'visor.run',\\n+          {\\n+            'visor.run.source': 'slack',\\n+            'slack.event.type': String(type || ''),\\n+            'slack.channel': channelId,\\n+            'slack.thread_ts': threadTs,\\n+          },\\n+          async () => {\\n+            if (path) {\\n+              try {\\n+                const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n+                const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n+                logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n+                await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n+                  webhookContext: { webhookData: map, eventType: 'manual' },\\n+                  debug: process.env.VISOR_DEBUG === 'true',\\n+                });\\n+                return;\\n+              } catch (e) {\\n+                logger.warn(\\n+                  `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n+                    e instanceof Error ? e.message : String(e)\\n+                  }`\\n+                );\\n+              }\\n+            }\\n \\n-      // Cold run (no snapshot)\\n-      await runEngine.executeChecks({\\n-        checks: allChecks,\\n-        showDetails: true,\\n-        outputFormat: 'json',\\n-        config: cfgForRun,\\n-        webhookContext: { webhookData: map, eventType: 'manual' },\\n-        debug: process.env.VISOR_DEBUG === 'true',\\n-      } as any);\\n-      if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+            // Cold run (no snapshot)\\n+            await runEngine.executeChecks({\\n+              checks: allChecks,\\n+              showDetails: true,\\n+              outputFormat: 'json',\\n+              config: cfgForRun,\\n+              webhookContext: { webhookData: map, eventType: 'manual' },\\n+              debug: process.env.VISOR_DEBUG === 'true',\\n+            } as any);\\n+          }\\n+        );\\n+      } finally {\\n+        if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+      }\\n     } catch (e) {\\n       logger.error(\\n         `[SlackSocket] Engine execution failed: ${e instanceof Error ? e.message : String(e)}`\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/context/build-engine-context.ts\",\"additions\":11,\"deletions\":4,\"changes\":15,\"patch\":\"diff --git a/src/state-machine/context/build-engine-context.ts b/src/state-machine/context/build-engine-context.ts\\nindex 149c73bf..45e63b6a 100644\\n--- a/src/state-machine/context/build-engine-context.ts\\n+++ b/src/state-machine/context/build-engine-context.ts\\n@@ -57,7 +57,8 @@ export function buildEngineContextForRun(\\n       ) as EventTrigger[],\\n       group: checkConfig.group,\\n       providerType: checkConfig.type || 'ai',\\n-      dependencies: checkConfig.depends_on || [],\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: Array.isArray(checkConfig.depends_on) ? checkConfig.depends_on : checkConfig.depends_on ? [checkConfig.depends_on] : [],\\n     };\\n   }\\n \\n@@ -134,11 +135,14 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n   const originalPath = context.workingDirectory || process.cwd();\\n \\n   try {\\n+    // Check if workspace should be kept (for debugging)\\n+    const keepWorkspace = process.env.VISOR_KEEP_WORKSPACE === 'true';\\n+\\n     // Create workspace manager\\n     const workspace = WorkspaceManager.getInstance(context.sessionId, originalPath, {\\n       enabled: true,\\n-      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH,\\n-      cleanupOnExit: workspaceConfig?.cleanup_on_exit !== false,\\n+      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH || '/tmp/visor-workspaces',\\n+      cleanupOnExit: keepWorkspace ? false : workspaceConfig?.cleanup_on_exit !== false,\\n     });\\n \\n     // Initialize workspace (creates main project worktree)\\n@@ -151,6 +155,9 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n \\n     logger.info(`[Workspace] Initialized workspace: ${info.workspacePath}`);\\n     logger.debug(`[Workspace] Main project at: ${info.mainProjectPath}`);\\n+    if (keepWorkspace) {\\n+      logger.info(`[Workspace] Keeping workspace after execution (--keep-workspace)`);\\n+    }\\n \\n     return context;\\n   } catch (error) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/execution-invoker.ts\",\"additions\":84,\"deletions\":4,\"changes\":88,\"patch\":\"diff --git a/src/state-machine/dispatch/execution-invoker.ts b/src/state-machine/dispatch/execution-invoker.ts\\nindex eb7587d3..798ca8e4 100644\\n--- a/src/state-machine/dispatch/execution-invoker.ts\\n+++ b/src/state-machine/dispatch/execution-invoker.ts\\n@@ -356,10 +356,17 @@ export async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -413,6 +420,31 @@ export async function executeSingleCheck(\\n     }\\n   }\\n \\n+  // Banner-style log when a check actually starts executing. This is emitted\\n+  // after all gating (if/depends_on) has passed so it only appears for real\\n+  // runs, and gives a clear visual separator in the logs between checks.\\n+  try {\\n+    const wave = state.wave;\\n+    const level = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${level}) ‚îÅ‚îÅ‚îÅ`;\\n+\\n+    // When running in a TTY, colour the entire banner line for extra\\n+    // visibility; keep plain text for JSON/SARIF or non-TTY environments.\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   let forEachParent: string | undefined;\\n   let forEachItems: unknown[] | undefined;\\n   for (const depId of depList) {\\n@@ -563,13 +595,56 @@ export async function executeSingleCheck(\\n       files: [],\\n       commits: [],\\n     };\\n+    // Derive AI session reuse context for this check (self-mode only for now).\\n+    // When reuse_ai_session: 'self' is configured, look up the last root-scope\\n+    // journal entry for this check in the current engine session and expose its\\n+    // sessionId via ExecutionContext so ai-check-provider can call\\n+    // executeReviewWithSessionReuse() against the same ProbeAgent session.\\n+    let parentSessionId: string | undefined;\\n+    let reuseSession = false;\\n+    try {\\n+      const reuseCfg: unknown = (checkConfig as any).reuse_ai_session;\\n+      if (reuseCfg === 'self') {\\n+        const snapshotId = context.journal.beginSnapshot();\\n+        const visible = context.journal.readVisible(\\n+          context.sessionId,\\n+          snapshotId,\\n+          context.event as any\\n+        );\\n+        // Prefer the most recent root-scope result for this check\\n+        const prior = visible.filter(\\n+          e => e.checkId === checkId && (!e.scope || e.scope.length === 0)\\n+        );\\n+        if (prior.length > 0) {\\n+          const last = prior[prior.length - 1];\\n+          const sess = (last.result as any)?.sessionId;\\n+          if (typeof sess === 'string' && sess.length > 0) {\\n+            parentSessionId = sess;\\n+            reuseSession = true;\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only ‚Äì fall back to normal (non-reuse) execution on error.\\n+      parentSessionId = undefined;\\n+      reuseSession = false;\\n+    }\\n+\\n     const executionContext = {\\n       ...context.executionContext,\\n       _engineMode: context.mode,\\n       _parentContext: context,\\n       _parentState: state,\\n+      // Explicitly propagate workspace reference for nested workflows\\n+      workspace: context.workspace,\\n     };\\n \\n+    // Attach session reuse hints for providers that support them (AI, Claude Code, etc).\\n+    if (reuseSession && parentSessionId) {\\n+      (executionContext as any).parentSessionId = parentSessionId;\\n+      (executionContext as any).reuseSession = true;\\n+    }\\n+\\n     // Handle on_init lifecycle hook BEFORE main execution\\n     if (checkConfig.on_init) {\\n       try {\\n@@ -613,7 +688,12 @@ export async function executeSingleCheck(\\n \\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/foreach-processor.ts\",\"additions\":27,\"deletions\":2,\"changes\":29,\"patch\":\"diff --git a/src/state-machine/dispatch/foreach-processor.ts b/src/state-machine/dispatch/foreach-processor.ts\\nindex 336bf9df..ab630986 100644\\n--- a/src/state-machine/dispatch/foreach-processor.ts\\n+++ b/src/state-machine/dispatch/foreach-processor.ts\\n@@ -77,6 +77,26 @@ export async function executeCheckWithForEachItems(\\n   const allContents: string[] = [];\\n   const perIterationDurations: number[] = [];\\n \\n+  // Emit a banner for the forEach parent so logs clearly show when we are\\n+  // entering the aggregated execution for that step.\\n+  try {\\n+    const wave = state.wave;\\n+    const lvl = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${lvl}, forEach parent) ‚îÅ‚îÅ‚îÅ`;\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   for (let itemIndex = 0; itemIndex < forEachItems.length; itemIndex++) {\\n     const iterationStartMs = Date.now();\\n     const scope: Array<{ check: string; index: number }> = [\\n@@ -248,7 +268,12 @@ export async function executeCheckWithForEachItems(\\n \\n       const result = await withActiveSpan(\\n         `visor.check.${checkId}`,\\n-        { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+        {\\n+          'visor.check.id': checkId,\\n+          'visor.check.type': providerType,\\n+          session_id: context.sessionId,\\n+          wave: state.wave,\\n+        },\\n         async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n       );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/stats-manager.ts\",\"additions\":21,\"deletions\":1,\"changes\":22,\"patch\":\"diff --git a/src/state-machine/dispatch/stats-manager.ts b/src/state-machine/dispatch/stats-manager.ts\\nindex 8d04436e..2f317b59 100644\\n--- a/src/state-machine/dispatch/stats-manager.ts\\n+++ b/src/state-machine/dispatch/stats-manager.ts\\n@@ -44,6 +44,26 @@ export function updateStats(\\n       issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n     };\\n \\n+    // Respect explicit skip markers on the result (set by LevelDispatch).\\n+    const skippedMarker = (result as any).__skipped;\\n+    if (skippedMarker) {\\n+      stats.skipped = true;\\n+      stats.skipReason =\\n+        typeof skippedMarker === 'string'\\n+          ? (skippedMarker as any)\\n+          : (stats.skipReason as any) || 'if_condition';\\n+      // A skipped result should not contribute to run counters; reset\\n+      // them explicitly to guard against any prior values lingering\\n+      // from earlier runs of the same check.\\n+      stats.totalRuns = 0;\\n+      stats.successfulRuns = 0;\\n+      stats.failedRuns = 0;\\n+      stats.skippedRuns++;\\n+      // Do not count this as a run; keep totalRuns/failed/success unchanged.\\n+      state.stats.set(checkId, stats);\\n+      continue;\\n+    }\\n+\\n     // If this check was previously marked as skipped and now executed,\\n     // clear the skipped flag and any skip counters to ensure the run is visible.\\n     if (stats.skipped) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/runner.ts\",\"additions\":10,\"deletions\":7,\"changes\":17,\"patch\":\"diff --git a/src/state-machine/runner.ts b/src/state-machine/runner.ts\\nindex ee86b876..a13a5efd 100644\\n--- a/src/state-machine/runner.ts\\n+++ b/src/state-machine/runner.ts\\n@@ -124,14 +124,17 @@ export class StateMachineRunner {\\n    */\\n   private async executeState(state: EngineState): Promise<void> {\\n     // M4: Wrap state execution in OTEL span\\n+    const attrs: Record<string, unknown> = {\\n+      state: state,\\n+      engine_mode: this.context.mode,\\n+      wave: this.state.wave,\\n+      session_id: this.context.sessionId,\\n+    };\\n+    const waveKind = (this.state as any)?.flags?.waveKind;\\n+    if (waveKind) attrs.wave_kind = waveKind;\\n     return withActiveSpan(\\n       `engine.state.${state.toLowerCase()}`,\\n-      {\\n-        state: state,\\n-        engine_mode: this.context.mode,\\n-        wave: this.state.wave,\\n-        session_id: this.context.sessionId,\\n-      },\\n+      attrs,\\n       async () => {\\n         try {\\n           switch (state) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/level-dispatch.ts\",\"additions\":100,\"deletions\":15,\"changes\":115,\"patch\":\"diff --git a/src/state-machine/states/level-dispatch.ts b/src/state-machine/states/level-dispatch.ts\\nindex 5588b0f5..42ee383e 100644\\n--- a/src/state-machine/states/level-dispatch.ts\\n+++ b/src/state-machine/states/level-dispatch.ts\\n@@ -26,7 +26,7 @@ import type { CheckExecutionStats } from '../../types/execution';\\n import type { CheckProviderConfig } from '../../providers/check-provider.interface';\\n import type { CheckConfig } from '../../types/config';\\n import { handleRouting, checkLoopBudget } from './routing';\\n-import { withActiveSpan } from '../../telemetry/trace-helpers';\\n+import { withActiveSpan, setSpanAttributes } from '../../telemetry/trace-helpers';\\n import { emitMermaidFromMarkdown } from '../../utils/mermaid-telemetry';\\n import { emitNdjsonSpanWithEvents, emitNdjsonFallback } from '../../telemetry/fallback-ndjson';\\n import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n@@ -121,12 +121,13 @@ async function evaluateIfCondition(\\n         return false;\\n       }\\n     })();\\n+    // Steps with dependencies should always see outputs from all completed steps.\\n     // In forward-run waves (from on_success/on_fail goto), guards should see the\\n     // latest global outputs even if the check has no explicit dependencies.\\n     // In wave-retry (from on_finish), restrict to checks with dependencies to\\n     // avoid wrongly skipping top-level prompts like 'ask'.\\n     const useGlobalOutputs =\\n-      (useGlobalOutputsFlag && waveKind === 'forward') || (useGlobalOutputsFlag && hasDeps);\\n+      hasDeps || (useGlobalOutputsFlag && waveKind === 'forward');\\n \\n     if (useGlobalOutputs) {\\n       // Forward-run wave: allow guards to consult latest outputs from the entire\\n@@ -199,6 +200,7 @@ async function evaluateIfCondition(\\n       baseBranch: (context.prInfo as any)?.baseBranch,\\n       filesChanged: context.prInfo?.files?.map(f => f.filename),\\n       environment: envSnapshot,\\n+      workflowInputs: (context.config as any).workflow_inputs || {},\\n     };\\n \\n     const shouldRun = await evaluator.evaluateIfCondition(checkId, ifExpression, contextData);\\n@@ -238,6 +240,7 @@ export async function handleLevelDispatch(\\n   // Update current level tracking\\n   state.currentLevel = level.level;\\n   state.currentLevelChecks = new Set(level.parallel);\\n+  setSpanAttributes({ level_size: level.parallel.length });\\n \\n   // Emit level ready event\\n   emitEvent({ type: 'LevelReady', level, wave: state.wave });\\n@@ -689,6 +692,39 @@ async function executeCheckWithForEachItems(\\n         }\\n       } catch {}\\n \\n+      // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+      // The socket-runner stores conversation data in webhookData under the endpoint key\\n+      try {\\n+        const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+        const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+        if (context.debug) {\\n+          logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+        }\\n+        if (webhookData && webhookData.size > 0) {\\n+          // Find the payload with slack_conversation\\n+          for (const payload of webhookData.values()) {\\n+            const slackConv = (payload as any)?.slack_conversation;\\n+            if (slackConv) {\\n+              // Build slack context with event and conversation\\n+              const event = (payload as any)?.event;\\n+              const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+              if (context.debug) {\\n+                logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+              }\\n+              (providerConfig as any).eventContext = {\\n+                ...(providerConfig as any).eventContext,\\n+                slack: {\\n+                  event: event || {},\\n+                  conversation: slackConv,\\n+                },\\n+                conversation: slackConv, // Also expose at top level for convenience\\n+              };\\n+              break;\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n+\\n       // Build dependency results with scope\\n       const dependencyResults = buildDependencyResultsWithScope(\\n         checkId,\\n@@ -820,15 +856,17 @@ async function executeCheckWithForEachItems(\\n       } catch {}\\n \\n       // Execute provider with telemetry\\n-      const itemResult = await withActiveSpan(\\n-        `visor.check.${checkId}`,\\n-        {\\n-          'visor.check.id': checkId,\\n-          'visor.check.type': providerType,\\n-          'visor.foreach.index': itemIndex,\\n-        },\\n-        async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n-      );\\n+    const itemResult = await withActiveSpan(\\n+      `visor.check.${checkId}`,\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        'visor.foreach.index': itemIndex,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n+      async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n+    );\\n \\n       // Enrich issues\\n       const enrichedIssues = (itemResult.issues || []).map((issue: ReviewIssue) => ({\\n@@ -942,6 +980,7 @@ async function executeCheckWithForEachItems(\\n             const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n               previousResults: dependencyResults as any,\\n               event: context.event || 'manual',\\n+              output: output, // Pass the iteration output for guarantee evaluation\\n             } as any);\\n             if (!holds) {\\n               const issue: ReviewIssue = {\\n@@ -1618,10 +1657,17 @@ async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -1923,6 +1969,39 @@ async function executeSingleCheck(\\n       }\\n     } catch {}\\n \\n+    // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+    // The socket-runner stores conversation data in webhookData under the endpoint key\\n+    try {\\n+      const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+      const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+      if (context.debug) {\\n+        logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+      }\\n+      if (webhookData && webhookData.size > 0) {\\n+        // Find the payload with slack_conversation\\n+        for (const payload of webhookData.values()) {\\n+          const slackConv = (payload as any)?.slack_conversation;\\n+          if (slackConv) {\\n+            // Build slack context with event and conversation\\n+            const event = (payload as any)?.event;\\n+            const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+            if (context.debug) {\\n+              logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+            }\\n+            (providerConfig as any).eventContext = {\\n+              ...(providerConfig as any).eventContext,\\n+              slack: {\\n+                event: event || {},\\n+                conversation: slackConv,\\n+              },\\n+              conversation: slackConv, // Also expose at top level for convenience\\n+            };\\n+            break;\\n+          }\\n+        }\\n+      }\\n+    } catch {}\\n+\\n     // Build dependency results\\n     const dependencyResults = buildDependencyResults(checkId, checkConfig, context, state);\\n \\n@@ -2016,7 +2095,12 @@ async function executeSingleCheck(\\n     // Execute provider with telemetry\\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n@@ -2098,6 +2182,7 @@ async function executeSingleCheck(\\n           const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n             previousResults: dependencyResults as any,\\n             event: context.event || 'manual',\\n+            output: enrichedResult.output,\\n           } as any);\\n           if (!holds) {\\n             const issue: ReviewIssue = {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/wave-planning.ts\",\"additions\":10,\"deletions\":3,\"changes\":13,\"patch\":\"diff --git a/src/state-machine/states/wave-planning.ts b/src/state-machine/states/wave-planning.ts\\nindex 7551d55c..7b42f043 100644\\n--- a/src/state-machine/states/wave-planning.ts\\n+++ b/src/state-machine/states/wave-planning.ts\\n@@ -351,8 +351,11 @@ export async function handleWavePlanning(\\n     for (const id of checksToRun) {\\n       // Only include dependencies that are within the same subset (often none).\\n       const cfg = context.config.checks?.[id];\\n-      const deps = (cfg?.depends_on || []).filter((d: string) => checksToRun.includes(d));\\n-      subDeps[id] = deps as string[];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = cfg?.depends_on;\\n+      const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n+      const deps = depsArray.filter((d: string) => checksToRun.includes(d));\\n+      subDeps[id] = deps;\\n     }\\n \\n     const subGraph = DependencyResolver.buildDependencyGraph(subDeps);\\n@@ -410,6 +413,10 @@ export async function handleWavePlanning(\\n     // Initialize current wave state\\n     (state as any).currentWaveCompletions = new Set<string>();\\n     (state as any).failedChecks = new Set<string>();\\n+    try {\\n+      (state as any).flags = (state as any).flags || {};\\n+      (state as any).flags.waveKind = 'initial';\\n+    } catch {}\\n   }\\n \\n   // Check if there are levels to execute\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/workflow-projection.ts\",\"additions\":3,\"deletions\":2,\"changes\":5,\"patch\":\"diff --git a/src/state-machine/workflow-projection.ts b/src/state-machine/workflow-projection.ts\\nindex b2d5eb24..2bcfd858 100644\\n--- a/src/state-machine/workflow-projection.ts\\n+++ b/src/state-machine/workflow-projection.ts\\n@@ -53,7 +53,8 @@ export function projectWorkflowToGraph(\\n       triggers: step.on || workflow.on || [],\\n       group: step.group,\\n       providerType: step.type || 'ai',\\n-      dependencies: (step.depends_on || []).map(dep => dep),\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: (Array.isArray(step.depends_on) ? step.depends_on : step.depends_on ? [step.depends_on] : []).map((dep: string) => dep),\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/flow-stage.ts\",\"additions\":37,\"deletions\":4,\"changes\":41,\"patch\":\"diff --git a/src/test-runner/core/flow-stage.ts b/src/test-runner/core/flow-stage.ts\\nindex a14256c2..a587f9f1 100644\\n--- a/src/test-runner/core/flow-stage.ts\\n+++ b/src/test-runner/core/flow-stage.ts\\n@@ -352,10 +352,30 @@ export class FlowStage {\\n       } catch {}\\n       // Prefer authoritative counts from res.statistics when available, then fall back to deltas/inferred\\n       const fromResStats: Record<string, number> = {};\\n+      const resSkipped: Record<string, boolean> = {};\\n       try {\\n         for (const s of (res.statistics?.checks || []) as any[]) {\\n           const n = (s && s.checkName) || '';\\n-          if (typeof n === 'string' && n) fromResStats[n] = (s.totalRuns || 0) as number;\\n+          if (typeof n === 'string' && n) {\\n+            fromResStats[n] = (s.totalRuns || 0) as number;\\n+            resSkipped[n] = !!(s as any).skipped || !!(s as any).skipReason;\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      // Steps that have explicit expectations in this stage. We will use\\n+      // this to avoid inflating run counts (via parent-alignment heuristics)\\n+      // for checks that are not even expected in this stage.\\n+      const expectedSteps = new Set<string>();\\n+      const expectedZero = new Set<string>();\\n+      try {\\n+        const expCalls = ((stage.expect || {}).calls || []) as Array<{ step?: string }>;\\n+        for (const c of expCalls) {\\n+          if (c && typeof c.step === 'string' && c.step) expectedSteps.add(c.step);\\n+          try {\\n+            if (c && typeof c.step === 'string' && c.step && (c as any).exactly === 0)\\n+              expectedZero.add(c.step);\\n+          } catch {}\\n         }\\n       } catch {}\\n \\n@@ -410,10 +430,15 @@ export class FlowStage {\\n           const base = stageOnly ? 0 : statBase[name] || 0;\\n           const d = Math.max(0, resTotal - base);\\n           runs = d > 0 ? d : 0;\\n+          // If the engine marked this check as skipped, force runs to 0\\n+          // so parent-alignment heuristics below do not inflate it.\\n+          if (resSkipped[name]) {\\n+            runs = 0;\\n+          }\\n         } else {\\n           runs = deltaMap[name] !== undefined ? deltaMap[name] : inferred;\\n         }\\n-        if (runs === 0 && presentInResults.has(name)) runs = 1;\\n+        if (runs === 0 && presentInResults.has(name) && !resSkipped[name]) runs = 1;\\n         if (!isForEachLike && histRuns > 0) runs = histRuns;\\n         // Only use per-item history counts for non-forEach checks. For forEach parents,\\n         // use aggregated totals from res.statistics to reflect number of parent executions.\\n@@ -442,7 +467,15 @@ export class FlowStage {\\n             parentMax = Math.max(parentMax, dP);\\n           }\\n           // Apply only for non-forEach checks with no observable history in this stage\\n-          if (!isForEachLike && histRuns === 0 && histPerItemRuns === 0 && parentMax > 0) {\\n+          if (\\n+            !isForEachLike &&\\n+            histRuns === 0 &&\\n+            histPerItemRuns === 0 &&\\n+            parentMax > 0 &&\\n+            !resSkipped[name] &&\\n+            (expectedSteps.size === 0 || expectedSteps.has(name)) &&\\n+            !expectedZero.has(name)\\n+          ) {\\n             runs = Math.max(runs, parentMax);\\n           }\\n         } catch {}\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/test-execution-wrapper.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/test-runner/core/test-execution-wrapper.ts b/src/test-runner/core/test-execution-wrapper.ts\\nindex 254d0970..d0333152 100644\\n--- a/src/test-runner/core/test-execution-wrapper.ts\\n+++ b/src/test-runner/core/test-execution-wrapper.ts\\n@@ -31,6 +31,8 @@ export class TestExecutionWrapper {\\n       const prev: any = (this.engine as any).executionContext || {};\\n       const merged = {\\n         ...prev,\\n+        // Inject workflow inputs for template access via {{ inputs.* }}\\n+        workflowInputs: cfg.workflow_inputs || prev.workflowInputs || {},\\n         mode: {\\n           ...(prev.mode || {}),\\n           test: true,\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/evaluators.ts\",\"additions\":67,\"deletions\":2,\"changes\":69,\"patch\":\"diff --git a/src/test-runner/evaluators.ts b/src/test-runner/evaluators.ts\\nindex 4843163a..75e9c174 100644\\n--- a/src/test-runner/evaluators.ts\\n+++ b/src/test-runner/evaluators.ts\\n@@ -252,6 +252,69 @@ export function evaluatePrompts(\\n   }\\n }\\n \\n+/**\\n+ * Evaluate workflow_output assertions against computed workflow outputs.\\n+ * Similar to evaluateOutputs but tests workflow-level outputs (defined in outputs: section)\\n+ * rather than step outputs.\\n+ */\\n+export function evaluateWorkflowOutputs(\\n+  errors: string[],\\n+  expect: ExpectBlock,\\n+  workflowOutputs: Record<string, unknown> | undefined\\n+): void {\\n+  const expectations = (expect as any).workflow_output;\\n+  if (!Array.isArray(expectations) || expectations.length === 0) return;\\n+  if (!workflowOutputs) {\\n+    errors.push('workflow_output assertions present but no workflow outputs computed');\\n+    return;\\n+  }\\n+\\n+  for (const o of expectations) {\\n+    const path = o.path as string;\\n+    if (!path) {\\n+      errors.push('workflow_output assertion missing path');\\n+      continue;\\n+    }\\n+    const v = deepGet(workflowOutputs, path);\\n+    if (o.equals !== undefined && !deepEqual(v, o.equals)) {\\n+      errors.push(\\n+        `Workflow output ${path} expected ${JSON.stringify(o.equals)} but got ${JSON.stringify(v)}`\\n+      );\\n+    }\\n+    if (o.equalsDeep !== undefined && !deepEqual(v, o.equalsDeep)) {\\n+      errors.push(`Workflow output ${path} deepEquals failed`);\\n+    }\\n+    if (o.matches && !parseRegex(o.matches).test(String(v))) {\\n+      errors.push(`Workflow output ${path} does not match ${o.matches}`);\\n+    }\\n+    if (o.contains) {\\n+      const contents = Array.isArray(o.contains) ? o.contains : [o.contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (!strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} expected to contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.not_contains) {\\n+      const contents = Array.isArray(o.not_contains) ? o.not_contains : [o.not_contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} should not contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.contains_unordered) {\\n+      if (!Array.isArray(v)) {\\n+        errors.push(`Workflow output ${path} not an array for contains_unordered`);\\n+      } else if (!containsUnordered(v as unknown[], o.contains_unordered)) {\\n+        errors.push(`Workflow output ${path} missing elements (unordered)`);\\n+      }\\n+    }\\n+  }\\n+}\\n+\\n export function evaluateOutputs(\\n   errors: string[],\\n   expect: ExpectBlock,\\n@@ -332,7 +395,8 @@ export function evaluateCase(\\n   strict: boolean,\\n   promptsByStep: Record<string, string[]>,\\n   _results: GroupedResults,\\n-  outputHistory: Record<string, unknown[]>\\n+  outputHistory: Record<string, unknown[]>,\\n+  workflowOutputs?: Record<string, unknown>\\n ): string[] {\\n   const errors: string[] = [];\\n   const executed = buildExecutedMap(stats);\\n@@ -350,5 +414,6 @@ export function evaluateCase(\\n   evaluateNoCalls(errors, expect, executed, recorder, slackRecorder);\\n   evaluatePrompts(errors, expect, promptsByStep);\\n   evaluateOutputs(errors, expect, outputHistory);\\n+  evaluateWorkflowOutputs(errors, expect, workflowOutputs);\\n   return errors;\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/index.ts\",\"additions\":112,\"deletions\":7,\"changes\":119,\"patch\":\"diff --git a/src/test-runner/index.ts b/src/test-runner/index.ts\\nindex 29916a16..4dcc3331 100644\\n--- a/src/test-runner/index.ts\\n+++ b/src/test-runner/index.ts\\n@@ -159,6 +159,7 @@ export async function runSuites(\\n   options: {\\n     only?: string;\\n     bail?: boolean;\\n+    noMocks?: boolean;\\n     maxParallelSuites?: number;\\n     maxParallel?: number;\\n     promptMaxChars?: number;\\n@@ -225,6 +226,7 @@ export async function runSuites(\\n       const r = await runner.runCases(fp, suite as TestSuite, {\\n         only: options.only,\\n         bail: options.bail,\\n+        noMocks: options.noMocks,\\n         maxParallel: options.maxParallel,\\n         promptMaxChars: options.promptMaxChars,\\n       });\\n@@ -285,7 +287,8 @@ export class VisorTestRunner {\\n     defaultPromptCap?: number,\\n     ghRec?: { error_code?: number; timeout_ms?: number },\\n     defaultIncludeTags?: string[] | undefined,\\n-    defaultExcludeTags?: string[] | undefined\\n+    defaultExcludeTags?: string[] | undefined,\\n+    noMocks?: boolean\\n   ): {\\n     name: string;\\n     strict: boolean;\\n@@ -399,9 +402,11 @@ export class VisorTestRunner {\\n               : info.prompt;\\n           prompts[k].push(p);\\n         },\\n-        mockForStep: (step: string) => mockMgr.get(step),\\n+        // In noMocks mode, always return undefined to let real providers execute\\n+        mockForStep: (step: string) => noMocks ? undefined : mockMgr.get(step),\\n         // Ensure human-input never blocks tests: prefer case mock, then default value\\n         onHumanInput: async (req: { checkId: string; default?: string }) => {\\n+          if (noMocks) return (req.default ?? '').toString();\\n           const m = mockMgr.get(req.checkId);\\n           if (m !== undefined && m !== null) return String(m);\\n           return (req.default ?? '').toString();\\n@@ -481,6 +486,60 @@ export class VisorTestRunner {\\n     if (!checks || checks.length === 0) return;\\n     console.log(`  checks: ${checks.join(', ')}`);\\n   }\\n+  /**\\n+   * Compute workflow outputs from output definitions and step results.\\n+   * Evaluates value_js and value (Liquid) expressions to produce output values.\\n+   */\\n+  private computeWorkflowOutputs(\\n+    outputDefs: Array<{ name: string; value?: string; value_js?: string }>,\\n+    outputHistory: Record<string, unknown[]>,\\n+    results: any[]\\n+  ): Record<string, unknown> {\\n+    const computed: Record<string, unknown> = {};\\n+    // Build a steps map from outputHistory (last output for each step)\\n+    const steps: Record<string, unknown> = {};\\n+    for (const [stepId, hist] of Object.entries(outputHistory)) {\\n+      if (Array.isArray(hist) && hist.length > 0) {\\n+        steps[stepId] = hist[hist.length - 1];\\n+      }\\n+    }\\n+    // Build outputs map from history (for {{ outputs[\\\"step\\\"] }} syntax)\\n+    const outputs: Record<string, unknown> = { ...steps };\\n+\\n+    for (const outputDef of outputDefs) {\\n+      if (!outputDef.name) continue;\\n+      try {\\n+        if (outputDef.value_js) {\\n+          // Evaluate JavaScript expression\\n+          const fn = new Function('steps', 'outputs', 'results', `\\n+            try {\\n+              ${outputDef.value_js}\\n+            } catch (e) {\\n+              return undefined;\\n+            }\\n+          `);\\n+          computed[outputDef.name] = fn(steps, outputs, results);\\n+        } else if (outputDef.value) {\\n+          // Evaluate Liquid template\\n+          const { Liquid } = require('liquidjs');\\n+          const engine = new Liquid();\\n+          const rendered = engine.parseAndRenderSync(outputDef.value, { steps, outputs, results });\\n+          // Try to parse as JSON, otherwise keep as string\\n+          try {\\n+            computed[outputDef.name] = JSON.parse(rendered);\\n+          } catch {\\n+            computed[outputDef.name] = rendered.trim();\\n+          }\\n+        }\\n+      } catch (e) {\\n+        // Skip outputs that fail to compute\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.log(`  ‚ö†Ô∏è Failed to compute output '${outputDef.name}': ${e}`);\\n+        }\\n+      }\\n+    }\\n+    return computed;\\n+  }\\n   /**\\n    * Locate a tests file: explicit path > ./.visor.tests.yaml > defaults/visor.tests.yaml\\n    */\\n@@ -606,7 +665,7 @@ export class VisorTestRunner {\\n   public async runCases(\\n     testsPath: string,\\n     suite: TestSuite,\\n-    options: { only?: string; bail?: boolean; maxParallel?: number; promptMaxChars?: number }\\n+    options: { only?: string; bail?: boolean; noMocks?: boolean; maxParallel?: number; promptMaxChars?: number }\\n   ): Promise<{\\n     failures: number;\\n     results: Array<{\\n@@ -748,9 +807,14 @@ export class VisorTestRunner {\\n     const __keepAlive = setInterval(() => {}, 1000);\\n     // Header: show suite path for clarity\\n     let __suiteRel = testsPath;\\n+    const noMocksMode = options.noMocks || false;\\n     try {\\n       __suiteRel = path.relative(this.cwd, testsPath) || testsPath;\\n       console.log(`Suite: ${__suiteRel}`);\\n+      if (noMocksMode) {\\n+        console.log(this.color('üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)', '33'));\\n+        console.log(this.gray('   Step outputs will be captured and printed as suggested mocks\\\\n'));\\n+      }\\n     } catch {}\\n     const runOne = async (_case: any): Promise<{ name: string; failed: number }> => {\\n       // Case header for clarity\\n@@ -805,6 +869,11 @@ export class VisorTestRunner {\\n           cfgLocal.checks[name] = chk;\\n         }\\n       }\\n+      // Workflow testing: inject workflow_input into config for template access\\n+      const workflowInput = (_case as any).workflow_input;\\n+      if (workflowInput && typeof workflowInput === 'object') {\\n+        (cfgLocal as any).workflow_inputs = workflowInput;\\n+      }\\n       const setup = this.setupTestCase(\\n         _case,\\n         cfgLocal,\\n@@ -812,7 +881,8 @@ export class VisorTestRunner {\\n         defaultPromptCap,\\n         ghRec,\\n         defaultIncludeTags,\\n-        defaultExcludeTags\\n+        defaultExcludeTags,\\n+        noMocksMode\\n       );\\n       try {\\n         this.printSelectedChecks(setup.checksToRun);\\n@@ -826,6 +896,24 @@ export class VisorTestRunner {\\n         } catch {}\\n         const exec = await this.executeTestCase(setup, cfgLocal);\\n         const res = exec.res;\\n+\\n+        // In no-mocks mode, print captured outputs as suggested mocks\\n+        if (noMocksMode && Object.keys(exec.outHistory).length > 0) {\\n+          console.log(this.color('\\\\nüìã Suggested mocks (copy to your test case):', '36'));\\n+          console.log(this.gray('mocks:'));\\n+          for (const [stepName, outputs] of Object.entries(exec.outHistory)) {\\n+            if (!Array.isArray(outputs) || outputs.length === 0) continue;\\n+            // Use the last output for the step\\n+            const lastOutput = outputs[outputs.length - 1];\\n+            // Format as YAML with proper indentation\\n+            const yamlOutput = yaml.dump({ [stepName]: lastOutput }, { indent: 2, lineWidth: 120, noRefs: true });\\n+            // Indent the YAML output for proper nesting under 'mocks:'\\n+            const indented = yamlOutput.split('\\\\n').map(line => '  ' + line).join('\\\\n');\\n+            console.log(indented);\\n+          }\\n+          console.log('');\\n+        }\\n+\\n         if (process.env.VISOR_DEBUG === 'true') {\\n           try {\\n             const names = (res.statistics.checks || []).map(\\n@@ -836,6 +924,21 @@ export class VisorTestRunner {\\n         }\\n         // avoid printing raw history keys each case\\n         // (fallback for on_finish static targets handled inside executeTestCase)\\n+        // Workflow testing: compute workflow outputs if workflow has outputs defined\\n+        let workflowOutputs: Record<string, unknown> | undefined;\\n+        if (Array.isArray((cfgLocal as any).outputs)) {\\n+          try {\\n+            workflowOutputs = this.computeWorkflowOutputs(\\n+              (cfgLocal as any).outputs,\\n+              exec.outHistory,\\n+              res.results\\n+            );\\n+          } catch (e) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ‚ö†Ô∏è Error computing workflow outputs: ${e}`);\\n+            }\\n+          }\\n+        }\\n         const caseFailures = require('./evaluators').evaluateCase(\\n           _case.name,\\n           res.statistics,\\n@@ -845,7 +948,8 @@ export class VisorTestRunner {\\n           setup.strict,\\n           setup.prompts,\\n           res.results,\\n-          exec.outHistory\\n+          exec.outHistory,\\n+          workflowOutputs\\n         );\\n         // Warn about unmocked AI/command steps that executed\\n         try {\\n@@ -1280,7 +1384,8 @@ export class VisorTestRunner {\\n   ): void {\\n     const executed: Record<string, number> = {};\\n     for (const s of stats.checks) {\\n-      if (!s.skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n+      const skipped = (s as any).skipped === true || !!(s as any).skipReason;\\n+      if (!skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n     }\\n     const expCalls = (expect.calls || []).filter(c => c.step);\\n     const expectedSteps = new Map<\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/validator.ts\",\"additions\":48,\"deletions\":2,\"changes\":50,\"patch\":\"diff --git a/src/test-runner/validator.ts b/src/test-runner/validator.ts\\nindex dbb890c5..5ffff5ee 100644\\n--- a/src/test-runner/validator.ts\\n+++ b/src/test-runner/validator.ts\\n@@ -25,6 +25,16 @@ const schema: any = {\\n     hooks: { type: 'object' },\\n     slack: { type: 'object' },\\n     frontends: { type: 'array' },\\n+    workspace: { type: 'object' },\\n+    // Workflow definition fields (for workflow files with co-located tests)\\n+    id: { type: 'string' },\\n+    name: { type: 'string' },\\n+    description: { type: 'string' },\\n+    inputs: { type: 'array' },\\n+    outputs: { type: 'array' },\\n+    ai_mcp_servers: { type: 'object' },\\n+    ai_provider: { type: 'string' },\\n+    ai_model: { type: 'string' },\\n     tests: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -136,6 +146,8 @@ const schema: any = {\\n             oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n           },\\n         },\\n+        // Workflow testing: input values to pass to the workflow\\n+        workflow_input: { type: 'object' },\\n         expect: { $ref: '#/$defs/expectBlock' },\\n         // Flow cases\\n         flow: {\\n@@ -270,6 +282,35 @@ const schema: any = {\\n       },\\n       required: ['step', 'path'],\\n     },\\n+    // Workflow output expectation: like outputsExpectation but without step (tests workflow-level outputs)\\n+    workflowOutputExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        path: { type: 'string' },\\n+        equals: {},\\n+        equalsDeep: {},\\n+        matches: { type: 'string' },\\n+        contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        not_contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            path: { type: 'string' },\\n+            equals: {},\\n+            matches: { type: 'string' },\\n+          },\\n+          required: ['path'],\\n+        },\\n+        contains_unordered: { type: 'array' },\\n+      },\\n+      required: ['path'],\\n+    },\\n     expectBlock: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -278,6 +319,8 @@ const schema: any = {\\n         calls: { type: 'array', items: { $ref: '#/$defs/callsExpectation' } },\\n         prompts: { type: 'array', items: { $ref: '#/$defs/promptsExpectation' } },\\n         outputs: { type: 'array', items: { $ref: '#/$defs/outputsExpectation' } },\\n+        // Workflow testing: assert on workflow-level outputs (defined in workflow's outputs: section)\\n+        workflow_output: { type: 'array', items: { $ref: '#/$defs/workflowOutputExpectation' } },\\n         no_calls: {\\n           type: 'array',\\n           items: {\\n@@ -343,6 +386,7 @@ const knownKeys = new Set([\\n   'version',\\n   'extends',\\n   'tests',\\n+  'workspace',\\n   // tests\\n   'tests.defaults',\\n   'tests.fixtures',\\n@@ -363,6 +407,7 @@ const knownKeys = new Set([\\n   'env',\\n   'routing',\\n   'mocks',\\n+  'workflow_input',\\n   'expect',\\n   'flow',\\n   // expect\\n@@ -370,6 +415,7 @@ const knownKeys = new Set([\\n   'expect.calls',\\n   'expect.prompts',\\n   'expect.outputs',\\n+  'expect.workflow_output',\\n   'expect.no_calls',\\n   'expect.fail',\\n   'expect.strict_violation',\\n@@ -424,7 +470,7 @@ function formatError(e: ErrorObject): string {\\n       if (hint) msg += ` (${hint})`;\\n       // Small curated allow-list for frequent nodes to reduce guesswork\\n       if (path.endsWith('expect')) {\\n-        msg += ` (allowed: use, calls, prompts, outputs, no_calls, fail, strict_violation)`;\\n+        msg += ` (allowed: use, calls, prompts, outputs, workflow_output, no_calls, fail, strict_violation)`;\\n       } else if (path.endsWith('env')) {\\n         msg += ` (values must be strings)`;\\n       } else if (path.endsWith('tests')) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/cli.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/cli.ts b/src/types/cli.ts\\nindex a7d9ca30..f06f93d5 100644\\n--- a/src/types/cli.ts\\n+++ b/src/types/cli.ts\\n@@ -61,6 +61,8 @@ export interface CliOptions {\\n   githubV2?: boolean;\\n   /** Enable Slack Socket Mode runner */\\n   slack?: boolean;\\n+  /** Keep workspace folders after execution (for debugging) */\\n+  keepWorkspace?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/config.ts\",\"additions\":114,\"deletions\":5,\"changes\":119,\"patch\":\"diff --git a/src/types/config.ts b/src/types/config.ts\\nindex ed239959..dd363de1 100644\\n--- a/src/types/config.ts\\n+++ b/src/types/config.ts\\n@@ -122,6 +122,9 @@ export interface FailureConditionContext {\\n   /** Environment variables */\\n   env?: Record<string, string>;\\n \\n+  /** Workflow inputs - values passed to the workflow */\\n+  inputs?: Record<string, unknown>;\\n+\\n   /** Memory accessor for accessing memory store in expressions */\\n   memory?: {\\n     get: (key: string, namespace?: string) => unknown;\\n@@ -356,6 +359,8 @@ export interface AIProviderConfig {\\n   allowBash?: boolean;\\n   /** Advanced bash command execution configuration */\\n   bashConfig?: BashConfig;\\n+  /** Completion prompt for post-completion validation/review (runs after attempt_completion) */\\n+  completion_prompt?: string;\\n }\\n \\n /**\\n@@ -461,8 +466,8 @@ export interface CheckConfig {\\n   env?: EnvConfig;\\n   /** Timeout in seconds for command execution (default: 60) */\\n   timeout?: number;\\n-  /** Check IDs that this check depends on (optional) */\\n-  depends_on?: string[];\\n+  /** Check IDs that this check depends on (optional). Accepts single string or array. */\\n+  depends_on?: string | string[];\\n   /** Group name for comment separation (e.g., \\\"code-review\\\", \\\"pr-overview\\\") - optional */\\n   group?: string;\\n   /** Schema type for template rendering (e.g., \\\"code-review\\\", \\\"markdown\\\") or inline JSON schema object - optional */\\n@@ -621,6 +626,46 @@ export interface CheckConfig {\\n   overrides?: Record<string, Partial<CheckConfig>>;\\n   /** Map workflow outputs to check outputs */\\n   output_mapping?: Record<string, string>;\\n+\\n+  // Workflow aliases for backward compatibility\\n+  /** Alias for args - workflow inputs (backward compatibility) */\\n+  workflow_inputs?: Record<string, unknown>;\\n+  /** Config file path - alternative to workflow ID (loads a Visor config file as workflow) */\\n+  config?: string;\\n+  /** Alias for overrides - workflow step overrides (backward compatibility) */\\n+  workflow_overrides?: Record<string, Partial<CheckConfig>>;\\n+\\n+  /**\\n+   * Git-checkout provider specific options (optional, only used when type === 'git-checkout').\\n+   */\\n+  /** Git reference to checkout (branch, tag, commit SHA) - supports templates */\\n+  ref?: string;\\n+  /** Repository URL or owner/repo format (defaults to current repository) */\\n+  repository?: string;\\n+  /** GitHub token for private repositories (defaults to GITHUB_TOKEN env) */\\n+  token?: string;\\n+  /** Number of commits to fetch (0 for full history, default: 1) */\\n+  fetch_depth?: number;\\n+  /** Whether to fetch tags (default: false) */\\n+  fetch_tags?: boolean;\\n+  /** Checkout submodules: false, true, or 'recursive' */\\n+  submodules?: boolean | 'recursive';\\n+  /** Working directory for the checkout (defaults to temp directory) */\\n+  working_directory?: string;\\n+  /** Use git worktree for efficient parallel checkouts (default: true) */\\n+  use_worktree?: boolean;\\n+  /** Clean the working directory before checkout (default: true) */\\n+  clean?: boolean;\\n+  /** Sparse checkout paths - only checkout specific directories/files */\\n+  sparse_checkout?: string[];\\n+  /** Enable Git LFS (Large File Storage) */\\n+  lfs?: boolean;\\n+  /** Timeout in ms for cloning the bare repository (default: 300000 = 5 min) */\\n+  clone_timeout_ms?: number;\\n+  /** Clean up worktree on failure (default: true) */\\n+  cleanup_on_failure?: boolean;\\n+  /** Keep worktree after workflow completion (default: false) */\\n+  persist_worktree?: boolean;\\n }\\n \\n /**\\n@@ -1037,6 +1082,36 @@ export interface CustomToolDefinition {\\n   outputSchema?: Record<string, unknown>;\\n }\\n \\n+/**\\n+ * Workflow input definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowInput {\\n+  /** Input parameter name */\\n+  name: string;\\n+  /** JSON Schema for the input */\\n+  schema?: Record<string, unknown>;\\n+  /** Whether this input is required */\\n+  required?: boolean;\\n+  /** Default value if not provided */\\n+  default?: unknown;\\n+  /** Human-readable description */\\n+  description?: string;\\n+}\\n+\\n+/**\\n+ * Workflow output definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowOutput {\\n+  /** Output name */\\n+  name: string;\\n+  /** Human-readable description */\\n+  description?: string;\\n+  /** Value using Liquid template syntax (references step outputs) */\\n+  value?: string;\\n+  /** Value using JavaScript expression (alternative to value) */\\n+  value_js?: string;\\n+}\\n+\\n /**\\n  * Main Visor configuration\\n  */\\n@@ -1051,12 +1126,16 @@ export interface VisorConfig {\\n   tools?: Record<string, CustomToolDefinition>;\\n   /** Import workflow definitions from external files or URLs */\\n   imports?: string[];\\n+  /** Workflow inputs (for standalone reusable workflows) */\\n+  inputs?: WorkflowInput[];\\n+  /** Workflow outputs (for standalone reusable workflows) */\\n+  outputs?: WorkflowOutput[];\\n   /** Step configurations (recommended) */\\n   steps?: Record<string, CheckConfig>;\\n   /** Check configurations (legacy, use 'steps' instead) - always populated after normalization */\\n   checks?: Record<string, CheckConfig>;\\n-  /** Output configuration */\\n-  output: OutputConfig;\\n+  /** Output configuration (optional - defaults provided) */\\n+  output?: OutputConfig;\\n   /** HTTP server configuration for receiving webhooks */\\n   http_server?: HttpServerConfig;\\n   /** Memory storage configuration */\\n@@ -1092,6 +1171,36 @@ export interface VisorConfig {\\n     /** Frontend-specific configuration */\\n     config?: unknown;\\n   }>;\\n+  /** Workspace isolation configuration for sandboxed execution */\\n+  workspace?: WorkspaceConfig;\\n+  /** Slack configuration */\\n+  slack?: SlackConfig;\\n+}\\n+\\n+/**\\n+ * Workspace isolation configuration\\n+ */\\n+export interface WorkspaceConfig {\\n+  /** Enable workspace isolation (default: true when config present) */\\n+  enabled?: boolean;\\n+  /** Base path for workspaces (default: /tmp/visor-workspaces) */\\n+  base_path?: string;\\n+  /** Clean up workspace on exit (default: true) */\\n+  cleanup_on_exit?: boolean;\\n+}\\n+\\n+/**\\n+ * Slack configuration\\n+ */\\n+export interface SlackConfig {\\n+  /** Slack API version */\\n+  version?: string;\\n+  /** Mention handling: 'all', 'direct', etc. */\\n+  mentions?: string;\\n+  /** Thread handling: 'required', 'optional', etc. */\\n+  threads?: string;\\n+  /** Show raw output in Slack responses */\\n+  show_raw_output?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/git-checkout.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/git-checkout.ts b/src/types/git-checkout.ts\\nindex eb1c5ebc..4d27855d 100644\\n--- a/src/types/git-checkout.ts\\n+++ b/src/types/git-checkout.ts\\n@@ -16,6 +16,8 @@ export interface GitCheckoutConfig {\\n   fetch_depth?: number;\\n   fetch_tags?: boolean;\\n   submodules?: boolean | 'recursive';\\n+  /** Timeout (ms) for cloning the bare repository; defaults to 300000 (5 minutes) */\\n+  clone_timeout_ms?: number;\\n \\n   // Worktree configuration\\n   working_directory?: string;\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/sandbox.ts\",\"additions\":46,\"deletions\":1,\"changes\":47,\"patch\":\"diff --git a/src/utils/sandbox.ts b/src/utils/sandbox.ts\\nindex 9914a3a5..12e27c30 100644\\n--- a/src/utils/sandbox.ts\\n+++ b/src/utils/sandbox.ts\\n@@ -14,6 +14,51 @@ export interface CompileOptions {\\n   wrapFunction?: boolean;\\n }\\n \\n+export interface JsSyntaxValidationResult {\\n+  valid: boolean;\\n+  error?: string;\\n+}\\n+\\n+/**\\n+ * Validate JavaScript syntax without executing it.\\n+ * Uses the sandbox's compile method to check for syntax errors.\\n+ * Returns validation result with error message if invalid.\\n+ */\\n+export function validateJsSyntax(code: string): JsSyntaxValidationResult {\\n+  if (!code || typeof code !== 'string') {\\n+    return { valid: false, error: 'Code must be a non-empty string' };\\n+  }\\n+\\n+  const trimmed = code.trim();\\n+  if (trimmed.length === 0) {\\n+    return { valid: false, error: 'Code cannot be empty' };\\n+  }\\n+\\n+  // Create a minimal sandbox instance for syntax checking\\n+  const sandbox = createSecureSandbox();\\n+\\n+  // Wrap code similar to compileAndRun to catch the same syntax issues\\n+  const looksLikeBlock = /\\\\breturn\\\\b/.test(trimmed) || /;/.test(trimmed) || /\\\\n/.test(trimmed);\\n+  const looksLikeIife = /\\\\)\\\\s*\\\\(\\\\s*\\\\)\\\\s*;?$/.test(trimmed);\\n+  const body = looksLikeBlock\\n+    ? looksLikeIife\\n+      ? `return (\\\\n${trimmed}\\\\n);\\\\n`\\n+      : `return (() => {\\\\n${trimmed}\\\\n})();\\\\n`\\n+    : `return (\\\\n${trimmed}\\\\n);\\\\n`;\\n+\\n+  // Add log injection header (same as compileAndRun)\\n+  const header = `const __lp = \\\"[syntax-check]\\\"; const log = (...a) => { try { console.log(__lp, ...a); } catch {} };\\\\n`;\\n+  const fullCode = `${header}${body}`;\\n+\\n+  try {\\n+    sandbox.compile(fullCode);\\n+    return { valid: true };\\n+  } catch (e) {\\n+    const msg = e instanceof Error ? e.message : String(e);\\n+    return { valid: false, error: msg };\\n+  }\\n+}\\n+\\n /**\\n  * Create a hardened Sandbox with a consistent set of globals and prototype\\n  * whitelists. This is a superset of the sets previously used by individual\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/tracer-init.ts\",\"additions\":19,\"deletions\":1,\"changes\":20,\"patch\":\"diff --git a/src/utils/tracer-init.ts b/src/utils/tracer-init.ts\\nindex b42995d9..4edf1ffc 100644\\n--- a/src/utils/tracer-init.ts\\n+++ b/src/utils/tracer-init.ts\\n@@ -76,6 +76,24 @@ export async function initializeTracer(\\n \\n       const tracer = new SimpleAppTracer(telemetry, sessionId);\\n \\n+      // WORKAROUND: Add missing recordEvent method for completionPrompt feature (probe #321)\\n+      // SimpleAppTracer doesn't have recordEvent but completionPrompt requires it\\n+      if (typeof (tracer as any).recordEvent !== 'function') {\\n+        (tracer as any).recordEvent = (\\n+          name: string,\\n+          attributes?: Record<string, unknown>\\n+        ) => {\\n+          // Log completion events to telemetry for debugging\\n+          try {\\n+            if ((telemetry as any).record) {\\n+              (telemetry as any).record({ event: name, ...attributes });\\n+            }\\n+          } catch {\\n+            // Best-effort only\\n+          }\\n+        };\\n+      }\\n+\\n       console.error(`üìä Simple tracing enabled, will save to: ${traceFilePath}`);\\n \\n       // If in GitHub Actions, log the path for artifact upload\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/worktree-manager.ts\",\"additions\":142,\"deletions\":27,\"changes\":169,\"patch\":\"diff --git a/src/utils/worktree-manager.ts b/src/utils/worktree-manager.ts\\nindex f3cebf3d..2de7a9f4 100644\\n--- a/src/utils/worktree-manager.ts\\n+++ b/src/utils/worktree-manager.ts\\n@@ -6,6 +6,7 @@\\n  */\\n \\n import * as fs from 'fs';\\n+import * as fsp from 'fs/promises';\\n import * as path from 'path';\\n import * as crypto from 'crypto';\\n import { commandExecutor } from './command-executor';\\n@@ -119,7 +120,8 @@ export class WorktreeManager {\\n     repository: string,\\n     repoUrl: string,\\n     token?: string,\\n-    fetchDepth?: number\\n+    fetchDepth?: number,\\n+    cloneTimeoutMs?: number\\n   ): Promise<string> {\\n     const reposDir = this.getReposDir();\\n     const repoName = repository.replace(/\\\\//g, '-');\\n@@ -129,9 +131,21 @@ export class WorktreeManager {\\n     if (fs.existsSync(bareRepoPath)) {\\n       logger.debug(`Bare repository already exists: ${bareRepoPath}`);\\n \\n-      // Update remote refs\\n-      await this.updateBareRepo(bareRepoPath);\\n-      return bareRepoPath;\\n+      // Verify the bare repo has the correct remote URL to prevent using corrupted repos\\n+      const verifyResult = await this.verifyBareRepoRemote(bareRepoPath, repoUrl);\\n+      if (verifyResult === 'timeout') {\\n+        // Timeout during verification - use stale cache to avoid hanging on re-clone\\n+        logger.info(`Using stale bare repository (verification timed out): ${bareRepoPath}`);\\n+        return bareRepoPath;\\n+      } else if (verifyResult === false) {\\n+        logger.warn(`Bare repository at ${bareRepoPath} has incorrect remote, removing and re-cloning`);\\n+        await fsp.rm(bareRepoPath, { recursive: true, force: true });\\n+        // Fall through to clone below\\n+      } else {\\n+        // Update remote refs\\n+        await this.updateBareRepo(bareRepoPath);\\n+        return bareRepoPath;\\n+      }\\n     }\\n \\n     // Clone as bare repository\\n@@ -153,7 +167,9 @@ export class WorktreeManager {\\n     }\\n     cloneCmd += ` ${this.escapeShellArg(cloneUrl)} ${this.escapeShellArg(bareRepoPath)}`;\\n \\n-    const result = await this.executeGitCommand(cloneCmd, { timeout: 300000 }); // 5 minute timeout\\n+    const result = await this.executeGitCommand(cloneCmd, {\\n+      timeout: cloneTimeoutMs || 300000, // default 5 minutes\\n+    });\\n \\n     if (result.exitCode !== 0) {\\n       // Redact tokens from error messages\\n@@ -171,19 +187,98 @@ export class WorktreeManager {\\n   private async updateBareRepo(bareRepoPath: string): Promise<void> {\\n     logger.debug(`Updating bare repository: ${bareRepoPath}`);\\n \\n-    const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n-    const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+    try {\\n+      const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n+      const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n+        // Don't throw - we can continue with stale refs\\n+      } else {\\n+        logger.debug(`Successfully updated bare repository`);\\n+      }\\n+    } catch (error) {\\n+      // Handle timeout or other errors gracefully - we can continue with stale refs\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      logger.warn(`Failed to update bare repository (will use stale refs): ${errorMessage}`);\\n+    }\\n+  }\\n \\n-    if (result.exitCode !== 0) {\\n-      logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n-      // Don't throw - we can continue with stale refs\\n-    } else {\\n-      logger.debug(`Successfully updated bare repository`);\\n+  /**\\n+   * Verify that a bare repository has the correct remote URL.\\n+   * This prevents reusing corrupted repos that were cloned from a different repository.\\n+   * Returns: true (valid), false (invalid - should re-clone), or 'timeout' (use stale cache)\\n+   */\\n+  private async verifyBareRepoRemote(\\n+    bareRepoPath: string,\\n+    expectedUrl: string\\n+  ): Promise<boolean | 'timeout'> {\\n+    try {\\n+      const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote get-url origin`;\\n+      const result = await this.executeGitCommand(cmd, { timeout: 10000 });\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to get remote URL for ${bareRepoPath}: ${result.stderr}`);\\n+        return false;\\n+      }\\n+\\n+      const actualUrl = result.stdout.trim();\\n+\\n+      // Normalize URLs for comparison:\\n+      // - remove credentials (tokens/username)\\n+      // - remove .git suffix if present\\n+      // - trim trailing slash\\n+      // - lowercase for case‚Äëinsensitive match\\n+      const normalizeUrl = (url: string): string => {\\n+        // Convert common SSH form to https for comparison\\n+        if (url.startsWith('git@github.com:')) {\\n+          url = url.replace('git@github.com:', 'https://github.com/');\\n+        }\\n+        return url\\n+          // strip userinfo part to avoid mismatches when the cached bare\\n+          // repo was cloned with an access token but the expected URL is\\n+          // tokenless (or vice‚Äëversa)\\n+          .replace(/:\\\\/\\\\/[^@]+@/, '://')\\n+          .replace(/\\\\.git$/, '')\\n+          .replace(/\\\\/$/, '')\\n+          .toLowerCase();\\n+      };\\n+\\n+      const normalizedExpected = normalizeUrl(expectedUrl);\\n+      const normalizedActual = normalizeUrl(actualUrl);\\n+\\n+      if (normalizedExpected !== normalizedActual) {\\n+        logger.warn(\\n+          `Bare repo remote mismatch: expected ${expectedUrl}, got ${actualUrl}`\\n+        );\\n+        return false;\\n+      }\\n+\\n+      logger.debug(`Bare repo remote verified: ${actualUrl}`);\\n+      return true;\\n+    } catch (error) {\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      // If it's a timeout, return 'timeout' so caller can use stale cache instead of re-cloning\\n+      if (errorMessage.includes('timed out')) {\\n+        logger.warn(`Timeout verifying bare repo remote (will use stale cache): ${errorMessage}`);\\n+        return 'timeout';\\n+      }\\n+      logger.warn(`Error verifying bare repo remote: ${error}`);\\n+      return false;\\n     }\\n   }\\n \\n   /**\\n-   * Create a new worktree\\n+   * Create a new worktree for the given repository/ref.\\n+   *\\n+   * Important: we always create worktrees in a detached HEAD state pinned\\n+   * to a specific commit SHA rather than a named branch like \\\"main\\\". Git\\n+   * only allows a branch to be checked out in a single worktree at a time;\\n+   * using the raw commit (plus --detach) lets multiple workflows safely\\n+   * create independent worktrees for the same branch without hitting\\n+   * errors like:\\n+   *\\n+   *   fatal: 'main' is already used by worktree at '.../TykTechnologies-tyk-docs-main-XXXX'\\n    */\\n   async createWorktree(\\n     repository: string,\\n@@ -195,6 +290,7 @@ export class WorktreeManager {\\n       clean?: boolean;\\n       workflowId?: string;\\n       fetchDepth?: number;\\n+      cloneTimeoutMs?: number;\\n     } = {}\\n   ): Promise<WorktreeInfo> {\\n     // Validate ref to prevent command injection\\n@@ -205,7 +301,8 @@ export class WorktreeManager {\\n       repository,\\n       repoUrl,\\n       options.token,\\n-      options.fetchDepth\\n+      options.fetchDepth,\\n+      options.cloneTimeoutMs\\n     );\\n \\n     // Generate worktree ID and path\\n@@ -241,21 +338,24 @@ export class WorktreeManager {\\n       }\\n     }\\n \\n-    // Fetch the ref if needed\\n+    // Fetch the ref if needed, then resolve it to a concrete commit SHA.\\n+    // We use the commit (detached HEAD) instead of the branch name so we\\n+    // can have multiple worktrees for the same branch without git refusing\\n+    // with \\\"branch X is already checked out\\\".\\n     await this.fetchRef(bareRepoPath, ref);\\n+    const commit = await this.getCommitShaForRef(bareRepoPath, ref);\\n \\n-    // Create worktree\\n-    logger.info(`Creating worktree for ${repository}@${ref}`);\\n-    const createCmd = `git -C ${this.escapeShellArg(bareRepoPath)} worktree add ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(ref)}`;\\n+    // Create worktree in detached HEAD state at the resolved commit\\n+    logger.info(`Creating worktree for ${repository}@${ref} (${commit})`);\\n+    const createCmd = `git -C ${this.escapeShellArg(\\n+      bareRepoPath\\n+    )} worktree add --detach ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(commit)}`;\\n     const result = await this.executeGitCommand(createCmd, { timeout: 60000 });\\n \\n     if (result.exitCode !== 0) {\\n       throw new Error(`Failed to create worktree: ${result.stderr}`);\\n     }\\n \\n-    // Get commit SHA\\n-    const commit = await this.getCommitSha(worktreePath);\\n-\\n     // Create metadata\\n     const metadata: WorktreeMetadata = {\\n       worktree_id: worktreeId,\\n@@ -316,14 +416,17 @@ export class WorktreeManager {\\n   }\\n \\n   /**\\n-   * Get commit SHA for worktree\\n+   * Get commit SHA for a given ref inside a bare repository.\\n+   *\\n+   * This runs after fetchRef so that <ref> should resolve to either a\\n+   * local branch, tag, or remote-tracking ref.\\n    */\\n-  private async getCommitSha(worktreePath: string): Promise<string> {\\n-    const cmd = `git -C ${this.escapeShellArg(worktreePath)} rev-parse HEAD`;\\n+  private async getCommitShaForRef(bareRepoPath: string, ref: string): Promise<string> {\\n+    const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} rev-parse ${this.escapeShellArg(ref)}`;\\n     const result = await this.executeGitCommand(cmd);\\n \\n     if (result.exitCode !== 0) {\\n-      throw new Error(`Failed to get commit SHA: ${result.stderr}`);\\n+      throw new Error(`Failed to get commit SHA for ref ${ref}: ${result.stderr}`);\\n     }\\n \\n     return result.stdout.trim();\\n@@ -621,9 +724,21 @@ export class WorktreeManager {\\n     command: string,\\n     options: { timeout?: number; env?: Record<string, string> } = {}\\n   ): Promise<GitCommandResult> {\\n+    // Merge provided env with process.env and add git-specific settings\\n+    // These settings prevent git from hanging on interactive prompts while\\n+    // still allowing OS-level credential helpers to work:\\n+    // - GIT_TERMINAL_PROMPT=0: Prevents terminal credential prompts\\n+    // - GIT_SSH_COMMAND: Disables SSH password prompts (BatchMode)\\n+    const gitEnv = {\\n+      ...process.env,\\n+      ...options.env,\\n+      GIT_TERMINAL_PROMPT: '0',\\n+      GIT_SSH_COMMAND: 'ssh -o BatchMode=yes -o StrictHostKeyChecking=no',\\n+    };\\n+\\n     const result = await commandExecutor.execute(command, {\\n       timeout: options.timeout || 30000,\\n-      env: options.env || process.env,\\n+      env: gitEnv,\\n     } as any);\\n \\n     return {\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-executor.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-executor.ts b/src/workflow-executor.ts\\nindex 751e7f16..df76762f 100644\\n--- a/src/workflow-executor.ts\\n+++ b/src/workflow-executor.ts\\n@@ -219,7 +219,9 @@ export class WorkflowExecutor {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps)) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     // Use static DependencyResolver\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-registry.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-registry.ts b/src/workflow-registry.ts\\nindex 2cbe3a89..6bd855fb 100644\\n--- a/src/workflow-registry.ts\\n+++ b/src/workflow-registry.ts\\n@@ -410,7 +410,9 @@ export class WorkflowRegistry {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps || {})) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     try {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/ai-review-service.test.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/tests/unit/ai-review-service.test.ts b/tests/unit/ai-review-service.test.ts\\nindex ec2b998a..2c616c99 100644\\n--- a/tests/unit/ai-review-service.test.ts\\n+++ b/tests/unit/ai-review-service.test.ts\\n@@ -482,6 +482,37 @@ describe('AIReviewService', () => {\\n       expect(context).toContain('<additions>10</additions>');\\n       expect(context).toContain('<deletions>5</deletions>');\\n     });\\n+\\n+    it('should exclude diffs by default when in Slack mode', async () => {\\n+      // Slack mode is detected by presence of slackConversation property\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // In Slack mode, diffs should be excluded by default\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+      // Should still include file summary\\n+      expect(context).toContain('<files_summary>');\\n+    });\\n+\\n+    it('should include diffs in Slack mode when explicitly requested', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      (mockPRInfo as any).includeCodeContext = true;\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // With explicit includeCodeContext: true, diffs should be included\\n+      expect(context).toContain('<full_diff>');\\n+      expect(context).toContain('--- test.ts');\\n+    });\\n+\\n+    it('should exclude diffs in Slack mode even when includeCodeContext is undefined', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      // Deliberately not setting includeCodeContext to test default behavior\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+    });\\n   });\\n \\n   describe('Comment Filtering for Code Review', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config-extends.test.ts\",\"additions\":8,\"deletions\":8,\"changes\":16,\"patch\":\"diff --git a/tests/unit/config-extends.test.ts b/tests/unit/config-extends.test.ts\\nindex b112a869..7ee69ce6 100644\\n--- a/tests/unit/config-extends.test.ts\\n+++ b/tests/unit/config-extends.test.ts\\n@@ -564,7 +564,7 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.security).toBeDefined();\\n       expect(config.checks!.performance).toBeDefined();\\n       expect(config.checks!.custom).toBeDefined();\\n-      expect(config.output.pr_comment.format).toBe('json');\\n+      expect(config.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should extend from default configuration', async () => {\\n@@ -1002,7 +1002,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.base).toBeDefined(); // From base\\n       expect(loaded.checks!.middle).toBeDefined(); // From middle\\n       expect(loaded.checks!.top).toBeDefined(); // From top\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From top\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From top\\n     });\\n \\n     it('should handle mixed local and remote extends', async () => {\\n@@ -1079,7 +1079,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.local).toBeDefined();\\n       expect(loaded.checks!.remote).toBeDefined();\\n       expect(loaded.checks!.child).toBeDefined();\\n-      expect(loaded.output.pr_comment.format).toBe('json');\\n+      expect(loaded.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should handle extends with relative path resolution', async () => {\\n@@ -1593,9 +1593,9 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.performance).toBeDefined(); // Added in child\\n \\n       // Verify output\\n-      expect(config.output.pr_comment.format).toBe('json'); // Overridden\\n-      expect(config.output.pr_comment.group_by).toBe('file'); // Overridden\\n-      expect(config.output.pr_comment.collapse).toBe(false); // Overridden\\n+      expect(config.output!.pr_comment.format).toBe('json'); // Overridden\\n+      expect(config.output!.pr_comment.group_by).toBe('file'); // Overridden\\n+      expect(config.output!.pr_comment.collapse).toBe(false); // Overridden\\n     });\\n \\n     it('should handle configuration with disabled checks removal', async () => {\\n@@ -1814,7 +1814,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.root).toBeDefined(); // From root\\n       expect(loaded.checks!.level1).toBeDefined(); // From level1\\n       expect(loaded.checks!.level2).toBeDefined(); // From level2\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From level2\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From level2\\n     });\\n \\n     it('should handle mixed absolute and relative paths in extends array', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config.test.ts\",\"additions\":7,\"deletions\":7,\"changes\":14,\"patch\":\"diff --git a/tests/unit/config.test.ts b/tests/unit/config.test.ts\\nindex 67ce61fd..0677a3d4 100644\\n--- a/tests/unit/config.test.ts\\n+++ b/tests/unit/config.test.ts\\n@@ -48,7 +48,7 @@ output:\\n       expect(config.version).toBe('1.0');\\n       expect(config.checks).toHaveProperty('performance');\\n       expect(config.checks).toHaveProperty('security');\\n-      expect(config.output.pr_comment.format).toBe('table');\\n+      expect(config.output!.pr_comment.format).toBe('table');\\n     });\\n \\n     it('should handle missing config file gracefully', async () => {\\n@@ -264,9 +264,9 @@ checks:\\n       const config = await configManager.loadConfig('/path/to/minimal.yaml');\\n \\n       // Should have default output configuration\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.group_by).toBe('check');\\n-      expect(config.output.pr_comment.collapse).toBe(true);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.group_by).toBe('check');\\n+      expect(config.output!.pr_comment.collapse).toBe(true);\\n     });\\n   });\\n \\n@@ -483,8 +483,8 @@ output:\\n       expect(config.checks!.performance.prompt).toContain('N+1 database queries');\\n       expect(config.checks!.security.prompt).toContain('SQL injection');\\n       expect(config.checks!.architecture.prompt).toContain('SOLID principles');\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.collapse).toBe(false);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.collapse).toBe(false);\\n     });\\n   });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/foreach-empty-skip.test.ts\",\"additions\":5,\"deletions\":3,\"changes\":8,\"patch\":\"diff --git a/tests/unit/foreach-empty-skip.test.ts b/tests/unit/foreach-empty-skip.test.ts\\nindex 1ea8e1e5..2640ed14 100644\\n--- a/tests/unit/foreach-empty-skip.test.ts\\n+++ b/tests/unit/foreach-empty-skip.test.ts\\n@@ -90,11 +90,13 @@ describe('forEach Empty Array Skip', () => {\\n     );\\n     expect(processStats?.totalRuns).toBe(0);\\n \\n-    // notify-ticket should also NOT run because process-ticket returned empty\\n+    // notify-ticket SHOULD run even though process-ticket had 0 runs from empty forEach\\n+    // This is the expected behavior: downstream steps that depend on forEach children\\n+    // should still execute (with empty/null inputs) to handle the \\\"no items\\\" case\\n     const notifyStats = result.executionStatistics?.checks!.find(\\n       c => c.checkName === 'notify-ticket'\\n     );\\n-    expect(notifyStats?.totalRuns).toBe(0);\\n+    expect(notifyStats?.totalRuns).toBe(1);\\n   });\\n \\n   it('should handle empty array with transform_js', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/ai-check-provider.test.ts\",\"additions\":61,\"deletions\":1,\"changes\":62,\"patch\":\"diff --git a/tests/unit/providers/ai-check-provider.test.ts b/tests/unit/providers/ai-check-provider.test.ts\\nindex d0b94086..a858ddae 100644\\n--- a/tests/unit/providers/ai-check-provider.test.ts\\n+++ b/tests/unit/providers/ai-check-provider.test.ts\\n@@ -399,6 +399,66 @@ describe('AICheckProvider', () => {\\n         },\\n       });\\n     });\\n+\\n+    it('derives allowedFolders and path from workspace when present on parent context', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const workspace = {\\n+        isEnabled: () => true,\\n+        getWorkspaceInfo: () => ({\\n+          workspacePath: '/tmp/ws-123',\\n+          mainProjectPath: '/tmp/ws-123/main-project',\\n+        }),\\n+        listProjects: () => [\\n+          { name: 'tyk', path: '/tmp/ws-123/tyk' },\\n+          { name: 'tyk-docs', path: '/tmp/ws-123/tyk-docs' },\\n+        ],\\n+      };\\n+\\n+      const execContext: any = {\\n+        _parentContext: {\\n+          workspace,\\n+        },\\n+      };\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'code_help',\\n+        ai: {\\n+          provider: 'google',\\n+          model: 'gemini-2.5-pro',\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, execContext);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'google',\\n+        model: 'gemini-2.5-pro',\\n+        // Workspace root should be used as primary working directory for tools\\n+        path: '/tmp/ws-123',\\n+      });\\n+\\n+      // allowedFolders should contain the workspace plus all project paths, de-duped\\n+      expect(capturedConfig.allowedFolders).toEqual(\\n+        expect.arrayContaining(['/tmp/ws-123', '/tmp/ws-123/tyk', '/tmp/ws-123/tyk-docs'])\\n+      );\\n+    });\\n   });\\n \\n   describe('getSupportedConfigKeys', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/command-check-provider.test.ts\",\"additions\":105,\"deletions\":1,\"changes\":106,\"patch\":\"diff --git a/tests/unit/providers/command-check-provider.test.ts b/tests/unit/providers/command-check-provider.test.ts\\nindex 3bf11a84..0a821bbd 100644\\n--- a/tests/unit/providers/command-check-provider.test.ts\\n+++ b/tests/unit/providers/command-check-provider.test.ts\\n@@ -666,4 +666,108 @@ describe('CommandCheckProvider', () => {\\n       expect((result as any).output).toBe('content'); // CommandCheckProvider trims whitespace\\n     });\\n   });\\n+\\n+  describe('Workflow Inputs', () => {\\n+    it('should use workflowInputs from config when available', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Hello from config.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from config.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with inputs from config.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from config.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should fall back to context.workflowInputs when config.workflowInputs is not set', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Hello from context.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from context.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify the command was rendered with inputs from context.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from context.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should prefer config.workflowInputs over context.workflowInputs', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Config takes precedence',\\n+        },\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Context should be ignored',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Config takes precedence\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify config.workflowInputs takes precedence\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Config takes precedence\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should provide empty inputs when neither config nor context has workflowInputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with empty inputs (undefined renders as empty)\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+  });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/git-checkout-workspace.test.ts\",\"additions\":4,\"deletions\":5,\"changes\":9,\"patch\":\"diff --git a/tests/unit/providers/git-checkout-workspace.test.ts b/tests/unit/providers/git-checkout-workspace.test.ts\\nindex 4eb625b7..4a0fbb6e 100644\\n--- a/tests/unit/providers/git-checkout-workspace.test.ts\\n+++ b/tests/unit/providers/git-checkout-workspace.test.ts\\n@@ -129,13 +129,12 @@ describe('GitCheckoutProvider Workspace Integration', () => {\\n         const output = (result as any).output as GitCheckoutOutput;\\n         expect(output.success).toBe(true);\\n         expect(output.workspace_path).toBeDefined();\\n-        // When checkName is provided, it's used as the project name\\n-        expect(output.workspace_path).toContain('test-checkout');\\n \\n-        // Verify project was added to workspace\\n+        // Verify project was added to workspace and the name is derived\\n+        // from the repository/description, not the checkName.\\n         const projects = workspace.listProjects();\\n         expect(projects.length).toBe(1);\\n-        expect(projects[0].name).toBe('test-checkout');\\n+        expect(projects[0].name).toBe('external-repo');\\n       } finally {\\n         await workspace.cleanup();\\n         fs.rmSync(mainProjectDir, { recursive: true, force: true });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/http-client-provider.test.ts\",\"additions\":15,\"deletions\":10,\"changes\":25,\"patch\":\"diff --git a/tests/unit/providers/http-client-provider.test.ts b/tests/unit/providers/http-client-provider.test.ts\\nindex b759ad75..caa73417 100644\\n--- a/tests/unit/providers/http-client-provider.test.ts\\n+++ b/tests/unit/providers/http-client-provider.test.ts\\n@@ -120,15 +120,17 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect(result).toEqual<ReviewSummary & { data: unknown }>({\\n+      // The provider returns data in the 'output' property (consistent with other providers)\\n+      expect(result).toEqual({\\n         issues: [],\\n-        data: responseData,\\n+        output: { status: 'ok', data: { value: 123 } },\\n       });\\n     });\\n \\n     it('should handle POST request with body', async () => {\\n       mockConfig.method = 'POST';\\n-      mockConfig.body = '{\\\"request\\\": \\\"data\\\"}';\\n+      // Use a body with Liquid template to trigger parseAndRender\\n+      mockConfig.body = '{\\\"request\\\": \\\"{{ pr.title }}\\\"}';\\n \\n       const responseData = { result: 'success' };\\n       const mockResponse = {\\n@@ -143,15 +145,16 @@ describe('HttpClientProvider', () => {\\n       };\\n \\n       mockFetch.mockResolvedValue(mockResponse);\\n-      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"data\\\"}');\\n+      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"Test PR\\\"}');\\n \\n       const result = await provider.execute(mockPRInfo, mockConfig, new Map());\\n \\n+      // Verify the body template was parsed by Liquid with the correct template\\n       expect(mockLiquid.parseAndRender).toHaveBeenCalledWith(\\n-        '{\\\"request\\\": \\\"data\\\"}',\\n+        '{\\\"request\\\": \\\"{{ pr.title }}\\\"}',\\n         expect.objectContaining({\\n           pr: expect.any(Object),\\n-          outputs: {},\\n+          outputs: expect.any(Object),\\n         })\\n       );\\n \\n@@ -159,7 +162,7 @@ describe('HttpClientProvider', () => {\\n         'https://api.example.com/data',\\n         expect.objectContaining({\\n           method: 'POST',\\n-          body: '{\\\"request\\\": \\\"data\\\"}',\\n+          body: '{\\\"request\\\": \\\"Test PR\\\"}',\\n           headers: expect.objectContaining({\\n             Authorization: 'Bearer token',\\n             'Content-Type': 'application/json',\\n@@ -167,7 +170,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(responseData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { result: string } }).output.result).toEqual('success');\\n     });\\n \\n     it('should handle HTTP errors', async () => {\\n@@ -235,7 +239,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(transformedData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { transformed: string } }).output.transformed).toEqual('result');\\n     });\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/session-reuse-config.test.ts\",\"additions\":20,\"deletions\":1,\"changes\":21,\"patch\":\"diff --git a/tests/unit/session-reuse-config.test.ts b/tests/unit/session-reuse-config.test.ts\\nindex a2e2ee26..18fc42e6 100644\\n--- a/tests/unit/session-reuse-config.test.ts\\n+++ b/tests/unit/session-reuse-config.test.ts\\n@@ -152,6 +152,25 @@ describe('Session Reuse Configuration Validation', () => {\\n       }).not.toThrow();\\n     });\\n \\n+    it('should accept reuse_ai_session=\\\\\\\"self\\\\\\\" without depends_on', () => {\\n+      const config: Partial<VisorConfig> = {\\n+        version: '1.0',\\n+        checks: {\\n+          'loop-check': {\\n+            type: 'ai',\\n+            prompt: 'Chat-like loop that reuses its own session',\\n+            on: ['pr_opened'],\\n+            // Special self-reuse mode does not require depends_on\\n+            reuse_ai_session: 'self',\\n+          },\\n+        },\\n+      };\\n+\\n+      expect(() => {\\n+        (configManager as any).validateConfig(config);\\n+      }).not.toThrow();\\n+    });\\n+\\n     it('should accept reuse_ai_session=true with multiple dependencies', () => {\\n       const config: Partial<VisorConfig> = {\\n         version: '1.0',\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/slack-markdown-format.test.ts\",\"additions\":25,\"deletions\":1,\"changes\":26,\"patch\":\"diff --git a/tests/unit/slack-markdown-format.test.ts b/tests/unit/slack-markdown-format.test.ts\\nindex 5e57ccae..39d0baf3 100644\\n--- a/tests/unit/slack-markdown-format.test.ts\\n+++ b/tests/unit/slack-markdown-format.test.ts\\n@@ -39,4 +39,28 @@ describe('markdownToSlack', () => {\\n       ['‚Ä¢ parent', '  ‚Ä¢ child', '```', '- not-a-bullet inside code', '```', '‚Ä¢ after'].join('\\\\n')\\n     );\\n   });\\n+\\n+  it('converts markdown headers to bold text', () => {\\n+    const input = '# Main Title\\\\n## Subtitle\\\\n### Section';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*Main Title*\\\\n*Subtitle*\\\\n*Section*');\\n+  });\\n+\\n+  it('adds newline before h1/h2 headers when preceded by content', () => {\\n+    const input = 'Some content\\\\n## New Section\\\\nMore content';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('Some content\\\\n\\\\n*New Section*\\\\nMore content');\\n+  });\\n+\\n+  it('does not add newline before h1 if it is the first line', () => {\\n+    const input = '# First Header\\\\nContent here';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*First Header*\\\\nContent here');\\n+  });\\n+\\n+  it('ignores headers inside code blocks', () => {\\n+    const input = '```\\\\n# This is a comment\\\\n```\\\\n# Real Header';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('```\\\\n# This is a comment\\\\n```\\\\n*Real Header*');\\n+  });\\n });\\n\",\"status\":\"modified\"}],\"outputs\":{\"ask\":{\"text\":\"what is the root cause of this bug:\\n<https://tyktech.atlassian.net/browse/TT-16304>\",\"ts\":1767892971440},\"jira-context\":{\"jira_context_xml\":\"<jira_context>\\n  <issue_count>1</issue_count>\\n  <issue key=\\\"TT-16304\\\">\\n    <summary>Valid OAS API cannot be used by GW due to validation failed error</summary>\\n    <description></description>\\n    <status>Open</status>\\n    <priority>Medium</priority>\\n    <assignee></assignee>\\n    <reporter>Radoslaw Krawczyk</reporter>\\n    <components>Tyk Gateway</components>\\n  </issue>\\n</jira_context>\",\"issues\":[{\"key\":\"TT-16304\",\"summary\":\"Valid OAS API cannot be used by GW due to validation failed error\",\"description\":\"\",\"status\":\"Open\",\"priority\":\"Medium\",\"assignee\":\"\",\"reporter\":\"Radoslaw Krawczyk\",\"labels\":[],\"components\":[\"Tyk Gateway\"],\"custom_fields\":{},\"parent\":null,\"subtasks\":[],\"comments\":[]}],\"issue_count\":1,\"ts\":1767892972384},\"zendesk-context\":{\"zendesk_context_xml\":\"<zendesk_context>\\n  <ticket_count>1</ticket_count>\\n  <ticket id=\\\"16304\\\">\\n    <subject>Authorization Header : starts with a space </subject>\\n    <description>\\n\\nWe have a WAF Compliance Issue whereby the WAF asserts a Header for Authorization as such the Value is leading by a space ; Hence a 403 is recieved\\n\\nurl -v --cert qa-ndex.enstreamidentity.com.cer --key bell-preprod.key \\\\\\n\\n-H &quot;enstream-authorization: 8087656ca224433466c00c75847ac248&quot; \\\\\\n\\n&quot;https://apistg.nbd.bell.ca/ndex/getAttributesFromId?appId=3049&amp;idType=mdn&amp;idList=4162160839&amp;attbList=*&quot;\\n\\n*   Trying 184.150.80.90:443...\\n\\n* Connected to apistg.nbd.bell.ca (184.150.80.90) port 443 (#0)\\n\\n* ALPN, offering h2\\n\\n* ALPN, offering http/1.1\\n\\n* successfully set certificate verify locations:\\n\\n*  CAfile: /etc/ssl/cert.pem\\n\\n*  CApath: none\\n\\n* (304) (OUT), TLS handshake, Client hello (1):\\n\\n* (304) (IN), TLS handshake, Server hello (2):\\n\\n* TLSv1.2 (IN), TLS handshake, Certificate (11):\\n\\n* TLSv1.2 (IN), TLS handshake, Server key exchange (12):\\n\\n* TLSv1.2 (IN), TLS handshake, Request CERT (13):\\n\\n* TLSv1.2 (IN), TLS handshake, Server finished (14):\\n\\n* TLSv1.2 (OUT), TLS handshake, Certificate (11):\\n\\n* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):\\n\\n* TLSv1.2 (OUT), TLS handshake, CERT verify (15):\\n\\n* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):\\n\\n* TLSv1.2 (OUT), TLS handshake, Finished (20):\\n\\n* TLSv1.2 (IN), TLS change cipher, Change cipher spec (1):\\n\\n* TLSv1.2 (IN), TLS handshake, Finished (20):\\n\\n* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256\\n\\n* ALPN, server did not agree to a protocol\\n\\n* Server certificate:\\n\\n*  subject: C=CA; ST=Ontario; L=Ottawa; O=Bell Canada; CN=apistg.nbd.bell.ca\\n\\n*  start date: Jan 20 20:32:24 2023 GMT\\n\\n*  expire date: Feb 19 20:32:21 2024 GMT\\n\\n*  subjectAltName: host &quot;apistg.nbd.bell.ca&quot; matched cert&apos;s &quot;apistg.nbd.bell.ca&quot;\\n\\n*  issuer: C=US; O=Entrust, Inc.; OU=See www.entrust.net/legal-terms; OU=(c) 2012 Entrust, Inc. - for authorized use only; CN=Entrust Certification Authority - L1K\\n\\n*  SSL certificate verify ok.\\n\\n&gt; GET /ndex/getAttributesFromId?appId=3049&amp;idType=mdn&amp;idList=4162160839&amp;attbList=* HTTP/1.1\\n\\n&gt; Host: apistg.nbd.bell.ca\\n\\n&gt; User-Agent: curl/7.79.1\\n\\n&gt; Accept: */*\\n\\n&gt; enstream-authorization: 2c760a6ca224433466c00c75847ac248\\n\\n&gt;\\n\\n* Mark bundle as not supporting multiuse\\n\\n&lt; HTTP/1.1 403 Forbidden\\n\\n&lt; Content-Type: application/json\\n\\n&lt; Date: Tue, 07 Mar 2023 16:35:42 GMT\\n\\n&lt; Content-Length: 57\\n\\n&lt; Set-Cookie: TS01ab8fe2=017e5ad8c4423a1f0c76c854eb18d47723f266acc5bb34020cbc2b9674adb0c3412fc32496f3685aa3b22cf1c284a800cc2f29fdcd; Path=/; Secure; HTTPOnly\\n\\n&lt;\\n\\n{\\n\\n    &quot;error&quot;: &quot;Access to this API has been disallowed&quot;\\n\\n* Connection #0 to host apistg.nbd.bell.ca left intact\\n\\n}</description>\\n    <status>closed</status>\\n    <priority>normal</priority>\\n    <created_at>2023-03-13T14:33:03Z</created_at>\\n    <updated_at>2023-03-21T18:02:27Z</updated_at>\\n    <tags>amer, gold, mongodb, on-prem, q_a, sla, us, wes</tags>\\n  </ticket>\\n</zendesk_context>\",\"tickets\":\"[{\\\"id\\\":\\\"16304\\\",\\\"subject\\\":\\\"Authorization Header : starts with a space \\\",\\\"description\\\":\\\"\\\\n\\\\nWe have a WAF Compliance Issue whereby the WAF asserts a Header for Authorization as such the Value is leading by a space ; Hence a 403 is recieved\\\\n\\\\nurl -v --cert qa-ndex.enstreamidentity.com.cer --key bell-preprod.key \\\\\\\\\\\\n\\\\n-H \\\\\\\"enstream-authorization: 8087656ca224433466c00c75847ac248\\\\\\\" \\\\\\\\\\\\n\\\\n\\\\\\\"https://apistg.nbd.bell.ca/ndex/getAttributesFromId?appId=3049&idType=mdn&idList=4162160839&attbList=*\\\\\\\"\\\\n\\\\n*   Trying 184.150.80.90:443...\\\\n\\\\n* Connected to apistg.nbd.bell.ca (184.150.80.90) port 443 (#0)\\\\n\\\\n* ALPN, offering h2\\\\n\\\\n* ALPN, offering http/1.1\\\\n\\\\n* successfully set certificate verify locations:\\\\n\\\\n*  CAfile: /etc/ssl/cert.pem\\\\n\\\\n*  CApath: none\\\\n\\\\n* (304) (OUT), TLS handshake, Client hello (1):\\\\n\\\\n* (304) (IN), TLS handshake, Server hello (2):\\\\n\\\\n* TLSv1.2 (IN), TLS handshake, Certificate (11):\\\\n\\\\n* TLSv1.2 (IN), TLS handshake, Server key exchange (12):\\\\n\\\\n* TLSv1.2 (IN), TLS handshake, Request CERT (13):\\\\n\\\\n* TLSv1.2 (IN), TLS handshake, Server finished (14):\\\\n\\\\n* TLSv1.2 (OUT), TLS handshake, Certificate (11):\\\\n\\\\n* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):\\\\n\\\\n* TLSv1.2 (OUT), TLS handshake, CERT verify (15):\\\\n\\\\n* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):\\\\n\\\\n* TLSv1.2 (OUT), TLS handshake, Finished (20):\\\\n\\\\n* TLSv1.2 (IN), TLS change cipher, Change cipher spec (1):\\\\n\\\\n* TLSv1.2 (IN), TLS handshake, Finished (20):\\\\n\\\\n* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256\\\\n\\\\n* ALPN, server did not agree to a protocol\\\\n\\\\n* Server certificate:\\\\n\\\\n*  subject: C=CA; ST=Ontario; L=Ottawa; O=Bell Canada; CN=apistg.nbd.bell.ca\\\\n\\\\n*  start date: Jan 20 20:32:24 2023 GMT\\\\n\\\\n*  expire date: Feb 19 20:32:21 2024 GMT\\\\n\\\\n*  subjectAltName: host \\\\\\\"apistg.nbd.bell.ca\\\\\\\" matched cert's \\\\\\\"apistg.nbd.bell.ca\\\\\\\"\\\\n\\\\n*  issuer: C=US; O=Entrust, Inc.; OU=See www.entrust.net/legal-terms; OU=(c) 2012 Entrust, Inc. - for authorized use only; CN=Entrust Certification Authority - L1K\\\\n\\\\n*  SSL certificate verify ok.\\\\n\\\\n> GET /ndex/getAttributesFromId?appId=3049&idType=mdn&idList=4162160839&attbList=* HTTP/1.1\\\\n\\\\n> Host: apistg.nbd.bell.ca\\\\n\\\\n> User-Agent: curl/7.79.1\\\\n\\\\n> Accept: */*\\\\n\\\\n> enstream-authorization: 2c760a6ca224433466c00c75847ac248\\\\n\\\\n>\\\\n\\\\n* Mark bundle as not supporting multiuse\\\\n\\\\n< HTTP/1.1 403 Forbidden\\\\n\\\\n< Content-Type: application/json\\\\n\\\\n< Date: Tue, 07 Mar 2023 16:35:42 GMT\\\\n\\\\n< Content-Length: 57\\\\n\\\\n< Set-Cookie: TS01ab8fe2=017e5ad8c4423a1f0c76c854eb18d47723f266acc5bb34020cbc2b9674adb0c3412fc32496f3685aa3b22cf1c284a800cc2f29fdcd; Path=/; Secure; HTTPOnly\\\\n\\\\n<\\\\n\\\\n{\\\\n\\\\n    \\\\\\\"error\\\\\\\": \\\\\\\"Access to this API has been disallowed\\\\\\\"\\\\n\\\\n* Connection #0 to host apistg.nbd.bell.ca left intact\\\\n\\\\n}\\\",\\\"status\\\":\\\"closed\\\",\\\"priority\\\":\\\"normal\\\",\\\"requester_id\\\":5942781629980,\\\"assignee_id\\\":389488497399,\\\"tags\\\":[\\\"amer\\\",\\\"gold\\\",\\\"mongodb\\\",\\\"on-prem\\\",\\\"q_a\\\",\\\"sla\\\",\\\"us\\\",\\\"wes\\\"],\\\"created_at\\\":\\\"2023-03-13T14:33:03Z\\\",\\\"updated_at\\\":\\\"2023-03-21T18:02:27Z\\\"}]\",\"ticket_count\":1,\"attachments\":\"{}\",\"ts\":1767892972223},\"log-request\":{\"issues\":[],\"logOutput\":\"‚ÑπÔ∏è **INFO**: üì• Request from user: unknown\\nChannel: unknown\\nThread: unknown\\nMessage: what is the root cause of this bug:\\n<https://tyktech.atlassian.net/browse/TT-16304>\\n\"},\"code-help-error\":{\"issues\":[]},\"customer-insights-error\":{\"issues\":[]},\"release-notes-error\":{\"issues\":[]}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"project-code-query","visor.check.output":"{\"answer\":{\"text\":\"Based on a comprehensive review of the Tyk Gateway, Pump, Dashboard, and Sink repositories, here are key areas and details to enhance your technical spike on exporting application logs. This analysis covers both operational logging and API traffic analytics, tracing the data flow from generation to export.\\n\\n### Key Areas for Additional Detail\\n\\n**1. Differentiating Between Operational Logs and API Analytics**\\n\\nThe spike should clearly distinguish between two types of data:\\n\\n*   **Operational Logs**: These are the application logs generated by each Tyk component (Gateway, Pump, Dashboard, Sink). They record the internal state, errors, and activities of the components themselves.\\n*   **API Analytics**: These are structured records (`AnalyticsRecord`) detailing each API transaction processed by the Gateway. This is the data that populates the analytics dashboards.\\n\\n**2. Deep Dive into Gateway Log Generation**\\n\\n*   **Operational Logging**: The Gateway uses the `logrus` library for structured logging. Key configuration is managed through environment variables:\\n    *   `TYK_LOGLEVEL` or `TYK_GW_LOGLEVEL`: Sets the verbosity (e.g., `debug`, `info`, `warn`, `error`).\\n    *   `TYK_LOGFORMAT` or `TYK_GW_LOGFORMAT`: Sets the output format (`default` for text, `json` for structured JSON).\\n    *   **Reference**: [tyk/log/log.go](https://github.com/TykTechnologies/tyk/blob/master/log/log.go#L46-L53)\\n\\n*   **API Analytics Generation**: This is a high-performance, asynchronous process:\\n    *   An `AnalyticsRecord` is created for each API request.\\n    *   The record is pushed onto an in-memory channel (`recordsChan`).\\n    *   A pool of worker goroutines (`recordWorker`) reads from this channel in batches.\\n    *   The records are enriched with tags (org ID, API ID, key ID), serialized (using MessagePack by default for efficiency), and written to a Redis Set using pipelined commands for minimal overhead.\\n    *   **Reference**: [tyk/gateway/analytics.go](https://github.com/TykTechnologies/tyk/blob/master/gateway/analytics.go#L117-L234) (specifically `RecordHit` and `recordWorker` functions).\\n\\n**3. The Central Role of Tyk Pump**\\n\\nTyk Pump is the cornerstone of analytics export. The spike should detail its architecture and capabilities:\\n\\n*   **Data Source**: Pump continuously polls Redis for analytics data written by the Gateway(s).\\n*   **Processing Loop**: In `main.go`, the `StartPurgeLoop` function orchestrates the process:\\n    1.  It fetches and deletes batches of serialized `AnalyticsRecord` data from Redis.\\n    2.  It deserializes the data back into `AnalyticsRecord` structs.\\n    3.  It applies filters and transformations based on the configuration of each pump (e.g., `omit_detailed_recording`, `raw_request_decoded`).\\n    4.  It sends the processed data to all configured pumps.\\n*   **Pluggable Pumps (Sinks)**: The spike should list the available pumps and their primary use cases. This is a major strength of Tyk's observability.\\n    *   **Examples**: Elasticsearch, Prometheus, Splunk, Kafka, Datadog (via `dogstatsd`), InfluxDB, SQL, MongoDB, stdout.\\n    *   **Reference**: The [tyk-pump/pumps/](https://github.com/TykTechnologies/tyk-pump/tree/master/pumps) directory contains the implementation for each sink.\\n\\n**4. Data Format and Serialization**\\n\\nThis is a critical detail for anyone consuming the logs:\\n\\n*   **Gateway to Pump**: The data in Redis is serialized. The default is MessagePack (`msgpack`), but Protobuf is also an option. This is configured in the Gateway.\\n*   **Pump to Sink**: The format sent to the final destination depends on the pump. For example, the Elasticsearch pump sends JSON documents, while the Prometheus pump exposes metrics in the Prometheus exposition format.\\n*   The spike should emphasize that the raw data in Redis is not directly human-readable and requires deserialization, which Pump handles automatically.\\n\\n**5. The Role of Tyk Sink in Hybrid and MDCB Deployments**\\n\\nFor complex architectures, Tyk Sink's role is often misunderstood. The spike can clarify:\\n\\n*   **Function**: Sink acts as an RPC bridge. Gateways in remote data centers or on-premises can be configured to send their analytics to a central Tyk Sink instance.\\n*   **`ForwardAnalyticsToPump`**: This is a key configuration option in `tyk-sink.conf`.\\n    *   If `true`, the Sink receives the analytics data and writes it *back* to a central Redis instance. A central Tyk Pump then processes this data as usual.\\n    *   If `false`, the Sink processes and stores the analytics directly into its own configured database (e.g., MongoDB), bypassing the need for a separate Pump.\\n*   **Reference**: [tyk-sink/storage/db_loader.go](https://github.com/TykTechnologies/tyk-sink/blob/master/storage/db_loader.go#L142-L220) (specifically the `StoreAnalytics` and `ForwardAnalyticsToPump` functions).\\n\\n### Potential Missing Areas in the Spike\\n\\n*   **Audit Logging**: The Tyk Dashboard (`tyk-analytics`) generates its own audit logs to track user activity *within the Dashboard*. This is distinct from API analytics. The spike could mention this as another source of observability data, focused on administrative actions.\\n    *   **Reference**: [tyk-analytics/dashboard/audit.go](https://github.com/TykTechnologies/tyk-analytics/blob/master/dashboard/audit.go#L56-L92)\\n*   **Health Checks**: Both Tyk Pump and Tyk Sink expose health check endpoints. These are crucial for monitoring the health of the data pipeline itself.\\n*   **Error Handling and Retries**: The spike could touch upon how the system behaves when a sink is unavailable. While some pumps have built-in retry logic, this is an important consideration for production deployments.\\n*   **Performance Tuning**: Mentioning key performance-related configurations would be valuable, such as the Gateway's analytics buffer size (`records_buffer_size`), the Pump's purge delay (`purge_delay`), and batch sizes for various pumps.\\n\\nBy incorporating these details, your technical spike will provide a comprehensive and actionable guide to exporting and managing logs and analytics across the entire Tyk ecosystem.\"},\"text\":\"{\\n  \\\"answer\\\": {\\n    \\\"text\\\": \\\"Based on a comprehensive review of the Tyk Gateway, Pump, Dashboard, and Sink repositories, here are key areas and details to enhance your technical spike on exporting application logs. This analysis covers both operational logging and API traffic analytics, tracing the data flow from generation to export.\\\\n\\\\n### Key Areas for Additional Detail\\\\n\\\\n**1. Differentiating Between Operational Logs and API Analytics**\\\\n\\\\nThe spike should clearly distinguish between two types of data:\\\\n\\\\n*   **Operational Logs**: These are the application logs generated by each Tyk component (Gateway, Pump, Dashboard, Sink). They record the internal state, errors, and activities of the components themselves.\\\\n*   **API Analytics**: These are structured records (`AnalyticsRecord`) detailing each API transaction processed by the Gateway. This is the data that populates the analytics dashboards.\\\\n\\\\n**2. Deep Dive into Gateway Log Generation**\\\\n\\\\n*   **Operational Logging**: The Gateway uses the `logrus` library for structured logging. Key configuration is managed through environment variables:\\\\n    *   `TYK_LOGLEVEL` or `TYK_GW_LOGLEVEL`: Sets the verbosity (e.g., `debug`, `info`, `warn`, `error`).\\\\n    *   `TYK_LOGFORMAT` or `TYK_GW_LOGFORMAT`: Sets the output format (`default` for text, `json` for structured JSON).\\\\n    *   **Reference**: [tyk/log/log.go](https://github.com/TykTechnologies/tyk/blob/master/log/log.go#L46-L53)\\\\n\\\\n*   **API Analytics Generation**: This is a high-performance, asynchronous process:\\\\n    *   An `AnalyticsRecord` is created for each API request.\\\\n    *   The record is pushed onto an in-memory channel (`recordsChan`).\\\\n    *   A pool of worker goroutines (`recordWorker`) reads from this channel in batches.\\\\n    *   The records are enriched with tags (org ID, API ID, key ID), serialized (using MessagePack by default for efficiency), and written to a Redis Set using pipelined commands for minimal overhead.\\\\n    *   **Reference**: [tyk/gateway/analytics.go](https://github.com/TykTechnologies/tyk/blob/master/gateway/analytics.go#L117-L234) (specifically `RecordHit` and `recordWorker` functions).\\\\n\\\\n**3. The Central Role of Tyk Pump**\\\\n\\\\nTyk Pump is the cornerstone of analytics export. The spike should detail its architecture and capabilities:\\\\n\\\\n*   **Data Source**: Pump continuously polls Redis for analytics data written by the Gateway(s).\\\\n*   **Processing Loop**: In `main.go`, the `StartPurgeLoop` function orchestrates the process:\\\\n    1.  It fetches and deletes batches of serialized `AnalyticsRecord` data from Redis.\\\\n    2.  It deserializes the data back into `AnalyticsRecord` structs.\\\\n    3.  It applies filters and transformations based on the configuration of each pump (e.g., `omit_detailed_recording`, `raw_request_decoded`).\\\\n    4.  It sends the processed data to all configured pumps.\\\\n*   **Pluggable Pumps (Sinks)**: The spike should list the available pumps and their primary use cases. This is a major strength of Tyk's observability.\\\\n    *   **Examples**: Elasticsearch, Prometheus, Splunk, Kafka, Datadog (via `dogstatsd`), InfluxDB, SQL, MongoDB, stdout.\\\\n    *   **Reference**: The [tyk-pump/pumps/](https://github.com/TykTechnologies/tyk-pump/tree/master/pumps) directory contains the implementation for each sink.\\\\n\\\\n**4. Data Format and Serialization**\\\\n\\\\nThis is a critical detail for anyone consuming the logs:\\\\n\\\\n*   **Gateway to Pump**: The data in Redis is serialized. The default is MessagePack (`msgpack`), but Protobuf is also an option. This is configured in the Gateway.\\\\n*   **Pump to Sink**: The format sent to the final destination depends on the pump. For example, the Elasticsearch pump sends JSON documents, while the Prometheus pump exposes metrics in the Prometheus exposition format.\\\\n*   The spike should emphasize that the raw data in Redis is not directly human-readable and requires deserialization, which Pump handles automatically.\\\\n\\\\n**5. The Role of Tyk Sink in Hybrid and MDCB Deployments**\\\\n\\\\nFor complex architectures, Tyk Sink's role is often misunderstood. The spike can clarify:\\\\n\\\\n*   **Function**: Sink acts as an RPC bridge. Gateways in remote data centers or on-premises can be configured to send their analytics to a central Tyk Sink instance.\\\\n*   **`ForwardAnalyticsToPump`**: This is a key configuration option in `tyk-sink.conf`.\\\\n    *   If `true`, the Sink receives the analytics data and writes it *back* to a central Redis instance. A central Tyk Pump then processes this data as usual.\\\\n    *   If `false`, the Sink processes and stores the analytics directly into its own configured database (e.g., MongoDB), bypassing the need for a separate Pump.\\\\n*   **Reference**: [tyk-sink/storage/db_loader.go](https://github.com/TykTechnologies/tyk-sink/blob/master/storage/db_loader.go#L142-L220) (specifically the `StoreAnalytics` and `ForwardAnalyticsToPump` functions).\\\\n\\\\n### Potential Missing Areas in the Spike\\\\n\\\\n*   **Audit Logging**: The Tyk Dashboard (`tyk-analytics`) generates its own audit logs to track user activity *within the Dashboard*. This is distinct from API analytics. The spike could mention this as another source of observability data, focused on administrative actions.\\\\n    *   **Reference**: [tyk-analytics/dashboard/audit.go](https://github.com/TykTechnologies/tyk-analytics/blob/master/dashboard/audit.go#L56-L92)\\\\n*   **Health Checks**: Both Tyk Pump and Tyk Sink expose health check endpoints. These are crucial for monitoring the health of the data pipeline itself.\\\\n*   **Error Handling and Retries**: The spike could touch upon how the system behaves when a sink is unavailable. While some pumps have built-in retry logic, this is an important consideration for production deployments.\\\\n*   **Performance Tuning**: Mentioning key performance-related configurations would be valuable, such as the Gateway's analytics buffer size (`records_buffer_size`), the Pump's purge delay (`purge_delay`), and batch sizes for various pumps.\\\\n\\\\nBy incorporating these details, your technical spike will provide a comprehensive and actionable guide to exporting and managing logs and analytics across the entire Tyk ecosystem.\\\"\\n  }\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"62ad7ac0-d2f5-416a-8e6d-75fdf292d95a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"62ad7ac0-d2f5-416a-8e6d-75fdf292d95a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"62ad7ac0-d2f5-416a-8e6d-75fdf292d95a"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":2,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"code-help-reply","visor.provider.type":"log"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":3,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":3,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":3,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":3,"session_id":"fb082dda-74d2-49c1-a4ee-2069f3bc40e9"}}]}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.output":"{\"intent\":\"code_help\",\"topic\":\"The root cause of the bug TT-16304, where a valid OAS API fails validation, is likely due to the `swagger-parser` library used for validation. The validation logic, found in `src/lib/spec.ts` in the Visor repository, uses this library to validate the OpenAPI specification. The issue could be that `swagger-parser` is stricter than other validation tools, or there might be a bug in the version of the library being used. This is based on the analysis of the Visor tool's source code, which may or may not be identical to the validation logic in the Tyk Gateway itself.\",\"text\":\"{\\n  \\\"intent\\\": \\\"code_help\\\",\\n  \\\"topic\\\": \\\"The root cause of the bug TT-16304, where a valid OAS API fails validation, is likely due to the `swagger-parser` library used for validation. The validation logic, found in `src/lib/spec.ts` in the Visor repository, uses this library to validate the OpenAPI specification. The issue could be that `swagger-parser` is stricter than other validation tools, or there might be a bug in the version of the library being used. This is based on the analysis of the Visor tool's source code, which may or may not be identical to the validation logic in the Tyk Gateway itself.\\\"\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":2,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"code-help","visor.provider.type":"workflow"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"e3e5b165-e653-488e-a800-eb0a5f10442b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"e3e5b165-e653-488e-a800-eb0a5f10442b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"e3e5b165-e653-488e-a800-eb0a5f10442b"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-tyk-docs","visor.provider.type":"git-checkout"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e3e5b165-e653-488e-a800-eb0a5f10442b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e3e5b165-e653-488e-a800-eb0a5f10442b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"e3e5b165-e653-488e-a800-eb0a5f10442b"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ensure-code-plan","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"ensure-code-plan","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"outputs\":{\"checkout-tyk-docs\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-docs-main-36d0c4b0\",\"ref\":\"main\",\"commit\":\"a022bb7c96ca4f48bcdac27cae3f7c4a24171ee9\",\"worktree_id\":\"TykTechnologies-tyk-docs-main-36d0c4b0\",\"repository\":\"TykTechnologies/tyk-docs\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/b11727db-76f7-48ce-b644-f77b533190d4/tyk-docs\",\"ts\":1767893046120}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"ensure-code-plan","visor.check.output":"{\"projects\":[{\"project_id\":\"tyk\",\"reason\":\"The bug is likely related to the Tyk Gateway's core functionality, as it handles the primary API traffic and policy enforcement.\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Dashboard backend is responsible for configuring the Gateway. The bug could stem from misconfiguration or state management issues originating from the Dashboard.\"},{\"project_id\":\"tyk-analytics-ui\",\"reason\":\"The bug might be related to how the feature is configured or displayed in the Dashboard UI, which could lead to incorrect behavior in the Gateway.\"}],\"notes\":\"Without access to the Jira ticket, it's impossible to know the specific feature involved. This plan includes the core components (Gateway, Dashboard Backend, Dashboard UI) to provide broad coverage for investigating a typical bug.\",\"text\":\"{\\n  \\\"projects\\\": [\\n    {\\n      \\\"project_id\\\": \\\"tyk\\\",\\n      \\\"reason\\\": \\\"The bug is likely related to the Tyk Gateway's core functionality, as it handles the primary API traffic and policy enforcement.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-analytics\\\",\\n      \\\"reason\\\": \\\"The Tyk Dashboard backend is responsible for configuring the Gateway. The bug could stem from misconfiguration or state management issues originating from the Dashboard.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-analytics-ui\\\",\\n      \\\"reason\\\": \\\"The bug might be related to how the feature is configured or displayed in the Dashboard UI, which could lead to incorrect behavior in the Gateway.\\\"\\n    }\\n  ],\\n  \\\"notes\\\": \\\"Without access to the Jira ticket, it's impossible to know the specific feature involved. This plan includes the core components (Gateway, Dashboard Backend, Dashboard UI) to provide broad coverage for investigating a typical bug.\\\"\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e3e5b165-e653-488e-a800-eb0a5f10442b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e3e5b165-e653-488e-a800-eb0a5f10442b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"e3e5b165-e653-488e-a800-eb0a5f10442b"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"project-items","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e3e5b165-e653-488e-a800-eb0a5f10442b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e3e5b165-e653-488e-a800-eb0a5f10442b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"e3e5b165-e653-488e-a800-eb0a5f10442b"}}]}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":0,"visor.foreach.total":3},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":1,"visor.foreach.total":3},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":2,"visor.foreach.total":3},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e3e5b165-e653-488e-a800-eb0a5f10442b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e3e5b165-e653-488e-a800-eb0a5f10442b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"e3e5b165-e653-488e-a800-eb0a5f10442b"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"project-code-query","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"project-code-query","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"outputs\":{\"ensure-code-plan\":{\"projects\":[{\"project_id\":\"tyk\",\"reason\":\"The bug is likely related to the Tyk Gateway's core functionality, as it handles the primary API traffic and policy enforcement.\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Dashboard backend is responsible for configuring the Gateway. The bug could stem from misconfiguration or state management issues originating from the Dashboard.\"},{\"project_id\":\"tyk-analytics-ui\",\"reason\":\"The bug might be related to how the feature is configured or displayed in the Dashboard UI, which could lead to incorrect behavior in the Gateway.\"}],\"notes\":\"Without access to the Jira ticket, it's impossible to know the specific feature involved. This plan includes the core components (Gateway, Dashboard Backend, Dashboard UI) to provide broad coverage for investigating a typical bug.\",\"text\":\"{\\n  \\\"projects\\\": [\\n    {\\n      \\\"project_id\\\": \\\"tyk\\\",\\n      \\\"reason\\\": \\\"The bug is likely related to the Tyk Gateway's core functionality, as it handles the primary API traffic and policy enforcement.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-analytics\\\",\\n      \\\"reason\\\": \\\"The Tyk Dashboard backend is responsible for configuring the Gateway. The bug could stem from misconfiguration or state management issues originating from the Dashboard.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-analytics-ui\\\",\\n      \\\"reason\\\": \\\"The bug might be related to how the feature is configured or displayed in the Dashboard UI, which could lead to incorrect behavior in the Gateway.\\\"\\n    }\\n  ],\\n  \\\"notes\\\": \\\"Without access to the Jira ticket, it's impossible to know the specific feature involved. This plan includes the core components (Gateway, Dashboard Backend, Dashboard UI) to provide broad coverage for investigating a typical bug.\\\"\\n}\",\"ts\":1767893087648},\"project-items\":[{\"project_id\":\"tyk\",\"reason\":\"The bug is likely related to the Tyk Gateway's core functionality, as it handles the primary API traffic and policy enforcement.\",\"repository\":\"TykTechnologies/tyk\",\"description\":\"Tyk Gateway\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Dashboard backend is responsible for configuring the Gateway. The bug could stem from misconfiguration or state management issues originating from the Dashboard.\",\"repository\":\"TykTechnologies/tyk-analytics\",\"description\":\"Tyk Dashboard backend\"},{\"project_id\":\"tyk-analytics-ui\",\"reason\":\"The bug might be related to how the feature is configured or displayed in the Dashboard UI, which could lead to incorrect behavior in the Gateway.\",\"repository\":\"TykTechnologies/tyk-analytics-ui\",\"description\":\"Tyk Dashboard UI\"}],\"checkout-projects\":{\"issues\":[],\"isForEach\":true,\"forEachItems\":[{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-HEAD-8c652541\",\"ref\":\"HEAD\",\"commit\":\"bb67e9e7f9382a09ff152c6942d00023c463feef\",\"worktree_id\":\"TykTechnologies-tyk-HEAD-8c652541\",\"repository\":\"TykTechnologies/tyk\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/b11727db-76f7-48ce-b644-f77b533190d4/tyk\"},{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-analytics-HEAD-cb7128d9\",\"ref\":\"HEAD\",\"commit\":\"b7ed8000a93e1284b28100938363a312d2d1fad1\",\"worktree_id\":\"TykTechnologies-tyk-analytics-HEAD-cb7128d9\",\"repository\":\"TykTechnologies/tyk-analytics\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/b11727db-76f7-48ce-b644-f77b533190d4/tyk-analytics\"},{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-analytics-ui-HEAD-cb199731\",\"ref\":\"HEAD\",\"commit\":\"8cbf10d89bbff5fc9a7c7a429948cb8666096e44\",\"worktree_id\":\"TykTechnologies-tyk-analytics-ui-HEAD-cb199731\",\"repository\":\"TykTechnologies/tyk-analytics-ui\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/b11727db-76f7-48ce-b644-f77b533190d4/tyk-analytics-ui\"}],\"forEachItemResults\":[{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-HEAD-8c652541\",\"ref\":\"HEAD\",\"commit\":\"bb67e9e7f9382a09ff152c6942d00023c463feef\",\"worktree_id\":\"TykTechnologies-tyk-HEAD-8c652541\",\"repository\":\"TykTechnologies/tyk\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/b11727db-76f7-48ce-b644-f77b533190d4/tyk\"}},{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-analytics-HEAD-cb7128d9\",\"ref\":\"HEAD\",\"commit\":\"b7ed8000a93e1284b28100938363a312d2d1fad1\",\"worktree_id\":\"TykTechnologies-tyk-analytics-HEAD-cb7128d9\",\"repository\":\"TykTechnologies/tyk-analytics\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/b11727db-76f7-48ce-b644-f77b533190d4/tyk-analytics\"}},{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-analytics-ui-HEAD-cb199731\",\"ref\":\"HEAD\",\"commit\":\"8cbf10d89bbff5fc9a7c7a429948cb8666096e44\",\"worktree_id\":\"TykTechnologies-tyk-analytics-ui-HEAD-cb199731\",\"repository\":\"TykTechnologies/tyk-analytics-ui\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/b11727db-76f7-48ce-b644-f77b533190d4/tyk-analytics-ui\"}}]},\"checkout-tyk-docs\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-docs-main-36d0c4b0\",\"ref\":\"main\",\"commit\":\"a022bb7c96ca4f48bcdac27cae3f7c4a24171ee9\",\"worktree_id\":\"TykTechnologies-tyk-docs-main-36d0c4b0\",\"repository\":\"TykTechnologies/tyk-docs\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/b11727db-76f7-48ce-b644-f77b533190d4/tyk-docs\",\"ts\":1767893046120},\"project-items-raw\":[{\"project_id\":\"tyk\",\"reason\":\"The bug is likely related to the Tyk Gateway's core functionality, as it handles the primary API traffic and policy enforcement.\",\"repository\":\"TykTechnologies/tyk\",\"description\":\"Tyk Gateway\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Dashboard backend is responsible for configuring the Gateway. The bug could stem from misconfiguration or state management issues originating from the Dashboard.\",\"repository\":\"TykTechnologies/tyk-analytics\",\"description\":\"Tyk Dashboard backend\"},{\"project_id\":\"tyk-analytics-ui\",\"reason\":\"The bug might be related to how the feature is configured or displayed in the Dashboard UI, which could lead to incorrect behavior in the Gateway.\",\"repository\":\"TykTechnologies/tyk-analytics-ui\",\"description\":\"Tyk Dashboard UI\"}]},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"project-code-query","visor.check.output":"{\"answer\":{\"text\":\"The root cause of the bug is a lack of validation for the `per` field in the rate limit configuration, allowing a value of `0` to be set. This can cause the rate limiter to fail open, effectively disabling rate limiting for an API. The issue is located in the `ForwardMessage` function in `tyk/gateway/session_manager.go`. The code checks if `apiLimit.Rate > 0` before applying the rate limit, but does not perform a similar check for `apiLimit.Per`. A `per` value of `0` can lead to a division-by-zero error or other undefined behavior in the underlying `limiters` package, which is responsible for the actual rate limiting calculation. The code only explicitly checks for the `rate.ErrLimitExhausted` error, so any other error returned by the rate limiter would be ignored, causing the rate limiter to fail open and allow the request to proceed. To fix this, validation should be added to ensure that the `per` value is always greater than zero before being used by the rate limiter.\\n\\n**References:**\\n\\n- **Tyk Gateway - Session Manager (`ForwardMessage` function)**: [https://github.com/TykTechnologies/tyk/blob/master/gateway/session_manager.go#L281-L388](https://github.com/TykTechnologies/tyk/blob/master/gateway/session_manager.go#L281-L388)\\n- **Tyk Gateway - Fixed Window Rate Limiter**: [https://github.com/TykTechnologies/tyk/blob/master/internal/rate/limiter/limiter_fixed_window.go](https://github.com/TykTechnologies/tyk/blob/master/internal/rate/limiter/limiter_fixed_window.go)\\n- **Tyk Gateway - Rate Limiter Middleware**: [https://github.com/TykTechnologies/tyk/blob/master/gateway/mw_api_rate_limit.go](https://github.com/TykTechnologies/tyk/blob/master/gateway/mw_api_rate_limit.go)\"},\"text\":\"{\\n  \\\"answer\\\": {\\n    \\\"text\\\": \\\"The root cause of the bug is a lack of validation for the `per` field in the rate limit configuration, allowing a value of `0` to be set. This can cause the rate limiter to fail open, effectively disabling rate limiting for an API. The issue is located in the `ForwardMessage` function in `tyk/gateway/session_manager.go`. The code checks if `apiLimit.Rate > 0` before applying the rate limit, but does not perform a similar check for `apiLimit.Per`. A `per` value of `0` can lead to a division-by-zero error or other undefined behavior in the underlying `limiters` package, which is responsible for the actual rate limiting calculation. The code only explicitly checks for the `rate.ErrLimitExhausted` error, so any other error returned by the rate limiter would be ignored, causing the rate limiter to fail open and allow the request to proceed. To fix this, validation should be added to ensure that the `per` value is always greater than zero before being used by the rate limiter.\\\\n\\\\n**References:**\\\\n\\\\n- **Tyk Gateway - Session Manager (`ForwardMessage` function)**: [https://github.com/TykTechnologies/tyk/blob/master/gateway/session_manager.go#L281-L388](https://github.com/TykTechnologies/tyk/blob/master/gateway/session_manager.go#L281-L388)\\\\n- **Tyk Gateway - Fixed Window Rate Limiter**: [https://github.com/TykTechnologies/tyk/blob/master/internal/rate/limiter/limiter_fixed_window.go](https://github.com/TykTechnologies/tyk/blob/master/internal/rate/limiter/limiter_fixed_window.go)\\\\n- **Tyk Gateway - Rate Limiter Middleware**: [https://github.com/TykTechnologies/tyk/blob/master/gateway/mw_api_rate_limit.go](https://github.com/TykTechnologies/tyk/blob/master/gateway/mw_api_rate_limit.go)\\\"\\n  }\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e3e5b165-e653-488e-a800-eb0a5f10442b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e3e5b165-e653-488e-a800-eb0a5f10442b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"e3e5b165-e653-488e-a800-eb0a5f10442b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":2,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"code-help-reply","visor.provider.type":"log"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":3,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":3,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":3,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":3,"session_id":"b11727db-76f7-48ce-b644-f77b533190d4"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"jira-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"zendesk-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"log-request","visor.provider.type":"log"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"bb002071-3af0-4b25-a26f-3ce62b64dd95"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"56be6bbe-076e-4b3e-b701-65815b27a62b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"bb002071-3af0-4b25-a26f-3ce62b64dd95"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"56be6bbe-076e-4b3e-b701-65815b27a62b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"bb002071-3af0-4b25-a26f-3ce62b64dd95"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"56be6bbe-076e-4b3e-b701-65815b27a62b"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-keys","visor.provider.type":"script"}}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-ids","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"history\":{}},\"outputs_history\":{},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"include_comments\":true,\"max_tickets\":5,\"download_assets\":true,\"cache_dir\":\".cache/zendesk\",\"text\":\"<@U09T5KRLMPU> If I'm using the OSS Gateway with API Keys, how would I migrate to using hybrid where the control plane is hosted on Tyk Cloud?  I want to preserve my APIs and Keys.\\nIf I'm using the OSS Gateway with API Keys, how would I migrate to using hybrid where the control plane is hosted on Tyk Cloud?  I want to preserve my APIs and Keys.\\n\"},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.output":"{\"data\":[],\"count\":0}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"bb002071-3af0-4b25-a26f-3ce62b64dd95"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"bb002071-3af0-4b25-a26f-3ce62b64dd95"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"bb002071-3af0-4b25-a26f-3ce62b64dd95"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"no-issues-fallback","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"no-issues-fallback","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"extract-keys\":{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767897523982},\"fetch-issues\":{\"issues\":[]},\"history\":{\"extract-keys\":[{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767897523982}],\"fetch-issues\":[{\"issues\":[]}]}},\"outputs_history\":{\"extract-keys\":[{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767897523982}],\"fetch-issues\":[{\"issues\":[]}]},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"text\":\"<@U09T5KRLMPU> If I'm using the OSS Gateway with API Keys, how would I migrate to using hybrid where the control plane is hosted on Tyk Cloud?  I want to preserve my APIs and Keys.\\nIf I'm using the OSS Gateway with API Keys, how would I migrate to using hybrid where the control plane is hosted on Tyk Cloud?  I want to preserve my APIs and Keys.\\n\",\"jql\":\"\",\"custom_field_aliases\":{},\"include_comments\":true,\"max_issues\":3,\"ticket_prefixes\":[\"TT\",\"DX\"]},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"56be6bbe-076e-4b3e-b701-65815b27a62b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"56be6bbe-076e-4b3e-b701-65815b27a62b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"56be6bbe-076e-4b3e-b701-65815b27a62b"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"no-tickets-fallback","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"no-tickets-fallback","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"extract-ids\":{\"data\":[],\"count\":0,\"ts\":1767897523992},\"fetch-tickets\":{\"issues\":[]},\"history\":{\"extract-ids\":[{\"data\":[],\"count\":0,\"ts\":1767897523992}],\"fetch-tickets\":[{\"issues\":[]}]}},\"outputs_history\":{\"extract-ids\":[{\"data\":[],\"count\":0,\"ts\":1767897523992}],\"fetch-tickets\":[{\"issues\":[]}]},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"include_comments\":true,\"max_tickets\":5,\"download_assets\":true,\"cache_dir\":\".cache/zendesk\",\"text\":\"<@U09T5KRLMPU> If I'm using the OSS Gateway with API Keys, how would I migrate to using hybrid where the control plane is hosted on Tyk Cloud?  I want to preserve my APIs and Keys.\\nIf I'm using the OSS Gateway with API Keys, how would I migrate to using hybrid where the control plane is hosted on Tyk Cloud?  I want to preserve my APIs and Keys.\\n\"},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.check","attributes":{"visor.check.id":"no-issues-fallback","visor.check.output":"\"<jira_context><issue_count>0</issue_count><message>No Jira issue keys found in the provided text</message></jira_context>\""},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"bb002071-3af0-4b25-a26f-3ce62b64dd95"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"bb002071-3af0-4b25-a26f-3ce62b64dd95"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"bb002071-3af0-4b25-a26f-3ce62b64dd95"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"bb002071-3af0-4b25-a26f-3ce62b64dd95"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"bb002071-3af0-4b25-a26f-3ce62b64dd95"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.check","attributes":{"visor.check.id":"no-tickets-fallback","visor.check.output":"\"<zendesk_context><ticket_count>0</ticket_count><message>No Zendesk ticket IDs found in the provided text</message></zendesk_context>\""},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"56be6bbe-076e-4b3e-b701-65815b27a62b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"56be6bbe-076e-4b3e-b701-65815b27a62b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"56be6bbe-076e-4b3e-b701-65815b27a62b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"56be6bbe-076e-4b3e-b701-65815b27a62b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"56be6bbe-076e-4b3e-b701-65815b27a62b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"56be6bbe-076e-4b3e-b701-65815b27a62b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"56be6bbe-076e-4b3e-b701-65815b27a62b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"56be6bbe-076e-4b3e-b701-65815b27a62b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"56be6bbe-076e-4b3e-b701-65815b27a62b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"56be6bbe-076e-4b3e-b701-65815b27a62b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"56be6bbe-076e-4b3e-b701-65815b27a62b"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"route-intent","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.input.context":"{\"pr\":{\"number\":0,\"title\":\"Local Analysis: feat/slack-frontend-v2 (71 modified)\",\"author\":\"Leonid Bugaev\",\"branch\":\"feat/slack-frontend-v2\",\"base\":\"main\"},\"files\":[{\"filename\":\"README.md\",\"additions\":7,\"deletions\":1,\"changes\":8,\"patch\":\"diff --git a/README.md b/README.md\\nindex 41131861..d92f02cc 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -534,6 +534,12 @@ steps:\\n     session_mode: append    # Shares history for full conversation\\n ```\\n \\n+You can also reuse the **same check‚Äôs** session when it loops back to itself, using:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` with `session_mode: append`\\n+\\n+See the standalone example at `examples/session-reuse-self.yaml` and the detailed guide in [docs/advanced-ai.md](docs/advanced-ai.md).\\n+\\n Learn more: [docs/advanced-ai.md](docs/advanced-ai.md)\\n \\n ## üìã Schema-Template System\\n\",\"status\":\"modified\"},{\"filename\":\"docs/advanced-ai.md\",\"additions\":61,\"deletions\":2,\"changes\":63,\"patch\":\"diff --git a/docs/advanced-ai.md b/docs/advanced-ai.md\\nindex bb8e684a..879285ed 100644\\n--- a/docs/advanced-ai.md\\n+++ b/docs/advanced-ai.md\\n@@ -1,7 +1,7 @@\\n ## üß† Advanced AI Features\\n \\n ### AI Session Reuse\\n-Use `reuse_ai_session: true` on dependent checks to continue conversation context with the AI across checks. This improves follow‚Äëups and consistency.\\n+Use `reuse_ai_session` on checks to continue conversation context with the AI across steps. This improves follow‚Äëups and consistency for follow‚Äëon analysis and chat‚Äëstyle flows.\\n \\n **Session Modes:**\\n - **`clone` (default)**: Creates a copy of the conversation history. Each check gets an independent session with the same starting context. Changes made by one check don't affect others.\\n@@ -26,6 +26,65 @@ steps:\\n     depends_on: [security-remediation]\\n     reuse_ai_session: true\\n     session_mode: append  # Share history - sees full conversation\\n+\\n+#### Reusing your own session: `reuse_ai_session: self`\\n+\\n+Sometimes the step you want to loop back into is the AI step itself (e.g. Slack assistants or multi‚Äëturn internal tools). For that case you can use:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` ‚Äì the step reuses its **own** Probe session when it runs again in the same engine run.\\n+- `session_mode: append` ‚Äì makes the follow‚Äëup behave like a normal conversation turn.\\n+\\n+On the first run of the step, Visor creates a new ProbeAgent session and registers it. If routing (`on_success.goto`, `goto_js`, etc.) later jumps back to the same step within the same run, the engine:\\n+\\n+- Finds the last result for that step in the current run.\\n+- Reads the `sessionId` stored in the result.\\n+- Calls the AI provider again using `executeReviewWithSessionReuse` with that session id.\\n+\\n+Simple example (no transport wiring, just CLI/tests):\\n+\\n+```yaml\\n+version: \\\"2.0\\\"\\n+\\n+steps:\\n+  seed:\\n+    type: script\\n+    content: |\\n+      return { text: \\\"hello from seed\\\" };\\n+\\n+  convo:\\n+    type: ai\\n+    depends_on: [seed]\\n+    reuse_ai_session: self\\n+    session_mode: append\\n+    ai:\\n+      provider: mock\\n+      model: mock\\n+      disableTools: true\\n+      allowedTools: []\\n+      system_prompt: \\\"You are a tiny echo assistant.\\\"\\n+    prompt: |\\n+      Seed message: {{ outputs['seed'].text }}\\n+\\n+      Past convo outputs in this run:\\n+      {% assign hist = outputs_history['convo'] | default: empty %}\\n+      {% if hist and hist.size > 0 %}\\n+      {% for h in hist %}\\n+      - Previous reply {{ forloop.index }}.\\n+      {% endfor %}\\n+      {% else %}\\n+      - No previous replies yet.\\n+      {% endif %}\\n+    on_success:\\n+      goto_js: |\\n+        // Example: re‚Äëenter this step up to 3 times in a single run\\n+        return attempt < 3 ? 'convo' : null;\\n+```\\n+\\n+The corresponding testable example lives at:\\n+\\n+- `examples/session-reuse-self.yaml`\\n+\\n+This keeps the configuration small but shows how to wire `reuse_ai_session: self` and `session_mode: append` without touching higher‚Äëlevel workflows like `tyk-assistant`.\\n ```\\n \\n **When to use each mode:**\\n\",\"status\":\"modified\"},{\"filename\":\"docs/recipes.md\",\"additions\":149,\"deletions\":3,\"changes\":152,\"patch\":\"diff --git a/docs/recipes.md b/docs/recipes.md\\nindex 1015bc6c..1089e9a5 100644\\n--- a/docs/recipes.md\\n+++ b/docs/recipes.md\\n@@ -321,8 +321,154 @@ Tip: When you define a JSON Schema, you generally do **not** need to tell the mo\\n - Avoid noisy fallbacks like `(outputs['x']?.kind ?? '') === 'status'` when `outputs['x']?.kind === 'status'` is equivalent.\\n - These conventions apply uniformly to any provider (`ai`, `command`, `script`, `github`, `http_client`, etc).\\n \\n+### Command step best practices\\n+\\n+When using `type: command` steps:\\n+\\n+**Avoid external tool dependencies** like `jq`, `yq`, `python`, etc.:\\n+- They may not be installed in all environments (GitHub Actions, Docker, CI)\\n+- Use `transform_js` to parse and transform output instead\\n+- Keep shell commands simple: `grep`, `sed`, `awk`, `sort`, `head` are universally available\\n+\\n+```yaml\\n+# Bad - requires jq\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | jq -R -s 'split(\\\"\\\\n\\\")'\\n+  parseJson: true\\n+\\n+# Good - use transform_js for parsing\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | sort -u\\n+  transform_js: |\\n+    const lines = (output || '').trim().split('\\\\n').filter(Boolean);\\n+    return { data: lines, count: lines.length };\\n+```\\n+\\n+**Prefer line-separated output** over JSON from shell:\\n+- Simple to parse with `transform_js`\\n+- No need for `parseJson: true`\\n+- More robust across different shells/environments\\n+\\n+**Use transform_js for structured output**:\\n+- The sandbox provides `output` (command stdout as string)\\n+- Return an object with the fields you need\\n+- Works consistently across all environments\\n+\\n+### Testing workflows with `--no-mocks`\\n+\\n+The `--no-mocks` flag runs your test cases with real providers instead of injecting mock responses. This is essential for:\\n+\\n+1. **Debugging integration issues** - See actual API responses and errors\\n+2. **Capturing realistic mock data** - Get real output to copy into your test cases\\n+3. **Validating credentials** - Verify environment variables are set correctly\\n+4. **Developing new workflows** - Build tests incrementally with real data\\n+\\n+#### Basic usage\\n+\\n+```bash\\n+# Run all test cases with real providers\\n+visor test --config my-workflow.yaml --no-mocks\\n+\\n+# Run a specific test case with real providers\\n+visor test --config my-workflow.yaml --no-mocks --only \\\"my-test-case\\\"\\n+```\\n+\\n+#### Suggested mocks output\\n+\\n+When running with `--no-mocks`, Visor captures each step's output and prints it as YAML you can copy directly into your test case:\\n+\\n+```\\n+üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)\\n+   Step outputs will be captured and printed as suggested mocks\\n+\\n+... test execution ...\\n+\\n+üìã Suggested mocks (copy to your test case):\\n+mocks:\\n+  extract-keys:\\n+    data:\\n+      - PROJ-123\\n+      - DEV-456\\n+    count: 2\\n+  fetch-issues:\\n+    data:\\n+      - key: PROJ-123\\n+        summary: Fix authentication bug\\n+        status: In Progress\\n+```\\n+\\n+Copy the YAML under `mocks:` into your test case's `mocks:` section.\\n+\\n+#### Workflow for building tests\\n+\\n+1. **Start with a minimal test case** (no mocks):\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+   ```\\n+\\n+2. **Run with `--no-mocks`** to capture real outputs:\\n+   ```bash\\n+   visor test --config workflow.yaml --no-mocks --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+3. **Copy the suggested mocks** into your test case:\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+         mocks:\\n+           extract-keys:\\n+             data: [\\\"PROJ-123\\\"]\\n+             count: 1\\n+           # ... rest of captured mocks\\n+   ```\\n+\\n+4. **Add assertions** based on the real data:\\n+   ```yaml\\n+         expect:\\n+           workflow_output:\\n+             - path: issue_count\\n+               equals: 1\\n+   ```\\n+\\n+5. **Run normally** to verify mocks work:\\n+   ```bash\\n+   visor test --config workflow.yaml --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+#### Debugging with `--no-mocks`\\n+\\n+When a test fails with mocks, use `--no-mocks` to see what's actually happening:\\n+\\n+```bash\\n+# See real API responses and errors\\n+visor test --config workflow.yaml --no-mocks --only \\\"failing-test\\\"\\n+\\n+# Common issues revealed:\\n+# - Missing or expired credentials\\n+# - API endpoint changes\\n+# - Unexpected response formats\\n+# - Network/timeout issues\\n+```\\n+\\n+The real error messages and responses help identify whether the issue is with your mocks or the actual integration.\\n+\\n ### More examples\\n \\n-- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags  \\n-- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration  \\n+- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags\\n+- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration\\n - `examples/` ‚Äì MCP, Jira, and advanced configs\\n\",\"status\":\"modified\"},{\"filename\":\"package-lock.json\",\"additions\":252,\"deletions\":9,\"changes\":261,\"patch\":\"diff --git a/package-lock.json b/package-lock.json\\nindex 5fb3ad89..e5b10d27 100644\\n--- a/package-lock.json\\n+++ b/package-lock.json\\n@@ -7,6 +7,7 @@\\n     \\\"\\\": {\\n       \\\"name\\\": \\\"@probelabs/visor\\\",\\n       \\\"version\\\": \\\"0.1.42\\\",\\n+      \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"MIT\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@actions/core\\\": \\\"^1.11.1\\\",\\n@@ -16,7 +17,7 @@\\n         \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n         \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n         \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n         \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n         \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -54,6 +55,7 @@\\n         \\\"husky\\\": \\\"^9.1.7\\\",\\n         \\\"jest\\\": \\\"^30.1.3\\\",\\n         \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+        \\\"patch-package\\\": \\\"^8.0.1\\\",\\n         \\\"prettier\\\": \\\"^3.6.2\\\",\\n         \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n         \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n@@ -5942,9 +5944,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/maid\\\": {\\n-      \\\"version\\\": \\\"0.0.21\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.21.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-H3EA2tDjWsAPgXZAwfHrNoEXnHmQFS9+JGDK/YpeiF3g3FxPgnWBgtgYPgtNuETQsEXxR1RzDgQVJL2JHWRRHw==\\\",\\n+      \\\"version\\\": \\\"0.0.22\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.22.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-FYcyyP4BlVrHQmBznXFPpMbvUqFI6wbecHUUe4S+yIeDvJ8W4bP0ndZ0cMoc2p7hekWYAG4RtlohQBr8e4TKOg==\\\",\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@types/dagre\\\": \\\"^0.7.53\\\",\\n@@ -5960,9 +5962,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/probe\\\": {\\n-      \\\"version\\\": \\\"0.6.0-rc169\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc169.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-/laV3+1TSjASru8gGSvGwyjQ7jalMkFuZETl35uv37J1Hb4eG95CbKGU5NihcCrLXR44q1lKJiQGHXcAav2mBA==\\\",\\n+      \\\"version\\\": \\\"0.6.0-rc176\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc176.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-w/pAvnz2/yXb4FQXZV2QazxuW02xsI/wRg36yxZoLSG0O/hIncjF4rquZe3O+emceI2Ks5zbrA7cgQEdpwT8vA==\\\",\\n       \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n@@ -5972,7 +5974,7 @@\\n         \\\"@ai-sdk/openai\\\": \\\"^2.0.10\\\",\\n         \\\"@anthropic-ai/claude-agent-sdk\\\": \\\"^0.1.46\\\",\\n         \\\"@modelcontextprotocol/sdk\\\": \\\"^1.0.0\\\",\\n-        \\\"@probelabs/maid\\\": \\\"^0.0.21\\\",\\n+        \\\"@probelabs/maid\\\": \\\"^0.0.22\\\",\\n         \\\"adm-zip\\\": \\\"^0.5.16\\\",\\n         \\\"ai\\\": \\\"^5.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -8192,6 +8194,13 @@\\n         \\\"node\\\": \\\">= 20\\\"\\n       }\\n     },\\n+    \\\"node_modules/@yarnpkg/lockfile\\\": {\\n+      \\\"version\\\": \\\"1.1.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@yarnpkg/lockfile/-/lockfile-1.1.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-GpSwvyXOcOOlV70vbnzjj4fW5xW/FdUF6nQEt1ENy7m4ZCczi1+/buVUPAqmGfqznsORNFzUMjctTIp8a9tuCQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"BSD-2-Clause\\\"\\n+    },\\n     \\\"node_modules/accepts\\\": {\\n       \\\"version\\\": \\\"1.3.8\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/accepts/-/accepts-1.3.8.tgz\\\",\\n@@ -9180,6 +9189,25 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/call-bind\\\": {\\n+      \\\"version\\\": \\\"1.0.8\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind/-/call-bind-1.0.8.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-oKlSFMcMwpUg2ednkhQ454wfWiU/ul3CkJe/PEHcTKuiX6RpbehUiFMXu13HalGZxfUwCQzZG747YXBn1im9ww==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind-apply-helpers\\\": \\\"^1.0.0\\\",\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"set-function-length\\\": \\\"^1.2.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/call-bind-apply-helpers\\\": {\\n       \\\"version\\\": \\\"1.0.2\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind-apply-helpers/-/call-bind-apply-helpers-1.0.2.tgz\\\",\\n@@ -10613,6 +10641,24 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/define-data-property\\\": {\\n+      \\\"version\\\": \\\"1.1.4\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/define-data-property/-/define-data-property-1.1.4.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-rBMvIzlpA8v6E+SJZoo++HAYqsLrkg7MSfIinMPFhmkorw7X+dOXVJQs+QT69zGkzMyfDnIMN2Wid1+NbL3T+A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/define-lazy-prop\\\": {\\n       \\\"version\\\": \\\"3.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/define-lazy-prop/-/define-lazy-prop-3.0.0.tgz\\\",\\n@@ -11794,6 +11840,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/find-yarn-workspace-root\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/find-yarn-workspace-root/-/find-yarn-workspace-root-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-1IMnbjt4KzsQfnhnzNd8wUEgXZ44IzZaZmnLYx7D5FZlaHt2gW20Cri8Q+E/t5tIj4+epTBub+2Zxu/vNILzqQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Apache-2.0\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"micromatch\\\": \\\"^4.0.2\\\"\\n+      }\\n+    },\\n     \\\"node_modules/fix-dts-default-cjs-exports\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/fix-dts-default-cjs-exports/-/fix-dts-default-cjs-exports-1.0.1.tgz\\\",\\n@@ -12267,6 +12323,19 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/has-property-descriptors\\\": {\\n+      \\\"version\\\": \\\"1.0.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/has-property-descriptors/-/has-property-descriptors-1.0.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-55JNKuIW+vq4Ke1BjOTjM2YctQIvCT7GFzHwmfZPGo5wnrgkid0YQtnAleFSqumZm4az3n2BS+erby5ipJdgrg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/has-symbols\\\": {\\n       \\\"version\\\": \\\"1.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/has-symbols/-/has-symbols-1.1.0.tgz\\\",\\n@@ -12784,6 +12853,13 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/isarray\\\": {\\n+      \\\"version\\\": \\\"2.0.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/isarray/-/isarray-2.0.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-xHjhDr3cNBK0BzdUJSPXZntQUx/mwMS5Rw4A7lPJ90XGAO6ISP/ePDNuo0vhqOZU+UD5JoodwCAAoZQd3FeAKw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\"\\n+    },\\n     \\\"node_modules/isexe\\\": {\\n       \\\"version\\\": \\\"2.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz\\\",\\n@@ -13594,6 +13670,26 @@\\n       \\\"integrity\\\": \\\"sha512-NM8/P9n3XjXhIZn1lLhkFaACTOURQXjWhV4BA/RnOv8xvgqtqpAX9IO4mRQxSx1Rlo4tqzeqb0sOlruaOy3dug==\\\",\\n       \\\"license\\\": \\\"MIT\\\"\\n     },\\n+    \\\"node_modules/json-stable-stringify\\\": {\\n+      \\\"version\\\": \\\"1.3.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify/-/json-stable-stringify-1.3.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-qtYiSSFlwot9XHtF9bD9c7rwKjr+RecWT//ZnPvSmEjpV5mmPOCN4j8UjY5hbjNkOwZ/jQv3J6R1/pL7RwgMsg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind\\\": \\\"^1.0.8\\\",\\n+        \\\"call-bound\\\": \\\"^1.0.4\\\",\\n+        \\\"isarray\\\": \\\"^2.0.5\\\",\\n+        \\\"jsonify\\\": \\\"^0.0.1\\\",\\n+        \\\"object-keys\\\": \\\"^1.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/json-stable-stringify-without-jsonify\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify-without-jsonify/-/json-stable-stringify-without-jsonify-1.0.1.tgz\\\",\\n@@ -13633,6 +13729,16 @@\\n         \\\"graceful-fs\\\": \\\"^4.1.6\\\"\\n       }\\n     },\\n+    \\\"node_modules/jsonify\\\": {\\n+      \\\"version\\\": \\\"0.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/jsonify/-/jsonify-0.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-2/Ki0GcmuqSrgFyelQq9M05y7PS0mEwuIzrf3f1fPqkVDVRvZrPZtVSMHxdgo8Aq0sxAOb/cr2aqqA3LeWHVPg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Public Domain\\\",\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/katex\\\": {\\n       \\\"version\\\": \\\"0.16.25\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/katex/-/katex-0.16.25.tgz\\\",\\n@@ -13676,6 +13782,16 @@\\n       \\\"integrity\\\": \\\"sha512-Ls993zuzfayK269Svk9hzpeGUKob/sIgZzyHYdjQoAdQetRKpOLj+k/QQQ/6Qi0Yz65mlROrfd+Ev+1+7dz9Kw==\\\",\\n       \\\"dev\\\": true\\n     },\\n+    \\\"node_modules/klaw-sync\\\": {\\n+      \\\"version\\\": \\\"6.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/klaw-sync/-/klaw-sync-6.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-nIeuVSzdCCs6TDPTqI8w1Yre34sSq7AkZ4B3sfOBbI2CgVSB4Du4aLQijFU2+lhAFCwt9+42Hel6lQNIv6AntQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"graceful-fs\\\": \\\"^4.1.11\\\"\\n+      }\\n+    },\\n     \\\"node_modules/kolorist\\\": {\\n       \\\"version\\\": \\\"1.8.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/kolorist/-/kolorist-1.8.0.tgz\\\",\\n@@ -14785,6 +14901,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n       }\\n     },\\n+    \\\"node_modules/object-keys\\\": {\\n+      \\\"version\\\": \\\"1.1.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/object-keys/-/object-keys-1.1.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NuAESUOUMrlIXOfHKzD6bpPu3tYt3xvjNdRIQ+FeT0lNb4K8WR70CaDxhuNguS2XG+GjkyMwOzsN5ZktImfhLA==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/ohash\\\": {\\n       \\\"version\\\": \\\"2.0.11\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/ohash/-/ohash-2.0.11.tgz\\\",\\n@@ -15022,6 +15148,95 @@\\n         \\\"node\\\": \\\">= 0.8\\\"\\n       }\\n     },\\n+    \\\"node_modules/patch-package\\\": {\\n+      \\\"version\\\": \\\"8.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/patch-package/-/patch-package-8.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-VsKRIA8f5uqHQ7NGhwIna6Bx6D9s/1iXlA1hthBVBEbkq+t4kXD0HHt+rJhf/Z+Ci0F/HCB2hvn0qLdLG+Qxlw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"@yarnpkg/lockfile\\\": \\\"^1.1.0\\\",\\n+        \\\"chalk\\\": \\\"^4.1.2\\\",\\n+        \\\"ci-info\\\": \\\"^3.7.0\\\",\\n+        \\\"cross-spawn\\\": \\\"^7.0.3\\\",\\n+        \\\"find-yarn-workspace-root\\\": \\\"^2.0.0\\\",\\n+        \\\"fs-extra\\\": \\\"^10.0.0\\\",\\n+        \\\"json-stable-stringify\\\": \\\"^1.0.2\\\",\\n+        \\\"klaw-sync\\\": \\\"^6.0.0\\\",\\n+        \\\"minimist\\\": \\\"^1.2.6\\\",\\n+        \\\"open\\\": \\\"^7.4.2\\\",\\n+        \\\"semver\\\": \\\"^7.5.3\\\",\\n+        \\\"slash\\\": \\\"^2.0.0\\\",\\n+        \\\"tmp\\\": \\\"^0.2.4\\\",\\n+        \\\"yaml\\\": \\\"^2.2.2\\\"\\n+      },\\n+      \\\"bin\\\": {\\n+        \\\"patch-package\\\": \\\"index.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14\\\",\\n+        \\\"npm\\\": \\\">5\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/ci-info\\\": {\\n+      \\\"version\\\": \\\"3.9.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/ci-info/-/ci-info-3.9.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NIxF55hv4nSqQswkAeiOi1r83xy8JldOFDTWiug55KBu9Jnblncd2U6ViHmYgHf01TPZS77NJBhBMKdWj9HQMQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"funding\\\": [\\n+        {\\n+          \\\"type\\\": \\\"github\\\",\\n+          \\\"url\\\": \\\"https://github.com/sponsors/sibiraj-s\\\"\\n+        }\\n+      ],\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/is-docker\\\": {\\n+      \\\"version\\\": \\\"2.2.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/is-docker/-/is-docker-2.2.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-F+i2BKsFrH66iaUFc0woD8sLy8getkwTwtOBjvs56Cx4CgJDeKQeqfz8wAYiSb8JOprWhHH5p77PbmYCvvUuXQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"bin\\\": {\\n+        \\\"is-docker\\\": \\\"cli.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/open\\\": {\\n+      \\\"version\\\": \\\"7.4.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/open/-/open-7.4.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-MVHddDVweXZF3awtlAS+6pgKLlm/JgxZ90+/NBurBoQctVOOB/zDdVjcyPzQ+0laDGbsWgrRkflI65sQeOgT9Q==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"is-docker\\\": \\\"^2.0.0\\\",\\n+        \\\"is-wsl\\\": \\\"^2.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/slash\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/slash/-/slash-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-ZYKh3Wh2z1PpEXWr0MpSBZ0V6mZHAQfYevttO11c51CaWjGTaadiKZ+wVt1PbMlDV5qhMFslpZCemhwOK7C89A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=6\\\"\\n+      }\\n+    },\\n     \\\"node_modules/path-data-parser\\\": {\\n       \\\"version\\\": \\\"0.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/path-data-parser/-/path-data-parser-0.1.0.tgz\\\",\\n@@ -16473,6 +16688,24 @@\\n         \\\"node\\\": \\\">= 0.8.0\\\"\\n       }\\n     },\\n+    \\\"node_modules/set-function-length\\\": {\\n+      \\\"version\\\": \\\"1.2.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/set-function-length/-/set-function-length-1.2.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-pgRc4hJ4/sNjWCSS9AmnS40x3bNMDTknHgL5UaMBTMyJnU90EgWh1Rz+MC9eFu4BuN/UwZjKQuY/1v3rM7HMfg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"define-data-property\\\": \\\"^1.1.4\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"function-bind\\\": \\\"^1.1.2\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\",\\n+        \\\"has-property-descriptors\\\": \\\"^1.0.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/setprototypeof\\\": {\\n       \\\"version\\\": \\\"1.2.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/setprototypeof/-/setprototypeof-1.2.0.tgz\\\",\\n@@ -17384,6 +17617,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/tmp\\\": {\\n+      \\\"version\\\": \\\"0.2.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/tmp/-/tmp-0.2.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-voyz6MApa1rQGUxT3E+BK7/ROe8itEx7vD8/HEvt4xwXucvQ5G5oeEiHkmHZJuBO21RpOf+YYm9MOivj709jow==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14.14\\\"\\n+      }\\n+    },\\n     \\\"node_modules/tmpl\\\": {\\n       \\\"version\\\": \\\"1.0.5\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/tmpl/-/tmpl-1.0.5.tgz\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"package.json\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/package.json b/package.json\\nindex f9ccbb6c..302d728f 100644\\n--- a/package.json\\n+++ b/package.json\\n@@ -49,6 +49,7 @@\\n     \\\"test:with-build\\\": \\\"npm run build:cli && jest\\\",\\n     \\\"test:yaml\\\": \\\"node dist/index.js test --progress compact\\\",\\n     \\\"prepare\\\": \\\"husky\\\",\\n+    \\\"postinstall\\\": \\\"patch-package\\\",\\n     \\\"pre-commit\\\": \\\"lint-staged\\\",\\n     \\\"deploy:site\\\": \\\"cd site && npx wrangler pages deploy . --project-name=visor-site --commit-dirty=true\\\",\\n     \\\"deploy:worker\\\": \\\"npx wrangler deploy\\\",\\n@@ -96,7 +97,7 @@\\n     \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n     \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n     \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n     \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n     \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n     \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -143,6 +144,7 @@\\n     \\\"husky\\\": \\\"^9.1.7\\\",\\n     \\\"jest\\\": \\\"^30.1.3\\\",\\n     \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+    \\\"patch-package\\\": \\\"^8.0.1\\\",\\n     \\\"prettier\\\": \\\"^3.6.2\\\",\\n     \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n     \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"src/ai-review-service.ts\",\"additions\":101,\"deletions\":151,\"changes\":252,\"patch\":\"diff --git a/src/ai-review-service.ts b/src/ai-review-service.ts\\nindex 6b9c5b2e..f18accea 100644\\n--- a/src/ai-review-service.ts\\n+++ b/src/ai-review-service.ts\\n@@ -65,6 +65,13 @@ export interface AIReviewConfig {\\n   allowBash?: boolean;\\n   // Advanced bash command execution configuration\\n   bashConfig?: import('./types/config').BashConfig;\\n+  // Optional workspace root and allowed folders for ProbeAgent.\\n+  // When provided, these are forwarded to ProbeAgent so tools like search/query\\n+  // operate inside the isolated workspace/projects instead of the Visor repo root.\\n+  path?: string;\\n+  allowedFolders?: string[];\\n+  // Completion prompt for post-completion validation/review (runs after attempt_completion)\\n+  completionPrompt?: string;\\n }\\n \\n export interface AIDebugInfo {\\n@@ -278,7 +285,7 @@ export class AIReviewService {\\n     try {\\n       const call = this.callProbeAgent(prompt, schema, debugInfo, checkName, sessionId);\\n       const timeoutMs = Math.max(0, this.config.timeout || 0);\\n-      const { response, effectiveSchema } =\\n+      const { response, effectiveSchema, sessionId: usedSessionId } =\\n         timeoutMs > 0 ? await this.withTimeout(call, timeoutMs, 'AI review') : await call;\\n       const processingTime = Date.now() - startTime;\\n \\n@@ -290,6 +297,11 @@ export class AIReviewService {\\n \\n       const result = this.parseAIResponse(response, debugInfo, effectiveSchema);\\n \\n+      // Expose the session ID used for this call so the engine can reuse it later\\n+      try {\\n+        (result as any).sessionId = usedSessionId;\\n+      } catch {}\\n+\\n       if (debugInfo) {\\n         result.debug = debugInfo;\\n       }\\n@@ -651,17 +663,34 @@ ${prContext}${slackContextXml}\\n     const prContextInfo = prInfo as PRInfo & {\\n       isPRContext?: boolean;\\n       includeCodeContext?: boolean;\\n+      slackConversation?: unknown;\\n     };\\n     const isIssue = prContextInfo.isIssue === true;\\n \\n     // Check if we should include code context (diffs)\\n     const isPRContext = prContextInfo.isPRContext === true;\\n-    // In PR context, always include diffs. Otherwise check the flag.\\n-    const includeCodeContext = isPRContext || prContextInfo.includeCodeContext !== false;\\n+    const isSlackMode = prContextInfo.slackConversation !== undefined;\\n+\\n+    // Determine whether to include code context:\\n+    // - In explicit PR context (GitHub PR events), always include diffs\\n+    // - In Slack mode, default to NO code context unless explicitly requested\\n+    // - Otherwise, include code context unless explicitly disabled\\n+    let includeCodeContext: boolean;\\n+    if (isPRContext) {\\n+      includeCodeContext = true;\\n+    } else if (isSlackMode) {\\n+      // In Slack mode, only include code context if explicitly set to true\\n+      includeCodeContext = prContextInfo.includeCodeContext === true;\\n+    } else {\\n+      // Default: include unless explicitly disabled\\n+      includeCodeContext = prContextInfo.includeCodeContext !== false;\\n+    }\\n \\n     // Log the decision for transparency (debug level)\\n     if (isPRContext) {\\n       log('üîç Including full code diffs in AI context (PR mode)');\\n+    } else if (isSlackMode && !includeCodeContext) {\\n+      log('üí¨ Slack mode: excluding code diffs (use includeCodeContext: true to enable)');\\n     } else if (!includeCodeContext) {\\n       log('üìä Including only file summary in AI context (no diffs)');\\n     } else {\\n@@ -1537,7 +1566,15 @@ ${'='.repeat(60)}\\n     debugInfo?: AIDebugInfo,\\n     _checkName?: string,\\n     providedSessionId?: string\\n-  ): Promise<{ response: string; effectiveSchema?: string }> {\\n+  ): Promise<{ response: string; effectiveSchema?: string; sessionId: string }> {\\n+    // Derive a stable session ID for this call so the engine can reuse it later\\n+    const sessionId =\\n+      providedSessionId ||\\n+      (() => {\\n+        const timestamp = new Date().toISOString();\\n+        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n+      })();\\n+\\n     // Handle mock model/provider\\n     if (this.config.model === 'mock' || this.config.provider === 'mock') {\\n       const inJest = !!process.env.JEST_WORKER_ID;\\n@@ -1545,19 +1582,17 @@ ${'='.repeat(60)}\\n       if (!inJest) {\\n         // Fast path for CLI/integration: synthesize a mock response without invoking ProbeAgent\\n         const response = await this.generateMockResponse(prompt, _checkName, schema);\\n-        return { response, effectiveSchema: typeof schema === 'object' ? 'custom' : schema };\\n+        return {\\n+          response,\\n+          effectiveSchema: typeof schema === 'object' ? 'custom' : schema,\\n+          sessionId,\\n+        };\\n       }\\n       // In unit tests, still invoke ProbeAgent so tests can assert on options (schema) passed in\\n       // Fall through to normal flow below\\n     }\\n \\n     // Create ProbeAgent instance with proper options\\n-    const sessionId =\\n-      providedSessionId ||\\n-      (() => {\\n-        const timestamp = new Date().toISOString();\\n-        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n-      })();\\n \\n     log('ü§ñ Creating ProbeAgent for AI review...');\\n     log(`üÜî Session ID: ${sessionId}`);\\n@@ -1674,6 +1709,35 @@ ${'='.repeat(60)}\\n         (options as any).bashConfig = this.config.bashConfig;\\n       }\\n \\n+      // Pass completion prompt for post-completion validation/review\\n+      if (this.config.completionPrompt !== undefined) {\\n+        (options as any).completionPrompt = this.config.completionPrompt;\\n+      }\\n+\\n+      // Propagate workspace / allowed folders to ProbeAgent so that tools\\n+      // operate inside the isolated workspace and project checkouts instead\\n+      // of the Visor repository root.\\n+      try {\\n+        const cfgAny: any = this.config as any;\\n+        const allowedFolders = cfgAny.allowedFolders as string[] | undefined;\\n+        const workspacePath =\\n+          cfgAny.workspacePath || cfgAny.path || (Array.isArray(allowedFolders) && allowedFolders[0]);\\n+        if (Array.isArray(allowedFolders) && allowedFolders.length > 0) {\\n+          (options as any).allowedFolders = allowedFolders;\\n+          if (!options.path && workspacePath) {\\n+            (options as any).path = workspacePath;\\n+          }\\n+          log(`üóÇÔ∏è ProbeAgent workspace config:`);\\n+          log(`   path (cwd): ${(options as any).path}`);\\n+          log(`   allowedFolders[0]: ${allowedFolders[0]}`);\\n+        } else if (workspacePath) {\\n+          (options as any).path = workspacePath;\\n+          log(`üóÇÔ∏è ProbeAgent path: ${workspacePath} (no allowedFolders)`);\\n+        }\\n+      } catch {\\n+        // Best-effort only; fall back to ProbeAgent defaults on error.\\n+      }\\n+\\n       // Add provider-specific options if configured\\n       if (this.config.provider) {\\n         // Map claude-code to anthropic for ProbeAgent compatibility\\n@@ -2037,7 +2101,7 @@ ${'='.repeat(60)}\\n         log(`üîß Debug: Registered AI session for potential reuse: ${sessionId}`);\\n       }\\n \\n-      return { response, effectiveSchema };\\n+      return { response, effectiveSchema, sessionId };\\n     } catch (error) {\\n       console.error('‚ùå ProbeAgent failed:', error);\\n       throw new Error(\\n@@ -2191,13 +2255,23 @@ ${'='.repeat(60)}\\n         // For other schemas (code-review, etc.), extract and parse JSON with boundary detection\\n         log('üîç Extracting JSON from AI response...');\\n \\n-        // Try direct parsing first - if AI returned pure JSON\\n+        // Sanitize response: strip BOM, zero-width chars, and other invisible characters\\n+        // that can cause JSON parsing to fail even when the text looks valid\\n+        const sanitizedResponse = response\\n+          .replace(/^\\\\uFEFF/, '') // BOM\\n+          .replace(/[\\\\u200B-\\\\u200D\\\\uFEFF\\\\u00A0]/g, '') // Zero-width chars, NBSP\\n+          .replace(/[\\\\x00-\\\\x08\\\\x0B\\\\x0C\\\\x0E-\\\\x1F\\\\x7F]/g, '') // Control chars (except \\\\t \\\\n \\\\r)\\n+          .trim();\\n+\\n+        // Try direct JSON parsing - no bracket-matching extraction\\n+        // JSON validation is offloaded to Probe agent when schema is provided\\n         try {\\n-          reviewData = JSON.parse(response.trim());\\n+          reviewData = JSON.parse(sanitizedResponse);\\n           log('‚úÖ Successfully parsed direct JSON response');\\n           if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-        } catch {\\n-          log('üîç Direct parsing failed, trying to extract JSON from response...');\\n+        } catch (parseErr) {\\n+          const errMsg = parseErr instanceof Error ? parseErr.message : String(parseErr);\\n+          log(`üîç Direct JSON parsing failed: ${errMsg}`);\\n \\n           // If the response starts with \\\"I cannot\\\" or similar, it's likely a refusal\\n           if (\\n@@ -2210,66 +2284,16 @@ ${'='.repeat(60)}\\n             };\\n           }\\n \\n-          // Try to extract JSON using improved method with proper bracket matching\\n-          const jsonString = this.extractJsonFromResponse(response);\\n-\\n-          if (jsonString) {\\n-            try {\\n-              reviewData = JSON.parse(jsonString);\\n-              log('‚úÖ Successfully parsed extracted JSON');\\n-              if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-            } catch {\\n-              log('üîß Extracted JSON parsing failed, falling back to plain text handling...');\\n-\\n-              // Check if response is plain text and doesn't contain structured data\\n-              if (!response.includes('{') && !response.includes('}')) {\\n-                log('üîß Plain text response detected, creating structured fallback...');\\n-\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              } else {\\n-                // Fallback: treat the entire response as an issue\\n-                log('üîß Creating fallback response from non-JSON content...');\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              }\\n-            }\\n-          } else {\\n-            // No JSON found at all - treat as plain text response\\n-            log('üîß No JSON found in response, treating as plain text...');\\n-            reviewData = {\\n-              issues: [\\n-                {\\n-                  file: 'AI_RESPONSE',\\n-                  line: 1,\\n-                  ruleId: 'ai/raw_response',\\n-                  message: response,\\n-                  severity: 'info',\\n-                  category: 'documentation',\\n-                },\\n-              ],\\n-            };\\n-          }\\n+          // Not valid JSON - treat entire response as text output\\n+          // This allows Probe (or other AI providers) to handle JSON validation\\n+          // and avoids false positives from bracket-matching (e.g., mermaid diagrams)\\n+          log('üîß Treating response as plain text (no JSON extraction)');\\n+          const trimmed = response.trim();\\n+          return {\\n+            issues: [],\\n+            output: { text: trimmed },\\n+            debug: debugInfo,\\n+          };\\n         }\\n       }\\n \\n@@ -2457,80 +2481,6 @@ ${'='.repeat(60)}\\n     }\\n   }\\n \\n-  /**\\n-   * Extract JSON from a response that might contain surrounding text\\n-   * Uses proper bracket matching to find valid JSON objects or arrays\\n-   */\\n-  private extractJsonFromResponse(response: string): string | null {\\n-    const text = response.trim();\\n-\\n-    // Try to find JSON objects first (higher priority)\\n-    let bestJson = this.findJsonWithBracketMatching(text, '{', '}');\\n-\\n-    // If no object found, try arrays\\n-    if (!bestJson) {\\n-      bestJson = this.findJsonWithBracketMatching(text, '[', ']');\\n-    }\\n-\\n-    return bestJson;\\n-  }\\n-\\n-  /**\\n-   * Find JSON with proper bracket matching to avoid false positives\\n-   */\\n-  private findJsonWithBracketMatching(\\n-    text: string,\\n-    openChar: string,\\n-    closeChar: string\\n-  ): string | null {\\n-    const firstIndex = text.indexOf(openChar);\\n-    if (firstIndex === -1) return null;\\n-\\n-    let depth = 0;\\n-    let inString = false;\\n-    let escaping = false;\\n-\\n-    for (let i = firstIndex; i < text.length; i++) {\\n-      const char = text[i];\\n-\\n-      if (escaping) {\\n-        escaping = false;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\\\\\\' && inString) {\\n-        escaping = true;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\"' && !escaping) {\\n-        inString = !inString;\\n-        continue;\\n-      }\\n-\\n-      if (!inString) {\\n-        if (char === openChar) {\\n-          depth++;\\n-        } else if (char === closeChar) {\\n-          depth--;\\n-          if (depth === 0) {\\n-            // Found matching closing bracket\\n-            const candidate = text.substring(firstIndex, i + 1);\\n-            try {\\n-              JSON.parse(candidate); // Validate it's actually valid JSON\\n-              return candidate;\\n-            } catch {\\n-              // This wasn't valid JSON, keep looking\\n-              break;\\n-            }\\n-          }\\n-        }\\n-      }\\n-    }\\n-\\n-    return null;\\n-  }\\n-\\n   /**\\n    * Generate mock response for testing\\n    */\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli-main.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/src/cli-main.ts b/src/cli-main.ts\\nindex 7dbef501..e671fbd8 100644\\n--- a/src/cli-main.ts\\n+++ b/src/cli-main.ts\\n@@ -158,6 +158,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n   }\\n   const only = getArg('--only');\\n   const bail = hasFlag('--bail');\\n+  const noMocks = hasFlag('--no-mocks');\\n   const listOnly = hasFlag('--list');\\n   const validateOnly = hasFlag('--validate');\\n   const progress = (getArg('--progress') as 'compact' | 'detailed' | undefined) || 'compact';\\n@@ -224,6 +225,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       const agg = await runSuites(multiFiles, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallelSuites: maxParallelSuites || Math.max(1, require('os').cpus()?.length || 2),\\n         maxParallel,\\n         promptMaxChars,\\n@@ -340,6 +342,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       runRes = await runner.runCases(tpath, suite, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallel,\\n         promptMaxChars,\\n         engineMode: 'state-machine',\\n@@ -609,6 +612,9 @@ export async function main(): Promise<void> {\\n     // Set environment variables early for proper logging in all modules\\n     process.env.VISOR_OUTPUT_FORMAT = options.output;\\n     process.env.VISOR_DEBUG = options.debug ? 'true' : 'false';\\n+    if (options.keepWorkspace) {\\n+      process.env.VISOR_KEEP_WORKSPACE = 'true';\\n+    }\\n     // Configure centralized logger\\n     configureLoggerFromCli({\\n       output: options.output,\\n@@ -716,6 +722,31 @@ export async function main(): Promise<void> {\\n       const threads = slackAny.threads || 'any';\\n       const allow = Array.isArray(slackAny.channel_allowlist) ? slackAny.channel_allowlist : [];\\n       const appToken = slackAny.app_token || process.env.SLACK_APP_TOKEN;\\n+\\n+      // Initialize telemetry for Slack mode (normally done later for CLI runs).\\n+      if ((config as any)?.telemetry) {\\n+        const t = (config as any).telemetry as {\\n+          enabled?: boolean;\\n+          sink?: 'otlp' | 'file' | 'console';\\n+          file?: { dir?: string; ndjson?: boolean };\\n+          tracing?: { auto_instrumentations?: boolean; trace_report?: { enabled?: boolean } };\\n+        };\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true' || !!t?.enabled,\\n+          sink:\\n+            (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || t?.sink || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR || t?.file?.dir, ndjson: !!t?.file?.ndjson },\\n+          autoInstrument: !!t?.tracing?.auto_instrumentations,\\n+          traceReport: !!t?.tracing?.trace_report?.enabled,\\n+        });\\n+      } else {\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true',\\n+          sink: (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR },\\n+        });\\n+      }\\n+\\n       const runner = new SlackSocketRunner(engine, config, {\\n         appToken,\\n         endpoint,\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli.ts\",\"additions\":5,\"deletions\":1,\"changes\":6,\"patch\":\"diff --git a/src/cli.ts b/src/cli.ts\\nindex 134429b2..b1c46213 100644\\n--- a/src/cli.ts\\n+++ b/src/cli.ts\\n@@ -76,6 +76,7 @@ export class CLI {\\n         parseInt(value, 10)\\n       )\\n       .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText())\\n       .exitOverride(); // Prevent automatic process.exit for better error handling\\n \\n@@ -154,6 +155,7 @@ export class CLI {\\n           parseInt(value, 10)\\n         )\\n         .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+        .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n         .allowUnknownOption(false)\\n         .allowExcessArguments(false) // Don't allow positional arguments\\n         .addHelpText('after', this.getExamplesText())\\n@@ -224,6 +226,7 @@ export class CLI {\\n         message: options.message,\\n         githubV2: false,\\n         slack: Boolean(options.slack),\\n+        keepWorkspace: Boolean(options.keepWorkspace),\\n       };\\n     } catch (error: unknown) {\\n       // Handle commander.js exit overrides for help/version ONLY\\n@@ -348,6 +351,7 @@ export class CLI {\\n       .option('--debug-port <port>', 'Port for debug server (default: 3456)', value =>\\n         parseInt(value, 10)\\n       )\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText());\\n \\n     // Get the basic help and append examples manually if addHelpText doesn't work\\n\",\"status\":\"modified\"},{\"filename\":\"src/config.ts\",\"additions\":67,\"deletions\":17,\"changes\":84,\"patch\":\"diff --git a/src/config.ts b/src/config.ts\\nindex 407efa2d..02abe9df 100644\\n--- a/src/config.ts\\n+++ b/src/config.ts\\n@@ -20,6 +20,7 @@ import { ConfigLoader, ConfigLoaderOptions } from './utils/config-loader';\\n import { ConfigMerger } from './utils/config-merger';\\n import Ajv from 'ajv';\\n import addFormats from 'ajv-formats';\\n+import { validateJsSyntax } from './utils/sandbox';\\n \\n /**\\n  * Valid event triggers for checks\\n@@ -477,16 +478,29 @@ export class ConfigManager {\\n \\n     logger.info(`Registered workflow '${workflowId}' for standalone execution`);\\n \\n-    // Create a COMPLETELY NEW visor config with ONLY the test checks\\n-    // This prevents any workflow fields from leaking into the config\\n-    const visorConfig: Partial<VisorConfig> = {\\n+    // For standalone workflow testing, use the workflow's steps as checks\\n+    // This allows the test runner to execute the workflow steps directly\\n+    const workflowSteps = workflowData.steps || {};\\n+\\n+    // Create a config that includes the workflow steps as checks,\\n+    // plus the tests section and workflow metadata for output computation\\n+    const visorConfig: Partial<VisorConfig> & { tests?: any; outputs?: any; inputs?: any } = {\\n       version: '1.0',\\n-      steps: tests,\\n-      checks: tests, // Backward compatibility\\n+      steps: workflowSteps,\\n+      checks: workflowSteps,\\n+      tests: tests, // Preserve test harness config (may be empty if stripped by test runner)\\n     };\\n \\n-    logger.debug(`Standalone workflow config has ${Object.keys(tests).length} test checks`);\\n-    logger.debug(`Test check names: ${Object.keys(tests).join(', ')}`);\\n+    // Preserve workflow metadata for output computation during tests\\n+    if (workflowData.outputs) {\\n+      visorConfig.outputs = workflowData.outputs;\\n+    }\\n+    if (workflowData.inputs) {\\n+      visorConfig.inputs = workflowData.inputs;\\n+    }\\n+\\n+    logger.debug(`Standalone workflow config has ${Object.keys(workflowSteps).length} workflow steps as checks`);\\n+    logger.debug(`Workflow step names: ${Object.keys(workflowSteps).join(', ')}`);\\n     logger.debug(`Config keys after conversion: ${Object.keys(visorConfig).join(', ')}`);\\n \\n     return visorConfig;\\n@@ -982,26 +996,29 @@ export class ConfigManager {\\n \\n     // Validate reuse_ai_session configuration\\n     if (checkConfig.reuse_ai_session !== undefined) {\\n-      const isString = typeof checkConfig.reuse_ai_session === 'string';\\n-      const isBoolean = typeof checkConfig.reuse_ai_session === 'boolean';\\n+      const reuseValue = checkConfig.reuse_ai_session as unknown;\\n+      const isString = typeof reuseValue === 'string';\\n+      const isBoolean = typeof reuseValue === 'boolean';\\n+      const isSelf = reuseValue === 'self';\\n \\n       if (!isString && !isBoolean) {\\n         errors.push({\\n           field: `checks.${checkName}.reuse_ai_session`,\\n           message: `Invalid reuse_ai_session value for \\\"${checkName}\\\": must be string (check name) or boolean`,\\n-          value: checkConfig.reuse_ai_session,\\n+          value: reuseValue,\\n         });\\n-      } else if (isString) {\\n-        // When reuse_ai_session is a string, it must refer to a valid check\\n-        const targetCheckName = checkConfig.reuse_ai_session as string;\\n+      } else if (isString && !isSelf) {\\n+        // When reuse_ai_session is a string (other than the special 'self' value),\\n+        // it must refer to a valid check name\\n+        const targetCheckName = reuseValue as string;\\n         if (!config?.checks || !config.checks[targetCheckName]) {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" references non-existent check \\\"${targetCheckName}\\\" for session reuse`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n-      } else if (checkConfig.reuse_ai_session === true) {\\n+      } else if (reuseValue === true) {\\n         // When reuse_ai_session is true, depends_on must be specified and non-empty\\n         if (\\n           !checkConfig.depends_on ||\\n@@ -1011,7 +1028,7 @@ export class ConfigManager {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" has reuse_ai_session=true but missing or empty depends_on. Session reuse requires dependency on another check.`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n       }\\n@@ -1076,6 +1093,39 @@ export class ConfigManager {\\n         });\\n       }\\n     }\\n+\\n+    // Validate JavaScript syntax in transform_js and script content\\n+    try {\\n+      // Validate transform_js if present\\n+      const transformJs = (checkConfig as any).transform_js;\\n+      if (typeof transformJs === 'string' && transformJs.trim().length > 0) {\\n+        const result = validateJsSyntax(transformJs);\\n+        if (!result.valid) {\\n+          errors.push({\\n+            field: `checks.${checkName}.transform_js`,\\n+            message: `JavaScript syntax error in \\\"${checkName}\\\" transform_js: ${result.error}`,\\n+            value: transformJs.slice(0, 100) + (transformJs.length > 100 ? '...' : ''),\\n+          });\\n+        }\\n+      }\\n+\\n+      // Validate script content if type is 'script'\\n+      if (checkConfig.type === 'script') {\\n+        const content = (checkConfig as any).content;\\n+        if (typeof content === 'string' && content.trim().length > 0) {\\n+          const result = validateJsSyntax(content);\\n+          if (!result.valid) {\\n+            errors.push({\\n+              field: `checks.${checkName}.content`,\\n+              message: `JavaScript syntax error in \\\"${checkName}\\\" script: ${result.error}`,\\n+              value: content.slice(0, 100) + (content.length > 100 ? '...' : ''),\\n+            });\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Syntax validation is best-effort; don't fail the whole config on validation errors\\n+    }\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/failure-condition-evaluator.ts\",\"additions\":25,\"deletions\":9,\"changes\":34,\"patch\":\"diff --git a/src/failure-condition-evaluator.ts b/src/failure-condition-evaluator.ts\\nindex 3b54a015..5800df0a 100644\\n--- a/src/failure-condition-evaluator.ts\\n+++ b/src/failure-condition-evaluator.ts\\n@@ -137,6 +137,8 @@ export class FailureConditionEvaluator {\\n       previousResults?: Map<string, ReviewSummary>;\\n       authorAssociation?: string;\\n       workflowInputs?: Record<string, unknown>;\\n+      /** Current step's output for guarantee evaluation */\\n+      output?: unknown;\\n     }\\n   ): Promise<boolean> {\\n     // Build context for if evaluation\\n@@ -177,10 +179,10 @@ export class FailureConditionEvaluator {\\n       // Workflow inputs (for workflows)\\n       inputs: contextData?.workflowInputs || {},\\n \\n-      // Required output property (empty for if conditions)\\n-      output: {\\n-        issues: [],\\n-      },\\n+      // Output property: use provided output for guarantee evaluation, or empty for if conditions\\n+      output: contextData?.output !== undefined && contextData.output !== null && typeof contextData.output === 'object'\\n+        ? (contextData.output as Record<string, unknown>)\\n+        : { issues: [] },\\n       // Author association (used by permission helpers)\\n       authorAssociation: contextData?.authorAssociation,\\n \\n@@ -213,10 +215,10 @@ export class FailureConditionEvaluator {\\n               m && typeof m.get === 'function' ? m.get('all_valid', 'fact-validation') : undefined;\\n             memStr = ` mem.fact-validation.all_valid=${String(v)}`;\\n           } catch {}\\n+          // Debug if-eval output (only when VISOR_DEBUG enabled)\\n+          const outputKeys = Object.keys(context.outputs || {});\\n           console.error(\\n-            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" env.ENABLE_FACT_VALIDATION=${String(\\n-              (envMap as any).ENABLE_FACT_VALIDATION\\n-            )} event=${context.event?.event_name} result=${String(res)}${memStr}`\\n+            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" result=${String(res)} outputKeys=[${outputKeys.join(',')}]`\\n           );\\n         }\\n       } catch {}\\n@@ -480,7 +482,18 @@ export class FailureConditionEvaluator {\\n \\n       // Extract context variables\\n       const output = context.output || {};\\n-      const issues = output.issues || [];\\n+      // Ensure issues is an array - it might be a JSON string from workflow outputs\\n+      let issues = output.issues || [];\\n+      if (typeof issues === 'string') {\\n+        try {\\n+          issues = JSON.parse(issues);\\n+        } catch {\\n+          issues = [];\\n+        }\\n+      }\\n+      if (!Array.isArray(issues)) {\\n+        issues = [];\\n+      }\\n \\n       // Backward compatibility: provide metadata for transition period\\n       // TODO: Remove after all configurations are updated\\n@@ -518,6 +531,7 @@ export class FailureConditionEvaluator {\\n       const event = context.event || 'manual';\\n       const env = context.env || {};\\n       const outputs = context.outputs || {};\\n+      const inputs = context.inputs || {};\\n       const debugData = context.debug || null;\\n \\n       // Get memory store and create accessor for fail_if expressions\\n@@ -555,6 +569,7 @@ export class FailureConditionEvaluator {\\n         filesCount,\\n         event,\\n         env,\\n+        inputs,\\n         // Helper functions\\n         contains,\\n         startsWith,\\n@@ -621,6 +636,7 @@ export class FailureConditionEvaluator {\\n             filesCount,\\n             event,\\n             env,\\n+            inputs,\\n             // Helpers\\n             contains,\\n             startsWith,\\n\",\"status\":\"modified\"},{\"filename\":\"src/frontends/slack-frontend.ts\",\"additions\":107,\"deletions\":6,\"changes\":113,\"patch\":\"diff --git a/src/frontends/slack-frontend.ts b/src/frontends/slack-frontend.ts\\nindex 97161a7d..41735aa2 100644\\n--- a/src/frontends/slack-frontend.ts\\n+++ b/src/frontends/slack-frontend.ts\\n@@ -1,6 +1,32 @@\\n+/**\\n+ * Slack Frontend for Visor workflows.\\n+ *\\n+ * Features:\\n+ * - Posts AI replies to Slack threads\\n+ * - Converts Markdown to Slack mrkdwn format\\n+ * - Renders mermaid diagrams to PNG and uploads as images\\n+ * - Manages üëÄ/üëç reactions for acknowledgement\\n+ * - Handles human input prompts via prompt-state\\n+ *\\n+ * Mermaid Diagram Rendering:\\n+ * - Detects ```mermaid code blocks in AI responses\\n+ * - Renders to PNG using @mermaid-js/mermaid-cli (mmdc)\\n+ * - Uploads rendered images to Slack thread\\n+ * - Replaces mermaid blocks with \\\"_(See diagram above)_\\\" placeholder\\n+ *\\n+ * Requirements for mermaid rendering:\\n+ * - Node.js and npx in PATH\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ * - On Linux: apt-get install chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ */\\n import type { Frontend, FrontendContext } from './host';\\n import { SlackClient } from '../slack/client';\\n-import { formatSlackText } from '../slack/markdown';\\n+import {\\n+  formatSlackText,\\n+  extractMermaidDiagrams,\\n+  renderMermaidToPng,\\n+  replaceMermaidBlocks,\\n+} from '../slack/markdown';\\n \\n type SlackFrontendConfig = {\\n   defaultChannel?: string;\\n@@ -284,6 +310,9 @@ export class SlackFrontend implements Frontend {\\n       const isLogChat = providerType === 'log' && checkCfg.group === 'chat';\\n       if (!isAi && !isLogChat) return;\\n \\n+      // Skip internal steps - they're intermediate processing and shouldn't post to Slack\\n+      if (checkCfg.criticality === 'internal') return;\\n+\\n       // For AI checks, only post when using simple/unstructured schemas (or none).\\n       if (isAi) {\\n         const schema = checkCfg.schema;\\n@@ -316,6 +345,13 @@ export class SlackFrontend implements Frontend {\\n         ) {\\n           text = (result as any).content.trim();\\n         }\\n+      } else if (isLogChat && typeof (result as any)?.logOutput === 'string') {\\n+        // For log-based chat checks, render the formatted log output as the\\n+        // Slack message when no structured text field is present.\\n+        const raw = (result as any).logOutput;\\n+        if (raw.trim().length > 0) {\\n+          text = raw.trim();\\n+        }\\n       } else if (isAi && showRawOutput && out !== undefined) {\\n         try {\\n           text = JSON.stringify(out, null, 2);\\n@@ -325,13 +361,78 @@ export class SlackFrontend implements Frontend {\\n       }\\n       if (!text) return;\\n \\n-      const formattedText = formatSlackText(text);\\n+      // Extract and render mermaid diagrams before posting\\n+      const diagrams = extractMermaidDiagrams(text);\\n+      let processedText = text;\\n+\\n+      if (diagrams.length > 0) {\\n+        try {\\n+          ctx.logger.info(\\n+            `[slack-frontend] found ${diagrams.length} mermaid diagram(s) to render for ${checkId}`\\n+          );\\n+        } catch {}\\n+\\n+        // Render and upload each diagram\\n+        const uploadedCount: number[] = [];\\n+        for (let i = 0; i < diagrams.length; i++) {\\n+          const diagram = diagrams[i];\\n+          try {\\n+            ctx.logger.info(`[slack-frontend] rendering mermaid diagram ${i + 1}...`);\\n+            const pngBuffer = await renderMermaidToPng(diagram.code);\\n+            if (pngBuffer) {\\n+              ctx.logger.info(\\n+                `[slack-frontend] rendered diagram ${i + 1}, size=${pngBuffer.length} bytes, uploading...`\\n+              );\\n+              const filename = `diagram-${i + 1}.png`;\\n+              const uploadResult = await slack.files.uploadV2({\\n+                content: pngBuffer,\\n+                filename,\\n+                channel,\\n+                thread_ts: threadTs,\\n+                title: `Diagram ${i + 1}`,\\n+              });\\n+              if (uploadResult.ok) {\\n+                uploadedCount.push(i);\\n+                ctx.logger.info(`[slack-frontend] uploaded mermaid diagram ${i + 1} to ${channel}`);\\n+              } else {\\n+                ctx.logger.warn(`[slack-frontend] upload failed for diagram ${i + 1}`);\\n+              }\\n+            } else {\\n+              ctx.logger.warn(\\n+                `[slack-frontend] mermaid rendering returned null for diagram ${i + 1} (mmdc failed or not installed)`\\n+              );\\n+            }\\n+          } catch (e) {\\n+            ctx.logger.warn(\\n+              `[slack-frontend] failed to render/upload mermaid diagram ${i + 1}: ${\\n+                e instanceof Error ? e.message : String(e)\\n+              }`\\n+            );\\n+          }\\n+        }\\n+\\n+        // Replace mermaid blocks with placeholder text\\n+        if (uploadedCount.length > 0) {\\n+          processedText = replaceMermaidBlocks(text, diagrams, (idx) =>\\n+            uploadedCount.includes(idx) ? '_(See diagram above)_' : '_(Diagram rendering failed)_'\\n+          );\\n+        }\\n+      }\\n+\\n+      const formattedText = formatSlackText(processedText);\\n       await slack.chat.postMessage({ channel, text: formattedText, thread_ts: threadTs });\\n+      ctx.logger.info(\\n+        `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+      );\\n+    } catch (outerErr) {\\n+      // Log errors instead of silently swallowing them\\n       try {\\n-        ctx.logger.info(\\n-          `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+        ctx.logger.warn(\\n+          `[slack-frontend] maybePostDirectReply failed for ${checkId}: ${\\n+            outerErr instanceof Error ? outerErr.message : String(outerErr)\\n+          }`\\n         );\\n       } catch {}\\n-    } catch {}\\n+    }\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/generated/config-schema.ts\",\"additions\":233,\"deletions\":13,\"changes\":246,\"patch\":\"diff --git a/src/generated/config-schema.ts b/src/generated/config-schema.ts\\nindex b3caca4f..ecab390d 100644\\n--- a/src/generated/config-schema.ts\\n+++ b/src/generated/config-schema.ts\\n@@ -56,6 +56,20 @@ export const configSchema = {\\n           },\\n           description: 'Import workflow definitions from external files or URLs',\\n         },\\n+        inputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowInput',\\n+          },\\n+          description: 'Workflow inputs (for standalone reusable workflows)',\\n+        },\\n+        outputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowOutput',\\n+          },\\n+          description: 'Workflow outputs (for standalone reusable workflows)',\\n+        },\\n         steps: {\\n           $ref: '#/definitions/Record%3Cstring%2CCheckConfig%3E',\\n           description: 'Step configurations (recommended)',\\n@@ -67,7 +81,7 @@ export const configSchema = {\\n         },\\n         output: {\\n           $ref: '#/definitions/OutputConfig',\\n-          description: 'Output configuration',\\n+          description: 'Output configuration (optional - defaults provided)',\\n         },\\n         http_server: {\\n           $ref: '#/definitions/HttpServerConfig',\\n@@ -139,8 +153,16 @@ export const configSchema = {\\n           },\\n           description: 'Optional integrations: event-driven frontends (e.g., ndjson-sink, github)',\\n         },\\n+        workspace: {\\n+          $ref: '#/definitions/WorkspaceConfig',\\n+          description: 'Workspace isolation configuration for sandboxed execution',\\n+        },\\n+        slack: {\\n+          $ref: '#/definitions/SlackConfig',\\n+          description: 'Slack configuration',\\n+        },\\n       },\\n-      required: ['output', 'version'],\\n+      required: ['version'],\\n       patternProperties: {\\n         '^x-': {},\\n       },\\n@@ -243,6 +265,63 @@ export const configSchema = {\\n         type: 'string',\\n       },\\n     },\\n+    WorkflowInput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Input parameter name',\\n+        },\\n+        schema: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'JSON Schema for the input',\\n+        },\\n+        required: {\\n+          type: 'boolean',\\n+          description: 'Whether this input is required',\\n+        },\\n+        default: {\\n+          description: 'Default value if not provided',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow input definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    WorkflowOutput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Output name',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+        value: {\\n+          type: 'string',\\n+          description: 'Value using Liquid template syntax (references step outputs)',\\n+        },\\n+        value_js: {\\n+          type: 'string',\\n+          description: 'Value using JavaScript expression (alternative to value)',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow output definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n     'Record<string,CheckConfig>': {\\n       type: 'object',\\n       additionalProperties: {\\n@@ -390,11 +469,19 @@ export const configSchema = {\\n           description: 'Timeout in seconds for command execution (default: 60)',\\n         },\\n         depends_on: {\\n-          type: 'array',\\n-          items: {\\n-            type: 'string',\\n-          },\\n-          description: 'Check IDs that this check depends on (optional)',\\n+          anyOf: [\\n+            {\\n+              type: 'string',\\n+            },\\n+            {\\n+              type: 'array',\\n+              items: {\\n+                type: 'string',\\n+              },\\n+            },\\n+          ],\\n+          description:\\n+            'Check IDs that this check depends on (optional). Accepts single string or array.',\\n         },\\n         group: {\\n           type: 'string',\\n@@ -653,13 +740,93 @@ export const configSchema = {\\n           description: 'Arguments/inputs for the workflow',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Override specific step configurations in the workflow',\\n         },\\n         output_mapping: {\\n           $ref: '#/definitions/Record%3Cstring%2Cstring%3E',\\n           description: 'Map workflow outputs to check outputs',\\n         },\\n+        workflow_inputs: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'Alias for args - workflow inputs (backward compatibility)',\\n+        },\\n+        config: {\\n+          type: 'string',\\n+          description:\\n+            'Config file path - alternative to workflow ID (loads a Visor config file as workflow)',\\n+        },\\n+        workflow_overrides: {\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n+          description: 'Alias for overrides - workflow step overrides (backward compatibility)',\\n+        },\\n+        ref: {\\n+          type: 'string',\\n+          description: 'Git reference to checkout (branch, tag, commit SHA) - supports templates',\\n+        },\\n+        repository: {\\n+          type: 'string',\\n+          description: 'Repository URL or owner/repo format (defaults to current repository)',\\n+        },\\n+        token: {\\n+          type: 'string',\\n+          description: 'GitHub token for private repositories (defaults to GITHUB_TOKEN env)',\\n+        },\\n+        fetch_depth: {\\n+          type: 'number',\\n+          description: 'Number of commits to fetch (0 for full history, default: 1)',\\n+        },\\n+        fetch_tags: {\\n+          type: 'boolean',\\n+          description: 'Whether to fetch tags (default: false)',\\n+        },\\n+        submodules: {\\n+          anyOf: [\\n+            {\\n+              type: 'boolean',\\n+            },\\n+            {\\n+              type: 'string',\\n+              const: 'recursive',\\n+            },\\n+          ],\\n+          description: \\\"Checkout submodules: false, true, or 'recursive'\\\",\\n+        },\\n+        working_directory: {\\n+          type: 'string',\\n+          description: 'Working directory for the checkout (defaults to temp directory)',\\n+        },\\n+        use_worktree: {\\n+          type: 'boolean',\\n+          description: 'Use git worktree for efficient parallel checkouts (default: true)',\\n+        },\\n+        clean: {\\n+          type: 'boolean',\\n+          description: 'Clean the working directory before checkout (default: true)',\\n+        },\\n+        sparse_checkout: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description: 'Sparse checkout paths - only checkout specific directories/files',\\n+        },\\n+        lfs: {\\n+          type: 'boolean',\\n+          description: 'Enable Git LFS (Large File Storage)',\\n+        },\\n+        clone_timeout_ms: {\\n+          type: 'number',\\n+          description: 'Timeout in ms for cloning the bare repository (default: 300000 = 5 min)',\\n+        },\\n+        cleanup_on_failure: {\\n+          type: 'boolean',\\n+          description: 'Clean up worktree on failure (default: true)',\\n+        },\\n+        persist_worktree: {\\n+          type: 'boolean',\\n+          description: 'Keep worktree after workflow completion (default: false)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'Configuration for a single check',\\n@@ -794,6 +961,11 @@ export const configSchema = {\\n           $ref: '#/definitions/BashConfig',\\n           description: 'Advanced bash command execution configuration',\\n         },\\n+        completion_prompt: {\\n+          type: 'string',\\n+          description:\\n+            'Completion prompt for post-completion validation/review (runs after attempt_completion)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'AI provider configuration',\\n@@ -1223,7 +1395,7 @@ export const configSchema = {\\n           description: 'Custom output name (defaults to workflow name)',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Step overrides',\\n         },\\n         output_mapping: {\\n@@ -1238,14 +1410,14 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n-    'Record<string,Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>>':\\n+    'Record<string,Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>>':\\n       {\\n         type: 'object',\\n         additionalProperties: {\\n-          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E',\\n+          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E',\\n         },\\n       },\\n-    'Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>': {\\n+    'Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>': {\\n       type: 'object',\\n       additionalProperties: false,\\n     },\\n@@ -1811,6 +1983,54 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n+    WorkspaceConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        enabled: {\\n+          type: 'boolean',\\n+          description: 'Enable workspace isolation (default: true when config present)',\\n+        },\\n+        base_path: {\\n+          type: 'string',\\n+          description: 'Base path for workspaces (default: /tmp/visor-workspaces)',\\n+        },\\n+        cleanup_on_exit: {\\n+          type: 'boolean',\\n+          description: 'Clean up workspace on exit (default: true)',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Workspace isolation configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    SlackConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        version: {\\n+          type: 'string',\\n+          description: 'Slack API version',\\n+        },\\n+        mentions: {\\n+          type: 'string',\\n+          description: \\\"Mention handling: 'all', 'direct', etc.\\\",\\n+        },\\n+        threads: {\\n+          type: 'string',\\n+          description: \\\"Thread handling: 'required', 'optional', etc.\\\",\\n+        },\\n+        show_raw_output: {\\n+          type: 'boolean',\\n+          description: 'Show raw output in Slack responses',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Slack configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n   },\\n } as const;\\n export default configSchema;\\n\",\"status\":\"modified\"},{\"filename\":\"src/index.ts\",\"additions\":5,\"deletions\":2,\"changes\":7,\"patch\":\"diff --git a/src/index.ts b/src/index.ts\\nindex 50b89df3..9621e733 100644\\n--- a/src/index.ts\\n+++ b/src/index.ts\\n@@ -628,8 +628,11 @@ function resolveDependencies(\\n \\n     // Get dependencies for this check\\n     const checkConfig = config?.checks?.[checkId];\\n+    // Normalize depends_on to array (supports string | string[])\\n+    const rawDeps = checkConfig?.depends_on;\\n+    const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     // Expand OR groups (pipe syntax) for dependency closure discovery\\n-    const dependencies = (checkConfig?.depends_on || []).flatMap(d =>\\n+    const dependencies = depsArray.flatMap((d: string) =>\\n       typeof d === 'string' && d.includes('|')\\n         ? d\\n             .split('|')\\n\",\"status\":\"modified\"},{\"filename\":\"src/liquid-extensions.ts\",\"additions\":68,\"deletions\":1,\"changes\":69,\"patch\":\"diff --git a/src/liquid-extensions.ts b/src/liquid-extensions.ts\\nindex e9efaf68..dbf9c6ed 100644\\n--- a/src/liquid-extensions.ts\\n+++ b/src/liquid-extensions.ts\\n@@ -124,6 +124,26 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     }\\n   });\\n \\n+  // Register base64 filter for encoding strings\\n+  // Usage: {{ \\\"user:password\\\" | base64 }}\\n+  liquid.registerFilter('base64', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    return Buffer.from(str).toString('base64');\\n+  });\\n+\\n+  // Register base64_decode filter for decoding base64 strings\\n+  // Usage: {{ encoded_value | base64_decode }}\\n+  liquid.registerFilter('base64_decode', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    try {\\n+      return Buffer.from(str, 'base64').toString('utf-8');\\n+    } catch {\\n+      return '[Error: Invalid base64 string]';\\n+    }\\n+  });\\n+\\n   // Sanitize a label to allowed characters only: [A-Za-z0-9:/]\\n   liquid.registerFilter('safe_label', (value: unknown) => sanitizeLabel(value));\\n \\n@@ -137,6 +157,53 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     return s.replace(/\\\\\\\\n/g, '\\\\n').replace(/\\\\\\\\r/g, '\\\\r').replace(/\\\\\\\\t/g, '\\\\t');\\n   });\\n \\n+  // JSON escape filter - escapes a string for use inside a JSON string value\\n+  // This escapes special characters like quotes, backslashes, and control characters\\n+  // Usage: \\\"jql\\\": \\\"{{ myValue | json_escape }}\\\"\\n+  liquid.registerFilter('json_escape', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const s = String(value);\\n+    // Use JSON.stringify which handles all escaping, then strip the surrounding quotes\\n+    const jsonStr = JSON.stringify(s);\\n+    // Remove the first and last character (the quotes added by JSON.stringify)\\n+    return jsonStr.slice(1, -1);\\n+  });\\n+\\n+  // Shell escape filter - wraps value in single quotes with proper escaping\\n+  // Usage: {{ value | shell_escape }}\\n+  // Example: \\\"hello'world\\\" becomes \\\"'hello'\\\\''world'\\\"\\n+  // This is POSIX-compliant and safe for arbitrary text including mermaid diagrams\\n+  liquid.registerFilter('shell_escape', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    // Replace single quotes with: end quote, escaped quote, start quote\\n+    // Then wrap the entire thing in single quotes\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Alias for shell_escape\\n+  liquid.registerFilter('escape_shell', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Shell escape for double quotes (less safe but sometimes needed)\\n+  // Usage: {{ value | shell_escape_double }}\\n+  // Escapes: $, `, \\\\, \\\", and !\\n+  liquid.registerFilter('shell_escape_double', (value: unknown) => {\\n+    if (value == null) return '\\\"\\\"';\\n+    const s = String(value);\\n+    // Escape characters that have special meaning inside double quotes\\n+    const escaped = s\\n+      .replace(/\\\\\\\\/g, '\\\\\\\\\\\\\\\\')  // backslash first\\n+      .replace(/\\\\$/g, '\\\\\\\\$')   // dollar sign\\n+      .replace(/`/g, '\\\\\\\\`')    // backticks\\n+      .replace(/\\\"/g, '\\\\\\\\\\\"')    // double quotes\\n+      .replace(/!/g, '\\\\\\\\!');   // history expansion\\n+    return '\\\"' + escaped + '\\\"';\\n+  });\\n+\\n   // Register author permission filters (from main)\\n   // These filters check the author's permission level; detect local mode for tests\\n   const isLocal = detectLocalMode();\\n\",\"status\":\"modified\"},{\"filename\":\"src/logger.ts\",\"additions\":31,\"deletions\":2,\"changes\":33,\"patch\":\"diff --git a/src/logger.ts b/src/logger.ts\\nindex 6148b77c..aee7136e 100644\\n--- a/src/logger.ts\\n+++ b/src/logger.ts\\n@@ -85,7 +85,36 @@ class Logger {\\n       if (this.showTimestamps) {\\n         const ts = new Date().toISOString();\\n         const lvl = level ? level : undefined;\\n-        const prefix = lvl ? `[${ts}] [${lvl}]` : `[${ts}]`;\\n+\\n+        let tsToken = `[${ts}]`;\\n+        let lvlToken = lvl ? `[${lvl}]` : '';\\n+\\n+        // Add simple ANSI colour when running in a TTY and not emitting\\n+        // JSON/SARIF. Colours are intentionally minimal and only applied\\n+        // to the prefix markers, not the full line.\\n+        if (this.isTTY && !this.isJsonLike) {\\n+          const reset = '\\\\x1b[0m';\\n+          const dim = '\\\\x1b[2m';\\n+          const colours: Record<LogLevel, string> = {\\n+            silent: '',\\n+            error: '\\\\x1b[31m', // red\\n+            warn: '\\\\x1b[33m', // yellow\\n+            info: '\\\\x1b[36m', // cyan\\n+            verbose: '\\\\x1b[35m', // magenta\\n+            debug: '\\\\x1b[90m', // bright black / gray\\n+          };\\n+\\n+          tsToken = `${dim}${tsToken}${reset}`;\\n+\\n+          if (lvl) {\\n+            const colour = colours[lvl] || '';\\n+            if (colour) {\\n+              lvlToken = `${colour}${lvlToken}${reset}`;\\n+            }\\n+          }\\n+        }\\n+\\n+        const prefix = lvl ? `${tsToken} ${lvlToken}` : tsToken;\\n         process.stderr.write(`${prefix} ${msg}\\\\n`);\\n       } else {\\n         process.stderr.write(msg + '\\\\n');\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/ai-check-provider.ts\",\"additions\":100,\"deletions\":6,\"changes\":106,\"patch\":\"diff --git a/src/providers/ai-check-provider.ts b/src/providers/ai-check-provider.ts\\nindex 53f1a350..b41d6af1 100644\\n--- a/src/providers/ai-check-provider.ts\\n+++ b/src/providers/ai-check-provider.ts\\n@@ -186,7 +186,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     let promptContent: string;\\n \\n@@ -204,7 +205,8 @@ export class AICheckProvider extends CheckProvider {\\n       eventContext,\\n       dependencyResults,\\n       outputHistory,\\n-      args\\n+      args,\\n+      workflowInputs\\n     );\\n   }\\n \\n@@ -335,7 +337,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     // Build outputs_raw from -raw keys (aggregate parent values)\\n     const outputsRaw: Record<string, unknown> = {};\\n@@ -535,6 +538,8 @@ export class AICheckProvider extends CheckProvider {\\n       outputs_raw: outputsRaw,\\n       // Custom arguments from on_init 'with' directive\\n       args: args || {},\\n+      // Workflow inputs (for nested workflow steps to access parent inputs like {{ inputs.context }})\\n+      inputs: workflowInputs || {},\\n     };\\n \\n     try {\\n@@ -621,7 +626,8 @@ export class AICheckProvider extends CheckProvider {\\n         console.error(`[ai-exec] step=${String((config as any).checkName || 'unknown')}`);\\n       }\\n     } catch {}\\n-    // Extract AI configuration - only set properties that are explicitly provided\\n+    // Extract AI configuration - only set properties that are explicitly provided.\\n+    // Workspace / allowedFolders will be derived below from the execution context.\\n     const aiConfig: AIReviewConfig = {};\\n \\n     // Check-level AI configuration (ai object)\\n@@ -671,6 +677,9 @@ export class AICheckProvider extends CheckProvider {\\n       if (aiAny.bashConfig !== undefined) {\\n         aiConfig.bashConfig = aiAny.bashConfig as import('../types/config').BashConfig;\\n       }\\n+      if (aiAny.completion_prompt !== undefined) {\\n+        aiConfig.completionPrompt = aiAny.completion_prompt as string;\\n+      }\\n       if (aiAny.skip_code_context !== undefined) {\\n         // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n         (aiConfig as any).skip_code_context = aiAny.skip_code_context as boolean;\\n@@ -691,6 +700,90 @@ export class AICheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Derive workspace-aware allowedFolders for ProbeAgent when workspace\\n+    // isolation is enabled. This ensures tools like search/query operate\\n+    // inside the isolated workspace (and its project symlinks) instead of\\n+    // the Visor repository root.\\n+    // Folder names are human-readable (tyk-docs, visor2) thanks to WorkspaceManager.\\n+    try {\\n+      const ctxAny: any = sessionInfo as any;\\n+      const parentCtx = ctxAny?._parentContext;\\n+      const workspace = parentCtx?.workspace;\\n+\\n+      // Enhanced debug logging for workspace propagation diagnosis\\n+      logger.debug(`[AI Provider] Workspace detection for check '${(config as any).checkName || 'unknown'}':`);\\n+      logger.debug(`[AI Provider]   sessionInfo exists: ${!!sessionInfo}`);\\n+      logger.debug(`[AI Provider]   _parentContext exists: ${!!parentCtx}`);\\n+      logger.debug(`[AI Provider]   workspace exists: ${!!workspace}`);\\n+      if (workspace) {\\n+        logger.debug(`[AI Provider]   workspace.isEnabled exists: ${typeof workspace.isEnabled === 'function'}`);\\n+        logger.debug(`[AI Provider]   workspace.isEnabled(): ${typeof workspace.isEnabled === 'function' ? workspace.isEnabled() : 'N/A'}`);\\n+        const projectCount = typeof workspace.listProjects === 'function' ? workspace.listProjects()?.length : 'N/A';\\n+        logger.debug(`[AI Provider]   workspace.listProjects() count: ${projectCount}`);\\n+      }\\n+\\n+      if (workspace && typeof workspace.isEnabled === 'function' && workspace.isEnabled()) {\\n+        const folders: string[] = [];\\n+        let workspaceRoot: string | undefined;\\n+        let mainProjectPath: string | undefined;\\n+        try {\\n+          const info = workspace.getWorkspaceInfo?.();\\n+          if (info && typeof info.workspacePath === 'string') {\\n+            workspaceRoot = info.workspacePath;\\n+            mainProjectPath = info.mainProjectPath;\\n+            // Add workspace root\\n+            folders.push(info.workspacePath);\\n+            // Include main project path (has human-readable name like \\\"visor2\\\")\\n+            if (mainProjectPath) {\\n+              folders.push(mainProjectPath);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore workspace info errors\\n+        }\\n+        try {\\n+          const projects = workspace.listProjects?.() || [];\\n+          for (const proj of projects as any[]) {\\n+            if (proj && typeof proj.path === 'string') {\\n+              // Project paths have human-readable names (tyk-docs, not checkout-tyk-docs)\\n+              folders.push(proj.path);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore project listing errors\\n+        }\\n+        const unique = Array.from(new Set(folders.filter(p => typeof p === 'string' && p)));\\n+        if (unique.length > 0 && workspaceRoot) {\\n+          (aiConfig as any).allowedFolders = unique;\\n+          // Set path and cwd to workspace root - AI will run with cwd set to workspace\\n+          // Both are set for compatibility: path for older probe versions, cwd for rc175+\\n+          (aiConfig as any).path = workspaceRoot;\\n+          (aiConfig as any).cwd = workspaceRoot;\\n+          // Also set workspacePath for the AI to know the root\\n+          (aiConfig as any).workspacePath = workspaceRoot;\\n+          logger.debug(`[AI Provider] Workspace isolation enabled:`);\\n+          logger.debug(`[AI Provider]   workspaceRoot (cwd): ${workspaceRoot}`);\\n+          logger.debug(`[AI Provider]   allowedFolders: ${JSON.stringify(unique)}`);\\n+        }\\n+      } else if (parentCtx && typeof parentCtx.workingDirectory === 'string') {\\n+        // Fallback: when workspace is not available (or disabled), still\\n+        // constrain tools to the engine's working directory so ProbeAgent\\n+        // operates inside the same logical root as the state machine. This\\n+        // also ensures nested workflows (e.g. code-question-helper) use the\\n+        // workspace main project path once initializeWorkspace has updated\\n+        // the parent context.\\n+        if (!(aiConfig as any).allowedFolders) {\\n+          (aiConfig as any).allowedFolders = [parentCtx.workingDirectory];\\n+        }\\n+        if (!(aiConfig as any).path) {\\n+          (aiConfig as any).path = parentCtx.workingDirectory;\\n+          (aiConfig as any).cwd = parentCtx.workingDirectory;\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only; fall back to defaults on error.\\n+    }\\n+\\n     // Check-level AI model and provider (top-level properties)\\n     if (config.ai_model !== undefined) {\\n       aiConfig.model = config.ai_model as string;\\n@@ -876,7 +969,8 @@ export class AICheckProvider extends CheckProvider {\\n       ctxWithStage,\\n       _dependencyResults,\\n       (config as any).__outputHistory as Map<string, unknown[]> | undefined,\\n-      (sessionInfo as any)?.args\\n+      (sessionInfo as any)?.args,\\n+      (config as any).workflowInputs as Record<string, unknown> | undefined\\n     );\\n \\n     // Optional persona (vendor extension): ai.ai_persona or ai_persona.\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/command-check-provider.ts\",\"additions\":50,\"deletions\":16,\"changes\":66,\"patch\":\"diff --git a/src/providers/command-check-provider.ts b/src/providers/command-check-provider.ts\\nindex e0c9d634..bf7ffd43 100644\\n--- a/src/providers/command-check-provider.ts\\n+++ b/src/providers/command-check-provider.ts\\n@@ -131,7 +131,8 @@ export class CommandCheckProvider extends CheckProvider {\\n       // New: outputs_raw exposes aggregate values (e.g., full arrays for forEach parents)\\n       outputs_raw: outputsRaw,\\n       // Workflow inputs (when executing within a workflow)\\n-      inputs: context?.workflowInputs || {},\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any).workflowInputs || context?.workflowInputs || {},\\n       // Custom arguments from on_init 'with' directive\\n       args: context?.args || {},\\n       env: this.getSafeEnvironmentVariables(),\\n@@ -178,17 +179,31 @@ export class CommandCheckProvider extends CheckProvider {\\n           mock = rawMock as Record<string, unknown>;\\n         }\\n         const m = mock as { stdout?: string; stderr?: string; exit_code?: number; exit?: number };\\n-        let out: unknown = m.stdout ?? '';\\n-        try {\\n-          if (\\n-            typeof out === 'string' &&\\n-            (out.trim().startsWith('{') || out.trim().startsWith('['))\\n-          ) {\\n-            out = JSON.parse(out);\\n-          }\\n-        } catch {}\\n-        const code =\\n-          typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0;\\n+\\n+        // Check if this looks like a command mock (has stdout/stderr/exit_code) or direct data output\\n+        const isCommandMock = m.stdout !== undefined || m.stderr !== undefined ||\\n+          m.exit_code !== undefined || m.exit !== undefined;\\n+\\n+        let out: unknown;\\n+        if (isCommandMock) {\\n+          // Traditional command mock format: { stdout: \\\"...\\\", exit_code: 0 }\\n+          out = m.stdout ?? '';\\n+          try {\\n+            if (\\n+              typeof out === 'string' &&\\n+              (out.trim().startsWith('{') || out.trim().startsWith('['))\\n+            ) {\\n+              out = JSON.parse(out);\\n+            }\\n+          } catch {}\\n+        } else {\\n+          // Direct data mock format: { data: [...], count: 1 } - use as-is\\n+          out = mock;\\n+        }\\n+\\n+        const code = isCommandMock\\n+          ? (typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0)\\n+          : 0;\\n         if (code !== 0) {\\n           return {\\n             issues: [\\n@@ -231,9 +246,8 @@ export class CommandCheckProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Get timeout from config (in seconds) or use default (60 seconds)\\n-      const timeoutSeconds = (config.timeout as number) || 60;\\n-      const timeoutMs = timeoutSeconds * 1000;\\n+      // Get timeout from config (in milliseconds) or use default (60 seconds = 60000ms)\\n+      const timeoutMs = (config.timeout as number) || 60000;\\n \\n       // Normalize only the eval payload for `node -e|--eval` invocations that may contain\\n       // literal newlines due to YAML processing (\\\"\\\\n\\\" -> newline). We re-escape newlines\\n@@ -1533,6 +1547,26 @@ ${bodyWithReturn}\\n       return null;\\n     }\\n \\n+    // Require at least one issue-specific field beyond just a message-like field.\\n+    // This prevents objects like {text: \\\"...\\\"} from being treated as issues.\\n+    const hasIssueField =\\n+      data.file !== undefined ||\\n+      data.path !== undefined ||\\n+      data.filename !== undefined ||\\n+      data.line !== undefined ||\\n+      data.startLine !== undefined ||\\n+      data.lineNumber !== undefined ||\\n+      data.severity !== undefined ||\\n+      data.level !== undefined ||\\n+      data.priority !== undefined ||\\n+      data.ruleId !== undefined ||\\n+      data.rule !== undefined ||\\n+      data.category !== undefined ||\\n+      data.type !== undefined;\\n+    if (!hasIssueField) {\\n+      return null;\\n+    }\\n+\\n     const allowedSeverities = new Set(['info', 'warning', 'error', 'critical']);\\n     const severityRaw = this.toTrimmedString(data.severity || data.level || data.priority);\\n     let severity: ReviewIssue['severity'] = 'warning';\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/git-checkout-provider.ts\",\"additions\":41,\"deletions\":6,\"changes\":47,\"patch\":\"diff --git a/src/providers/git-checkout-provider.ts b/src/providers/git-checkout-provider.ts\\nindex baac0001..6d3a2852 100644\\n--- a/src/providers/git-checkout-provider.ts\\n+++ b/src/providers/git-checkout-provider.ts\\n@@ -69,6 +69,16 @@ export class GitCheckoutProvider extends CheckProvider {\\n       return false;\\n     }\\n \\n+    if (checkoutConfig.clone_timeout_ms !== undefined) {\\n+      if (\\n+        typeof checkoutConfig.clone_timeout_ms !== 'number' ||\\n+        checkoutConfig.clone_timeout_ms <= 0\\n+      ) {\\n+        logger.error('Invalid config: clone_timeout_ms must be a positive number (milliseconds)');\\n+        return false;\\n+      }\\n+    }\\n+\\n     return true;\\n   }\\n \\n@@ -91,7 +101,13 @@ export class GitCheckoutProvider extends CheckProvider {\\n       );\\n \\n       // Resolve dynamic variables\\n-      const resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      let resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      // If ref resolves to an empty string, fall back to HEAD so we rely on\\n+      // the repository's default branch. This keeps downstream git commands\\n+      // well-defined and matches existing expectations/tests.\\n+      if (!resolvedRef || resolvedRef.trim().length === 0) {\\n+        resolvedRef = 'HEAD';\\n+      }\\n       const resolvedRepository = checkoutConfig.repository\\n         ? await this.liquid.parseAndRender(checkoutConfig.repository, templateContext)\\n         : process.env.GITHUB_REPOSITORY || 'unknown/unknown';\\n@@ -118,6 +134,7 @@ export class GitCheckoutProvider extends CheckProvider {\\n           clean: checkoutConfig.clean !== false, // Default: true\\n           workflowId: (context as any)?.workflowId,\\n           fetchDepth: checkoutConfig.fetch_depth,\\n+          cloneTimeoutMs: checkoutConfig.clone_timeout_ms,\\n         }\\n       );\\n \\n@@ -134,18 +151,35 @@ export class GitCheckoutProvider extends CheckProvider {\\n \\n       // Add project to workspace if workspace isolation is enabled\\n       const workspace = (context as any)?._parentContext?.workspace;\\n+\\n+      // Enhanced debug logging for workspace project addition diagnosis\\n+      const checkName = (config as any)?.checkName || 'unknown';\\n+      logger.info(`[GitCheckout] Workspace check for '${checkName}':`);\\n+      logger.info(`[GitCheckout]   _parentContext exists: ${!!(context as any)?._parentContext}`);\\n+      logger.info(`[GitCheckout]   workspace exists: ${!!workspace}`);\\n+      logger.info(`[GitCheckout]   workspace.isEnabled(): ${workspace?.isEnabled?.() ?? 'N/A'}`);\\n+      if (workspace) {\\n+        const projectCountBefore = workspace.listProjects?.()?.length ?? 'N/A';\\n+        logger.debug(`[GitCheckout]   projects before addProject: ${projectCountBefore}`);\\n+      }\\n+\\n       if (workspace?.isEnabled()) {\\n         try {\\n+          // Don't pass checkName as description - let workspace use repo name\\n+          // This results in human-readable names like \\\"tyk-docs\\\" instead of \\\"checkout-tyk-docs\\\"\\n           const workspacePath = await workspace.addProject(\\n             resolvedRepository,\\n-            worktree.path,\\n-            checkoutConfig.checkName\\n+            worktree.path\\n           );\\n           output.workspace_path = workspacePath;\\n-          logger.debug(`Added project to workspace: ${workspacePath}`);\\n+          const projectCountAfter = workspace.listProjects?.()?.length ?? 'N/A';\\n+          logger.debug(`[GitCheckout] Added project to workspace: ${workspacePath}`);\\n+          logger.debug(`[GitCheckout]   projects after addProject: ${projectCountAfter}`);\\n         } catch (error) {\\n           logger.warn(`Failed to add project to workspace: ${error}`);\\n         }\\n+      } else {\\n+        logger.debug(`[GitCheckout] Workspace not enabled, skipping addProject`);\\n       }\\n \\n       logger.info(\\n@@ -228,7 +262,8 @@ export class GitCheckoutProvider extends CheckProvider {\\n       outputs: outputsObj,\\n       outputs_history: historyObj,\\n       env: safeEnv,\\n-      inputs: context?.workflowInputs,\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any)?.workflowInputs || context?.workflowInputs,\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/http-client-provider.ts\",\"additions\":279,\"deletions\":35,\"changes\":314,\"patch\":\"diff --git a/src/providers/http-client-provider.ts b/src/providers/http-client-provider.ts\\nindex 4d8c41be..6db81591 100644\\n--- a/src/providers/http-client-provider.ts\\n+++ b/src/providers/http-client-provider.ts\\n@@ -4,18 +4,29 @@ import { ReviewSummary } from '../reviewer';\\n import { Liquid } from 'liquidjs';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n import { EnvironmentResolver } from '../utils/env-resolver';\\n+import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n+import { buildProviderTemplateContext } from '../utils/template-context';\\n+import Sandbox from '@nyariv/sandboxjs';\\n+import { logger } from '../logger';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n \\n /**\\n  * Check provider that fetches data from HTTP endpoints\\n  */\\n export class HttpClientProvider extends CheckProvider {\\n   private liquid: Liquid;\\n+  private sandbox?: Sandbox;\\n \\n   constructor() {\\n     super();\\n     this.liquid = createExtendedLiquid();\\n   }\\n \\n+  private createSecureSandbox(): Sandbox {\\n+    return createSecureSandbox();\\n+  }\\n+\\n   getName(): string {\\n     return 'http_client';\\n   }\\n@@ -61,50 +72,124 @@ export class HttpClientProvider extends CheckProvider {\\n     const headers = (config.headers as Record<string, string>) || {};\\n     const timeout = (config.timeout as number) || 30000;\\n     const transform = config.transform as string | undefined;\\n+    const transformJs = config.transform_js as string | undefined;\\n     const bodyTemplate = config.body as string | undefined;\\n+    const outputFileTemplate = config.output_file as string | undefined;\\n+    const skipIfExists = config.skip_if_exists !== false; // Default true for caching\\n \\n-    try {\\n-      // Prepare template context for URL and body\\n-      const templateContext = {\\n-        pr: {\\n-          number: prInfo.number,\\n-          title: prInfo.title,\\n-          body: prInfo.body,\\n-          author: prInfo.author,\\n-          base: prInfo.base,\\n-          head: prInfo.head,\\n-          totalAdditions: prInfo.totalAdditions,\\n-          totalDeletions: prInfo.totalDeletions,\\n-        },\\n-        outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-        env: process.env,\\n-      };\\n+    // Track resolved URL for error messages\\n+    let resolvedUrlForErrors = url;\\n \\n-      // Render URL with template if it contains liquid syntax\\n-      let renderedUrl = url;\\n-      if (url.includes('{{') || url.includes('{%')) {\\n-        renderedUrl = await this.liquid.parseAndRender(url, templateContext);\\n+    try {\\n+      // Use shared template context builder for consistent output extraction\\n+      const templateContext = buildProviderTemplateContext(\\n+        prInfo,\\n+        dependencyResults,\\n+        undefined, // memoryStore\\n+        undefined, // outputHistory\\n+        undefined, // stageHistoryBase\\n+        { attachMemoryReadHelpers: false }\\n+      );\\n+      // Add env to context for shell-style variable resolution\\n+      (templateContext as Record<string, unknown>).env = process.env;\\n+\\n+      // First resolve shell-style environment variables (${VAR}, $VAR, ${{ env.VAR }})\\n+      let renderedUrl = String(EnvironmentResolver.resolveValue(url));\\n+      resolvedUrlForErrors = renderedUrl; // Track for error messages\\n+\\n+      // Then render Liquid templates if present\\n+      if (renderedUrl.includes('{{') || renderedUrl.includes('{%')) {\\n+        renderedUrl = await this.liquid.parseAndRender(renderedUrl, templateContext);\\n+        resolvedUrlForErrors = renderedUrl; // Update after Liquid rendering\\n       }\\n \\n       // Prepare request body if provided\\n       let requestBody: string | undefined;\\n       if (bodyTemplate) {\\n-        const renderedBody = await this.liquid.parseAndRender(bodyTemplate, templateContext);\\n-        requestBody = renderedBody;\\n+        // First resolve shell-style environment variables\\n+        let resolvedBody = String(EnvironmentResolver.resolveValue(bodyTemplate));\\n+        // Then render Liquid templates if present\\n+        if (resolvedBody.includes('{{') || resolvedBody.includes('{%')) {\\n+          resolvedBody = await this.liquid.parseAndRender(resolvedBody, templateContext);\\n+        }\\n+        requestBody = resolvedBody;\\n       }\\n \\n-      // Resolve environment variables in headers\\n-      const resolvedHeaders = EnvironmentResolver.resolveHeaders(headers);\\n+      // Resolve environment variables and Liquid templates in headers\\n+      const resolvedHeaders: Record<string, string> = {};\\n+      for (const [key, value] of Object.entries(headers)) {\\n+        let resolvedValue = String(EnvironmentResolver.resolveValue(value));\\n+        // Render Liquid templates if present\\n+        if (resolvedValue.includes('{{') || resolvedValue.includes('{%')) {\\n+          resolvedValue = await this.liquid.parseAndRender(resolvedValue, templateContext);\\n+        }\\n+        resolvedHeaders[key] = resolvedValue;\\n+        // Debug auth header (mask most of the value for security)\\n+        if (key.toLowerCase() === 'authorization') {\\n+          const maskedValue = resolvedValue.length > 20\\n+            ? `${resolvedValue.substring(0, 15)}...${resolvedValue.substring(resolvedValue.length - 5)}`\\n+            : resolvedValue;\\n+          logger.verbose(`[http_client] ${key}: ${maskedValue}`);\\n+        }\\n+      }\\n+\\n+      // Resolve output_file path if specified\\n+      let resolvedOutputFile: string | undefined;\\n+      if (outputFileTemplate) {\\n+        let outputPath = String(EnvironmentResolver.resolveValue(outputFileTemplate));\\n+        if (outputPath.includes('{{') || outputPath.includes('{%')) {\\n+          outputPath = await this.liquid.parseAndRender(outputPath, templateContext);\\n+        }\\n+        resolvedOutputFile = outputPath.trim();\\n+\\n+        // Check if file already exists (caching)\\n+        if (skipIfExists && fs.existsSync(resolvedOutputFile)) {\\n+          const stats = fs.statSync(resolvedOutputFile);\\n+          logger.verbose(`[http_client] File cached: ${resolvedOutputFile} (${stats.size} bytes)`);\\n+          return {\\n+            issues: [],\\n+            file_path: resolvedOutputFile,\\n+            size: stats.size,\\n+            cached: true,\\n+          } as unknown as ReviewSummary;\\n+        }\\n+      }\\n \\n       // Test hook: mock HTTP response for this step\\n       const stepName = (config as any).checkName || 'unknown';\\n       const mock = context?.hooks?.mockForStep?.(String(stepName));\\n-      const data =\\n-        mock !== undefined\\n-          ? mock\\n-          : await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n \\n-      // Apply transformation if specified\\n+      // If mock is provided, return it directly as the step output (with issues: [])\\n+      if (mock !== undefined) {\\n+        const mockObj = typeof mock === 'object' && mock !== null ? mock : { data: mock };\\n+        return {\\n+          issues: [],\\n+          ...mockObj,\\n+        } as unknown as ReviewSummary & { data: unknown };\\n+      }\\n+\\n+      // Debug log the request for troubleshooting\\n+      logger.verbose(`[http_client] ${method} ${renderedUrl}`);\\n+      if (requestBody) {\\n+        logger.verbose(`[http_client] Body: ${requestBody.substring(0, 500)}${requestBody.length > 500 ? '...' : ''}`);\\n+      }\\n+\\n+      // If output_file is specified, download to file instead of returning data\\n+      if (resolvedOutputFile) {\\n+        const fileResult = await this.downloadToFile(\\n+          renderedUrl,\\n+          method,\\n+          resolvedHeaders,\\n+          requestBody,\\n+          timeout,\\n+          resolvedOutputFile\\n+        );\\n+        return fileResult;\\n+      }\\n+\\n+      const data = await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n+\\n+      // Apply Liquid transformation if specified\\n       let processedData = data;\\n       if (transform) {\\n         try {\\n@@ -136,12 +221,53 @@ export class HttpClientProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Return the fetched data as a custom field for dependent checks to access\\n+      // Apply JavaScript transformation if specified\\n+      if (transformJs) {\\n+        try {\\n+          if (!this.sandbox) {\\n+            this.sandbox = this.createSecureSandbox();\\n+          }\\n+\\n+          // Create scope for JavaScript transform (scope, not context)\\n+          const jsScope: Record<string, unknown> = {\\n+            output: data,\\n+            pr: templateContext.pr,\\n+            outputs: templateContext.outputs,\\n+            env: process.env,\\n+          };\\n+\\n+          const result = compileAndRun(this.sandbox, transformJs, jsScope, {\\n+            injectLog: true,\\n+            logPrefix: 'üîç [transform_js]',\\n+            wrapFunction: true,\\n+          });\\n+          processedData = result;\\n+          logger.verbose(`‚úì Applied JavaScript transform successfully`);\\n+        } catch (error) {\\n+          logger.error(\\n+            `‚úó Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`\\n+          );\\n+          return {\\n+            issues: [\\n+              {\\n+                file: 'http_client',\\n+                line: 0,\\n+                ruleId: 'http_client/transform_js_error',\\n+                message: `Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              },\\n+            ],\\n+          };\\n+        }\\n+      }\\n+\\n+      // Return the fetched data in the standard `output` property for guarantee evaluation\\n+      // This is consistent with other providers (script, command, ai, etc.)\\n       return {\\n         issues: [],\\n-        // Add custom data field that will be passed through to dependent checks\\n-        data: processedData,\\n-      } as ReviewSummary & { data: unknown };\\n+        output: processedData,\\n+      } as unknown as ReviewSummary;\\n     } catch (error) {\\n       return {\\n         issues: [\\n@@ -149,7 +275,7 @@ export class HttpClientProvider extends CheckProvider {\\n             file: 'http_client',\\n             line: 0,\\n             ruleId: 'http_client/fetch_error',\\n-            message: `Failed to fetch from ${url}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            message: `Failed to fetch from ${resolvedUrlForErrors}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n             severity: 'error',\\n             category: 'logic',\\n           },\\n@@ -198,7 +324,14 @@ export class HttpClientProvider extends CheckProvider {\\n \\n       clearTimeout(timeoutId);\\n \\n+      logger.verbose(`[http_client] Response: ${response.status} ${response.statusText}`);\\n+\\n       if (!response.ok) {\\n+        // Log response body for debugging auth failures\\n+        try {\\n+          const errorBody = await response.text();\\n+          logger.warn(`[http_client] Error body: ${errorBody.substring(0, 500)}`);\\n+        } catch {}\\n         throw new Error(`HTTP ${response.status}: ${response.statusText}`);\\n       }\\n \\n@@ -233,6 +366,112 @@ export class HttpClientProvider extends CheckProvider {\\n     }\\n   }\\n \\n+  private async downloadToFile(\\n+    url: string,\\n+    method: string,\\n+    headers: Record<string, string>,\\n+    body: string | undefined,\\n+    timeout: number,\\n+    outputFile: string\\n+  ): Promise<ReviewSummary> {\\n+    // Check if fetch is available (Node 18+)\\n+    if (typeof fetch === 'undefined') {\\n+      throw new Error('HTTP client provider requires Node.js 18+ or node-fetch package');\\n+    }\\n+\\n+    const controller = new AbortController();\\n+    const timeoutId = setTimeout(() => controller.abort(), timeout);\\n+\\n+    try {\\n+      const requestOptions: RequestInit = {\\n+        method,\\n+        headers: { ...headers },\\n+        signal: controller.signal,\\n+      };\\n+\\n+      // Add body for non-GET requests\\n+      if (method !== 'GET' && body) {\\n+        requestOptions.body = body;\\n+        if (!headers['Content-Type'] && !headers['content-type']) {\\n+          requestOptions.headers = {\\n+            ...requestOptions.headers,\\n+            'Content-Type': 'application/json',\\n+          };\\n+        }\\n+      }\\n+\\n+      const response = await fetch(url, requestOptions);\\n+      clearTimeout(timeoutId);\\n+\\n+      if (!response.ok) {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_error',\\n+              message: `Failed to download file: HTTP ${response.status}: ${response.statusText}`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      // Create parent directory if it doesn't exist\\n+      const parentDir = path.dirname(outputFile);\\n+      if (parentDir && !fs.existsSync(parentDir)) {\\n+        fs.mkdirSync(parentDir, { recursive: true });\\n+      }\\n+\\n+      // Get the response as an ArrayBuffer and write to file\\n+      const arrayBuffer = await response.arrayBuffer();\\n+      const buffer = Buffer.from(arrayBuffer);\\n+      fs.writeFileSync(outputFile, buffer);\\n+\\n+      const contentType = response.headers.get('content-type') || 'application/octet-stream';\\n+      logger.verbose(`[http_client] Downloaded: ${outputFile} (${buffer.length} bytes)`);\\n+\\n+      return {\\n+        issues: [],\\n+        file_path: outputFile,\\n+        size: buffer.length,\\n+        content_type: contentType,\\n+        cached: false,\\n+      } as unknown as ReviewSummary;\\n+    } catch (error: unknown) {\\n+      clearTimeout(timeoutId);\\n+\\n+      if (error instanceof Error && error.name === 'AbortError') {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_timeout',\\n+              message: `Download timed out after ${timeout}ms`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      return {\\n+        issues: [\\n+          {\\n+            file: 'http_client',\\n+            line: 0,\\n+            ruleId: 'http_client/download_error',\\n+            message: `Failed to download file: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            severity: 'error',\\n+            category: 'logic',\\n+          },\\n+        ],\\n+      };\\n+    }\\n+  }\\n+\\n   getSupportedConfigKeys(): string[] {\\n     return [\\n       'type',\\n@@ -241,7 +480,10 @@ export class HttpClientProvider extends CheckProvider {\\n       'headers',\\n       'body',\\n       'transform',\\n+      'transform_js',\\n       'timeout',\\n+      'output_file',\\n+      'skip_if_exists',\\n       'depends_on',\\n       'on',\\n       'if',\\n@@ -261,6 +503,8 @@ export class HttpClientProvider extends CheckProvider {\\n       'Network access to the endpoint',\\n       'Optional: Transform template for processing response data',\\n       'Optional: Body template for POST/PUT requests',\\n+      'Optional: output_file path to download response to a file',\\n+      'Optional: skip_if_exists (default: true) to enable caching for file downloads',\\n     ];\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/log-check-provider.ts\",\"additions\":19,\"deletions\":8,\"changes\":27,\"patch\":\"diff --git a/src/providers/log-check-provider.ts b/src/providers/log-check-provider.ts\\nindex 9c6332d1..b57f632c 100644\\n--- a/src/providers/log-check-provider.ts\\n+++ b/src/providers/log-check-provider.ts\\n@@ -78,7 +78,8 @@ export class LogCheckProvider extends CheckProvider {\\n       includeDependencies,\\n       includeMetadata,\\n       config.__outputHistory as Map<string, unknown[]> | undefined,\\n-      context\\n+      context,\\n+      config\\n     );\\n \\n     // Render the log message template\\n@@ -100,12 +101,20 @@ export class LogCheckProvider extends CheckProvider {\\n     else if (level === 'debug') logger.debug(logOutput);\\n     else logger.info(logOutput);\\n \\n-    // Return with the log content as custom data for dependent checks\\n-    return {\\n+    // Return with the log content as custom data for dependent checks.\\n+    // For chat-style logs (group: chat), also expose a structured\\n+    // `output.text` field so frontends like Slack can post a clean\\n+    // human-facing message without the level prefix.\\n+    const summary: ReviewSummary & { logOutput: string; output?: unknown } = {\\n       issues: [],\\n-      // Add log output as custom field\\n       logOutput,\\n-    } as ReviewSummary & { logOutput: string };\\n+    };\\n+\\n+    if ((config as any).group === 'chat') {\\n+      (summary as any).output = { text: renderedMessage };\\n+    }\\n+\\n+    return summary;\\n   }\\n \\n   private buildTemplateContext(\\n@@ -115,7 +124,8 @@ export class LogCheckProvider extends CheckProvider {\\n     _includeDependencies: boolean = true,\\n     includeMetadata: boolean = true,\\n     outputHistory?: Map<string, unknown[]>,\\n-    executionContext?: ExecutionContext\\n+    executionContext?: ExecutionContext,\\n+    config?: CheckProviderConfig\\n   ): Record<string, unknown> {\\n     const context: Record<string, unknown> = {};\\n \\n@@ -197,7 +207,8 @@ export class LogCheckProvider extends CheckProvider {\\n     }\\n \\n     // Add workflow inputs if available\\n-    const workflowInputs = executionContext?.workflowInputs || {};\\n+    // Check config first (set by projectWorkflowToGraph), then fall back to executionContext\\n+    const workflowInputs = (config as any)?.workflowInputs || executionContext?.workflowInputs || {};\\n     logger.debug(\\n       `[LogProvider] Adding ${Object.keys(workflowInputs).length} workflow inputs to context`\\n     );\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/script-check-provider.ts\",\"additions\":37,\"deletions\":1,\"changes\":38,\"patch\":\"diff --git a/src/providers/script-check-provider.ts b/src/providers/script-check-provider.ts\\nindex 9825bb69..947dbd73 100644\\n--- a/src/providers/script-check-provider.ts\\n+++ b/src/providers/script-check-provider.ts\\n@@ -59,6 +59,16 @@ export class ScriptCheckProvider extends CheckProvider {\\n       reuseSession?: boolean;\\n     } & import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n+    // Test hook: mock output for this step (short-circuit execution)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = _sessionInfo?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        // Return mock directly as step output\\n+        return { issues: [], output: mock } as ReviewSummary & { output: unknown };\\n+      }\\n+    } catch {}\\n+\\n     const script = String(config.content || '');\\n     const memoryStore = MemoryStore.getInstance();\\n     const ctx = buildProviderTemplateContext(\\n@@ -72,10 +82,36 @@ export class ScriptCheckProvider extends CheckProvider {\\n     // Keep provider quiet by default; no step-specific debug\\n     // (historical ad-hoc logs removed to avoid hardcoding step names).\\n \\n+    // Add workflow inputs to the context\\n+    const inputs = (config as any).workflowInputs || _sessionInfo?.workflowInputs || {};\\n+    (ctx as any).inputs = inputs;\\n+\\n+    // Add environment variables to context (consistent with http-client-provider)\\n+    (ctx as any).env = process.env;\\n+\\n     // Attach synchronous memory ops consistent with memory provider\\n     const { ops, needsSave } = createSyncMemoryOps(memoryStore);\\n     (ctx as any).memory = ops as unknown as Record<string, unknown>;\\n \\n+    // Add helper functions to the context\\n+    (ctx as any).escapeXml = (str: unknown): string => {\\n+      if (str == null) return '';\\n+      return String(str)\\n+        .replace(/&/g, '&amp;')\\n+        .replace(/</g, '&lt;')\\n+        .replace(/>/g, '&gt;')\\n+        .replace(/\\\"/g, '&quot;')\\n+        .replace(/'/g, '&apos;');\\n+    };\\n+\\n+    // Add btoa/atob for base64 encoding/decoding (browser API polyfill)\\n+    (ctx as any).btoa = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'binary').toString('base64');\\n+    };\\n+    (ctx as any).atob = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'base64').toString('binary');\\n+    };\\n+\\n     // Evaluate the script in a secure sandbox (per-execution instance)\\n     const sandbox = this.createSecureSandbox();\\n     let result: unknown;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/workflow-check-provider.ts\",\"additions\":164,\"deletions\":43,\"changes\":207,\"patch\":\"diff --git a/src/providers/workflow-check-provider.ts b/src/providers/workflow-check-provider.ts\\nindex 116d4dff..681703ce 100644\\n--- a/src/providers/workflow-check-provider.ts\\n+++ b/src/providers/workflow-check-provider.ts\\n@@ -67,34 +67,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n   ): Promise<ReviewSummary> {\\n     const cfg = config as CheckProviderConfig & { workflow?: string; config?: string };\\n     const isConfigPathMode = !!cfg.config && !cfg.workflow;\\n+    const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n \\n-    // Test harness support: allow mocking workflow checks as a black box.\\n-    // If a mock is provided for this step, short-circuit nested execution and\\n-    // return a ReviewSummary with optional output field.\\n-    try {\\n-      const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n-      // test-runner passes hooks on execution context\\n-      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n-      if (mock !== undefined) {\\n-        const ms = mock as any;\\n-        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n-        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n-        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n-        const summary: ReviewSummary & { output?: unknown } = {\\n-          issues: issuesArr,\\n-          output: out,\\n-          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n-        } as any;\\n-        return summary;\\n-      }\\n-    } catch {}\\n-\\n-    // Resolve workflow definition\\n+    // Resolve workflow definition FIRST (needed for input preparation and validation)\\n     let workflow: WorkflowDefinition | undefined;\\n     let workflowId = cfg.workflow as string | undefined;\\n \\n     if (isConfigPathMode) {\\n-      const parentCwd = ((context as any)?._parentContext?.workingDirectory ||\\n+      // Use originalWorkingDirectory for config file resolution - workflow configs\\n+      // should be loaded from the original project path, not the sandbox\\n+      const parentCwd = ((context as any)?._parentContext?.originalWorkingDirectory ||\\n+        (context as any)?._parentContext?.workingDirectory ||\\n+        (context as any)?.originalWorkingDirectory ||\\n         (context as any)?.workingDirectory ||\\n         process.cwd()) as string;\\n       workflow = await this.loadWorkflowFromConfigPath(String(cfg.config), parentCwd);\\n@@ -109,9 +93,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       logger.info(`Executing workflow '${workflowId}'`);\\n     }\\n \\n-    // Prepare inputs\\n+    // Prepare inputs - do this BEFORE mock check so we can validate inputs even when mocked\\n+    // This allows tests to assert that the correct inputs were passed to workflow steps\\n     const inputs = await this.prepareInputs(workflow, config, prInfo, dependencyResults);\\n \\n+    // Capture resolved workflow inputs for testing assertions (reuse prompt capture infrastructure)\\n+    // This allows using `prompts` assertions with `provider: 'workflow'` to verify inputs\\n+    try {\\n+      // Serialize inputs to capture for assertions - specifically the context field\\n+      // which contains the rendered template with dependency outputs\\n+      const inputsCapture = Object.entries(inputs)\\n+        .map(([k, v]) => `${k}: ${typeof v === 'string' ? v : JSON.stringify(v)}`)\\n+        .join('\\\\n\\\\n');\\n+      (context as any)?.hooks?.onPromptCaptured?.({\\n+        step: String(stepName),\\n+        provider: 'workflow',\\n+        prompt: inputsCapture,\\n+      });\\n+    } catch {\\n+      // Ignore capture errors - this is only for testing\\n+    }\\n+\\n     // Validate inputs\\n     const validation = this.registry.validateInputs(workflow, inputs);\\n     if (!validation.valid) {\\n@@ -119,6 +121,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       throw new Error(`Invalid workflow inputs: ${errors}`);\\n     }\\n \\n+    // Test harness support: allow mocking workflow checks as a black box.\\n+    // If a mock is provided for this step, short-circuit nested execution and\\n+    // return a ReviewSummary with optional output field.\\n+    // NOTE: This happens AFTER input preparation/validation so tests can assert inputs are correct\\n+    try {\\n+      // test-runner passes hooks on execution context\\n+      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        const ms = mock as any;\\n+        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n+        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n+        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n+        const summary: ReviewSummary & { output?: unknown } = {\\n+          issues: issuesArr,\\n+          output: out,\\n+          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n+        } as any;\\n+        return summary;\\n+      }\\n+    } catch {}\\n+\\n     // Apply overrides to workflow steps if specified\\n     const modifiedWorkflow = this.applyOverrides(workflow, config);\\n \\n@@ -211,6 +234,80 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Extract eventContext for slack/conversation\\n+    const eventContext = config.eventContext || {};\\n+\\n+    // Debug logging for conversation context\\n+    logger.debug(`[WorkflowProvider] prepareInputs for ${workflow.id}`);\\n+    logger.debug(`[WorkflowProvider] eventContext keys: ${Object.keys(eventContext).join(', ') || 'none'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.slack: ${eventContext.slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.conversation: ${(eventContext as any).conversation ? 'present' : 'absent'}`);\\n+\\n+    // Extract slack context (if provided via eventContext.slack)\\n+    const slack = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        const slackCtx = anyCtx?.slack;\\n+        if (slackCtx && typeof slackCtx === 'object') return slackCtx;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Extract unified conversation context across transports (Slack & GitHub)\\n+    const conversation = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        if (anyCtx?.slack?.conversation) return anyCtx.slack.conversation;\\n+        if (anyCtx?.github?.conversation) return anyCtx.github.conversation;\\n+        if (anyCtx?.conversation) return anyCtx.conversation;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Debug logging for extracted context\\n+    logger.debug(`[WorkflowProvider] slack extracted: ${slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] conversation extracted: ${conversation ? 'present' : 'absent'}`);\\n+    if (conversation) {\\n+      logger.debug(`[WorkflowProvider] conversation.messages count: ${Array.isArray((conversation as any).messages) ? (conversation as any).messages.length : 0}`);\\n+    }\\n+\\n+    // Extract output history from config (passed via __outputHistory)\\n+    const outputHistory = (config as any).__outputHistory as Map<string, unknown[]> | undefined;\\n+    const outputs_history: Record<string, unknown[]> = {};\\n+    if (outputHistory) {\\n+      for (const [k, v] of outputHistory.entries()) {\\n+        outputs_history[k] = v;\\n+      }\\n+    }\\n+\\n+    // Build template context with all available data\\n+    // Extract .output from each dependency result so that outputs['step-name'].field works naturally\\n+    // (not outputs['step-name'].output.field)\\n+    const outputsMap: Record<string, unknown> = {};\\n+    logger.debug(`[WorkflowProvider] dependencyResults: ${dependencyResults ? dependencyResults.size : 'undefined'} entries`);\\n+    if (dependencyResults) {\\n+      for (const [key, result] of dependencyResults.entries()) {\\n+        // Extract the output property, or use the whole result if output is undefined\\n+        const extracted = (result as any).output ?? result;\\n+        outputsMap[key] = extracted;\\n+        // Debug: log what we extracted for each dependency\\n+        const extractedKeys = extracted && typeof extracted === 'object' ? Object.keys(extracted).join(', ') : 'not-object';\\n+        logger.debug(`[WorkflowProvider] outputs['${key}']: keys=[${extractedKeys}]`);\\n+      }\\n+    }\\n+    const templateContext = {\\n+      pr: prInfo,\\n+      outputs: outputsMap,\\n+      env: process.env,\\n+      slack,\\n+      conversation,\\n+      outputs_history,\\n+    };\\n+\\n     // Apply user-provided inputs (args)\\n     const userInputs = config.args || config.workflow_inputs; // Support both for compatibility\\n     if (userInputs) {\\n@@ -219,11 +316,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         if (typeof value === 'string') {\\n           // Check if it's a Liquid template\\n           if (value.includes('{{') || value.includes('{%')) {\\n-            inputs[key] = await this.liquid.parseAndRender(value, {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            });\\n+            inputs[key] = await this.liquid.parseAndRender(value, templateContext);\\n+            // Debug: log rendered template value for important inputs\\n+            if (key === 'text' || key === 'question' || key === 'context') {\\n+              const rendered = String(inputs[key]);\\n+              logger.info(`[WorkflowProvider] Rendered '${key}' input (${rendered.length} chars): ${rendered.substring(0, 500)}${rendered.length > 500 ? '...' : ''}`);\\n+            }\\n           } else {\\n             inputs[key] = value;\\n           }\\n@@ -234,11 +332,7 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           inputs[key] = compileAndRun(\\n             sandbox,\\n             exprValue.expression,\\n-            {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            },\\n+            templateContext,\\n             { injectLog: true, logPrefix: `workflow.input.${key}` }\\n           );\\n         } else {\\n@@ -367,6 +461,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n     try {\\n       await childMemory.initialize();\\n     } catch {}\\n+    // Enhanced debug logging for workspace propagation diagnosis\\n+    const parentWorkspace = parentContext?.workspace;\\n+    logger.info(`[WorkflowProvider] Workspace propagation for nested workflow '${workflow.id}':`);\\n+    logger.info(`[WorkflowProvider]   parentContext exists: ${!!parentContext}`);\\n+    logger.info(`[WorkflowProvider]   parentContext.workspace exists: ${!!parentWorkspace}`);\\n+    if (parentWorkspace) {\\n+      logger.info(`[WorkflowProvider]   parentWorkspace.isEnabled(): ${parentWorkspace.isEnabled?.() ?? 'N/A'}`);\\n+      const projectCount = parentWorkspace.listProjects?.()?.length ?? 'N/A';\\n+      logger.info(`[WorkflowProvider]   parentWorkspace project count: ${projectCount}`);\\n+    } else {\\n+      logger.warn(`[WorkflowProvider]   NO WORKSPACE from parent - nested checkouts won't be added to workspace!`);\\n+    }\\n \\n     const childContext = {\\n       mode: 'state-machine' as const,\\n@@ -374,7 +480,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       checks: checksMetadata,\\n       journal: childJournal,\\n       memory: childMemory,\\n+      // For nested workflows we continue to execute inside the same logical\\n+      // working directory as the parent run. When workspace isolation is\\n+      // enabled on the parent engine, its WorkspaceManager is also propagated\\n+      // so that nested checks (AI, git-checkout, etc.) see the same isolated\\n+      // workspace and project symlinks instead of falling back to the Visor\\n+      // repository root.\\n       workingDirectory: parentContext?.workingDirectory || process.cwd(),\\n+      originalWorkingDirectory:\\n+        parentContext?.originalWorkingDirectory ||\\n+        parentContext?.workingDirectory ||\\n+        process.cwd(),\\n+      workspace: parentWorkspace,\\n       // Always use a fresh session for nested workflows to isolate history\\n       sessionId: uuidv4(),\\n       event: parentContext?.event || prInfo.eventType,\\n@@ -500,6 +617,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     } catch {}\\n \\n+    // Create outputs map that directly exposes .output values (not wrapped in { output, issues })\\n+    // This makes {{ outputs['check-name'] }} work naturally without needing .output\\n+    const outputsMap = Object.fromEntries(\\n+      Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n+    );\\n+\\n     for (const output of workflow.outputs) {\\n       if (output.value_js) {\\n         // JavaScript expression\\n@@ -508,10 +631,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           output.value_js,\\n           {\\n             inputs,\\n-            steps: Object.fromEntries(\\n-              Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-            ),\\n-            outputs: flat,\\n+            outputs: outputsMap,\\n+            // Keep 'steps' as alias for backwards compatibility\\n+            steps: outputsMap,\\n             pr: prInfo,\\n           },\\n           { injectLog: true, logPrefix: `workflow.output.${output.name}` }\\n@@ -520,10 +642,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         // Liquid template\\n         outputs[output.name] = await this.liquid.parseAndRender(output.value, {\\n           inputs,\\n-          steps: Object.fromEntries(\\n-            Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-          ),\\n-          outputs: flat,\\n+          outputs: outputsMap,\\n+          // Keep 'steps' as alias for backwards compatibility\\n+          steps: outputsMap,\\n           pr: prInfo,\\n         });\\n       }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/client.ts\",\"additions\":65,\"deletions\":1,\"changes\":66,\"patch\":\"diff --git a/src/slack/client.ts b/src/slack/client.ts\\nindex 292cc358..435f8b0d 100644\\n--- a/src/slack/client.ts\\n+++ b/src/slack/client.ts\\n@@ -142,6 +142,70 @@ export class SlackClient {\\n     }\\n   }\\n \\n+  public readonly files = {\\n+    /**\\n+     * Upload a file to Slack using files.uploadV2 API\\n+     * @param options Upload options including file content, filename, channel, and thread_ts\\n+     */\\n+    uploadV2: async ({\\n+      content,\\n+      filename,\\n+      channel,\\n+      thread_ts,\\n+      title,\\n+      initial_comment,\\n+    }: {\\n+      content: Buffer;\\n+      filename: string;\\n+      channel: string;\\n+      thread_ts?: string;\\n+      title?: string;\\n+      initial_comment?: string;\\n+    }): Promise<{ ok: boolean; file?: { id: string; permalink?: string } }> => {\\n+      try {\\n+        // Step 1: Get upload URL\\n+        const getUrlResp: any = await this.api('files.getUploadURLExternal', {\\n+          filename,\\n+          length: content.length,\\n+        });\\n+        if (!getUrlResp || getUrlResp.ok !== true || !getUrlResp.upload_url) {\\n+          console.warn(`Slack files.getUploadURLExternal failed: ${getUrlResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 2: Upload file content to the URL\\n+        const uploadResp = await fetch(getUrlResp.upload_url, {\\n+          method: 'POST',\\n+          body: content,\\n+        });\\n+        if (!uploadResp.ok) {\\n+          console.warn(`Slack file upload to URL failed: ${uploadResp.status}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 3: Complete the upload and share to channel\\n+        const completeResp: any = await this.api('files.completeUploadExternal', {\\n+          files: [{ id: getUrlResp.file_id, title: title || filename }],\\n+          channel_id: channel,\\n+          thread_ts,\\n+          initial_comment,\\n+        });\\n+        if (!completeResp || completeResp.ok !== true) {\\n+          console.warn(`Slack files.completeUploadExternal failed: ${completeResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        return {\\n+          ok: true,\\n+          file: completeResp.files?.[0] || { id: getUrlResp.file_id },\\n+        };\\n+      } catch (e) {\\n+        console.warn(`Slack file upload failed: ${e instanceof Error ? e.message : String(e)}`);\\n+        return { ok: false };\\n+      }\\n+    },\\n+  };\\n+\\n   getWebClient(): any {\\n     return {\\n       conversations: {\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/markdown.ts\",\"additions\":204,\"deletions\":5,\"changes\":209,\"patch\":\"diff --git a/src/slack/markdown.ts b/src/slack/markdown.ts\\nindex cc63fb5f..9f91c25f 100644\\n--- a/src/slack/markdown.ts\\n+++ b/src/slack/markdown.ts\\n@@ -3,14 +3,191 @@\\n // without pulling in a full Markdown parser.\\n //\\n // Supported conversions:\\n+// - # Header / ## Header  ‚Üí *Header* (bold with visual separation)\\n // - **bold** / __bold__   ‚Üí *bold*\\n // - [label](url)          ‚Üí <url|label>\\n // - ![alt](url)           ‚Üí <url|alt>\\n // - *italic* (inline)     ‚Üí _italic_\\n+// - ```mermaid blocks     ‚Üí rendered to PNG and uploaded to Slack\\n //\\n // Everything else is passed through unchanged; Slack will still render many\\n // Markdown-like constructs (lists, code fences, etc.) natively.\\n \\n+import { spawn } from 'child_process';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n+import * as os from 'os';\\n+\\n+/**\\n+ * Represents an extracted mermaid diagram\\n+ */\\n+export interface MermaidDiagram {\\n+  /** The full match including ```mermaid and ``` */\\n+  fullMatch: string;\\n+  /** The mermaid code content */\\n+  code: string;\\n+  /** Start index in the original text */\\n+  startIndex: number;\\n+  /** End index in the original text */\\n+  endIndex: number;\\n+}\\n+\\n+/**\\n+ * Extract all mermaid code blocks from text\\n+ */\\n+export function extractMermaidDiagrams(text: string): MermaidDiagram[] {\\n+  const diagrams: MermaidDiagram[] = [];\\n+  // Match ```mermaid followed by newline, content, and closing ```\\n+  const regex = /```mermaid\\\\s*\\\\n([\\\\s\\\\S]*?)```/g;\\n+  let match;\\n+  while ((match = regex.exec(text)) !== null) {\\n+    diagrams.push({\\n+      fullMatch: match[0],\\n+      code: match[1].trim(),\\n+      startIndex: match.index,\\n+      endIndex: match.index + match[0].length,\\n+    });\\n+  }\\n+  return diagrams;\\n+}\\n+\\n+/**\\n+ * Render a mermaid diagram to PNG using mmdc CLI (@mermaid-js/mermaid-cli).\\n+ *\\n+ * Requirements:\\n+ * - Node.js and npx must be available in PATH\\n+ * - Network access on first run (npx downloads the package)\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ *\\n+ * On Linux, you may need to install chromium dependencies:\\n+ *   apt-get install -y chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ *\\n+ * On Docker/CI, consider using a base image with puppeteer support or\\n+ * pre-installing @mermaid-js/mermaid-cli globally.\\n+ *\\n+ * @param mermaidCode The mermaid diagram code\\n+ * @returns Buffer containing PNG data, or null if rendering failed\\n+ */\\n+export async function renderMermaidToPng(mermaidCode: string): Promise<Buffer | null> {\\n+  // Create temp files for input and output\\n+  const tmpDir = os.tmpdir();\\n+  const inputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.mmd`);\\n+  const outputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.png`);\\n+\\n+  try {\\n+    // Write mermaid code to temp file\\n+    fs.writeFileSync(inputFile, mermaidCode, 'utf-8');\\n+\\n+    // Detect system chromium for puppeteer (mermaid-cli dependency)\\n+    // Without this, puppeteer may hang trying to download its own chromium\\n+    const chromiumPaths = [\\n+      '/usr/bin/chromium',\\n+      '/usr/bin/chromium-browser',\\n+      '/usr/bin/google-chrome',\\n+      '/usr/bin/chrome',\\n+    ];\\n+    let chromiumPath: string | undefined;\\n+    for (const p of chromiumPaths) {\\n+      if (fs.existsSync(p)) {\\n+        chromiumPath = p;\\n+        break;\\n+      }\\n+    }\\n+\\n+    // Build environment with chromium path if found\\n+    const env = { ...process.env };\\n+    if (chromiumPath) {\\n+      env.PUPPETEER_EXECUTABLE_PATH = chromiumPath;\\n+    }\\n+\\n+    // Run mmdc to render PNG\\n+    const result = await new Promise<{ success: boolean; error?: string }>((resolve) => {\\n+      const proc = spawn('npx', [\\n+        '--yes',\\n+        '@mermaid-js/mermaid-cli',\\n+        '-i', inputFile,\\n+        '-o', outputFile,\\n+        '-e', 'png',\\n+        '-b', 'white',\\n+        '-w', '1200',\\n+      ], {\\n+        timeout: 60000, // 60 second timeout (first run may download packages)\\n+        stdio: ['pipe', 'pipe', 'pipe'],\\n+        env,\\n+      });\\n+\\n+      let stderr = '';\\n+      proc.stderr?.on('data', (data) => {\\n+        stderr += data.toString();\\n+      });\\n+\\n+      proc.on('close', (code) => {\\n+        if (code === 0) {\\n+          resolve({ success: true });\\n+        } else {\\n+          resolve({ success: false, error: stderr || `Exit code ${code}` });\\n+        }\\n+      });\\n+\\n+      proc.on('error', (err) => {\\n+        resolve({ success: false, error: err.message });\\n+      });\\n+    });\\n+\\n+    if (!result.success) {\\n+      console.warn(`Mermaid rendering failed: ${result.error}`);\\n+      return null;\\n+    }\\n+\\n+    // Read the output PNG\\n+    if (!fs.existsSync(outputFile)) {\\n+      console.warn('Mermaid output file not created');\\n+      return null;\\n+    }\\n+\\n+    const pngBuffer = fs.readFileSync(outputFile);\\n+    return pngBuffer;\\n+  } catch (e) {\\n+    console.warn(`Mermaid rendering error: ${e instanceof Error ? e.message : String(e)}`);\\n+    return null;\\n+  } finally {\\n+    // Cleanup temp files\\n+    try {\\n+      if (fs.existsSync(inputFile)) fs.unlinkSync(inputFile);\\n+      if (fs.existsSync(outputFile)) fs.unlinkSync(outputFile);\\n+    } catch {\\n+      // Ignore cleanup errors\\n+    }\\n+  }\\n+}\\n+\\n+/**\\n+ * Replace mermaid blocks in text with a placeholder message\\n+ * @param text Original text\\n+ * @param diagrams Extracted diagrams\\n+ * @param replacement Text to replace each diagram with (or a function that returns replacement for each index)\\n+ */\\n+export function replaceMermaidBlocks(\\n+  text: string,\\n+  diagrams: MermaidDiagram[],\\n+  replacement: string | ((index: number) => string) = '_(See diagram above)_'\\n+): string {\\n+  if (diagrams.length === 0) return text;\\n+\\n+  // Sort by start index descending to replace from end to start (preserves indices)\\n+  const sorted = [...diagrams].sort((a, b) => b.startIndex - a.startIndex);\\n+\\n+  let result = text;\\n+  sorted.forEach((diagram, sortedIndex) => {\\n+    // Calculate original index (since we sorted in reverse)\\n+    const originalIndex = diagrams.length - 1 - sortedIndex;\\n+    const rep = typeof replacement === 'function' ? replacement(originalIndex) : replacement;\\n+    result = result.slice(0, diagram.startIndex) + rep + result.slice(diagram.endIndex);\\n+  });\\n+\\n+  return result;\\n+}\\n+\\n export function markdownToSlack(text: string): string {\\n   if (!text || typeof text !== 'string') return '';\\n \\n@@ -33,7 +210,7 @@ export function markdownToSlack(text: string): string {\\n   out = out.replace(/\\\\*\\\\*([^*]+)\\\\*\\\\*/g, (_m, inner: string) => `*${inner}*`);\\n   out = out.replace(/__([^_]+)__/g, (_m, inner: string) => `*${inner}*`);\\n \\n-  // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation).\\n+  // Process lines for headers and bullet lists.\\n   // Slack's mrkdwn handles \\\"‚Ä¢\\\" bullets more naturally than raw \\\"-\\\" Markdown.\\n   const lines = out.split(/\\\\r?\\\\n/);\\n   let inCodeBlock = false;\\n@@ -46,9 +223,31 @@ export function markdownToSlack(text: string): string {\\n       continue;\\n     }\\n     if (inCodeBlock) continue;\\n-    const match = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n-    if (match) {\\n-      const [, indent, , rest] = match;\\n+\\n+    // Headers: # Header ‚Üí *Header* (Slack doesn't have native headers)\\n+    // Match 1-6 # at start of line, followed by space and text\\n+    const headerMatch = /^(#{1,6})\\\\s+(.+)$/.exec(trimmed);\\n+    if (headerMatch) {\\n+      const [, hashes, headerText] = headerMatch;\\n+      // For h1/h2, add extra emphasis with newline before (if not first line\\n+      // and previous line is not empty/header/code-fence)\\n+      const prevLine = i > 0 ? lines[i - 1].trim() : '';\\n+      const prevIsHeaderOrFence =\\n+        /^#{1,6}\\\\s+/.test(prevLine) ||\\n+        /^\\\\*[^*]+\\\\*$/.test(prevLine) ||\\n+        /^```/.test(prevLine);\\n+      if (hashes.length <= 2 && i > 0 && prevLine !== '' && !prevIsHeaderOrFence) {\\n+        lines[i] = `\\\\n*${headerText.trim()}*`;\\n+      } else {\\n+        lines[i] = `*${headerText.trim()}*`;\\n+      }\\n+      continue;\\n+    }\\n+\\n+    // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation)\\n+    const bulletMatch = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n+    if (bulletMatch) {\\n+      const [, indent, , rest] = bulletMatch;\\n       lines[i] = `${indent}‚Ä¢ ${rest}`;\\n     }\\n   }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/prompt-state.ts\",\"additions\":19,\"deletions\":2,\"changes\":21,\"patch\":\"diff --git a/src/slack/prompt-state.ts b/src/slack/prompt-state.ts\\nindex a13233b2..544777c6 100644\\n--- a/src/slack/prompt-state.ts\\n+++ b/src/slack/prompt-state.ts\\n@@ -95,7 +95,10 @@ export class PromptStateManager {\\n   setFirstMessage(channel: string, threadTs: string, text: string): void {\\n     const key = this.key(channel, threadTs);\\n     if (!text || !text.trim()) return;\\n-    if (!this.firstMessage.has(key)) {\\n+    const existing = this.firstMessage.get(key);\\n+    // Only set if: no entry exists OR the existing entry was already consumed\\n+    // This allows new messages to be captured after a resume cycle\\n+    if (!existing || existing.consumed) {\\n       this.firstMessage.set(key, { text, consumed: false });\\n     }\\n   }\\n@@ -130,6 +133,20 @@ export class PromptStateManager {\\n         removed++;\\n       }\\n     }\\n+    // Also clean up stale firstMessage entries (consumed entries older than TTL)\\n+    // Keep unconsumed entries to avoid losing user messages\\n+    for (const [key] of this.firstMessage.entries()) {\\n+      const waitingInfo = this.waiting.get(key);\\n+      // If no corresponding waiting entry exists and the firstMessage was consumed,\\n+      // the conversation is likely complete - safe to remove\\n+      if (!waitingInfo) {\\n+        const entry = this.firstMessage.get(key);\\n+        if (entry?.consumed) {\\n+          this.firstMessage.delete(key);\\n+          removed++;\\n+        }\\n+      }\\n+    }\\n     if (removed) {\\n       try {\\n         logger.info(`[prompt-state] cleanup removed ${removed} entries`);\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/socket-runner.ts\",\"additions\":56,\"deletions\":32,\"changes\":88,\"patch\":\"diff --git a/src/slack/socket-runner.ts b/src/slack/socket-runner.ts\\nindex d1681a03..5b461d22 100644\\n--- a/src/slack/socket-runner.ts\\n+++ b/src/slack/socket-runner.ts\\n@@ -7,6 +7,7 @@ import { SlackAdapter } from './adapter';\\n import { CachePrewarmer } from './cache-prewarmer';\\n import { RateLimiter, type RateLimitConfig } from './rate-limiter';\\n import type { SlackBotConfig } from '../types/bot';\\n+import { withActiveSpan } from '../telemetry/trace-helpers';\\n \\n type SlackSocketConfig = {\\n   appToken?: string; // xapp- token\\n@@ -30,6 +31,7 @@ export class SlackSocketRunner {\\n   private botUserId?: string;\\n   private processedKeys: Map<string, number> = new Map();\\n   private adapter?: SlackAdapter;\\n+  private retryCount = 0;\\n \\n   constructor(engine: StateMachineExecutionEngine, cfg: VisorConfig, opts: SlackSocketConfig) {\\n     const app = opts.appToken || process.env.SLACK_APP_TOKEN || '';\\n@@ -107,7 +109,10 @@ export class SlackSocketRunner {\\n \\n   private async connect(url: string): Promise<void> {\\n     this.ws = new WebSocket(url);\\n-    this.ws.on('open', () => logger.info('[SlackSocket] WebSocket connected'));\\n+    this.ws.on('open', () => {\\n+      this.retryCount = 0; // Reset on successful connection\\n+      logger.info('[SlackSocket] WebSocket connected');\\n+    });\\n     this.ws.on('close', (code, reason) => {\\n       logger.warn(`[SlackSocket] WebSocket closed: ${code} ${reason}`);\\n       setTimeout(() => this.restart().catch(() => {}), 1000);\\n@@ -123,7 +128,13 @@ export class SlackSocketRunner {\\n       const url = await this.openConnection();\\n       await this.connect(url);\\n     } catch (e) {\\n-      logger.error(`[SlackSocket] Restart failed: ${e instanceof Error ? e.message : e}`);\\n+      this.retryCount++;\\n+      // Exponential backoff: 2s, 4s, 8s, 16s, 32s, capped at 60s\\n+      const delay = Math.min(2000 * Math.pow(2, this.retryCount - 1), 60000);\\n+      logger.error(\\n+        `[SlackSocket] Restart failed (attempt ${this.retryCount}), retrying in ${Math.round(delay / 1000)}s: ${e instanceof Error ? e.message : e}`\\n+      );\\n+      setTimeout(() => this.restart().catch(() => {}), delay);\\n     }\\n   }\\n \\n@@ -296,36 +307,49 @@ export class SlackSocketRunner {\\n           });\\n         }\\n       } catch {}\\n-      if (path) {\\n-        try {\\n-          const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n-          const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n-          logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n-          await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n-            webhookContext: { webhookData: map, eventType: 'manual' },\\n-            debug: process.env.VISOR_DEBUG === 'true',\\n-          });\\n-          if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n-          return; // resume path handled\\n-        } catch (e) {\\n-          logger.warn(\\n-            `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n-              e instanceof Error ? e.message : String(e)\\n-            }`\\n-          );\\n-        }\\n-      }\\n+      try {\\n+        await withActiveSpan(\\n+          'visor.run',\\n+          {\\n+            'visor.run.source': 'slack',\\n+            'slack.event.type': String(type || ''),\\n+            'slack.channel': channelId,\\n+            'slack.thread_ts': threadTs,\\n+          },\\n+          async () => {\\n+            if (path) {\\n+              try {\\n+                const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n+                const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n+                logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n+                await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n+                  webhookContext: { webhookData: map, eventType: 'manual' },\\n+                  debug: process.env.VISOR_DEBUG === 'true',\\n+                });\\n+                return;\\n+              } catch (e) {\\n+                logger.warn(\\n+                  `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n+                    e instanceof Error ? e.message : String(e)\\n+                  }`\\n+                );\\n+              }\\n+            }\\n \\n-      // Cold run (no snapshot)\\n-      await runEngine.executeChecks({\\n-        checks: allChecks,\\n-        showDetails: true,\\n-        outputFormat: 'json',\\n-        config: cfgForRun,\\n-        webhookContext: { webhookData: map, eventType: 'manual' },\\n-        debug: process.env.VISOR_DEBUG === 'true',\\n-      } as any);\\n-      if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+            // Cold run (no snapshot)\\n+            await runEngine.executeChecks({\\n+              checks: allChecks,\\n+              showDetails: true,\\n+              outputFormat: 'json',\\n+              config: cfgForRun,\\n+              webhookContext: { webhookData: map, eventType: 'manual' },\\n+              debug: process.env.VISOR_DEBUG === 'true',\\n+            } as any);\\n+          }\\n+        );\\n+      } finally {\\n+        if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+      }\\n     } catch (e) {\\n       logger.error(\\n         `[SlackSocket] Engine execution failed: ${e instanceof Error ? e.message : String(e)}`\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/context/build-engine-context.ts\",\"additions\":11,\"deletions\":4,\"changes\":15,\"patch\":\"diff --git a/src/state-machine/context/build-engine-context.ts b/src/state-machine/context/build-engine-context.ts\\nindex 149c73bf..45e63b6a 100644\\n--- a/src/state-machine/context/build-engine-context.ts\\n+++ b/src/state-machine/context/build-engine-context.ts\\n@@ -57,7 +57,8 @@ export function buildEngineContextForRun(\\n       ) as EventTrigger[],\\n       group: checkConfig.group,\\n       providerType: checkConfig.type || 'ai',\\n-      dependencies: checkConfig.depends_on || [],\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: Array.isArray(checkConfig.depends_on) ? checkConfig.depends_on : checkConfig.depends_on ? [checkConfig.depends_on] : [],\\n     };\\n   }\\n \\n@@ -134,11 +135,14 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n   const originalPath = context.workingDirectory || process.cwd();\\n \\n   try {\\n+    // Check if workspace should be kept (for debugging)\\n+    const keepWorkspace = process.env.VISOR_KEEP_WORKSPACE === 'true';\\n+\\n     // Create workspace manager\\n     const workspace = WorkspaceManager.getInstance(context.sessionId, originalPath, {\\n       enabled: true,\\n-      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH,\\n-      cleanupOnExit: workspaceConfig?.cleanup_on_exit !== false,\\n+      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH || '/tmp/visor-workspaces',\\n+      cleanupOnExit: keepWorkspace ? false : workspaceConfig?.cleanup_on_exit !== false,\\n     });\\n \\n     // Initialize workspace (creates main project worktree)\\n@@ -151,6 +155,9 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n \\n     logger.info(`[Workspace] Initialized workspace: ${info.workspacePath}`);\\n     logger.debug(`[Workspace] Main project at: ${info.mainProjectPath}`);\\n+    if (keepWorkspace) {\\n+      logger.info(`[Workspace] Keeping workspace after execution (--keep-workspace)`);\\n+    }\\n \\n     return context;\\n   } catch (error) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/execution-invoker.ts\",\"additions\":84,\"deletions\":4,\"changes\":88,\"patch\":\"diff --git a/src/state-machine/dispatch/execution-invoker.ts b/src/state-machine/dispatch/execution-invoker.ts\\nindex eb7587d3..798ca8e4 100644\\n--- a/src/state-machine/dispatch/execution-invoker.ts\\n+++ b/src/state-machine/dispatch/execution-invoker.ts\\n@@ -356,10 +356,17 @@ export async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -413,6 +420,31 @@ export async function executeSingleCheck(\\n     }\\n   }\\n \\n+  // Banner-style log when a check actually starts executing. This is emitted\\n+  // after all gating (if/depends_on) has passed so it only appears for real\\n+  // runs, and gives a clear visual separator in the logs between checks.\\n+  try {\\n+    const wave = state.wave;\\n+    const level = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${level}) ‚îÅ‚îÅ‚îÅ`;\\n+\\n+    // When running in a TTY, colour the entire banner line for extra\\n+    // visibility; keep plain text for JSON/SARIF or non-TTY environments.\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   let forEachParent: string | undefined;\\n   let forEachItems: unknown[] | undefined;\\n   for (const depId of depList) {\\n@@ -563,13 +595,56 @@ export async function executeSingleCheck(\\n       files: [],\\n       commits: [],\\n     };\\n+    // Derive AI session reuse context for this check (self-mode only for now).\\n+    // When reuse_ai_session: 'self' is configured, look up the last root-scope\\n+    // journal entry for this check in the current engine session and expose its\\n+    // sessionId via ExecutionContext so ai-check-provider can call\\n+    // executeReviewWithSessionReuse() against the same ProbeAgent session.\\n+    let parentSessionId: string | undefined;\\n+    let reuseSession = false;\\n+    try {\\n+      const reuseCfg: unknown = (checkConfig as any).reuse_ai_session;\\n+      if (reuseCfg === 'self') {\\n+        const snapshotId = context.journal.beginSnapshot();\\n+        const visible = context.journal.readVisible(\\n+          context.sessionId,\\n+          snapshotId,\\n+          context.event as any\\n+        );\\n+        // Prefer the most recent root-scope result for this check\\n+        const prior = visible.filter(\\n+          e => e.checkId === checkId && (!e.scope || e.scope.length === 0)\\n+        );\\n+        if (prior.length > 0) {\\n+          const last = prior[prior.length - 1];\\n+          const sess = (last.result as any)?.sessionId;\\n+          if (typeof sess === 'string' && sess.length > 0) {\\n+            parentSessionId = sess;\\n+            reuseSession = true;\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only ‚Äì fall back to normal (non-reuse) execution on error.\\n+      parentSessionId = undefined;\\n+      reuseSession = false;\\n+    }\\n+\\n     const executionContext = {\\n       ...context.executionContext,\\n       _engineMode: context.mode,\\n       _parentContext: context,\\n       _parentState: state,\\n+      // Explicitly propagate workspace reference for nested workflows\\n+      workspace: context.workspace,\\n     };\\n \\n+    // Attach session reuse hints for providers that support them (AI, Claude Code, etc).\\n+    if (reuseSession && parentSessionId) {\\n+      (executionContext as any).parentSessionId = parentSessionId;\\n+      (executionContext as any).reuseSession = true;\\n+    }\\n+\\n     // Handle on_init lifecycle hook BEFORE main execution\\n     if (checkConfig.on_init) {\\n       try {\\n@@ -613,7 +688,12 @@ export async function executeSingleCheck(\\n \\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/foreach-processor.ts\",\"additions\":27,\"deletions\":2,\"changes\":29,\"patch\":\"diff --git a/src/state-machine/dispatch/foreach-processor.ts b/src/state-machine/dispatch/foreach-processor.ts\\nindex 336bf9df..ab630986 100644\\n--- a/src/state-machine/dispatch/foreach-processor.ts\\n+++ b/src/state-machine/dispatch/foreach-processor.ts\\n@@ -77,6 +77,26 @@ export async function executeCheckWithForEachItems(\\n   const allContents: string[] = [];\\n   const perIterationDurations: number[] = [];\\n \\n+  // Emit a banner for the forEach parent so logs clearly show when we are\\n+  // entering the aggregated execution for that step.\\n+  try {\\n+    const wave = state.wave;\\n+    const lvl = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${lvl}, forEach parent) ‚îÅ‚îÅ‚îÅ`;\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   for (let itemIndex = 0; itemIndex < forEachItems.length; itemIndex++) {\\n     const iterationStartMs = Date.now();\\n     const scope: Array<{ check: string; index: number }> = [\\n@@ -248,7 +268,12 @@ export async function executeCheckWithForEachItems(\\n \\n       const result = await withActiveSpan(\\n         `visor.check.${checkId}`,\\n-        { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+        {\\n+          'visor.check.id': checkId,\\n+          'visor.check.type': providerType,\\n+          session_id: context.sessionId,\\n+          wave: state.wave,\\n+        },\\n         async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n       );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/stats-manager.ts\",\"additions\":21,\"deletions\":1,\"changes\":22,\"patch\":\"diff --git a/src/state-machine/dispatch/stats-manager.ts b/src/state-machine/dispatch/stats-manager.ts\\nindex 8d04436e..2f317b59 100644\\n--- a/src/state-machine/dispatch/stats-manager.ts\\n+++ b/src/state-machine/dispatch/stats-manager.ts\\n@@ -44,6 +44,26 @@ export function updateStats(\\n       issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n     };\\n \\n+    // Respect explicit skip markers on the result (set by LevelDispatch).\\n+    const skippedMarker = (result as any).__skipped;\\n+    if (skippedMarker) {\\n+      stats.skipped = true;\\n+      stats.skipReason =\\n+        typeof skippedMarker === 'string'\\n+          ? (skippedMarker as any)\\n+          : (stats.skipReason as any) || 'if_condition';\\n+      // A skipped result should not contribute to run counters; reset\\n+      // them explicitly to guard against any prior values lingering\\n+      // from earlier runs of the same check.\\n+      stats.totalRuns = 0;\\n+      stats.successfulRuns = 0;\\n+      stats.failedRuns = 0;\\n+      stats.skippedRuns++;\\n+      // Do not count this as a run; keep totalRuns/failed/success unchanged.\\n+      state.stats.set(checkId, stats);\\n+      continue;\\n+    }\\n+\\n     // If this check was previously marked as skipped and now executed,\\n     // clear the skipped flag and any skip counters to ensure the run is visible.\\n     if (stats.skipped) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/runner.ts\",\"additions\":10,\"deletions\":7,\"changes\":17,\"patch\":\"diff --git a/src/state-machine/runner.ts b/src/state-machine/runner.ts\\nindex ee86b876..a13a5efd 100644\\n--- a/src/state-machine/runner.ts\\n+++ b/src/state-machine/runner.ts\\n@@ -124,14 +124,17 @@ export class StateMachineRunner {\\n    */\\n   private async executeState(state: EngineState): Promise<void> {\\n     // M4: Wrap state execution in OTEL span\\n+    const attrs: Record<string, unknown> = {\\n+      state: state,\\n+      engine_mode: this.context.mode,\\n+      wave: this.state.wave,\\n+      session_id: this.context.sessionId,\\n+    };\\n+    const waveKind = (this.state as any)?.flags?.waveKind;\\n+    if (waveKind) attrs.wave_kind = waveKind;\\n     return withActiveSpan(\\n       `engine.state.${state.toLowerCase()}`,\\n-      {\\n-        state: state,\\n-        engine_mode: this.context.mode,\\n-        wave: this.state.wave,\\n-        session_id: this.context.sessionId,\\n-      },\\n+      attrs,\\n       async () => {\\n         try {\\n           switch (state) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/level-dispatch.ts\",\"additions\":152,\"deletions\":15,\"changes\":167,\"patch\":\"diff --git a/src/state-machine/states/level-dispatch.ts b/src/state-machine/states/level-dispatch.ts\\nindex 5588b0f5..95392197 100644\\n--- a/src/state-machine/states/level-dispatch.ts\\n+++ b/src/state-machine/states/level-dispatch.ts\\n@@ -26,7 +26,7 @@ import type { CheckExecutionStats } from '../../types/execution';\\n import type { CheckProviderConfig } from '../../providers/check-provider.interface';\\n import type { CheckConfig } from '../../types/config';\\n import { handleRouting, checkLoopBudget } from './routing';\\n-import { withActiveSpan } from '../../telemetry/trace-helpers';\\n+import { withActiveSpan, setSpanAttributes, addEvent } from '../../telemetry/trace-helpers';\\n import { emitMermaidFromMarkdown } from '../../utils/mermaid-telemetry';\\n import { emitNdjsonSpanWithEvents, emitNdjsonFallback } from '../../telemetry/fallback-ndjson';\\n import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n@@ -46,6 +46,32 @@ function mapCheckNameToFocus(checkName: string): string {\\n   return focusMap[checkName] || 'all';\\n }\\n \\n+function formatScopeLabel(scope: Array<{ check: string; index: number }> | undefined): string {\\n+  if (!scope || scope.length === 0) return '';\\n+  return scope.map(item => `${item.check}:${item.index}`).join('|');\\n+}\\n+\\n+function recordOnFinishRoutingEvent(args: {\\n+  checkId: string;\\n+  action: 'run' | 'goto';\\n+  target: string;\\n+  source: 'run' | 'goto' | 'goto_js' | 'transitions';\\n+  scope?: Array<{ check: string; index: number }>;\\n+  gotoEvent?: string;\\n+}): void {\\n+  const attrs: Record<string, unknown> = {\\n+    check_id: args.checkId,\\n+    trigger: 'on_finish',\\n+    action: args.action,\\n+    target: args.target,\\n+    source: args.source,\\n+  };\\n+  const scopeLabel = formatScopeLabel(args.scope);\\n+  if (scopeLabel) attrs.scope = scopeLabel;\\n+  if (args.gotoEvent) attrs.goto_event = args.gotoEvent;\\n+  addEvent('visor.routing', attrs);\\n+}\\n+\\n /**\\n  * Build output history Map from journal for template rendering\\n  * This matches the format expected by AI providers\\n@@ -121,12 +147,13 @@ async function evaluateIfCondition(\\n         return false;\\n       }\\n     })();\\n+    // Steps with dependencies should always see outputs from all completed steps.\\n     // In forward-run waves (from on_success/on_fail goto), guards should see the\\n     // latest global outputs even if the check has no explicit dependencies.\\n     // In wave-retry (from on_finish), restrict to checks with dependencies to\\n     // avoid wrongly skipping top-level prompts like 'ask'.\\n     const useGlobalOutputs =\\n-      (useGlobalOutputsFlag && waveKind === 'forward') || (useGlobalOutputsFlag && hasDeps);\\n+      hasDeps || (useGlobalOutputsFlag && waveKind === 'forward');\\n \\n     if (useGlobalOutputs) {\\n       // Forward-run wave: allow guards to consult latest outputs from the entire\\n@@ -199,6 +226,7 @@ async function evaluateIfCondition(\\n       baseBranch: (context.prInfo as any)?.baseBranch,\\n       filesChanged: context.prInfo?.files?.map(f => f.filename),\\n       environment: envSnapshot,\\n+      workflowInputs: (context.config as any).workflow_inputs || {},\\n     };\\n \\n     const shouldRun = await evaluator.evaluateIfCondition(checkId, ifExpression, contextData);\\n@@ -238,6 +266,11 @@ export async function handleLevelDispatch(\\n   // Update current level tracking\\n   state.currentLevel = level.level;\\n   state.currentLevelChecks = new Set(level.parallel);\\n+  const levelChecksPreview = level.parallel.slice(0, 5).join(',');\\n+  setSpanAttributes({\\n+    level_size: level.parallel.length,\\n+    level_checks_preview: levelChecksPreview,\\n+  });\\n \\n   // Emit level ready event\\n   emitEvent({ type: 'LevelReady', level, wave: state.wave });\\n@@ -689,6 +722,39 @@ async function executeCheckWithForEachItems(\\n         }\\n       } catch {}\\n \\n+      // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+      // The socket-runner stores conversation data in webhookData under the endpoint key\\n+      try {\\n+        const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+        const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+        if (context.debug) {\\n+          logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+        }\\n+        if (webhookData && webhookData.size > 0) {\\n+          // Find the payload with slack_conversation\\n+          for (const payload of webhookData.values()) {\\n+            const slackConv = (payload as any)?.slack_conversation;\\n+            if (slackConv) {\\n+              // Build slack context with event and conversation\\n+              const event = (payload as any)?.event;\\n+              const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+              if (context.debug) {\\n+                logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+              }\\n+              (providerConfig as any).eventContext = {\\n+                ...(providerConfig as any).eventContext,\\n+                slack: {\\n+                  event: event || {},\\n+                  conversation: slackConv,\\n+                },\\n+                conversation: slackConv, // Also expose at top level for convenience\\n+              };\\n+              break;\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n+\\n       // Build dependency results with scope\\n       const dependencyResults = buildDependencyResultsWithScope(\\n         checkId,\\n@@ -820,15 +886,17 @@ async function executeCheckWithForEachItems(\\n       } catch {}\\n \\n       // Execute provider with telemetry\\n-      const itemResult = await withActiveSpan(\\n-        `visor.check.${checkId}`,\\n-        {\\n-          'visor.check.id': checkId,\\n-          'visor.check.type': providerType,\\n-          'visor.foreach.index': itemIndex,\\n-        },\\n-        async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n-      );\\n+    const itemResult = await withActiveSpan(\\n+      `visor.check.${checkId}`,\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        'visor.foreach.index': itemIndex,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n+      async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n+    );\\n \\n       // Enrich issues\\n       const enrichedIssues = (itemResult.issues || []).map((issue: ReviewIssue) => ({\\n@@ -942,6 +1010,7 @@ async function executeCheckWithForEachItems(\\n             const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n               previousResults: dependencyResults as any,\\n               event: context.event || 'manual',\\n+              output: output, // Pass the iteration output for guarantee evaluation\\n             } as any);\\n             if (!holds) {\\n               const issue: ReviewIssue = {\\n@@ -1311,6 +1380,13 @@ async function executeCheckWithForEachItems(\\n             // Increment loop count\\n             state.routingLoopCount++;\\n \\n+            recordOnFinishRoutingEvent({\\n+              checkId: forEachParent,\\n+              action: 'run',\\n+              target: targetCheck,\\n+              source: 'run',\\n+              scope: [],\\n+            });\\n             emitEvent({\\n               type: 'ForwardRunRequested',\\n               target: targetCheck,\\n@@ -1358,6 +1434,14 @@ async function executeCheckWithForEachItems(\\n                 return aggregatedResult; // abort further routing\\n               }\\n               state.routingLoopCount++;\\n+              recordOnFinishRoutingEvent({\\n+                checkId: forEachParent,\\n+                action: 'goto',\\n+                target: transTarget.to,\\n+                source: 'transitions',\\n+                scope: [],\\n+                gotoEvent: (transTarget as any).goto_event,\\n+              });\\n               emitEvent({\\n                 type: 'ForwardRunRequested',\\n                 target: transTarget.to,\\n@@ -1464,6 +1548,13 @@ async function executeCheckWithForEachItems(\\n           // Increment loop count\\n           state.routingLoopCount++;\\n \\n+          recordOnFinishRoutingEvent({\\n+            checkId: forEachParent,\\n+            action: 'goto',\\n+            target: gotoTarget,\\n+            source: onFinish.goto_js ? 'goto_js' : 'goto',\\n+            scope: [],\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: gotoTarget,\\n@@ -1618,10 +1709,17 @@ async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -1923,6 +2021,39 @@ async function executeSingleCheck(\\n       }\\n     } catch {}\\n \\n+    // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+    // The socket-runner stores conversation data in webhookData under the endpoint key\\n+    try {\\n+      const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+      const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+      if (context.debug) {\\n+        logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+      }\\n+      if (webhookData && webhookData.size > 0) {\\n+        // Find the payload with slack_conversation\\n+        for (const payload of webhookData.values()) {\\n+          const slackConv = (payload as any)?.slack_conversation;\\n+          if (slackConv) {\\n+            // Build slack context with event and conversation\\n+            const event = (payload as any)?.event;\\n+            const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+            if (context.debug) {\\n+              logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+            }\\n+            (providerConfig as any).eventContext = {\\n+              ...(providerConfig as any).eventContext,\\n+              slack: {\\n+                event: event || {},\\n+                conversation: slackConv,\\n+              },\\n+              conversation: slackConv, // Also expose at top level for convenience\\n+            };\\n+            break;\\n+          }\\n+        }\\n+      }\\n+    } catch {}\\n+\\n     // Build dependency results\\n     const dependencyResults = buildDependencyResults(checkId, checkConfig, context, state);\\n \\n@@ -2016,7 +2147,12 @@ async function executeSingleCheck(\\n     // Execute provider with telemetry\\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n@@ -2098,6 +2234,7 @@ async function executeSingleCheck(\\n           const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n             previousResults: dependencyResults as any,\\n             event: context.event || 'manual',\\n+            output: enrichedResult.output,\\n           } as any);\\n           if (!holds) {\\n             const issue: ReviewIssue = {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/routing.ts\",\"additions\":165,\"deletions\":1,\"changes\":166,\"patch\":\"diff --git a/src/state-machine/states/routing.ts b/src/state-machine/states/routing.ts\\nindex 5752a5bb..b1fd8654 100644\\n--- a/src/state-machine/states/routing.ts\\n+++ b/src/state-machine/states/routing.ts\\n@@ -15,6 +15,7 @@ import type { EngineContext, RunState, EngineState, EngineEvent } from '../../ty\\n import type { ReviewSummary, ReviewIssue } from '../../reviewer';\\n import type { CheckConfig, OnFailConfig, TransitionRule } from '../../types/config';\\n import { logger } from '../../logger';\\n+import { addEvent } from '../../telemetry/trace-helpers';\\n import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n import { createSecureSandbox, compileAndRun } from '../../utils/sandbox';\\n import { MemoryStore } from '../../memory-store';\\n@@ -153,6 +154,37 @@ function createMemoryHelpers() {\\n   };\\n }\\n \\n+type RoutingTrigger = 'on_success' | 'on_fail' | 'on_finish';\\n+type RoutingAction = 'run' | 'goto' | 'retry';\\n+type RoutingSource = 'run' | 'run_js' | 'goto' | 'goto_js' | 'transitions' | 'retry';\\n+\\n+function formatScopeLabel(scope: Array<{ check: string; index: number }> | undefined): string {\\n+  if (!scope || scope.length === 0) return '';\\n+  return scope.map(item => `${item.check}:${item.index}`).join('|');\\n+}\\n+\\n+function recordRoutingEvent(args: {\\n+  checkId: string;\\n+  trigger: RoutingTrigger;\\n+  action: RoutingAction;\\n+  target?: string;\\n+  source?: RoutingSource;\\n+  scope?: Array<{ check: string; index: number }>;\\n+  gotoEvent?: string;\\n+}): void {\\n+  const attrs: Record<string, unknown> = {\\n+    check_id: args.checkId,\\n+    trigger: args.trigger,\\n+    action: args.action,\\n+  };\\n+  if (args.target) attrs.target = args.target;\\n+  if (args.source) attrs.source = args.source;\\n+  const scopeLabel = formatScopeLabel(args.scope);\\n+  if (scopeLabel) attrs.scope = scopeLabel;\\n+  if (args.gotoEvent) attrs.goto_event = args.gotoEvent;\\n+  addEvent('visor.routing', attrs);\\n+}\\n+\\n /**\\n  * Handle routing state - evaluate conditions and decide next actions\\n  */\\n@@ -271,6 +303,14 @@ async function processOnFinish(\\n             { check: checkId, index: itemIndex },\\n           ];\\n \\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_finish',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -284,6 +324,14 @@ async function processOnFinish(\\n         // Increment loop count\\n         state.routingLoopCount++;\\n \\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_finish',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope: [],\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -328,6 +376,14 @@ async function processOnFinish(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_finish',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -362,6 +418,15 @@ async function processOnFinish(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_finish',\\n+        action: 'goto',\\n+        target: finishTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: finishTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: finishTransTarget.to,\\n@@ -406,6 +471,14 @@ async function processOnFinish(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_finish',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onFinish.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+    });\\n     // Enqueue forward run event\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n@@ -640,6 +713,14 @@ async function processOnSuccess(\\n             { check: checkId, index: itemIndex },\\n           ];\\n \\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_success',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -652,6 +733,14 @@ async function processOnSuccess(\\n         // Increment loop count\\n         state.routingLoopCount++;\\n \\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_success',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope,\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -695,6 +784,14 @@ async function processOnSuccess(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_success',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -728,6 +825,15 @@ async function processOnSuccess(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_success',\\n+        action: 'goto',\\n+        target: successTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: successTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: successTransTarget.to,\\n@@ -773,6 +879,15 @@ async function processOnSuccess(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_success',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onSuccess.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+      gotoEvent: onSuccess.goto_event,\\n+    });\\n     // Enqueue forward run event with optional event override\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n@@ -864,6 +979,14 @@ async function processOnFail(\\n           const itemScope: Array<{ check: string; index: number }> = [\\n             { check: checkId, index: itemIndex },\\n           ];\\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_fail',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -874,6 +997,14 @@ async function processOnFail(\\n       } else {\\n         // No forEach context: preserve current scope (if any)\\n         state.routingLoopCount++;\\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_fail',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope,\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -917,6 +1048,14 @@ async function processOnFail(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_fail',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -958,6 +1097,13 @@ async function processOnFail(\\n \\n         // Increment loop count and schedule forward run for the same check\\n         state.routingLoopCount++;\\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_fail',\\n+          action: 'retry',\\n+          source: 'retry',\\n+          scope: sc || [],\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: checkId,\\n@@ -1013,6 +1159,15 @@ async function processOnFail(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_fail',\\n+        action: 'goto',\\n+        target: failTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: failTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: failTransTarget.to,\\n@@ -1058,6 +1213,15 @@ async function processOnFail(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_fail',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onFail.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+      gotoEvent: onFail.goto_event,\\n+    });\\n     // Enqueue forward run event with optional event override\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/wave-planning.ts\",\"additions\":10,\"deletions\":3,\"changes\":13,\"patch\":\"diff --git a/src/state-machine/states/wave-planning.ts b/src/state-machine/states/wave-planning.ts\\nindex 7551d55c..7b42f043 100644\\n--- a/src/state-machine/states/wave-planning.ts\\n+++ b/src/state-machine/states/wave-planning.ts\\n@@ -351,8 +351,11 @@ export async function handleWavePlanning(\\n     for (const id of checksToRun) {\\n       // Only include dependencies that are within the same subset (often none).\\n       const cfg = context.config.checks?.[id];\\n-      const deps = (cfg?.depends_on || []).filter((d: string) => checksToRun.includes(d));\\n-      subDeps[id] = deps as string[];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = cfg?.depends_on;\\n+      const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n+      const deps = depsArray.filter((d: string) => checksToRun.includes(d));\\n+      subDeps[id] = deps;\\n     }\\n \\n     const subGraph = DependencyResolver.buildDependencyGraph(subDeps);\\n@@ -410,6 +413,10 @@ export async function handleWavePlanning(\\n     // Initialize current wave state\\n     (state as any).currentWaveCompletions = new Set<string>();\\n     (state as any).failedChecks = new Set<string>();\\n+    try {\\n+      (state as any).flags = (state as any).flags || {};\\n+      (state as any).flags.waveKind = 'initial';\\n+    } catch {}\\n   }\\n \\n   // Check if there are levels to execute\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/workflow-projection.ts\",\"additions\":3,\"deletions\":2,\"changes\":5,\"patch\":\"diff --git a/src/state-machine/workflow-projection.ts b/src/state-machine/workflow-projection.ts\\nindex b2d5eb24..2bcfd858 100644\\n--- a/src/state-machine/workflow-projection.ts\\n+++ b/src/state-machine/workflow-projection.ts\\n@@ -53,7 +53,8 @@ export function projectWorkflowToGraph(\\n       triggers: step.on || workflow.on || [],\\n       group: step.group,\\n       providerType: step.type || 'ai',\\n-      dependencies: (step.depends_on || []).map(dep => dep),\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: (Array.isArray(step.depends_on) ? step.depends_on : step.depends_on ? [step.depends_on] : []).map((dep: string) => dep),\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/flow-stage.ts\",\"additions\":37,\"deletions\":4,\"changes\":41,\"patch\":\"diff --git a/src/test-runner/core/flow-stage.ts b/src/test-runner/core/flow-stage.ts\\nindex a14256c2..a587f9f1 100644\\n--- a/src/test-runner/core/flow-stage.ts\\n+++ b/src/test-runner/core/flow-stage.ts\\n@@ -352,10 +352,30 @@ export class FlowStage {\\n       } catch {}\\n       // Prefer authoritative counts from res.statistics when available, then fall back to deltas/inferred\\n       const fromResStats: Record<string, number> = {};\\n+      const resSkipped: Record<string, boolean> = {};\\n       try {\\n         for (const s of (res.statistics?.checks || []) as any[]) {\\n           const n = (s && s.checkName) || '';\\n-          if (typeof n === 'string' && n) fromResStats[n] = (s.totalRuns || 0) as number;\\n+          if (typeof n === 'string' && n) {\\n+            fromResStats[n] = (s.totalRuns || 0) as number;\\n+            resSkipped[n] = !!(s as any).skipped || !!(s as any).skipReason;\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      // Steps that have explicit expectations in this stage. We will use\\n+      // this to avoid inflating run counts (via parent-alignment heuristics)\\n+      // for checks that are not even expected in this stage.\\n+      const expectedSteps = new Set<string>();\\n+      const expectedZero = new Set<string>();\\n+      try {\\n+        const expCalls = ((stage.expect || {}).calls || []) as Array<{ step?: string }>;\\n+        for (const c of expCalls) {\\n+          if (c && typeof c.step === 'string' && c.step) expectedSteps.add(c.step);\\n+          try {\\n+            if (c && typeof c.step === 'string' && c.step && (c as any).exactly === 0)\\n+              expectedZero.add(c.step);\\n+          } catch {}\\n         }\\n       } catch {}\\n \\n@@ -410,10 +430,15 @@ export class FlowStage {\\n           const base = stageOnly ? 0 : statBase[name] || 0;\\n           const d = Math.max(0, resTotal - base);\\n           runs = d > 0 ? d : 0;\\n+          // If the engine marked this check as skipped, force runs to 0\\n+          // so parent-alignment heuristics below do not inflate it.\\n+          if (resSkipped[name]) {\\n+            runs = 0;\\n+          }\\n         } else {\\n           runs = deltaMap[name] !== undefined ? deltaMap[name] : inferred;\\n         }\\n-        if (runs === 0 && presentInResults.has(name)) runs = 1;\\n+        if (runs === 0 && presentInResults.has(name) && !resSkipped[name]) runs = 1;\\n         if (!isForEachLike && histRuns > 0) runs = histRuns;\\n         // Only use per-item history counts for non-forEach checks. For forEach parents,\\n         // use aggregated totals from res.statistics to reflect number of parent executions.\\n@@ -442,7 +467,15 @@ export class FlowStage {\\n             parentMax = Math.max(parentMax, dP);\\n           }\\n           // Apply only for non-forEach checks with no observable history in this stage\\n-          if (!isForEachLike && histRuns === 0 && histPerItemRuns === 0 && parentMax > 0) {\\n+          if (\\n+            !isForEachLike &&\\n+            histRuns === 0 &&\\n+            histPerItemRuns === 0 &&\\n+            parentMax > 0 &&\\n+            !resSkipped[name] &&\\n+            (expectedSteps.size === 0 || expectedSteps.has(name)) &&\\n+            !expectedZero.has(name)\\n+          ) {\\n             runs = Math.max(runs, parentMax);\\n           }\\n         } catch {}\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/test-execution-wrapper.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/test-runner/core/test-execution-wrapper.ts b/src/test-runner/core/test-execution-wrapper.ts\\nindex 254d0970..d0333152 100644\\n--- a/src/test-runner/core/test-execution-wrapper.ts\\n+++ b/src/test-runner/core/test-execution-wrapper.ts\\n@@ -31,6 +31,8 @@ export class TestExecutionWrapper {\\n       const prev: any = (this.engine as any).executionContext || {};\\n       const merged = {\\n         ...prev,\\n+        // Inject workflow inputs for template access via {{ inputs.* }}\\n+        workflowInputs: cfg.workflow_inputs || prev.workflowInputs || {},\\n         mode: {\\n           ...(prev.mode || {}),\\n           test: true,\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/evaluators.ts\",\"additions\":67,\"deletions\":2,\"changes\":69,\"patch\":\"diff --git a/src/test-runner/evaluators.ts b/src/test-runner/evaluators.ts\\nindex 4843163a..75e9c174 100644\\n--- a/src/test-runner/evaluators.ts\\n+++ b/src/test-runner/evaluators.ts\\n@@ -252,6 +252,69 @@ export function evaluatePrompts(\\n   }\\n }\\n \\n+/**\\n+ * Evaluate workflow_output assertions against computed workflow outputs.\\n+ * Similar to evaluateOutputs but tests workflow-level outputs (defined in outputs: section)\\n+ * rather than step outputs.\\n+ */\\n+export function evaluateWorkflowOutputs(\\n+  errors: string[],\\n+  expect: ExpectBlock,\\n+  workflowOutputs: Record<string, unknown> | undefined\\n+): void {\\n+  const expectations = (expect as any).workflow_output;\\n+  if (!Array.isArray(expectations) || expectations.length === 0) return;\\n+  if (!workflowOutputs) {\\n+    errors.push('workflow_output assertions present but no workflow outputs computed');\\n+    return;\\n+  }\\n+\\n+  for (const o of expectations) {\\n+    const path = o.path as string;\\n+    if (!path) {\\n+      errors.push('workflow_output assertion missing path');\\n+      continue;\\n+    }\\n+    const v = deepGet(workflowOutputs, path);\\n+    if (o.equals !== undefined && !deepEqual(v, o.equals)) {\\n+      errors.push(\\n+        `Workflow output ${path} expected ${JSON.stringify(o.equals)} but got ${JSON.stringify(v)}`\\n+      );\\n+    }\\n+    if (o.equalsDeep !== undefined && !deepEqual(v, o.equalsDeep)) {\\n+      errors.push(`Workflow output ${path} deepEquals failed`);\\n+    }\\n+    if (o.matches && !parseRegex(o.matches).test(String(v))) {\\n+      errors.push(`Workflow output ${path} does not match ${o.matches}`);\\n+    }\\n+    if (o.contains) {\\n+      const contents = Array.isArray(o.contains) ? o.contains : [o.contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (!strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} expected to contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.not_contains) {\\n+      const contents = Array.isArray(o.not_contains) ? o.not_contains : [o.not_contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} should not contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.contains_unordered) {\\n+      if (!Array.isArray(v)) {\\n+        errors.push(`Workflow output ${path} not an array for contains_unordered`);\\n+      } else if (!containsUnordered(v as unknown[], o.contains_unordered)) {\\n+        errors.push(`Workflow output ${path} missing elements (unordered)`);\\n+      }\\n+    }\\n+  }\\n+}\\n+\\n export function evaluateOutputs(\\n   errors: string[],\\n   expect: ExpectBlock,\\n@@ -332,7 +395,8 @@ export function evaluateCase(\\n   strict: boolean,\\n   promptsByStep: Record<string, string[]>,\\n   _results: GroupedResults,\\n-  outputHistory: Record<string, unknown[]>\\n+  outputHistory: Record<string, unknown[]>,\\n+  workflowOutputs?: Record<string, unknown>\\n ): string[] {\\n   const errors: string[] = [];\\n   const executed = buildExecutedMap(stats);\\n@@ -350,5 +414,6 @@ export function evaluateCase(\\n   evaluateNoCalls(errors, expect, executed, recorder, slackRecorder);\\n   evaluatePrompts(errors, expect, promptsByStep);\\n   evaluateOutputs(errors, expect, outputHistory);\\n+  evaluateWorkflowOutputs(errors, expect, workflowOutputs);\\n   return errors;\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/index.ts\",\"additions\":112,\"deletions\":7,\"changes\":119,\"patch\":\"diff --git a/src/test-runner/index.ts b/src/test-runner/index.ts\\nindex 29916a16..4dcc3331 100644\\n--- a/src/test-runner/index.ts\\n+++ b/src/test-runner/index.ts\\n@@ -159,6 +159,7 @@ export async function runSuites(\\n   options: {\\n     only?: string;\\n     bail?: boolean;\\n+    noMocks?: boolean;\\n     maxParallelSuites?: number;\\n     maxParallel?: number;\\n     promptMaxChars?: number;\\n@@ -225,6 +226,7 @@ export async function runSuites(\\n       const r = await runner.runCases(fp, suite as TestSuite, {\\n         only: options.only,\\n         bail: options.bail,\\n+        noMocks: options.noMocks,\\n         maxParallel: options.maxParallel,\\n         promptMaxChars: options.promptMaxChars,\\n       });\\n@@ -285,7 +287,8 @@ export class VisorTestRunner {\\n     defaultPromptCap?: number,\\n     ghRec?: { error_code?: number; timeout_ms?: number },\\n     defaultIncludeTags?: string[] | undefined,\\n-    defaultExcludeTags?: string[] | undefined\\n+    defaultExcludeTags?: string[] | undefined,\\n+    noMocks?: boolean\\n   ): {\\n     name: string;\\n     strict: boolean;\\n@@ -399,9 +402,11 @@ export class VisorTestRunner {\\n               : info.prompt;\\n           prompts[k].push(p);\\n         },\\n-        mockForStep: (step: string) => mockMgr.get(step),\\n+        // In noMocks mode, always return undefined to let real providers execute\\n+        mockForStep: (step: string) => noMocks ? undefined : mockMgr.get(step),\\n         // Ensure human-input never blocks tests: prefer case mock, then default value\\n         onHumanInput: async (req: { checkId: string; default?: string }) => {\\n+          if (noMocks) return (req.default ?? '').toString();\\n           const m = mockMgr.get(req.checkId);\\n           if (m !== undefined && m !== null) return String(m);\\n           return (req.default ?? '').toString();\\n@@ -481,6 +486,60 @@ export class VisorTestRunner {\\n     if (!checks || checks.length === 0) return;\\n     console.log(`  checks: ${checks.join(', ')}`);\\n   }\\n+  /**\\n+   * Compute workflow outputs from output definitions and step results.\\n+   * Evaluates value_js and value (Liquid) expressions to produce output values.\\n+   */\\n+  private computeWorkflowOutputs(\\n+    outputDefs: Array<{ name: string; value?: string; value_js?: string }>,\\n+    outputHistory: Record<string, unknown[]>,\\n+    results: any[]\\n+  ): Record<string, unknown> {\\n+    const computed: Record<string, unknown> = {};\\n+    // Build a steps map from outputHistory (last output for each step)\\n+    const steps: Record<string, unknown> = {};\\n+    for (const [stepId, hist] of Object.entries(outputHistory)) {\\n+      if (Array.isArray(hist) && hist.length > 0) {\\n+        steps[stepId] = hist[hist.length - 1];\\n+      }\\n+    }\\n+    // Build outputs map from history (for {{ outputs[\\\"step\\\"] }} syntax)\\n+    const outputs: Record<string, unknown> = { ...steps };\\n+\\n+    for (const outputDef of outputDefs) {\\n+      if (!outputDef.name) continue;\\n+      try {\\n+        if (outputDef.value_js) {\\n+          // Evaluate JavaScript expression\\n+          const fn = new Function('steps', 'outputs', 'results', `\\n+            try {\\n+              ${outputDef.value_js}\\n+            } catch (e) {\\n+              return undefined;\\n+            }\\n+          `);\\n+          computed[outputDef.name] = fn(steps, outputs, results);\\n+        } else if (outputDef.value) {\\n+          // Evaluate Liquid template\\n+          const { Liquid } = require('liquidjs');\\n+          const engine = new Liquid();\\n+          const rendered = engine.parseAndRenderSync(outputDef.value, { steps, outputs, results });\\n+          // Try to parse as JSON, otherwise keep as string\\n+          try {\\n+            computed[outputDef.name] = JSON.parse(rendered);\\n+          } catch {\\n+            computed[outputDef.name] = rendered.trim();\\n+          }\\n+        }\\n+      } catch (e) {\\n+        // Skip outputs that fail to compute\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.log(`  ‚ö†Ô∏è Failed to compute output '${outputDef.name}': ${e}`);\\n+        }\\n+      }\\n+    }\\n+    return computed;\\n+  }\\n   /**\\n    * Locate a tests file: explicit path > ./.visor.tests.yaml > defaults/visor.tests.yaml\\n    */\\n@@ -606,7 +665,7 @@ export class VisorTestRunner {\\n   public async runCases(\\n     testsPath: string,\\n     suite: TestSuite,\\n-    options: { only?: string; bail?: boolean; maxParallel?: number; promptMaxChars?: number }\\n+    options: { only?: string; bail?: boolean; noMocks?: boolean; maxParallel?: number; promptMaxChars?: number }\\n   ): Promise<{\\n     failures: number;\\n     results: Array<{\\n@@ -748,9 +807,14 @@ export class VisorTestRunner {\\n     const __keepAlive = setInterval(() => {}, 1000);\\n     // Header: show suite path for clarity\\n     let __suiteRel = testsPath;\\n+    const noMocksMode = options.noMocks || false;\\n     try {\\n       __suiteRel = path.relative(this.cwd, testsPath) || testsPath;\\n       console.log(`Suite: ${__suiteRel}`);\\n+      if (noMocksMode) {\\n+        console.log(this.color('üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)', '33'));\\n+        console.log(this.gray('   Step outputs will be captured and printed as suggested mocks\\\\n'));\\n+      }\\n     } catch {}\\n     const runOne = async (_case: any): Promise<{ name: string; failed: number }> => {\\n       // Case header for clarity\\n@@ -805,6 +869,11 @@ export class VisorTestRunner {\\n           cfgLocal.checks[name] = chk;\\n         }\\n       }\\n+      // Workflow testing: inject workflow_input into config for template access\\n+      const workflowInput = (_case as any).workflow_input;\\n+      if (workflowInput && typeof workflowInput === 'object') {\\n+        (cfgLocal as any).workflow_inputs = workflowInput;\\n+      }\\n       const setup = this.setupTestCase(\\n         _case,\\n         cfgLocal,\\n@@ -812,7 +881,8 @@ export class VisorTestRunner {\\n         defaultPromptCap,\\n         ghRec,\\n         defaultIncludeTags,\\n-        defaultExcludeTags\\n+        defaultExcludeTags,\\n+        noMocksMode\\n       );\\n       try {\\n         this.printSelectedChecks(setup.checksToRun);\\n@@ -826,6 +896,24 @@ export class VisorTestRunner {\\n         } catch {}\\n         const exec = await this.executeTestCase(setup, cfgLocal);\\n         const res = exec.res;\\n+\\n+        // In no-mocks mode, print captured outputs as suggested mocks\\n+        if (noMocksMode && Object.keys(exec.outHistory).length > 0) {\\n+          console.log(this.color('\\\\nüìã Suggested mocks (copy to your test case):', '36'));\\n+          console.log(this.gray('mocks:'));\\n+          for (const [stepName, outputs] of Object.entries(exec.outHistory)) {\\n+            if (!Array.isArray(outputs) || outputs.length === 0) continue;\\n+            // Use the last output for the step\\n+            const lastOutput = outputs[outputs.length - 1];\\n+            // Format as YAML with proper indentation\\n+            const yamlOutput = yaml.dump({ [stepName]: lastOutput }, { indent: 2, lineWidth: 120, noRefs: true });\\n+            // Indent the YAML output for proper nesting under 'mocks:'\\n+            const indented = yamlOutput.split('\\\\n').map(line => '  ' + line).join('\\\\n');\\n+            console.log(indented);\\n+          }\\n+          console.log('');\\n+        }\\n+\\n         if (process.env.VISOR_DEBUG === 'true') {\\n           try {\\n             const names = (res.statistics.checks || []).map(\\n@@ -836,6 +924,21 @@ export class VisorTestRunner {\\n         }\\n         // avoid printing raw history keys each case\\n         // (fallback for on_finish static targets handled inside executeTestCase)\\n+        // Workflow testing: compute workflow outputs if workflow has outputs defined\\n+        let workflowOutputs: Record<string, unknown> | undefined;\\n+        if (Array.isArray((cfgLocal as any).outputs)) {\\n+          try {\\n+            workflowOutputs = this.computeWorkflowOutputs(\\n+              (cfgLocal as any).outputs,\\n+              exec.outHistory,\\n+              res.results\\n+            );\\n+          } catch (e) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ‚ö†Ô∏è Error computing workflow outputs: ${e}`);\\n+            }\\n+          }\\n+        }\\n         const caseFailures = require('./evaluators').evaluateCase(\\n           _case.name,\\n           res.statistics,\\n@@ -845,7 +948,8 @@ export class VisorTestRunner {\\n           setup.strict,\\n           setup.prompts,\\n           res.results,\\n-          exec.outHistory\\n+          exec.outHistory,\\n+          workflowOutputs\\n         );\\n         // Warn about unmocked AI/command steps that executed\\n         try {\\n@@ -1280,7 +1384,8 @@ export class VisorTestRunner {\\n   ): void {\\n     const executed: Record<string, number> = {};\\n     for (const s of stats.checks) {\\n-      if (!s.skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n+      const skipped = (s as any).skipped === true || !!(s as any).skipReason;\\n+      if (!skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n     }\\n     const expCalls = (expect.calls || []).filter(c => c.step);\\n     const expectedSteps = new Map<\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/validator.ts\",\"additions\":48,\"deletions\":2,\"changes\":50,\"patch\":\"diff --git a/src/test-runner/validator.ts b/src/test-runner/validator.ts\\nindex dbb890c5..5ffff5ee 100644\\n--- a/src/test-runner/validator.ts\\n+++ b/src/test-runner/validator.ts\\n@@ -25,6 +25,16 @@ const schema: any = {\\n     hooks: { type: 'object' },\\n     slack: { type: 'object' },\\n     frontends: { type: 'array' },\\n+    workspace: { type: 'object' },\\n+    // Workflow definition fields (for workflow files with co-located tests)\\n+    id: { type: 'string' },\\n+    name: { type: 'string' },\\n+    description: { type: 'string' },\\n+    inputs: { type: 'array' },\\n+    outputs: { type: 'array' },\\n+    ai_mcp_servers: { type: 'object' },\\n+    ai_provider: { type: 'string' },\\n+    ai_model: { type: 'string' },\\n     tests: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -136,6 +146,8 @@ const schema: any = {\\n             oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n           },\\n         },\\n+        // Workflow testing: input values to pass to the workflow\\n+        workflow_input: { type: 'object' },\\n         expect: { $ref: '#/$defs/expectBlock' },\\n         // Flow cases\\n         flow: {\\n@@ -270,6 +282,35 @@ const schema: any = {\\n       },\\n       required: ['step', 'path'],\\n     },\\n+    // Workflow output expectation: like outputsExpectation but without step (tests workflow-level outputs)\\n+    workflowOutputExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        path: { type: 'string' },\\n+        equals: {},\\n+        equalsDeep: {},\\n+        matches: { type: 'string' },\\n+        contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        not_contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            path: { type: 'string' },\\n+            equals: {},\\n+            matches: { type: 'string' },\\n+          },\\n+          required: ['path'],\\n+        },\\n+        contains_unordered: { type: 'array' },\\n+      },\\n+      required: ['path'],\\n+    },\\n     expectBlock: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -278,6 +319,8 @@ const schema: any = {\\n         calls: { type: 'array', items: { $ref: '#/$defs/callsExpectation' } },\\n         prompts: { type: 'array', items: { $ref: '#/$defs/promptsExpectation' } },\\n         outputs: { type: 'array', items: { $ref: '#/$defs/outputsExpectation' } },\\n+        // Workflow testing: assert on workflow-level outputs (defined in workflow's outputs: section)\\n+        workflow_output: { type: 'array', items: { $ref: '#/$defs/workflowOutputExpectation' } },\\n         no_calls: {\\n           type: 'array',\\n           items: {\\n@@ -343,6 +386,7 @@ const knownKeys = new Set([\\n   'version',\\n   'extends',\\n   'tests',\\n+  'workspace',\\n   // tests\\n   'tests.defaults',\\n   'tests.fixtures',\\n@@ -363,6 +407,7 @@ const knownKeys = new Set([\\n   'env',\\n   'routing',\\n   'mocks',\\n+  'workflow_input',\\n   'expect',\\n   'flow',\\n   // expect\\n@@ -370,6 +415,7 @@ const knownKeys = new Set([\\n   'expect.calls',\\n   'expect.prompts',\\n   'expect.outputs',\\n+  'expect.workflow_output',\\n   'expect.no_calls',\\n   'expect.fail',\\n   'expect.strict_violation',\\n@@ -424,7 +470,7 @@ function formatError(e: ErrorObject): string {\\n       if (hint) msg += ` (${hint})`;\\n       // Small curated allow-list for frequent nodes to reduce guesswork\\n       if (path.endsWith('expect')) {\\n-        msg += ` (allowed: use, calls, prompts, outputs, no_calls, fail, strict_violation)`;\\n+        msg += ` (allowed: use, calls, prompts, outputs, workflow_output, no_calls, fail, strict_violation)`;\\n       } else if (path.endsWith('env')) {\\n         msg += ` (values must be strings)`;\\n       } else if (path.endsWith('tests')) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/cli.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/cli.ts b/src/types/cli.ts\\nindex a7d9ca30..f06f93d5 100644\\n--- a/src/types/cli.ts\\n+++ b/src/types/cli.ts\\n@@ -61,6 +61,8 @@ export interface CliOptions {\\n   githubV2?: boolean;\\n   /** Enable Slack Socket Mode runner */\\n   slack?: boolean;\\n+  /** Keep workspace folders after execution (for debugging) */\\n+  keepWorkspace?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/config.ts\",\"additions\":114,\"deletions\":5,\"changes\":119,\"patch\":\"diff --git a/src/types/config.ts b/src/types/config.ts\\nindex ed239959..dd363de1 100644\\n--- a/src/types/config.ts\\n+++ b/src/types/config.ts\\n@@ -122,6 +122,9 @@ export interface FailureConditionContext {\\n   /** Environment variables */\\n   env?: Record<string, string>;\\n \\n+  /** Workflow inputs - values passed to the workflow */\\n+  inputs?: Record<string, unknown>;\\n+\\n   /** Memory accessor for accessing memory store in expressions */\\n   memory?: {\\n     get: (key: string, namespace?: string) => unknown;\\n@@ -356,6 +359,8 @@ export interface AIProviderConfig {\\n   allowBash?: boolean;\\n   /** Advanced bash command execution configuration */\\n   bashConfig?: BashConfig;\\n+  /** Completion prompt for post-completion validation/review (runs after attempt_completion) */\\n+  completion_prompt?: string;\\n }\\n \\n /**\\n@@ -461,8 +466,8 @@ export interface CheckConfig {\\n   env?: EnvConfig;\\n   /** Timeout in seconds for command execution (default: 60) */\\n   timeout?: number;\\n-  /** Check IDs that this check depends on (optional) */\\n-  depends_on?: string[];\\n+  /** Check IDs that this check depends on (optional). Accepts single string or array. */\\n+  depends_on?: string | string[];\\n   /** Group name for comment separation (e.g., \\\"code-review\\\", \\\"pr-overview\\\") - optional */\\n   group?: string;\\n   /** Schema type for template rendering (e.g., \\\"code-review\\\", \\\"markdown\\\") or inline JSON schema object - optional */\\n@@ -621,6 +626,46 @@ export interface CheckConfig {\\n   overrides?: Record<string, Partial<CheckConfig>>;\\n   /** Map workflow outputs to check outputs */\\n   output_mapping?: Record<string, string>;\\n+\\n+  // Workflow aliases for backward compatibility\\n+  /** Alias for args - workflow inputs (backward compatibility) */\\n+  workflow_inputs?: Record<string, unknown>;\\n+  /** Config file path - alternative to workflow ID (loads a Visor config file as workflow) */\\n+  config?: string;\\n+  /** Alias for overrides - workflow step overrides (backward compatibility) */\\n+  workflow_overrides?: Record<string, Partial<CheckConfig>>;\\n+\\n+  /**\\n+   * Git-checkout provider specific options (optional, only used when type === 'git-checkout').\\n+   */\\n+  /** Git reference to checkout (branch, tag, commit SHA) - supports templates */\\n+  ref?: string;\\n+  /** Repository URL or owner/repo format (defaults to current repository) */\\n+  repository?: string;\\n+  /** GitHub token for private repositories (defaults to GITHUB_TOKEN env) */\\n+  token?: string;\\n+  /** Number of commits to fetch (0 for full history, default: 1) */\\n+  fetch_depth?: number;\\n+  /** Whether to fetch tags (default: false) */\\n+  fetch_tags?: boolean;\\n+  /** Checkout submodules: false, true, or 'recursive' */\\n+  submodules?: boolean | 'recursive';\\n+  /** Working directory for the checkout (defaults to temp directory) */\\n+  working_directory?: string;\\n+  /** Use git worktree for efficient parallel checkouts (default: true) */\\n+  use_worktree?: boolean;\\n+  /** Clean the working directory before checkout (default: true) */\\n+  clean?: boolean;\\n+  /** Sparse checkout paths - only checkout specific directories/files */\\n+  sparse_checkout?: string[];\\n+  /** Enable Git LFS (Large File Storage) */\\n+  lfs?: boolean;\\n+  /** Timeout in ms for cloning the bare repository (default: 300000 = 5 min) */\\n+  clone_timeout_ms?: number;\\n+  /** Clean up worktree on failure (default: true) */\\n+  cleanup_on_failure?: boolean;\\n+  /** Keep worktree after workflow completion (default: false) */\\n+  persist_worktree?: boolean;\\n }\\n \\n /**\\n@@ -1037,6 +1082,36 @@ export interface CustomToolDefinition {\\n   outputSchema?: Record<string, unknown>;\\n }\\n \\n+/**\\n+ * Workflow input definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowInput {\\n+  /** Input parameter name */\\n+  name: string;\\n+  /** JSON Schema for the input */\\n+  schema?: Record<string, unknown>;\\n+  /** Whether this input is required */\\n+  required?: boolean;\\n+  /** Default value if not provided */\\n+  default?: unknown;\\n+  /** Human-readable description */\\n+  description?: string;\\n+}\\n+\\n+/**\\n+ * Workflow output definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowOutput {\\n+  /** Output name */\\n+  name: string;\\n+  /** Human-readable description */\\n+  description?: string;\\n+  /** Value using Liquid template syntax (references step outputs) */\\n+  value?: string;\\n+  /** Value using JavaScript expression (alternative to value) */\\n+  value_js?: string;\\n+}\\n+\\n /**\\n  * Main Visor configuration\\n  */\\n@@ -1051,12 +1126,16 @@ export interface VisorConfig {\\n   tools?: Record<string, CustomToolDefinition>;\\n   /** Import workflow definitions from external files or URLs */\\n   imports?: string[];\\n+  /** Workflow inputs (for standalone reusable workflows) */\\n+  inputs?: WorkflowInput[];\\n+  /** Workflow outputs (for standalone reusable workflows) */\\n+  outputs?: WorkflowOutput[];\\n   /** Step configurations (recommended) */\\n   steps?: Record<string, CheckConfig>;\\n   /** Check configurations (legacy, use 'steps' instead) - always populated after normalization */\\n   checks?: Record<string, CheckConfig>;\\n-  /** Output configuration */\\n-  output: OutputConfig;\\n+  /** Output configuration (optional - defaults provided) */\\n+  output?: OutputConfig;\\n   /** HTTP server configuration for receiving webhooks */\\n   http_server?: HttpServerConfig;\\n   /** Memory storage configuration */\\n@@ -1092,6 +1171,36 @@ export interface VisorConfig {\\n     /** Frontend-specific configuration */\\n     config?: unknown;\\n   }>;\\n+  /** Workspace isolation configuration for sandboxed execution */\\n+  workspace?: WorkspaceConfig;\\n+  /** Slack configuration */\\n+  slack?: SlackConfig;\\n+}\\n+\\n+/**\\n+ * Workspace isolation configuration\\n+ */\\n+export interface WorkspaceConfig {\\n+  /** Enable workspace isolation (default: true when config present) */\\n+  enabled?: boolean;\\n+  /** Base path for workspaces (default: /tmp/visor-workspaces) */\\n+  base_path?: string;\\n+  /** Clean up workspace on exit (default: true) */\\n+  cleanup_on_exit?: boolean;\\n+}\\n+\\n+/**\\n+ * Slack configuration\\n+ */\\n+export interface SlackConfig {\\n+  /** Slack API version */\\n+  version?: string;\\n+  /** Mention handling: 'all', 'direct', etc. */\\n+  mentions?: string;\\n+  /** Thread handling: 'required', 'optional', etc. */\\n+  threads?: string;\\n+  /** Show raw output in Slack responses */\\n+  show_raw_output?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/git-checkout.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/git-checkout.ts b/src/types/git-checkout.ts\\nindex eb1c5ebc..4d27855d 100644\\n--- a/src/types/git-checkout.ts\\n+++ b/src/types/git-checkout.ts\\n@@ -16,6 +16,8 @@ export interface GitCheckoutConfig {\\n   fetch_depth?: number;\\n   fetch_tags?: boolean;\\n   submodules?: boolean | 'recursive';\\n+  /** Timeout (ms) for cloning the bare repository; defaults to 300000 (5 minutes) */\\n+  clone_timeout_ms?: number;\\n \\n   // Worktree configuration\\n   working_directory?: string;\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/sandbox.ts\",\"additions\":46,\"deletions\":1,\"changes\":47,\"patch\":\"diff --git a/src/utils/sandbox.ts b/src/utils/sandbox.ts\\nindex 9914a3a5..12e27c30 100644\\n--- a/src/utils/sandbox.ts\\n+++ b/src/utils/sandbox.ts\\n@@ -14,6 +14,51 @@ export interface CompileOptions {\\n   wrapFunction?: boolean;\\n }\\n \\n+export interface JsSyntaxValidationResult {\\n+  valid: boolean;\\n+  error?: string;\\n+}\\n+\\n+/**\\n+ * Validate JavaScript syntax without executing it.\\n+ * Uses the sandbox's compile method to check for syntax errors.\\n+ * Returns validation result with error message if invalid.\\n+ */\\n+export function validateJsSyntax(code: string): JsSyntaxValidationResult {\\n+  if (!code || typeof code !== 'string') {\\n+    return { valid: false, error: 'Code must be a non-empty string' };\\n+  }\\n+\\n+  const trimmed = code.trim();\\n+  if (trimmed.length === 0) {\\n+    return { valid: false, error: 'Code cannot be empty' };\\n+  }\\n+\\n+  // Create a minimal sandbox instance for syntax checking\\n+  const sandbox = createSecureSandbox();\\n+\\n+  // Wrap code similar to compileAndRun to catch the same syntax issues\\n+  const looksLikeBlock = /\\\\breturn\\\\b/.test(trimmed) || /;/.test(trimmed) || /\\\\n/.test(trimmed);\\n+  const looksLikeIife = /\\\\)\\\\s*\\\\(\\\\s*\\\\)\\\\s*;?$/.test(trimmed);\\n+  const body = looksLikeBlock\\n+    ? looksLikeIife\\n+      ? `return (\\\\n${trimmed}\\\\n);\\\\n`\\n+      : `return (() => {\\\\n${trimmed}\\\\n})();\\\\n`\\n+    : `return (\\\\n${trimmed}\\\\n);\\\\n`;\\n+\\n+  // Add log injection header (same as compileAndRun)\\n+  const header = `const __lp = \\\"[syntax-check]\\\"; const log = (...a) => { try { console.log(__lp, ...a); } catch {} };\\\\n`;\\n+  const fullCode = `${header}${body}`;\\n+\\n+  try {\\n+    sandbox.compile(fullCode);\\n+    return { valid: true };\\n+  } catch (e) {\\n+    const msg = e instanceof Error ? e.message : String(e);\\n+    return { valid: false, error: msg };\\n+  }\\n+}\\n+\\n /**\\n  * Create a hardened Sandbox with a consistent set of globals and prototype\\n  * whitelists. This is a superset of the sets previously used by individual\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/tracer-init.ts\",\"additions\":19,\"deletions\":1,\"changes\":20,\"patch\":\"diff --git a/src/utils/tracer-init.ts b/src/utils/tracer-init.ts\\nindex b42995d9..4edf1ffc 100644\\n--- a/src/utils/tracer-init.ts\\n+++ b/src/utils/tracer-init.ts\\n@@ -76,6 +76,24 @@ export async function initializeTracer(\\n \\n       const tracer = new SimpleAppTracer(telemetry, sessionId);\\n \\n+      // WORKAROUND: Add missing recordEvent method for completionPrompt feature (probe #321)\\n+      // SimpleAppTracer doesn't have recordEvent but completionPrompt requires it\\n+      if (typeof (tracer as any).recordEvent !== 'function') {\\n+        (tracer as any).recordEvent = (\\n+          name: string,\\n+          attributes?: Record<string, unknown>\\n+        ) => {\\n+          // Log completion events to telemetry for debugging\\n+          try {\\n+            if ((telemetry as any).record) {\\n+              (telemetry as any).record({ event: name, ...attributes });\\n+            }\\n+          } catch {\\n+            // Best-effort only\\n+          }\\n+        };\\n+      }\\n+\\n       console.error(`üìä Simple tracing enabled, will save to: ${traceFilePath}`);\\n \\n       // If in GitHub Actions, log the path for artifact upload\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/worktree-manager.ts\",\"additions\":142,\"deletions\":27,\"changes\":169,\"patch\":\"diff --git a/src/utils/worktree-manager.ts b/src/utils/worktree-manager.ts\\nindex f3cebf3d..2de7a9f4 100644\\n--- a/src/utils/worktree-manager.ts\\n+++ b/src/utils/worktree-manager.ts\\n@@ -6,6 +6,7 @@\\n  */\\n \\n import * as fs from 'fs';\\n+import * as fsp from 'fs/promises';\\n import * as path from 'path';\\n import * as crypto from 'crypto';\\n import { commandExecutor } from './command-executor';\\n@@ -119,7 +120,8 @@ export class WorktreeManager {\\n     repository: string,\\n     repoUrl: string,\\n     token?: string,\\n-    fetchDepth?: number\\n+    fetchDepth?: number,\\n+    cloneTimeoutMs?: number\\n   ): Promise<string> {\\n     const reposDir = this.getReposDir();\\n     const repoName = repository.replace(/\\\\//g, '-');\\n@@ -129,9 +131,21 @@ export class WorktreeManager {\\n     if (fs.existsSync(bareRepoPath)) {\\n       logger.debug(`Bare repository already exists: ${bareRepoPath}`);\\n \\n-      // Update remote refs\\n-      await this.updateBareRepo(bareRepoPath);\\n-      return bareRepoPath;\\n+      // Verify the bare repo has the correct remote URL to prevent using corrupted repos\\n+      const verifyResult = await this.verifyBareRepoRemote(bareRepoPath, repoUrl);\\n+      if (verifyResult === 'timeout') {\\n+        // Timeout during verification - use stale cache to avoid hanging on re-clone\\n+        logger.info(`Using stale bare repository (verification timed out): ${bareRepoPath}`);\\n+        return bareRepoPath;\\n+      } else if (verifyResult === false) {\\n+        logger.warn(`Bare repository at ${bareRepoPath} has incorrect remote, removing and re-cloning`);\\n+        await fsp.rm(bareRepoPath, { recursive: true, force: true });\\n+        // Fall through to clone below\\n+      } else {\\n+        // Update remote refs\\n+        await this.updateBareRepo(bareRepoPath);\\n+        return bareRepoPath;\\n+      }\\n     }\\n \\n     // Clone as bare repository\\n@@ -153,7 +167,9 @@ export class WorktreeManager {\\n     }\\n     cloneCmd += ` ${this.escapeShellArg(cloneUrl)} ${this.escapeShellArg(bareRepoPath)}`;\\n \\n-    const result = await this.executeGitCommand(cloneCmd, { timeout: 300000 }); // 5 minute timeout\\n+    const result = await this.executeGitCommand(cloneCmd, {\\n+      timeout: cloneTimeoutMs || 300000, // default 5 minutes\\n+    });\\n \\n     if (result.exitCode !== 0) {\\n       // Redact tokens from error messages\\n@@ -171,19 +187,98 @@ export class WorktreeManager {\\n   private async updateBareRepo(bareRepoPath: string): Promise<void> {\\n     logger.debug(`Updating bare repository: ${bareRepoPath}`);\\n \\n-    const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n-    const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+    try {\\n+      const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n+      const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n+        // Don't throw - we can continue with stale refs\\n+      } else {\\n+        logger.debug(`Successfully updated bare repository`);\\n+      }\\n+    } catch (error) {\\n+      // Handle timeout or other errors gracefully - we can continue with stale refs\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      logger.warn(`Failed to update bare repository (will use stale refs): ${errorMessage}`);\\n+    }\\n+  }\\n \\n-    if (result.exitCode !== 0) {\\n-      logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n-      // Don't throw - we can continue with stale refs\\n-    } else {\\n-      logger.debug(`Successfully updated bare repository`);\\n+  /**\\n+   * Verify that a bare repository has the correct remote URL.\\n+   * This prevents reusing corrupted repos that were cloned from a different repository.\\n+   * Returns: true (valid), false (invalid - should re-clone), or 'timeout' (use stale cache)\\n+   */\\n+  private async verifyBareRepoRemote(\\n+    bareRepoPath: string,\\n+    expectedUrl: string\\n+  ): Promise<boolean | 'timeout'> {\\n+    try {\\n+      const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote get-url origin`;\\n+      const result = await this.executeGitCommand(cmd, { timeout: 10000 });\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to get remote URL for ${bareRepoPath}: ${result.stderr}`);\\n+        return false;\\n+      }\\n+\\n+      const actualUrl = result.stdout.trim();\\n+\\n+      // Normalize URLs for comparison:\\n+      // - remove credentials (tokens/username)\\n+      // - remove .git suffix if present\\n+      // - trim trailing slash\\n+      // - lowercase for case‚Äëinsensitive match\\n+      const normalizeUrl = (url: string): string => {\\n+        // Convert common SSH form to https for comparison\\n+        if (url.startsWith('git@github.com:')) {\\n+          url = url.replace('git@github.com:', 'https://github.com/');\\n+        }\\n+        return url\\n+          // strip userinfo part to avoid mismatches when the cached bare\\n+          // repo was cloned with an access token but the expected URL is\\n+          // tokenless (or vice‚Äëversa)\\n+          .replace(/:\\\\/\\\\/[^@]+@/, '://')\\n+          .replace(/\\\\.git$/, '')\\n+          .replace(/\\\\/$/, '')\\n+          .toLowerCase();\\n+      };\\n+\\n+      const normalizedExpected = normalizeUrl(expectedUrl);\\n+      const normalizedActual = normalizeUrl(actualUrl);\\n+\\n+      if (normalizedExpected !== normalizedActual) {\\n+        logger.warn(\\n+          `Bare repo remote mismatch: expected ${expectedUrl}, got ${actualUrl}`\\n+        );\\n+        return false;\\n+      }\\n+\\n+      logger.debug(`Bare repo remote verified: ${actualUrl}`);\\n+      return true;\\n+    } catch (error) {\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      // If it's a timeout, return 'timeout' so caller can use stale cache instead of re-cloning\\n+      if (errorMessage.includes('timed out')) {\\n+        logger.warn(`Timeout verifying bare repo remote (will use stale cache): ${errorMessage}`);\\n+        return 'timeout';\\n+      }\\n+      logger.warn(`Error verifying bare repo remote: ${error}`);\\n+      return false;\\n     }\\n   }\\n \\n   /**\\n-   * Create a new worktree\\n+   * Create a new worktree for the given repository/ref.\\n+   *\\n+   * Important: we always create worktrees in a detached HEAD state pinned\\n+   * to a specific commit SHA rather than a named branch like \\\"main\\\". Git\\n+   * only allows a branch to be checked out in a single worktree at a time;\\n+   * using the raw commit (plus --detach) lets multiple workflows safely\\n+   * create independent worktrees for the same branch without hitting\\n+   * errors like:\\n+   *\\n+   *   fatal: 'main' is already used by worktree at '.../TykTechnologies-tyk-docs-main-XXXX'\\n    */\\n   async createWorktree(\\n     repository: string,\\n@@ -195,6 +290,7 @@ export class WorktreeManager {\\n       clean?: boolean;\\n       workflowId?: string;\\n       fetchDepth?: number;\\n+      cloneTimeoutMs?: number;\\n     } = {}\\n   ): Promise<WorktreeInfo> {\\n     // Validate ref to prevent command injection\\n@@ -205,7 +301,8 @@ export class WorktreeManager {\\n       repository,\\n       repoUrl,\\n       options.token,\\n-      options.fetchDepth\\n+      options.fetchDepth,\\n+      options.cloneTimeoutMs\\n     );\\n \\n     // Generate worktree ID and path\\n@@ -241,21 +338,24 @@ export class WorktreeManager {\\n       }\\n     }\\n \\n-    // Fetch the ref if needed\\n+    // Fetch the ref if needed, then resolve it to a concrete commit SHA.\\n+    // We use the commit (detached HEAD) instead of the branch name so we\\n+    // can have multiple worktrees for the same branch without git refusing\\n+    // with \\\"branch X is already checked out\\\".\\n     await this.fetchRef(bareRepoPath, ref);\\n+    const commit = await this.getCommitShaForRef(bareRepoPath, ref);\\n \\n-    // Create worktree\\n-    logger.info(`Creating worktree for ${repository}@${ref}`);\\n-    const createCmd = `git -C ${this.escapeShellArg(bareRepoPath)} worktree add ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(ref)}`;\\n+    // Create worktree in detached HEAD state at the resolved commit\\n+    logger.info(`Creating worktree for ${repository}@${ref} (${commit})`);\\n+    const createCmd = `git -C ${this.escapeShellArg(\\n+      bareRepoPath\\n+    )} worktree add --detach ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(commit)}`;\\n     const result = await this.executeGitCommand(createCmd, { timeout: 60000 });\\n \\n     if (result.exitCode !== 0) {\\n       throw new Error(`Failed to create worktree: ${result.stderr}`);\\n     }\\n \\n-    // Get commit SHA\\n-    const commit = await this.getCommitSha(worktreePath);\\n-\\n     // Create metadata\\n     const metadata: WorktreeMetadata = {\\n       worktree_id: worktreeId,\\n@@ -316,14 +416,17 @@ export class WorktreeManager {\\n   }\\n \\n   /**\\n-   * Get commit SHA for worktree\\n+   * Get commit SHA for a given ref inside a bare repository.\\n+   *\\n+   * This runs after fetchRef so that <ref> should resolve to either a\\n+   * local branch, tag, or remote-tracking ref.\\n    */\\n-  private async getCommitSha(worktreePath: string): Promise<string> {\\n-    const cmd = `git -C ${this.escapeShellArg(worktreePath)} rev-parse HEAD`;\\n+  private async getCommitShaForRef(bareRepoPath: string, ref: string): Promise<string> {\\n+    const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} rev-parse ${this.escapeShellArg(ref)}`;\\n     const result = await this.executeGitCommand(cmd);\\n \\n     if (result.exitCode !== 0) {\\n-      throw new Error(`Failed to get commit SHA: ${result.stderr}`);\\n+      throw new Error(`Failed to get commit SHA for ref ${ref}: ${result.stderr}`);\\n     }\\n \\n     return result.stdout.trim();\\n@@ -621,9 +724,21 @@ export class WorktreeManager {\\n     command: string,\\n     options: { timeout?: number; env?: Record<string, string> } = {}\\n   ): Promise<GitCommandResult> {\\n+    // Merge provided env with process.env and add git-specific settings\\n+    // These settings prevent git from hanging on interactive prompts while\\n+    // still allowing OS-level credential helpers to work:\\n+    // - GIT_TERMINAL_PROMPT=0: Prevents terminal credential prompts\\n+    // - GIT_SSH_COMMAND: Disables SSH password prompts (BatchMode)\\n+    const gitEnv = {\\n+      ...process.env,\\n+      ...options.env,\\n+      GIT_TERMINAL_PROMPT: '0',\\n+      GIT_SSH_COMMAND: 'ssh -o BatchMode=yes -o StrictHostKeyChecking=no',\\n+    };\\n+\\n     const result = await commandExecutor.execute(command, {\\n       timeout: options.timeout || 30000,\\n-      env: options.env || process.env,\\n+      env: gitEnv,\\n     } as any);\\n \\n     return {\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-executor.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-executor.ts b/src/workflow-executor.ts\\nindex 751e7f16..df76762f 100644\\n--- a/src/workflow-executor.ts\\n+++ b/src/workflow-executor.ts\\n@@ -219,7 +219,9 @@ export class WorkflowExecutor {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps)) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     // Use static DependencyResolver\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-registry.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-registry.ts b/src/workflow-registry.ts\\nindex 2cbe3a89..6bd855fb 100644\\n--- a/src/workflow-registry.ts\\n+++ b/src/workflow-registry.ts\\n@@ -410,7 +410,9 @@ export class WorkflowRegistry {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps || {})) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     try {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/ai-review-service.test.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/tests/unit/ai-review-service.test.ts b/tests/unit/ai-review-service.test.ts\\nindex ec2b998a..2c616c99 100644\\n--- a/tests/unit/ai-review-service.test.ts\\n+++ b/tests/unit/ai-review-service.test.ts\\n@@ -482,6 +482,37 @@ describe('AIReviewService', () => {\\n       expect(context).toContain('<additions>10</additions>');\\n       expect(context).toContain('<deletions>5</deletions>');\\n     });\\n+\\n+    it('should exclude diffs by default when in Slack mode', async () => {\\n+      // Slack mode is detected by presence of slackConversation property\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // In Slack mode, diffs should be excluded by default\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+      // Should still include file summary\\n+      expect(context).toContain('<files_summary>');\\n+    });\\n+\\n+    it('should include diffs in Slack mode when explicitly requested', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      (mockPRInfo as any).includeCodeContext = true;\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // With explicit includeCodeContext: true, diffs should be included\\n+      expect(context).toContain('<full_diff>');\\n+      expect(context).toContain('--- test.ts');\\n+    });\\n+\\n+    it('should exclude diffs in Slack mode even when includeCodeContext is undefined', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      // Deliberately not setting includeCodeContext to test default behavior\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+    });\\n   });\\n \\n   describe('Comment Filtering for Code Review', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config-extends.test.ts\",\"additions\":8,\"deletions\":8,\"changes\":16,\"patch\":\"diff --git a/tests/unit/config-extends.test.ts b/tests/unit/config-extends.test.ts\\nindex b112a869..7ee69ce6 100644\\n--- a/tests/unit/config-extends.test.ts\\n+++ b/tests/unit/config-extends.test.ts\\n@@ -564,7 +564,7 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.security).toBeDefined();\\n       expect(config.checks!.performance).toBeDefined();\\n       expect(config.checks!.custom).toBeDefined();\\n-      expect(config.output.pr_comment.format).toBe('json');\\n+      expect(config.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should extend from default configuration', async () => {\\n@@ -1002,7 +1002,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.base).toBeDefined(); // From base\\n       expect(loaded.checks!.middle).toBeDefined(); // From middle\\n       expect(loaded.checks!.top).toBeDefined(); // From top\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From top\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From top\\n     });\\n \\n     it('should handle mixed local and remote extends', async () => {\\n@@ -1079,7 +1079,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.local).toBeDefined();\\n       expect(loaded.checks!.remote).toBeDefined();\\n       expect(loaded.checks!.child).toBeDefined();\\n-      expect(loaded.output.pr_comment.format).toBe('json');\\n+      expect(loaded.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should handle extends with relative path resolution', async () => {\\n@@ -1593,9 +1593,9 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.performance).toBeDefined(); // Added in child\\n \\n       // Verify output\\n-      expect(config.output.pr_comment.format).toBe('json'); // Overridden\\n-      expect(config.output.pr_comment.group_by).toBe('file'); // Overridden\\n-      expect(config.output.pr_comment.collapse).toBe(false); // Overridden\\n+      expect(config.output!.pr_comment.format).toBe('json'); // Overridden\\n+      expect(config.output!.pr_comment.group_by).toBe('file'); // Overridden\\n+      expect(config.output!.pr_comment.collapse).toBe(false); // Overridden\\n     });\\n \\n     it('should handle configuration with disabled checks removal', async () => {\\n@@ -1814,7 +1814,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.root).toBeDefined(); // From root\\n       expect(loaded.checks!.level1).toBeDefined(); // From level1\\n       expect(loaded.checks!.level2).toBeDefined(); // From level2\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From level2\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From level2\\n     });\\n \\n     it('should handle mixed absolute and relative paths in extends array', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config.test.ts\",\"additions\":7,\"deletions\":7,\"changes\":14,\"patch\":\"diff --git a/tests/unit/config.test.ts b/tests/unit/config.test.ts\\nindex 67ce61fd..0677a3d4 100644\\n--- a/tests/unit/config.test.ts\\n+++ b/tests/unit/config.test.ts\\n@@ -48,7 +48,7 @@ output:\\n       expect(config.version).toBe('1.0');\\n       expect(config.checks).toHaveProperty('performance');\\n       expect(config.checks).toHaveProperty('security');\\n-      expect(config.output.pr_comment.format).toBe('table');\\n+      expect(config.output!.pr_comment.format).toBe('table');\\n     });\\n \\n     it('should handle missing config file gracefully', async () => {\\n@@ -264,9 +264,9 @@ checks:\\n       const config = await configManager.loadConfig('/path/to/minimal.yaml');\\n \\n       // Should have default output configuration\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.group_by).toBe('check');\\n-      expect(config.output.pr_comment.collapse).toBe(true);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.group_by).toBe('check');\\n+      expect(config.output!.pr_comment.collapse).toBe(true);\\n     });\\n   });\\n \\n@@ -483,8 +483,8 @@ output:\\n       expect(config.checks!.performance.prompt).toContain('N+1 database queries');\\n       expect(config.checks!.security.prompt).toContain('SQL injection');\\n       expect(config.checks!.architecture.prompt).toContain('SOLID principles');\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.collapse).toBe(false);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.collapse).toBe(false);\\n     });\\n   });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/foreach-empty-skip.test.ts\",\"additions\":5,\"deletions\":3,\"changes\":8,\"patch\":\"diff --git a/tests/unit/foreach-empty-skip.test.ts b/tests/unit/foreach-empty-skip.test.ts\\nindex 1ea8e1e5..2640ed14 100644\\n--- a/tests/unit/foreach-empty-skip.test.ts\\n+++ b/tests/unit/foreach-empty-skip.test.ts\\n@@ -90,11 +90,13 @@ describe('forEach Empty Array Skip', () => {\\n     );\\n     expect(processStats?.totalRuns).toBe(0);\\n \\n-    // notify-ticket should also NOT run because process-ticket returned empty\\n+    // notify-ticket SHOULD run even though process-ticket had 0 runs from empty forEach\\n+    // This is the expected behavior: downstream steps that depend on forEach children\\n+    // should still execute (with empty/null inputs) to handle the \\\"no items\\\" case\\n     const notifyStats = result.executionStatistics?.checks!.find(\\n       c => c.checkName === 'notify-ticket'\\n     );\\n-    expect(notifyStats?.totalRuns).toBe(0);\\n+    expect(notifyStats?.totalRuns).toBe(1);\\n   });\\n \\n   it('should handle empty array with transform_js', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/ai-check-provider.test.ts\",\"additions\":61,\"deletions\":1,\"changes\":62,\"patch\":\"diff --git a/tests/unit/providers/ai-check-provider.test.ts b/tests/unit/providers/ai-check-provider.test.ts\\nindex d0b94086..a858ddae 100644\\n--- a/tests/unit/providers/ai-check-provider.test.ts\\n+++ b/tests/unit/providers/ai-check-provider.test.ts\\n@@ -399,6 +399,66 @@ describe('AICheckProvider', () => {\\n         },\\n       });\\n     });\\n+\\n+    it('derives allowedFolders and path from workspace when present on parent context', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const workspace = {\\n+        isEnabled: () => true,\\n+        getWorkspaceInfo: () => ({\\n+          workspacePath: '/tmp/ws-123',\\n+          mainProjectPath: '/tmp/ws-123/main-project',\\n+        }),\\n+        listProjects: () => [\\n+          { name: 'tyk', path: '/tmp/ws-123/tyk' },\\n+          { name: 'tyk-docs', path: '/tmp/ws-123/tyk-docs' },\\n+        ],\\n+      };\\n+\\n+      const execContext: any = {\\n+        _parentContext: {\\n+          workspace,\\n+        },\\n+      };\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'code_help',\\n+        ai: {\\n+          provider: 'google',\\n+          model: 'gemini-2.5-pro',\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, execContext);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'google',\\n+        model: 'gemini-2.5-pro',\\n+        // Workspace root should be used as primary working directory for tools\\n+        path: '/tmp/ws-123',\\n+      });\\n+\\n+      // allowedFolders should contain the workspace plus all project paths, de-duped\\n+      expect(capturedConfig.allowedFolders).toEqual(\\n+        expect.arrayContaining(['/tmp/ws-123', '/tmp/ws-123/tyk', '/tmp/ws-123/tyk-docs'])\\n+      );\\n+    });\\n   });\\n \\n   describe('getSupportedConfigKeys', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/command-check-provider.test.ts\",\"additions\":105,\"deletions\":1,\"changes\":106,\"patch\":\"diff --git a/tests/unit/providers/command-check-provider.test.ts b/tests/unit/providers/command-check-provider.test.ts\\nindex 3bf11a84..0a821bbd 100644\\n--- a/tests/unit/providers/command-check-provider.test.ts\\n+++ b/tests/unit/providers/command-check-provider.test.ts\\n@@ -666,4 +666,108 @@ describe('CommandCheckProvider', () => {\\n       expect((result as any).output).toBe('content'); // CommandCheckProvider trims whitespace\\n     });\\n   });\\n+\\n+  describe('Workflow Inputs', () => {\\n+    it('should use workflowInputs from config when available', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Hello from config.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from config.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with inputs from config.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from config.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should fall back to context.workflowInputs when config.workflowInputs is not set', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Hello from context.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from context.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify the command was rendered with inputs from context.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from context.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should prefer config.workflowInputs over context.workflowInputs', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Config takes precedence',\\n+        },\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Context should be ignored',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Config takes precedence\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify config.workflowInputs takes precedence\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Config takes precedence\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should provide empty inputs when neither config nor context has workflowInputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with empty inputs (undefined renders as empty)\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+  });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/git-checkout-workspace.test.ts\",\"additions\":4,\"deletions\":5,\"changes\":9,\"patch\":\"diff --git a/tests/unit/providers/git-checkout-workspace.test.ts b/tests/unit/providers/git-checkout-workspace.test.ts\\nindex 4eb625b7..4a0fbb6e 100644\\n--- a/tests/unit/providers/git-checkout-workspace.test.ts\\n+++ b/tests/unit/providers/git-checkout-workspace.test.ts\\n@@ -129,13 +129,12 @@ describe('GitCheckoutProvider Workspace Integration', () => {\\n         const output = (result as any).output as GitCheckoutOutput;\\n         expect(output.success).toBe(true);\\n         expect(output.workspace_path).toBeDefined();\\n-        // When checkName is provided, it's used as the project name\\n-        expect(output.workspace_path).toContain('test-checkout');\\n \\n-        // Verify project was added to workspace\\n+        // Verify project was added to workspace and the name is derived\\n+        // from the repository/description, not the checkName.\\n         const projects = workspace.listProjects();\\n         expect(projects.length).toBe(1);\\n-        expect(projects[0].name).toBe('test-checkout');\\n+        expect(projects[0].name).toBe('external-repo');\\n       } finally {\\n         await workspace.cleanup();\\n         fs.rmSync(mainProjectDir, { recursive: true, force: true });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/http-client-provider.test.ts\",\"additions\":15,\"deletions\":10,\"changes\":25,\"patch\":\"diff --git a/tests/unit/providers/http-client-provider.test.ts b/tests/unit/providers/http-client-provider.test.ts\\nindex b759ad75..caa73417 100644\\n--- a/tests/unit/providers/http-client-provider.test.ts\\n+++ b/tests/unit/providers/http-client-provider.test.ts\\n@@ -120,15 +120,17 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect(result).toEqual<ReviewSummary & { data: unknown }>({\\n+      // The provider returns data in the 'output' property (consistent with other providers)\\n+      expect(result).toEqual({\\n         issues: [],\\n-        data: responseData,\\n+        output: { status: 'ok', data: { value: 123 } },\\n       });\\n     });\\n \\n     it('should handle POST request with body', async () => {\\n       mockConfig.method = 'POST';\\n-      mockConfig.body = '{\\\"request\\\": \\\"data\\\"}';\\n+      // Use a body with Liquid template to trigger parseAndRender\\n+      mockConfig.body = '{\\\"request\\\": \\\"{{ pr.title }}\\\"}';\\n \\n       const responseData = { result: 'success' };\\n       const mockResponse = {\\n@@ -143,15 +145,16 @@ describe('HttpClientProvider', () => {\\n       };\\n \\n       mockFetch.mockResolvedValue(mockResponse);\\n-      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"data\\\"}');\\n+      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"Test PR\\\"}');\\n \\n       const result = await provider.execute(mockPRInfo, mockConfig, new Map());\\n \\n+      // Verify the body template was parsed by Liquid with the correct template\\n       expect(mockLiquid.parseAndRender).toHaveBeenCalledWith(\\n-        '{\\\"request\\\": \\\"data\\\"}',\\n+        '{\\\"request\\\": \\\"{{ pr.title }}\\\"}',\\n         expect.objectContaining({\\n           pr: expect.any(Object),\\n-          outputs: {},\\n+          outputs: expect.any(Object),\\n         })\\n       );\\n \\n@@ -159,7 +162,7 @@ describe('HttpClientProvider', () => {\\n         'https://api.example.com/data',\\n         expect.objectContaining({\\n           method: 'POST',\\n-          body: '{\\\"request\\\": \\\"data\\\"}',\\n+          body: '{\\\"request\\\": \\\"Test PR\\\"}',\\n           headers: expect.objectContaining({\\n             Authorization: 'Bearer token',\\n             'Content-Type': 'application/json',\\n@@ -167,7 +170,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(responseData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { result: string } }).output.result).toEqual('success');\\n     });\\n \\n     it('should handle HTTP errors', async () => {\\n@@ -235,7 +239,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(transformedData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { transformed: string } }).output.transformed).toEqual('result');\\n     });\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/session-reuse-config.test.ts\",\"additions\":20,\"deletions\":1,\"changes\":21,\"patch\":\"diff --git a/tests/unit/session-reuse-config.test.ts b/tests/unit/session-reuse-config.test.ts\\nindex a2e2ee26..18fc42e6 100644\\n--- a/tests/unit/session-reuse-config.test.ts\\n+++ b/tests/unit/session-reuse-config.test.ts\\n@@ -152,6 +152,25 @@ describe('Session Reuse Configuration Validation', () => {\\n       }).not.toThrow();\\n     });\\n \\n+    it('should accept reuse_ai_session=\\\\\\\"self\\\\\\\" without depends_on', () => {\\n+      const config: Partial<VisorConfig> = {\\n+        version: '1.0',\\n+        checks: {\\n+          'loop-check': {\\n+            type: 'ai',\\n+            prompt: 'Chat-like loop that reuses its own session',\\n+            on: ['pr_opened'],\\n+            // Special self-reuse mode does not require depends_on\\n+            reuse_ai_session: 'self',\\n+          },\\n+        },\\n+      };\\n+\\n+      expect(() => {\\n+        (configManager as any).validateConfig(config);\\n+      }).not.toThrow();\\n+    });\\n+\\n     it('should accept reuse_ai_session=true with multiple dependencies', () => {\\n       const config: Partial<VisorConfig> = {\\n         version: '1.0',\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/slack-markdown-format.test.ts\",\"additions\":25,\"deletions\":1,\"changes\":26,\"patch\":\"diff --git a/tests/unit/slack-markdown-format.test.ts b/tests/unit/slack-markdown-format.test.ts\\nindex 5e57ccae..39d0baf3 100644\\n--- a/tests/unit/slack-markdown-format.test.ts\\n+++ b/tests/unit/slack-markdown-format.test.ts\\n@@ -39,4 +39,28 @@ describe('markdownToSlack', () => {\\n       ['‚Ä¢ parent', '  ‚Ä¢ child', '```', '- not-a-bullet inside code', '```', '‚Ä¢ after'].join('\\\\n')\\n     );\\n   });\\n+\\n+  it('converts markdown headers to bold text', () => {\\n+    const input = '# Main Title\\\\n## Subtitle\\\\n### Section';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*Main Title*\\\\n*Subtitle*\\\\n*Section*');\\n+  });\\n+\\n+  it('adds newline before h1/h2 headers when preceded by content', () => {\\n+    const input = 'Some content\\\\n## New Section\\\\nMore content';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('Some content\\\\n\\\\n*New Section*\\\\nMore content');\\n+  });\\n+\\n+  it('does not add newline before h1 if it is the first line', () => {\\n+    const input = '# First Header\\\\nContent here';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*First Header*\\\\nContent here');\\n+  });\\n+\\n+  it('ignores headers inside code blocks', () => {\\n+    const input = '```\\\\n# This is a comment\\\\n```\\\\n# Real Header';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('```\\\\n# This is a comment\\\\n```\\\\n*Real Header*');\\n+  });\\n });\\n\",\"status\":\"modified\"}],\"outputs\":{\"ask\":{\"text\":\"If I'm using the OSS Gateway with API Keys, how would I migrate to using hybrid where the control plane is hosted on Tyk Cloud?  I want to preserve my APIs and Keys.\",\"ts\":1767897523902},\"jira-context\":{\"jira_context_xml\":\"\",\"issues\":[],\"issue_count\":0,\"ts\":1767897523999},\"zendesk-context\":{\"zendesk_context_xml\":\"\",\"tickets\":\"\",\"ticket_count\":0,\"attachments\":\"\",\"ts\":1767897524019},\"log-request\":{\"issues\":[],\"logOutput\":\"‚ÑπÔ∏è **INFO**: üì• Request from user: unknown\\nChannel: unknown\\nThread: unknown\\nMessage: If I'm using the OSS Gateway with API Keys, how would I migrate to using hybrid where the control plane is hosted on Tyk Cloud?  I want to preserve my APIs and Keys.\\n\"},\"code-help-error\":{\"issues\":[]},\"customer-insights-error\":{\"issues\":[]},\"release-notes-error\":{\"issues\":[]}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.output":"{\"intent\":\"code_help\",\"topic\":\"How to migrate from an OSS Gateway with API Keys to a hybrid setup with Tyk Cloud, while preserving existing APIs and Keys.\",\"text\":\"{\\n  \\\"intent\\\": \\\"code_help\\\",\\n  \\\"topic\\\": \\\"How to migrate from an OSS Gateway with API Keys to a hybrid setup with Tyk Cloud, while preserving existing APIs and Keys.\\\"\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":2,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"code-help","visor.provider.type":"workflow"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"4856a035-f202-4f87-948b-f87f88cd6e81"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"4856a035-f202-4f87-948b-f87f88cd6e81"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"4856a035-f202-4f87-948b-f87f88cd6e81"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-tyk-docs","visor.provider.type":"git-checkout"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4856a035-f202-4f87-948b-f87f88cd6e81"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4856a035-f202-4f87-948b-f87f88cd6e81"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"4856a035-f202-4f87-948b-f87f88cd6e81"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ensure-code-plan","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"ensure-code-plan","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"outputs\":{\"checkout-tyk-docs\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-docs-main-5812080a\",\"ref\":\"main\",\"commit\":\"a022bb7c96ca4f48bcdac27cae3f7c4a24171ee9\",\"worktree_id\":\"TykTechnologies-tyk-docs-main-5812080a\",\"repository\":\"TykTechnologies/tyk-docs\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/2535b0e0-3760-4b68-8045-9a7f68bb76d6/tyk-docs\",\"ts\":1767897553112}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"ensure-code-plan","visor.check.output":"{\"projects\":[{\"project_id\":\"tyk\",\"reason\":\"The user is migrating from an OSS Tyk Gateway. This project contains the gateway's core logic, including how it loads API definitions and keys from files (in OSS mode) versus how it synchronizes them from a control plane (in Hybrid mode). The gateway's configuration (`tyk.conf`) is central to this change.\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Cloud control plane is a managed Tyk Dashboard. This project is the backend for the Dashboard. The migration requires importing existing API definitions and keys into the Dashboard, which would be done via the Dashboard's API. This project handles the storage and management of those objects.\"},{\"project_id\":\"tyk-sink\",\"reason\":\"In a hybrid architecture, the gateway (data plane) synchronizes with the dashboard (control plane) via MDCB (Tyk Sink). This project is the synchronization mechanism. Understanding its role is crucial for understanding the target architecture the user is migrating to.\"}],\"notes\":\"The user wants to migrate from a file-based OSS Gateway to a Tyk Cloud Hybrid setup, preserving APIs and keys. This involves reconfiguring the Gateway to connect to the Cloud control plane (Dashboard/MDCB) and migrating the configuration (APIs and keys) from local files/Redis into the Dashboard's database. The selected projects cover the source (Gateway), the destination (Dashboard), and the synchronization mechanism (MDCB).\",\"text\":\"{\\n  \\\"projects\\\": [\\n    {\\n      \\\"project_id\\\": \\\"tyk\\\",\\n      \\\"reason\\\": \\\"The user is migrating from an OSS Tyk Gateway. This project contains the gateway's core logic, including how it loads API definitions and keys from files (in OSS mode) versus how it synchronizes them from a control plane (in Hybrid mode). The gateway's configuration (`tyk.conf`) is central to this change.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-analytics\\\",\\n      \\\"reason\\\": \\\"The Tyk Cloud control plane is a managed Tyk Dashboard. This project is the backend for the Dashboard. The migration requires importing existing API definitions and keys into the Dashboard, which would be done via the Dashboard's API. This project handles the storage and management of those objects.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-sink\\\",\\n      \\\"reason\\\": \\\"In a hybrid architecture, the gateway (data plane) synchronizes with the dashboard (control plane) via MDCB (Tyk Sink). This project is the synchronization mechanism. Understanding its role is crucial for understanding the target architecture the user is migrating to.\\\"\\n    }\\n  ],\\n  \\\"notes\\\": \\\"The user wants to migrate from a file-based OSS Gateway to a Tyk Cloud Hybrid setup, preserving APIs and keys. This involves reconfiguring the Gateway to connect to the Cloud control plane (Dashboard/MDCB) and migrating the configuration (APIs and keys) from local files/Redis into the Dashboard's database. The selected projects cover the source (Gateway), the destination (Dashboard), and the synchronization mechanism (MDCB).\\\"\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4856a035-f202-4f87-948b-f87f88cd6e81"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4856a035-f202-4f87-948b-f87f88cd6e81"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"4856a035-f202-4f87-948b-f87f88cd6e81"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"project-items","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4856a035-f202-4f87-948b-f87f88cd6e81"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4856a035-f202-4f87-948b-f87f88cd6e81"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"4856a035-f202-4f87-948b-f87f88cd6e81"}}]}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":0,"visor.foreach.total":3},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":1,"visor.foreach.total":3},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":2,"visor.foreach.total":3},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4856a035-f202-4f87-948b-f87f88cd6e81"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4856a035-f202-4f87-948b-f87f88cd6e81"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"4856a035-f202-4f87-948b-f87f88cd6e81"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"project-code-query","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"project-code-query","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"outputs\":{\"ensure-code-plan\":{\"projects\":[{\"project_id\":\"tyk\",\"reason\":\"The user is migrating from an OSS Tyk Gateway. This project contains the gateway's core logic, including how it loads API definitions and keys from files (in OSS mode) versus how it synchronizes them from a control plane (in Hybrid mode). The gateway's configuration (`tyk.conf`) is central to this change.\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Cloud control plane is a managed Tyk Dashboard. This project is the backend for the Dashboard. The migration requires importing existing API definitions and keys into the Dashboard, which would be done via the Dashboard's API. This project handles the storage and management of those objects.\"},{\"project_id\":\"tyk-sink\",\"reason\":\"In a hybrid architecture, the gateway (data plane) synchronizes with the dashboard (control plane) via MDCB (Tyk Sink). This project is the synchronization mechanism. Understanding its role is crucial for understanding the target architecture the user is migrating to.\"}],\"notes\":\"The user wants to migrate from a file-based OSS Gateway to a Tyk Cloud Hybrid setup, preserving APIs and keys. This involves reconfiguring the Gateway to connect to the Cloud control plane (Dashboard/MDCB) and migrating the configuration (APIs and keys) from local files/Redis into the Dashboard's database. The selected projects cover the source (Gateway), the destination (Dashboard), and the synchronization mechanism (MDCB).\",\"text\":\"{\\n  \\\"projects\\\": [\\n    {\\n      \\\"project_id\\\": \\\"tyk\\\",\\n      \\\"reason\\\": \\\"The user is migrating from an OSS Tyk Gateway. This project contains the gateway's core logic, including how it loads API definitions and keys from files (in OSS mode) versus how it synchronizes them from a control plane (in Hybrid mode). The gateway's configuration (`tyk.conf`) is central to this change.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-analytics\\\",\\n      \\\"reason\\\": \\\"The Tyk Cloud control plane is a managed Tyk Dashboard. This project is the backend for the Dashboard. The migration requires importing existing API definitions and keys into the Dashboard, which would be done via the Dashboard's API. This project handles the storage and management of those objects.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-sink\\\",\\n      \\\"reason\\\": \\\"In a hybrid architecture, the gateway (data plane) synchronizes with the dashboard (control plane) via MDCB (Tyk Sink). This project is the synchronization mechanism. Understanding its role is crucial for understanding the target architecture the user is migrating to.\\\"\\n    }\\n  ],\\n  \\\"notes\\\": \\\"The user wants to migrate from a file-based OSS Gateway to a Tyk Cloud Hybrid setup, preserving APIs and keys. This involves reconfiguring the Gateway to connect to the Cloud control plane (Dashboard/MDCB) and migrating the configuration (APIs and keys) from local files/Redis into the Dashboard's database. The selected projects cover the source (Gateway), the destination (Dashboard), and the synchronization mechanism (MDCB).\\\"\\n}\",\"ts\":1767897578864},\"project-items\":[{\"project_id\":\"tyk\",\"reason\":\"The user is migrating from an OSS Tyk Gateway. This project contains the gateway's core logic, including how it loads API definitions and keys from files (in OSS mode) versus how it synchronizes them from a control plane (in Hybrid mode). The gateway's configuration (`tyk.conf`) is central to this change.\",\"repository\":\"TykTechnologies/tyk\",\"description\":\"Tyk Gateway\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Cloud control plane is a managed Tyk Dashboard. This project is the backend for the Dashboard. The migration requires importing existing API definitions and keys into the Dashboard, which would be done via the Dashboard's API. This project handles the storage and management of those objects.\",\"repository\":\"TykTechnologies/tyk-analytics\",\"description\":\"Tyk Dashboard backend\"},{\"project_id\":\"tyk-sink\",\"reason\":\"In a hybrid architecture, the gateway (data plane) synchronizes with the dashboard (control plane) via MDCB (Tyk Sink). This project is the synchronization mechanism. Understanding its role is crucial for understanding the target architecture the user is migrating to.\",\"repository\":\"TykTechnologies/tyk-sink\",\"description\":\"Tyk MDCB\"}],\"checkout-projects\":{\"issues\":[],\"isForEach\":true,\"forEachItems\":[{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-HEAD-4917b1fc\",\"ref\":\"HEAD\",\"commit\":\"bb67e9e7f9382a09ff152c6942d00023c463feef\",\"worktree_id\":\"TykTechnologies-tyk-HEAD-4917b1fc\",\"repository\":\"TykTechnologies/tyk\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/2535b0e0-3760-4b68-8045-9a7f68bb76d6/tyk\"},{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-analytics-HEAD-565786e8\",\"ref\":\"HEAD\",\"commit\":\"b7ed8000a93e1284b28100938363a312d2d1fad1\",\"worktree_id\":\"TykTechnologies-tyk-analytics-HEAD-565786e8\",\"repository\":\"TykTechnologies/tyk-analytics\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/2535b0e0-3760-4b68-8045-9a7f68bb76d6/tyk-analytics\"},{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-sink-HEAD-9867953e\",\"ref\":\"HEAD\",\"commit\":\"d015f96a8988b5cad4dfc51ff4b48fd77e9aec8b\",\"worktree_id\":\"TykTechnologies-tyk-sink-HEAD-9867953e\",\"repository\":\"TykTechnologies/tyk-sink\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/2535b0e0-3760-4b68-8045-9a7f68bb76d6/tyk-sink\"}],\"forEachItemResults\":[{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-HEAD-4917b1fc\",\"ref\":\"HEAD\",\"commit\":\"bb67e9e7f9382a09ff152c6942d00023c463feef\",\"worktree_id\":\"TykTechnologies-tyk-HEAD-4917b1fc\",\"repository\":\"TykTechnologies/tyk\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/2535b0e0-3760-4b68-8045-9a7f68bb76d6/tyk\"}},{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-analytics-HEAD-565786e8\",\"ref\":\"HEAD\",\"commit\":\"b7ed8000a93e1284b28100938363a312d2d1fad1\",\"worktree_id\":\"TykTechnologies-tyk-analytics-HEAD-565786e8\",\"repository\":\"TykTechnologies/tyk-analytics\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/2535b0e0-3760-4b68-8045-9a7f68bb76d6/tyk-analytics\"}},{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-sink-HEAD-9867953e\",\"ref\":\"HEAD\",\"commit\":\"d015f96a8988b5cad4dfc51ff4b48fd77e9aec8b\",\"worktree_id\":\"TykTechnologies-tyk-sink-HEAD-9867953e\",\"repository\":\"TykTechnologies/tyk-sink\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/2535b0e0-3760-4b68-8045-9a7f68bb76d6/tyk-sink\"}}]},\"checkout-tyk-docs\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-docs-main-5812080a\",\"ref\":\"main\",\"commit\":\"a022bb7c96ca4f48bcdac27cae3f7c4a24171ee9\",\"worktree_id\":\"TykTechnologies-tyk-docs-main-5812080a\",\"repository\":\"TykTechnologies/tyk-docs\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/2535b0e0-3760-4b68-8045-9a7f68bb76d6/tyk-docs\",\"ts\":1767897553112},\"project-items-raw\":[{\"project_id\":\"tyk\",\"reason\":\"The user is migrating from an OSS Tyk Gateway. This project contains the gateway's core logic, including how it loads API definitions and keys from files (in OSS mode) versus how it synchronizes them from a control plane (in Hybrid mode). The gateway's configuration (`tyk.conf`) is central to this change.\",\"repository\":\"TykTechnologies/tyk\",\"description\":\"Tyk Gateway\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Cloud control plane is a managed Tyk Dashboard. This project is the backend for the Dashboard. The migration requires importing existing API definitions and keys into the Dashboard, which would be done via the Dashboard's API. This project handles the storage and management of those objects.\",\"repository\":\"TykTechnologies/tyk-analytics\",\"description\":\"Tyk Dashboard backend\"},{\"project_id\":\"tyk-sink\",\"reason\":\"In a hybrid architecture, the gateway (data plane) synchronizes with the dashboard (control plane) via MDCB (Tyk Sink). This project is the synchronization mechanism. Understanding its role is crucial for understanding the target architecture the user is migrating to.\",\"repository\":\"TykTechnologies/tyk-sink\",\"description\":\"Tyk MDCB\"}]},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"project-code-query","visor.check.output":"{\"answer\":{\"text\":\"Migrating your Tyk OSS Gateway to a Tyk Cloud Hybrid setup while preserving your APIs and keys involves two main steps: reconfiguring your gateway to connect to the Tyk Cloud control plane and then migrating your existing API definitions and keys using the Tyk Dashboard API.\\n\\n### 1. Reconfiguring Your Gateway\\n\\nYou'll need to update your gateway's `tyk.conf` file to switch it from a file-based configuration to a hybrid setup that communicates with the Tyk Cloud control plane. Here are the key changes:\\n\\n1.  **`use_db_app_configs`**: Set this to `true`. This tells the gateway to fetch API configurations from the remote control plane instead of the local `apps` directory.\\n\\n2.  **`policies.policy_source`**: Change this from `\\\"file\\\"` to `\\\"rpc\\\"`. This instructs the gateway to fetch policies from the control plane via RPC.\\n\\n3.  **`slave_options`**: Add this section to your `tyk.conf`. You will receive the values for these fields from your Tyk Cloud account.\\n\\n    ```json\\n    \\\"slave_options\\\": {\\n      \\\"use_rpc\\\": true,\\n      \\\"use_ssl\\\": true,\\n      \\\"connection_string\\\": \\\"your-tyk-cloud-rpc-connection-string\\\",\\n      \\\"rpc_key\\\": \\\"your-tyk-cloud-rpc-key\\\",\\n      \\\"api_key\\\": \\\"your-tyk-cloud-api-key\\\",\\n      \\\"group_id\\\": \\\"\\\"\\n    }\\n    ```\\n\\n4.  **`enable_analytics`**: Ensure this is set to `true` to send analytics data to your Tyk Cloud dashboard.\\n\\nAfter making these changes and restarting your gateway, it will connect to Tyk Cloud for its configuration.\\n\\n### 2. Migrating Your APIs and Keys\\n\\nNext, you'll need to import your existing API definitions and keys into your Tyk Cloud account. This is done by scripting against the Tyk Dashboard API.\\n\\n#### Migrating API Definitions\\n\\nYou can import your API definitions using the `POST /admin/apis/import` endpoint. For each API definition file in your OSS gateway's `apps` directory, you'll need to send a POST request with the API definition as the payload.\\n\\n**Endpoint**: `POST /admin/apis/import`\\n\\n**Request Body**:\\n\\n```json\\n{\\n  \\\"apis\\\": [\\n    {\\n      \\\"api_definition\\\": {\\n        // Your API definition from the JSON file goes here\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nYou will need to authenticate this request with an admin user's API key from your Tyk Cloud account, sent as an `Admin-Auth` header.\\n\\n#### Migrating API Keys\\n\\nTo migrate your API keys, you'll need to extract the session data for each key from your OSS Redis database and then create a new key in Tyk Cloud with the same data. This is done using the `POST /admin/org/keys` endpoint.\\n\\n1.  **Extract Key Data from Redis**: Your API key data is stored in Redis with keys prefixed by `apikey-`. You'll need to retrieve the JSON value for each key.\\n\\n2.  **Create New Keys in Tyk Cloud**: For each key's session data, make a `POST` request to the `/admin/org/keys` endpoint with the session data as the request body.\\n\\n    **Endpoint**: `POST /admin/org/keys`\\n\\n    **Request Body** (example `SessionState` object from Redis):\\n\\n    ```json\\n    {\\n      \\\"allowance\\\": 1000,\\n      \\\"rate\\\": 1000,\\n      \\\"per\\\": 60,\\n      \\\"expires\\\": 0,\\n      \\\"quota_max\\\": -1,\\n      \\\"quota_renews\\\": -1,\\n      \\\"quota_remaining\\\": 0,\\n      \\\"quota_renewal_rate\\\": -1,\\n      \\\"access_rights\\\": {\\n        \\\"your-api-id\\\": {\\n          \\\"api_name\\\": \\\"Your API Name\\\",\\n          \\\"api_id\\\": \\\"your-api-id\\\",\\n          \\\"versions\\\": [\\\"Default\\\"]\\n        }\\n      },\\n      \\\"org_id\\\": \\\"your-tyk-cloud-org-id\\\"\\n    }\\n    ```\\n\\nThe API will respond with a new key ID. You will need to distribute these new keys to your API consumers to replace their old ones. This process creates new keys in Tyk Cloud with the same settings as your old keys, effectively preserving your key configurations.\\n\\n### Code and Documentation References\\n\\n*   **Gateway Configuration (`SlaveOptionsConfig`)**: [https://github.com/TykTechnologies/tyk/blob/master/config/config.go#L538](https://github.com/TykTechnologies/tyk/blob/master/config/config.go#L538)\\n*   **Dashboard Admin API for Importing APIs**: [https://github.com/TykTechnologies/tyk-analytics/blob/master/swagger-admin.yml#L53](https://github.com/TykTechnologies/tyk-analytics/blob/master/swagger-admin.yml#L53)\\n*   **Dashboard Admin API for Creating Keys**: [https://github.com/TykTechnologies/tyk-analytics/blob/master/swagger-admin.yml#L138](https://github.com/TykTechnologies/tyk-analytics/blob/master/swagger-admin.yml#L138)\"},\"text\":\"{\\n  \\\"answer\\\": {\\n    \\\"text\\\": \\\"Migrating your Tyk OSS Gateway to a Tyk Cloud Hybrid setup while preserving your APIs and keys involves two main steps: reconfiguring your gateway to connect to the Tyk Cloud control plane and then migrating your existing API definitions and keys using the Tyk Dashboard API.\\\\n\\\\n### 1. Reconfiguring Your Gateway\\\\n\\\\nYou'll need to update your gateway's `tyk.conf` file to switch it from a file-based configuration to a hybrid setup that communicates with the Tyk Cloud control plane. Here are the key changes:\\\\n\\\\n1.  **`use_db_app_configs`**: Set this to `true`. This tells the gateway to fetch API configurations from the remote control plane instead of the local `apps` directory.\\\\n\\\\n2.  **`policies.policy_source`**: Change this from `\\\\\\\"file\\\\\\\"` to `\\\\\\\"rpc\\\\\\\"`. This instructs the gateway to fetch policies from the control plane via RPC.\\\\n\\\\n3.  **`slave_options`**: Add this section to your `tyk.conf`. You will receive the values for these fields from your Tyk Cloud account.\\\\n\\\\n    ```json\\\\n    \\\\\\\"slave_options\\\\\\\": {\\\\n      \\\\\\\"use_rpc\\\\\\\": true,\\\\n      \\\\\\\"use_ssl\\\\\\\": true,\\\\n      \\\\\\\"connection_string\\\\\\\": \\\\\\\"your-tyk-cloud-rpc-connection-string\\\\\\\",\\\\n      \\\\\\\"rpc_key\\\\\\\": \\\\\\\"your-tyk-cloud-rpc-key\\\\\\\",\\\\n      \\\\\\\"api_key\\\\\\\": \\\\\\\"your-tyk-cloud-api-key\\\\\\\",\\\\n      \\\\\\\"group_id\\\\\\\": \\\\\\\"\\\\\\\"\\\\n    }\\\\n    ```\\\\n\\\\n4.  **`enable_analytics`**: Ensure this is set to `true` to send analytics data to your Tyk Cloud dashboard.\\\\n\\\\nAfter making these changes and restarting your gateway, it will connect to Tyk Cloud for its configuration.\\\\n\\\\n### 2. Migrating Your APIs and Keys\\\\n\\\\nNext, you'll need to import your existing API definitions and keys into your Tyk Cloud account. This is done by scripting against the Tyk Dashboard API.\\\\n\\\\n#### Migrating API Definitions\\\\n\\\\nYou can import your API definitions using the `POST /admin/apis/import` endpoint. For each API definition file in your OSS gateway's `apps` directory, you'll need to send a POST request with the API definition as the payload.\\\\n\\\\n**Endpoint**: `POST /admin/apis/import`\\\\n\\\\n**Request Body**:\\\\n\\\\n```json\\\\n{\\\\n  \\\\\\\"apis\\\\\\\": [\\\\n    {\\\\n      \\\\\\\"api_definition\\\\\\\": {\\\\n        // Your API definition from the JSON file goes here\\\\n      }\\\\n    }\\\\n  ]\\\\n}\\\\n```\\\\n\\\\nYou will need to authenticate this request with an admin user's API key from your Tyk Cloud account, sent as an `Admin-Auth` header.\\\\n\\\\n#### Migrating API Keys\\\\n\\\\nTo migrate your API keys, you'll need to extract the session data for each key from your OSS Redis database and then create a new key in Tyk Cloud with the same data. This is done using the `POST /admin/org/keys` endpoint.\\\\n\\\\n1.  **Extract Key Data from Redis**: Your API key data is stored in Redis with keys prefixed by `apikey-`. You'll need to retrieve the JSON value for each key.\\\\n\\\\n2.  **Create New Keys in Tyk Cloud**: For each key's session data, make a `POST` request to the `/admin/org/keys` endpoint with the session data as the request body.\\\\n\\\\n    **Endpoint**: `POST /admin/org/keys`\\\\n\\\\n    **Request Body** (example `SessionState` object from Redis):\\\\n\\\\n    ```json\\\\n    {\\\\n      \\\\\\\"allowance\\\\\\\": 1000,\\\\n      \\\\\\\"rate\\\\\\\": 1000,\\\\n      \\\\\\\"per\\\\\\\": 60,\\\\n      \\\\\\\"expires\\\\\\\": 0,\\\\n      \\\\\\\"quota_max\\\\\\\": -1,\\\\n      \\\\\\\"quota_renews\\\\\\\": -1,\\\\n      \\\\\\\"quota_remaining\\\\\\\": 0,\\\\n      \\\\\\\"quota_renewal_rate\\\\\\\": -1,\\\\n      \\\\\\\"access_rights\\\\\\\": {\\\\n        \\\\\\\"your-api-id\\\\\\\": {\\\\n          \\\\\\\"api_name\\\\\\\": \\\\\\\"Your API Name\\\\\\\",\\\\n          \\\\\\\"api_id\\\\\\\": \\\\\\\"your-api-id\\\\\\\",\\\\n          \\\\\\\"versions\\\\\\\": [\\\\\\\"Default\\\\\\\"]\\\\n        }\\\\n      },\\\\n      \\\\\\\"org_id\\\\\\\": \\\\\\\"your-tyk-cloud-org-id\\\\\\\"\\\\n    }\\\\n    ```\\\\n\\\\nThe API will respond with a new key ID. You will need to distribute these new keys to your API consumers to replace their old ones. This process creates new keys in Tyk Cloud with the same settings as your old keys, effectively preserving your key configurations.\\\\n\\\\n### Code and Documentation References\\\\n\\\\n*   **Gateway Configuration (`SlaveOptionsConfig`)**: [https://github.com/TykTechnologies/tyk/blob/master/config/config.go#L538](https://github.com/TykTechnologies/tyk/blob/master/config/config.go#L538)\\\\n*   **Dashboard Admin API for Importing APIs**: [https://github.com/TykTechnologies/tyk-analytics/blob/master/swagger-admin.yml#L53](https://github.com/TykTechnologies/tyk-analytics/blob/master/swagger-admin.yml#L53)\\\\n*   **Dashboard Admin API for Creating Keys**: [https://github.com/TykTechnologies/tyk-analytics/blob/master/swagger-admin.yml#L138](https://github.com/TykTechnologies/tyk-analytics/blob/master/swagger-admin.yml#L138)\\\"\\n  }\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4856a035-f202-4f87-948b-f87f88cd6e81"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4856a035-f202-4f87-948b-f87f88cd6e81"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"4856a035-f202-4f87-948b-f87f88cd6e81"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":2,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"code-help-reply","visor.provider.type":"log"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":3,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":3,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":3,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":3,"session_id":"2535b0e0-3760-4b68-8045-9a7f68bb76d6"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"jira-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"zendesk-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"log-request","visor.provider.type":"log"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"10d38540-4f08-4593-90ff-2b12a0956679"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"10d38540-4f08-4593-90ff-2b12a0956679"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"10d38540-4f08-4593-90ff-2b12a0956679"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-keys","visor.provider.type":"script"}}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-ids","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"history\":{}},\"outputs_history\":{},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"include_comments\":true,\"max_tickets\":5,\"download_assets\":true,\"cache_dir\":\".cache/zendesk\",\"text\":\"<@U09T5KRLMPU> what is the root cause of this bug:\\n<https://tyktech.atlassian.net/browse/TT-16304>\\nwhat is the root cause of this bug:\\n<https://tyktech.atlassian.net/browse/TT-16304>\\n\"},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"10d38540-4f08-4593-90ff-2b12a0956679"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"10d38540-4f08-4593-90ff-2b12a0956679"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"10d38540-4f08-4593-90ff-2b12a0956679"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"fetch-issues","visor.provider.type":"http_client"}}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.output":"{\"data\":[\"16304\"],\"count\":1}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"fetch-tickets","visor.provider.type":"http_client"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"10d38540-4f08-4593-90ff-2b12a0956679"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"10d38540-4f08-4593-90ff-2b12a0956679"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"10d38540-4f08-4593-90ff-2b12a0956679"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"format-output","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"10d38540-4f08-4593-90ff-2b12a0956679"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"10d38540-4f08-4593-90ff-2b12a0956679"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"10d38540-4f08-4593-90ff-2b12a0956679"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"prepare-attachments","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"aggregate-downloads","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"format-output","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"e1e74d74-c17c-4320-b5ad-ac82edb98c34"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"route-intent","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.input.context":"{\"pr\":{\"number\":0,\"title\":\"Local Analysis: feat/slack-frontend-v2 (71 modified)\",\"author\":\"Leonid Bugaev\",\"branch\":\"feat/slack-frontend-v2\",\"base\":\"main\"},\"files\":[{\"filename\":\"README.md\",\"additions\":7,\"deletions\":1,\"changes\":8,\"patch\":\"diff --git a/README.md b/README.md\\nindex 41131861..d92f02cc 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -534,6 +534,12 @@ steps:\\n     session_mode: append    # Shares history for full conversation\\n ```\\n \\n+You can also reuse the **same check‚Äôs** session when it loops back to itself, using:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` with `session_mode: append`\\n+\\n+See the standalone example at `examples/session-reuse-self.yaml` and the detailed guide in [docs/advanced-ai.md](docs/advanced-ai.md).\\n+\\n Learn more: [docs/advanced-ai.md](docs/advanced-ai.md)\\n \\n ## üìã Schema-Template System\\n\",\"status\":\"modified\"},{\"filename\":\"docs/advanced-ai.md\",\"additions\":61,\"deletions\":2,\"changes\":63,\"patch\":\"diff --git a/docs/advanced-ai.md b/docs/advanced-ai.md\\nindex bb8e684a..879285ed 100644\\n--- a/docs/advanced-ai.md\\n+++ b/docs/advanced-ai.md\\n@@ -1,7 +1,7 @@\\n ## üß† Advanced AI Features\\n \\n ### AI Session Reuse\\n-Use `reuse_ai_session: true` on dependent checks to continue conversation context with the AI across checks. This improves follow‚Äëups and consistency.\\n+Use `reuse_ai_session` on checks to continue conversation context with the AI across steps. This improves follow‚Äëups and consistency for follow‚Äëon analysis and chat‚Äëstyle flows.\\n \\n **Session Modes:**\\n - **`clone` (default)**: Creates a copy of the conversation history. Each check gets an independent session with the same starting context. Changes made by one check don't affect others.\\n@@ -26,6 +26,65 @@ steps:\\n     depends_on: [security-remediation]\\n     reuse_ai_session: true\\n     session_mode: append  # Share history - sees full conversation\\n+\\n+#### Reusing your own session: `reuse_ai_session: self`\\n+\\n+Sometimes the step you want to loop back into is the AI step itself (e.g. Slack assistants or multi‚Äëturn internal tools). For that case you can use:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` ‚Äì the step reuses its **own** Probe session when it runs again in the same engine run.\\n+- `session_mode: append` ‚Äì makes the follow‚Äëup behave like a normal conversation turn.\\n+\\n+On the first run of the step, Visor creates a new ProbeAgent session and registers it. If routing (`on_success.goto`, `goto_js`, etc.) later jumps back to the same step within the same run, the engine:\\n+\\n+- Finds the last result for that step in the current run.\\n+- Reads the `sessionId` stored in the result.\\n+- Calls the AI provider again using `executeReviewWithSessionReuse` with that session id.\\n+\\n+Simple example (no transport wiring, just CLI/tests):\\n+\\n+```yaml\\n+version: \\\"2.0\\\"\\n+\\n+steps:\\n+  seed:\\n+    type: script\\n+    content: |\\n+      return { text: \\\"hello from seed\\\" };\\n+\\n+  convo:\\n+    type: ai\\n+    depends_on: [seed]\\n+    reuse_ai_session: self\\n+    session_mode: append\\n+    ai:\\n+      provider: mock\\n+      model: mock\\n+      disableTools: true\\n+      allowedTools: []\\n+      system_prompt: \\\"You are a tiny echo assistant.\\\"\\n+    prompt: |\\n+      Seed message: {{ outputs['seed'].text }}\\n+\\n+      Past convo outputs in this run:\\n+      {% assign hist = outputs_history['convo'] | default: empty %}\\n+      {% if hist and hist.size > 0 %}\\n+      {% for h in hist %}\\n+      - Previous reply {{ forloop.index }}.\\n+      {% endfor %}\\n+      {% else %}\\n+      - No previous replies yet.\\n+      {% endif %}\\n+    on_success:\\n+      goto_js: |\\n+        // Example: re‚Äëenter this step up to 3 times in a single run\\n+        return attempt < 3 ? 'convo' : null;\\n+```\\n+\\n+The corresponding testable example lives at:\\n+\\n+- `examples/session-reuse-self.yaml`\\n+\\n+This keeps the configuration small but shows how to wire `reuse_ai_session: self` and `session_mode: append` without touching higher‚Äëlevel workflows like `tyk-assistant`.\\n ```\\n \\n **When to use each mode:**\\n\",\"status\":\"modified\"},{\"filename\":\"docs/recipes.md\",\"additions\":149,\"deletions\":3,\"changes\":152,\"patch\":\"diff --git a/docs/recipes.md b/docs/recipes.md\\nindex 1015bc6c..1089e9a5 100644\\n--- a/docs/recipes.md\\n+++ b/docs/recipes.md\\n@@ -321,8 +321,154 @@ Tip: When you define a JSON Schema, you generally do **not** need to tell the mo\\n - Avoid noisy fallbacks like `(outputs['x']?.kind ?? '') === 'status'` when `outputs['x']?.kind === 'status'` is equivalent.\\n - These conventions apply uniformly to any provider (`ai`, `command`, `script`, `github`, `http_client`, etc).\\n \\n+### Command step best practices\\n+\\n+When using `type: command` steps:\\n+\\n+**Avoid external tool dependencies** like `jq`, `yq`, `python`, etc.:\\n+- They may not be installed in all environments (GitHub Actions, Docker, CI)\\n+- Use `transform_js` to parse and transform output instead\\n+- Keep shell commands simple: `grep`, `sed`, `awk`, `sort`, `head` are universally available\\n+\\n+```yaml\\n+# Bad - requires jq\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | jq -R -s 'split(\\\"\\\\n\\\")'\\n+  parseJson: true\\n+\\n+# Good - use transform_js for parsing\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | sort -u\\n+  transform_js: |\\n+    const lines = (output || '').trim().split('\\\\n').filter(Boolean);\\n+    return { data: lines, count: lines.length };\\n+```\\n+\\n+**Prefer line-separated output** over JSON from shell:\\n+- Simple to parse with `transform_js`\\n+- No need for `parseJson: true`\\n+- More robust across different shells/environments\\n+\\n+**Use transform_js for structured output**:\\n+- The sandbox provides `output` (command stdout as string)\\n+- Return an object with the fields you need\\n+- Works consistently across all environments\\n+\\n+### Testing workflows with `--no-mocks`\\n+\\n+The `--no-mocks` flag runs your test cases with real providers instead of injecting mock responses. This is essential for:\\n+\\n+1. **Debugging integration issues** - See actual API responses and errors\\n+2. **Capturing realistic mock data** - Get real output to copy into your test cases\\n+3. **Validating credentials** - Verify environment variables are set correctly\\n+4. **Developing new workflows** - Build tests incrementally with real data\\n+\\n+#### Basic usage\\n+\\n+```bash\\n+# Run all test cases with real providers\\n+visor test --config my-workflow.yaml --no-mocks\\n+\\n+# Run a specific test case with real providers\\n+visor test --config my-workflow.yaml --no-mocks --only \\\"my-test-case\\\"\\n+```\\n+\\n+#### Suggested mocks output\\n+\\n+When running with `--no-mocks`, Visor captures each step's output and prints it as YAML you can copy directly into your test case:\\n+\\n+```\\n+üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)\\n+   Step outputs will be captured and printed as suggested mocks\\n+\\n+... test execution ...\\n+\\n+üìã Suggested mocks (copy to your test case):\\n+mocks:\\n+  extract-keys:\\n+    data:\\n+      - PROJ-123\\n+      - DEV-456\\n+    count: 2\\n+  fetch-issues:\\n+    data:\\n+      - key: PROJ-123\\n+        summary: Fix authentication bug\\n+        status: In Progress\\n+```\\n+\\n+Copy the YAML under `mocks:` into your test case's `mocks:` section.\\n+\\n+#### Workflow for building tests\\n+\\n+1. **Start with a minimal test case** (no mocks):\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+   ```\\n+\\n+2. **Run with `--no-mocks`** to capture real outputs:\\n+   ```bash\\n+   visor test --config workflow.yaml --no-mocks --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+3. **Copy the suggested mocks** into your test case:\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+         mocks:\\n+           extract-keys:\\n+             data: [\\\"PROJ-123\\\"]\\n+             count: 1\\n+           # ... rest of captured mocks\\n+   ```\\n+\\n+4. **Add assertions** based on the real data:\\n+   ```yaml\\n+         expect:\\n+           workflow_output:\\n+             - path: issue_count\\n+               equals: 1\\n+   ```\\n+\\n+5. **Run normally** to verify mocks work:\\n+   ```bash\\n+   visor test --config workflow.yaml --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+#### Debugging with `--no-mocks`\\n+\\n+When a test fails with mocks, use `--no-mocks` to see what's actually happening:\\n+\\n+```bash\\n+# See real API responses and errors\\n+visor test --config workflow.yaml --no-mocks --only \\\"failing-test\\\"\\n+\\n+# Common issues revealed:\\n+# - Missing or expired credentials\\n+# - API endpoint changes\\n+# - Unexpected response formats\\n+# - Network/timeout issues\\n+```\\n+\\n+The real error messages and responses help identify whether the issue is with your mocks or the actual integration.\\n+\\n ### More examples\\n \\n-- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags  \\n-- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration  \\n+- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags\\n+- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration\\n - `examples/` ‚Äì MCP, Jira, and advanced configs\\n\",\"status\":\"modified\"},{\"filename\":\"package-lock.json\",\"additions\":252,\"deletions\":9,\"changes\":261,\"patch\":\"diff --git a/package-lock.json b/package-lock.json\\nindex 5fb3ad89..e5b10d27 100644\\n--- a/package-lock.json\\n+++ b/package-lock.json\\n@@ -7,6 +7,7 @@\\n     \\\"\\\": {\\n       \\\"name\\\": \\\"@probelabs/visor\\\",\\n       \\\"version\\\": \\\"0.1.42\\\",\\n+      \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"MIT\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@actions/core\\\": \\\"^1.11.1\\\",\\n@@ -16,7 +17,7 @@\\n         \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n         \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n         \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n         \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n         \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -54,6 +55,7 @@\\n         \\\"husky\\\": \\\"^9.1.7\\\",\\n         \\\"jest\\\": \\\"^30.1.3\\\",\\n         \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+        \\\"patch-package\\\": \\\"^8.0.1\\\",\\n         \\\"prettier\\\": \\\"^3.6.2\\\",\\n         \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n         \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n@@ -5942,9 +5944,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/maid\\\": {\\n-      \\\"version\\\": \\\"0.0.21\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.21.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-H3EA2tDjWsAPgXZAwfHrNoEXnHmQFS9+JGDK/YpeiF3g3FxPgnWBgtgYPgtNuETQsEXxR1RzDgQVJL2JHWRRHw==\\\",\\n+      \\\"version\\\": \\\"0.0.22\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.22.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-FYcyyP4BlVrHQmBznXFPpMbvUqFI6wbecHUUe4S+yIeDvJ8W4bP0ndZ0cMoc2p7hekWYAG4RtlohQBr8e4TKOg==\\\",\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@types/dagre\\\": \\\"^0.7.53\\\",\\n@@ -5960,9 +5962,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/probe\\\": {\\n-      \\\"version\\\": \\\"0.6.0-rc169\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc169.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-/laV3+1TSjASru8gGSvGwyjQ7jalMkFuZETl35uv37J1Hb4eG95CbKGU5NihcCrLXR44q1lKJiQGHXcAav2mBA==\\\",\\n+      \\\"version\\\": \\\"0.6.0-rc176\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc176.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-w/pAvnz2/yXb4FQXZV2QazxuW02xsI/wRg36yxZoLSG0O/hIncjF4rquZe3O+emceI2Ks5zbrA7cgQEdpwT8vA==\\\",\\n       \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n@@ -5972,7 +5974,7 @@\\n         \\\"@ai-sdk/openai\\\": \\\"^2.0.10\\\",\\n         \\\"@anthropic-ai/claude-agent-sdk\\\": \\\"^0.1.46\\\",\\n         \\\"@modelcontextprotocol/sdk\\\": \\\"^1.0.0\\\",\\n-        \\\"@probelabs/maid\\\": \\\"^0.0.21\\\",\\n+        \\\"@probelabs/maid\\\": \\\"^0.0.22\\\",\\n         \\\"adm-zip\\\": \\\"^0.5.16\\\",\\n         \\\"ai\\\": \\\"^5.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -8192,6 +8194,13 @@\\n         \\\"node\\\": \\\">= 20\\\"\\n       }\\n     },\\n+    \\\"node_modules/@yarnpkg/lockfile\\\": {\\n+      \\\"version\\\": \\\"1.1.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@yarnpkg/lockfile/-/lockfile-1.1.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-GpSwvyXOcOOlV70vbnzjj4fW5xW/FdUF6nQEt1ENy7m4ZCczi1+/buVUPAqmGfqznsORNFzUMjctTIp8a9tuCQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"BSD-2-Clause\\\"\\n+    },\\n     \\\"node_modules/accepts\\\": {\\n       \\\"version\\\": \\\"1.3.8\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/accepts/-/accepts-1.3.8.tgz\\\",\\n@@ -9180,6 +9189,25 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/call-bind\\\": {\\n+      \\\"version\\\": \\\"1.0.8\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind/-/call-bind-1.0.8.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-oKlSFMcMwpUg2ednkhQ454wfWiU/ul3CkJe/PEHcTKuiX6RpbehUiFMXu13HalGZxfUwCQzZG747YXBn1im9ww==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind-apply-helpers\\\": \\\"^1.0.0\\\",\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"set-function-length\\\": \\\"^1.2.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/call-bind-apply-helpers\\\": {\\n       \\\"version\\\": \\\"1.0.2\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind-apply-helpers/-/call-bind-apply-helpers-1.0.2.tgz\\\",\\n@@ -10613,6 +10641,24 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/define-data-property\\\": {\\n+      \\\"version\\\": \\\"1.1.4\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/define-data-property/-/define-data-property-1.1.4.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-rBMvIzlpA8v6E+SJZoo++HAYqsLrkg7MSfIinMPFhmkorw7X+dOXVJQs+QT69zGkzMyfDnIMN2Wid1+NbL3T+A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/define-lazy-prop\\\": {\\n       \\\"version\\\": \\\"3.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/define-lazy-prop/-/define-lazy-prop-3.0.0.tgz\\\",\\n@@ -11794,6 +11840,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/find-yarn-workspace-root\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/find-yarn-workspace-root/-/find-yarn-workspace-root-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-1IMnbjt4KzsQfnhnzNd8wUEgXZ44IzZaZmnLYx7D5FZlaHt2gW20Cri8Q+E/t5tIj4+epTBub+2Zxu/vNILzqQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Apache-2.0\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"micromatch\\\": \\\"^4.0.2\\\"\\n+      }\\n+    },\\n     \\\"node_modules/fix-dts-default-cjs-exports\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/fix-dts-default-cjs-exports/-/fix-dts-default-cjs-exports-1.0.1.tgz\\\",\\n@@ -12267,6 +12323,19 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/has-property-descriptors\\\": {\\n+      \\\"version\\\": \\\"1.0.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/has-property-descriptors/-/has-property-descriptors-1.0.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-55JNKuIW+vq4Ke1BjOTjM2YctQIvCT7GFzHwmfZPGo5wnrgkid0YQtnAleFSqumZm4az3n2BS+erby5ipJdgrg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/has-symbols\\\": {\\n       \\\"version\\\": \\\"1.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/has-symbols/-/has-symbols-1.1.0.tgz\\\",\\n@@ -12784,6 +12853,13 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/isarray\\\": {\\n+      \\\"version\\\": \\\"2.0.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/isarray/-/isarray-2.0.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-xHjhDr3cNBK0BzdUJSPXZntQUx/mwMS5Rw4A7lPJ90XGAO6ISP/ePDNuo0vhqOZU+UD5JoodwCAAoZQd3FeAKw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\"\\n+    },\\n     \\\"node_modules/isexe\\\": {\\n       \\\"version\\\": \\\"2.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz\\\",\\n@@ -13594,6 +13670,26 @@\\n       \\\"integrity\\\": \\\"sha512-NM8/P9n3XjXhIZn1lLhkFaACTOURQXjWhV4BA/RnOv8xvgqtqpAX9IO4mRQxSx1Rlo4tqzeqb0sOlruaOy3dug==\\\",\\n       \\\"license\\\": \\\"MIT\\\"\\n     },\\n+    \\\"node_modules/json-stable-stringify\\\": {\\n+      \\\"version\\\": \\\"1.3.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify/-/json-stable-stringify-1.3.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-qtYiSSFlwot9XHtF9bD9c7rwKjr+RecWT//ZnPvSmEjpV5mmPOCN4j8UjY5hbjNkOwZ/jQv3J6R1/pL7RwgMsg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind\\\": \\\"^1.0.8\\\",\\n+        \\\"call-bound\\\": \\\"^1.0.4\\\",\\n+        \\\"isarray\\\": \\\"^2.0.5\\\",\\n+        \\\"jsonify\\\": \\\"^0.0.1\\\",\\n+        \\\"object-keys\\\": \\\"^1.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/json-stable-stringify-without-jsonify\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify-without-jsonify/-/json-stable-stringify-without-jsonify-1.0.1.tgz\\\",\\n@@ -13633,6 +13729,16 @@\\n         \\\"graceful-fs\\\": \\\"^4.1.6\\\"\\n       }\\n     },\\n+    \\\"node_modules/jsonify\\\": {\\n+      \\\"version\\\": \\\"0.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/jsonify/-/jsonify-0.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-2/Ki0GcmuqSrgFyelQq9M05y7PS0mEwuIzrf3f1fPqkVDVRvZrPZtVSMHxdgo8Aq0sxAOb/cr2aqqA3LeWHVPg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Public Domain\\\",\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/katex\\\": {\\n       \\\"version\\\": \\\"0.16.25\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/katex/-/katex-0.16.25.tgz\\\",\\n@@ -13676,6 +13782,16 @@\\n       \\\"integrity\\\": \\\"sha512-Ls993zuzfayK269Svk9hzpeGUKob/sIgZzyHYdjQoAdQetRKpOLj+k/QQQ/6Qi0Yz65mlROrfd+Ev+1+7dz9Kw==\\\",\\n       \\\"dev\\\": true\\n     },\\n+    \\\"node_modules/klaw-sync\\\": {\\n+      \\\"version\\\": \\\"6.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/klaw-sync/-/klaw-sync-6.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-nIeuVSzdCCs6TDPTqI8w1Yre34sSq7AkZ4B3sfOBbI2CgVSB4Du4aLQijFU2+lhAFCwt9+42Hel6lQNIv6AntQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"graceful-fs\\\": \\\"^4.1.11\\\"\\n+      }\\n+    },\\n     \\\"node_modules/kolorist\\\": {\\n       \\\"version\\\": \\\"1.8.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/kolorist/-/kolorist-1.8.0.tgz\\\",\\n@@ -14785,6 +14901,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n       }\\n     },\\n+    \\\"node_modules/object-keys\\\": {\\n+      \\\"version\\\": \\\"1.1.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/object-keys/-/object-keys-1.1.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NuAESUOUMrlIXOfHKzD6bpPu3tYt3xvjNdRIQ+FeT0lNb4K8WR70CaDxhuNguS2XG+GjkyMwOzsN5ZktImfhLA==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/ohash\\\": {\\n       \\\"version\\\": \\\"2.0.11\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/ohash/-/ohash-2.0.11.tgz\\\",\\n@@ -15022,6 +15148,95 @@\\n         \\\"node\\\": \\\">= 0.8\\\"\\n       }\\n     },\\n+    \\\"node_modules/patch-package\\\": {\\n+      \\\"version\\\": \\\"8.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/patch-package/-/patch-package-8.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-VsKRIA8f5uqHQ7NGhwIna6Bx6D9s/1iXlA1hthBVBEbkq+t4kXD0HHt+rJhf/Z+Ci0F/HCB2hvn0qLdLG+Qxlw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"@yarnpkg/lockfile\\\": \\\"^1.1.0\\\",\\n+        \\\"chalk\\\": \\\"^4.1.2\\\",\\n+        \\\"ci-info\\\": \\\"^3.7.0\\\",\\n+        \\\"cross-spawn\\\": \\\"^7.0.3\\\",\\n+        \\\"find-yarn-workspace-root\\\": \\\"^2.0.0\\\",\\n+        \\\"fs-extra\\\": \\\"^10.0.0\\\",\\n+        \\\"json-stable-stringify\\\": \\\"^1.0.2\\\",\\n+        \\\"klaw-sync\\\": \\\"^6.0.0\\\",\\n+        \\\"minimist\\\": \\\"^1.2.6\\\",\\n+        \\\"open\\\": \\\"^7.4.2\\\",\\n+        \\\"semver\\\": \\\"^7.5.3\\\",\\n+        \\\"slash\\\": \\\"^2.0.0\\\",\\n+        \\\"tmp\\\": \\\"^0.2.4\\\",\\n+        \\\"yaml\\\": \\\"^2.2.2\\\"\\n+      },\\n+      \\\"bin\\\": {\\n+        \\\"patch-package\\\": \\\"index.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14\\\",\\n+        \\\"npm\\\": \\\">5\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/ci-info\\\": {\\n+      \\\"version\\\": \\\"3.9.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/ci-info/-/ci-info-3.9.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NIxF55hv4nSqQswkAeiOi1r83xy8JldOFDTWiug55KBu9Jnblncd2U6ViHmYgHf01TPZS77NJBhBMKdWj9HQMQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"funding\\\": [\\n+        {\\n+          \\\"type\\\": \\\"github\\\",\\n+          \\\"url\\\": \\\"https://github.com/sponsors/sibiraj-s\\\"\\n+        }\\n+      ],\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/is-docker\\\": {\\n+      \\\"version\\\": \\\"2.2.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/is-docker/-/is-docker-2.2.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-F+i2BKsFrH66iaUFc0woD8sLy8getkwTwtOBjvs56Cx4CgJDeKQeqfz8wAYiSb8JOprWhHH5p77PbmYCvvUuXQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"bin\\\": {\\n+        \\\"is-docker\\\": \\\"cli.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/open\\\": {\\n+      \\\"version\\\": \\\"7.4.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/open/-/open-7.4.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-MVHddDVweXZF3awtlAS+6pgKLlm/JgxZ90+/NBurBoQctVOOB/zDdVjcyPzQ+0laDGbsWgrRkflI65sQeOgT9Q==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"is-docker\\\": \\\"^2.0.0\\\",\\n+        \\\"is-wsl\\\": \\\"^2.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/slash\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/slash/-/slash-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-ZYKh3Wh2z1PpEXWr0MpSBZ0V6mZHAQfYevttO11c51CaWjGTaadiKZ+wVt1PbMlDV5qhMFslpZCemhwOK7C89A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=6\\\"\\n+      }\\n+    },\\n     \\\"node_modules/path-data-parser\\\": {\\n       \\\"version\\\": \\\"0.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/path-data-parser/-/path-data-parser-0.1.0.tgz\\\",\\n@@ -16473,6 +16688,24 @@\\n         \\\"node\\\": \\\">= 0.8.0\\\"\\n       }\\n     },\\n+    \\\"node_modules/set-function-length\\\": {\\n+      \\\"version\\\": \\\"1.2.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/set-function-length/-/set-function-length-1.2.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-pgRc4hJ4/sNjWCSS9AmnS40x3bNMDTknHgL5UaMBTMyJnU90EgWh1Rz+MC9eFu4BuN/UwZjKQuY/1v3rM7HMfg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"define-data-property\\\": \\\"^1.1.4\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"function-bind\\\": \\\"^1.1.2\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\",\\n+        \\\"has-property-descriptors\\\": \\\"^1.0.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/setprototypeof\\\": {\\n       \\\"version\\\": \\\"1.2.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/setprototypeof/-/setprototypeof-1.2.0.tgz\\\",\\n@@ -17384,6 +17617,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/tmp\\\": {\\n+      \\\"version\\\": \\\"0.2.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/tmp/-/tmp-0.2.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-voyz6MApa1rQGUxT3E+BK7/ROe8itEx7vD8/HEvt4xwXucvQ5G5oeEiHkmHZJuBO21RpOf+YYm9MOivj709jow==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14.14\\\"\\n+      }\\n+    },\\n     \\\"node_modules/tmpl\\\": {\\n       \\\"version\\\": \\\"1.0.5\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/tmpl/-/tmpl-1.0.5.tgz\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"package.json\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/package.json b/package.json\\nindex f9ccbb6c..302d728f 100644\\n--- a/package.json\\n+++ b/package.json\\n@@ -49,6 +49,7 @@\\n     \\\"test:with-build\\\": \\\"npm run build:cli && jest\\\",\\n     \\\"test:yaml\\\": \\\"node dist/index.js test --progress compact\\\",\\n     \\\"prepare\\\": \\\"husky\\\",\\n+    \\\"postinstall\\\": \\\"patch-package\\\",\\n     \\\"pre-commit\\\": \\\"lint-staged\\\",\\n     \\\"deploy:site\\\": \\\"cd site && npx wrangler pages deploy . --project-name=visor-site --commit-dirty=true\\\",\\n     \\\"deploy:worker\\\": \\\"npx wrangler deploy\\\",\\n@@ -96,7 +97,7 @@\\n     \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n     \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n     \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n     \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n     \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n     \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -143,6 +144,7 @@\\n     \\\"husky\\\": \\\"^9.1.7\\\",\\n     \\\"jest\\\": \\\"^30.1.3\\\",\\n     \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+    \\\"patch-package\\\": \\\"^8.0.1\\\",\\n     \\\"prettier\\\": \\\"^3.6.2\\\",\\n     \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n     \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"src/ai-review-service.ts\",\"additions\":101,\"deletions\":151,\"changes\":252,\"patch\":\"diff --git a/src/ai-review-service.ts b/src/ai-review-service.ts\\nindex 6b9c5b2e..f18accea 100644\\n--- a/src/ai-review-service.ts\\n+++ b/src/ai-review-service.ts\\n@@ -65,6 +65,13 @@ export interface AIReviewConfig {\\n   allowBash?: boolean;\\n   // Advanced bash command execution configuration\\n   bashConfig?: import('./types/config').BashConfig;\\n+  // Optional workspace root and allowed folders for ProbeAgent.\\n+  // When provided, these are forwarded to ProbeAgent so tools like search/query\\n+  // operate inside the isolated workspace/projects instead of the Visor repo root.\\n+  path?: string;\\n+  allowedFolders?: string[];\\n+  // Completion prompt for post-completion validation/review (runs after attempt_completion)\\n+  completionPrompt?: string;\\n }\\n \\n export interface AIDebugInfo {\\n@@ -278,7 +285,7 @@ export class AIReviewService {\\n     try {\\n       const call = this.callProbeAgent(prompt, schema, debugInfo, checkName, sessionId);\\n       const timeoutMs = Math.max(0, this.config.timeout || 0);\\n-      const { response, effectiveSchema } =\\n+      const { response, effectiveSchema, sessionId: usedSessionId } =\\n         timeoutMs > 0 ? await this.withTimeout(call, timeoutMs, 'AI review') : await call;\\n       const processingTime = Date.now() - startTime;\\n \\n@@ -290,6 +297,11 @@ export class AIReviewService {\\n \\n       const result = this.parseAIResponse(response, debugInfo, effectiveSchema);\\n \\n+      // Expose the session ID used for this call so the engine can reuse it later\\n+      try {\\n+        (result as any).sessionId = usedSessionId;\\n+      } catch {}\\n+\\n       if (debugInfo) {\\n         result.debug = debugInfo;\\n       }\\n@@ -651,17 +663,34 @@ ${prContext}${slackContextXml}\\n     const prContextInfo = prInfo as PRInfo & {\\n       isPRContext?: boolean;\\n       includeCodeContext?: boolean;\\n+      slackConversation?: unknown;\\n     };\\n     const isIssue = prContextInfo.isIssue === true;\\n \\n     // Check if we should include code context (diffs)\\n     const isPRContext = prContextInfo.isPRContext === true;\\n-    // In PR context, always include diffs. Otherwise check the flag.\\n-    const includeCodeContext = isPRContext || prContextInfo.includeCodeContext !== false;\\n+    const isSlackMode = prContextInfo.slackConversation !== undefined;\\n+\\n+    // Determine whether to include code context:\\n+    // - In explicit PR context (GitHub PR events), always include diffs\\n+    // - In Slack mode, default to NO code context unless explicitly requested\\n+    // - Otherwise, include code context unless explicitly disabled\\n+    let includeCodeContext: boolean;\\n+    if (isPRContext) {\\n+      includeCodeContext = true;\\n+    } else if (isSlackMode) {\\n+      // In Slack mode, only include code context if explicitly set to true\\n+      includeCodeContext = prContextInfo.includeCodeContext === true;\\n+    } else {\\n+      // Default: include unless explicitly disabled\\n+      includeCodeContext = prContextInfo.includeCodeContext !== false;\\n+    }\\n \\n     // Log the decision for transparency (debug level)\\n     if (isPRContext) {\\n       log('üîç Including full code diffs in AI context (PR mode)');\\n+    } else if (isSlackMode && !includeCodeContext) {\\n+      log('üí¨ Slack mode: excluding code diffs (use includeCodeContext: true to enable)');\\n     } else if (!includeCodeContext) {\\n       log('üìä Including only file summary in AI context (no diffs)');\\n     } else {\\n@@ -1537,7 +1566,15 @@ ${'='.repeat(60)}\\n     debugInfo?: AIDebugInfo,\\n     _checkName?: string,\\n     providedSessionId?: string\\n-  ): Promise<{ response: string; effectiveSchema?: string }> {\\n+  ): Promise<{ response: string; effectiveSchema?: string; sessionId: string }> {\\n+    // Derive a stable session ID for this call so the engine can reuse it later\\n+    const sessionId =\\n+      providedSessionId ||\\n+      (() => {\\n+        const timestamp = new Date().toISOString();\\n+        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n+      })();\\n+\\n     // Handle mock model/provider\\n     if (this.config.model === 'mock' || this.config.provider === 'mock') {\\n       const inJest = !!process.env.JEST_WORKER_ID;\\n@@ -1545,19 +1582,17 @@ ${'='.repeat(60)}\\n       if (!inJest) {\\n         // Fast path for CLI/integration: synthesize a mock response without invoking ProbeAgent\\n         const response = await this.generateMockResponse(prompt, _checkName, schema);\\n-        return { response, effectiveSchema: typeof schema === 'object' ? 'custom' : schema };\\n+        return {\\n+          response,\\n+          effectiveSchema: typeof schema === 'object' ? 'custom' : schema,\\n+          sessionId,\\n+        };\\n       }\\n       // In unit tests, still invoke ProbeAgent so tests can assert on options (schema) passed in\\n       // Fall through to normal flow below\\n     }\\n \\n     // Create ProbeAgent instance with proper options\\n-    const sessionId =\\n-      providedSessionId ||\\n-      (() => {\\n-        const timestamp = new Date().toISOString();\\n-        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n-      })();\\n \\n     log('ü§ñ Creating ProbeAgent for AI review...');\\n     log(`üÜî Session ID: ${sessionId}`);\\n@@ -1674,6 +1709,35 @@ ${'='.repeat(60)}\\n         (options as any).bashConfig = this.config.bashConfig;\\n       }\\n \\n+      // Pass completion prompt for post-completion validation/review\\n+      if (this.config.completionPrompt !== undefined) {\\n+        (options as any).completionPrompt = this.config.completionPrompt;\\n+      }\\n+\\n+      // Propagate workspace / allowed folders to ProbeAgent so that tools\\n+      // operate inside the isolated workspace and project checkouts instead\\n+      // of the Visor repository root.\\n+      try {\\n+        const cfgAny: any = this.config as any;\\n+        const allowedFolders = cfgAny.allowedFolders as string[] | undefined;\\n+        const workspacePath =\\n+          cfgAny.workspacePath || cfgAny.path || (Array.isArray(allowedFolders) && allowedFolders[0]);\\n+        if (Array.isArray(allowedFolders) && allowedFolders.length > 0) {\\n+          (options as any).allowedFolders = allowedFolders;\\n+          if (!options.path && workspacePath) {\\n+            (options as any).path = workspacePath;\\n+          }\\n+          log(`üóÇÔ∏è ProbeAgent workspace config:`);\\n+          log(`   path (cwd): ${(options as any).path}`);\\n+          log(`   allowedFolders[0]: ${allowedFolders[0]}`);\\n+        } else if (workspacePath) {\\n+          (options as any).path = workspacePath;\\n+          log(`üóÇÔ∏è ProbeAgent path: ${workspacePath} (no allowedFolders)`);\\n+        }\\n+      } catch {\\n+        // Best-effort only; fall back to ProbeAgent defaults on error.\\n+      }\\n+\\n       // Add provider-specific options if configured\\n       if (this.config.provider) {\\n         // Map claude-code to anthropic for ProbeAgent compatibility\\n@@ -2037,7 +2101,7 @@ ${'='.repeat(60)}\\n         log(`üîß Debug: Registered AI session for potential reuse: ${sessionId}`);\\n       }\\n \\n-      return { response, effectiveSchema };\\n+      return { response, effectiveSchema, sessionId };\\n     } catch (error) {\\n       console.error('‚ùå ProbeAgent failed:', error);\\n       throw new Error(\\n@@ -2191,13 +2255,23 @@ ${'='.repeat(60)}\\n         // For other schemas (code-review, etc.), extract and parse JSON with boundary detection\\n         log('üîç Extracting JSON from AI response...');\\n \\n-        // Try direct parsing first - if AI returned pure JSON\\n+        // Sanitize response: strip BOM, zero-width chars, and other invisible characters\\n+        // that can cause JSON parsing to fail even when the text looks valid\\n+        const sanitizedResponse = response\\n+          .replace(/^\\\\uFEFF/, '') // BOM\\n+          .replace(/[\\\\u200B-\\\\u200D\\\\uFEFF\\\\u00A0]/g, '') // Zero-width chars, NBSP\\n+          .replace(/[\\\\x00-\\\\x08\\\\x0B\\\\x0C\\\\x0E-\\\\x1F\\\\x7F]/g, '') // Control chars (except \\\\t \\\\n \\\\r)\\n+          .trim();\\n+\\n+        // Try direct JSON parsing - no bracket-matching extraction\\n+        // JSON validation is offloaded to Probe agent when schema is provided\\n         try {\\n-          reviewData = JSON.parse(response.trim());\\n+          reviewData = JSON.parse(sanitizedResponse);\\n           log('‚úÖ Successfully parsed direct JSON response');\\n           if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-        } catch {\\n-          log('üîç Direct parsing failed, trying to extract JSON from response...');\\n+        } catch (parseErr) {\\n+          const errMsg = parseErr instanceof Error ? parseErr.message : String(parseErr);\\n+          log(`üîç Direct JSON parsing failed: ${errMsg}`);\\n \\n           // If the response starts with \\\"I cannot\\\" or similar, it's likely a refusal\\n           if (\\n@@ -2210,66 +2284,16 @@ ${'='.repeat(60)}\\n             };\\n           }\\n \\n-          // Try to extract JSON using improved method with proper bracket matching\\n-          const jsonString = this.extractJsonFromResponse(response);\\n-\\n-          if (jsonString) {\\n-            try {\\n-              reviewData = JSON.parse(jsonString);\\n-              log('‚úÖ Successfully parsed extracted JSON');\\n-              if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-            } catch {\\n-              log('üîß Extracted JSON parsing failed, falling back to plain text handling...');\\n-\\n-              // Check if response is plain text and doesn't contain structured data\\n-              if (!response.includes('{') && !response.includes('}')) {\\n-                log('üîß Plain text response detected, creating structured fallback...');\\n-\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              } else {\\n-                // Fallback: treat the entire response as an issue\\n-                log('üîß Creating fallback response from non-JSON content...');\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              }\\n-            }\\n-          } else {\\n-            // No JSON found at all - treat as plain text response\\n-            log('üîß No JSON found in response, treating as plain text...');\\n-            reviewData = {\\n-              issues: [\\n-                {\\n-                  file: 'AI_RESPONSE',\\n-                  line: 1,\\n-                  ruleId: 'ai/raw_response',\\n-                  message: response,\\n-                  severity: 'info',\\n-                  category: 'documentation',\\n-                },\\n-              ],\\n-            };\\n-          }\\n+          // Not valid JSON - treat entire response as text output\\n+          // This allows Probe (or other AI providers) to handle JSON validation\\n+          // and avoids false positives from bracket-matching (e.g., mermaid diagrams)\\n+          log('üîß Treating response as plain text (no JSON extraction)');\\n+          const trimmed = response.trim();\\n+          return {\\n+            issues: [],\\n+            output: { text: trimmed },\\n+            debug: debugInfo,\\n+          };\\n         }\\n       }\\n \\n@@ -2457,80 +2481,6 @@ ${'='.repeat(60)}\\n     }\\n   }\\n \\n-  /**\\n-   * Extract JSON from a response that might contain surrounding text\\n-   * Uses proper bracket matching to find valid JSON objects or arrays\\n-   */\\n-  private extractJsonFromResponse(response: string): string | null {\\n-    const text = response.trim();\\n-\\n-    // Try to find JSON objects first (higher priority)\\n-    let bestJson = this.findJsonWithBracketMatching(text, '{', '}');\\n-\\n-    // If no object found, try arrays\\n-    if (!bestJson) {\\n-      bestJson = this.findJsonWithBracketMatching(text, '[', ']');\\n-    }\\n-\\n-    return bestJson;\\n-  }\\n-\\n-  /**\\n-   * Find JSON with proper bracket matching to avoid false positives\\n-   */\\n-  private findJsonWithBracketMatching(\\n-    text: string,\\n-    openChar: string,\\n-    closeChar: string\\n-  ): string | null {\\n-    const firstIndex = text.indexOf(openChar);\\n-    if (firstIndex === -1) return null;\\n-\\n-    let depth = 0;\\n-    let inString = false;\\n-    let escaping = false;\\n-\\n-    for (let i = firstIndex; i < text.length; i++) {\\n-      const char = text[i];\\n-\\n-      if (escaping) {\\n-        escaping = false;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\\\\\\' && inString) {\\n-        escaping = true;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\"' && !escaping) {\\n-        inString = !inString;\\n-        continue;\\n-      }\\n-\\n-      if (!inString) {\\n-        if (char === openChar) {\\n-          depth++;\\n-        } else if (char === closeChar) {\\n-          depth--;\\n-          if (depth === 0) {\\n-            // Found matching closing bracket\\n-            const candidate = text.substring(firstIndex, i + 1);\\n-            try {\\n-              JSON.parse(candidate); // Validate it's actually valid JSON\\n-              return candidate;\\n-            } catch {\\n-              // This wasn't valid JSON, keep looking\\n-              break;\\n-            }\\n-          }\\n-        }\\n-      }\\n-    }\\n-\\n-    return null;\\n-  }\\n-\\n   /**\\n    * Generate mock response for testing\\n    */\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli-main.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/src/cli-main.ts b/src/cli-main.ts\\nindex 7dbef501..e671fbd8 100644\\n--- a/src/cli-main.ts\\n+++ b/src/cli-main.ts\\n@@ -158,6 +158,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n   }\\n   const only = getArg('--only');\\n   const bail = hasFlag('--bail');\\n+  const noMocks = hasFlag('--no-mocks');\\n   const listOnly = hasFlag('--list');\\n   const validateOnly = hasFlag('--validate');\\n   const progress = (getArg('--progress') as 'compact' | 'detailed' | undefined) || 'compact';\\n@@ -224,6 +225,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       const agg = await runSuites(multiFiles, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallelSuites: maxParallelSuites || Math.max(1, require('os').cpus()?.length || 2),\\n         maxParallel,\\n         promptMaxChars,\\n@@ -340,6 +342,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       runRes = await runner.runCases(tpath, suite, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallel,\\n         promptMaxChars,\\n         engineMode: 'state-machine',\\n@@ -609,6 +612,9 @@ export async function main(): Promise<void> {\\n     // Set environment variables early for proper logging in all modules\\n     process.env.VISOR_OUTPUT_FORMAT = options.output;\\n     process.env.VISOR_DEBUG = options.debug ? 'true' : 'false';\\n+    if (options.keepWorkspace) {\\n+      process.env.VISOR_KEEP_WORKSPACE = 'true';\\n+    }\\n     // Configure centralized logger\\n     configureLoggerFromCli({\\n       output: options.output,\\n@@ -716,6 +722,31 @@ export async function main(): Promise<void> {\\n       const threads = slackAny.threads || 'any';\\n       const allow = Array.isArray(slackAny.channel_allowlist) ? slackAny.channel_allowlist : [];\\n       const appToken = slackAny.app_token || process.env.SLACK_APP_TOKEN;\\n+\\n+      // Initialize telemetry for Slack mode (normally done later for CLI runs).\\n+      if ((config as any)?.telemetry) {\\n+        const t = (config as any).telemetry as {\\n+          enabled?: boolean;\\n+          sink?: 'otlp' | 'file' | 'console';\\n+          file?: { dir?: string; ndjson?: boolean };\\n+          tracing?: { auto_instrumentations?: boolean; trace_report?: { enabled?: boolean } };\\n+        };\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true' || !!t?.enabled,\\n+          sink:\\n+            (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || t?.sink || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR || t?.file?.dir, ndjson: !!t?.file?.ndjson },\\n+          autoInstrument: !!t?.tracing?.auto_instrumentations,\\n+          traceReport: !!t?.tracing?.trace_report?.enabled,\\n+        });\\n+      } else {\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true',\\n+          sink: (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR },\\n+        });\\n+      }\\n+\\n       const runner = new SlackSocketRunner(engine, config, {\\n         appToken,\\n         endpoint,\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli.ts\",\"additions\":5,\"deletions\":1,\"changes\":6,\"patch\":\"diff --git a/src/cli.ts b/src/cli.ts\\nindex 134429b2..b1c46213 100644\\n--- a/src/cli.ts\\n+++ b/src/cli.ts\\n@@ -76,6 +76,7 @@ export class CLI {\\n         parseInt(value, 10)\\n       )\\n       .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText())\\n       .exitOverride(); // Prevent automatic process.exit for better error handling\\n \\n@@ -154,6 +155,7 @@ export class CLI {\\n           parseInt(value, 10)\\n         )\\n         .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+        .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n         .allowUnknownOption(false)\\n         .allowExcessArguments(false) // Don't allow positional arguments\\n         .addHelpText('after', this.getExamplesText())\\n@@ -224,6 +226,7 @@ export class CLI {\\n         message: options.message,\\n         githubV2: false,\\n         slack: Boolean(options.slack),\\n+        keepWorkspace: Boolean(options.keepWorkspace),\\n       };\\n     } catch (error: unknown) {\\n       // Handle commander.js exit overrides for help/version ONLY\\n@@ -348,6 +351,7 @@ export class CLI {\\n       .option('--debug-port <port>', 'Port for debug server (default: 3456)', value =>\\n         parseInt(value, 10)\\n       )\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText());\\n \\n     // Get the basic help and append examples manually if addHelpText doesn't work\\n\",\"status\":\"modified\"},{\"filename\":\"src/config.ts\",\"additions\":67,\"deletions\":17,\"changes\":84,\"patch\":\"diff --git a/src/config.ts b/src/config.ts\\nindex 407efa2d..02abe9df 100644\\n--- a/src/config.ts\\n+++ b/src/config.ts\\n@@ -20,6 +20,7 @@ import { ConfigLoader, ConfigLoaderOptions } from './utils/config-loader';\\n import { ConfigMerger } from './utils/config-merger';\\n import Ajv from 'ajv';\\n import addFormats from 'ajv-formats';\\n+import { validateJsSyntax } from './utils/sandbox';\\n \\n /**\\n  * Valid event triggers for checks\\n@@ -477,16 +478,29 @@ export class ConfigManager {\\n \\n     logger.info(`Registered workflow '${workflowId}' for standalone execution`);\\n \\n-    // Create a COMPLETELY NEW visor config with ONLY the test checks\\n-    // This prevents any workflow fields from leaking into the config\\n-    const visorConfig: Partial<VisorConfig> = {\\n+    // For standalone workflow testing, use the workflow's steps as checks\\n+    // This allows the test runner to execute the workflow steps directly\\n+    const workflowSteps = workflowData.steps || {};\\n+\\n+    // Create a config that includes the workflow steps as checks,\\n+    // plus the tests section and workflow metadata for output computation\\n+    const visorConfig: Partial<VisorConfig> & { tests?: any; outputs?: any; inputs?: any } = {\\n       version: '1.0',\\n-      steps: tests,\\n-      checks: tests, // Backward compatibility\\n+      steps: workflowSteps,\\n+      checks: workflowSteps,\\n+      tests: tests, // Preserve test harness config (may be empty if stripped by test runner)\\n     };\\n \\n-    logger.debug(`Standalone workflow config has ${Object.keys(tests).length} test checks`);\\n-    logger.debug(`Test check names: ${Object.keys(tests).join(', ')}`);\\n+    // Preserve workflow metadata for output computation during tests\\n+    if (workflowData.outputs) {\\n+      visorConfig.outputs = workflowData.outputs;\\n+    }\\n+    if (workflowData.inputs) {\\n+      visorConfig.inputs = workflowData.inputs;\\n+    }\\n+\\n+    logger.debug(`Standalone workflow config has ${Object.keys(workflowSteps).length} workflow steps as checks`);\\n+    logger.debug(`Workflow step names: ${Object.keys(workflowSteps).join(', ')}`);\\n     logger.debug(`Config keys after conversion: ${Object.keys(visorConfig).join(', ')}`);\\n \\n     return visorConfig;\\n@@ -982,26 +996,29 @@ export class ConfigManager {\\n \\n     // Validate reuse_ai_session configuration\\n     if (checkConfig.reuse_ai_session !== undefined) {\\n-      const isString = typeof checkConfig.reuse_ai_session === 'string';\\n-      const isBoolean = typeof checkConfig.reuse_ai_session === 'boolean';\\n+      const reuseValue = checkConfig.reuse_ai_session as unknown;\\n+      const isString = typeof reuseValue === 'string';\\n+      const isBoolean = typeof reuseValue === 'boolean';\\n+      const isSelf = reuseValue === 'self';\\n \\n       if (!isString && !isBoolean) {\\n         errors.push({\\n           field: `checks.${checkName}.reuse_ai_session`,\\n           message: `Invalid reuse_ai_session value for \\\"${checkName}\\\": must be string (check name) or boolean`,\\n-          value: checkConfig.reuse_ai_session,\\n+          value: reuseValue,\\n         });\\n-      } else if (isString) {\\n-        // When reuse_ai_session is a string, it must refer to a valid check\\n-        const targetCheckName = checkConfig.reuse_ai_session as string;\\n+      } else if (isString && !isSelf) {\\n+        // When reuse_ai_session is a string (other than the special 'self' value),\\n+        // it must refer to a valid check name\\n+        const targetCheckName = reuseValue as string;\\n         if (!config?.checks || !config.checks[targetCheckName]) {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" references non-existent check \\\"${targetCheckName}\\\" for session reuse`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n-      } else if (checkConfig.reuse_ai_session === true) {\\n+      } else if (reuseValue === true) {\\n         // When reuse_ai_session is true, depends_on must be specified and non-empty\\n         if (\\n           !checkConfig.depends_on ||\\n@@ -1011,7 +1028,7 @@ export class ConfigManager {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" has reuse_ai_session=true but missing or empty depends_on. Session reuse requires dependency on another check.`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n       }\\n@@ -1076,6 +1093,39 @@ export class ConfigManager {\\n         });\\n       }\\n     }\\n+\\n+    // Validate JavaScript syntax in transform_js and script content\\n+    try {\\n+      // Validate transform_js if present\\n+      const transformJs = (checkConfig as any).transform_js;\\n+      if (typeof transformJs === 'string' && transformJs.trim().length > 0) {\\n+        const result = validateJsSyntax(transformJs);\\n+        if (!result.valid) {\\n+          errors.push({\\n+            field: `checks.${checkName}.transform_js`,\\n+            message: `JavaScript syntax error in \\\"${checkName}\\\" transform_js: ${result.error}`,\\n+            value: transformJs.slice(0, 100) + (transformJs.length > 100 ? '...' : ''),\\n+          });\\n+        }\\n+      }\\n+\\n+      // Validate script content if type is 'script'\\n+      if (checkConfig.type === 'script') {\\n+        const content = (checkConfig as any).content;\\n+        if (typeof content === 'string' && content.trim().length > 0) {\\n+          const result = validateJsSyntax(content);\\n+          if (!result.valid) {\\n+            errors.push({\\n+              field: `checks.${checkName}.content`,\\n+              message: `JavaScript syntax error in \\\"${checkName}\\\" script: ${result.error}`,\\n+              value: content.slice(0, 100) + (content.length > 100 ? '...' : ''),\\n+            });\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Syntax validation is best-effort; don't fail the whole config on validation errors\\n+    }\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/failure-condition-evaluator.ts\",\"additions\":25,\"deletions\":9,\"changes\":34,\"patch\":\"diff --git a/src/failure-condition-evaluator.ts b/src/failure-condition-evaluator.ts\\nindex 3b54a015..5800df0a 100644\\n--- a/src/failure-condition-evaluator.ts\\n+++ b/src/failure-condition-evaluator.ts\\n@@ -137,6 +137,8 @@ export class FailureConditionEvaluator {\\n       previousResults?: Map<string, ReviewSummary>;\\n       authorAssociation?: string;\\n       workflowInputs?: Record<string, unknown>;\\n+      /** Current step's output for guarantee evaluation */\\n+      output?: unknown;\\n     }\\n   ): Promise<boolean> {\\n     // Build context for if evaluation\\n@@ -177,10 +179,10 @@ export class FailureConditionEvaluator {\\n       // Workflow inputs (for workflows)\\n       inputs: contextData?.workflowInputs || {},\\n \\n-      // Required output property (empty for if conditions)\\n-      output: {\\n-        issues: [],\\n-      },\\n+      // Output property: use provided output for guarantee evaluation, or empty for if conditions\\n+      output: contextData?.output !== undefined && contextData.output !== null && typeof contextData.output === 'object'\\n+        ? (contextData.output as Record<string, unknown>)\\n+        : { issues: [] },\\n       // Author association (used by permission helpers)\\n       authorAssociation: contextData?.authorAssociation,\\n \\n@@ -213,10 +215,10 @@ export class FailureConditionEvaluator {\\n               m && typeof m.get === 'function' ? m.get('all_valid', 'fact-validation') : undefined;\\n             memStr = ` mem.fact-validation.all_valid=${String(v)}`;\\n           } catch {}\\n+          // Debug if-eval output (only when VISOR_DEBUG enabled)\\n+          const outputKeys = Object.keys(context.outputs || {});\\n           console.error(\\n-            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" env.ENABLE_FACT_VALIDATION=${String(\\n-              (envMap as any).ENABLE_FACT_VALIDATION\\n-            )} event=${context.event?.event_name} result=${String(res)}${memStr}`\\n+            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" result=${String(res)} outputKeys=[${outputKeys.join(',')}]`\\n           );\\n         }\\n       } catch {}\\n@@ -480,7 +482,18 @@ export class FailureConditionEvaluator {\\n \\n       // Extract context variables\\n       const output = context.output || {};\\n-      const issues = output.issues || [];\\n+      // Ensure issues is an array - it might be a JSON string from workflow outputs\\n+      let issues = output.issues || [];\\n+      if (typeof issues === 'string') {\\n+        try {\\n+          issues = JSON.parse(issues);\\n+        } catch {\\n+          issues = [];\\n+        }\\n+      }\\n+      if (!Array.isArray(issues)) {\\n+        issues = [];\\n+      }\\n \\n       // Backward compatibility: provide metadata for transition period\\n       // TODO: Remove after all configurations are updated\\n@@ -518,6 +531,7 @@ export class FailureConditionEvaluator {\\n       const event = context.event || 'manual';\\n       const env = context.env || {};\\n       const outputs = context.outputs || {};\\n+      const inputs = context.inputs || {};\\n       const debugData = context.debug || null;\\n \\n       // Get memory store and create accessor for fail_if expressions\\n@@ -555,6 +569,7 @@ export class FailureConditionEvaluator {\\n         filesCount,\\n         event,\\n         env,\\n+        inputs,\\n         // Helper functions\\n         contains,\\n         startsWith,\\n@@ -621,6 +636,7 @@ export class FailureConditionEvaluator {\\n             filesCount,\\n             event,\\n             env,\\n+            inputs,\\n             // Helpers\\n             contains,\\n             startsWith,\\n\",\"status\":\"modified\"},{\"filename\":\"src/frontends/slack-frontend.ts\",\"additions\":107,\"deletions\":6,\"changes\":113,\"patch\":\"diff --git a/src/frontends/slack-frontend.ts b/src/frontends/slack-frontend.ts\\nindex 97161a7d..41735aa2 100644\\n--- a/src/frontends/slack-frontend.ts\\n+++ b/src/frontends/slack-frontend.ts\\n@@ -1,6 +1,32 @@\\n+/**\\n+ * Slack Frontend for Visor workflows.\\n+ *\\n+ * Features:\\n+ * - Posts AI replies to Slack threads\\n+ * - Converts Markdown to Slack mrkdwn format\\n+ * - Renders mermaid diagrams to PNG and uploads as images\\n+ * - Manages üëÄ/üëç reactions for acknowledgement\\n+ * - Handles human input prompts via prompt-state\\n+ *\\n+ * Mermaid Diagram Rendering:\\n+ * - Detects ```mermaid code blocks in AI responses\\n+ * - Renders to PNG using @mermaid-js/mermaid-cli (mmdc)\\n+ * - Uploads rendered images to Slack thread\\n+ * - Replaces mermaid blocks with \\\"_(See diagram above)_\\\" placeholder\\n+ *\\n+ * Requirements for mermaid rendering:\\n+ * - Node.js and npx in PATH\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ * - On Linux: apt-get install chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ */\\n import type { Frontend, FrontendContext } from './host';\\n import { SlackClient } from '../slack/client';\\n-import { formatSlackText } from '../slack/markdown';\\n+import {\\n+  formatSlackText,\\n+  extractMermaidDiagrams,\\n+  renderMermaidToPng,\\n+  replaceMermaidBlocks,\\n+} from '../slack/markdown';\\n \\n type SlackFrontendConfig = {\\n   defaultChannel?: string;\\n@@ -284,6 +310,9 @@ export class SlackFrontend implements Frontend {\\n       const isLogChat = providerType === 'log' && checkCfg.group === 'chat';\\n       if (!isAi && !isLogChat) return;\\n \\n+      // Skip internal steps - they're intermediate processing and shouldn't post to Slack\\n+      if (checkCfg.criticality === 'internal') return;\\n+\\n       // For AI checks, only post when using simple/unstructured schemas (or none).\\n       if (isAi) {\\n         const schema = checkCfg.schema;\\n@@ -316,6 +345,13 @@ export class SlackFrontend implements Frontend {\\n         ) {\\n           text = (result as any).content.trim();\\n         }\\n+      } else if (isLogChat && typeof (result as any)?.logOutput === 'string') {\\n+        // For log-based chat checks, render the formatted log output as the\\n+        // Slack message when no structured text field is present.\\n+        const raw = (result as any).logOutput;\\n+        if (raw.trim().length > 0) {\\n+          text = raw.trim();\\n+        }\\n       } else if (isAi && showRawOutput && out !== undefined) {\\n         try {\\n           text = JSON.stringify(out, null, 2);\\n@@ -325,13 +361,78 @@ export class SlackFrontend implements Frontend {\\n       }\\n       if (!text) return;\\n \\n-      const formattedText = formatSlackText(text);\\n+      // Extract and render mermaid diagrams before posting\\n+      const diagrams = extractMermaidDiagrams(text);\\n+      let processedText = text;\\n+\\n+      if (diagrams.length > 0) {\\n+        try {\\n+          ctx.logger.info(\\n+            `[slack-frontend] found ${diagrams.length} mermaid diagram(s) to render for ${checkId}`\\n+          );\\n+        } catch {}\\n+\\n+        // Render and upload each diagram\\n+        const uploadedCount: number[] = [];\\n+        for (let i = 0; i < diagrams.length; i++) {\\n+          const diagram = diagrams[i];\\n+          try {\\n+            ctx.logger.info(`[slack-frontend] rendering mermaid diagram ${i + 1}...`);\\n+            const pngBuffer = await renderMermaidToPng(diagram.code);\\n+            if (pngBuffer) {\\n+              ctx.logger.info(\\n+                `[slack-frontend] rendered diagram ${i + 1}, size=${pngBuffer.length} bytes, uploading...`\\n+              );\\n+              const filename = `diagram-${i + 1}.png`;\\n+              const uploadResult = await slack.files.uploadV2({\\n+                content: pngBuffer,\\n+                filename,\\n+                channel,\\n+                thread_ts: threadTs,\\n+                title: `Diagram ${i + 1}`,\\n+              });\\n+              if (uploadResult.ok) {\\n+                uploadedCount.push(i);\\n+                ctx.logger.info(`[slack-frontend] uploaded mermaid diagram ${i + 1} to ${channel}`);\\n+              } else {\\n+                ctx.logger.warn(`[slack-frontend] upload failed for diagram ${i + 1}`);\\n+              }\\n+            } else {\\n+              ctx.logger.warn(\\n+                `[slack-frontend] mermaid rendering returned null for diagram ${i + 1} (mmdc failed or not installed)`\\n+              );\\n+            }\\n+          } catch (e) {\\n+            ctx.logger.warn(\\n+              `[slack-frontend] failed to render/upload mermaid diagram ${i + 1}: ${\\n+                e instanceof Error ? e.message : String(e)\\n+              }`\\n+            );\\n+          }\\n+        }\\n+\\n+        // Replace mermaid blocks with placeholder text\\n+        if (uploadedCount.length > 0) {\\n+          processedText = replaceMermaidBlocks(text, diagrams, (idx) =>\\n+            uploadedCount.includes(idx) ? '_(See diagram above)_' : '_(Diagram rendering failed)_'\\n+          );\\n+        }\\n+      }\\n+\\n+      const formattedText = formatSlackText(processedText);\\n       await slack.chat.postMessage({ channel, text: formattedText, thread_ts: threadTs });\\n+      ctx.logger.info(\\n+        `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+      );\\n+    } catch (outerErr) {\\n+      // Log errors instead of silently swallowing them\\n       try {\\n-        ctx.logger.info(\\n-          `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+        ctx.logger.warn(\\n+          `[slack-frontend] maybePostDirectReply failed for ${checkId}: ${\\n+            outerErr instanceof Error ? outerErr.message : String(outerErr)\\n+          }`\\n         );\\n       } catch {}\\n-    } catch {}\\n+    }\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/generated/config-schema.ts\",\"additions\":233,\"deletions\":13,\"changes\":246,\"patch\":\"diff --git a/src/generated/config-schema.ts b/src/generated/config-schema.ts\\nindex b3caca4f..ecab390d 100644\\n--- a/src/generated/config-schema.ts\\n+++ b/src/generated/config-schema.ts\\n@@ -56,6 +56,20 @@ export const configSchema = {\\n           },\\n           description: 'Import workflow definitions from external files or URLs',\\n         },\\n+        inputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowInput',\\n+          },\\n+          description: 'Workflow inputs (for standalone reusable workflows)',\\n+        },\\n+        outputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowOutput',\\n+          },\\n+          description: 'Workflow outputs (for standalone reusable workflows)',\\n+        },\\n         steps: {\\n           $ref: '#/definitions/Record%3Cstring%2CCheckConfig%3E',\\n           description: 'Step configurations (recommended)',\\n@@ -67,7 +81,7 @@ export const configSchema = {\\n         },\\n         output: {\\n           $ref: '#/definitions/OutputConfig',\\n-          description: 'Output configuration',\\n+          description: 'Output configuration (optional - defaults provided)',\\n         },\\n         http_server: {\\n           $ref: '#/definitions/HttpServerConfig',\\n@@ -139,8 +153,16 @@ export const configSchema = {\\n           },\\n           description: 'Optional integrations: event-driven frontends (e.g., ndjson-sink, github)',\\n         },\\n+        workspace: {\\n+          $ref: '#/definitions/WorkspaceConfig',\\n+          description: 'Workspace isolation configuration for sandboxed execution',\\n+        },\\n+        slack: {\\n+          $ref: '#/definitions/SlackConfig',\\n+          description: 'Slack configuration',\\n+        },\\n       },\\n-      required: ['output', 'version'],\\n+      required: ['version'],\\n       patternProperties: {\\n         '^x-': {},\\n       },\\n@@ -243,6 +265,63 @@ export const configSchema = {\\n         type: 'string',\\n       },\\n     },\\n+    WorkflowInput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Input parameter name',\\n+        },\\n+        schema: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'JSON Schema for the input',\\n+        },\\n+        required: {\\n+          type: 'boolean',\\n+          description: 'Whether this input is required',\\n+        },\\n+        default: {\\n+          description: 'Default value if not provided',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow input definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    WorkflowOutput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Output name',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+        value: {\\n+          type: 'string',\\n+          description: 'Value using Liquid template syntax (references step outputs)',\\n+        },\\n+        value_js: {\\n+          type: 'string',\\n+          description: 'Value using JavaScript expression (alternative to value)',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow output definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n     'Record<string,CheckConfig>': {\\n       type: 'object',\\n       additionalProperties: {\\n@@ -390,11 +469,19 @@ export const configSchema = {\\n           description: 'Timeout in seconds for command execution (default: 60)',\\n         },\\n         depends_on: {\\n-          type: 'array',\\n-          items: {\\n-            type: 'string',\\n-          },\\n-          description: 'Check IDs that this check depends on (optional)',\\n+          anyOf: [\\n+            {\\n+              type: 'string',\\n+            },\\n+            {\\n+              type: 'array',\\n+              items: {\\n+                type: 'string',\\n+              },\\n+            },\\n+          ],\\n+          description:\\n+            'Check IDs that this check depends on (optional). Accepts single string or array.',\\n         },\\n         group: {\\n           type: 'string',\\n@@ -653,13 +740,93 @@ export const configSchema = {\\n           description: 'Arguments/inputs for the workflow',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Override specific step configurations in the workflow',\\n         },\\n         output_mapping: {\\n           $ref: '#/definitions/Record%3Cstring%2Cstring%3E',\\n           description: 'Map workflow outputs to check outputs',\\n         },\\n+        workflow_inputs: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'Alias for args - workflow inputs (backward compatibility)',\\n+        },\\n+        config: {\\n+          type: 'string',\\n+          description:\\n+            'Config file path - alternative to workflow ID (loads a Visor config file as workflow)',\\n+        },\\n+        workflow_overrides: {\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n+          description: 'Alias for overrides - workflow step overrides (backward compatibility)',\\n+        },\\n+        ref: {\\n+          type: 'string',\\n+          description: 'Git reference to checkout (branch, tag, commit SHA) - supports templates',\\n+        },\\n+        repository: {\\n+          type: 'string',\\n+          description: 'Repository URL or owner/repo format (defaults to current repository)',\\n+        },\\n+        token: {\\n+          type: 'string',\\n+          description: 'GitHub token for private repositories (defaults to GITHUB_TOKEN env)',\\n+        },\\n+        fetch_depth: {\\n+          type: 'number',\\n+          description: 'Number of commits to fetch (0 for full history, default: 1)',\\n+        },\\n+        fetch_tags: {\\n+          type: 'boolean',\\n+          description: 'Whether to fetch tags (default: false)',\\n+        },\\n+        submodules: {\\n+          anyOf: [\\n+            {\\n+              type: 'boolean',\\n+            },\\n+            {\\n+              type: 'string',\\n+              const: 'recursive',\\n+            },\\n+          ],\\n+          description: \\\"Checkout submodules: false, true, or 'recursive'\\\",\\n+        },\\n+        working_directory: {\\n+          type: 'string',\\n+          description: 'Working directory for the checkout (defaults to temp directory)',\\n+        },\\n+        use_worktree: {\\n+          type: 'boolean',\\n+          description: 'Use git worktree for efficient parallel checkouts (default: true)',\\n+        },\\n+        clean: {\\n+          type: 'boolean',\\n+          description: 'Clean the working directory before checkout (default: true)',\\n+        },\\n+        sparse_checkout: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description: 'Sparse checkout paths - only checkout specific directories/files',\\n+        },\\n+        lfs: {\\n+          type: 'boolean',\\n+          description: 'Enable Git LFS (Large File Storage)',\\n+        },\\n+        clone_timeout_ms: {\\n+          type: 'number',\\n+          description: 'Timeout in ms for cloning the bare repository (default: 300000 = 5 min)',\\n+        },\\n+        cleanup_on_failure: {\\n+          type: 'boolean',\\n+          description: 'Clean up worktree on failure (default: true)',\\n+        },\\n+        persist_worktree: {\\n+          type: 'boolean',\\n+          description: 'Keep worktree after workflow completion (default: false)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'Configuration for a single check',\\n@@ -794,6 +961,11 @@ export const configSchema = {\\n           $ref: '#/definitions/BashConfig',\\n           description: 'Advanced bash command execution configuration',\\n         },\\n+        completion_prompt: {\\n+          type: 'string',\\n+          description:\\n+            'Completion prompt for post-completion validation/review (runs after attempt_completion)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'AI provider configuration',\\n@@ -1223,7 +1395,7 @@ export const configSchema = {\\n           description: 'Custom output name (defaults to workflow name)',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Step overrides',\\n         },\\n         output_mapping: {\\n@@ -1238,14 +1410,14 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n-    'Record<string,Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>>':\\n+    'Record<string,Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>>':\\n       {\\n         type: 'object',\\n         additionalProperties: {\\n-          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E',\\n+          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E',\\n         },\\n       },\\n-    'Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>': {\\n+    'Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>': {\\n       type: 'object',\\n       additionalProperties: false,\\n     },\\n@@ -1811,6 +1983,54 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n+    WorkspaceConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        enabled: {\\n+          type: 'boolean',\\n+          description: 'Enable workspace isolation (default: true when config present)',\\n+        },\\n+        base_path: {\\n+          type: 'string',\\n+          description: 'Base path for workspaces (default: /tmp/visor-workspaces)',\\n+        },\\n+        cleanup_on_exit: {\\n+          type: 'boolean',\\n+          description: 'Clean up workspace on exit (default: true)',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Workspace isolation configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    SlackConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        version: {\\n+          type: 'string',\\n+          description: 'Slack API version',\\n+        },\\n+        mentions: {\\n+          type: 'string',\\n+          description: \\\"Mention handling: 'all', 'direct', etc.\\\",\\n+        },\\n+        threads: {\\n+          type: 'string',\\n+          description: \\\"Thread handling: 'required', 'optional', etc.\\\",\\n+        },\\n+        show_raw_output: {\\n+          type: 'boolean',\\n+          description: 'Show raw output in Slack responses',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Slack configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n   },\\n } as const;\\n export default configSchema;\\n\",\"status\":\"modified\"},{\"filename\":\"src/index.ts\",\"additions\":5,\"deletions\":2,\"changes\":7,\"patch\":\"diff --git a/src/index.ts b/src/index.ts\\nindex 50b89df3..9621e733 100644\\n--- a/src/index.ts\\n+++ b/src/index.ts\\n@@ -628,8 +628,11 @@ function resolveDependencies(\\n \\n     // Get dependencies for this check\\n     const checkConfig = config?.checks?.[checkId];\\n+    // Normalize depends_on to array (supports string | string[])\\n+    const rawDeps = checkConfig?.depends_on;\\n+    const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     // Expand OR groups (pipe syntax) for dependency closure discovery\\n-    const dependencies = (checkConfig?.depends_on || []).flatMap(d =>\\n+    const dependencies = depsArray.flatMap((d: string) =>\\n       typeof d === 'string' && d.includes('|')\\n         ? d\\n             .split('|')\\n\",\"status\":\"modified\"},{\"filename\":\"src/liquid-extensions.ts\",\"additions\":68,\"deletions\":1,\"changes\":69,\"patch\":\"diff --git a/src/liquid-extensions.ts b/src/liquid-extensions.ts\\nindex e9efaf68..dbf9c6ed 100644\\n--- a/src/liquid-extensions.ts\\n+++ b/src/liquid-extensions.ts\\n@@ -124,6 +124,26 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     }\\n   });\\n \\n+  // Register base64 filter for encoding strings\\n+  // Usage: {{ \\\"user:password\\\" | base64 }}\\n+  liquid.registerFilter('base64', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    return Buffer.from(str).toString('base64');\\n+  });\\n+\\n+  // Register base64_decode filter for decoding base64 strings\\n+  // Usage: {{ encoded_value | base64_decode }}\\n+  liquid.registerFilter('base64_decode', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    try {\\n+      return Buffer.from(str, 'base64').toString('utf-8');\\n+    } catch {\\n+      return '[Error: Invalid base64 string]';\\n+    }\\n+  });\\n+\\n   // Sanitize a label to allowed characters only: [A-Za-z0-9:/]\\n   liquid.registerFilter('safe_label', (value: unknown) => sanitizeLabel(value));\\n \\n@@ -137,6 +157,53 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     return s.replace(/\\\\\\\\n/g, '\\\\n').replace(/\\\\\\\\r/g, '\\\\r').replace(/\\\\\\\\t/g, '\\\\t');\\n   });\\n \\n+  // JSON escape filter - escapes a string for use inside a JSON string value\\n+  // This escapes special characters like quotes, backslashes, and control characters\\n+  // Usage: \\\"jql\\\": \\\"{{ myValue | json_escape }}\\\"\\n+  liquid.registerFilter('json_escape', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const s = String(value);\\n+    // Use JSON.stringify which handles all escaping, then strip the surrounding quotes\\n+    const jsonStr = JSON.stringify(s);\\n+    // Remove the first and last character (the quotes added by JSON.stringify)\\n+    return jsonStr.slice(1, -1);\\n+  });\\n+\\n+  // Shell escape filter - wraps value in single quotes with proper escaping\\n+  // Usage: {{ value | shell_escape }}\\n+  // Example: \\\"hello'world\\\" becomes \\\"'hello'\\\\''world'\\\"\\n+  // This is POSIX-compliant and safe for arbitrary text including mermaid diagrams\\n+  liquid.registerFilter('shell_escape', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    // Replace single quotes with: end quote, escaped quote, start quote\\n+    // Then wrap the entire thing in single quotes\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Alias for shell_escape\\n+  liquid.registerFilter('escape_shell', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Shell escape for double quotes (less safe but sometimes needed)\\n+  // Usage: {{ value | shell_escape_double }}\\n+  // Escapes: $, `, \\\\, \\\", and !\\n+  liquid.registerFilter('shell_escape_double', (value: unknown) => {\\n+    if (value == null) return '\\\"\\\"';\\n+    const s = String(value);\\n+    // Escape characters that have special meaning inside double quotes\\n+    const escaped = s\\n+      .replace(/\\\\\\\\/g, '\\\\\\\\\\\\\\\\')  // backslash first\\n+      .replace(/\\\\$/g, '\\\\\\\\$')   // dollar sign\\n+      .replace(/`/g, '\\\\\\\\`')    // backticks\\n+      .replace(/\\\"/g, '\\\\\\\\\\\"')    // double quotes\\n+      .replace(/!/g, '\\\\\\\\!');   // history expansion\\n+    return '\\\"' + escaped + '\\\"';\\n+  });\\n+\\n   // Register author permission filters (from main)\\n   // These filters check the author's permission level; detect local mode for tests\\n   const isLocal = detectLocalMode();\\n\",\"status\":\"modified\"},{\"filename\":\"src/logger.ts\",\"additions\":31,\"deletions\":2,\"changes\":33,\"patch\":\"diff --git a/src/logger.ts b/src/logger.ts\\nindex 6148b77c..aee7136e 100644\\n--- a/src/logger.ts\\n+++ b/src/logger.ts\\n@@ -85,7 +85,36 @@ class Logger {\\n       if (this.showTimestamps) {\\n         const ts = new Date().toISOString();\\n         const lvl = level ? level : undefined;\\n-        const prefix = lvl ? `[${ts}] [${lvl}]` : `[${ts}]`;\\n+\\n+        let tsToken = `[${ts}]`;\\n+        let lvlToken = lvl ? `[${lvl}]` : '';\\n+\\n+        // Add simple ANSI colour when running in a TTY and not emitting\\n+        // JSON/SARIF. Colours are intentionally minimal and only applied\\n+        // to the prefix markers, not the full line.\\n+        if (this.isTTY && !this.isJsonLike) {\\n+          const reset = '\\\\x1b[0m';\\n+          const dim = '\\\\x1b[2m';\\n+          const colours: Record<LogLevel, string> = {\\n+            silent: '',\\n+            error: '\\\\x1b[31m', // red\\n+            warn: '\\\\x1b[33m', // yellow\\n+            info: '\\\\x1b[36m', // cyan\\n+            verbose: '\\\\x1b[35m', // magenta\\n+            debug: '\\\\x1b[90m', // bright black / gray\\n+          };\\n+\\n+          tsToken = `${dim}${tsToken}${reset}`;\\n+\\n+          if (lvl) {\\n+            const colour = colours[lvl] || '';\\n+            if (colour) {\\n+              lvlToken = `${colour}${lvlToken}${reset}`;\\n+            }\\n+          }\\n+        }\\n+\\n+        const prefix = lvl ? `${tsToken} ${lvlToken}` : tsToken;\\n         process.stderr.write(`${prefix} ${msg}\\\\n`);\\n       } else {\\n         process.stderr.write(msg + '\\\\n');\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/ai-check-provider.ts\",\"additions\":100,\"deletions\":6,\"changes\":106,\"patch\":\"diff --git a/src/providers/ai-check-provider.ts b/src/providers/ai-check-provider.ts\\nindex 53f1a350..b41d6af1 100644\\n--- a/src/providers/ai-check-provider.ts\\n+++ b/src/providers/ai-check-provider.ts\\n@@ -186,7 +186,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     let promptContent: string;\\n \\n@@ -204,7 +205,8 @@ export class AICheckProvider extends CheckProvider {\\n       eventContext,\\n       dependencyResults,\\n       outputHistory,\\n-      args\\n+      args,\\n+      workflowInputs\\n     );\\n   }\\n \\n@@ -335,7 +337,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     // Build outputs_raw from -raw keys (aggregate parent values)\\n     const outputsRaw: Record<string, unknown> = {};\\n@@ -535,6 +538,8 @@ export class AICheckProvider extends CheckProvider {\\n       outputs_raw: outputsRaw,\\n       // Custom arguments from on_init 'with' directive\\n       args: args || {},\\n+      // Workflow inputs (for nested workflow steps to access parent inputs like {{ inputs.context }})\\n+      inputs: workflowInputs || {},\\n     };\\n \\n     try {\\n@@ -621,7 +626,8 @@ export class AICheckProvider extends CheckProvider {\\n         console.error(`[ai-exec] step=${String((config as any).checkName || 'unknown')}`);\\n       }\\n     } catch {}\\n-    // Extract AI configuration - only set properties that are explicitly provided\\n+    // Extract AI configuration - only set properties that are explicitly provided.\\n+    // Workspace / allowedFolders will be derived below from the execution context.\\n     const aiConfig: AIReviewConfig = {};\\n \\n     // Check-level AI configuration (ai object)\\n@@ -671,6 +677,9 @@ export class AICheckProvider extends CheckProvider {\\n       if (aiAny.bashConfig !== undefined) {\\n         aiConfig.bashConfig = aiAny.bashConfig as import('../types/config').BashConfig;\\n       }\\n+      if (aiAny.completion_prompt !== undefined) {\\n+        aiConfig.completionPrompt = aiAny.completion_prompt as string;\\n+      }\\n       if (aiAny.skip_code_context !== undefined) {\\n         // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n         (aiConfig as any).skip_code_context = aiAny.skip_code_context as boolean;\\n@@ -691,6 +700,90 @@ export class AICheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Derive workspace-aware allowedFolders for ProbeAgent when workspace\\n+    // isolation is enabled. This ensures tools like search/query operate\\n+    // inside the isolated workspace (and its project symlinks) instead of\\n+    // the Visor repository root.\\n+    // Folder names are human-readable (tyk-docs, visor2) thanks to WorkspaceManager.\\n+    try {\\n+      const ctxAny: any = sessionInfo as any;\\n+      const parentCtx = ctxAny?._parentContext;\\n+      const workspace = parentCtx?.workspace;\\n+\\n+      // Enhanced debug logging for workspace propagation diagnosis\\n+      logger.debug(`[AI Provider] Workspace detection for check '${(config as any).checkName || 'unknown'}':`);\\n+      logger.debug(`[AI Provider]   sessionInfo exists: ${!!sessionInfo}`);\\n+      logger.debug(`[AI Provider]   _parentContext exists: ${!!parentCtx}`);\\n+      logger.debug(`[AI Provider]   workspace exists: ${!!workspace}`);\\n+      if (workspace) {\\n+        logger.debug(`[AI Provider]   workspace.isEnabled exists: ${typeof workspace.isEnabled === 'function'}`);\\n+        logger.debug(`[AI Provider]   workspace.isEnabled(): ${typeof workspace.isEnabled === 'function' ? workspace.isEnabled() : 'N/A'}`);\\n+        const projectCount = typeof workspace.listProjects === 'function' ? workspace.listProjects()?.length : 'N/A';\\n+        logger.debug(`[AI Provider]   workspace.listProjects() count: ${projectCount}`);\\n+      }\\n+\\n+      if (workspace && typeof workspace.isEnabled === 'function' && workspace.isEnabled()) {\\n+        const folders: string[] = [];\\n+        let workspaceRoot: string | undefined;\\n+        let mainProjectPath: string | undefined;\\n+        try {\\n+          const info = workspace.getWorkspaceInfo?.();\\n+          if (info && typeof info.workspacePath === 'string') {\\n+            workspaceRoot = info.workspacePath;\\n+            mainProjectPath = info.mainProjectPath;\\n+            // Add workspace root\\n+            folders.push(info.workspacePath);\\n+            // Include main project path (has human-readable name like \\\"visor2\\\")\\n+            if (mainProjectPath) {\\n+              folders.push(mainProjectPath);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore workspace info errors\\n+        }\\n+        try {\\n+          const projects = workspace.listProjects?.() || [];\\n+          for (const proj of projects as any[]) {\\n+            if (proj && typeof proj.path === 'string') {\\n+              // Project paths have human-readable names (tyk-docs, not checkout-tyk-docs)\\n+              folders.push(proj.path);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore project listing errors\\n+        }\\n+        const unique = Array.from(new Set(folders.filter(p => typeof p === 'string' && p)));\\n+        if (unique.length > 0 && workspaceRoot) {\\n+          (aiConfig as any).allowedFolders = unique;\\n+          // Set path and cwd to workspace root - AI will run with cwd set to workspace\\n+          // Both are set for compatibility: path for older probe versions, cwd for rc175+\\n+          (aiConfig as any).path = workspaceRoot;\\n+          (aiConfig as any).cwd = workspaceRoot;\\n+          // Also set workspacePath for the AI to know the root\\n+          (aiConfig as any).workspacePath = workspaceRoot;\\n+          logger.debug(`[AI Provider] Workspace isolation enabled:`);\\n+          logger.debug(`[AI Provider]   workspaceRoot (cwd): ${workspaceRoot}`);\\n+          logger.debug(`[AI Provider]   allowedFolders: ${JSON.stringify(unique)}`);\\n+        }\\n+      } else if (parentCtx && typeof parentCtx.workingDirectory === 'string') {\\n+        // Fallback: when workspace is not available (or disabled), still\\n+        // constrain tools to the engine's working directory so ProbeAgent\\n+        // operates inside the same logical root as the state machine. This\\n+        // also ensures nested workflows (e.g. code-question-helper) use the\\n+        // workspace main project path once initializeWorkspace has updated\\n+        // the parent context.\\n+        if (!(aiConfig as any).allowedFolders) {\\n+          (aiConfig as any).allowedFolders = [parentCtx.workingDirectory];\\n+        }\\n+        if (!(aiConfig as any).path) {\\n+          (aiConfig as any).path = parentCtx.workingDirectory;\\n+          (aiConfig as any).cwd = parentCtx.workingDirectory;\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only; fall back to defaults on error.\\n+    }\\n+\\n     // Check-level AI model and provider (top-level properties)\\n     if (config.ai_model !== undefined) {\\n       aiConfig.model = config.ai_model as string;\\n@@ -876,7 +969,8 @@ export class AICheckProvider extends CheckProvider {\\n       ctxWithStage,\\n       _dependencyResults,\\n       (config as any).__outputHistory as Map<string, unknown[]> | undefined,\\n-      (sessionInfo as any)?.args\\n+      (sessionInfo as any)?.args,\\n+      (config as any).workflowInputs as Record<string, unknown> | undefined\\n     );\\n \\n     // Optional persona (vendor extension): ai.ai_persona or ai_persona.\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/command-check-provider.ts\",\"additions\":50,\"deletions\":16,\"changes\":66,\"patch\":\"diff --git a/src/providers/command-check-provider.ts b/src/providers/command-check-provider.ts\\nindex e0c9d634..bf7ffd43 100644\\n--- a/src/providers/command-check-provider.ts\\n+++ b/src/providers/command-check-provider.ts\\n@@ -131,7 +131,8 @@ export class CommandCheckProvider extends CheckProvider {\\n       // New: outputs_raw exposes aggregate values (e.g., full arrays for forEach parents)\\n       outputs_raw: outputsRaw,\\n       // Workflow inputs (when executing within a workflow)\\n-      inputs: context?.workflowInputs || {},\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any).workflowInputs || context?.workflowInputs || {},\\n       // Custom arguments from on_init 'with' directive\\n       args: context?.args || {},\\n       env: this.getSafeEnvironmentVariables(),\\n@@ -178,17 +179,31 @@ export class CommandCheckProvider extends CheckProvider {\\n           mock = rawMock as Record<string, unknown>;\\n         }\\n         const m = mock as { stdout?: string; stderr?: string; exit_code?: number; exit?: number };\\n-        let out: unknown = m.stdout ?? '';\\n-        try {\\n-          if (\\n-            typeof out === 'string' &&\\n-            (out.trim().startsWith('{') || out.trim().startsWith('['))\\n-          ) {\\n-            out = JSON.parse(out);\\n-          }\\n-        } catch {}\\n-        const code =\\n-          typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0;\\n+\\n+        // Check if this looks like a command mock (has stdout/stderr/exit_code) or direct data output\\n+        const isCommandMock = m.stdout !== undefined || m.stderr !== undefined ||\\n+          m.exit_code !== undefined || m.exit !== undefined;\\n+\\n+        let out: unknown;\\n+        if (isCommandMock) {\\n+          // Traditional command mock format: { stdout: \\\"...\\\", exit_code: 0 }\\n+          out = m.stdout ?? '';\\n+          try {\\n+            if (\\n+              typeof out === 'string' &&\\n+              (out.trim().startsWith('{') || out.trim().startsWith('['))\\n+            ) {\\n+              out = JSON.parse(out);\\n+            }\\n+          } catch {}\\n+        } else {\\n+          // Direct data mock format: { data: [...], count: 1 } - use as-is\\n+          out = mock;\\n+        }\\n+\\n+        const code = isCommandMock\\n+          ? (typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0)\\n+          : 0;\\n         if (code !== 0) {\\n           return {\\n             issues: [\\n@@ -231,9 +246,8 @@ export class CommandCheckProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Get timeout from config (in seconds) or use default (60 seconds)\\n-      const timeoutSeconds = (config.timeout as number) || 60;\\n-      const timeoutMs = timeoutSeconds * 1000;\\n+      // Get timeout from config (in milliseconds) or use default (60 seconds = 60000ms)\\n+      const timeoutMs = (config.timeout as number) || 60000;\\n \\n       // Normalize only the eval payload for `node -e|--eval` invocations that may contain\\n       // literal newlines due to YAML processing (\\\"\\\\n\\\" -> newline). We re-escape newlines\\n@@ -1533,6 +1547,26 @@ ${bodyWithReturn}\\n       return null;\\n     }\\n \\n+    // Require at least one issue-specific field beyond just a message-like field.\\n+    // This prevents objects like {text: \\\"...\\\"} from being treated as issues.\\n+    const hasIssueField =\\n+      data.file !== undefined ||\\n+      data.path !== undefined ||\\n+      data.filename !== undefined ||\\n+      data.line !== undefined ||\\n+      data.startLine !== undefined ||\\n+      data.lineNumber !== undefined ||\\n+      data.severity !== undefined ||\\n+      data.level !== undefined ||\\n+      data.priority !== undefined ||\\n+      data.ruleId !== undefined ||\\n+      data.rule !== undefined ||\\n+      data.category !== undefined ||\\n+      data.type !== undefined;\\n+    if (!hasIssueField) {\\n+      return null;\\n+    }\\n+\\n     const allowedSeverities = new Set(['info', 'warning', 'error', 'critical']);\\n     const severityRaw = this.toTrimmedString(data.severity || data.level || data.priority);\\n     let severity: ReviewIssue['severity'] = 'warning';\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/git-checkout-provider.ts\",\"additions\":41,\"deletions\":6,\"changes\":47,\"patch\":\"diff --git a/src/providers/git-checkout-provider.ts b/src/providers/git-checkout-provider.ts\\nindex baac0001..6d3a2852 100644\\n--- a/src/providers/git-checkout-provider.ts\\n+++ b/src/providers/git-checkout-provider.ts\\n@@ -69,6 +69,16 @@ export class GitCheckoutProvider extends CheckProvider {\\n       return false;\\n     }\\n \\n+    if (checkoutConfig.clone_timeout_ms !== undefined) {\\n+      if (\\n+        typeof checkoutConfig.clone_timeout_ms !== 'number' ||\\n+        checkoutConfig.clone_timeout_ms <= 0\\n+      ) {\\n+        logger.error('Invalid config: clone_timeout_ms must be a positive number (milliseconds)');\\n+        return false;\\n+      }\\n+    }\\n+\\n     return true;\\n   }\\n \\n@@ -91,7 +101,13 @@ export class GitCheckoutProvider extends CheckProvider {\\n       );\\n \\n       // Resolve dynamic variables\\n-      const resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      let resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      // If ref resolves to an empty string, fall back to HEAD so we rely on\\n+      // the repository's default branch. This keeps downstream git commands\\n+      // well-defined and matches existing expectations/tests.\\n+      if (!resolvedRef || resolvedRef.trim().length === 0) {\\n+        resolvedRef = 'HEAD';\\n+      }\\n       const resolvedRepository = checkoutConfig.repository\\n         ? await this.liquid.parseAndRender(checkoutConfig.repository, templateContext)\\n         : process.env.GITHUB_REPOSITORY || 'unknown/unknown';\\n@@ -118,6 +134,7 @@ export class GitCheckoutProvider extends CheckProvider {\\n           clean: checkoutConfig.clean !== false, // Default: true\\n           workflowId: (context as any)?.workflowId,\\n           fetchDepth: checkoutConfig.fetch_depth,\\n+          cloneTimeoutMs: checkoutConfig.clone_timeout_ms,\\n         }\\n       );\\n \\n@@ -134,18 +151,35 @@ export class GitCheckoutProvider extends CheckProvider {\\n \\n       // Add project to workspace if workspace isolation is enabled\\n       const workspace = (context as any)?._parentContext?.workspace;\\n+\\n+      // Enhanced debug logging for workspace project addition diagnosis\\n+      const checkName = (config as any)?.checkName || 'unknown';\\n+      logger.info(`[GitCheckout] Workspace check for '${checkName}':`);\\n+      logger.info(`[GitCheckout]   _parentContext exists: ${!!(context as any)?._parentContext}`);\\n+      logger.info(`[GitCheckout]   workspace exists: ${!!workspace}`);\\n+      logger.info(`[GitCheckout]   workspace.isEnabled(): ${workspace?.isEnabled?.() ?? 'N/A'}`);\\n+      if (workspace) {\\n+        const projectCountBefore = workspace.listProjects?.()?.length ?? 'N/A';\\n+        logger.debug(`[GitCheckout]   projects before addProject: ${projectCountBefore}`);\\n+      }\\n+\\n       if (workspace?.isEnabled()) {\\n         try {\\n+          // Don't pass checkName as description - let workspace use repo name\\n+          // This results in human-readable names like \\\"tyk-docs\\\" instead of \\\"checkout-tyk-docs\\\"\\n           const workspacePath = await workspace.addProject(\\n             resolvedRepository,\\n-            worktree.path,\\n-            checkoutConfig.checkName\\n+            worktree.path\\n           );\\n           output.workspace_path = workspacePath;\\n-          logger.debug(`Added project to workspace: ${workspacePath}`);\\n+          const projectCountAfter = workspace.listProjects?.()?.length ?? 'N/A';\\n+          logger.debug(`[GitCheckout] Added project to workspace: ${workspacePath}`);\\n+          logger.debug(`[GitCheckout]   projects after addProject: ${projectCountAfter}`);\\n         } catch (error) {\\n           logger.warn(`Failed to add project to workspace: ${error}`);\\n         }\\n+      } else {\\n+        logger.debug(`[GitCheckout] Workspace not enabled, skipping addProject`);\\n       }\\n \\n       logger.info(\\n@@ -228,7 +262,8 @@ export class GitCheckoutProvider extends CheckProvider {\\n       outputs: outputsObj,\\n       outputs_history: historyObj,\\n       env: safeEnv,\\n-      inputs: context?.workflowInputs,\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any)?.workflowInputs || context?.workflowInputs,\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/http-client-provider.ts\",\"additions\":279,\"deletions\":35,\"changes\":314,\"patch\":\"diff --git a/src/providers/http-client-provider.ts b/src/providers/http-client-provider.ts\\nindex 4d8c41be..6db81591 100644\\n--- a/src/providers/http-client-provider.ts\\n+++ b/src/providers/http-client-provider.ts\\n@@ -4,18 +4,29 @@ import { ReviewSummary } from '../reviewer';\\n import { Liquid } from 'liquidjs';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n import { EnvironmentResolver } from '../utils/env-resolver';\\n+import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n+import { buildProviderTemplateContext } from '../utils/template-context';\\n+import Sandbox from '@nyariv/sandboxjs';\\n+import { logger } from '../logger';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n \\n /**\\n  * Check provider that fetches data from HTTP endpoints\\n  */\\n export class HttpClientProvider extends CheckProvider {\\n   private liquid: Liquid;\\n+  private sandbox?: Sandbox;\\n \\n   constructor() {\\n     super();\\n     this.liquid = createExtendedLiquid();\\n   }\\n \\n+  private createSecureSandbox(): Sandbox {\\n+    return createSecureSandbox();\\n+  }\\n+\\n   getName(): string {\\n     return 'http_client';\\n   }\\n@@ -61,50 +72,124 @@ export class HttpClientProvider extends CheckProvider {\\n     const headers = (config.headers as Record<string, string>) || {};\\n     const timeout = (config.timeout as number) || 30000;\\n     const transform = config.transform as string | undefined;\\n+    const transformJs = config.transform_js as string | undefined;\\n     const bodyTemplate = config.body as string | undefined;\\n+    const outputFileTemplate = config.output_file as string | undefined;\\n+    const skipIfExists = config.skip_if_exists !== false; // Default true for caching\\n \\n-    try {\\n-      // Prepare template context for URL and body\\n-      const templateContext = {\\n-        pr: {\\n-          number: prInfo.number,\\n-          title: prInfo.title,\\n-          body: prInfo.body,\\n-          author: prInfo.author,\\n-          base: prInfo.base,\\n-          head: prInfo.head,\\n-          totalAdditions: prInfo.totalAdditions,\\n-          totalDeletions: prInfo.totalDeletions,\\n-        },\\n-        outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-        env: process.env,\\n-      };\\n+    // Track resolved URL for error messages\\n+    let resolvedUrlForErrors = url;\\n \\n-      // Render URL with template if it contains liquid syntax\\n-      let renderedUrl = url;\\n-      if (url.includes('{{') || url.includes('{%')) {\\n-        renderedUrl = await this.liquid.parseAndRender(url, templateContext);\\n+    try {\\n+      // Use shared template context builder for consistent output extraction\\n+      const templateContext = buildProviderTemplateContext(\\n+        prInfo,\\n+        dependencyResults,\\n+        undefined, // memoryStore\\n+        undefined, // outputHistory\\n+        undefined, // stageHistoryBase\\n+        { attachMemoryReadHelpers: false }\\n+      );\\n+      // Add env to context for shell-style variable resolution\\n+      (templateContext as Record<string, unknown>).env = process.env;\\n+\\n+      // First resolve shell-style environment variables (${VAR}, $VAR, ${{ env.VAR }})\\n+      let renderedUrl = String(EnvironmentResolver.resolveValue(url));\\n+      resolvedUrlForErrors = renderedUrl; // Track for error messages\\n+\\n+      // Then render Liquid templates if present\\n+      if (renderedUrl.includes('{{') || renderedUrl.includes('{%')) {\\n+        renderedUrl = await this.liquid.parseAndRender(renderedUrl, templateContext);\\n+        resolvedUrlForErrors = renderedUrl; // Update after Liquid rendering\\n       }\\n \\n       // Prepare request body if provided\\n       let requestBody: string | undefined;\\n       if (bodyTemplate) {\\n-        const renderedBody = await this.liquid.parseAndRender(bodyTemplate, templateContext);\\n-        requestBody = renderedBody;\\n+        // First resolve shell-style environment variables\\n+        let resolvedBody = String(EnvironmentResolver.resolveValue(bodyTemplate));\\n+        // Then render Liquid templates if present\\n+        if (resolvedBody.includes('{{') || resolvedBody.includes('{%')) {\\n+          resolvedBody = await this.liquid.parseAndRender(resolvedBody, templateContext);\\n+        }\\n+        requestBody = resolvedBody;\\n       }\\n \\n-      // Resolve environment variables in headers\\n-      const resolvedHeaders = EnvironmentResolver.resolveHeaders(headers);\\n+      // Resolve environment variables and Liquid templates in headers\\n+      const resolvedHeaders: Record<string, string> = {};\\n+      for (const [key, value] of Object.entries(headers)) {\\n+        let resolvedValue = String(EnvironmentResolver.resolveValue(value));\\n+        // Render Liquid templates if present\\n+        if (resolvedValue.includes('{{') || resolvedValue.includes('{%')) {\\n+          resolvedValue = await this.liquid.parseAndRender(resolvedValue, templateContext);\\n+        }\\n+        resolvedHeaders[key] = resolvedValue;\\n+        // Debug auth header (mask most of the value for security)\\n+        if (key.toLowerCase() === 'authorization') {\\n+          const maskedValue = resolvedValue.length > 20\\n+            ? `${resolvedValue.substring(0, 15)}...${resolvedValue.substring(resolvedValue.length - 5)}`\\n+            : resolvedValue;\\n+          logger.verbose(`[http_client] ${key}: ${maskedValue}`);\\n+        }\\n+      }\\n+\\n+      // Resolve output_file path if specified\\n+      let resolvedOutputFile: string | undefined;\\n+      if (outputFileTemplate) {\\n+        let outputPath = String(EnvironmentResolver.resolveValue(outputFileTemplate));\\n+        if (outputPath.includes('{{') || outputPath.includes('{%')) {\\n+          outputPath = await this.liquid.parseAndRender(outputPath, templateContext);\\n+        }\\n+        resolvedOutputFile = outputPath.trim();\\n+\\n+        // Check if file already exists (caching)\\n+        if (skipIfExists && fs.existsSync(resolvedOutputFile)) {\\n+          const stats = fs.statSync(resolvedOutputFile);\\n+          logger.verbose(`[http_client] File cached: ${resolvedOutputFile} (${stats.size} bytes)`);\\n+          return {\\n+            issues: [],\\n+            file_path: resolvedOutputFile,\\n+            size: stats.size,\\n+            cached: true,\\n+          } as unknown as ReviewSummary;\\n+        }\\n+      }\\n \\n       // Test hook: mock HTTP response for this step\\n       const stepName = (config as any).checkName || 'unknown';\\n       const mock = context?.hooks?.mockForStep?.(String(stepName));\\n-      const data =\\n-        mock !== undefined\\n-          ? mock\\n-          : await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n \\n-      // Apply transformation if specified\\n+      // If mock is provided, return it directly as the step output (with issues: [])\\n+      if (mock !== undefined) {\\n+        const mockObj = typeof mock === 'object' && mock !== null ? mock : { data: mock };\\n+        return {\\n+          issues: [],\\n+          ...mockObj,\\n+        } as unknown as ReviewSummary & { data: unknown };\\n+      }\\n+\\n+      // Debug log the request for troubleshooting\\n+      logger.verbose(`[http_client] ${method} ${renderedUrl}`);\\n+      if (requestBody) {\\n+        logger.verbose(`[http_client] Body: ${requestBody.substring(0, 500)}${requestBody.length > 500 ? '...' : ''}`);\\n+      }\\n+\\n+      // If output_file is specified, download to file instead of returning data\\n+      if (resolvedOutputFile) {\\n+        const fileResult = await this.downloadToFile(\\n+          renderedUrl,\\n+          method,\\n+          resolvedHeaders,\\n+          requestBody,\\n+          timeout,\\n+          resolvedOutputFile\\n+        );\\n+        return fileResult;\\n+      }\\n+\\n+      const data = await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n+\\n+      // Apply Liquid transformation if specified\\n       let processedData = data;\\n       if (transform) {\\n         try {\\n@@ -136,12 +221,53 @@ export class HttpClientProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Return the fetched data as a custom field for dependent checks to access\\n+      // Apply JavaScript transformation if specified\\n+      if (transformJs) {\\n+        try {\\n+          if (!this.sandbox) {\\n+            this.sandbox = this.createSecureSandbox();\\n+          }\\n+\\n+          // Create scope for JavaScript transform (scope, not context)\\n+          const jsScope: Record<string, unknown> = {\\n+            output: data,\\n+            pr: templateContext.pr,\\n+            outputs: templateContext.outputs,\\n+            env: process.env,\\n+          };\\n+\\n+          const result = compileAndRun(this.sandbox, transformJs, jsScope, {\\n+            injectLog: true,\\n+            logPrefix: 'üîç [transform_js]',\\n+            wrapFunction: true,\\n+          });\\n+          processedData = result;\\n+          logger.verbose(`‚úì Applied JavaScript transform successfully`);\\n+        } catch (error) {\\n+          logger.error(\\n+            `‚úó Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`\\n+          );\\n+          return {\\n+            issues: [\\n+              {\\n+                file: 'http_client',\\n+                line: 0,\\n+                ruleId: 'http_client/transform_js_error',\\n+                message: `Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              },\\n+            ],\\n+          };\\n+        }\\n+      }\\n+\\n+      // Return the fetched data in the standard `output` property for guarantee evaluation\\n+      // This is consistent with other providers (script, command, ai, etc.)\\n       return {\\n         issues: [],\\n-        // Add custom data field that will be passed through to dependent checks\\n-        data: processedData,\\n-      } as ReviewSummary & { data: unknown };\\n+        output: processedData,\\n+      } as unknown as ReviewSummary;\\n     } catch (error) {\\n       return {\\n         issues: [\\n@@ -149,7 +275,7 @@ export class HttpClientProvider extends CheckProvider {\\n             file: 'http_client',\\n             line: 0,\\n             ruleId: 'http_client/fetch_error',\\n-            message: `Failed to fetch from ${url}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            message: `Failed to fetch from ${resolvedUrlForErrors}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n             severity: 'error',\\n             category: 'logic',\\n           },\\n@@ -198,7 +324,14 @@ export class HttpClientProvider extends CheckProvider {\\n \\n       clearTimeout(timeoutId);\\n \\n+      logger.verbose(`[http_client] Response: ${response.status} ${response.statusText}`);\\n+\\n       if (!response.ok) {\\n+        // Log response body for debugging auth failures\\n+        try {\\n+          const errorBody = await response.text();\\n+          logger.warn(`[http_client] Error body: ${errorBody.substring(0, 500)}`);\\n+        } catch {}\\n         throw new Error(`HTTP ${response.status}: ${response.statusText}`);\\n       }\\n \\n@@ -233,6 +366,112 @@ export class HttpClientProvider extends CheckProvider {\\n     }\\n   }\\n \\n+  private async downloadToFile(\\n+    url: string,\\n+    method: string,\\n+    headers: Record<string, string>,\\n+    body: string | undefined,\\n+    timeout: number,\\n+    outputFile: string\\n+  ): Promise<ReviewSummary> {\\n+    // Check if fetch is available (Node 18+)\\n+    if (typeof fetch === 'undefined') {\\n+      throw new Error('HTTP client provider requires Node.js 18+ or node-fetch package');\\n+    }\\n+\\n+    const controller = new AbortController();\\n+    const timeoutId = setTimeout(() => controller.abort(), timeout);\\n+\\n+    try {\\n+      const requestOptions: RequestInit = {\\n+        method,\\n+        headers: { ...headers },\\n+        signal: controller.signal,\\n+      };\\n+\\n+      // Add body for non-GET requests\\n+      if (method !== 'GET' && body) {\\n+        requestOptions.body = body;\\n+        if (!headers['Content-Type'] && !headers['content-type']) {\\n+          requestOptions.headers = {\\n+            ...requestOptions.headers,\\n+            'Content-Type': 'application/json',\\n+          };\\n+        }\\n+      }\\n+\\n+      const response = await fetch(url, requestOptions);\\n+      clearTimeout(timeoutId);\\n+\\n+      if (!response.ok) {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_error',\\n+              message: `Failed to download file: HTTP ${response.status}: ${response.statusText}`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      // Create parent directory if it doesn't exist\\n+      const parentDir = path.dirname(outputFile);\\n+      if (parentDir && !fs.existsSync(parentDir)) {\\n+        fs.mkdirSync(parentDir, { recursive: true });\\n+      }\\n+\\n+      // Get the response as an ArrayBuffer and write to file\\n+      const arrayBuffer = await response.arrayBuffer();\\n+      const buffer = Buffer.from(arrayBuffer);\\n+      fs.writeFileSync(outputFile, buffer);\\n+\\n+      const contentType = response.headers.get('content-type') || 'application/octet-stream';\\n+      logger.verbose(`[http_client] Downloaded: ${outputFile} (${buffer.length} bytes)`);\\n+\\n+      return {\\n+        issues: [],\\n+        file_path: outputFile,\\n+        size: buffer.length,\\n+        content_type: contentType,\\n+        cached: false,\\n+      } as unknown as ReviewSummary;\\n+    } catch (error: unknown) {\\n+      clearTimeout(timeoutId);\\n+\\n+      if (error instanceof Error && error.name === 'AbortError') {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_timeout',\\n+              message: `Download timed out after ${timeout}ms`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      return {\\n+        issues: [\\n+          {\\n+            file: 'http_client',\\n+            line: 0,\\n+            ruleId: 'http_client/download_error',\\n+            message: `Failed to download file: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            severity: 'error',\\n+            category: 'logic',\\n+          },\\n+        ],\\n+      };\\n+    }\\n+  }\\n+\\n   getSupportedConfigKeys(): string[] {\\n     return [\\n       'type',\\n@@ -241,7 +480,10 @@ export class HttpClientProvider extends CheckProvider {\\n       'headers',\\n       'body',\\n       'transform',\\n+      'transform_js',\\n       'timeout',\\n+      'output_file',\\n+      'skip_if_exists',\\n       'depends_on',\\n       'on',\\n       'if',\\n@@ -261,6 +503,8 @@ export class HttpClientProvider extends CheckProvider {\\n       'Network access to the endpoint',\\n       'Optional: Transform template for processing response data',\\n       'Optional: Body template for POST/PUT requests',\\n+      'Optional: output_file path to download response to a file',\\n+      'Optional: skip_if_exists (default: true) to enable caching for file downloads',\\n     ];\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/log-check-provider.ts\",\"additions\":19,\"deletions\":8,\"changes\":27,\"patch\":\"diff --git a/src/providers/log-check-provider.ts b/src/providers/log-check-provider.ts\\nindex 9c6332d1..b57f632c 100644\\n--- a/src/providers/log-check-provider.ts\\n+++ b/src/providers/log-check-provider.ts\\n@@ -78,7 +78,8 @@ export class LogCheckProvider extends CheckProvider {\\n       includeDependencies,\\n       includeMetadata,\\n       config.__outputHistory as Map<string, unknown[]> | undefined,\\n-      context\\n+      context,\\n+      config\\n     );\\n \\n     // Render the log message template\\n@@ -100,12 +101,20 @@ export class LogCheckProvider extends CheckProvider {\\n     else if (level === 'debug') logger.debug(logOutput);\\n     else logger.info(logOutput);\\n \\n-    // Return with the log content as custom data for dependent checks\\n-    return {\\n+    // Return with the log content as custom data for dependent checks.\\n+    // For chat-style logs (group: chat), also expose a structured\\n+    // `output.text` field so frontends like Slack can post a clean\\n+    // human-facing message without the level prefix.\\n+    const summary: ReviewSummary & { logOutput: string; output?: unknown } = {\\n       issues: [],\\n-      // Add log output as custom field\\n       logOutput,\\n-    } as ReviewSummary & { logOutput: string };\\n+    };\\n+\\n+    if ((config as any).group === 'chat') {\\n+      (summary as any).output = { text: renderedMessage };\\n+    }\\n+\\n+    return summary;\\n   }\\n \\n   private buildTemplateContext(\\n@@ -115,7 +124,8 @@ export class LogCheckProvider extends CheckProvider {\\n     _includeDependencies: boolean = true,\\n     includeMetadata: boolean = true,\\n     outputHistory?: Map<string, unknown[]>,\\n-    executionContext?: ExecutionContext\\n+    executionContext?: ExecutionContext,\\n+    config?: CheckProviderConfig\\n   ): Record<string, unknown> {\\n     const context: Record<string, unknown> = {};\\n \\n@@ -197,7 +207,8 @@ export class LogCheckProvider extends CheckProvider {\\n     }\\n \\n     // Add workflow inputs if available\\n-    const workflowInputs = executionContext?.workflowInputs || {};\\n+    // Check config first (set by projectWorkflowToGraph), then fall back to executionContext\\n+    const workflowInputs = (config as any)?.workflowInputs || executionContext?.workflowInputs || {};\\n     logger.debug(\\n       `[LogProvider] Adding ${Object.keys(workflowInputs).length} workflow inputs to context`\\n     );\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/script-check-provider.ts\",\"additions\":37,\"deletions\":1,\"changes\":38,\"patch\":\"diff --git a/src/providers/script-check-provider.ts b/src/providers/script-check-provider.ts\\nindex 9825bb69..947dbd73 100644\\n--- a/src/providers/script-check-provider.ts\\n+++ b/src/providers/script-check-provider.ts\\n@@ -59,6 +59,16 @@ export class ScriptCheckProvider extends CheckProvider {\\n       reuseSession?: boolean;\\n     } & import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n+    // Test hook: mock output for this step (short-circuit execution)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = _sessionInfo?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        // Return mock directly as step output\\n+        return { issues: [], output: mock } as ReviewSummary & { output: unknown };\\n+      }\\n+    } catch {}\\n+\\n     const script = String(config.content || '');\\n     const memoryStore = MemoryStore.getInstance();\\n     const ctx = buildProviderTemplateContext(\\n@@ -72,10 +82,36 @@ export class ScriptCheckProvider extends CheckProvider {\\n     // Keep provider quiet by default; no step-specific debug\\n     // (historical ad-hoc logs removed to avoid hardcoding step names).\\n \\n+    // Add workflow inputs to the context\\n+    const inputs = (config as any).workflowInputs || _sessionInfo?.workflowInputs || {};\\n+    (ctx as any).inputs = inputs;\\n+\\n+    // Add environment variables to context (consistent with http-client-provider)\\n+    (ctx as any).env = process.env;\\n+\\n     // Attach synchronous memory ops consistent with memory provider\\n     const { ops, needsSave } = createSyncMemoryOps(memoryStore);\\n     (ctx as any).memory = ops as unknown as Record<string, unknown>;\\n \\n+    // Add helper functions to the context\\n+    (ctx as any).escapeXml = (str: unknown): string => {\\n+      if (str == null) return '';\\n+      return String(str)\\n+        .replace(/&/g, '&amp;')\\n+        .replace(/</g, '&lt;')\\n+        .replace(/>/g, '&gt;')\\n+        .replace(/\\\"/g, '&quot;')\\n+        .replace(/'/g, '&apos;');\\n+    };\\n+\\n+    // Add btoa/atob for base64 encoding/decoding (browser API polyfill)\\n+    (ctx as any).btoa = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'binary').toString('base64');\\n+    };\\n+    (ctx as any).atob = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'base64').toString('binary');\\n+    };\\n+\\n     // Evaluate the script in a secure sandbox (per-execution instance)\\n     const sandbox = this.createSecureSandbox();\\n     let result: unknown;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/workflow-check-provider.ts\",\"additions\":164,\"deletions\":43,\"changes\":207,\"patch\":\"diff --git a/src/providers/workflow-check-provider.ts b/src/providers/workflow-check-provider.ts\\nindex 116d4dff..681703ce 100644\\n--- a/src/providers/workflow-check-provider.ts\\n+++ b/src/providers/workflow-check-provider.ts\\n@@ -67,34 +67,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n   ): Promise<ReviewSummary> {\\n     const cfg = config as CheckProviderConfig & { workflow?: string; config?: string };\\n     const isConfigPathMode = !!cfg.config && !cfg.workflow;\\n+    const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n \\n-    // Test harness support: allow mocking workflow checks as a black box.\\n-    // If a mock is provided for this step, short-circuit nested execution and\\n-    // return a ReviewSummary with optional output field.\\n-    try {\\n-      const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n-      // test-runner passes hooks on execution context\\n-      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n-      if (mock !== undefined) {\\n-        const ms = mock as any;\\n-        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n-        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n-        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n-        const summary: ReviewSummary & { output?: unknown } = {\\n-          issues: issuesArr,\\n-          output: out,\\n-          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n-        } as any;\\n-        return summary;\\n-      }\\n-    } catch {}\\n-\\n-    // Resolve workflow definition\\n+    // Resolve workflow definition FIRST (needed for input preparation and validation)\\n     let workflow: WorkflowDefinition | undefined;\\n     let workflowId = cfg.workflow as string | undefined;\\n \\n     if (isConfigPathMode) {\\n-      const parentCwd = ((context as any)?._parentContext?.workingDirectory ||\\n+      // Use originalWorkingDirectory for config file resolution - workflow configs\\n+      // should be loaded from the original project path, not the sandbox\\n+      const parentCwd = ((context as any)?._parentContext?.originalWorkingDirectory ||\\n+        (context as any)?._parentContext?.workingDirectory ||\\n+        (context as any)?.originalWorkingDirectory ||\\n         (context as any)?.workingDirectory ||\\n         process.cwd()) as string;\\n       workflow = await this.loadWorkflowFromConfigPath(String(cfg.config), parentCwd);\\n@@ -109,9 +93,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       logger.info(`Executing workflow '${workflowId}'`);\\n     }\\n \\n-    // Prepare inputs\\n+    // Prepare inputs - do this BEFORE mock check so we can validate inputs even when mocked\\n+    // This allows tests to assert that the correct inputs were passed to workflow steps\\n     const inputs = await this.prepareInputs(workflow, config, prInfo, dependencyResults);\\n \\n+    // Capture resolved workflow inputs for testing assertions (reuse prompt capture infrastructure)\\n+    // This allows using `prompts` assertions with `provider: 'workflow'` to verify inputs\\n+    try {\\n+      // Serialize inputs to capture for assertions - specifically the context field\\n+      // which contains the rendered template with dependency outputs\\n+      const inputsCapture = Object.entries(inputs)\\n+        .map(([k, v]) => `${k}: ${typeof v === 'string' ? v : JSON.stringify(v)}`)\\n+        .join('\\\\n\\\\n');\\n+      (context as any)?.hooks?.onPromptCaptured?.({\\n+        step: String(stepName),\\n+        provider: 'workflow',\\n+        prompt: inputsCapture,\\n+      });\\n+    } catch {\\n+      // Ignore capture errors - this is only for testing\\n+    }\\n+\\n     // Validate inputs\\n     const validation = this.registry.validateInputs(workflow, inputs);\\n     if (!validation.valid) {\\n@@ -119,6 +121,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       throw new Error(`Invalid workflow inputs: ${errors}`);\\n     }\\n \\n+    // Test harness support: allow mocking workflow checks as a black box.\\n+    // If a mock is provided for this step, short-circuit nested execution and\\n+    // return a ReviewSummary with optional output field.\\n+    // NOTE: This happens AFTER input preparation/validation so tests can assert inputs are correct\\n+    try {\\n+      // test-runner passes hooks on execution context\\n+      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        const ms = mock as any;\\n+        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n+        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n+        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n+        const summary: ReviewSummary & { output?: unknown } = {\\n+          issues: issuesArr,\\n+          output: out,\\n+          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n+        } as any;\\n+        return summary;\\n+      }\\n+    } catch {}\\n+\\n     // Apply overrides to workflow steps if specified\\n     const modifiedWorkflow = this.applyOverrides(workflow, config);\\n \\n@@ -211,6 +234,80 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Extract eventContext for slack/conversation\\n+    const eventContext = config.eventContext || {};\\n+\\n+    // Debug logging for conversation context\\n+    logger.debug(`[WorkflowProvider] prepareInputs for ${workflow.id}`);\\n+    logger.debug(`[WorkflowProvider] eventContext keys: ${Object.keys(eventContext).join(', ') || 'none'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.slack: ${eventContext.slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.conversation: ${(eventContext as any).conversation ? 'present' : 'absent'}`);\\n+\\n+    // Extract slack context (if provided via eventContext.slack)\\n+    const slack = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        const slackCtx = anyCtx?.slack;\\n+        if (slackCtx && typeof slackCtx === 'object') return slackCtx;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Extract unified conversation context across transports (Slack & GitHub)\\n+    const conversation = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        if (anyCtx?.slack?.conversation) return anyCtx.slack.conversation;\\n+        if (anyCtx?.github?.conversation) return anyCtx.github.conversation;\\n+        if (anyCtx?.conversation) return anyCtx.conversation;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Debug logging for extracted context\\n+    logger.debug(`[WorkflowProvider] slack extracted: ${slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] conversation extracted: ${conversation ? 'present' : 'absent'}`);\\n+    if (conversation) {\\n+      logger.debug(`[WorkflowProvider] conversation.messages count: ${Array.isArray((conversation as any).messages) ? (conversation as any).messages.length : 0}`);\\n+    }\\n+\\n+    // Extract output history from config (passed via __outputHistory)\\n+    const outputHistory = (config as any).__outputHistory as Map<string, unknown[]> | undefined;\\n+    const outputs_history: Record<string, unknown[]> = {};\\n+    if (outputHistory) {\\n+      for (const [k, v] of outputHistory.entries()) {\\n+        outputs_history[k] = v;\\n+      }\\n+    }\\n+\\n+    // Build template context with all available data\\n+    // Extract .output from each dependency result so that outputs['step-name'].field works naturally\\n+    // (not outputs['step-name'].output.field)\\n+    const outputsMap: Record<string, unknown> = {};\\n+    logger.debug(`[WorkflowProvider] dependencyResults: ${dependencyResults ? dependencyResults.size : 'undefined'} entries`);\\n+    if (dependencyResults) {\\n+      for (const [key, result] of dependencyResults.entries()) {\\n+        // Extract the output property, or use the whole result if output is undefined\\n+        const extracted = (result as any).output ?? result;\\n+        outputsMap[key] = extracted;\\n+        // Debug: log what we extracted for each dependency\\n+        const extractedKeys = extracted && typeof extracted === 'object' ? Object.keys(extracted).join(', ') : 'not-object';\\n+        logger.debug(`[WorkflowProvider] outputs['${key}']: keys=[${extractedKeys}]`);\\n+      }\\n+    }\\n+    const templateContext = {\\n+      pr: prInfo,\\n+      outputs: outputsMap,\\n+      env: process.env,\\n+      slack,\\n+      conversation,\\n+      outputs_history,\\n+    };\\n+\\n     // Apply user-provided inputs (args)\\n     const userInputs = config.args || config.workflow_inputs; // Support both for compatibility\\n     if (userInputs) {\\n@@ -219,11 +316,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         if (typeof value === 'string') {\\n           // Check if it's a Liquid template\\n           if (value.includes('{{') || value.includes('{%')) {\\n-            inputs[key] = await this.liquid.parseAndRender(value, {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            });\\n+            inputs[key] = await this.liquid.parseAndRender(value, templateContext);\\n+            // Debug: log rendered template value for important inputs\\n+            if (key === 'text' || key === 'question' || key === 'context') {\\n+              const rendered = String(inputs[key]);\\n+              logger.info(`[WorkflowProvider] Rendered '${key}' input (${rendered.length} chars): ${rendered.substring(0, 500)}${rendered.length > 500 ? '...' : ''}`);\\n+            }\\n           } else {\\n             inputs[key] = value;\\n           }\\n@@ -234,11 +332,7 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           inputs[key] = compileAndRun(\\n             sandbox,\\n             exprValue.expression,\\n-            {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            },\\n+            templateContext,\\n             { injectLog: true, logPrefix: `workflow.input.${key}` }\\n           );\\n         } else {\\n@@ -367,6 +461,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n     try {\\n       await childMemory.initialize();\\n     } catch {}\\n+    // Enhanced debug logging for workspace propagation diagnosis\\n+    const parentWorkspace = parentContext?.workspace;\\n+    logger.info(`[WorkflowProvider] Workspace propagation for nested workflow '${workflow.id}':`);\\n+    logger.info(`[WorkflowProvider]   parentContext exists: ${!!parentContext}`);\\n+    logger.info(`[WorkflowProvider]   parentContext.workspace exists: ${!!parentWorkspace}`);\\n+    if (parentWorkspace) {\\n+      logger.info(`[WorkflowProvider]   parentWorkspace.isEnabled(): ${parentWorkspace.isEnabled?.() ?? 'N/A'}`);\\n+      const projectCount = parentWorkspace.listProjects?.()?.length ?? 'N/A';\\n+      logger.info(`[WorkflowProvider]   parentWorkspace project count: ${projectCount}`);\\n+    } else {\\n+      logger.warn(`[WorkflowProvider]   NO WORKSPACE from parent - nested checkouts won't be added to workspace!`);\\n+    }\\n \\n     const childContext = {\\n       mode: 'state-machine' as const,\\n@@ -374,7 +480,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       checks: checksMetadata,\\n       journal: childJournal,\\n       memory: childMemory,\\n+      // For nested workflows we continue to execute inside the same logical\\n+      // working directory as the parent run. When workspace isolation is\\n+      // enabled on the parent engine, its WorkspaceManager is also propagated\\n+      // so that nested checks (AI, git-checkout, etc.) see the same isolated\\n+      // workspace and project symlinks instead of falling back to the Visor\\n+      // repository root.\\n       workingDirectory: parentContext?.workingDirectory || process.cwd(),\\n+      originalWorkingDirectory:\\n+        parentContext?.originalWorkingDirectory ||\\n+        parentContext?.workingDirectory ||\\n+        process.cwd(),\\n+      workspace: parentWorkspace,\\n       // Always use a fresh session for nested workflows to isolate history\\n       sessionId: uuidv4(),\\n       event: parentContext?.event || prInfo.eventType,\\n@@ -500,6 +617,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     } catch {}\\n \\n+    // Create outputs map that directly exposes .output values (not wrapped in { output, issues })\\n+    // This makes {{ outputs['check-name'] }} work naturally without needing .output\\n+    const outputsMap = Object.fromEntries(\\n+      Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n+    );\\n+\\n     for (const output of workflow.outputs) {\\n       if (output.value_js) {\\n         // JavaScript expression\\n@@ -508,10 +631,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           output.value_js,\\n           {\\n             inputs,\\n-            steps: Object.fromEntries(\\n-              Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-            ),\\n-            outputs: flat,\\n+            outputs: outputsMap,\\n+            // Keep 'steps' as alias for backwards compatibility\\n+            steps: outputsMap,\\n             pr: prInfo,\\n           },\\n           { injectLog: true, logPrefix: `workflow.output.${output.name}` }\\n@@ -520,10 +642,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         // Liquid template\\n         outputs[output.name] = await this.liquid.parseAndRender(output.value, {\\n           inputs,\\n-          steps: Object.fromEntries(\\n-            Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-          ),\\n-          outputs: flat,\\n+          outputs: outputsMap,\\n+          // Keep 'steps' as alias for backwards compatibility\\n+          steps: outputsMap,\\n           pr: prInfo,\\n         });\\n       }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/client.ts\",\"additions\":65,\"deletions\":1,\"changes\":66,\"patch\":\"diff --git a/src/slack/client.ts b/src/slack/client.ts\\nindex 292cc358..435f8b0d 100644\\n--- a/src/slack/client.ts\\n+++ b/src/slack/client.ts\\n@@ -142,6 +142,70 @@ export class SlackClient {\\n     }\\n   }\\n \\n+  public readonly files = {\\n+    /**\\n+     * Upload a file to Slack using files.uploadV2 API\\n+     * @param options Upload options including file content, filename, channel, and thread_ts\\n+     */\\n+    uploadV2: async ({\\n+      content,\\n+      filename,\\n+      channel,\\n+      thread_ts,\\n+      title,\\n+      initial_comment,\\n+    }: {\\n+      content: Buffer;\\n+      filename: string;\\n+      channel: string;\\n+      thread_ts?: string;\\n+      title?: string;\\n+      initial_comment?: string;\\n+    }): Promise<{ ok: boolean; file?: { id: string; permalink?: string } }> => {\\n+      try {\\n+        // Step 1: Get upload URL\\n+        const getUrlResp: any = await this.api('files.getUploadURLExternal', {\\n+          filename,\\n+          length: content.length,\\n+        });\\n+        if (!getUrlResp || getUrlResp.ok !== true || !getUrlResp.upload_url) {\\n+          console.warn(`Slack files.getUploadURLExternal failed: ${getUrlResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 2: Upload file content to the URL\\n+        const uploadResp = await fetch(getUrlResp.upload_url, {\\n+          method: 'POST',\\n+          body: content,\\n+        });\\n+        if (!uploadResp.ok) {\\n+          console.warn(`Slack file upload to URL failed: ${uploadResp.status}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 3: Complete the upload and share to channel\\n+        const completeResp: any = await this.api('files.completeUploadExternal', {\\n+          files: [{ id: getUrlResp.file_id, title: title || filename }],\\n+          channel_id: channel,\\n+          thread_ts,\\n+          initial_comment,\\n+        });\\n+        if (!completeResp || completeResp.ok !== true) {\\n+          console.warn(`Slack files.completeUploadExternal failed: ${completeResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        return {\\n+          ok: true,\\n+          file: completeResp.files?.[0] || { id: getUrlResp.file_id },\\n+        };\\n+      } catch (e) {\\n+        console.warn(`Slack file upload failed: ${e instanceof Error ? e.message : String(e)}`);\\n+        return { ok: false };\\n+      }\\n+    },\\n+  };\\n+\\n   getWebClient(): any {\\n     return {\\n       conversations: {\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/markdown.ts\",\"additions\":204,\"deletions\":5,\"changes\":209,\"patch\":\"diff --git a/src/slack/markdown.ts b/src/slack/markdown.ts\\nindex cc63fb5f..9f91c25f 100644\\n--- a/src/slack/markdown.ts\\n+++ b/src/slack/markdown.ts\\n@@ -3,14 +3,191 @@\\n // without pulling in a full Markdown parser.\\n //\\n // Supported conversions:\\n+// - # Header / ## Header  ‚Üí *Header* (bold with visual separation)\\n // - **bold** / __bold__   ‚Üí *bold*\\n // - [label](url)          ‚Üí <url|label>\\n // - ![alt](url)           ‚Üí <url|alt>\\n // - *italic* (inline)     ‚Üí _italic_\\n+// - ```mermaid blocks     ‚Üí rendered to PNG and uploaded to Slack\\n //\\n // Everything else is passed through unchanged; Slack will still render many\\n // Markdown-like constructs (lists, code fences, etc.) natively.\\n \\n+import { spawn } from 'child_process';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n+import * as os from 'os';\\n+\\n+/**\\n+ * Represents an extracted mermaid diagram\\n+ */\\n+export interface MermaidDiagram {\\n+  /** The full match including ```mermaid and ``` */\\n+  fullMatch: string;\\n+  /** The mermaid code content */\\n+  code: string;\\n+  /** Start index in the original text */\\n+  startIndex: number;\\n+  /** End index in the original text */\\n+  endIndex: number;\\n+}\\n+\\n+/**\\n+ * Extract all mermaid code blocks from text\\n+ */\\n+export function extractMermaidDiagrams(text: string): MermaidDiagram[] {\\n+  const diagrams: MermaidDiagram[] = [];\\n+  // Match ```mermaid followed by newline, content, and closing ```\\n+  const regex = /```mermaid\\\\s*\\\\n([\\\\s\\\\S]*?)```/g;\\n+  let match;\\n+  while ((match = regex.exec(text)) !== null) {\\n+    diagrams.push({\\n+      fullMatch: match[0],\\n+      code: match[1].trim(),\\n+      startIndex: match.index,\\n+      endIndex: match.index + match[0].length,\\n+    });\\n+  }\\n+  return diagrams;\\n+}\\n+\\n+/**\\n+ * Render a mermaid diagram to PNG using mmdc CLI (@mermaid-js/mermaid-cli).\\n+ *\\n+ * Requirements:\\n+ * - Node.js and npx must be available in PATH\\n+ * - Network access on first run (npx downloads the package)\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ *\\n+ * On Linux, you may need to install chromium dependencies:\\n+ *   apt-get install -y chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ *\\n+ * On Docker/CI, consider using a base image with puppeteer support or\\n+ * pre-installing @mermaid-js/mermaid-cli globally.\\n+ *\\n+ * @param mermaidCode The mermaid diagram code\\n+ * @returns Buffer containing PNG data, or null if rendering failed\\n+ */\\n+export async function renderMermaidToPng(mermaidCode: string): Promise<Buffer | null> {\\n+  // Create temp files for input and output\\n+  const tmpDir = os.tmpdir();\\n+  const inputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.mmd`);\\n+  const outputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.png`);\\n+\\n+  try {\\n+    // Write mermaid code to temp file\\n+    fs.writeFileSync(inputFile, mermaidCode, 'utf-8');\\n+\\n+    // Detect system chromium for puppeteer (mermaid-cli dependency)\\n+    // Without this, puppeteer may hang trying to download its own chromium\\n+    const chromiumPaths = [\\n+      '/usr/bin/chromium',\\n+      '/usr/bin/chromium-browser',\\n+      '/usr/bin/google-chrome',\\n+      '/usr/bin/chrome',\\n+    ];\\n+    let chromiumPath: string | undefined;\\n+    for (const p of chromiumPaths) {\\n+      if (fs.existsSync(p)) {\\n+        chromiumPath = p;\\n+        break;\\n+      }\\n+    }\\n+\\n+    // Build environment with chromium path if found\\n+    const env = { ...process.env };\\n+    if (chromiumPath) {\\n+      env.PUPPETEER_EXECUTABLE_PATH = chromiumPath;\\n+    }\\n+\\n+    // Run mmdc to render PNG\\n+    const result = await new Promise<{ success: boolean; error?: string }>((resolve) => {\\n+      const proc = spawn('npx', [\\n+        '--yes',\\n+        '@mermaid-js/mermaid-cli',\\n+        '-i', inputFile,\\n+        '-o', outputFile,\\n+        '-e', 'png',\\n+        '-b', 'white',\\n+        '-w', '1200',\\n+      ], {\\n+        timeout: 60000, // 60 second timeout (first run may download packages)\\n+        stdio: ['pipe', 'pipe', 'pipe'],\\n+        env,\\n+      });\\n+\\n+      let stderr = '';\\n+      proc.stderr?.on('data', (data) => {\\n+        stderr += data.toString();\\n+      });\\n+\\n+      proc.on('close', (code) => {\\n+        if (code === 0) {\\n+          resolve({ success: true });\\n+        } else {\\n+          resolve({ success: false, error: stderr || `Exit code ${code}` });\\n+        }\\n+      });\\n+\\n+      proc.on('error', (err) => {\\n+        resolve({ success: false, error: err.message });\\n+      });\\n+    });\\n+\\n+    if (!result.success) {\\n+      console.warn(`Mermaid rendering failed: ${result.error}`);\\n+      return null;\\n+    }\\n+\\n+    // Read the output PNG\\n+    if (!fs.existsSync(outputFile)) {\\n+      console.warn('Mermaid output file not created');\\n+      return null;\\n+    }\\n+\\n+    const pngBuffer = fs.readFileSync(outputFile);\\n+    return pngBuffer;\\n+  } catch (e) {\\n+    console.warn(`Mermaid rendering error: ${e instanceof Error ? e.message : String(e)}`);\\n+    return null;\\n+  } finally {\\n+    // Cleanup temp files\\n+    try {\\n+      if (fs.existsSync(inputFile)) fs.unlinkSync(inputFile);\\n+      if (fs.existsSync(outputFile)) fs.unlinkSync(outputFile);\\n+    } catch {\\n+      // Ignore cleanup errors\\n+    }\\n+  }\\n+}\\n+\\n+/**\\n+ * Replace mermaid blocks in text with a placeholder message\\n+ * @param text Original text\\n+ * @param diagrams Extracted diagrams\\n+ * @param replacement Text to replace each diagram with (or a function that returns replacement for each index)\\n+ */\\n+export function replaceMermaidBlocks(\\n+  text: string,\\n+  diagrams: MermaidDiagram[],\\n+  replacement: string | ((index: number) => string) = '_(See diagram above)_'\\n+): string {\\n+  if (diagrams.length === 0) return text;\\n+\\n+  // Sort by start index descending to replace from end to start (preserves indices)\\n+  const sorted = [...diagrams].sort((a, b) => b.startIndex - a.startIndex);\\n+\\n+  let result = text;\\n+  sorted.forEach((diagram, sortedIndex) => {\\n+    // Calculate original index (since we sorted in reverse)\\n+    const originalIndex = diagrams.length - 1 - sortedIndex;\\n+    const rep = typeof replacement === 'function' ? replacement(originalIndex) : replacement;\\n+    result = result.slice(0, diagram.startIndex) + rep + result.slice(diagram.endIndex);\\n+  });\\n+\\n+  return result;\\n+}\\n+\\n export function markdownToSlack(text: string): string {\\n   if (!text || typeof text !== 'string') return '';\\n \\n@@ -33,7 +210,7 @@ export function markdownToSlack(text: string): string {\\n   out = out.replace(/\\\\*\\\\*([^*]+)\\\\*\\\\*/g, (_m, inner: string) => `*${inner}*`);\\n   out = out.replace(/__([^_]+)__/g, (_m, inner: string) => `*${inner}*`);\\n \\n-  // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation).\\n+  // Process lines for headers and bullet lists.\\n   // Slack's mrkdwn handles \\\"‚Ä¢\\\" bullets more naturally than raw \\\"-\\\" Markdown.\\n   const lines = out.split(/\\\\r?\\\\n/);\\n   let inCodeBlock = false;\\n@@ -46,9 +223,31 @@ export function markdownToSlack(text: string): string {\\n       continue;\\n     }\\n     if (inCodeBlock) continue;\\n-    const match = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n-    if (match) {\\n-      const [, indent, , rest] = match;\\n+\\n+    // Headers: # Header ‚Üí *Header* (Slack doesn't have native headers)\\n+    // Match 1-6 # at start of line, followed by space and text\\n+    const headerMatch = /^(#{1,6})\\\\s+(.+)$/.exec(trimmed);\\n+    if (headerMatch) {\\n+      const [, hashes, headerText] = headerMatch;\\n+      // For h1/h2, add extra emphasis with newline before (if not first line\\n+      // and previous line is not empty/header/code-fence)\\n+      const prevLine = i > 0 ? lines[i - 1].trim() : '';\\n+      const prevIsHeaderOrFence =\\n+        /^#{1,6}\\\\s+/.test(prevLine) ||\\n+        /^\\\\*[^*]+\\\\*$/.test(prevLine) ||\\n+        /^```/.test(prevLine);\\n+      if (hashes.length <= 2 && i > 0 && prevLine !== '' && !prevIsHeaderOrFence) {\\n+        lines[i] = `\\\\n*${headerText.trim()}*`;\\n+      } else {\\n+        lines[i] = `*${headerText.trim()}*`;\\n+      }\\n+      continue;\\n+    }\\n+\\n+    // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation)\\n+    const bulletMatch = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n+    if (bulletMatch) {\\n+      const [, indent, , rest] = bulletMatch;\\n       lines[i] = `${indent}‚Ä¢ ${rest}`;\\n     }\\n   }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/prompt-state.ts\",\"additions\":19,\"deletions\":2,\"changes\":21,\"patch\":\"diff --git a/src/slack/prompt-state.ts b/src/slack/prompt-state.ts\\nindex a13233b2..544777c6 100644\\n--- a/src/slack/prompt-state.ts\\n+++ b/src/slack/prompt-state.ts\\n@@ -95,7 +95,10 @@ export class PromptStateManager {\\n   setFirstMessage(channel: string, threadTs: string, text: string): void {\\n     const key = this.key(channel, threadTs);\\n     if (!text || !text.trim()) return;\\n-    if (!this.firstMessage.has(key)) {\\n+    const existing = this.firstMessage.get(key);\\n+    // Only set if: no entry exists OR the existing entry was already consumed\\n+    // This allows new messages to be captured after a resume cycle\\n+    if (!existing || existing.consumed) {\\n       this.firstMessage.set(key, { text, consumed: false });\\n     }\\n   }\\n@@ -130,6 +133,20 @@ export class PromptStateManager {\\n         removed++;\\n       }\\n     }\\n+    // Also clean up stale firstMessage entries (consumed entries older than TTL)\\n+    // Keep unconsumed entries to avoid losing user messages\\n+    for (const [key] of this.firstMessage.entries()) {\\n+      const waitingInfo = this.waiting.get(key);\\n+      // If no corresponding waiting entry exists and the firstMessage was consumed,\\n+      // the conversation is likely complete - safe to remove\\n+      if (!waitingInfo) {\\n+        const entry = this.firstMessage.get(key);\\n+        if (entry?.consumed) {\\n+          this.firstMessage.delete(key);\\n+          removed++;\\n+        }\\n+      }\\n+    }\\n     if (removed) {\\n       try {\\n         logger.info(`[prompt-state] cleanup removed ${removed} entries`);\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/socket-runner.ts\",\"additions\":56,\"deletions\":32,\"changes\":88,\"patch\":\"diff --git a/src/slack/socket-runner.ts b/src/slack/socket-runner.ts\\nindex d1681a03..5b461d22 100644\\n--- a/src/slack/socket-runner.ts\\n+++ b/src/slack/socket-runner.ts\\n@@ -7,6 +7,7 @@ import { SlackAdapter } from './adapter';\\n import { CachePrewarmer } from './cache-prewarmer';\\n import { RateLimiter, type RateLimitConfig } from './rate-limiter';\\n import type { SlackBotConfig } from '../types/bot';\\n+import { withActiveSpan } from '../telemetry/trace-helpers';\\n \\n type SlackSocketConfig = {\\n   appToken?: string; // xapp- token\\n@@ -30,6 +31,7 @@ export class SlackSocketRunner {\\n   private botUserId?: string;\\n   private processedKeys: Map<string, number> = new Map();\\n   private adapter?: SlackAdapter;\\n+  private retryCount = 0;\\n \\n   constructor(engine: StateMachineExecutionEngine, cfg: VisorConfig, opts: SlackSocketConfig) {\\n     const app = opts.appToken || process.env.SLACK_APP_TOKEN || '';\\n@@ -107,7 +109,10 @@ export class SlackSocketRunner {\\n \\n   private async connect(url: string): Promise<void> {\\n     this.ws = new WebSocket(url);\\n-    this.ws.on('open', () => logger.info('[SlackSocket] WebSocket connected'));\\n+    this.ws.on('open', () => {\\n+      this.retryCount = 0; // Reset on successful connection\\n+      logger.info('[SlackSocket] WebSocket connected');\\n+    });\\n     this.ws.on('close', (code, reason) => {\\n       logger.warn(`[SlackSocket] WebSocket closed: ${code} ${reason}`);\\n       setTimeout(() => this.restart().catch(() => {}), 1000);\\n@@ -123,7 +128,13 @@ export class SlackSocketRunner {\\n       const url = await this.openConnection();\\n       await this.connect(url);\\n     } catch (e) {\\n-      logger.error(`[SlackSocket] Restart failed: ${e instanceof Error ? e.message : e}`);\\n+      this.retryCount++;\\n+      // Exponential backoff: 2s, 4s, 8s, 16s, 32s, capped at 60s\\n+      const delay = Math.min(2000 * Math.pow(2, this.retryCount - 1), 60000);\\n+      logger.error(\\n+        `[SlackSocket] Restart failed (attempt ${this.retryCount}), retrying in ${Math.round(delay / 1000)}s: ${e instanceof Error ? e.message : e}`\\n+      );\\n+      setTimeout(() => this.restart().catch(() => {}), delay);\\n     }\\n   }\\n \\n@@ -296,36 +307,49 @@ export class SlackSocketRunner {\\n           });\\n         }\\n       } catch {}\\n-      if (path) {\\n-        try {\\n-          const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n-          const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n-          logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n-          await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n-            webhookContext: { webhookData: map, eventType: 'manual' },\\n-            debug: process.env.VISOR_DEBUG === 'true',\\n-          });\\n-          if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n-          return; // resume path handled\\n-        } catch (e) {\\n-          logger.warn(\\n-            `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n-              e instanceof Error ? e.message : String(e)\\n-            }`\\n-          );\\n-        }\\n-      }\\n+      try {\\n+        await withActiveSpan(\\n+          'visor.run',\\n+          {\\n+            'visor.run.source': 'slack',\\n+            'slack.event.type': String(type || ''),\\n+            'slack.channel': channelId,\\n+            'slack.thread_ts': threadTs,\\n+          },\\n+          async () => {\\n+            if (path) {\\n+              try {\\n+                const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n+                const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n+                logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n+                await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n+                  webhookContext: { webhookData: map, eventType: 'manual' },\\n+                  debug: process.env.VISOR_DEBUG === 'true',\\n+                });\\n+                return;\\n+              } catch (e) {\\n+                logger.warn(\\n+                  `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n+                    e instanceof Error ? e.message : String(e)\\n+                  }`\\n+                );\\n+              }\\n+            }\\n \\n-      // Cold run (no snapshot)\\n-      await runEngine.executeChecks({\\n-        checks: allChecks,\\n-        showDetails: true,\\n-        outputFormat: 'json',\\n-        config: cfgForRun,\\n-        webhookContext: { webhookData: map, eventType: 'manual' },\\n-        debug: process.env.VISOR_DEBUG === 'true',\\n-      } as any);\\n-      if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+            // Cold run (no snapshot)\\n+            await runEngine.executeChecks({\\n+              checks: allChecks,\\n+              showDetails: true,\\n+              outputFormat: 'json',\\n+              config: cfgForRun,\\n+              webhookContext: { webhookData: map, eventType: 'manual' },\\n+              debug: process.env.VISOR_DEBUG === 'true',\\n+            } as any);\\n+          }\\n+        );\\n+      } finally {\\n+        if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+      }\\n     } catch (e) {\\n       logger.error(\\n         `[SlackSocket] Engine execution failed: ${e instanceof Error ? e.message : String(e)}`\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/context/build-engine-context.ts\",\"additions\":11,\"deletions\":4,\"changes\":15,\"patch\":\"diff --git a/src/state-machine/context/build-engine-context.ts b/src/state-machine/context/build-engine-context.ts\\nindex 149c73bf..45e63b6a 100644\\n--- a/src/state-machine/context/build-engine-context.ts\\n+++ b/src/state-machine/context/build-engine-context.ts\\n@@ -57,7 +57,8 @@ export function buildEngineContextForRun(\\n       ) as EventTrigger[],\\n       group: checkConfig.group,\\n       providerType: checkConfig.type || 'ai',\\n-      dependencies: checkConfig.depends_on || [],\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: Array.isArray(checkConfig.depends_on) ? checkConfig.depends_on : checkConfig.depends_on ? [checkConfig.depends_on] : [],\\n     };\\n   }\\n \\n@@ -134,11 +135,14 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n   const originalPath = context.workingDirectory || process.cwd();\\n \\n   try {\\n+    // Check if workspace should be kept (for debugging)\\n+    const keepWorkspace = process.env.VISOR_KEEP_WORKSPACE === 'true';\\n+\\n     // Create workspace manager\\n     const workspace = WorkspaceManager.getInstance(context.sessionId, originalPath, {\\n       enabled: true,\\n-      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH,\\n-      cleanupOnExit: workspaceConfig?.cleanup_on_exit !== false,\\n+      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH || '/tmp/visor-workspaces',\\n+      cleanupOnExit: keepWorkspace ? false : workspaceConfig?.cleanup_on_exit !== false,\\n     });\\n \\n     // Initialize workspace (creates main project worktree)\\n@@ -151,6 +155,9 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n \\n     logger.info(`[Workspace] Initialized workspace: ${info.workspacePath}`);\\n     logger.debug(`[Workspace] Main project at: ${info.mainProjectPath}`);\\n+    if (keepWorkspace) {\\n+      logger.info(`[Workspace] Keeping workspace after execution (--keep-workspace)`);\\n+    }\\n \\n     return context;\\n   } catch (error) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/execution-invoker.ts\",\"additions\":84,\"deletions\":4,\"changes\":88,\"patch\":\"diff --git a/src/state-machine/dispatch/execution-invoker.ts b/src/state-machine/dispatch/execution-invoker.ts\\nindex eb7587d3..798ca8e4 100644\\n--- a/src/state-machine/dispatch/execution-invoker.ts\\n+++ b/src/state-machine/dispatch/execution-invoker.ts\\n@@ -356,10 +356,17 @@ export async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -413,6 +420,31 @@ export async function executeSingleCheck(\\n     }\\n   }\\n \\n+  // Banner-style log when a check actually starts executing. This is emitted\\n+  // after all gating (if/depends_on) has passed so it only appears for real\\n+  // runs, and gives a clear visual separator in the logs between checks.\\n+  try {\\n+    const wave = state.wave;\\n+    const level = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${level}) ‚îÅ‚îÅ‚îÅ`;\\n+\\n+    // When running in a TTY, colour the entire banner line for extra\\n+    // visibility; keep plain text for JSON/SARIF or non-TTY environments.\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   let forEachParent: string | undefined;\\n   let forEachItems: unknown[] | undefined;\\n   for (const depId of depList) {\\n@@ -563,13 +595,56 @@ export async function executeSingleCheck(\\n       files: [],\\n       commits: [],\\n     };\\n+    // Derive AI session reuse context for this check (self-mode only for now).\\n+    // When reuse_ai_session: 'self' is configured, look up the last root-scope\\n+    // journal entry for this check in the current engine session and expose its\\n+    // sessionId via ExecutionContext so ai-check-provider can call\\n+    // executeReviewWithSessionReuse() against the same ProbeAgent session.\\n+    let parentSessionId: string | undefined;\\n+    let reuseSession = false;\\n+    try {\\n+      const reuseCfg: unknown = (checkConfig as any).reuse_ai_session;\\n+      if (reuseCfg === 'self') {\\n+        const snapshotId = context.journal.beginSnapshot();\\n+        const visible = context.journal.readVisible(\\n+          context.sessionId,\\n+          snapshotId,\\n+          context.event as any\\n+        );\\n+        // Prefer the most recent root-scope result for this check\\n+        const prior = visible.filter(\\n+          e => e.checkId === checkId && (!e.scope || e.scope.length === 0)\\n+        );\\n+        if (prior.length > 0) {\\n+          const last = prior[prior.length - 1];\\n+          const sess = (last.result as any)?.sessionId;\\n+          if (typeof sess === 'string' && sess.length > 0) {\\n+            parentSessionId = sess;\\n+            reuseSession = true;\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only ‚Äì fall back to normal (non-reuse) execution on error.\\n+      parentSessionId = undefined;\\n+      reuseSession = false;\\n+    }\\n+\\n     const executionContext = {\\n       ...context.executionContext,\\n       _engineMode: context.mode,\\n       _parentContext: context,\\n       _parentState: state,\\n+      // Explicitly propagate workspace reference for nested workflows\\n+      workspace: context.workspace,\\n     };\\n \\n+    // Attach session reuse hints for providers that support them (AI, Claude Code, etc).\\n+    if (reuseSession && parentSessionId) {\\n+      (executionContext as any).parentSessionId = parentSessionId;\\n+      (executionContext as any).reuseSession = true;\\n+    }\\n+\\n     // Handle on_init lifecycle hook BEFORE main execution\\n     if (checkConfig.on_init) {\\n       try {\\n@@ -613,7 +688,12 @@ export async function executeSingleCheck(\\n \\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/foreach-processor.ts\",\"additions\":27,\"deletions\":2,\"changes\":29,\"patch\":\"diff --git a/src/state-machine/dispatch/foreach-processor.ts b/src/state-machine/dispatch/foreach-processor.ts\\nindex 336bf9df..ab630986 100644\\n--- a/src/state-machine/dispatch/foreach-processor.ts\\n+++ b/src/state-machine/dispatch/foreach-processor.ts\\n@@ -77,6 +77,26 @@ export async function executeCheckWithForEachItems(\\n   const allContents: string[] = [];\\n   const perIterationDurations: number[] = [];\\n \\n+  // Emit a banner for the forEach parent so logs clearly show when we are\\n+  // entering the aggregated execution for that step.\\n+  try {\\n+    const wave = state.wave;\\n+    const lvl = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${lvl}, forEach parent) ‚îÅ‚îÅ‚îÅ`;\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   for (let itemIndex = 0; itemIndex < forEachItems.length; itemIndex++) {\\n     const iterationStartMs = Date.now();\\n     const scope: Array<{ check: string; index: number }> = [\\n@@ -248,7 +268,12 @@ export async function executeCheckWithForEachItems(\\n \\n       const result = await withActiveSpan(\\n         `visor.check.${checkId}`,\\n-        { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+        {\\n+          'visor.check.id': checkId,\\n+          'visor.check.type': providerType,\\n+          session_id: context.sessionId,\\n+          wave: state.wave,\\n+        },\\n         async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n       );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/stats-manager.ts\",\"additions\":21,\"deletions\":1,\"changes\":22,\"patch\":\"diff --git a/src/state-machine/dispatch/stats-manager.ts b/src/state-machine/dispatch/stats-manager.ts\\nindex 8d04436e..2f317b59 100644\\n--- a/src/state-machine/dispatch/stats-manager.ts\\n+++ b/src/state-machine/dispatch/stats-manager.ts\\n@@ -44,6 +44,26 @@ export function updateStats(\\n       issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n     };\\n \\n+    // Respect explicit skip markers on the result (set by LevelDispatch).\\n+    const skippedMarker = (result as any).__skipped;\\n+    if (skippedMarker) {\\n+      stats.skipped = true;\\n+      stats.skipReason =\\n+        typeof skippedMarker === 'string'\\n+          ? (skippedMarker as any)\\n+          : (stats.skipReason as any) || 'if_condition';\\n+      // A skipped result should not contribute to run counters; reset\\n+      // them explicitly to guard against any prior values lingering\\n+      // from earlier runs of the same check.\\n+      stats.totalRuns = 0;\\n+      stats.successfulRuns = 0;\\n+      stats.failedRuns = 0;\\n+      stats.skippedRuns++;\\n+      // Do not count this as a run; keep totalRuns/failed/success unchanged.\\n+      state.stats.set(checkId, stats);\\n+      continue;\\n+    }\\n+\\n     // If this check was previously marked as skipped and now executed,\\n     // clear the skipped flag and any skip counters to ensure the run is visible.\\n     if (stats.skipped) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/runner.ts\",\"additions\":10,\"deletions\":7,\"changes\":17,\"patch\":\"diff --git a/src/state-machine/runner.ts b/src/state-machine/runner.ts\\nindex ee86b876..a13a5efd 100644\\n--- a/src/state-machine/runner.ts\\n+++ b/src/state-machine/runner.ts\\n@@ -124,14 +124,17 @@ export class StateMachineRunner {\\n    */\\n   private async executeState(state: EngineState): Promise<void> {\\n     // M4: Wrap state execution in OTEL span\\n+    const attrs: Record<string, unknown> = {\\n+      state: state,\\n+      engine_mode: this.context.mode,\\n+      wave: this.state.wave,\\n+      session_id: this.context.sessionId,\\n+    };\\n+    const waveKind = (this.state as any)?.flags?.waveKind;\\n+    if (waveKind) attrs.wave_kind = waveKind;\\n     return withActiveSpan(\\n       `engine.state.${state.toLowerCase()}`,\\n-      {\\n-        state: state,\\n-        engine_mode: this.context.mode,\\n-        wave: this.state.wave,\\n-        session_id: this.context.sessionId,\\n-      },\\n+      attrs,\\n       async () => {\\n         try {\\n           switch (state) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/level-dispatch.ts\",\"additions\":152,\"deletions\":15,\"changes\":167,\"patch\":\"diff --git a/src/state-machine/states/level-dispatch.ts b/src/state-machine/states/level-dispatch.ts\\nindex 5588b0f5..95392197 100644\\n--- a/src/state-machine/states/level-dispatch.ts\\n+++ b/src/state-machine/states/level-dispatch.ts\\n@@ -26,7 +26,7 @@ import type { CheckExecutionStats } from '../../types/execution';\\n import type { CheckProviderConfig } from '../../providers/check-provider.interface';\\n import type { CheckConfig } from '../../types/config';\\n import { handleRouting, checkLoopBudget } from './routing';\\n-import { withActiveSpan } from '../../telemetry/trace-helpers';\\n+import { withActiveSpan, setSpanAttributes, addEvent } from '../../telemetry/trace-helpers';\\n import { emitMermaidFromMarkdown } from '../../utils/mermaid-telemetry';\\n import { emitNdjsonSpanWithEvents, emitNdjsonFallback } from '../../telemetry/fallback-ndjson';\\n import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n@@ -46,6 +46,32 @@ function mapCheckNameToFocus(checkName: string): string {\\n   return focusMap[checkName] || 'all';\\n }\\n \\n+function formatScopeLabel(scope: Array<{ check: string; index: number }> | undefined): string {\\n+  if (!scope || scope.length === 0) return '';\\n+  return scope.map(item => `${item.check}:${item.index}`).join('|');\\n+}\\n+\\n+function recordOnFinishRoutingEvent(args: {\\n+  checkId: string;\\n+  action: 'run' | 'goto';\\n+  target: string;\\n+  source: 'run' | 'goto' | 'goto_js' | 'transitions';\\n+  scope?: Array<{ check: string; index: number }>;\\n+  gotoEvent?: string;\\n+}): void {\\n+  const attrs: Record<string, unknown> = {\\n+    check_id: args.checkId,\\n+    trigger: 'on_finish',\\n+    action: args.action,\\n+    target: args.target,\\n+    source: args.source,\\n+  };\\n+  const scopeLabel = formatScopeLabel(args.scope);\\n+  if (scopeLabel) attrs.scope = scopeLabel;\\n+  if (args.gotoEvent) attrs.goto_event = args.gotoEvent;\\n+  addEvent('visor.routing', attrs);\\n+}\\n+\\n /**\\n  * Build output history Map from journal for template rendering\\n  * This matches the format expected by AI providers\\n@@ -121,12 +147,13 @@ async function evaluateIfCondition(\\n         return false;\\n       }\\n     })();\\n+    // Steps with dependencies should always see outputs from all completed steps.\\n     // In forward-run waves (from on_success/on_fail goto), guards should see the\\n     // latest global outputs even if the check has no explicit dependencies.\\n     // In wave-retry (from on_finish), restrict to checks with dependencies to\\n     // avoid wrongly skipping top-level prompts like 'ask'.\\n     const useGlobalOutputs =\\n-      (useGlobalOutputsFlag && waveKind === 'forward') || (useGlobalOutputsFlag && hasDeps);\\n+      hasDeps || (useGlobalOutputsFlag && waveKind === 'forward');\\n \\n     if (useGlobalOutputs) {\\n       // Forward-run wave: allow guards to consult latest outputs from the entire\\n@@ -199,6 +226,7 @@ async function evaluateIfCondition(\\n       baseBranch: (context.prInfo as any)?.baseBranch,\\n       filesChanged: context.prInfo?.files?.map(f => f.filename),\\n       environment: envSnapshot,\\n+      workflowInputs: (context.config as any).workflow_inputs || {},\\n     };\\n \\n     const shouldRun = await evaluator.evaluateIfCondition(checkId, ifExpression, contextData);\\n@@ -238,6 +266,11 @@ export async function handleLevelDispatch(\\n   // Update current level tracking\\n   state.currentLevel = level.level;\\n   state.currentLevelChecks = new Set(level.parallel);\\n+  const levelChecksPreview = level.parallel.slice(0, 5).join(',');\\n+  setSpanAttributes({\\n+    level_size: level.parallel.length,\\n+    level_checks_preview: levelChecksPreview,\\n+  });\\n \\n   // Emit level ready event\\n   emitEvent({ type: 'LevelReady', level, wave: state.wave });\\n@@ -689,6 +722,39 @@ async function executeCheckWithForEachItems(\\n         }\\n       } catch {}\\n \\n+      // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+      // The socket-runner stores conversation data in webhookData under the endpoint key\\n+      try {\\n+        const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+        const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+        if (context.debug) {\\n+          logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+        }\\n+        if (webhookData && webhookData.size > 0) {\\n+          // Find the payload with slack_conversation\\n+          for (const payload of webhookData.values()) {\\n+            const slackConv = (payload as any)?.slack_conversation;\\n+            if (slackConv) {\\n+              // Build slack context with event and conversation\\n+              const event = (payload as any)?.event;\\n+              const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+              if (context.debug) {\\n+                logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+              }\\n+              (providerConfig as any).eventContext = {\\n+                ...(providerConfig as any).eventContext,\\n+                slack: {\\n+                  event: event || {},\\n+                  conversation: slackConv,\\n+                },\\n+                conversation: slackConv, // Also expose at top level for convenience\\n+              };\\n+              break;\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n+\\n       // Build dependency results with scope\\n       const dependencyResults = buildDependencyResultsWithScope(\\n         checkId,\\n@@ -820,15 +886,17 @@ async function executeCheckWithForEachItems(\\n       } catch {}\\n \\n       // Execute provider with telemetry\\n-      const itemResult = await withActiveSpan(\\n-        `visor.check.${checkId}`,\\n-        {\\n-          'visor.check.id': checkId,\\n-          'visor.check.type': providerType,\\n-          'visor.foreach.index': itemIndex,\\n-        },\\n-        async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n-      );\\n+    const itemResult = await withActiveSpan(\\n+      `visor.check.${checkId}`,\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        'visor.foreach.index': itemIndex,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n+      async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n+    );\\n \\n       // Enrich issues\\n       const enrichedIssues = (itemResult.issues || []).map((issue: ReviewIssue) => ({\\n@@ -942,6 +1010,7 @@ async function executeCheckWithForEachItems(\\n             const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n               previousResults: dependencyResults as any,\\n               event: context.event || 'manual',\\n+              output: output, // Pass the iteration output for guarantee evaluation\\n             } as any);\\n             if (!holds) {\\n               const issue: ReviewIssue = {\\n@@ -1311,6 +1380,13 @@ async function executeCheckWithForEachItems(\\n             // Increment loop count\\n             state.routingLoopCount++;\\n \\n+            recordOnFinishRoutingEvent({\\n+              checkId: forEachParent,\\n+              action: 'run',\\n+              target: targetCheck,\\n+              source: 'run',\\n+              scope: [],\\n+            });\\n             emitEvent({\\n               type: 'ForwardRunRequested',\\n               target: targetCheck,\\n@@ -1358,6 +1434,14 @@ async function executeCheckWithForEachItems(\\n                 return aggregatedResult; // abort further routing\\n               }\\n               state.routingLoopCount++;\\n+              recordOnFinishRoutingEvent({\\n+                checkId: forEachParent,\\n+                action: 'goto',\\n+                target: transTarget.to,\\n+                source: 'transitions',\\n+                scope: [],\\n+                gotoEvent: (transTarget as any).goto_event,\\n+              });\\n               emitEvent({\\n                 type: 'ForwardRunRequested',\\n                 target: transTarget.to,\\n@@ -1464,6 +1548,13 @@ async function executeCheckWithForEachItems(\\n           // Increment loop count\\n           state.routingLoopCount++;\\n \\n+          recordOnFinishRoutingEvent({\\n+            checkId: forEachParent,\\n+            action: 'goto',\\n+            target: gotoTarget,\\n+            source: onFinish.goto_js ? 'goto_js' : 'goto',\\n+            scope: [],\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: gotoTarget,\\n@@ -1618,10 +1709,17 @@ async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -1923,6 +2021,39 @@ async function executeSingleCheck(\\n       }\\n     } catch {}\\n \\n+    // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+    // The socket-runner stores conversation data in webhookData under the endpoint key\\n+    try {\\n+      const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+      const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+      if (context.debug) {\\n+        logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+      }\\n+      if (webhookData && webhookData.size > 0) {\\n+        // Find the payload with slack_conversation\\n+        for (const payload of webhookData.values()) {\\n+          const slackConv = (payload as any)?.slack_conversation;\\n+          if (slackConv) {\\n+            // Build slack context with event and conversation\\n+            const event = (payload as any)?.event;\\n+            const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+            if (context.debug) {\\n+              logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+            }\\n+            (providerConfig as any).eventContext = {\\n+              ...(providerConfig as any).eventContext,\\n+              slack: {\\n+                event: event || {},\\n+                conversation: slackConv,\\n+              },\\n+              conversation: slackConv, // Also expose at top level for convenience\\n+            };\\n+            break;\\n+          }\\n+        }\\n+      }\\n+    } catch {}\\n+\\n     // Build dependency results\\n     const dependencyResults = buildDependencyResults(checkId, checkConfig, context, state);\\n \\n@@ -2016,7 +2147,12 @@ async function executeSingleCheck(\\n     // Execute provider with telemetry\\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n@@ -2098,6 +2234,7 @@ async function executeSingleCheck(\\n           const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n             previousResults: dependencyResults as any,\\n             event: context.event || 'manual',\\n+            output: enrichedResult.output,\\n           } as any);\\n           if (!holds) {\\n             const issue: ReviewIssue = {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/routing.ts\",\"additions\":165,\"deletions\":1,\"changes\":166,\"patch\":\"diff --git a/src/state-machine/states/routing.ts b/src/state-machine/states/routing.ts\\nindex 5752a5bb..b1fd8654 100644\\n--- a/src/state-machine/states/routing.ts\\n+++ b/src/state-machine/states/routing.ts\\n@@ -15,6 +15,7 @@ import type { EngineContext, RunState, EngineState, EngineEvent } from '../../ty\\n import type { ReviewSummary, ReviewIssue } from '../../reviewer';\\n import type { CheckConfig, OnFailConfig, TransitionRule } from '../../types/config';\\n import { logger } from '../../logger';\\n+import { addEvent } from '../../telemetry/trace-helpers';\\n import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n import { createSecureSandbox, compileAndRun } from '../../utils/sandbox';\\n import { MemoryStore } from '../../memory-store';\\n@@ -153,6 +154,37 @@ function createMemoryHelpers() {\\n   };\\n }\\n \\n+type RoutingTrigger = 'on_success' | 'on_fail' | 'on_finish';\\n+type RoutingAction = 'run' | 'goto' | 'retry';\\n+type RoutingSource = 'run' | 'run_js' | 'goto' | 'goto_js' | 'transitions' | 'retry';\\n+\\n+function formatScopeLabel(scope: Array<{ check: string; index: number }> | undefined): string {\\n+  if (!scope || scope.length === 0) return '';\\n+  return scope.map(item => `${item.check}:${item.index}`).join('|');\\n+}\\n+\\n+function recordRoutingEvent(args: {\\n+  checkId: string;\\n+  trigger: RoutingTrigger;\\n+  action: RoutingAction;\\n+  target?: string;\\n+  source?: RoutingSource;\\n+  scope?: Array<{ check: string; index: number }>;\\n+  gotoEvent?: string;\\n+}): void {\\n+  const attrs: Record<string, unknown> = {\\n+    check_id: args.checkId,\\n+    trigger: args.trigger,\\n+    action: args.action,\\n+  };\\n+  if (args.target) attrs.target = args.target;\\n+  if (args.source) attrs.source = args.source;\\n+  const scopeLabel = formatScopeLabel(args.scope);\\n+  if (scopeLabel) attrs.scope = scopeLabel;\\n+  if (args.gotoEvent) attrs.goto_event = args.gotoEvent;\\n+  addEvent('visor.routing', attrs);\\n+}\\n+\\n /**\\n  * Handle routing state - evaluate conditions and decide next actions\\n  */\\n@@ -271,6 +303,14 @@ async function processOnFinish(\\n             { check: checkId, index: itemIndex },\\n           ];\\n \\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_finish',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -284,6 +324,14 @@ async function processOnFinish(\\n         // Increment loop count\\n         state.routingLoopCount++;\\n \\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_finish',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope: [],\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -328,6 +376,14 @@ async function processOnFinish(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_finish',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -362,6 +418,15 @@ async function processOnFinish(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_finish',\\n+        action: 'goto',\\n+        target: finishTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: finishTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: finishTransTarget.to,\\n@@ -406,6 +471,14 @@ async function processOnFinish(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_finish',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onFinish.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+    });\\n     // Enqueue forward run event\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n@@ -640,6 +713,14 @@ async function processOnSuccess(\\n             { check: checkId, index: itemIndex },\\n           ];\\n \\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_success',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -652,6 +733,14 @@ async function processOnSuccess(\\n         // Increment loop count\\n         state.routingLoopCount++;\\n \\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_success',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope,\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -695,6 +784,14 @@ async function processOnSuccess(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_success',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -728,6 +825,15 @@ async function processOnSuccess(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_success',\\n+        action: 'goto',\\n+        target: successTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: successTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: successTransTarget.to,\\n@@ -773,6 +879,15 @@ async function processOnSuccess(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_success',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onSuccess.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+      gotoEvent: onSuccess.goto_event,\\n+    });\\n     // Enqueue forward run event with optional event override\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n@@ -864,6 +979,14 @@ async function processOnFail(\\n           const itemScope: Array<{ check: string; index: number }> = [\\n             { check: checkId, index: itemIndex },\\n           ];\\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_fail',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -874,6 +997,14 @@ async function processOnFail(\\n       } else {\\n         // No forEach context: preserve current scope (if any)\\n         state.routingLoopCount++;\\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_fail',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope,\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -917,6 +1048,14 @@ async function processOnFail(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_fail',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -958,6 +1097,13 @@ async function processOnFail(\\n \\n         // Increment loop count and schedule forward run for the same check\\n         state.routingLoopCount++;\\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_fail',\\n+          action: 'retry',\\n+          source: 'retry',\\n+          scope: sc || [],\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: checkId,\\n@@ -1013,6 +1159,15 @@ async function processOnFail(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_fail',\\n+        action: 'goto',\\n+        target: failTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: failTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: failTransTarget.to,\\n@@ -1058,6 +1213,15 @@ async function processOnFail(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_fail',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onFail.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+      gotoEvent: onFail.goto_event,\\n+    });\\n     // Enqueue forward run event with optional event override\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/wave-planning.ts\",\"additions\":10,\"deletions\":3,\"changes\":13,\"patch\":\"diff --git a/src/state-machine/states/wave-planning.ts b/src/state-machine/states/wave-planning.ts\\nindex 7551d55c..7b42f043 100644\\n--- a/src/state-machine/states/wave-planning.ts\\n+++ b/src/state-machine/states/wave-planning.ts\\n@@ -351,8 +351,11 @@ export async function handleWavePlanning(\\n     for (const id of checksToRun) {\\n       // Only include dependencies that are within the same subset (often none).\\n       const cfg = context.config.checks?.[id];\\n-      const deps = (cfg?.depends_on || []).filter((d: string) => checksToRun.includes(d));\\n-      subDeps[id] = deps as string[];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = cfg?.depends_on;\\n+      const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n+      const deps = depsArray.filter((d: string) => checksToRun.includes(d));\\n+      subDeps[id] = deps;\\n     }\\n \\n     const subGraph = DependencyResolver.buildDependencyGraph(subDeps);\\n@@ -410,6 +413,10 @@ export async function handleWavePlanning(\\n     // Initialize current wave state\\n     (state as any).currentWaveCompletions = new Set<string>();\\n     (state as any).failedChecks = new Set<string>();\\n+    try {\\n+      (state as any).flags = (state as any).flags || {};\\n+      (state as any).flags.waveKind = 'initial';\\n+    } catch {}\\n   }\\n \\n   // Check if there are levels to execute\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/workflow-projection.ts\",\"additions\":3,\"deletions\":2,\"changes\":5,\"patch\":\"diff --git a/src/state-machine/workflow-projection.ts b/src/state-machine/workflow-projection.ts\\nindex b2d5eb24..2bcfd858 100644\\n--- a/src/state-machine/workflow-projection.ts\\n+++ b/src/state-machine/workflow-projection.ts\\n@@ -53,7 +53,8 @@ export function projectWorkflowToGraph(\\n       triggers: step.on || workflow.on || [],\\n       group: step.group,\\n       providerType: step.type || 'ai',\\n-      dependencies: (step.depends_on || []).map(dep => dep),\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: (Array.isArray(step.depends_on) ? step.depends_on : step.depends_on ? [step.depends_on] : []).map((dep: string) => dep),\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/flow-stage.ts\",\"additions\":37,\"deletions\":4,\"changes\":41,\"patch\":\"diff --git a/src/test-runner/core/flow-stage.ts b/src/test-runner/core/flow-stage.ts\\nindex a14256c2..a587f9f1 100644\\n--- a/src/test-runner/core/flow-stage.ts\\n+++ b/src/test-runner/core/flow-stage.ts\\n@@ -352,10 +352,30 @@ export class FlowStage {\\n       } catch {}\\n       // Prefer authoritative counts from res.statistics when available, then fall back to deltas/inferred\\n       const fromResStats: Record<string, number> = {};\\n+      const resSkipped: Record<string, boolean> = {};\\n       try {\\n         for (const s of (res.statistics?.checks || []) as any[]) {\\n           const n = (s && s.checkName) || '';\\n-          if (typeof n === 'string' && n) fromResStats[n] = (s.totalRuns || 0) as number;\\n+          if (typeof n === 'string' && n) {\\n+            fromResStats[n] = (s.totalRuns || 0) as number;\\n+            resSkipped[n] = !!(s as any).skipped || !!(s as any).skipReason;\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      // Steps that have explicit expectations in this stage. We will use\\n+      // this to avoid inflating run counts (via parent-alignment heuristics)\\n+      // for checks that are not even expected in this stage.\\n+      const expectedSteps = new Set<string>();\\n+      const expectedZero = new Set<string>();\\n+      try {\\n+        const expCalls = ((stage.expect || {}).calls || []) as Array<{ step?: string }>;\\n+        for (const c of expCalls) {\\n+          if (c && typeof c.step === 'string' && c.step) expectedSteps.add(c.step);\\n+          try {\\n+            if (c && typeof c.step === 'string' && c.step && (c as any).exactly === 0)\\n+              expectedZero.add(c.step);\\n+          } catch {}\\n         }\\n       } catch {}\\n \\n@@ -410,10 +430,15 @@ export class FlowStage {\\n           const base = stageOnly ? 0 : statBase[name] || 0;\\n           const d = Math.max(0, resTotal - base);\\n           runs = d > 0 ? d : 0;\\n+          // If the engine marked this check as skipped, force runs to 0\\n+          // so parent-alignment heuristics below do not inflate it.\\n+          if (resSkipped[name]) {\\n+            runs = 0;\\n+          }\\n         } else {\\n           runs = deltaMap[name] !== undefined ? deltaMap[name] : inferred;\\n         }\\n-        if (runs === 0 && presentInResults.has(name)) runs = 1;\\n+        if (runs === 0 && presentInResults.has(name) && !resSkipped[name]) runs = 1;\\n         if (!isForEachLike && histRuns > 0) runs = histRuns;\\n         // Only use per-item history counts for non-forEach checks. For forEach parents,\\n         // use aggregated totals from res.statistics to reflect number of parent executions.\\n@@ -442,7 +467,15 @@ export class FlowStage {\\n             parentMax = Math.max(parentMax, dP);\\n           }\\n           // Apply only for non-forEach checks with no observable history in this stage\\n-          if (!isForEachLike && histRuns === 0 && histPerItemRuns === 0 && parentMax > 0) {\\n+          if (\\n+            !isForEachLike &&\\n+            histRuns === 0 &&\\n+            histPerItemRuns === 0 &&\\n+            parentMax > 0 &&\\n+            !resSkipped[name] &&\\n+            (expectedSteps.size === 0 || expectedSteps.has(name)) &&\\n+            !expectedZero.has(name)\\n+          ) {\\n             runs = Math.max(runs, parentMax);\\n           }\\n         } catch {}\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/test-execution-wrapper.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/test-runner/core/test-execution-wrapper.ts b/src/test-runner/core/test-execution-wrapper.ts\\nindex 254d0970..d0333152 100644\\n--- a/src/test-runner/core/test-execution-wrapper.ts\\n+++ b/src/test-runner/core/test-execution-wrapper.ts\\n@@ -31,6 +31,8 @@ export class TestExecutionWrapper {\\n       const prev: any = (this.engine as any).executionContext || {};\\n       const merged = {\\n         ...prev,\\n+        // Inject workflow inputs for template access via {{ inputs.* }}\\n+        workflowInputs: cfg.workflow_inputs || prev.workflowInputs || {},\\n         mode: {\\n           ...(prev.mode || {}),\\n           test: true,\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/evaluators.ts\",\"additions\":67,\"deletions\":2,\"changes\":69,\"patch\":\"diff --git a/src/test-runner/evaluators.ts b/src/test-runner/evaluators.ts\\nindex 4843163a..75e9c174 100644\\n--- a/src/test-runner/evaluators.ts\\n+++ b/src/test-runner/evaluators.ts\\n@@ -252,6 +252,69 @@ export function evaluatePrompts(\\n   }\\n }\\n \\n+/**\\n+ * Evaluate workflow_output assertions against computed workflow outputs.\\n+ * Similar to evaluateOutputs but tests workflow-level outputs (defined in outputs: section)\\n+ * rather than step outputs.\\n+ */\\n+export function evaluateWorkflowOutputs(\\n+  errors: string[],\\n+  expect: ExpectBlock,\\n+  workflowOutputs: Record<string, unknown> | undefined\\n+): void {\\n+  const expectations = (expect as any).workflow_output;\\n+  if (!Array.isArray(expectations) || expectations.length === 0) return;\\n+  if (!workflowOutputs) {\\n+    errors.push('workflow_output assertions present but no workflow outputs computed');\\n+    return;\\n+  }\\n+\\n+  for (const o of expectations) {\\n+    const path = o.path as string;\\n+    if (!path) {\\n+      errors.push('workflow_output assertion missing path');\\n+      continue;\\n+    }\\n+    const v = deepGet(workflowOutputs, path);\\n+    if (o.equals !== undefined && !deepEqual(v, o.equals)) {\\n+      errors.push(\\n+        `Workflow output ${path} expected ${JSON.stringify(o.equals)} but got ${JSON.stringify(v)}`\\n+      );\\n+    }\\n+    if (o.equalsDeep !== undefined && !deepEqual(v, o.equalsDeep)) {\\n+      errors.push(`Workflow output ${path} deepEquals failed`);\\n+    }\\n+    if (o.matches && !parseRegex(o.matches).test(String(v))) {\\n+      errors.push(`Workflow output ${path} does not match ${o.matches}`);\\n+    }\\n+    if (o.contains) {\\n+      const contents = Array.isArray(o.contains) ? o.contains : [o.contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (!strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} expected to contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.not_contains) {\\n+      const contents = Array.isArray(o.not_contains) ? o.not_contains : [o.not_contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} should not contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.contains_unordered) {\\n+      if (!Array.isArray(v)) {\\n+        errors.push(`Workflow output ${path} not an array for contains_unordered`);\\n+      } else if (!containsUnordered(v as unknown[], o.contains_unordered)) {\\n+        errors.push(`Workflow output ${path} missing elements (unordered)`);\\n+      }\\n+    }\\n+  }\\n+}\\n+\\n export function evaluateOutputs(\\n   errors: string[],\\n   expect: ExpectBlock,\\n@@ -332,7 +395,8 @@ export function evaluateCase(\\n   strict: boolean,\\n   promptsByStep: Record<string, string[]>,\\n   _results: GroupedResults,\\n-  outputHistory: Record<string, unknown[]>\\n+  outputHistory: Record<string, unknown[]>,\\n+  workflowOutputs?: Record<string, unknown>\\n ): string[] {\\n   const errors: string[] = [];\\n   const executed = buildExecutedMap(stats);\\n@@ -350,5 +414,6 @@ export function evaluateCase(\\n   evaluateNoCalls(errors, expect, executed, recorder, slackRecorder);\\n   evaluatePrompts(errors, expect, promptsByStep);\\n   evaluateOutputs(errors, expect, outputHistory);\\n+  evaluateWorkflowOutputs(errors, expect, workflowOutputs);\\n   return errors;\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/index.ts\",\"additions\":112,\"deletions\":7,\"changes\":119,\"patch\":\"diff --git a/src/test-runner/index.ts b/src/test-runner/index.ts\\nindex 29916a16..4dcc3331 100644\\n--- a/src/test-runner/index.ts\\n+++ b/src/test-runner/index.ts\\n@@ -159,6 +159,7 @@ export async function runSuites(\\n   options: {\\n     only?: string;\\n     bail?: boolean;\\n+    noMocks?: boolean;\\n     maxParallelSuites?: number;\\n     maxParallel?: number;\\n     promptMaxChars?: number;\\n@@ -225,6 +226,7 @@ export async function runSuites(\\n       const r = await runner.runCases(fp, suite as TestSuite, {\\n         only: options.only,\\n         bail: options.bail,\\n+        noMocks: options.noMocks,\\n         maxParallel: options.maxParallel,\\n         promptMaxChars: options.promptMaxChars,\\n       });\\n@@ -285,7 +287,8 @@ export class VisorTestRunner {\\n     defaultPromptCap?: number,\\n     ghRec?: { error_code?: number; timeout_ms?: number },\\n     defaultIncludeTags?: string[] | undefined,\\n-    defaultExcludeTags?: string[] | undefined\\n+    defaultExcludeTags?: string[] | undefined,\\n+    noMocks?: boolean\\n   ): {\\n     name: string;\\n     strict: boolean;\\n@@ -399,9 +402,11 @@ export class VisorTestRunner {\\n               : info.prompt;\\n           prompts[k].push(p);\\n         },\\n-        mockForStep: (step: string) => mockMgr.get(step),\\n+        // In noMocks mode, always return undefined to let real providers execute\\n+        mockForStep: (step: string) => noMocks ? undefined : mockMgr.get(step),\\n         // Ensure human-input never blocks tests: prefer case mock, then default value\\n         onHumanInput: async (req: { checkId: string; default?: string }) => {\\n+          if (noMocks) return (req.default ?? '').toString();\\n           const m = mockMgr.get(req.checkId);\\n           if (m !== undefined && m !== null) return String(m);\\n           return (req.default ?? '').toString();\\n@@ -481,6 +486,60 @@ export class VisorTestRunner {\\n     if (!checks || checks.length === 0) return;\\n     console.log(`  checks: ${checks.join(', ')}`);\\n   }\\n+  /**\\n+   * Compute workflow outputs from output definitions and step results.\\n+   * Evaluates value_js and value (Liquid) expressions to produce output values.\\n+   */\\n+  private computeWorkflowOutputs(\\n+    outputDefs: Array<{ name: string; value?: string; value_js?: string }>,\\n+    outputHistory: Record<string, unknown[]>,\\n+    results: any[]\\n+  ): Record<string, unknown> {\\n+    const computed: Record<string, unknown> = {};\\n+    // Build a steps map from outputHistory (last output for each step)\\n+    const steps: Record<string, unknown> = {};\\n+    for (const [stepId, hist] of Object.entries(outputHistory)) {\\n+      if (Array.isArray(hist) && hist.length > 0) {\\n+        steps[stepId] = hist[hist.length - 1];\\n+      }\\n+    }\\n+    // Build outputs map from history (for {{ outputs[\\\"step\\\"] }} syntax)\\n+    const outputs: Record<string, unknown> = { ...steps };\\n+\\n+    for (const outputDef of outputDefs) {\\n+      if (!outputDef.name) continue;\\n+      try {\\n+        if (outputDef.value_js) {\\n+          // Evaluate JavaScript expression\\n+          const fn = new Function('steps', 'outputs', 'results', `\\n+            try {\\n+              ${outputDef.value_js}\\n+            } catch (e) {\\n+              return undefined;\\n+            }\\n+          `);\\n+          computed[outputDef.name] = fn(steps, outputs, results);\\n+        } else if (outputDef.value) {\\n+          // Evaluate Liquid template\\n+          const { Liquid } = require('liquidjs');\\n+          const engine = new Liquid();\\n+          const rendered = engine.parseAndRenderSync(outputDef.value, { steps, outputs, results });\\n+          // Try to parse as JSON, otherwise keep as string\\n+          try {\\n+            computed[outputDef.name] = JSON.parse(rendered);\\n+          } catch {\\n+            computed[outputDef.name] = rendered.trim();\\n+          }\\n+        }\\n+      } catch (e) {\\n+        // Skip outputs that fail to compute\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.log(`  ‚ö†Ô∏è Failed to compute output '${outputDef.name}': ${e}`);\\n+        }\\n+      }\\n+    }\\n+    return computed;\\n+  }\\n   /**\\n    * Locate a tests file: explicit path > ./.visor.tests.yaml > defaults/visor.tests.yaml\\n    */\\n@@ -606,7 +665,7 @@ export class VisorTestRunner {\\n   public async runCases(\\n     testsPath: string,\\n     suite: TestSuite,\\n-    options: { only?: string; bail?: boolean; maxParallel?: number; promptMaxChars?: number }\\n+    options: { only?: string; bail?: boolean; noMocks?: boolean; maxParallel?: number; promptMaxChars?: number }\\n   ): Promise<{\\n     failures: number;\\n     results: Array<{\\n@@ -748,9 +807,14 @@ export class VisorTestRunner {\\n     const __keepAlive = setInterval(() => {}, 1000);\\n     // Header: show suite path for clarity\\n     let __suiteRel = testsPath;\\n+    const noMocksMode = options.noMocks || false;\\n     try {\\n       __suiteRel = path.relative(this.cwd, testsPath) || testsPath;\\n       console.log(`Suite: ${__suiteRel}`);\\n+      if (noMocksMode) {\\n+        console.log(this.color('üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)', '33'));\\n+        console.log(this.gray('   Step outputs will be captured and printed as suggested mocks\\\\n'));\\n+      }\\n     } catch {}\\n     const runOne = async (_case: any): Promise<{ name: string; failed: number }> => {\\n       // Case header for clarity\\n@@ -805,6 +869,11 @@ export class VisorTestRunner {\\n           cfgLocal.checks[name] = chk;\\n         }\\n       }\\n+      // Workflow testing: inject workflow_input into config for template access\\n+      const workflowInput = (_case as any).workflow_input;\\n+      if (workflowInput && typeof workflowInput === 'object') {\\n+        (cfgLocal as any).workflow_inputs = workflowInput;\\n+      }\\n       const setup = this.setupTestCase(\\n         _case,\\n         cfgLocal,\\n@@ -812,7 +881,8 @@ export class VisorTestRunner {\\n         defaultPromptCap,\\n         ghRec,\\n         defaultIncludeTags,\\n-        defaultExcludeTags\\n+        defaultExcludeTags,\\n+        noMocksMode\\n       );\\n       try {\\n         this.printSelectedChecks(setup.checksToRun);\\n@@ -826,6 +896,24 @@ export class VisorTestRunner {\\n         } catch {}\\n         const exec = await this.executeTestCase(setup, cfgLocal);\\n         const res = exec.res;\\n+\\n+        // In no-mocks mode, print captured outputs as suggested mocks\\n+        if (noMocksMode && Object.keys(exec.outHistory).length > 0) {\\n+          console.log(this.color('\\\\nüìã Suggested mocks (copy to your test case):', '36'));\\n+          console.log(this.gray('mocks:'));\\n+          for (const [stepName, outputs] of Object.entries(exec.outHistory)) {\\n+            if (!Array.isArray(outputs) || outputs.length === 0) continue;\\n+            // Use the last output for the step\\n+            const lastOutput = outputs[outputs.length - 1];\\n+            // Format as YAML with proper indentation\\n+            const yamlOutput = yaml.dump({ [stepName]: lastOutput }, { indent: 2, lineWidth: 120, noRefs: true });\\n+            // Indent the YAML output for proper nesting under 'mocks:'\\n+            const indented = yamlOutput.split('\\\\n').map(line => '  ' + line).join('\\\\n');\\n+            console.log(indented);\\n+          }\\n+          console.log('');\\n+        }\\n+\\n         if (process.env.VISOR_DEBUG === 'true') {\\n           try {\\n             const names = (res.statistics.checks || []).map(\\n@@ -836,6 +924,21 @@ export class VisorTestRunner {\\n         }\\n         // avoid printing raw history keys each case\\n         // (fallback for on_finish static targets handled inside executeTestCase)\\n+        // Workflow testing: compute workflow outputs if workflow has outputs defined\\n+        let workflowOutputs: Record<string, unknown> | undefined;\\n+        if (Array.isArray((cfgLocal as any).outputs)) {\\n+          try {\\n+            workflowOutputs = this.computeWorkflowOutputs(\\n+              (cfgLocal as any).outputs,\\n+              exec.outHistory,\\n+              res.results\\n+            );\\n+          } catch (e) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ‚ö†Ô∏è Error computing workflow outputs: ${e}`);\\n+            }\\n+          }\\n+        }\\n         const caseFailures = require('./evaluators').evaluateCase(\\n           _case.name,\\n           res.statistics,\\n@@ -845,7 +948,8 @@ export class VisorTestRunner {\\n           setup.strict,\\n           setup.prompts,\\n           res.results,\\n-          exec.outHistory\\n+          exec.outHistory,\\n+          workflowOutputs\\n         );\\n         // Warn about unmocked AI/command steps that executed\\n         try {\\n@@ -1280,7 +1384,8 @@ export class VisorTestRunner {\\n   ): void {\\n     const executed: Record<string, number> = {};\\n     for (const s of stats.checks) {\\n-      if (!s.skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n+      const skipped = (s as any).skipped === true || !!(s as any).skipReason;\\n+      if (!skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n     }\\n     const expCalls = (expect.calls || []).filter(c => c.step);\\n     const expectedSteps = new Map<\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/validator.ts\",\"additions\":48,\"deletions\":2,\"changes\":50,\"patch\":\"diff --git a/src/test-runner/validator.ts b/src/test-runner/validator.ts\\nindex dbb890c5..5ffff5ee 100644\\n--- a/src/test-runner/validator.ts\\n+++ b/src/test-runner/validator.ts\\n@@ -25,6 +25,16 @@ const schema: any = {\\n     hooks: { type: 'object' },\\n     slack: { type: 'object' },\\n     frontends: { type: 'array' },\\n+    workspace: { type: 'object' },\\n+    // Workflow definition fields (for workflow files with co-located tests)\\n+    id: { type: 'string' },\\n+    name: { type: 'string' },\\n+    description: { type: 'string' },\\n+    inputs: { type: 'array' },\\n+    outputs: { type: 'array' },\\n+    ai_mcp_servers: { type: 'object' },\\n+    ai_provider: { type: 'string' },\\n+    ai_model: { type: 'string' },\\n     tests: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -136,6 +146,8 @@ const schema: any = {\\n             oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n           },\\n         },\\n+        // Workflow testing: input values to pass to the workflow\\n+        workflow_input: { type: 'object' },\\n         expect: { $ref: '#/$defs/expectBlock' },\\n         // Flow cases\\n         flow: {\\n@@ -270,6 +282,35 @@ const schema: any = {\\n       },\\n       required: ['step', 'path'],\\n     },\\n+    // Workflow output expectation: like outputsExpectation but without step (tests workflow-level outputs)\\n+    workflowOutputExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        path: { type: 'string' },\\n+        equals: {},\\n+        equalsDeep: {},\\n+        matches: { type: 'string' },\\n+        contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        not_contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            path: { type: 'string' },\\n+            equals: {},\\n+            matches: { type: 'string' },\\n+          },\\n+          required: ['path'],\\n+        },\\n+        contains_unordered: { type: 'array' },\\n+      },\\n+      required: ['path'],\\n+    },\\n     expectBlock: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -278,6 +319,8 @@ const schema: any = {\\n         calls: { type: 'array', items: { $ref: '#/$defs/callsExpectation' } },\\n         prompts: { type: 'array', items: { $ref: '#/$defs/promptsExpectation' } },\\n         outputs: { type: 'array', items: { $ref: '#/$defs/outputsExpectation' } },\\n+        // Workflow testing: assert on workflow-level outputs (defined in workflow's outputs: section)\\n+        workflow_output: { type: 'array', items: { $ref: '#/$defs/workflowOutputExpectation' } },\\n         no_calls: {\\n           type: 'array',\\n           items: {\\n@@ -343,6 +386,7 @@ const knownKeys = new Set([\\n   'version',\\n   'extends',\\n   'tests',\\n+  'workspace',\\n   // tests\\n   'tests.defaults',\\n   'tests.fixtures',\\n@@ -363,6 +407,7 @@ const knownKeys = new Set([\\n   'env',\\n   'routing',\\n   'mocks',\\n+  'workflow_input',\\n   'expect',\\n   'flow',\\n   // expect\\n@@ -370,6 +415,7 @@ const knownKeys = new Set([\\n   'expect.calls',\\n   'expect.prompts',\\n   'expect.outputs',\\n+  'expect.workflow_output',\\n   'expect.no_calls',\\n   'expect.fail',\\n   'expect.strict_violation',\\n@@ -424,7 +470,7 @@ function formatError(e: ErrorObject): string {\\n       if (hint) msg += ` (${hint})`;\\n       // Small curated allow-list for frequent nodes to reduce guesswork\\n       if (path.endsWith('expect')) {\\n-        msg += ` (allowed: use, calls, prompts, outputs, no_calls, fail, strict_violation)`;\\n+        msg += ` (allowed: use, calls, prompts, outputs, workflow_output, no_calls, fail, strict_violation)`;\\n       } else if (path.endsWith('env')) {\\n         msg += ` (values must be strings)`;\\n       } else if (path.endsWith('tests')) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/cli.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/cli.ts b/src/types/cli.ts\\nindex a7d9ca30..f06f93d5 100644\\n--- a/src/types/cli.ts\\n+++ b/src/types/cli.ts\\n@@ -61,6 +61,8 @@ export interface CliOptions {\\n   githubV2?: boolean;\\n   /** Enable Slack Socket Mode runner */\\n   slack?: boolean;\\n+  /** Keep workspace folders after execution (for debugging) */\\n+  keepWorkspace?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/config.ts\",\"additions\":114,\"deletions\":5,\"changes\":119,\"patch\":\"diff --git a/src/types/config.ts b/src/types/config.ts\\nindex ed239959..dd363de1 100644\\n--- a/src/types/config.ts\\n+++ b/src/types/config.ts\\n@@ -122,6 +122,9 @@ export interface FailureConditionContext {\\n   /** Environment variables */\\n   env?: Record<string, string>;\\n \\n+  /** Workflow inputs - values passed to the workflow */\\n+  inputs?: Record<string, unknown>;\\n+\\n   /** Memory accessor for accessing memory store in expressions */\\n   memory?: {\\n     get: (key: string, namespace?: string) => unknown;\\n@@ -356,6 +359,8 @@ export interface AIProviderConfig {\\n   allowBash?: boolean;\\n   /** Advanced bash command execution configuration */\\n   bashConfig?: BashConfig;\\n+  /** Completion prompt for post-completion validation/review (runs after attempt_completion) */\\n+  completion_prompt?: string;\\n }\\n \\n /**\\n@@ -461,8 +466,8 @@ export interface CheckConfig {\\n   env?: EnvConfig;\\n   /** Timeout in seconds for command execution (default: 60) */\\n   timeout?: number;\\n-  /** Check IDs that this check depends on (optional) */\\n-  depends_on?: string[];\\n+  /** Check IDs that this check depends on (optional). Accepts single string or array. */\\n+  depends_on?: string | string[];\\n   /** Group name for comment separation (e.g., \\\"code-review\\\", \\\"pr-overview\\\") - optional */\\n   group?: string;\\n   /** Schema type for template rendering (e.g., \\\"code-review\\\", \\\"markdown\\\") or inline JSON schema object - optional */\\n@@ -621,6 +626,46 @@ export interface CheckConfig {\\n   overrides?: Record<string, Partial<CheckConfig>>;\\n   /** Map workflow outputs to check outputs */\\n   output_mapping?: Record<string, string>;\\n+\\n+  // Workflow aliases for backward compatibility\\n+  /** Alias for args - workflow inputs (backward compatibility) */\\n+  workflow_inputs?: Record<string, unknown>;\\n+  /** Config file path - alternative to workflow ID (loads a Visor config file as workflow) */\\n+  config?: string;\\n+  /** Alias for overrides - workflow step overrides (backward compatibility) */\\n+  workflow_overrides?: Record<string, Partial<CheckConfig>>;\\n+\\n+  /**\\n+   * Git-checkout provider specific options (optional, only used when type === 'git-checkout').\\n+   */\\n+  /** Git reference to checkout (branch, tag, commit SHA) - supports templates */\\n+  ref?: string;\\n+  /** Repository URL or owner/repo format (defaults to current repository) */\\n+  repository?: string;\\n+  /** GitHub token for private repositories (defaults to GITHUB_TOKEN env) */\\n+  token?: string;\\n+  /** Number of commits to fetch (0 for full history, default: 1) */\\n+  fetch_depth?: number;\\n+  /** Whether to fetch tags (default: false) */\\n+  fetch_tags?: boolean;\\n+  /** Checkout submodules: false, true, or 'recursive' */\\n+  submodules?: boolean | 'recursive';\\n+  /** Working directory for the checkout (defaults to temp directory) */\\n+  working_directory?: string;\\n+  /** Use git worktree for efficient parallel checkouts (default: true) */\\n+  use_worktree?: boolean;\\n+  /** Clean the working directory before checkout (default: true) */\\n+  clean?: boolean;\\n+  /** Sparse checkout paths - only checkout specific directories/files */\\n+  sparse_checkout?: string[];\\n+  /** Enable Git LFS (Large File Storage) */\\n+  lfs?: boolean;\\n+  /** Timeout in ms for cloning the bare repository (default: 300000 = 5 min) */\\n+  clone_timeout_ms?: number;\\n+  /** Clean up worktree on failure (default: true) */\\n+  cleanup_on_failure?: boolean;\\n+  /** Keep worktree after workflow completion (default: false) */\\n+  persist_worktree?: boolean;\\n }\\n \\n /**\\n@@ -1037,6 +1082,36 @@ export interface CustomToolDefinition {\\n   outputSchema?: Record<string, unknown>;\\n }\\n \\n+/**\\n+ * Workflow input definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowInput {\\n+  /** Input parameter name */\\n+  name: string;\\n+  /** JSON Schema for the input */\\n+  schema?: Record<string, unknown>;\\n+  /** Whether this input is required */\\n+  required?: boolean;\\n+  /** Default value if not provided */\\n+  default?: unknown;\\n+  /** Human-readable description */\\n+  description?: string;\\n+}\\n+\\n+/**\\n+ * Workflow output definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowOutput {\\n+  /** Output name */\\n+  name: string;\\n+  /** Human-readable description */\\n+  description?: string;\\n+  /** Value using Liquid template syntax (references step outputs) */\\n+  value?: string;\\n+  /** Value using JavaScript expression (alternative to value) */\\n+  value_js?: string;\\n+}\\n+\\n /**\\n  * Main Visor configuration\\n  */\\n@@ -1051,12 +1126,16 @@ export interface VisorConfig {\\n   tools?: Record<string, CustomToolDefinition>;\\n   /** Import workflow definitions from external files or URLs */\\n   imports?: string[];\\n+  /** Workflow inputs (for standalone reusable workflows) */\\n+  inputs?: WorkflowInput[];\\n+  /** Workflow outputs (for standalone reusable workflows) */\\n+  outputs?: WorkflowOutput[];\\n   /** Step configurations (recommended) */\\n   steps?: Record<string, CheckConfig>;\\n   /** Check configurations (legacy, use 'steps' instead) - always populated after normalization */\\n   checks?: Record<string, CheckConfig>;\\n-  /** Output configuration */\\n-  output: OutputConfig;\\n+  /** Output configuration (optional - defaults provided) */\\n+  output?: OutputConfig;\\n   /** HTTP server configuration for receiving webhooks */\\n   http_server?: HttpServerConfig;\\n   /** Memory storage configuration */\\n@@ -1092,6 +1171,36 @@ export interface VisorConfig {\\n     /** Frontend-specific configuration */\\n     config?: unknown;\\n   }>;\\n+  /** Workspace isolation configuration for sandboxed execution */\\n+  workspace?: WorkspaceConfig;\\n+  /** Slack configuration */\\n+  slack?: SlackConfig;\\n+}\\n+\\n+/**\\n+ * Workspace isolation configuration\\n+ */\\n+export interface WorkspaceConfig {\\n+  /** Enable workspace isolation (default: true when config present) */\\n+  enabled?: boolean;\\n+  /** Base path for workspaces (default: /tmp/visor-workspaces) */\\n+  base_path?: string;\\n+  /** Clean up workspace on exit (default: true) */\\n+  cleanup_on_exit?: boolean;\\n+}\\n+\\n+/**\\n+ * Slack configuration\\n+ */\\n+export interface SlackConfig {\\n+  /** Slack API version */\\n+  version?: string;\\n+  /** Mention handling: 'all', 'direct', etc. */\\n+  mentions?: string;\\n+  /** Thread handling: 'required', 'optional', etc. */\\n+  threads?: string;\\n+  /** Show raw output in Slack responses */\\n+  show_raw_output?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/git-checkout.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/git-checkout.ts b/src/types/git-checkout.ts\\nindex eb1c5ebc..4d27855d 100644\\n--- a/src/types/git-checkout.ts\\n+++ b/src/types/git-checkout.ts\\n@@ -16,6 +16,8 @@ export interface GitCheckoutConfig {\\n   fetch_depth?: number;\\n   fetch_tags?: boolean;\\n   submodules?: boolean | 'recursive';\\n+  /** Timeout (ms) for cloning the bare repository; defaults to 300000 (5 minutes) */\\n+  clone_timeout_ms?: number;\\n \\n   // Worktree configuration\\n   working_directory?: string;\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/sandbox.ts\",\"additions\":46,\"deletions\":1,\"changes\":47,\"patch\":\"diff --git a/src/utils/sandbox.ts b/src/utils/sandbox.ts\\nindex 9914a3a5..12e27c30 100644\\n--- a/src/utils/sandbox.ts\\n+++ b/src/utils/sandbox.ts\\n@@ -14,6 +14,51 @@ export interface CompileOptions {\\n   wrapFunction?: boolean;\\n }\\n \\n+export interface JsSyntaxValidationResult {\\n+  valid: boolean;\\n+  error?: string;\\n+}\\n+\\n+/**\\n+ * Validate JavaScript syntax without executing it.\\n+ * Uses the sandbox's compile method to check for syntax errors.\\n+ * Returns validation result with error message if invalid.\\n+ */\\n+export function validateJsSyntax(code: string): JsSyntaxValidationResult {\\n+  if (!code || typeof code !== 'string') {\\n+    return { valid: false, error: 'Code must be a non-empty string' };\\n+  }\\n+\\n+  const trimmed = code.trim();\\n+  if (trimmed.length === 0) {\\n+    return { valid: false, error: 'Code cannot be empty' };\\n+  }\\n+\\n+  // Create a minimal sandbox instance for syntax checking\\n+  const sandbox = createSecureSandbox();\\n+\\n+  // Wrap code similar to compileAndRun to catch the same syntax issues\\n+  const looksLikeBlock = /\\\\breturn\\\\b/.test(trimmed) || /;/.test(trimmed) || /\\\\n/.test(trimmed);\\n+  const looksLikeIife = /\\\\)\\\\s*\\\\(\\\\s*\\\\)\\\\s*;?$/.test(trimmed);\\n+  const body = looksLikeBlock\\n+    ? looksLikeIife\\n+      ? `return (\\\\n${trimmed}\\\\n);\\\\n`\\n+      : `return (() => {\\\\n${trimmed}\\\\n})();\\\\n`\\n+    : `return (\\\\n${trimmed}\\\\n);\\\\n`;\\n+\\n+  // Add log injection header (same as compileAndRun)\\n+  const header = `const __lp = \\\"[syntax-check]\\\"; const log = (...a) => { try { console.log(__lp, ...a); } catch {} };\\\\n`;\\n+  const fullCode = `${header}${body}`;\\n+\\n+  try {\\n+    sandbox.compile(fullCode);\\n+    return { valid: true };\\n+  } catch (e) {\\n+    const msg = e instanceof Error ? e.message : String(e);\\n+    return { valid: false, error: msg };\\n+  }\\n+}\\n+\\n /**\\n  * Create a hardened Sandbox with a consistent set of globals and prototype\\n  * whitelists. This is a superset of the sets previously used by individual\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/tracer-init.ts\",\"additions\":19,\"deletions\":1,\"changes\":20,\"patch\":\"diff --git a/src/utils/tracer-init.ts b/src/utils/tracer-init.ts\\nindex b42995d9..4edf1ffc 100644\\n--- a/src/utils/tracer-init.ts\\n+++ b/src/utils/tracer-init.ts\\n@@ -76,6 +76,24 @@ export async function initializeTracer(\\n \\n       const tracer = new SimpleAppTracer(telemetry, sessionId);\\n \\n+      // WORKAROUND: Add missing recordEvent method for completionPrompt feature (probe #321)\\n+      // SimpleAppTracer doesn't have recordEvent but completionPrompt requires it\\n+      if (typeof (tracer as any).recordEvent !== 'function') {\\n+        (tracer as any).recordEvent = (\\n+          name: string,\\n+          attributes?: Record<string, unknown>\\n+        ) => {\\n+          // Log completion events to telemetry for debugging\\n+          try {\\n+            if ((telemetry as any).record) {\\n+              (telemetry as any).record({ event: name, ...attributes });\\n+            }\\n+          } catch {\\n+            // Best-effort only\\n+          }\\n+        };\\n+      }\\n+\\n       console.error(`üìä Simple tracing enabled, will save to: ${traceFilePath}`);\\n \\n       // If in GitHub Actions, log the path for artifact upload\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/worktree-manager.ts\",\"additions\":142,\"deletions\":27,\"changes\":169,\"patch\":\"diff --git a/src/utils/worktree-manager.ts b/src/utils/worktree-manager.ts\\nindex f3cebf3d..2de7a9f4 100644\\n--- a/src/utils/worktree-manager.ts\\n+++ b/src/utils/worktree-manager.ts\\n@@ -6,6 +6,7 @@\\n  */\\n \\n import * as fs from 'fs';\\n+import * as fsp from 'fs/promises';\\n import * as path from 'path';\\n import * as crypto from 'crypto';\\n import { commandExecutor } from './command-executor';\\n@@ -119,7 +120,8 @@ export class WorktreeManager {\\n     repository: string,\\n     repoUrl: string,\\n     token?: string,\\n-    fetchDepth?: number\\n+    fetchDepth?: number,\\n+    cloneTimeoutMs?: number\\n   ): Promise<string> {\\n     const reposDir = this.getReposDir();\\n     const repoName = repository.replace(/\\\\//g, '-');\\n@@ -129,9 +131,21 @@ export class WorktreeManager {\\n     if (fs.existsSync(bareRepoPath)) {\\n       logger.debug(`Bare repository already exists: ${bareRepoPath}`);\\n \\n-      // Update remote refs\\n-      await this.updateBareRepo(bareRepoPath);\\n-      return bareRepoPath;\\n+      // Verify the bare repo has the correct remote URL to prevent using corrupted repos\\n+      const verifyResult = await this.verifyBareRepoRemote(bareRepoPath, repoUrl);\\n+      if (verifyResult === 'timeout') {\\n+        // Timeout during verification - use stale cache to avoid hanging on re-clone\\n+        logger.info(`Using stale bare repository (verification timed out): ${bareRepoPath}`);\\n+        return bareRepoPath;\\n+      } else if (verifyResult === false) {\\n+        logger.warn(`Bare repository at ${bareRepoPath} has incorrect remote, removing and re-cloning`);\\n+        await fsp.rm(bareRepoPath, { recursive: true, force: true });\\n+        // Fall through to clone below\\n+      } else {\\n+        // Update remote refs\\n+        await this.updateBareRepo(bareRepoPath);\\n+        return bareRepoPath;\\n+      }\\n     }\\n \\n     // Clone as bare repository\\n@@ -153,7 +167,9 @@ export class WorktreeManager {\\n     }\\n     cloneCmd += ` ${this.escapeShellArg(cloneUrl)} ${this.escapeShellArg(bareRepoPath)}`;\\n \\n-    const result = await this.executeGitCommand(cloneCmd, { timeout: 300000 }); // 5 minute timeout\\n+    const result = await this.executeGitCommand(cloneCmd, {\\n+      timeout: cloneTimeoutMs || 300000, // default 5 minutes\\n+    });\\n \\n     if (result.exitCode !== 0) {\\n       // Redact tokens from error messages\\n@@ -171,19 +187,98 @@ export class WorktreeManager {\\n   private async updateBareRepo(bareRepoPath: string): Promise<void> {\\n     logger.debug(`Updating bare repository: ${bareRepoPath}`);\\n \\n-    const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n-    const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+    try {\\n+      const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n+      const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n+        // Don't throw - we can continue with stale refs\\n+      } else {\\n+        logger.debug(`Successfully updated bare repository`);\\n+      }\\n+    } catch (error) {\\n+      // Handle timeout or other errors gracefully - we can continue with stale refs\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      logger.warn(`Failed to update bare repository (will use stale refs): ${errorMessage}`);\\n+    }\\n+  }\\n \\n-    if (result.exitCode !== 0) {\\n-      logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n-      // Don't throw - we can continue with stale refs\\n-    } else {\\n-      logger.debug(`Successfully updated bare repository`);\\n+  /**\\n+   * Verify that a bare repository has the correct remote URL.\\n+   * This prevents reusing corrupted repos that were cloned from a different repository.\\n+   * Returns: true (valid), false (invalid - should re-clone), or 'timeout' (use stale cache)\\n+   */\\n+  private async verifyBareRepoRemote(\\n+    bareRepoPath: string,\\n+    expectedUrl: string\\n+  ): Promise<boolean | 'timeout'> {\\n+    try {\\n+      const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote get-url origin`;\\n+      const result = await this.executeGitCommand(cmd, { timeout: 10000 });\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to get remote URL for ${bareRepoPath}: ${result.stderr}`);\\n+        return false;\\n+      }\\n+\\n+      const actualUrl = result.stdout.trim();\\n+\\n+      // Normalize URLs for comparison:\\n+      // - remove credentials (tokens/username)\\n+      // - remove .git suffix if present\\n+      // - trim trailing slash\\n+      // - lowercase for case‚Äëinsensitive match\\n+      const normalizeUrl = (url: string): string => {\\n+        // Convert common SSH form to https for comparison\\n+        if (url.startsWith('git@github.com:')) {\\n+          url = url.replace('git@github.com:', 'https://github.com/');\\n+        }\\n+        return url\\n+          // strip userinfo part to avoid mismatches when the cached bare\\n+          // repo was cloned with an access token but the expected URL is\\n+          // tokenless (or vice‚Äëversa)\\n+          .replace(/:\\\\/\\\\/[^@]+@/, '://')\\n+          .replace(/\\\\.git$/, '')\\n+          .replace(/\\\\/$/, '')\\n+          .toLowerCase();\\n+      };\\n+\\n+      const normalizedExpected = normalizeUrl(expectedUrl);\\n+      const normalizedActual = normalizeUrl(actualUrl);\\n+\\n+      if (normalizedExpected !== normalizedActual) {\\n+        logger.warn(\\n+          `Bare repo remote mismatch: expected ${expectedUrl}, got ${actualUrl}`\\n+        );\\n+        return false;\\n+      }\\n+\\n+      logger.debug(`Bare repo remote verified: ${actualUrl}`);\\n+      return true;\\n+    } catch (error) {\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      // If it's a timeout, return 'timeout' so caller can use stale cache instead of re-cloning\\n+      if (errorMessage.includes('timed out')) {\\n+        logger.warn(`Timeout verifying bare repo remote (will use stale cache): ${errorMessage}`);\\n+        return 'timeout';\\n+      }\\n+      logger.warn(`Error verifying bare repo remote: ${error}`);\\n+      return false;\\n     }\\n   }\\n \\n   /**\\n-   * Create a new worktree\\n+   * Create a new worktree for the given repository/ref.\\n+   *\\n+   * Important: we always create worktrees in a detached HEAD state pinned\\n+   * to a specific commit SHA rather than a named branch like \\\"main\\\". Git\\n+   * only allows a branch to be checked out in a single worktree at a time;\\n+   * using the raw commit (plus --detach) lets multiple workflows safely\\n+   * create independent worktrees for the same branch without hitting\\n+   * errors like:\\n+   *\\n+   *   fatal: 'main' is already used by worktree at '.../TykTechnologies-tyk-docs-main-XXXX'\\n    */\\n   async createWorktree(\\n     repository: string,\\n@@ -195,6 +290,7 @@ export class WorktreeManager {\\n       clean?: boolean;\\n       workflowId?: string;\\n       fetchDepth?: number;\\n+      cloneTimeoutMs?: number;\\n     } = {}\\n   ): Promise<WorktreeInfo> {\\n     // Validate ref to prevent command injection\\n@@ -205,7 +301,8 @@ export class WorktreeManager {\\n       repository,\\n       repoUrl,\\n       options.token,\\n-      options.fetchDepth\\n+      options.fetchDepth,\\n+      options.cloneTimeoutMs\\n     );\\n \\n     // Generate worktree ID and path\\n@@ -241,21 +338,24 @@ export class WorktreeManager {\\n       }\\n     }\\n \\n-    // Fetch the ref if needed\\n+    // Fetch the ref if needed, then resolve it to a concrete commit SHA.\\n+    // We use the commit (detached HEAD) instead of the branch name so we\\n+    // can have multiple worktrees for the same branch without git refusing\\n+    // with \\\"branch X is already checked out\\\".\\n     await this.fetchRef(bareRepoPath, ref);\\n+    const commit = await this.getCommitShaForRef(bareRepoPath, ref);\\n \\n-    // Create worktree\\n-    logger.info(`Creating worktree for ${repository}@${ref}`);\\n-    const createCmd = `git -C ${this.escapeShellArg(bareRepoPath)} worktree add ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(ref)}`;\\n+    // Create worktree in detached HEAD state at the resolved commit\\n+    logger.info(`Creating worktree for ${repository}@${ref} (${commit})`);\\n+    const createCmd = `git -C ${this.escapeShellArg(\\n+      bareRepoPath\\n+    )} worktree add --detach ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(commit)}`;\\n     const result = await this.executeGitCommand(createCmd, { timeout: 60000 });\\n \\n     if (result.exitCode !== 0) {\\n       throw new Error(`Failed to create worktree: ${result.stderr}`);\\n     }\\n \\n-    // Get commit SHA\\n-    const commit = await this.getCommitSha(worktreePath);\\n-\\n     // Create metadata\\n     const metadata: WorktreeMetadata = {\\n       worktree_id: worktreeId,\\n@@ -316,14 +416,17 @@ export class WorktreeManager {\\n   }\\n \\n   /**\\n-   * Get commit SHA for worktree\\n+   * Get commit SHA for a given ref inside a bare repository.\\n+   *\\n+   * This runs after fetchRef so that <ref> should resolve to either a\\n+   * local branch, tag, or remote-tracking ref.\\n    */\\n-  private async getCommitSha(worktreePath: string): Promise<string> {\\n-    const cmd = `git -C ${this.escapeShellArg(worktreePath)} rev-parse HEAD`;\\n+  private async getCommitShaForRef(bareRepoPath: string, ref: string): Promise<string> {\\n+    const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} rev-parse ${this.escapeShellArg(ref)}`;\\n     const result = await this.executeGitCommand(cmd);\\n \\n     if (result.exitCode !== 0) {\\n-      throw new Error(`Failed to get commit SHA: ${result.stderr}`);\\n+      throw new Error(`Failed to get commit SHA for ref ${ref}: ${result.stderr}`);\\n     }\\n \\n     return result.stdout.trim();\\n@@ -621,9 +724,21 @@ export class WorktreeManager {\\n     command: string,\\n     options: { timeout?: number; env?: Record<string, string> } = {}\\n   ): Promise<GitCommandResult> {\\n+    // Merge provided env with process.env and add git-specific settings\\n+    // These settings prevent git from hanging on interactive prompts while\\n+    // still allowing OS-level credential helpers to work:\\n+    // - GIT_TERMINAL_PROMPT=0: Prevents terminal credential prompts\\n+    // - GIT_SSH_COMMAND: Disables SSH password prompts (BatchMode)\\n+    const gitEnv = {\\n+      ...process.env,\\n+      ...options.env,\\n+      GIT_TERMINAL_PROMPT: '0',\\n+      GIT_SSH_COMMAND: 'ssh -o BatchMode=yes -o StrictHostKeyChecking=no',\\n+    };\\n+\\n     const result = await commandExecutor.execute(command, {\\n       timeout: options.timeout || 30000,\\n-      env: options.env || process.env,\\n+      env: gitEnv,\\n     } as any);\\n \\n     return {\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-executor.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-executor.ts b/src/workflow-executor.ts\\nindex 751e7f16..df76762f 100644\\n--- a/src/workflow-executor.ts\\n+++ b/src/workflow-executor.ts\\n@@ -219,7 +219,9 @@ export class WorkflowExecutor {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps)) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     // Use static DependencyResolver\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-registry.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-registry.ts b/src/workflow-registry.ts\\nindex 2cbe3a89..6bd855fb 100644\\n--- a/src/workflow-registry.ts\\n+++ b/src/workflow-registry.ts\\n@@ -410,7 +410,9 @@ export class WorkflowRegistry {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps || {})) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     try {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/ai-review-service.test.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/tests/unit/ai-review-service.test.ts b/tests/unit/ai-review-service.test.ts\\nindex ec2b998a..2c616c99 100644\\n--- a/tests/unit/ai-review-service.test.ts\\n+++ b/tests/unit/ai-review-service.test.ts\\n@@ -482,6 +482,37 @@ describe('AIReviewService', () => {\\n       expect(context).toContain('<additions>10</additions>');\\n       expect(context).toContain('<deletions>5</deletions>');\\n     });\\n+\\n+    it('should exclude diffs by default when in Slack mode', async () => {\\n+      // Slack mode is detected by presence of slackConversation property\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // In Slack mode, diffs should be excluded by default\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+      // Should still include file summary\\n+      expect(context).toContain('<files_summary>');\\n+    });\\n+\\n+    it('should include diffs in Slack mode when explicitly requested', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      (mockPRInfo as any).includeCodeContext = true;\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // With explicit includeCodeContext: true, diffs should be included\\n+      expect(context).toContain('<full_diff>');\\n+      expect(context).toContain('--- test.ts');\\n+    });\\n+\\n+    it('should exclude diffs in Slack mode even when includeCodeContext is undefined', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      // Deliberately not setting includeCodeContext to test default behavior\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+    });\\n   });\\n \\n   describe('Comment Filtering for Code Review', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config-extends.test.ts\",\"additions\":8,\"deletions\":8,\"changes\":16,\"patch\":\"diff --git a/tests/unit/config-extends.test.ts b/tests/unit/config-extends.test.ts\\nindex b112a869..7ee69ce6 100644\\n--- a/tests/unit/config-extends.test.ts\\n+++ b/tests/unit/config-extends.test.ts\\n@@ -564,7 +564,7 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.security).toBeDefined();\\n       expect(config.checks!.performance).toBeDefined();\\n       expect(config.checks!.custom).toBeDefined();\\n-      expect(config.output.pr_comment.format).toBe('json');\\n+      expect(config.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should extend from default configuration', async () => {\\n@@ -1002,7 +1002,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.base).toBeDefined(); // From base\\n       expect(loaded.checks!.middle).toBeDefined(); // From middle\\n       expect(loaded.checks!.top).toBeDefined(); // From top\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From top\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From top\\n     });\\n \\n     it('should handle mixed local and remote extends', async () => {\\n@@ -1079,7 +1079,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.local).toBeDefined();\\n       expect(loaded.checks!.remote).toBeDefined();\\n       expect(loaded.checks!.child).toBeDefined();\\n-      expect(loaded.output.pr_comment.format).toBe('json');\\n+      expect(loaded.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should handle extends with relative path resolution', async () => {\\n@@ -1593,9 +1593,9 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.performance).toBeDefined(); // Added in child\\n \\n       // Verify output\\n-      expect(config.output.pr_comment.format).toBe('json'); // Overridden\\n-      expect(config.output.pr_comment.group_by).toBe('file'); // Overridden\\n-      expect(config.output.pr_comment.collapse).toBe(false); // Overridden\\n+      expect(config.output!.pr_comment.format).toBe('json'); // Overridden\\n+      expect(config.output!.pr_comment.group_by).toBe('file'); // Overridden\\n+      expect(config.output!.pr_comment.collapse).toBe(false); // Overridden\\n     });\\n \\n     it('should handle configuration with disabled checks removal', async () => {\\n@@ -1814,7 +1814,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.root).toBeDefined(); // From root\\n       expect(loaded.checks!.level1).toBeDefined(); // From level1\\n       expect(loaded.checks!.level2).toBeDefined(); // From level2\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From level2\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From level2\\n     });\\n \\n     it('should handle mixed absolute and relative paths in extends array', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config.test.ts\",\"additions\":7,\"deletions\":7,\"changes\":14,\"patch\":\"diff --git a/tests/unit/config.test.ts b/tests/unit/config.test.ts\\nindex 67ce61fd..0677a3d4 100644\\n--- a/tests/unit/config.test.ts\\n+++ b/tests/unit/config.test.ts\\n@@ -48,7 +48,7 @@ output:\\n       expect(config.version).toBe('1.0');\\n       expect(config.checks).toHaveProperty('performance');\\n       expect(config.checks).toHaveProperty('security');\\n-      expect(config.output.pr_comment.format).toBe('table');\\n+      expect(config.output!.pr_comment.format).toBe('table');\\n     });\\n \\n     it('should handle missing config file gracefully', async () => {\\n@@ -264,9 +264,9 @@ checks:\\n       const config = await configManager.loadConfig('/path/to/minimal.yaml');\\n \\n       // Should have default output configuration\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.group_by).toBe('check');\\n-      expect(config.output.pr_comment.collapse).toBe(true);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.group_by).toBe('check');\\n+      expect(config.output!.pr_comment.collapse).toBe(true);\\n     });\\n   });\\n \\n@@ -483,8 +483,8 @@ output:\\n       expect(config.checks!.performance.prompt).toContain('N+1 database queries');\\n       expect(config.checks!.security.prompt).toContain('SQL injection');\\n       expect(config.checks!.architecture.prompt).toContain('SOLID principles');\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.collapse).toBe(false);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.collapse).toBe(false);\\n     });\\n   });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/foreach-empty-skip.test.ts\",\"additions\":5,\"deletions\":3,\"changes\":8,\"patch\":\"diff --git a/tests/unit/foreach-empty-skip.test.ts b/tests/unit/foreach-empty-skip.test.ts\\nindex 1ea8e1e5..2640ed14 100644\\n--- a/tests/unit/foreach-empty-skip.test.ts\\n+++ b/tests/unit/foreach-empty-skip.test.ts\\n@@ -90,11 +90,13 @@ describe('forEach Empty Array Skip', () => {\\n     );\\n     expect(processStats?.totalRuns).toBe(0);\\n \\n-    // notify-ticket should also NOT run because process-ticket returned empty\\n+    // notify-ticket SHOULD run even though process-ticket had 0 runs from empty forEach\\n+    // This is the expected behavior: downstream steps that depend on forEach children\\n+    // should still execute (with empty/null inputs) to handle the \\\"no items\\\" case\\n     const notifyStats = result.executionStatistics?.checks!.find(\\n       c => c.checkName === 'notify-ticket'\\n     );\\n-    expect(notifyStats?.totalRuns).toBe(0);\\n+    expect(notifyStats?.totalRuns).toBe(1);\\n   });\\n \\n   it('should handle empty array with transform_js', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/ai-check-provider.test.ts\",\"additions\":61,\"deletions\":1,\"changes\":62,\"patch\":\"diff --git a/tests/unit/providers/ai-check-provider.test.ts b/tests/unit/providers/ai-check-provider.test.ts\\nindex d0b94086..a858ddae 100644\\n--- a/tests/unit/providers/ai-check-provider.test.ts\\n+++ b/tests/unit/providers/ai-check-provider.test.ts\\n@@ -399,6 +399,66 @@ describe('AICheckProvider', () => {\\n         },\\n       });\\n     });\\n+\\n+    it('derives allowedFolders and path from workspace when present on parent context', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const workspace = {\\n+        isEnabled: () => true,\\n+        getWorkspaceInfo: () => ({\\n+          workspacePath: '/tmp/ws-123',\\n+          mainProjectPath: '/tmp/ws-123/main-project',\\n+        }),\\n+        listProjects: () => [\\n+          { name: 'tyk', path: '/tmp/ws-123/tyk' },\\n+          { name: 'tyk-docs', path: '/tmp/ws-123/tyk-docs' },\\n+        ],\\n+      };\\n+\\n+      const execContext: any = {\\n+        _parentContext: {\\n+          workspace,\\n+        },\\n+      };\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'code_help',\\n+        ai: {\\n+          provider: 'google',\\n+          model: 'gemini-2.5-pro',\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, execContext);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'google',\\n+        model: 'gemini-2.5-pro',\\n+        // Workspace root should be used as primary working directory for tools\\n+        path: '/tmp/ws-123',\\n+      });\\n+\\n+      // allowedFolders should contain the workspace plus all project paths, de-duped\\n+      expect(capturedConfig.allowedFolders).toEqual(\\n+        expect.arrayContaining(['/tmp/ws-123', '/tmp/ws-123/tyk', '/tmp/ws-123/tyk-docs'])\\n+      );\\n+    });\\n   });\\n \\n   describe('getSupportedConfigKeys', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/command-check-provider.test.ts\",\"additions\":105,\"deletions\":1,\"changes\":106,\"patch\":\"diff --git a/tests/unit/providers/command-check-provider.test.ts b/tests/unit/providers/command-check-provider.test.ts\\nindex 3bf11a84..0a821bbd 100644\\n--- a/tests/unit/providers/command-check-provider.test.ts\\n+++ b/tests/unit/providers/command-check-provider.test.ts\\n@@ -666,4 +666,108 @@ describe('CommandCheckProvider', () => {\\n       expect((result as any).output).toBe('content'); // CommandCheckProvider trims whitespace\\n     });\\n   });\\n+\\n+  describe('Workflow Inputs', () => {\\n+    it('should use workflowInputs from config when available', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Hello from config.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from config.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with inputs from config.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from config.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should fall back to context.workflowInputs when config.workflowInputs is not set', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Hello from context.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from context.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify the command was rendered with inputs from context.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from context.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should prefer config.workflowInputs over context.workflowInputs', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Config takes precedence',\\n+        },\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Context should be ignored',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Config takes precedence\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify config.workflowInputs takes precedence\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Config takes precedence\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should provide empty inputs when neither config nor context has workflowInputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with empty inputs (undefined renders as empty)\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+  });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/git-checkout-workspace.test.ts\",\"additions\":4,\"deletions\":5,\"changes\":9,\"patch\":\"diff --git a/tests/unit/providers/git-checkout-workspace.test.ts b/tests/unit/providers/git-checkout-workspace.test.ts\\nindex 4eb625b7..4a0fbb6e 100644\\n--- a/tests/unit/providers/git-checkout-workspace.test.ts\\n+++ b/tests/unit/providers/git-checkout-workspace.test.ts\\n@@ -129,13 +129,12 @@ describe('GitCheckoutProvider Workspace Integration', () => {\\n         const output = (result as any).output as GitCheckoutOutput;\\n         expect(output.success).toBe(true);\\n         expect(output.workspace_path).toBeDefined();\\n-        // When checkName is provided, it's used as the project name\\n-        expect(output.workspace_path).toContain('test-checkout');\\n \\n-        // Verify project was added to workspace\\n+        // Verify project was added to workspace and the name is derived\\n+        // from the repository/description, not the checkName.\\n         const projects = workspace.listProjects();\\n         expect(projects.length).toBe(1);\\n-        expect(projects[0].name).toBe('test-checkout');\\n+        expect(projects[0].name).toBe('external-repo');\\n       } finally {\\n         await workspace.cleanup();\\n         fs.rmSync(mainProjectDir, { recursive: true, force: true });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/http-client-provider.test.ts\",\"additions\":15,\"deletions\":10,\"changes\":25,\"patch\":\"diff --git a/tests/unit/providers/http-client-provider.test.ts b/tests/unit/providers/http-client-provider.test.ts\\nindex b759ad75..caa73417 100644\\n--- a/tests/unit/providers/http-client-provider.test.ts\\n+++ b/tests/unit/providers/http-client-provider.test.ts\\n@@ -120,15 +120,17 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect(result).toEqual<ReviewSummary & { data: unknown }>({\\n+      // The provider returns data in the 'output' property (consistent with other providers)\\n+      expect(result).toEqual({\\n         issues: [],\\n-        data: responseData,\\n+        output: { status: 'ok', data: { value: 123 } },\\n       });\\n     });\\n \\n     it('should handle POST request with body', async () => {\\n       mockConfig.method = 'POST';\\n-      mockConfig.body = '{\\\"request\\\": \\\"data\\\"}';\\n+      // Use a body with Liquid template to trigger parseAndRender\\n+      mockConfig.body = '{\\\"request\\\": \\\"{{ pr.title }}\\\"}';\\n \\n       const responseData = { result: 'success' };\\n       const mockResponse = {\\n@@ -143,15 +145,16 @@ describe('HttpClientProvider', () => {\\n       };\\n \\n       mockFetch.mockResolvedValue(mockResponse);\\n-      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"data\\\"}');\\n+      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"Test PR\\\"}');\\n \\n       const result = await provider.execute(mockPRInfo, mockConfig, new Map());\\n \\n+      // Verify the body template was parsed by Liquid with the correct template\\n       expect(mockLiquid.parseAndRender).toHaveBeenCalledWith(\\n-        '{\\\"request\\\": \\\"data\\\"}',\\n+        '{\\\"request\\\": \\\"{{ pr.title }}\\\"}',\\n         expect.objectContaining({\\n           pr: expect.any(Object),\\n-          outputs: {},\\n+          outputs: expect.any(Object),\\n         })\\n       );\\n \\n@@ -159,7 +162,7 @@ describe('HttpClientProvider', () => {\\n         'https://api.example.com/data',\\n         expect.objectContaining({\\n           method: 'POST',\\n-          body: '{\\\"request\\\": \\\"data\\\"}',\\n+          body: '{\\\"request\\\": \\\"Test PR\\\"}',\\n           headers: expect.objectContaining({\\n             Authorization: 'Bearer token',\\n             'Content-Type': 'application/json',\\n@@ -167,7 +170,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(responseData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { result: string } }).output.result).toEqual('success');\\n     });\\n \\n     it('should handle HTTP errors', async () => {\\n@@ -235,7 +239,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(transformedData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { transformed: string } }).output.transformed).toEqual('result');\\n     });\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/session-reuse-config.test.ts\",\"additions\":20,\"deletions\":1,\"changes\":21,\"patch\":\"diff --git a/tests/unit/session-reuse-config.test.ts b/tests/unit/session-reuse-config.test.ts\\nindex a2e2ee26..18fc42e6 100644\\n--- a/tests/unit/session-reuse-config.test.ts\\n+++ b/tests/unit/session-reuse-config.test.ts\\n@@ -152,6 +152,25 @@ describe('Session Reuse Configuration Validation', () => {\\n       }).not.toThrow();\\n     });\\n \\n+    it('should accept reuse_ai_session=\\\\\\\"self\\\\\\\" without depends_on', () => {\\n+      const config: Partial<VisorConfig> = {\\n+        version: '1.0',\\n+        checks: {\\n+          'loop-check': {\\n+            type: 'ai',\\n+            prompt: 'Chat-like loop that reuses its own session',\\n+            on: ['pr_opened'],\\n+            // Special self-reuse mode does not require depends_on\\n+            reuse_ai_session: 'self',\\n+          },\\n+        },\\n+      };\\n+\\n+      expect(() => {\\n+        (configManager as any).validateConfig(config);\\n+      }).not.toThrow();\\n+    });\\n+\\n     it('should accept reuse_ai_session=true with multiple dependencies', () => {\\n       const config: Partial<VisorConfig> = {\\n         version: '1.0',\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/slack-markdown-format.test.ts\",\"additions\":25,\"deletions\":1,\"changes\":26,\"patch\":\"diff --git a/tests/unit/slack-markdown-format.test.ts b/tests/unit/slack-markdown-format.test.ts\\nindex 5e57ccae..39d0baf3 100644\\n--- a/tests/unit/slack-markdown-format.test.ts\\n+++ b/tests/unit/slack-markdown-format.test.ts\\n@@ -39,4 +39,28 @@ describe('markdownToSlack', () => {\\n       ['‚Ä¢ parent', '  ‚Ä¢ child', '```', '- not-a-bullet inside code', '```', '‚Ä¢ after'].join('\\\\n')\\n     );\\n   });\\n+\\n+  it('converts markdown headers to bold text', () => {\\n+    const input = '# Main Title\\\\n## Subtitle\\\\n### Section';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*Main Title*\\\\n*Subtitle*\\\\n*Section*');\\n+  });\\n+\\n+  it('adds newline before h1/h2 headers when preceded by content', () => {\\n+    const input = 'Some content\\\\n## New Section\\\\nMore content';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('Some content\\\\n\\\\n*New Section*\\\\nMore content');\\n+  });\\n+\\n+  it('does not add newline before h1 if it is the first line', () => {\\n+    const input = '# First Header\\\\nContent here';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*First Header*\\\\nContent here');\\n+  });\\n+\\n+  it('ignores headers inside code blocks', () => {\\n+    const input = '```\\\\n# This is a comment\\\\n```\\\\n# Real Header';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('```\\\\n# This is a comment\\\\n```\\\\n*Real Header*');\\n+  });\\n });\\n\",\"status\":\"modified\"}],\"outputs\":{\"ask\":{\"text\":\"what is the root cause of this bug:\\n<https://tyktech.atlassian.net/browse/TT-16304>\",\"ts\":1767898856931},\"jira-context\":{\"jira_context_xml\":\"<jira_context>\\n  <issue_count>1</issue_count>\\n  <issue key=\\\"TT-16304\\\">\\n    <summary>Valid OAS API cannot be used by GW due to validation failed error</summary>\\n    <description></description>\\n    <status>Open</status>\\n    <priority>Medium</priority>\\n    <assignee></assignee>\\n    <reporter>Radoslaw Krawczyk</reporter>\\n    <components>Tyk Gateway</components>\\n  </issue>\\n</jira_context>\",\"issues\":[{\"key\":\"TT-16304\",\"summary\":\"Valid OAS API cannot be used by GW due to validation failed error\",\"description\":\"\",\"status\":\"Open\",\"priority\":\"Medium\",\"assignee\":\"\",\"reporter\":\"Radoslaw Krawczyk\",\"labels\":[],\"components\":[\"Tyk Gateway\"],\"custom_fields\":{},\"parent\":null,\"subtasks\":[],\"comments\":[]}],\"issue_count\":1,\"ts\":1767898857743},\"zendesk-context\":{\"zendesk_context_xml\":\"<zendesk_context>\\n  <ticket_count>1</ticket_count>\\n  <ticket id=\\\"16304\\\">\\n    <subject>Authorization Header : starts with a space </subject>\\n    <description>\\n\\nWe have a WAF Compliance Issue whereby the WAF asserts a Header for Authorization as such the Value is leading by a space ; Hence a 403 is recieved\\n\\nurl -v --cert qa-ndex.enstreamidentity.com.cer --key bell-preprod.key \\\\\\n\\n-H &quot;enstream-authorization: 8087656ca224433466c00c75847ac248&quot; \\\\\\n\\n&quot;https://apistg.nbd.bell.ca/ndex/getAttributesFromId?appId=3049&amp;idType=mdn&amp;idList=4162160839&amp;attbList=*&quot;\\n\\n*   Trying 184.150.80.90:443...\\n\\n* Connected to apistg.nbd.bell.ca (184.150.80.90) port 443 (#0)\\n\\n* ALPN, offering h2\\n\\n* ALPN, offering http/1.1\\n\\n* successfully set certificate verify locations:\\n\\n*  CAfile: /etc/ssl/cert.pem\\n\\n*  CApath: none\\n\\n* (304) (OUT), TLS handshake, Client hello (1):\\n\\n* (304) (IN), TLS handshake, Server hello (2):\\n\\n* TLSv1.2 (IN), TLS handshake, Certificate (11):\\n\\n* TLSv1.2 (IN), TLS handshake, Server key exchange (12):\\n\\n* TLSv1.2 (IN), TLS handshake, Request CERT (13):\\n\\n* TLSv1.2 (IN), TLS handshake, Server finished (14):\\n\\n* TLSv1.2 (OUT), TLS handshake, Certificate (11):\\n\\n* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):\\n\\n* TLSv1.2 (OUT), TLS handshake, CERT verify (15):\\n\\n* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):\\n\\n* TLSv1.2 (OUT), TLS handshake, Finished (20):\\n\\n* TLSv1.2 (IN), TLS change cipher, Change cipher spec (1):\\n\\n* TLSv1.2 (IN), TLS handshake, Finished (20):\\n\\n* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256\\n\\n* ALPN, server did not agree to a protocol\\n\\n* Server certificate:\\n\\n*  subject: C=CA; ST=Ontario; L=Ottawa; O=Bell Canada; CN=apistg.nbd.bell.ca\\n\\n*  start date: Jan 20 20:32:24 2023 GMT\\n\\n*  expire date: Feb 19 20:32:21 2024 GMT\\n\\n*  subjectAltName: host &quot;apistg.nbd.bell.ca&quot; matched cert&apos;s &quot;apistg.nbd.bell.ca&quot;\\n\\n*  issuer: C=US; O=Entrust, Inc.; OU=See www.entrust.net/legal-terms; OU=(c) 2012 Entrust, Inc. - for authorized use only; CN=Entrust Certification Authority - L1K\\n\\n*  SSL certificate verify ok.\\n\\n&gt; GET /ndex/getAttributesFromId?appId=3049&amp;idType=mdn&amp;idList=4162160839&amp;attbList=* HTTP/1.1\\n\\n&gt; Host: apistg.nbd.bell.ca\\n\\n&gt; User-Agent: curl/7.79.1\\n\\n&gt; Accept: */*\\n\\n&gt; enstream-authorization: 2c760a6ca224433466c00c75847ac248\\n\\n&gt;\\n\\n* Mark bundle as not supporting multiuse\\n\\n&lt; HTTP/1.1 403 Forbidden\\n\\n&lt; Content-Type: application/json\\n\\n&lt; Date: Tue, 07 Mar 2023 16:35:42 GMT\\n\\n&lt; Content-Length: 57\\n\\n&lt; Set-Cookie: TS01ab8fe2=017e5ad8c4423a1f0c76c854eb18d47723f266acc5bb34020cbc2b9674adb0c3412fc32496f3685aa3b22cf1c284a800cc2f29fdcd; Path=/; Secure; HTTPOnly\\n\\n&lt;\\n\\n{\\n\\n    &quot;error&quot;: &quot;Access to this API has been disallowed&quot;\\n\\n* Connection #0 to host apistg.nbd.bell.ca left intact\\n\\n}</description>\\n    <status>closed</status>\\n    <priority>normal</priority>\\n    <created_at>2023-03-13T14:33:03Z</created_at>\\n    <updated_at>2023-03-21T18:02:27Z</updated_at>\\n    <tags>amer, gold, mongodb, on-prem, q_a, sla, us, wes</tags>\\n  </ticket>\\n</zendesk_context>\",\"tickets\":\"[{\\\"id\\\":\\\"16304\\\",\\\"subject\\\":\\\"Authorization Header : starts with a space \\\",\\\"description\\\":\\\"\\\\n\\\\nWe have a WAF Compliance Issue whereby the WAF asserts a Header for Authorization as such the Value is leading by a space ; Hence a 403 is recieved\\\\n\\\\nurl -v --cert qa-ndex.enstreamidentity.com.cer --key bell-preprod.key \\\\\\\\\\\\n\\\\n-H \\\\\\\"enstream-authorization: 8087656ca224433466c00c75847ac248\\\\\\\" \\\\\\\\\\\\n\\\\n\\\\\\\"https://apistg.nbd.bell.ca/ndex/getAttributesFromId?appId=3049&idType=mdn&idList=4162160839&attbList=*\\\\\\\"\\\\n\\\\n*   Trying 184.150.80.90:443...\\\\n\\\\n* Connected to apistg.nbd.bell.ca (184.150.80.90) port 443 (#0)\\\\n\\\\n* ALPN, offering h2\\\\n\\\\n* ALPN, offering http/1.1\\\\n\\\\n* successfully set certificate verify locations:\\\\n\\\\n*  CAfile: /etc/ssl/cert.pem\\\\n\\\\n*  CApath: none\\\\n\\\\n* (304) (OUT), TLS handshake, Client hello (1):\\\\n\\\\n* (304) (IN), TLS handshake, Server hello (2):\\\\n\\\\n* TLSv1.2 (IN), TLS handshake, Certificate (11):\\\\n\\\\n* TLSv1.2 (IN), TLS handshake, Server key exchange (12):\\\\n\\\\n* TLSv1.2 (IN), TLS handshake, Request CERT (13):\\\\n\\\\n* TLSv1.2 (IN), TLS handshake, Server finished (14):\\\\n\\\\n* TLSv1.2 (OUT), TLS handshake, Certificate (11):\\\\n\\\\n* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):\\\\n\\\\n* TLSv1.2 (OUT), TLS handshake, CERT verify (15):\\\\n\\\\n* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):\\\\n\\\\n* TLSv1.2 (OUT), TLS handshake, Finished (20):\\\\n\\\\n* TLSv1.2 (IN), TLS change cipher, Change cipher spec (1):\\\\n\\\\n* TLSv1.2 (IN), TLS handshake, Finished (20):\\\\n\\\\n* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256\\\\n\\\\n* ALPN, server did not agree to a protocol\\\\n\\\\n* Server certificate:\\\\n\\\\n*  subject: C=CA; ST=Ontario; L=Ottawa; O=Bell Canada; CN=apistg.nbd.bell.ca\\\\n\\\\n*  start date: Jan 20 20:32:24 2023 GMT\\\\n\\\\n*  expire date: Feb 19 20:32:21 2024 GMT\\\\n\\\\n*  subjectAltName: host \\\\\\\"apistg.nbd.bell.ca\\\\\\\" matched cert's \\\\\\\"apistg.nbd.bell.ca\\\\\\\"\\\\n\\\\n*  issuer: C=US; O=Entrust, Inc.; OU=See www.entrust.net/legal-terms; OU=(c) 2012 Entrust, Inc. - for authorized use only; CN=Entrust Certification Authority - L1K\\\\n\\\\n*  SSL certificate verify ok.\\\\n\\\\n> GET /ndex/getAttributesFromId?appId=3049&idType=mdn&idList=4162160839&attbList=* HTTP/1.1\\\\n\\\\n> Host: apistg.nbd.bell.ca\\\\n\\\\n> User-Agent: curl/7.79.1\\\\n\\\\n> Accept: */*\\\\n\\\\n> enstream-authorization: 2c760a6ca224433466c00c75847ac248\\\\n\\\\n>\\\\n\\\\n* Mark bundle as not supporting multiuse\\\\n\\\\n< HTTP/1.1 403 Forbidden\\\\n\\\\n< Content-Type: application/json\\\\n\\\\n< Date: Tue, 07 Mar 2023 16:35:42 GMT\\\\n\\\\n< Content-Length: 57\\\\n\\\\n< Set-Cookie: TS01ab8fe2=017e5ad8c4423a1f0c76c854eb18d47723f266acc5bb34020cbc2b9674adb0c3412fc32496f3685aa3b22cf1c284a800cc2f29fdcd; Path=/; Secure; HTTPOnly\\\\n\\\\n<\\\\n\\\\n{\\\\n\\\\n    \\\\\\\"error\\\\\\\": \\\\\\\"Access to this API has been disallowed\\\\\\\"\\\\n\\\\n* Connection #0 to host apistg.nbd.bell.ca left intact\\\\n\\\\n}\\\",\\\"status\\\":\\\"closed\\\",\\\"priority\\\":\\\"normal\\\",\\\"requester_id\\\":5942781629980,\\\"assignee_id\\\":389488497399,\\\"tags\\\":[\\\"amer\\\",\\\"gold\\\",\\\"mongodb\\\",\\\"on-prem\\\",\\\"q_a\\\",\\\"sla\\\",\\\"us\\\",\\\"wes\\\"],\\\"created_at\\\":\\\"2023-03-13T14:33:03Z\\\",\\\"updated_at\\\":\\\"2023-03-21T18:02:27Z\\\"}]\",\"ticket_count\":1,\"attachments\":\"{}\",\"ts\":1767898858021},\"log-request\":{\"issues\":[],\"logOutput\":\"‚ÑπÔ∏è **INFO**: üì• Request from user: unknown\\nChannel: unknown\\nThread: unknown\\nMessage: what is the root cause of this bug:\\n<https://tyktech.atlassian.net/browse/TT-16304>\\n\"},\"code-help-error\":{\"issues\":[]},\"customer-insights-error\":{\"issues\":[]},\"release-notes-error\":{\"issues\":[]}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.output":"{\"issues\":[{\"file\":\"system\",\"line\":0,\"ruleId\":\"system/ai-execution-error\",\"message\":\"AI review timed out after 600000ms\",\"severity\":\"error\",\"category\":\"logic\"}],\"debug\":{\"prompt\":\"<instructions>\\n<role>\\nYou are an internal assistant for Tyk API Management engineers.\\nYour task is to classify the user's request and determine what automation should run.\\n</role>\\n\\n<slack_context_instructions>\\nThe full Slack thread conversation is available in the <slack_context> element inside\\nthe <context> block. This XML contains:\\n  - <messages> ‚Äî the complete conversation history from the thread (oldest to newest)\\n  - <current_message> ‚Äî the latest message that triggered this classification\\n\\nIMPORTANT: Always use <slack_context> to understand what the user is really asking:\\n  - Look at <messages> to understand the evolving conversation and prior context\\n  - The <current_message> is the PRIMARY signal for determining intent\\n  - If the latest message is a follow-up, correction, or refers to something discussed earlier,\\n    use the message history to reconstruct the full context\\n</slack_context_instructions>\\n\\n\\n<jira_tickets_referenced>\\n<jira_context>\\n  <issue_count>1</issue_count>\\n  <issue key=\\\"TT-16304\\\">\\n    <summary>Valid OAS API cannot be used by GW due to validation failed error</summary>\\n    <description></description>\\n    <status>Open</status>\\n    <priority>Medium</priority>\\n    <assignee></assignee>\\n    <reporter>Radoslaw Krawczyk</reporter>\\n    <components>Tyk Gateway</components>\\n  </issue>\\n</jira_context>\\n</jira_tickets_referenced>\\n\\n\\n\\n<zendesk_tickets_referenced>\\n<zendesk_context>\\n  <ticket_count>1</ticket_count>\\n  <ticket id=\\\"16304\\\">\\n    <subject>Authorization Header : starts with a space </subject>\\n    <description>\\n\\nWe have a WAF Compliance Issue whereby the WAF asserts a Header for Authorization as such the Value is leading by a space ; Hence a 403 is recieved\\n\\nurl -v --cert qa-ndex.enstreamidentity.com.cer --key bell-preprod.key \\\\\\n\\n-H &quot;enstream-authorization: 8087656ca224433466c00c75847ac248&quot; \\\\\\n\\n&quot;https://apistg.nbd.bell.ca/ndex/getAttributesFromId?appId=3049&amp;idType=mdn&amp;idList=4162160839&amp;attbList=*&quot;\\n\\n*   Trying 184.150.80.90:443...\\n\\n* Connected to apistg.nbd.bell.ca (184.150.80.90) port 443 (#0)\\n\\n* ALPN, offering h2\\n\\n* ALPN, offering http/1.1\\n\\n* successfully set certificate verify locations:\\n\\n*  CAfile: /etc/ssl/cert.pem\\n\\n*  CApath: none\\n\\n* (304) (OUT), TLS handshake, Client hello (1):\\n\\n* (304) (IN), TLS handshake, Server hello (2):\\n\\n* TLSv1.2 (IN), TLS handshake, Certificate (11):\\n\\n* TLSv1.2 (IN), TLS handshake, Server key exchange (12):\\n\\n* TLSv1.2 (IN), TLS handshake, Request CERT (13):\\n\\n* TLSv1.2 (IN), TLS handshake, Server finished (14):\\n\\n* TLSv1.2 (OUT), TLS handshake, Certificate (11):\\n\\n* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):\\n\\n* TLSv1.2 (OUT), TLS handshake, CERT verify (15):\\n\\n* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):\\n\\n* TLSv1.2 (OUT), TLS handshake, Finished (20):\\n\\n* TLSv1.2 (IN), TLS change cipher, Change cipher spec (1):\\n\\n* TLSv1.2 (IN), TLS handshake, Finished (20):\\n\\n* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256\\n\\n* ALPN, server did not agree to a protocol\\n\\n* Server certificate:\\n\\n*  subject: C=CA; ST=Ontario; L=Ottawa; O=Bell Canada; CN=apistg.nbd.bell.ca\\n\\n*  start date: Jan 20 20:32:24 2023 GMT\\n\\n*  expire date: Feb 19 20:32:21 2024 GMT\\n\\n*  subjectAltName: host &quot;apistg.nbd.bell.ca&quot; matched cert&apos;s &quot;apistg.nbd.bell.ca&quot;\\n\\n*  issuer: C=US; O=Entrust, Inc.; OU=See www.entrust.net/legal-terms; OU=(c) 2012 Entrust, Inc. - for authorized use only; CN=Entrust Certification Authority - L1K\\n\\n*  SSL certificate verify ok.\\n\\n&gt; GET /ndex/getAttributesFromId?appId=3049&amp;idType=mdn&amp;idList=4162160839&amp;attbList=* HTTP/1.1\\n\\n&gt; Host: apistg.nbd.bell.ca\\n\\n&gt; User-Agent: curl/7.79.1\\n\\n&gt; Accept: */*\\n\\n&gt; enstream-authorization: 2c760a6ca224433466c00c75847ac248\\n\\n&gt;\\n\\n* Mark bundle as not supporting multiuse\\n\\n&lt; HTTP/1.1 403 Forbidden\\n\\n&lt; Content-Type: application/json\\n\\n&lt; Date: Tue, 07 Mar 2023 16:35:42 GMT\\n\\n&lt; Content-Length: 57\\n\\n&lt; Set-Cookie: TS01ab8fe2=017e5ad8c4423a1f0c76c854eb18d47723f266acc5bb34020cbc2b9674adb0c3412fc32496f3685aa3b22cf1c284a800cc2f29fdcd; Path=/; Secure; HTTPOnly\\n\\n&lt;\\n\\n{\\n\\n    &quot;error&quot;: &quot;Access to this API has been disallowed&quot;\\n\\n* Connection #0 to host apistg.nbd.bell.ca left intact\\n\\n}</description>\\n    <status>closed</status>\\n    <priority>normal</priority>\\n    <created_at>2023-03-13T14:33:03Z</created_at>\\n    <updated_at>2023-03-21T18:02:27Z</updated_at>\\n    <tags>amer, gold, mongodb, on-prem, q_a, sla, us, wes</tags>\\n  </ticket>\\n</zendesk_context>\\n</zendesk_tickets_referenced>\\n\\n\\n<classification_instructions>\\n<intent_determination>\\nDetermine what the user wants based on:\\n  - The <current_message> in <slack_context> is the PRIMARY signal for intent\\n  - Use <messages> history to understand context that informs the current message\\n  - Consider any referenced Jira/Zendesk tickets shown above\\n</intent_determination>\\n\\n<intent_options>\\n  <option value=\\\"chat\\\">general Q&A, follow-up questions, or small talk</option>\\n  <option value=\\\"thread_summary\\\">user explicitly asks for a summary of the thread</option>\\n  <option value=\\\"capabilities\\\">user asks what this assistant can do</option>\\n  <option value=\\\"code_help\\\">user asks how something works in code, implementation details, or documentation. This includes:\\n    - Code/implementation questions (\\\"how does X work in the gateway?\\\")\\n    - Documentation questions (\\\"where is X documented?\\\")\\n    - Technical architecture questions (\\\"how is X implemented?\\\")</option>\\n  <option value=\\\"customer_insights\\\">user asks about existing customers, their usage patterns, deployment architectures, or customer-specific knowledge. This includes:\\n    - Customer usage questions (\\\"how do customers use X?\\\", \\\"what are common deployment patterns?\\\", \\\"who uses feature X?\\\")\\n    - Customer-specific inquiries (\\\"what do we know about customer Y?\\\", \\\"which customers have this problem?\\\")\\n    - Deployment patterns (\\\"what's the typical setup for X?\\\", \\\"how do customers configure Y?\\\")\\n    - Cross-customer analysis (\\\"what are common issues?\\\", \\\"trending problems?\\\")</option>\\n  <option value=\\\"evaluate_ticket\\\">user EXPLICITLY asks to evaluate/assess quality of a Jira ticket AND provides a ticket ID (e.g., \\\"evaluate TT-123\\\", \\\"assess quality of TT-456\\\", \\\"is TT-789 ready for refinement?\\\")</option>\\n  <option value=\\\"engineer\\\">user EXPLICITLY asks to make code changes, create a PR, implement a feature, fix a bug, or modify code (e.g., \\\"create a PR to fix...\\\", \\\"implement...\\\", \\\"add a new endpoint...\\\", \\\"fix the bug in...\\\")</option>\\n  <option value=\\\"release_notes\\\">user asks to generate release notes for a specific product version. Examples:\\n    - \\\"Generate release notes for 5.11.0\\\"\\n    - \\\"Create release notes for Tyk Gateway 5.8.3\\\"\\n    - \\\"Release notes for Pump 1.13.2\\\"\\n    - \\\"What's in Tyk Charts 5.0.0?\\\"\\n    IMPORTANT: For this intent, the topic MUST be ONLY the version identifier in format like \\\"Tyk 5.11.0\\\" or \\\"Tyk Pump 1.13.2\\\" or \\\"Tyk Charts 5.0.0\\\" - nothing else.</option>\\n</intent_options>\\n</classification_instructions>\\n\\n<topic_instructions>\\nBuild a COMPREHENSIVE task description that captures EVERYTHING needed to implement the request.\\n\\n<engineering_intent_requirements critical=\\\"true\\\">\\nWhen intent is \\\"engineer\\\", the topic is NOT a summary ‚Äî it is the COMPLETE SPECIFICATION that will be\\npassed directly to an AI coding agent. The agent will ONLY see this topic, so it must contain:\\n\\n<requirement name=\\\"problem_description\\\">\\n  - What is broken/missing and how it manifests\\n  - Error messages, stack traces, or symptoms discussed\\n  - Steps to reproduce if mentioned\\n  - Root cause analysis from the conversation\\n</requirement>\\n\\n<requirement name=\\\"implementation_details\\\">\\n  - Specific file paths mentioned (e.g., \\\"internal/oashelpers/oashelpers.go\\\")\\n  - Function or method names discussed\\n  - The proposed solution approach from the conversation\\n  - Code snippets or pseudocode if shared\\n  - Any alternative approaches that were rejected and WHY\\n</requirement>\\n\\n<requirement name=\\\"technical_context\\\">\\n  - Related Jira ticket IDs with their key details (don't just reference ‚Äî include the relevant info)\\n  - API contracts, data structures, or interfaces involved\\n  - Concurrency/threading considerations\\n  - Performance requirements or constraints\\n  - Backward compatibility requirements\\n</requirement>\\n\\n<requirement name=\\\"acceptance_criteria\\\">\\n  - What tests should verify (if discussed)\\n  - Edge cases mentioned\\n  - How to validate the fix works\\n</requirement>\\n\\n<requirement name=\\\"conversation_nuances\\\">\\n  - Decisions made during the discussion\\n  - Concerns raised by team members\\n  - Trade-offs that were discussed\\n</requirement>\\n\\n<warning>\\nDO NOT COMPRESS OR SUMMARIZE for engineering tasks. Include every relevant detail from the thread.\\nThe topic can be multiple paragraphs if needed ‚Äî completeness is more important than brevity.\\n</warning>\\n</engineering_intent_requirements>\\n\\n<other_intents_requirements>\\nFor chat, code_help, and other non-engineering intents:\\n  - Keep it concise but still self-contained\\n  - 1-3 sentences capturing the question/request\\n</other_intents_requirements>\\n</topic_instructions>\\n\\n<examples intent=\\\"engineer\\\">\\n<example quality=\\\"bad\\\" reason=\\\"too vague\\\">\\nFix race condition in oashelpers.go\\n</example>\\n\\n<example quality=\\\"bad\\\" reason=\\\"missing all details\\\">\\nCreate PR to fix concurrent import issue as discussed\\n</example>\\n\\n<example quality=\\\"good\\\">\\nFix race condition in tyk-analytics internal/oashelpers/oashelpers.go that occurs during\\nconcurrent OpenAPI bundle imports. The issue: when multiple imports use the same filename, they\\nconflict because they share a temp directory. Root cause identified by @alice: the createTempDir()\\nfunction uses a static path. Solution discussed: generate unique temp directory per import using\\nUUID prefix. Must handle cleanup on both success and failure paths. Per TT-1234, this blocks the\\n3.0 release. Add test that spawns 10 concurrent imports with same filename and verifies no conflicts.\\n@bob noted we should also add a mutex as defense-in-depth. Consider backward compat with existing\\ntemp file cleanup cron job.\\n</example>\\n</examples>\\n\\n</instructions>\\n\\n<context>\\n\\n<slack_context>\\n  <transport>slack</transport>\\n  <thread>\\n    <id>D09SZABNLG3:1767898854.946159</id>\\n    <url>https://slack.com/archives/D09SZABNLG3/p1767898854946159</url>\\n  </thread>\\n  <attributes>\\n    <attribute>\\n      <key>channel</key>\\n      <value>D09SZABNLG3</value>\\n    </attribute>\\n    <attribute>\\n      <key>user</key>\\n      <value>U3P2L4XNE</value>\\n    </attribute>\\n    <attribute>\\n      <key>thread_ts</key>\\n      <value>1767898854.946159</value>\\n    </attribute>\\n    <attribute>\\n      <key>event_timestamp</key>\\n      <value>1767898855831</value>\\n    </attribute>\\n  </attributes>\\n  <messages>\\n    <message>\\n      <role>user</role>\\n      <user>U3P2L4XNE</user>\\n      <text><@U09T5KRLMPU> what is the root cause of this bug:\\n<https://tyktech.atlassian.net/browse/TT-16304></text>\\n      <timestamp>1767898854.946159</timestamp>\\n      <origin></origin>\\n    </message>\\n  </messages>\\n  <current>\\n    <role>user</role>\\n    <user>U3P2L4XNE</user>\\n    <text>what is the root cause of this bug:\\n<https://tyktech.atlassian.net/browse/TT-16304></text>\\n    <timestamp>1767898854.946159</timestamp>\\n    <origin></origin>\\n  </current>\\n</slack_context>\\n</context>\",\"rawResponse\":\"\",\"provider\":\"google\",\"model\":\"default\",\"apiKeySource\":\"GOOGLE_API_KEY\",\"processingTime\":600000,\"promptLength\":11290,\"responseLength\":0,\"errors\":[\"AI review timed out after 600000ms\"],\"jsonParseSuccess\":false,\"timestamp\":\"2026-01-08T19:00:58.032Z\",\"schemaName\":\"custom\",\"schema\":\"{\\n  \\\"schema\\\": \\\"{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"intent\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"enum\\\\\\\":[\\\\\\\"chat\\\\\\\",\\\\\\\"thread_summary\\\\\\\",\\\\\\\"capabilities\\\\\\\",\\\\\\\"refine\\\\\\\",\\\\\\\"code_help\\\\\\\",\\\\\\\"customer_insights\\\\\\\",\\\\\\\"evaluate_ticket\\\\\\\",\\\\\\\"engineer\\\\\\\",\\\\\\\"release_notes\\\\\\\"]},\\\\\\\"topic\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\"}},\\\\\\\"required\\\\\\\":[\\\\\\\"intent\\\\\\\",\\\\\\\"topic\\\\\\\"]}\\\"\\n}\"}}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"4e292bff-4f85-4205-8705-43be10ba4797"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"jira-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"zendesk-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"log-request","visor.provider.type":"log"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"d524830f-cb64-46b2-8b1e-73a546f4394d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"d524830f-cb64-46b2-8b1e-73a546f4394d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"d524830f-cb64-46b2-8b1e-73a546f4394d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-keys","visor.provider.type":"script"}}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-ids","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"history\":{}},\"outputs_history\":{},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"include_comments\":true,\"max_tickets\":5,\"download_assets\":true,\"cache_dir\":\".cache/zendesk\",\"text\":\"<@U09T5KRLMPU> what is the root cause of this bug:\\n<https://tyktech.atlassian.net/browse/TT-16304>\\nwhat is the root cause of this bug:\\n<https://tyktech.atlassian.net/browse/TT-16304>\\n\"},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.output":"{\"data\":[\"16304\"],\"count\":1}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"d524830f-cb64-46b2-8b1e-73a546f4394d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"d524830f-cb64-46b2-8b1e-73a546f4394d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"d524830f-cb64-46b2-8b1e-73a546f4394d"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"fetch-issues","visor.provider.type":"http_client"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"fetch-tickets","visor.provider.type":"http_client"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"d524830f-cb64-46b2-8b1e-73a546f4394d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"d524830f-cb64-46b2-8b1e-73a546f4394d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"d524830f-cb64-46b2-8b1e-73a546f4394d"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"format-output","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"d524830f-cb64-46b2-8b1e-73a546f4394d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"d524830f-cb64-46b2-8b1e-73a546f4394d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"d524830f-cb64-46b2-8b1e-73a546f4394d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"prepare-attachments","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"aggregate-downloads","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"format-output","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"ed4900c3-7954-45a9-8c8b-23e216cba78f"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"route-intent","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.input.context":"{\"pr\":{\"number\":0,\"title\":\"Local Analysis: feat/slack-frontend-v2 (72 modified)\",\"author\":\"Leonid Bugaev\",\"branch\":\"feat/slack-frontend-v2\",\"base\":\"main\"},\"files\":[{\"filename\":\"CLAUDE.md\",\"additions\":14,\"deletions\":1,\"changes\":15,\"patch\":\"diff --git a/CLAUDE.md b/CLAUDE.md\\nindex 009271f9..ae68c57e 100644\\n--- a/CLAUDE.md\\n+++ b/CLAUDE.md\\n@@ -150,4 +150,17 @@ Configuration supports:\\n    - Safe JSON parsing: `try { JSON.parse(output) } catch(e) { log(\\\"Error:\\\", e) }`\\n    - Validate structure: `log(\\\"Is array?\\\", Array.isArray(outputs[\\\"check-name\\\"]));`\\n \\n+6. **Tracing with OTel/Jaeger**:\\n+   - Enable telemetry: `VISOR_TELEMETRY_ENABLED=true`, `VISOR_TELEMETRY_SINK=otlp`,\\n+     `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://localhost:4318/v1/traces`\\n+   - Root span: `visor.run` (one per CLI/Slack execution)\\n+   - State spans: `engine.state.*` with `wave`, `wave_kind`, `session_id`\\n+   - Check spans: `visor.check.<checkId>` with `visor.check.id`, `visor.check.type`,\\n+     `visor.foreach.index` (for map fanout)\\n+   - Routing decisions: `visor.routing` events attached to the active state span; fields\\n+     include `trigger`, `action`, `source`, `target`, `scope`, `goto_event` (repeats\\n+     across waves show routing loops)\\n+   - Wave visibility: `engine.state.level_dispatch` includes `level_size` and\\n+     `level_checks_preview` for the planned wave\\n+\\n See `docs/debugging.md` for comprehensive debugging guide.\\n\",\"status\":\"modified\"},{\"filename\":\"README.md\",\"additions\":7,\"deletions\":1,\"changes\":8,\"patch\":\"diff --git a/README.md b/README.md\\nindex 41131861..d92f02cc 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -534,6 +534,12 @@ steps:\\n     session_mode: append    # Shares history for full conversation\\n ```\\n \\n+You can also reuse the **same check‚Äôs** session when it loops back to itself, using:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` with `session_mode: append`\\n+\\n+See the standalone example at `examples/session-reuse-self.yaml` and the detailed guide in [docs/advanced-ai.md](docs/advanced-ai.md).\\n+\\n Learn more: [docs/advanced-ai.md](docs/advanced-ai.md)\\n \\n ## üìã Schema-Template System\\n\",\"status\":\"modified\"},{\"filename\":\"docs/advanced-ai.md\",\"additions\":61,\"deletions\":2,\"changes\":63,\"patch\":\"diff --git a/docs/advanced-ai.md b/docs/advanced-ai.md\\nindex bb8e684a..879285ed 100644\\n--- a/docs/advanced-ai.md\\n+++ b/docs/advanced-ai.md\\n@@ -1,7 +1,7 @@\\n ## üß† Advanced AI Features\\n \\n ### AI Session Reuse\\n-Use `reuse_ai_session: true` on dependent checks to continue conversation context with the AI across checks. This improves follow‚Äëups and consistency.\\n+Use `reuse_ai_session` on checks to continue conversation context with the AI across steps. This improves follow‚Äëups and consistency for follow‚Äëon analysis and chat‚Äëstyle flows.\\n \\n **Session Modes:**\\n - **`clone` (default)**: Creates a copy of the conversation history. Each check gets an independent session with the same starting context. Changes made by one check don't affect others.\\n@@ -26,6 +26,65 @@ steps:\\n     depends_on: [security-remediation]\\n     reuse_ai_session: true\\n     session_mode: append  # Share history - sees full conversation\\n+\\n+#### Reusing your own session: `reuse_ai_session: self`\\n+\\n+Sometimes the step you want to loop back into is the AI step itself (e.g. Slack assistants or multi‚Äëturn internal tools). For that case you can use:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` ‚Äì the step reuses its **own** Probe session when it runs again in the same engine run.\\n+- `session_mode: append` ‚Äì makes the follow‚Äëup behave like a normal conversation turn.\\n+\\n+On the first run of the step, Visor creates a new ProbeAgent session and registers it. If routing (`on_success.goto`, `goto_js`, etc.) later jumps back to the same step within the same run, the engine:\\n+\\n+- Finds the last result for that step in the current run.\\n+- Reads the `sessionId` stored in the result.\\n+- Calls the AI provider again using `executeReviewWithSessionReuse` with that session id.\\n+\\n+Simple example (no transport wiring, just CLI/tests):\\n+\\n+```yaml\\n+version: \\\"2.0\\\"\\n+\\n+steps:\\n+  seed:\\n+    type: script\\n+    content: |\\n+      return { text: \\\"hello from seed\\\" };\\n+\\n+  convo:\\n+    type: ai\\n+    depends_on: [seed]\\n+    reuse_ai_session: self\\n+    session_mode: append\\n+    ai:\\n+      provider: mock\\n+      model: mock\\n+      disableTools: true\\n+      allowedTools: []\\n+      system_prompt: \\\"You are a tiny echo assistant.\\\"\\n+    prompt: |\\n+      Seed message: {{ outputs['seed'].text }}\\n+\\n+      Past convo outputs in this run:\\n+      {% assign hist = outputs_history['convo'] | default: empty %}\\n+      {% if hist and hist.size > 0 %}\\n+      {% for h in hist %}\\n+      - Previous reply {{ forloop.index }}.\\n+      {% endfor %}\\n+      {% else %}\\n+      - No previous replies yet.\\n+      {% endif %}\\n+    on_success:\\n+      goto_js: |\\n+        // Example: re‚Äëenter this step up to 3 times in a single run\\n+        return attempt < 3 ? 'convo' : null;\\n+```\\n+\\n+The corresponding testable example lives at:\\n+\\n+- `examples/session-reuse-self.yaml`\\n+\\n+This keeps the configuration small but shows how to wire `reuse_ai_session: self` and `session_mode: append` without touching higher‚Äëlevel workflows like `tyk-assistant`.\\n ```\\n \\n **When to use each mode:**\\n\",\"status\":\"modified\"},{\"filename\":\"docs/recipes.md\",\"additions\":149,\"deletions\":3,\"changes\":152,\"patch\":\"diff --git a/docs/recipes.md b/docs/recipes.md\\nindex 1015bc6c..1089e9a5 100644\\n--- a/docs/recipes.md\\n+++ b/docs/recipes.md\\n@@ -321,8 +321,154 @@ Tip: When you define a JSON Schema, you generally do **not** need to tell the mo\\n - Avoid noisy fallbacks like `(outputs['x']?.kind ?? '') === 'status'` when `outputs['x']?.kind === 'status'` is equivalent.\\n - These conventions apply uniformly to any provider (`ai`, `command`, `script`, `github`, `http_client`, etc).\\n \\n+### Command step best practices\\n+\\n+When using `type: command` steps:\\n+\\n+**Avoid external tool dependencies** like `jq`, `yq`, `python`, etc.:\\n+- They may not be installed in all environments (GitHub Actions, Docker, CI)\\n+- Use `transform_js` to parse and transform output instead\\n+- Keep shell commands simple: `grep`, `sed`, `awk`, `sort`, `head` are universally available\\n+\\n+```yaml\\n+# Bad - requires jq\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | jq -R -s 'split(\\\"\\\\n\\\")'\\n+  parseJson: true\\n+\\n+# Good - use transform_js for parsing\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | sort -u\\n+  transform_js: |\\n+    const lines = (output || '').trim().split('\\\\n').filter(Boolean);\\n+    return { data: lines, count: lines.length };\\n+```\\n+\\n+**Prefer line-separated output** over JSON from shell:\\n+- Simple to parse with `transform_js`\\n+- No need for `parseJson: true`\\n+- More robust across different shells/environments\\n+\\n+**Use transform_js for structured output**:\\n+- The sandbox provides `output` (command stdout as string)\\n+- Return an object with the fields you need\\n+- Works consistently across all environments\\n+\\n+### Testing workflows with `--no-mocks`\\n+\\n+The `--no-mocks` flag runs your test cases with real providers instead of injecting mock responses. This is essential for:\\n+\\n+1. **Debugging integration issues** - See actual API responses and errors\\n+2. **Capturing realistic mock data** - Get real output to copy into your test cases\\n+3. **Validating credentials** - Verify environment variables are set correctly\\n+4. **Developing new workflows** - Build tests incrementally with real data\\n+\\n+#### Basic usage\\n+\\n+```bash\\n+# Run all test cases with real providers\\n+visor test --config my-workflow.yaml --no-mocks\\n+\\n+# Run a specific test case with real providers\\n+visor test --config my-workflow.yaml --no-mocks --only \\\"my-test-case\\\"\\n+```\\n+\\n+#### Suggested mocks output\\n+\\n+When running with `--no-mocks`, Visor captures each step's output and prints it as YAML you can copy directly into your test case:\\n+\\n+```\\n+üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)\\n+   Step outputs will be captured and printed as suggested mocks\\n+\\n+... test execution ...\\n+\\n+üìã Suggested mocks (copy to your test case):\\n+mocks:\\n+  extract-keys:\\n+    data:\\n+      - PROJ-123\\n+      - DEV-456\\n+    count: 2\\n+  fetch-issues:\\n+    data:\\n+      - key: PROJ-123\\n+        summary: Fix authentication bug\\n+        status: In Progress\\n+```\\n+\\n+Copy the YAML under `mocks:` into your test case's `mocks:` section.\\n+\\n+#### Workflow for building tests\\n+\\n+1. **Start with a minimal test case** (no mocks):\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+   ```\\n+\\n+2. **Run with `--no-mocks`** to capture real outputs:\\n+   ```bash\\n+   visor test --config workflow.yaml --no-mocks --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+3. **Copy the suggested mocks** into your test case:\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+         mocks:\\n+           extract-keys:\\n+             data: [\\\"PROJ-123\\\"]\\n+             count: 1\\n+           # ... rest of captured mocks\\n+   ```\\n+\\n+4. **Add assertions** based on the real data:\\n+   ```yaml\\n+         expect:\\n+           workflow_output:\\n+             - path: issue_count\\n+               equals: 1\\n+   ```\\n+\\n+5. **Run normally** to verify mocks work:\\n+   ```bash\\n+   visor test --config workflow.yaml --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+#### Debugging with `--no-mocks`\\n+\\n+When a test fails with mocks, use `--no-mocks` to see what's actually happening:\\n+\\n+```bash\\n+# See real API responses and errors\\n+visor test --config workflow.yaml --no-mocks --only \\\"failing-test\\\"\\n+\\n+# Common issues revealed:\\n+# - Missing or expired credentials\\n+# - API endpoint changes\\n+# - Unexpected response formats\\n+# - Network/timeout issues\\n+```\\n+\\n+The real error messages and responses help identify whether the issue is with your mocks or the actual integration.\\n+\\n ### More examples\\n \\n-- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags  \\n-- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration  \\n+- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags\\n+- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration\\n - `examples/` ‚Äì MCP, Jira, and advanced configs\\n\",\"status\":\"modified\"},{\"filename\":\"package-lock.json\",\"additions\":252,\"deletions\":9,\"changes\":261,\"patch\":\"diff --git a/package-lock.json b/package-lock.json\\nindex 5fb3ad89..e5b10d27 100644\\n--- a/package-lock.json\\n+++ b/package-lock.json\\n@@ -7,6 +7,7 @@\\n     \\\"\\\": {\\n       \\\"name\\\": \\\"@probelabs/visor\\\",\\n       \\\"version\\\": \\\"0.1.42\\\",\\n+      \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"MIT\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@actions/core\\\": \\\"^1.11.1\\\",\\n@@ -16,7 +17,7 @@\\n         \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n         \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n         \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n         \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n         \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -54,6 +55,7 @@\\n         \\\"husky\\\": \\\"^9.1.7\\\",\\n         \\\"jest\\\": \\\"^30.1.3\\\",\\n         \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+        \\\"patch-package\\\": \\\"^8.0.1\\\",\\n         \\\"prettier\\\": \\\"^3.6.2\\\",\\n         \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n         \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n@@ -5942,9 +5944,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/maid\\\": {\\n-      \\\"version\\\": \\\"0.0.21\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.21.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-H3EA2tDjWsAPgXZAwfHrNoEXnHmQFS9+JGDK/YpeiF3g3FxPgnWBgtgYPgtNuETQsEXxR1RzDgQVJL2JHWRRHw==\\\",\\n+      \\\"version\\\": \\\"0.0.22\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.22.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-FYcyyP4BlVrHQmBznXFPpMbvUqFI6wbecHUUe4S+yIeDvJ8W4bP0ndZ0cMoc2p7hekWYAG4RtlohQBr8e4TKOg==\\\",\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@types/dagre\\\": \\\"^0.7.53\\\",\\n@@ -5960,9 +5962,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/probe\\\": {\\n-      \\\"version\\\": \\\"0.6.0-rc169\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc169.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-/laV3+1TSjASru8gGSvGwyjQ7jalMkFuZETl35uv37J1Hb4eG95CbKGU5NihcCrLXR44q1lKJiQGHXcAav2mBA==\\\",\\n+      \\\"version\\\": \\\"0.6.0-rc176\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc176.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-w/pAvnz2/yXb4FQXZV2QazxuW02xsI/wRg36yxZoLSG0O/hIncjF4rquZe3O+emceI2Ks5zbrA7cgQEdpwT8vA==\\\",\\n       \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n@@ -5972,7 +5974,7 @@\\n         \\\"@ai-sdk/openai\\\": \\\"^2.0.10\\\",\\n         \\\"@anthropic-ai/claude-agent-sdk\\\": \\\"^0.1.46\\\",\\n         \\\"@modelcontextprotocol/sdk\\\": \\\"^1.0.0\\\",\\n-        \\\"@probelabs/maid\\\": \\\"^0.0.21\\\",\\n+        \\\"@probelabs/maid\\\": \\\"^0.0.22\\\",\\n         \\\"adm-zip\\\": \\\"^0.5.16\\\",\\n         \\\"ai\\\": \\\"^5.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -8192,6 +8194,13 @@\\n         \\\"node\\\": \\\">= 20\\\"\\n       }\\n     },\\n+    \\\"node_modules/@yarnpkg/lockfile\\\": {\\n+      \\\"version\\\": \\\"1.1.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@yarnpkg/lockfile/-/lockfile-1.1.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-GpSwvyXOcOOlV70vbnzjj4fW5xW/FdUF6nQEt1ENy7m4ZCczi1+/buVUPAqmGfqznsORNFzUMjctTIp8a9tuCQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"BSD-2-Clause\\\"\\n+    },\\n     \\\"node_modules/accepts\\\": {\\n       \\\"version\\\": \\\"1.3.8\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/accepts/-/accepts-1.3.8.tgz\\\",\\n@@ -9180,6 +9189,25 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/call-bind\\\": {\\n+      \\\"version\\\": \\\"1.0.8\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind/-/call-bind-1.0.8.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-oKlSFMcMwpUg2ednkhQ454wfWiU/ul3CkJe/PEHcTKuiX6RpbehUiFMXu13HalGZxfUwCQzZG747YXBn1im9ww==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind-apply-helpers\\\": \\\"^1.0.0\\\",\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"set-function-length\\\": \\\"^1.2.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/call-bind-apply-helpers\\\": {\\n       \\\"version\\\": \\\"1.0.2\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind-apply-helpers/-/call-bind-apply-helpers-1.0.2.tgz\\\",\\n@@ -10613,6 +10641,24 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/define-data-property\\\": {\\n+      \\\"version\\\": \\\"1.1.4\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/define-data-property/-/define-data-property-1.1.4.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-rBMvIzlpA8v6E+SJZoo++HAYqsLrkg7MSfIinMPFhmkorw7X+dOXVJQs+QT69zGkzMyfDnIMN2Wid1+NbL3T+A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/define-lazy-prop\\\": {\\n       \\\"version\\\": \\\"3.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/define-lazy-prop/-/define-lazy-prop-3.0.0.tgz\\\",\\n@@ -11794,6 +11840,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/find-yarn-workspace-root\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/find-yarn-workspace-root/-/find-yarn-workspace-root-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-1IMnbjt4KzsQfnhnzNd8wUEgXZ44IzZaZmnLYx7D5FZlaHt2gW20Cri8Q+E/t5tIj4+epTBub+2Zxu/vNILzqQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Apache-2.0\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"micromatch\\\": \\\"^4.0.2\\\"\\n+      }\\n+    },\\n     \\\"node_modules/fix-dts-default-cjs-exports\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/fix-dts-default-cjs-exports/-/fix-dts-default-cjs-exports-1.0.1.tgz\\\",\\n@@ -12267,6 +12323,19 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/has-property-descriptors\\\": {\\n+      \\\"version\\\": \\\"1.0.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/has-property-descriptors/-/has-property-descriptors-1.0.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-55JNKuIW+vq4Ke1BjOTjM2YctQIvCT7GFzHwmfZPGo5wnrgkid0YQtnAleFSqumZm4az3n2BS+erby5ipJdgrg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/has-symbols\\\": {\\n       \\\"version\\\": \\\"1.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/has-symbols/-/has-symbols-1.1.0.tgz\\\",\\n@@ -12784,6 +12853,13 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/isarray\\\": {\\n+      \\\"version\\\": \\\"2.0.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/isarray/-/isarray-2.0.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-xHjhDr3cNBK0BzdUJSPXZntQUx/mwMS5Rw4A7lPJ90XGAO6ISP/ePDNuo0vhqOZU+UD5JoodwCAAoZQd3FeAKw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\"\\n+    },\\n     \\\"node_modules/isexe\\\": {\\n       \\\"version\\\": \\\"2.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz\\\",\\n@@ -13594,6 +13670,26 @@\\n       \\\"integrity\\\": \\\"sha512-NM8/P9n3XjXhIZn1lLhkFaACTOURQXjWhV4BA/RnOv8xvgqtqpAX9IO4mRQxSx1Rlo4tqzeqb0sOlruaOy3dug==\\\",\\n       \\\"license\\\": \\\"MIT\\\"\\n     },\\n+    \\\"node_modules/json-stable-stringify\\\": {\\n+      \\\"version\\\": \\\"1.3.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify/-/json-stable-stringify-1.3.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-qtYiSSFlwot9XHtF9bD9c7rwKjr+RecWT//ZnPvSmEjpV5mmPOCN4j8UjY5hbjNkOwZ/jQv3J6R1/pL7RwgMsg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind\\\": \\\"^1.0.8\\\",\\n+        \\\"call-bound\\\": \\\"^1.0.4\\\",\\n+        \\\"isarray\\\": \\\"^2.0.5\\\",\\n+        \\\"jsonify\\\": \\\"^0.0.1\\\",\\n+        \\\"object-keys\\\": \\\"^1.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/json-stable-stringify-without-jsonify\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify-without-jsonify/-/json-stable-stringify-without-jsonify-1.0.1.tgz\\\",\\n@@ -13633,6 +13729,16 @@\\n         \\\"graceful-fs\\\": \\\"^4.1.6\\\"\\n       }\\n     },\\n+    \\\"node_modules/jsonify\\\": {\\n+      \\\"version\\\": \\\"0.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/jsonify/-/jsonify-0.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-2/Ki0GcmuqSrgFyelQq9M05y7PS0mEwuIzrf3f1fPqkVDVRvZrPZtVSMHxdgo8Aq0sxAOb/cr2aqqA3LeWHVPg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Public Domain\\\",\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/katex\\\": {\\n       \\\"version\\\": \\\"0.16.25\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/katex/-/katex-0.16.25.tgz\\\",\\n@@ -13676,6 +13782,16 @@\\n       \\\"integrity\\\": \\\"sha512-Ls993zuzfayK269Svk9hzpeGUKob/sIgZzyHYdjQoAdQetRKpOLj+k/QQQ/6Qi0Yz65mlROrfd+Ev+1+7dz9Kw==\\\",\\n       \\\"dev\\\": true\\n     },\\n+    \\\"node_modules/klaw-sync\\\": {\\n+      \\\"version\\\": \\\"6.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/klaw-sync/-/klaw-sync-6.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-nIeuVSzdCCs6TDPTqI8w1Yre34sSq7AkZ4B3sfOBbI2CgVSB4Du4aLQijFU2+lhAFCwt9+42Hel6lQNIv6AntQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"graceful-fs\\\": \\\"^4.1.11\\\"\\n+      }\\n+    },\\n     \\\"node_modules/kolorist\\\": {\\n       \\\"version\\\": \\\"1.8.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/kolorist/-/kolorist-1.8.0.tgz\\\",\\n@@ -14785,6 +14901,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n       }\\n     },\\n+    \\\"node_modules/object-keys\\\": {\\n+      \\\"version\\\": \\\"1.1.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/object-keys/-/object-keys-1.1.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NuAESUOUMrlIXOfHKzD6bpPu3tYt3xvjNdRIQ+FeT0lNb4K8WR70CaDxhuNguS2XG+GjkyMwOzsN5ZktImfhLA==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/ohash\\\": {\\n       \\\"version\\\": \\\"2.0.11\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/ohash/-/ohash-2.0.11.tgz\\\",\\n@@ -15022,6 +15148,95 @@\\n         \\\"node\\\": \\\">= 0.8\\\"\\n       }\\n     },\\n+    \\\"node_modules/patch-package\\\": {\\n+      \\\"version\\\": \\\"8.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/patch-package/-/patch-package-8.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-VsKRIA8f5uqHQ7NGhwIna6Bx6D9s/1iXlA1hthBVBEbkq+t4kXD0HHt+rJhf/Z+Ci0F/HCB2hvn0qLdLG+Qxlw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"@yarnpkg/lockfile\\\": \\\"^1.1.0\\\",\\n+        \\\"chalk\\\": \\\"^4.1.2\\\",\\n+        \\\"ci-info\\\": \\\"^3.7.0\\\",\\n+        \\\"cross-spawn\\\": \\\"^7.0.3\\\",\\n+        \\\"find-yarn-workspace-root\\\": \\\"^2.0.0\\\",\\n+        \\\"fs-extra\\\": \\\"^10.0.0\\\",\\n+        \\\"json-stable-stringify\\\": \\\"^1.0.2\\\",\\n+        \\\"klaw-sync\\\": \\\"^6.0.0\\\",\\n+        \\\"minimist\\\": \\\"^1.2.6\\\",\\n+        \\\"open\\\": \\\"^7.4.2\\\",\\n+        \\\"semver\\\": \\\"^7.5.3\\\",\\n+        \\\"slash\\\": \\\"^2.0.0\\\",\\n+        \\\"tmp\\\": \\\"^0.2.4\\\",\\n+        \\\"yaml\\\": \\\"^2.2.2\\\"\\n+      },\\n+      \\\"bin\\\": {\\n+        \\\"patch-package\\\": \\\"index.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14\\\",\\n+        \\\"npm\\\": \\\">5\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/ci-info\\\": {\\n+      \\\"version\\\": \\\"3.9.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/ci-info/-/ci-info-3.9.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NIxF55hv4nSqQswkAeiOi1r83xy8JldOFDTWiug55KBu9Jnblncd2U6ViHmYgHf01TPZS77NJBhBMKdWj9HQMQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"funding\\\": [\\n+        {\\n+          \\\"type\\\": \\\"github\\\",\\n+          \\\"url\\\": \\\"https://github.com/sponsors/sibiraj-s\\\"\\n+        }\\n+      ],\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/is-docker\\\": {\\n+      \\\"version\\\": \\\"2.2.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/is-docker/-/is-docker-2.2.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-F+i2BKsFrH66iaUFc0woD8sLy8getkwTwtOBjvs56Cx4CgJDeKQeqfz8wAYiSb8JOprWhHH5p77PbmYCvvUuXQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"bin\\\": {\\n+        \\\"is-docker\\\": \\\"cli.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/open\\\": {\\n+      \\\"version\\\": \\\"7.4.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/open/-/open-7.4.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-MVHddDVweXZF3awtlAS+6pgKLlm/JgxZ90+/NBurBoQctVOOB/zDdVjcyPzQ+0laDGbsWgrRkflI65sQeOgT9Q==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"is-docker\\\": \\\"^2.0.0\\\",\\n+        \\\"is-wsl\\\": \\\"^2.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/slash\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/slash/-/slash-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-ZYKh3Wh2z1PpEXWr0MpSBZ0V6mZHAQfYevttO11c51CaWjGTaadiKZ+wVt1PbMlDV5qhMFslpZCemhwOK7C89A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=6\\\"\\n+      }\\n+    },\\n     \\\"node_modules/path-data-parser\\\": {\\n       \\\"version\\\": \\\"0.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/path-data-parser/-/path-data-parser-0.1.0.tgz\\\",\\n@@ -16473,6 +16688,24 @@\\n         \\\"node\\\": \\\">= 0.8.0\\\"\\n       }\\n     },\\n+    \\\"node_modules/set-function-length\\\": {\\n+      \\\"version\\\": \\\"1.2.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/set-function-length/-/set-function-length-1.2.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-pgRc4hJ4/sNjWCSS9AmnS40x3bNMDTknHgL5UaMBTMyJnU90EgWh1Rz+MC9eFu4BuN/UwZjKQuY/1v3rM7HMfg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"define-data-property\\\": \\\"^1.1.4\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"function-bind\\\": \\\"^1.1.2\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\",\\n+        \\\"has-property-descriptors\\\": \\\"^1.0.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/setprototypeof\\\": {\\n       \\\"version\\\": \\\"1.2.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/setprototypeof/-/setprototypeof-1.2.0.tgz\\\",\\n@@ -17384,6 +17617,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/tmp\\\": {\\n+      \\\"version\\\": \\\"0.2.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/tmp/-/tmp-0.2.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-voyz6MApa1rQGUxT3E+BK7/ROe8itEx7vD8/HEvt4xwXucvQ5G5oeEiHkmHZJuBO21RpOf+YYm9MOivj709jow==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14.14\\\"\\n+      }\\n+    },\\n     \\\"node_modules/tmpl\\\": {\\n       \\\"version\\\": \\\"1.0.5\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/tmpl/-/tmpl-1.0.5.tgz\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"package.json\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/package.json b/package.json\\nindex f9ccbb6c..302d728f 100644\\n--- a/package.json\\n+++ b/package.json\\n@@ -49,6 +49,7 @@\\n     \\\"test:with-build\\\": \\\"npm run build:cli && jest\\\",\\n     \\\"test:yaml\\\": \\\"node dist/index.js test --progress compact\\\",\\n     \\\"prepare\\\": \\\"husky\\\",\\n+    \\\"postinstall\\\": \\\"patch-package\\\",\\n     \\\"pre-commit\\\": \\\"lint-staged\\\",\\n     \\\"deploy:site\\\": \\\"cd site && npx wrangler pages deploy . --project-name=visor-site --commit-dirty=true\\\",\\n     \\\"deploy:worker\\\": \\\"npx wrangler deploy\\\",\\n@@ -96,7 +97,7 @@\\n     \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n     \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n     \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n     \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n     \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n     \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -143,6 +144,7 @@\\n     \\\"husky\\\": \\\"^9.1.7\\\",\\n     \\\"jest\\\": \\\"^30.1.3\\\",\\n     \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+    \\\"patch-package\\\": \\\"^8.0.1\\\",\\n     \\\"prettier\\\": \\\"^3.6.2\\\",\\n     \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n     \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"src/ai-review-service.ts\",\"additions\":101,\"deletions\":151,\"changes\":252,\"patch\":\"diff --git a/src/ai-review-service.ts b/src/ai-review-service.ts\\nindex 6b9c5b2e..f18accea 100644\\n--- a/src/ai-review-service.ts\\n+++ b/src/ai-review-service.ts\\n@@ -65,6 +65,13 @@ export interface AIReviewConfig {\\n   allowBash?: boolean;\\n   // Advanced bash command execution configuration\\n   bashConfig?: import('./types/config').BashConfig;\\n+  // Optional workspace root and allowed folders for ProbeAgent.\\n+  // When provided, these are forwarded to ProbeAgent so tools like search/query\\n+  // operate inside the isolated workspace/projects instead of the Visor repo root.\\n+  path?: string;\\n+  allowedFolders?: string[];\\n+  // Completion prompt for post-completion validation/review (runs after attempt_completion)\\n+  completionPrompt?: string;\\n }\\n \\n export interface AIDebugInfo {\\n@@ -278,7 +285,7 @@ export class AIReviewService {\\n     try {\\n       const call = this.callProbeAgent(prompt, schema, debugInfo, checkName, sessionId);\\n       const timeoutMs = Math.max(0, this.config.timeout || 0);\\n-      const { response, effectiveSchema } =\\n+      const { response, effectiveSchema, sessionId: usedSessionId } =\\n         timeoutMs > 0 ? await this.withTimeout(call, timeoutMs, 'AI review') : await call;\\n       const processingTime = Date.now() - startTime;\\n \\n@@ -290,6 +297,11 @@ export class AIReviewService {\\n \\n       const result = this.parseAIResponse(response, debugInfo, effectiveSchema);\\n \\n+      // Expose the session ID used for this call so the engine can reuse it later\\n+      try {\\n+        (result as any).sessionId = usedSessionId;\\n+      } catch {}\\n+\\n       if (debugInfo) {\\n         result.debug = debugInfo;\\n       }\\n@@ -651,17 +663,34 @@ ${prContext}${slackContextXml}\\n     const prContextInfo = prInfo as PRInfo & {\\n       isPRContext?: boolean;\\n       includeCodeContext?: boolean;\\n+      slackConversation?: unknown;\\n     };\\n     const isIssue = prContextInfo.isIssue === true;\\n \\n     // Check if we should include code context (diffs)\\n     const isPRContext = prContextInfo.isPRContext === true;\\n-    // In PR context, always include diffs. Otherwise check the flag.\\n-    const includeCodeContext = isPRContext || prContextInfo.includeCodeContext !== false;\\n+    const isSlackMode = prContextInfo.slackConversation !== undefined;\\n+\\n+    // Determine whether to include code context:\\n+    // - In explicit PR context (GitHub PR events), always include diffs\\n+    // - In Slack mode, default to NO code context unless explicitly requested\\n+    // - Otherwise, include code context unless explicitly disabled\\n+    let includeCodeContext: boolean;\\n+    if (isPRContext) {\\n+      includeCodeContext = true;\\n+    } else if (isSlackMode) {\\n+      // In Slack mode, only include code context if explicitly set to true\\n+      includeCodeContext = prContextInfo.includeCodeContext === true;\\n+    } else {\\n+      // Default: include unless explicitly disabled\\n+      includeCodeContext = prContextInfo.includeCodeContext !== false;\\n+    }\\n \\n     // Log the decision for transparency (debug level)\\n     if (isPRContext) {\\n       log('üîç Including full code diffs in AI context (PR mode)');\\n+    } else if (isSlackMode && !includeCodeContext) {\\n+      log('üí¨ Slack mode: excluding code diffs (use includeCodeContext: true to enable)');\\n     } else if (!includeCodeContext) {\\n       log('üìä Including only file summary in AI context (no diffs)');\\n     } else {\\n@@ -1537,7 +1566,15 @@ ${'='.repeat(60)}\\n     debugInfo?: AIDebugInfo,\\n     _checkName?: string,\\n     providedSessionId?: string\\n-  ): Promise<{ response: string; effectiveSchema?: string }> {\\n+  ): Promise<{ response: string; effectiveSchema?: string; sessionId: string }> {\\n+    // Derive a stable session ID for this call so the engine can reuse it later\\n+    const sessionId =\\n+      providedSessionId ||\\n+      (() => {\\n+        const timestamp = new Date().toISOString();\\n+        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n+      })();\\n+\\n     // Handle mock model/provider\\n     if (this.config.model === 'mock' || this.config.provider === 'mock') {\\n       const inJest = !!process.env.JEST_WORKER_ID;\\n@@ -1545,19 +1582,17 @@ ${'='.repeat(60)}\\n       if (!inJest) {\\n         // Fast path for CLI/integration: synthesize a mock response without invoking ProbeAgent\\n         const response = await this.generateMockResponse(prompt, _checkName, schema);\\n-        return { response, effectiveSchema: typeof schema === 'object' ? 'custom' : schema };\\n+        return {\\n+          response,\\n+          effectiveSchema: typeof schema === 'object' ? 'custom' : schema,\\n+          sessionId,\\n+        };\\n       }\\n       // In unit tests, still invoke ProbeAgent so tests can assert on options (schema) passed in\\n       // Fall through to normal flow below\\n     }\\n \\n     // Create ProbeAgent instance with proper options\\n-    const sessionId =\\n-      providedSessionId ||\\n-      (() => {\\n-        const timestamp = new Date().toISOString();\\n-        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n-      })();\\n \\n     log('ü§ñ Creating ProbeAgent for AI review...');\\n     log(`üÜî Session ID: ${sessionId}`);\\n@@ -1674,6 +1709,35 @@ ${'='.repeat(60)}\\n         (options as any).bashConfig = this.config.bashConfig;\\n       }\\n \\n+      // Pass completion prompt for post-completion validation/review\\n+      if (this.config.completionPrompt !== undefined) {\\n+        (options as any).completionPrompt = this.config.completionPrompt;\\n+      }\\n+\\n+      // Propagate workspace / allowed folders to ProbeAgent so that tools\\n+      // operate inside the isolated workspace and project checkouts instead\\n+      // of the Visor repository root.\\n+      try {\\n+        const cfgAny: any = this.config as any;\\n+        const allowedFolders = cfgAny.allowedFolders as string[] | undefined;\\n+        const workspacePath =\\n+          cfgAny.workspacePath || cfgAny.path || (Array.isArray(allowedFolders) && allowedFolders[0]);\\n+        if (Array.isArray(allowedFolders) && allowedFolders.length > 0) {\\n+          (options as any).allowedFolders = allowedFolders;\\n+          if (!options.path && workspacePath) {\\n+            (options as any).path = workspacePath;\\n+          }\\n+          log(`üóÇÔ∏è ProbeAgent workspace config:`);\\n+          log(`   path (cwd): ${(options as any).path}`);\\n+          log(`   allowedFolders[0]: ${allowedFolders[0]}`);\\n+        } else if (workspacePath) {\\n+          (options as any).path = workspacePath;\\n+          log(`üóÇÔ∏è ProbeAgent path: ${workspacePath} (no allowedFolders)`);\\n+        }\\n+      } catch {\\n+        // Best-effort only; fall back to ProbeAgent defaults on error.\\n+      }\\n+\\n       // Add provider-specific options if configured\\n       if (this.config.provider) {\\n         // Map claude-code to anthropic for ProbeAgent compatibility\\n@@ -2037,7 +2101,7 @@ ${'='.repeat(60)}\\n         log(`üîß Debug: Registered AI session for potential reuse: ${sessionId}`);\\n       }\\n \\n-      return { response, effectiveSchema };\\n+      return { response, effectiveSchema, sessionId };\\n     } catch (error) {\\n       console.error('‚ùå ProbeAgent failed:', error);\\n       throw new Error(\\n@@ -2191,13 +2255,23 @@ ${'='.repeat(60)}\\n         // For other schemas (code-review, etc.), extract and parse JSON with boundary detection\\n         log('üîç Extracting JSON from AI response...');\\n \\n-        // Try direct parsing first - if AI returned pure JSON\\n+        // Sanitize response: strip BOM, zero-width chars, and other invisible characters\\n+        // that can cause JSON parsing to fail even when the text looks valid\\n+        const sanitizedResponse = response\\n+          .replace(/^\\\\uFEFF/, '') // BOM\\n+          .replace(/[\\\\u200B-\\\\u200D\\\\uFEFF\\\\u00A0]/g, '') // Zero-width chars, NBSP\\n+          .replace(/[\\\\x00-\\\\x08\\\\x0B\\\\x0C\\\\x0E-\\\\x1F\\\\x7F]/g, '') // Control chars (except \\\\t \\\\n \\\\r)\\n+          .trim();\\n+\\n+        // Try direct JSON parsing - no bracket-matching extraction\\n+        // JSON validation is offloaded to Probe agent when schema is provided\\n         try {\\n-          reviewData = JSON.parse(response.trim());\\n+          reviewData = JSON.parse(sanitizedResponse);\\n           log('‚úÖ Successfully parsed direct JSON response');\\n           if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-        } catch {\\n-          log('üîç Direct parsing failed, trying to extract JSON from response...');\\n+        } catch (parseErr) {\\n+          const errMsg = parseErr instanceof Error ? parseErr.message : String(parseErr);\\n+          log(`üîç Direct JSON parsing failed: ${errMsg}`);\\n \\n           // If the response starts with \\\"I cannot\\\" or similar, it's likely a refusal\\n           if (\\n@@ -2210,66 +2284,16 @@ ${'='.repeat(60)}\\n             };\\n           }\\n \\n-          // Try to extract JSON using improved method with proper bracket matching\\n-          const jsonString = this.extractJsonFromResponse(response);\\n-\\n-          if (jsonString) {\\n-            try {\\n-              reviewData = JSON.parse(jsonString);\\n-              log('‚úÖ Successfully parsed extracted JSON');\\n-              if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-            } catch {\\n-              log('üîß Extracted JSON parsing failed, falling back to plain text handling...');\\n-\\n-              // Check if response is plain text and doesn't contain structured data\\n-              if (!response.includes('{') && !response.includes('}')) {\\n-                log('üîß Plain text response detected, creating structured fallback...');\\n-\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              } else {\\n-                // Fallback: treat the entire response as an issue\\n-                log('üîß Creating fallback response from non-JSON content...');\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              }\\n-            }\\n-          } else {\\n-            // No JSON found at all - treat as plain text response\\n-            log('üîß No JSON found in response, treating as plain text...');\\n-            reviewData = {\\n-              issues: [\\n-                {\\n-                  file: 'AI_RESPONSE',\\n-                  line: 1,\\n-                  ruleId: 'ai/raw_response',\\n-                  message: response,\\n-                  severity: 'info',\\n-                  category: 'documentation',\\n-                },\\n-              ],\\n-            };\\n-          }\\n+          // Not valid JSON - treat entire response as text output\\n+          // This allows Probe (or other AI providers) to handle JSON validation\\n+          // and avoids false positives from bracket-matching (e.g., mermaid diagrams)\\n+          log('üîß Treating response as plain text (no JSON extraction)');\\n+          const trimmed = response.trim();\\n+          return {\\n+            issues: [],\\n+            output: { text: trimmed },\\n+            debug: debugInfo,\\n+          };\\n         }\\n       }\\n \\n@@ -2457,80 +2481,6 @@ ${'='.repeat(60)}\\n     }\\n   }\\n \\n-  /**\\n-   * Extract JSON from a response that might contain surrounding text\\n-   * Uses proper bracket matching to find valid JSON objects or arrays\\n-   */\\n-  private extractJsonFromResponse(response: string): string | null {\\n-    const text = response.trim();\\n-\\n-    // Try to find JSON objects first (higher priority)\\n-    let bestJson = this.findJsonWithBracketMatching(text, '{', '}');\\n-\\n-    // If no object found, try arrays\\n-    if (!bestJson) {\\n-      bestJson = this.findJsonWithBracketMatching(text, '[', ']');\\n-    }\\n-\\n-    return bestJson;\\n-  }\\n-\\n-  /**\\n-   * Find JSON with proper bracket matching to avoid false positives\\n-   */\\n-  private findJsonWithBracketMatching(\\n-    text: string,\\n-    openChar: string,\\n-    closeChar: string\\n-  ): string | null {\\n-    const firstIndex = text.indexOf(openChar);\\n-    if (firstIndex === -1) return null;\\n-\\n-    let depth = 0;\\n-    let inString = false;\\n-    let escaping = false;\\n-\\n-    for (let i = firstIndex; i < text.length; i++) {\\n-      const char = text[i];\\n-\\n-      if (escaping) {\\n-        escaping = false;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\\\\\\' && inString) {\\n-        escaping = true;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\"' && !escaping) {\\n-        inString = !inString;\\n-        continue;\\n-      }\\n-\\n-      if (!inString) {\\n-        if (char === openChar) {\\n-          depth++;\\n-        } else if (char === closeChar) {\\n-          depth--;\\n-          if (depth === 0) {\\n-            // Found matching closing bracket\\n-            const candidate = text.substring(firstIndex, i + 1);\\n-            try {\\n-              JSON.parse(candidate); // Validate it's actually valid JSON\\n-              return candidate;\\n-            } catch {\\n-              // This wasn't valid JSON, keep looking\\n-              break;\\n-            }\\n-          }\\n-        }\\n-      }\\n-    }\\n-\\n-    return null;\\n-  }\\n-\\n   /**\\n    * Generate mock response for testing\\n    */\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli-main.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/src/cli-main.ts b/src/cli-main.ts\\nindex 7dbef501..e671fbd8 100644\\n--- a/src/cli-main.ts\\n+++ b/src/cli-main.ts\\n@@ -158,6 +158,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n   }\\n   const only = getArg('--only');\\n   const bail = hasFlag('--bail');\\n+  const noMocks = hasFlag('--no-mocks');\\n   const listOnly = hasFlag('--list');\\n   const validateOnly = hasFlag('--validate');\\n   const progress = (getArg('--progress') as 'compact' | 'detailed' | undefined) || 'compact';\\n@@ -224,6 +225,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       const agg = await runSuites(multiFiles, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallelSuites: maxParallelSuites || Math.max(1, require('os').cpus()?.length || 2),\\n         maxParallel,\\n         promptMaxChars,\\n@@ -340,6 +342,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       runRes = await runner.runCases(tpath, suite, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallel,\\n         promptMaxChars,\\n         engineMode: 'state-machine',\\n@@ -609,6 +612,9 @@ export async function main(): Promise<void> {\\n     // Set environment variables early for proper logging in all modules\\n     process.env.VISOR_OUTPUT_FORMAT = options.output;\\n     process.env.VISOR_DEBUG = options.debug ? 'true' : 'false';\\n+    if (options.keepWorkspace) {\\n+      process.env.VISOR_KEEP_WORKSPACE = 'true';\\n+    }\\n     // Configure centralized logger\\n     configureLoggerFromCli({\\n       output: options.output,\\n@@ -716,6 +722,31 @@ export async function main(): Promise<void> {\\n       const threads = slackAny.threads || 'any';\\n       const allow = Array.isArray(slackAny.channel_allowlist) ? slackAny.channel_allowlist : [];\\n       const appToken = slackAny.app_token || process.env.SLACK_APP_TOKEN;\\n+\\n+      // Initialize telemetry for Slack mode (normally done later for CLI runs).\\n+      if ((config as any)?.telemetry) {\\n+        const t = (config as any).telemetry as {\\n+          enabled?: boolean;\\n+          sink?: 'otlp' | 'file' | 'console';\\n+          file?: { dir?: string; ndjson?: boolean };\\n+          tracing?: { auto_instrumentations?: boolean; trace_report?: { enabled?: boolean } };\\n+        };\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true' || !!t?.enabled,\\n+          sink:\\n+            (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || t?.sink || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR || t?.file?.dir, ndjson: !!t?.file?.ndjson },\\n+          autoInstrument: !!t?.tracing?.auto_instrumentations,\\n+          traceReport: !!t?.tracing?.trace_report?.enabled,\\n+        });\\n+      } else {\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true',\\n+          sink: (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR },\\n+        });\\n+      }\\n+\\n       const runner = new SlackSocketRunner(engine, config, {\\n         appToken,\\n         endpoint,\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli.ts\",\"additions\":5,\"deletions\":1,\"changes\":6,\"patch\":\"diff --git a/src/cli.ts b/src/cli.ts\\nindex 134429b2..b1c46213 100644\\n--- a/src/cli.ts\\n+++ b/src/cli.ts\\n@@ -76,6 +76,7 @@ export class CLI {\\n         parseInt(value, 10)\\n       )\\n       .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText())\\n       .exitOverride(); // Prevent automatic process.exit for better error handling\\n \\n@@ -154,6 +155,7 @@ export class CLI {\\n           parseInt(value, 10)\\n         )\\n         .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+        .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n         .allowUnknownOption(false)\\n         .allowExcessArguments(false) // Don't allow positional arguments\\n         .addHelpText('after', this.getExamplesText())\\n@@ -224,6 +226,7 @@ export class CLI {\\n         message: options.message,\\n         githubV2: false,\\n         slack: Boolean(options.slack),\\n+        keepWorkspace: Boolean(options.keepWorkspace),\\n       };\\n     } catch (error: unknown) {\\n       // Handle commander.js exit overrides for help/version ONLY\\n@@ -348,6 +351,7 @@ export class CLI {\\n       .option('--debug-port <port>', 'Port for debug server (default: 3456)', value =>\\n         parseInt(value, 10)\\n       )\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText());\\n \\n     // Get the basic help and append examples manually if addHelpText doesn't work\\n\",\"status\":\"modified\"},{\"filename\":\"src/config.ts\",\"additions\":67,\"deletions\":17,\"changes\":84,\"patch\":\"diff --git a/src/config.ts b/src/config.ts\\nindex 407efa2d..02abe9df 100644\\n--- a/src/config.ts\\n+++ b/src/config.ts\\n@@ -20,6 +20,7 @@ import { ConfigLoader, ConfigLoaderOptions } from './utils/config-loader';\\n import { ConfigMerger } from './utils/config-merger';\\n import Ajv from 'ajv';\\n import addFormats from 'ajv-formats';\\n+import { validateJsSyntax } from './utils/sandbox';\\n \\n /**\\n  * Valid event triggers for checks\\n@@ -477,16 +478,29 @@ export class ConfigManager {\\n \\n     logger.info(`Registered workflow '${workflowId}' for standalone execution`);\\n \\n-    // Create a COMPLETELY NEW visor config with ONLY the test checks\\n-    // This prevents any workflow fields from leaking into the config\\n-    const visorConfig: Partial<VisorConfig> = {\\n+    // For standalone workflow testing, use the workflow's steps as checks\\n+    // This allows the test runner to execute the workflow steps directly\\n+    const workflowSteps = workflowData.steps || {};\\n+\\n+    // Create a config that includes the workflow steps as checks,\\n+    // plus the tests section and workflow metadata for output computation\\n+    const visorConfig: Partial<VisorConfig> & { tests?: any; outputs?: any; inputs?: any } = {\\n       version: '1.0',\\n-      steps: tests,\\n-      checks: tests, // Backward compatibility\\n+      steps: workflowSteps,\\n+      checks: workflowSteps,\\n+      tests: tests, // Preserve test harness config (may be empty if stripped by test runner)\\n     };\\n \\n-    logger.debug(`Standalone workflow config has ${Object.keys(tests).length} test checks`);\\n-    logger.debug(`Test check names: ${Object.keys(tests).join(', ')}`);\\n+    // Preserve workflow metadata for output computation during tests\\n+    if (workflowData.outputs) {\\n+      visorConfig.outputs = workflowData.outputs;\\n+    }\\n+    if (workflowData.inputs) {\\n+      visorConfig.inputs = workflowData.inputs;\\n+    }\\n+\\n+    logger.debug(`Standalone workflow config has ${Object.keys(workflowSteps).length} workflow steps as checks`);\\n+    logger.debug(`Workflow step names: ${Object.keys(workflowSteps).join(', ')}`);\\n     logger.debug(`Config keys after conversion: ${Object.keys(visorConfig).join(', ')}`);\\n \\n     return visorConfig;\\n@@ -982,26 +996,29 @@ export class ConfigManager {\\n \\n     // Validate reuse_ai_session configuration\\n     if (checkConfig.reuse_ai_session !== undefined) {\\n-      const isString = typeof checkConfig.reuse_ai_session === 'string';\\n-      const isBoolean = typeof checkConfig.reuse_ai_session === 'boolean';\\n+      const reuseValue = checkConfig.reuse_ai_session as unknown;\\n+      const isString = typeof reuseValue === 'string';\\n+      const isBoolean = typeof reuseValue === 'boolean';\\n+      const isSelf = reuseValue === 'self';\\n \\n       if (!isString && !isBoolean) {\\n         errors.push({\\n           field: `checks.${checkName}.reuse_ai_session`,\\n           message: `Invalid reuse_ai_session value for \\\"${checkName}\\\": must be string (check name) or boolean`,\\n-          value: checkConfig.reuse_ai_session,\\n+          value: reuseValue,\\n         });\\n-      } else if (isString) {\\n-        // When reuse_ai_session is a string, it must refer to a valid check\\n-        const targetCheckName = checkConfig.reuse_ai_session as string;\\n+      } else if (isString && !isSelf) {\\n+        // When reuse_ai_session is a string (other than the special 'self' value),\\n+        // it must refer to a valid check name\\n+        const targetCheckName = reuseValue as string;\\n         if (!config?.checks || !config.checks[targetCheckName]) {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" references non-existent check \\\"${targetCheckName}\\\" for session reuse`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n-      } else if (checkConfig.reuse_ai_session === true) {\\n+      } else if (reuseValue === true) {\\n         // When reuse_ai_session is true, depends_on must be specified and non-empty\\n         if (\\n           !checkConfig.depends_on ||\\n@@ -1011,7 +1028,7 @@ export class ConfigManager {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" has reuse_ai_session=true but missing or empty depends_on. Session reuse requires dependency on another check.`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n       }\\n@@ -1076,6 +1093,39 @@ export class ConfigManager {\\n         });\\n       }\\n     }\\n+\\n+    // Validate JavaScript syntax in transform_js and script content\\n+    try {\\n+      // Validate transform_js if present\\n+      const transformJs = (checkConfig as any).transform_js;\\n+      if (typeof transformJs === 'string' && transformJs.trim().length > 0) {\\n+        const result = validateJsSyntax(transformJs);\\n+        if (!result.valid) {\\n+          errors.push({\\n+            field: `checks.${checkName}.transform_js`,\\n+            message: `JavaScript syntax error in \\\"${checkName}\\\" transform_js: ${result.error}`,\\n+            value: transformJs.slice(0, 100) + (transformJs.length > 100 ? '...' : ''),\\n+          });\\n+        }\\n+      }\\n+\\n+      // Validate script content if type is 'script'\\n+      if (checkConfig.type === 'script') {\\n+        const content = (checkConfig as any).content;\\n+        if (typeof content === 'string' && content.trim().length > 0) {\\n+          const result = validateJsSyntax(content);\\n+          if (!result.valid) {\\n+            errors.push({\\n+              field: `checks.${checkName}.content`,\\n+              message: `JavaScript syntax error in \\\"${checkName}\\\" script: ${result.error}`,\\n+              value: content.slice(0, 100) + (content.length > 100 ? '...' : ''),\\n+            });\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Syntax validation is best-effort; don't fail the whole config on validation errors\\n+    }\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/failure-condition-evaluator.ts\",\"additions\":25,\"deletions\":9,\"changes\":34,\"patch\":\"diff --git a/src/failure-condition-evaluator.ts b/src/failure-condition-evaluator.ts\\nindex 3b54a015..5800df0a 100644\\n--- a/src/failure-condition-evaluator.ts\\n+++ b/src/failure-condition-evaluator.ts\\n@@ -137,6 +137,8 @@ export class FailureConditionEvaluator {\\n       previousResults?: Map<string, ReviewSummary>;\\n       authorAssociation?: string;\\n       workflowInputs?: Record<string, unknown>;\\n+      /** Current step's output for guarantee evaluation */\\n+      output?: unknown;\\n     }\\n   ): Promise<boolean> {\\n     // Build context for if evaluation\\n@@ -177,10 +179,10 @@ export class FailureConditionEvaluator {\\n       // Workflow inputs (for workflows)\\n       inputs: contextData?.workflowInputs || {},\\n \\n-      // Required output property (empty for if conditions)\\n-      output: {\\n-        issues: [],\\n-      },\\n+      // Output property: use provided output for guarantee evaluation, or empty for if conditions\\n+      output: contextData?.output !== undefined && contextData.output !== null && typeof contextData.output === 'object'\\n+        ? (contextData.output as Record<string, unknown>)\\n+        : { issues: [] },\\n       // Author association (used by permission helpers)\\n       authorAssociation: contextData?.authorAssociation,\\n \\n@@ -213,10 +215,10 @@ export class FailureConditionEvaluator {\\n               m && typeof m.get === 'function' ? m.get('all_valid', 'fact-validation') : undefined;\\n             memStr = ` mem.fact-validation.all_valid=${String(v)}`;\\n           } catch {}\\n+          // Debug if-eval output (only when VISOR_DEBUG enabled)\\n+          const outputKeys = Object.keys(context.outputs || {});\\n           console.error(\\n-            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" env.ENABLE_FACT_VALIDATION=${String(\\n-              (envMap as any).ENABLE_FACT_VALIDATION\\n-            )} event=${context.event?.event_name} result=${String(res)}${memStr}`\\n+            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" result=${String(res)} outputKeys=[${outputKeys.join(',')}]`\\n           );\\n         }\\n       } catch {}\\n@@ -480,7 +482,18 @@ export class FailureConditionEvaluator {\\n \\n       // Extract context variables\\n       const output = context.output || {};\\n-      const issues = output.issues || [];\\n+      // Ensure issues is an array - it might be a JSON string from workflow outputs\\n+      let issues = output.issues || [];\\n+      if (typeof issues === 'string') {\\n+        try {\\n+          issues = JSON.parse(issues);\\n+        } catch {\\n+          issues = [];\\n+        }\\n+      }\\n+      if (!Array.isArray(issues)) {\\n+        issues = [];\\n+      }\\n \\n       // Backward compatibility: provide metadata for transition period\\n       // TODO: Remove after all configurations are updated\\n@@ -518,6 +531,7 @@ export class FailureConditionEvaluator {\\n       const event = context.event || 'manual';\\n       const env = context.env || {};\\n       const outputs = context.outputs || {};\\n+      const inputs = context.inputs || {};\\n       const debugData = context.debug || null;\\n \\n       // Get memory store and create accessor for fail_if expressions\\n@@ -555,6 +569,7 @@ export class FailureConditionEvaluator {\\n         filesCount,\\n         event,\\n         env,\\n+        inputs,\\n         // Helper functions\\n         contains,\\n         startsWith,\\n@@ -621,6 +636,7 @@ export class FailureConditionEvaluator {\\n             filesCount,\\n             event,\\n             env,\\n+            inputs,\\n             // Helpers\\n             contains,\\n             startsWith,\\n\",\"status\":\"modified\"},{\"filename\":\"src/frontends/slack-frontend.ts\",\"additions\":107,\"deletions\":6,\"changes\":113,\"patch\":\"diff --git a/src/frontends/slack-frontend.ts b/src/frontends/slack-frontend.ts\\nindex 97161a7d..41735aa2 100644\\n--- a/src/frontends/slack-frontend.ts\\n+++ b/src/frontends/slack-frontend.ts\\n@@ -1,6 +1,32 @@\\n+/**\\n+ * Slack Frontend for Visor workflows.\\n+ *\\n+ * Features:\\n+ * - Posts AI replies to Slack threads\\n+ * - Converts Markdown to Slack mrkdwn format\\n+ * - Renders mermaid diagrams to PNG and uploads as images\\n+ * - Manages üëÄ/üëç reactions for acknowledgement\\n+ * - Handles human input prompts via prompt-state\\n+ *\\n+ * Mermaid Diagram Rendering:\\n+ * - Detects ```mermaid code blocks in AI responses\\n+ * - Renders to PNG using @mermaid-js/mermaid-cli (mmdc)\\n+ * - Uploads rendered images to Slack thread\\n+ * - Replaces mermaid blocks with \\\"_(See diagram above)_\\\" placeholder\\n+ *\\n+ * Requirements for mermaid rendering:\\n+ * - Node.js and npx in PATH\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ * - On Linux: apt-get install chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ */\\n import type { Frontend, FrontendContext } from './host';\\n import { SlackClient } from '../slack/client';\\n-import { formatSlackText } from '../slack/markdown';\\n+import {\\n+  formatSlackText,\\n+  extractMermaidDiagrams,\\n+  renderMermaidToPng,\\n+  replaceMermaidBlocks,\\n+} from '../slack/markdown';\\n \\n type SlackFrontendConfig = {\\n   defaultChannel?: string;\\n@@ -284,6 +310,9 @@ export class SlackFrontend implements Frontend {\\n       const isLogChat = providerType === 'log' && checkCfg.group === 'chat';\\n       if (!isAi && !isLogChat) return;\\n \\n+      // Skip internal steps - they're intermediate processing and shouldn't post to Slack\\n+      if (checkCfg.criticality === 'internal') return;\\n+\\n       // For AI checks, only post when using simple/unstructured schemas (or none).\\n       if (isAi) {\\n         const schema = checkCfg.schema;\\n@@ -316,6 +345,13 @@ export class SlackFrontend implements Frontend {\\n         ) {\\n           text = (result as any).content.trim();\\n         }\\n+      } else if (isLogChat && typeof (result as any)?.logOutput === 'string') {\\n+        // For log-based chat checks, render the formatted log output as the\\n+        // Slack message when no structured text field is present.\\n+        const raw = (result as any).logOutput;\\n+        if (raw.trim().length > 0) {\\n+          text = raw.trim();\\n+        }\\n       } else if (isAi && showRawOutput && out !== undefined) {\\n         try {\\n           text = JSON.stringify(out, null, 2);\\n@@ -325,13 +361,78 @@ export class SlackFrontend implements Frontend {\\n       }\\n       if (!text) return;\\n \\n-      const formattedText = formatSlackText(text);\\n+      // Extract and render mermaid diagrams before posting\\n+      const diagrams = extractMermaidDiagrams(text);\\n+      let processedText = text;\\n+\\n+      if (diagrams.length > 0) {\\n+        try {\\n+          ctx.logger.info(\\n+            `[slack-frontend] found ${diagrams.length} mermaid diagram(s) to render for ${checkId}`\\n+          );\\n+        } catch {}\\n+\\n+        // Render and upload each diagram\\n+        const uploadedCount: number[] = [];\\n+        for (let i = 0; i < diagrams.length; i++) {\\n+          const diagram = diagrams[i];\\n+          try {\\n+            ctx.logger.info(`[slack-frontend] rendering mermaid diagram ${i + 1}...`);\\n+            const pngBuffer = await renderMermaidToPng(diagram.code);\\n+            if (pngBuffer) {\\n+              ctx.logger.info(\\n+                `[slack-frontend] rendered diagram ${i + 1}, size=${pngBuffer.length} bytes, uploading...`\\n+              );\\n+              const filename = `diagram-${i + 1}.png`;\\n+              const uploadResult = await slack.files.uploadV2({\\n+                content: pngBuffer,\\n+                filename,\\n+                channel,\\n+                thread_ts: threadTs,\\n+                title: `Diagram ${i + 1}`,\\n+              });\\n+              if (uploadResult.ok) {\\n+                uploadedCount.push(i);\\n+                ctx.logger.info(`[slack-frontend] uploaded mermaid diagram ${i + 1} to ${channel}`);\\n+              } else {\\n+                ctx.logger.warn(`[slack-frontend] upload failed for diagram ${i + 1}`);\\n+              }\\n+            } else {\\n+              ctx.logger.warn(\\n+                `[slack-frontend] mermaid rendering returned null for diagram ${i + 1} (mmdc failed or not installed)`\\n+              );\\n+            }\\n+          } catch (e) {\\n+            ctx.logger.warn(\\n+              `[slack-frontend] failed to render/upload mermaid diagram ${i + 1}: ${\\n+                e instanceof Error ? e.message : String(e)\\n+              }`\\n+            );\\n+          }\\n+        }\\n+\\n+        // Replace mermaid blocks with placeholder text\\n+        if (uploadedCount.length > 0) {\\n+          processedText = replaceMermaidBlocks(text, diagrams, (idx) =>\\n+            uploadedCount.includes(idx) ? '_(See diagram above)_' : '_(Diagram rendering failed)_'\\n+          );\\n+        }\\n+      }\\n+\\n+      const formattedText = formatSlackText(processedText);\\n       await slack.chat.postMessage({ channel, text: formattedText, thread_ts: threadTs });\\n+      ctx.logger.info(\\n+        `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+      );\\n+    } catch (outerErr) {\\n+      // Log errors instead of silently swallowing them\\n       try {\\n-        ctx.logger.info(\\n-          `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+        ctx.logger.warn(\\n+          `[slack-frontend] maybePostDirectReply failed for ${checkId}: ${\\n+            outerErr instanceof Error ? outerErr.message : String(outerErr)\\n+          }`\\n         );\\n       } catch {}\\n-    } catch {}\\n+    }\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/generated/config-schema.ts\",\"additions\":233,\"deletions\":13,\"changes\":246,\"patch\":\"diff --git a/src/generated/config-schema.ts b/src/generated/config-schema.ts\\nindex b3caca4f..ecab390d 100644\\n--- a/src/generated/config-schema.ts\\n+++ b/src/generated/config-schema.ts\\n@@ -56,6 +56,20 @@ export const configSchema = {\\n           },\\n           description: 'Import workflow definitions from external files or URLs',\\n         },\\n+        inputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowInput',\\n+          },\\n+          description: 'Workflow inputs (for standalone reusable workflows)',\\n+        },\\n+        outputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowOutput',\\n+          },\\n+          description: 'Workflow outputs (for standalone reusable workflows)',\\n+        },\\n         steps: {\\n           $ref: '#/definitions/Record%3Cstring%2CCheckConfig%3E',\\n           description: 'Step configurations (recommended)',\\n@@ -67,7 +81,7 @@ export const configSchema = {\\n         },\\n         output: {\\n           $ref: '#/definitions/OutputConfig',\\n-          description: 'Output configuration',\\n+          description: 'Output configuration (optional - defaults provided)',\\n         },\\n         http_server: {\\n           $ref: '#/definitions/HttpServerConfig',\\n@@ -139,8 +153,16 @@ export const configSchema = {\\n           },\\n           description: 'Optional integrations: event-driven frontends (e.g., ndjson-sink, github)',\\n         },\\n+        workspace: {\\n+          $ref: '#/definitions/WorkspaceConfig',\\n+          description: 'Workspace isolation configuration for sandboxed execution',\\n+        },\\n+        slack: {\\n+          $ref: '#/definitions/SlackConfig',\\n+          description: 'Slack configuration',\\n+        },\\n       },\\n-      required: ['output', 'version'],\\n+      required: ['version'],\\n       patternProperties: {\\n         '^x-': {},\\n       },\\n@@ -243,6 +265,63 @@ export const configSchema = {\\n         type: 'string',\\n       },\\n     },\\n+    WorkflowInput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Input parameter name',\\n+        },\\n+        schema: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'JSON Schema for the input',\\n+        },\\n+        required: {\\n+          type: 'boolean',\\n+          description: 'Whether this input is required',\\n+        },\\n+        default: {\\n+          description: 'Default value if not provided',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow input definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    WorkflowOutput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Output name',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+        value: {\\n+          type: 'string',\\n+          description: 'Value using Liquid template syntax (references step outputs)',\\n+        },\\n+        value_js: {\\n+          type: 'string',\\n+          description: 'Value using JavaScript expression (alternative to value)',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow output definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n     'Record<string,CheckConfig>': {\\n       type: 'object',\\n       additionalProperties: {\\n@@ -390,11 +469,19 @@ export const configSchema = {\\n           description: 'Timeout in seconds for command execution (default: 60)',\\n         },\\n         depends_on: {\\n-          type: 'array',\\n-          items: {\\n-            type: 'string',\\n-          },\\n-          description: 'Check IDs that this check depends on (optional)',\\n+          anyOf: [\\n+            {\\n+              type: 'string',\\n+            },\\n+            {\\n+              type: 'array',\\n+              items: {\\n+                type: 'string',\\n+              },\\n+            },\\n+          ],\\n+          description:\\n+            'Check IDs that this check depends on (optional). Accepts single string or array.',\\n         },\\n         group: {\\n           type: 'string',\\n@@ -653,13 +740,93 @@ export const configSchema = {\\n           description: 'Arguments/inputs for the workflow',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Override specific step configurations in the workflow',\\n         },\\n         output_mapping: {\\n           $ref: '#/definitions/Record%3Cstring%2Cstring%3E',\\n           description: 'Map workflow outputs to check outputs',\\n         },\\n+        workflow_inputs: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'Alias for args - workflow inputs (backward compatibility)',\\n+        },\\n+        config: {\\n+          type: 'string',\\n+          description:\\n+            'Config file path - alternative to workflow ID (loads a Visor config file as workflow)',\\n+        },\\n+        workflow_overrides: {\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n+          description: 'Alias for overrides - workflow step overrides (backward compatibility)',\\n+        },\\n+        ref: {\\n+          type: 'string',\\n+          description: 'Git reference to checkout (branch, tag, commit SHA) - supports templates',\\n+        },\\n+        repository: {\\n+          type: 'string',\\n+          description: 'Repository URL or owner/repo format (defaults to current repository)',\\n+        },\\n+        token: {\\n+          type: 'string',\\n+          description: 'GitHub token for private repositories (defaults to GITHUB_TOKEN env)',\\n+        },\\n+        fetch_depth: {\\n+          type: 'number',\\n+          description: 'Number of commits to fetch (0 for full history, default: 1)',\\n+        },\\n+        fetch_tags: {\\n+          type: 'boolean',\\n+          description: 'Whether to fetch tags (default: false)',\\n+        },\\n+        submodules: {\\n+          anyOf: [\\n+            {\\n+              type: 'boolean',\\n+            },\\n+            {\\n+              type: 'string',\\n+              const: 'recursive',\\n+            },\\n+          ],\\n+          description: \\\"Checkout submodules: false, true, or 'recursive'\\\",\\n+        },\\n+        working_directory: {\\n+          type: 'string',\\n+          description: 'Working directory for the checkout (defaults to temp directory)',\\n+        },\\n+        use_worktree: {\\n+          type: 'boolean',\\n+          description: 'Use git worktree for efficient parallel checkouts (default: true)',\\n+        },\\n+        clean: {\\n+          type: 'boolean',\\n+          description: 'Clean the working directory before checkout (default: true)',\\n+        },\\n+        sparse_checkout: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description: 'Sparse checkout paths - only checkout specific directories/files',\\n+        },\\n+        lfs: {\\n+          type: 'boolean',\\n+          description: 'Enable Git LFS (Large File Storage)',\\n+        },\\n+        clone_timeout_ms: {\\n+          type: 'number',\\n+          description: 'Timeout in ms for cloning the bare repository (default: 300000 = 5 min)',\\n+        },\\n+        cleanup_on_failure: {\\n+          type: 'boolean',\\n+          description: 'Clean up worktree on failure (default: true)',\\n+        },\\n+        persist_worktree: {\\n+          type: 'boolean',\\n+          description: 'Keep worktree after workflow completion (default: false)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'Configuration for a single check',\\n@@ -794,6 +961,11 @@ export const configSchema = {\\n           $ref: '#/definitions/BashConfig',\\n           description: 'Advanced bash command execution configuration',\\n         },\\n+        completion_prompt: {\\n+          type: 'string',\\n+          description:\\n+            'Completion prompt for post-completion validation/review (runs after attempt_completion)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'AI provider configuration',\\n@@ -1223,7 +1395,7 @@ export const configSchema = {\\n           description: 'Custom output name (defaults to workflow name)',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Step overrides',\\n         },\\n         output_mapping: {\\n@@ -1238,14 +1410,14 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n-    'Record<string,Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>>':\\n+    'Record<string,Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>>':\\n       {\\n         type: 'object',\\n         additionalProperties: {\\n-          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E',\\n+          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E',\\n         },\\n       },\\n-    'Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>': {\\n+    'Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>': {\\n       type: 'object',\\n       additionalProperties: false,\\n     },\\n@@ -1811,6 +1983,54 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n+    WorkspaceConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        enabled: {\\n+          type: 'boolean',\\n+          description: 'Enable workspace isolation (default: true when config present)',\\n+        },\\n+        base_path: {\\n+          type: 'string',\\n+          description: 'Base path for workspaces (default: /tmp/visor-workspaces)',\\n+        },\\n+        cleanup_on_exit: {\\n+          type: 'boolean',\\n+          description: 'Clean up workspace on exit (default: true)',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Workspace isolation configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    SlackConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        version: {\\n+          type: 'string',\\n+          description: 'Slack API version',\\n+        },\\n+        mentions: {\\n+          type: 'string',\\n+          description: \\\"Mention handling: 'all', 'direct', etc.\\\",\\n+        },\\n+        threads: {\\n+          type: 'string',\\n+          description: \\\"Thread handling: 'required', 'optional', etc.\\\",\\n+        },\\n+        show_raw_output: {\\n+          type: 'boolean',\\n+          description: 'Show raw output in Slack responses',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Slack configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n   },\\n } as const;\\n export default configSchema;\\n\",\"status\":\"modified\"},{\"filename\":\"src/index.ts\",\"additions\":5,\"deletions\":2,\"changes\":7,\"patch\":\"diff --git a/src/index.ts b/src/index.ts\\nindex 50b89df3..9621e733 100644\\n--- a/src/index.ts\\n+++ b/src/index.ts\\n@@ -628,8 +628,11 @@ function resolveDependencies(\\n \\n     // Get dependencies for this check\\n     const checkConfig = config?.checks?.[checkId];\\n+    // Normalize depends_on to array (supports string | string[])\\n+    const rawDeps = checkConfig?.depends_on;\\n+    const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     // Expand OR groups (pipe syntax) for dependency closure discovery\\n-    const dependencies = (checkConfig?.depends_on || []).flatMap(d =>\\n+    const dependencies = depsArray.flatMap((d: string) =>\\n       typeof d === 'string' && d.includes('|')\\n         ? d\\n             .split('|')\\n\",\"status\":\"modified\"},{\"filename\":\"src/liquid-extensions.ts\",\"additions\":68,\"deletions\":1,\"changes\":69,\"patch\":\"diff --git a/src/liquid-extensions.ts b/src/liquid-extensions.ts\\nindex e9efaf68..dbf9c6ed 100644\\n--- a/src/liquid-extensions.ts\\n+++ b/src/liquid-extensions.ts\\n@@ -124,6 +124,26 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     }\\n   });\\n \\n+  // Register base64 filter for encoding strings\\n+  // Usage: {{ \\\"user:password\\\" | base64 }}\\n+  liquid.registerFilter('base64', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    return Buffer.from(str).toString('base64');\\n+  });\\n+\\n+  // Register base64_decode filter for decoding base64 strings\\n+  // Usage: {{ encoded_value | base64_decode }}\\n+  liquid.registerFilter('base64_decode', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    try {\\n+      return Buffer.from(str, 'base64').toString('utf-8');\\n+    } catch {\\n+      return '[Error: Invalid base64 string]';\\n+    }\\n+  });\\n+\\n   // Sanitize a label to allowed characters only: [A-Za-z0-9:/]\\n   liquid.registerFilter('safe_label', (value: unknown) => sanitizeLabel(value));\\n \\n@@ -137,6 +157,53 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     return s.replace(/\\\\\\\\n/g, '\\\\n').replace(/\\\\\\\\r/g, '\\\\r').replace(/\\\\\\\\t/g, '\\\\t');\\n   });\\n \\n+  // JSON escape filter - escapes a string for use inside a JSON string value\\n+  // This escapes special characters like quotes, backslashes, and control characters\\n+  // Usage: \\\"jql\\\": \\\"{{ myValue | json_escape }}\\\"\\n+  liquid.registerFilter('json_escape', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const s = String(value);\\n+    // Use JSON.stringify which handles all escaping, then strip the surrounding quotes\\n+    const jsonStr = JSON.stringify(s);\\n+    // Remove the first and last character (the quotes added by JSON.stringify)\\n+    return jsonStr.slice(1, -1);\\n+  });\\n+\\n+  // Shell escape filter - wraps value in single quotes with proper escaping\\n+  // Usage: {{ value | shell_escape }}\\n+  // Example: \\\"hello'world\\\" becomes \\\"'hello'\\\\''world'\\\"\\n+  // This is POSIX-compliant and safe for arbitrary text including mermaid diagrams\\n+  liquid.registerFilter('shell_escape', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    // Replace single quotes with: end quote, escaped quote, start quote\\n+    // Then wrap the entire thing in single quotes\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Alias for shell_escape\\n+  liquid.registerFilter('escape_shell', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Shell escape for double quotes (less safe but sometimes needed)\\n+  // Usage: {{ value | shell_escape_double }}\\n+  // Escapes: $, `, \\\\, \\\", and !\\n+  liquid.registerFilter('shell_escape_double', (value: unknown) => {\\n+    if (value == null) return '\\\"\\\"';\\n+    const s = String(value);\\n+    // Escape characters that have special meaning inside double quotes\\n+    const escaped = s\\n+      .replace(/\\\\\\\\/g, '\\\\\\\\\\\\\\\\')  // backslash first\\n+      .replace(/\\\\$/g, '\\\\\\\\$')   // dollar sign\\n+      .replace(/`/g, '\\\\\\\\`')    // backticks\\n+      .replace(/\\\"/g, '\\\\\\\\\\\"')    // double quotes\\n+      .replace(/!/g, '\\\\\\\\!');   // history expansion\\n+    return '\\\"' + escaped + '\\\"';\\n+  });\\n+\\n   // Register author permission filters (from main)\\n   // These filters check the author's permission level; detect local mode for tests\\n   const isLocal = detectLocalMode();\\n\",\"status\":\"modified\"},{\"filename\":\"src/logger.ts\",\"additions\":31,\"deletions\":2,\"changes\":33,\"patch\":\"diff --git a/src/logger.ts b/src/logger.ts\\nindex 6148b77c..aee7136e 100644\\n--- a/src/logger.ts\\n+++ b/src/logger.ts\\n@@ -85,7 +85,36 @@ class Logger {\\n       if (this.showTimestamps) {\\n         const ts = new Date().toISOString();\\n         const lvl = level ? level : undefined;\\n-        const prefix = lvl ? `[${ts}] [${lvl}]` : `[${ts}]`;\\n+\\n+        let tsToken = `[${ts}]`;\\n+        let lvlToken = lvl ? `[${lvl}]` : '';\\n+\\n+        // Add simple ANSI colour when running in a TTY and not emitting\\n+        // JSON/SARIF. Colours are intentionally minimal and only applied\\n+        // to the prefix markers, not the full line.\\n+        if (this.isTTY && !this.isJsonLike) {\\n+          const reset = '\\\\x1b[0m';\\n+          const dim = '\\\\x1b[2m';\\n+          const colours: Record<LogLevel, string> = {\\n+            silent: '',\\n+            error: '\\\\x1b[31m', // red\\n+            warn: '\\\\x1b[33m', // yellow\\n+            info: '\\\\x1b[36m', // cyan\\n+            verbose: '\\\\x1b[35m', // magenta\\n+            debug: '\\\\x1b[90m', // bright black / gray\\n+          };\\n+\\n+          tsToken = `${dim}${tsToken}${reset}`;\\n+\\n+          if (lvl) {\\n+            const colour = colours[lvl] || '';\\n+            if (colour) {\\n+              lvlToken = `${colour}${lvlToken}${reset}`;\\n+            }\\n+          }\\n+        }\\n+\\n+        const prefix = lvl ? `${tsToken} ${lvlToken}` : tsToken;\\n         process.stderr.write(`${prefix} ${msg}\\\\n`);\\n       } else {\\n         process.stderr.write(msg + '\\\\n');\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/ai-check-provider.ts\",\"additions\":100,\"deletions\":6,\"changes\":106,\"patch\":\"diff --git a/src/providers/ai-check-provider.ts b/src/providers/ai-check-provider.ts\\nindex 53f1a350..b41d6af1 100644\\n--- a/src/providers/ai-check-provider.ts\\n+++ b/src/providers/ai-check-provider.ts\\n@@ -186,7 +186,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     let promptContent: string;\\n \\n@@ -204,7 +205,8 @@ export class AICheckProvider extends CheckProvider {\\n       eventContext,\\n       dependencyResults,\\n       outputHistory,\\n-      args\\n+      args,\\n+      workflowInputs\\n     );\\n   }\\n \\n@@ -335,7 +337,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     // Build outputs_raw from -raw keys (aggregate parent values)\\n     const outputsRaw: Record<string, unknown> = {};\\n@@ -535,6 +538,8 @@ export class AICheckProvider extends CheckProvider {\\n       outputs_raw: outputsRaw,\\n       // Custom arguments from on_init 'with' directive\\n       args: args || {},\\n+      // Workflow inputs (for nested workflow steps to access parent inputs like {{ inputs.context }})\\n+      inputs: workflowInputs || {},\\n     };\\n \\n     try {\\n@@ -621,7 +626,8 @@ export class AICheckProvider extends CheckProvider {\\n         console.error(`[ai-exec] step=${String((config as any).checkName || 'unknown')}`);\\n       }\\n     } catch {}\\n-    // Extract AI configuration - only set properties that are explicitly provided\\n+    // Extract AI configuration - only set properties that are explicitly provided.\\n+    // Workspace / allowedFolders will be derived below from the execution context.\\n     const aiConfig: AIReviewConfig = {};\\n \\n     // Check-level AI configuration (ai object)\\n@@ -671,6 +677,9 @@ export class AICheckProvider extends CheckProvider {\\n       if (aiAny.bashConfig !== undefined) {\\n         aiConfig.bashConfig = aiAny.bashConfig as import('../types/config').BashConfig;\\n       }\\n+      if (aiAny.completion_prompt !== undefined) {\\n+        aiConfig.completionPrompt = aiAny.completion_prompt as string;\\n+      }\\n       if (aiAny.skip_code_context !== undefined) {\\n         // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n         (aiConfig as any).skip_code_context = aiAny.skip_code_context as boolean;\\n@@ -691,6 +700,90 @@ export class AICheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Derive workspace-aware allowedFolders for ProbeAgent when workspace\\n+    // isolation is enabled. This ensures tools like search/query operate\\n+    // inside the isolated workspace (and its project symlinks) instead of\\n+    // the Visor repository root.\\n+    // Folder names are human-readable (tyk-docs, visor2) thanks to WorkspaceManager.\\n+    try {\\n+      const ctxAny: any = sessionInfo as any;\\n+      const parentCtx = ctxAny?._parentContext;\\n+      const workspace = parentCtx?.workspace;\\n+\\n+      // Enhanced debug logging for workspace propagation diagnosis\\n+      logger.debug(`[AI Provider] Workspace detection for check '${(config as any).checkName || 'unknown'}':`);\\n+      logger.debug(`[AI Provider]   sessionInfo exists: ${!!sessionInfo}`);\\n+      logger.debug(`[AI Provider]   _parentContext exists: ${!!parentCtx}`);\\n+      logger.debug(`[AI Provider]   workspace exists: ${!!workspace}`);\\n+      if (workspace) {\\n+        logger.debug(`[AI Provider]   workspace.isEnabled exists: ${typeof workspace.isEnabled === 'function'}`);\\n+        logger.debug(`[AI Provider]   workspace.isEnabled(): ${typeof workspace.isEnabled === 'function' ? workspace.isEnabled() : 'N/A'}`);\\n+        const projectCount = typeof workspace.listProjects === 'function' ? workspace.listProjects()?.length : 'N/A';\\n+        logger.debug(`[AI Provider]   workspace.listProjects() count: ${projectCount}`);\\n+      }\\n+\\n+      if (workspace && typeof workspace.isEnabled === 'function' && workspace.isEnabled()) {\\n+        const folders: string[] = [];\\n+        let workspaceRoot: string | undefined;\\n+        let mainProjectPath: string | undefined;\\n+        try {\\n+          const info = workspace.getWorkspaceInfo?.();\\n+          if (info && typeof info.workspacePath === 'string') {\\n+            workspaceRoot = info.workspacePath;\\n+            mainProjectPath = info.mainProjectPath;\\n+            // Add workspace root\\n+            folders.push(info.workspacePath);\\n+            // Include main project path (has human-readable name like \\\"visor2\\\")\\n+            if (mainProjectPath) {\\n+              folders.push(mainProjectPath);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore workspace info errors\\n+        }\\n+        try {\\n+          const projects = workspace.listProjects?.() || [];\\n+          for (const proj of projects as any[]) {\\n+            if (proj && typeof proj.path === 'string') {\\n+              // Project paths have human-readable names (tyk-docs, not checkout-tyk-docs)\\n+              folders.push(proj.path);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore project listing errors\\n+        }\\n+        const unique = Array.from(new Set(folders.filter(p => typeof p === 'string' && p)));\\n+        if (unique.length > 0 && workspaceRoot) {\\n+          (aiConfig as any).allowedFolders = unique;\\n+          // Set path and cwd to workspace root - AI will run with cwd set to workspace\\n+          // Both are set for compatibility: path for older probe versions, cwd for rc175+\\n+          (aiConfig as any).path = workspaceRoot;\\n+          (aiConfig as any).cwd = workspaceRoot;\\n+          // Also set workspacePath for the AI to know the root\\n+          (aiConfig as any).workspacePath = workspaceRoot;\\n+          logger.debug(`[AI Provider] Workspace isolation enabled:`);\\n+          logger.debug(`[AI Provider]   workspaceRoot (cwd): ${workspaceRoot}`);\\n+          logger.debug(`[AI Provider]   allowedFolders: ${JSON.stringify(unique)}`);\\n+        }\\n+      } else if (parentCtx && typeof parentCtx.workingDirectory === 'string') {\\n+        // Fallback: when workspace is not available (or disabled), still\\n+        // constrain tools to the engine's working directory so ProbeAgent\\n+        // operates inside the same logical root as the state machine. This\\n+        // also ensures nested workflows (e.g. code-question-helper) use the\\n+        // workspace main project path once initializeWorkspace has updated\\n+        // the parent context.\\n+        if (!(aiConfig as any).allowedFolders) {\\n+          (aiConfig as any).allowedFolders = [parentCtx.workingDirectory];\\n+        }\\n+        if (!(aiConfig as any).path) {\\n+          (aiConfig as any).path = parentCtx.workingDirectory;\\n+          (aiConfig as any).cwd = parentCtx.workingDirectory;\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only; fall back to defaults on error.\\n+    }\\n+\\n     // Check-level AI model and provider (top-level properties)\\n     if (config.ai_model !== undefined) {\\n       aiConfig.model = config.ai_model as string;\\n@@ -876,7 +969,8 @@ export class AICheckProvider extends CheckProvider {\\n       ctxWithStage,\\n       _dependencyResults,\\n       (config as any).__outputHistory as Map<string, unknown[]> | undefined,\\n-      (sessionInfo as any)?.args\\n+      (sessionInfo as any)?.args,\\n+      (config as any).workflowInputs as Record<string, unknown> | undefined\\n     );\\n \\n     // Optional persona (vendor extension): ai.ai_persona or ai_persona.\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/command-check-provider.ts\",\"additions\":50,\"deletions\":16,\"changes\":66,\"patch\":\"diff --git a/src/providers/command-check-provider.ts b/src/providers/command-check-provider.ts\\nindex e0c9d634..bf7ffd43 100644\\n--- a/src/providers/command-check-provider.ts\\n+++ b/src/providers/command-check-provider.ts\\n@@ -131,7 +131,8 @@ export class CommandCheckProvider extends CheckProvider {\\n       // New: outputs_raw exposes aggregate values (e.g., full arrays for forEach parents)\\n       outputs_raw: outputsRaw,\\n       // Workflow inputs (when executing within a workflow)\\n-      inputs: context?.workflowInputs || {},\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any).workflowInputs || context?.workflowInputs || {},\\n       // Custom arguments from on_init 'with' directive\\n       args: context?.args || {},\\n       env: this.getSafeEnvironmentVariables(),\\n@@ -178,17 +179,31 @@ export class CommandCheckProvider extends CheckProvider {\\n           mock = rawMock as Record<string, unknown>;\\n         }\\n         const m = mock as { stdout?: string; stderr?: string; exit_code?: number; exit?: number };\\n-        let out: unknown = m.stdout ?? '';\\n-        try {\\n-          if (\\n-            typeof out === 'string' &&\\n-            (out.trim().startsWith('{') || out.trim().startsWith('['))\\n-          ) {\\n-            out = JSON.parse(out);\\n-          }\\n-        } catch {}\\n-        const code =\\n-          typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0;\\n+\\n+        // Check if this looks like a command mock (has stdout/stderr/exit_code) or direct data output\\n+        const isCommandMock = m.stdout !== undefined || m.stderr !== undefined ||\\n+          m.exit_code !== undefined || m.exit !== undefined;\\n+\\n+        let out: unknown;\\n+        if (isCommandMock) {\\n+          // Traditional command mock format: { stdout: \\\"...\\\", exit_code: 0 }\\n+          out = m.stdout ?? '';\\n+          try {\\n+            if (\\n+              typeof out === 'string' &&\\n+              (out.trim().startsWith('{') || out.trim().startsWith('['))\\n+            ) {\\n+              out = JSON.parse(out);\\n+            }\\n+          } catch {}\\n+        } else {\\n+          // Direct data mock format: { data: [...], count: 1 } - use as-is\\n+          out = mock;\\n+        }\\n+\\n+        const code = isCommandMock\\n+          ? (typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0)\\n+          : 0;\\n         if (code !== 0) {\\n           return {\\n             issues: [\\n@@ -231,9 +246,8 @@ export class CommandCheckProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Get timeout from config (in seconds) or use default (60 seconds)\\n-      const timeoutSeconds = (config.timeout as number) || 60;\\n-      const timeoutMs = timeoutSeconds * 1000;\\n+      // Get timeout from config (in milliseconds) or use default (60 seconds = 60000ms)\\n+      const timeoutMs = (config.timeout as number) || 60000;\\n \\n       // Normalize only the eval payload for `node -e|--eval` invocations that may contain\\n       // literal newlines due to YAML processing (\\\"\\\\n\\\" -> newline). We re-escape newlines\\n@@ -1533,6 +1547,26 @@ ${bodyWithReturn}\\n       return null;\\n     }\\n \\n+    // Require at least one issue-specific field beyond just a message-like field.\\n+    // This prevents objects like {text: \\\"...\\\"} from being treated as issues.\\n+    const hasIssueField =\\n+      data.file !== undefined ||\\n+      data.path !== undefined ||\\n+      data.filename !== undefined ||\\n+      data.line !== undefined ||\\n+      data.startLine !== undefined ||\\n+      data.lineNumber !== undefined ||\\n+      data.severity !== undefined ||\\n+      data.level !== undefined ||\\n+      data.priority !== undefined ||\\n+      data.ruleId !== undefined ||\\n+      data.rule !== undefined ||\\n+      data.category !== undefined ||\\n+      data.type !== undefined;\\n+    if (!hasIssueField) {\\n+      return null;\\n+    }\\n+\\n     const allowedSeverities = new Set(['info', 'warning', 'error', 'critical']);\\n     const severityRaw = this.toTrimmedString(data.severity || data.level || data.priority);\\n     let severity: ReviewIssue['severity'] = 'warning';\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/git-checkout-provider.ts\",\"additions\":41,\"deletions\":6,\"changes\":47,\"patch\":\"diff --git a/src/providers/git-checkout-provider.ts b/src/providers/git-checkout-provider.ts\\nindex baac0001..6d3a2852 100644\\n--- a/src/providers/git-checkout-provider.ts\\n+++ b/src/providers/git-checkout-provider.ts\\n@@ -69,6 +69,16 @@ export class GitCheckoutProvider extends CheckProvider {\\n       return false;\\n     }\\n \\n+    if (checkoutConfig.clone_timeout_ms !== undefined) {\\n+      if (\\n+        typeof checkoutConfig.clone_timeout_ms !== 'number' ||\\n+        checkoutConfig.clone_timeout_ms <= 0\\n+      ) {\\n+        logger.error('Invalid config: clone_timeout_ms must be a positive number (milliseconds)');\\n+        return false;\\n+      }\\n+    }\\n+\\n     return true;\\n   }\\n \\n@@ -91,7 +101,13 @@ export class GitCheckoutProvider extends CheckProvider {\\n       );\\n \\n       // Resolve dynamic variables\\n-      const resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      let resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      // If ref resolves to an empty string, fall back to HEAD so we rely on\\n+      // the repository's default branch. This keeps downstream git commands\\n+      // well-defined and matches existing expectations/tests.\\n+      if (!resolvedRef || resolvedRef.trim().length === 0) {\\n+        resolvedRef = 'HEAD';\\n+      }\\n       const resolvedRepository = checkoutConfig.repository\\n         ? await this.liquid.parseAndRender(checkoutConfig.repository, templateContext)\\n         : process.env.GITHUB_REPOSITORY || 'unknown/unknown';\\n@@ -118,6 +134,7 @@ export class GitCheckoutProvider extends CheckProvider {\\n           clean: checkoutConfig.clean !== false, // Default: true\\n           workflowId: (context as any)?.workflowId,\\n           fetchDepth: checkoutConfig.fetch_depth,\\n+          cloneTimeoutMs: checkoutConfig.clone_timeout_ms,\\n         }\\n       );\\n \\n@@ -134,18 +151,35 @@ export class GitCheckoutProvider extends CheckProvider {\\n \\n       // Add project to workspace if workspace isolation is enabled\\n       const workspace = (context as any)?._parentContext?.workspace;\\n+\\n+      // Enhanced debug logging for workspace project addition diagnosis\\n+      const checkName = (config as any)?.checkName || 'unknown';\\n+      logger.info(`[GitCheckout] Workspace check for '${checkName}':`);\\n+      logger.info(`[GitCheckout]   _parentContext exists: ${!!(context as any)?._parentContext}`);\\n+      logger.info(`[GitCheckout]   workspace exists: ${!!workspace}`);\\n+      logger.info(`[GitCheckout]   workspace.isEnabled(): ${workspace?.isEnabled?.() ?? 'N/A'}`);\\n+      if (workspace) {\\n+        const projectCountBefore = workspace.listProjects?.()?.length ?? 'N/A';\\n+        logger.debug(`[GitCheckout]   projects before addProject: ${projectCountBefore}`);\\n+      }\\n+\\n       if (workspace?.isEnabled()) {\\n         try {\\n+          // Don't pass checkName as description - let workspace use repo name\\n+          // This results in human-readable names like \\\"tyk-docs\\\" instead of \\\"checkout-tyk-docs\\\"\\n           const workspacePath = await workspace.addProject(\\n             resolvedRepository,\\n-            worktree.path,\\n-            checkoutConfig.checkName\\n+            worktree.path\\n           );\\n           output.workspace_path = workspacePath;\\n-          logger.debug(`Added project to workspace: ${workspacePath}`);\\n+          const projectCountAfter = workspace.listProjects?.()?.length ?? 'N/A';\\n+          logger.debug(`[GitCheckout] Added project to workspace: ${workspacePath}`);\\n+          logger.debug(`[GitCheckout]   projects after addProject: ${projectCountAfter}`);\\n         } catch (error) {\\n           logger.warn(`Failed to add project to workspace: ${error}`);\\n         }\\n+      } else {\\n+        logger.debug(`[GitCheckout] Workspace not enabled, skipping addProject`);\\n       }\\n \\n       logger.info(\\n@@ -228,7 +262,8 @@ export class GitCheckoutProvider extends CheckProvider {\\n       outputs: outputsObj,\\n       outputs_history: historyObj,\\n       env: safeEnv,\\n-      inputs: context?.workflowInputs,\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any)?.workflowInputs || context?.workflowInputs,\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/http-client-provider.ts\",\"additions\":279,\"deletions\":35,\"changes\":314,\"patch\":\"diff --git a/src/providers/http-client-provider.ts b/src/providers/http-client-provider.ts\\nindex 4d8c41be..6db81591 100644\\n--- a/src/providers/http-client-provider.ts\\n+++ b/src/providers/http-client-provider.ts\\n@@ -4,18 +4,29 @@ import { ReviewSummary } from '../reviewer';\\n import { Liquid } from 'liquidjs';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n import { EnvironmentResolver } from '../utils/env-resolver';\\n+import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n+import { buildProviderTemplateContext } from '../utils/template-context';\\n+import Sandbox from '@nyariv/sandboxjs';\\n+import { logger } from '../logger';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n \\n /**\\n  * Check provider that fetches data from HTTP endpoints\\n  */\\n export class HttpClientProvider extends CheckProvider {\\n   private liquid: Liquid;\\n+  private sandbox?: Sandbox;\\n \\n   constructor() {\\n     super();\\n     this.liquid = createExtendedLiquid();\\n   }\\n \\n+  private createSecureSandbox(): Sandbox {\\n+    return createSecureSandbox();\\n+  }\\n+\\n   getName(): string {\\n     return 'http_client';\\n   }\\n@@ -61,50 +72,124 @@ export class HttpClientProvider extends CheckProvider {\\n     const headers = (config.headers as Record<string, string>) || {};\\n     const timeout = (config.timeout as number) || 30000;\\n     const transform = config.transform as string | undefined;\\n+    const transformJs = config.transform_js as string | undefined;\\n     const bodyTemplate = config.body as string | undefined;\\n+    const outputFileTemplate = config.output_file as string | undefined;\\n+    const skipIfExists = config.skip_if_exists !== false; // Default true for caching\\n \\n-    try {\\n-      // Prepare template context for URL and body\\n-      const templateContext = {\\n-        pr: {\\n-          number: prInfo.number,\\n-          title: prInfo.title,\\n-          body: prInfo.body,\\n-          author: prInfo.author,\\n-          base: prInfo.base,\\n-          head: prInfo.head,\\n-          totalAdditions: prInfo.totalAdditions,\\n-          totalDeletions: prInfo.totalDeletions,\\n-        },\\n-        outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-        env: process.env,\\n-      };\\n+    // Track resolved URL for error messages\\n+    let resolvedUrlForErrors = url;\\n \\n-      // Render URL with template if it contains liquid syntax\\n-      let renderedUrl = url;\\n-      if (url.includes('{{') || url.includes('{%')) {\\n-        renderedUrl = await this.liquid.parseAndRender(url, templateContext);\\n+    try {\\n+      // Use shared template context builder for consistent output extraction\\n+      const templateContext = buildProviderTemplateContext(\\n+        prInfo,\\n+        dependencyResults,\\n+        undefined, // memoryStore\\n+        undefined, // outputHistory\\n+        undefined, // stageHistoryBase\\n+        { attachMemoryReadHelpers: false }\\n+      );\\n+      // Add env to context for shell-style variable resolution\\n+      (templateContext as Record<string, unknown>).env = process.env;\\n+\\n+      // First resolve shell-style environment variables (${VAR}, $VAR, ${{ env.VAR }})\\n+      let renderedUrl = String(EnvironmentResolver.resolveValue(url));\\n+      resolvedUrlForErrors = renderedUrl; // Track for error messages\\n+\\n+      // Then render Liquid templates if present\\n+      if (renderedUrl.includes('{{') || renderedUrl.includes('{%')) {\\n+        renderedUrl = await this.liquid.parseAndRender(renderedUrl, templateContext);\\n+        resolvedUrlForErrors = renderedUrl; // Update after Liquid rendering\\n       }\\n \\n       // Prepare request body if provided\\n       let requestBody: string | undefined;\\n       if (bodyTemplate) {\\n-        const renderedBody = await this.liquid.parseAndRender(bodyTemplate, templateContext);\\n-        requestBody = renderedBody;\\n+        // First resolve shell-style environment variables\\n+        let resolvedBody = String(EnvironmentResolver.resolveValue(bodyTemplate));\\n+        // Then render Liquid templates if present\\n+        if (resolvedBody.includes('{{') || resolvedBody.includes('{%')) {\\n+          resolvedBody = await this.liquid.parseAndRender(resolvedBody, templateContext);\\n+        }\\n+        requestBody = resolvedBody;\\n       }\\n \\n-      // Resolve environment variables in headers\\n-      const resolvedHeaders = EnvironmentResolver.resolveHeaders(headers);\\n+      // Resolve environment variables and Liquid templates in headers\\n+      const resolvedHeaders: Record<string, string> = {};\\n+      for (const [key, value] of Object.entries(headers)) {\\n+        let resolvedValue = String(EnvironmentResolver.resolveValue(value));\\n+        // Render Liquid templates if present\\n+        if (resolvedValue.includes('{{') || resolvedValue.includes('{%')) {\\n+          resolvedValue = await this.liquid.parseAndRender(resolvedValue, templateContext);\\n+        }\\n+        resolvedHeaders[key] = resolvedValue;\\n+        // Debug auth header (mask most of the value for security)\\n+        if (key.toLowerCase() === 'authorization') {\\n+          const maskedValue = resolvedValue.length > 20\\n+            ? `${resolvedValue.substring(0, 15)}...${resolvedValue.substring(resolvedValue.length - 5)}`\\n+            : resolvedValue;\\n+          logger.verbose(`[http_client] ${key}: ${maskedValue}`);\\n+        }\\n+      }\\n+\\n+      // Resolve output_file path if specified\\n+      let resolvedOutputFile: string | undefined;\\n+      if (outputFileTemplate) {\\n+        let outputPath = String(EnvironmentResolver.resolveValue(outputFileTemplate));\\n+        if (outputPath.includes('{{') || outputPath.includes('{%')) {\\n+          outputPath = await this.liquid.parseAndRender(outputPath, templateContext);\\n+        }\\n+        resolvedOutputFile = outputPath.trim();\\n+\\n+        // Check if file already exists (caching)\\n+        if (skipIfExists && fs.existsSync(resolvedOutputFile)) {\\n+          const stats = fs.statSync(resolvedOutputFile);\\n+          logger.verbose(`[http_client] File cached: ${resolvedOutputFile} (${stats.size} bytes)`);\\n+          return {\\n+            issues: [],\\n+            file_path: resolvedOutputFile,\\n+            size: stats.size,\\n+            cached: true,\\n+          } as unknown as ReviewSummary;\\n+        }\\n+      }\\n \\n       // Test hook: mock HTTP response for this step\\n       const stepName = (config as any).checkName || 'unknown';\\n       const mock = context?.hooks?.mockForStep?.(String(stepName));\\n-      const data =\\n-        mock !== undefined\\n-          ? mock\\n-          : await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n \\n-      // Apply transformation if specified\\n+      // If mock is provided, return it directly as the step output (with issues: [])\\n+      if (mock !== undefined) {\\n+        const mockObj = typeof mock === 'object' && mock !== null ? mock : { data: mock };\\n+        return {\\n+          issues: [],\\n+          ...mockObj,\\n+        } as unknown as ReviewSummary & { data: unknown };\\n+      }\\n+\\n+      // Debug log the request for troubleshooting\\n+      logger.verbose(`[http_client] ${method} ${renderedUrl}`);\\n+      if (requestBody) {\\n+        logger.verbose(`[http_client] Body: ${requestBody.substring(0, 500)}${requestBody.length > 500 ? '...' : ''}`);\\n+      }\\n+\\n+      // If output_file is specified, download to file instead of returning data\\n+      if (resolvedOutputFile) {\\n+        const fileResult = await this.downloadToFile(\\n+          renderedUrl,\\n+          method,\\n+          resolvedHeaders,\\n+          requestBody,\\n+          timeout,\\n+          resolvedOutputFile\\n+        );\\n+        return fileResult;\\n+      }\\n+\\n+      const data = await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n+\\n+      // Apply Liquid transformation if specified\\n       let processedData = data;\\n       if (transform) {\\n         try {\\n@@ -136,12 +221,53 @@ export class HttpClientProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Return the fetched data as a custom field for dependent checks to access\\n+      // Apply JavaScript transformation if specified\\n+      if (transformJs) {\\n+        try {\\n+          if (!this.sandbox) {\\n+            this.sandbox = this.createSecureSandbox();\\n+          }\\n+\\n+          // Create scope for JavaScript transform (scope, not context)\\n+          const jsScope: Record<string, unknown> = {\\n+            output: data,\\n+            pr: templateContext.pr,\\n+            outputs: templateContext.outputs,\\n+            env: process.env,\\n+          };\\n+\\n+          const result = compileAndRun(this.sandbox, transformJs, jsScope, {\\n+            injectLog: true,\\n+            logPrefix: 'üîç [transform_js]',\\n+            wrapFunction: true,\\n+          });\\n+          processedData = result;\\n+          logger.verbose(`‚úì Applied JavaScript transform successfully`);\\n+        } catch (error) {\\n+          logger.error(\\n+            `‚úó Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`\\n+          );\\n+          return {\\n+            issues: [\\n+              {\\n+                file: 'http_client',\\n+                line: 0,\\n+                ruleId: 'http_client/transform_js_error',\\n+                message: `Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              },\\n+            ],\\n+          };\\n+        }\\n+      }\\n+\\n+      // Return the fetched data in the standard `output` property for guarantee evaluation\\n+      // This is consistent with other providers (script, command, ai, etc.)\\n       return {\\n         issues: [],\\n-        // Add custom data field that will be passed through to dependent checks\\n-        data: processedData,\\n-      } as ReviewSummary & { data: unknown };\\n+        output: processedData,\\n+      } as unknown as ReviewSummary;\\n     } catch (error) {\\n       return {\\n         issues: [\\n@@ -149,7 +275,7 @@ export class HttpClientProvider extends CheckProvider {\\n             file: 'http_client',\\n             line: 0,\\n             ruleId: 'http_client/fetch_error',\\n-            message: `Failed to fetch from ${url}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            message: `Failed to fetch from ${resolvedUrlForErrors}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n             severity: 'error',\\n             category: 'logic',\\n           },\\n@@ -198,7 +324,14 @@ export class HttpClientProvider extends CheckProvider {\\n \\n       clearTimeout(timeoutId);\\n \\n+      logger.verbose(`[http_client] Response: ${response.status} ${response.statusText}`);\\n+\\n       if (!response.ok) {\\n+        // Log response body for debugging auth failures\\n+        try {\\n+          const errorBody = await response.text();\\n+          logger.warn(`[http_client] Error body: ${errorBody.substring(0, 500)}`);\\n+        } catch {}\\n         throw new Error(`HTTP ${response.status}: ${response.statusText}`);\\n       }\\n \\n@@ -233,6 +366,112 @@ export class HttpClientProvider extends CheckProvider {\\n     }\\n   }\\n \\n+  private async downloadToFile(\\n+    url: string,\\n+    method: string,\\n+    headers: Record<string, string>,\\n+    body: string | undefined,\\n+    timeout: number,\\n+    outputFile: string\\n+  ): Promise<ReviewSummary> {\\n+    // Check if fetch is available (Node 18+)\\n+    if (typeof fetch === 'undefined') {\\n+      throw new Error('HTTP client provider requires Node.js 18+ or node-fetch package');\\n+    }\\n+\\n+    const controller = new AbortController();\\n+    const timeoutId = setTimeout(() => controller.abort(), timeout);\\n+\\n+    try {\\n+      const requestOptions: RequestInit = {\\n+        method,\\n+        headers: { ...headers },\\n+        signal: controller.signal,\\n+      };\\n+\\n+      // Add body for non-GET requests\\n+      if (method !== 'GET' && body) {\\n+        requestOptions.body = body;\\n+        if (!headers['Content-Type'] && !headers['content-type']) {\\n+          requestOptions.headers = {\\n+            ...requestOptions.headers,\\n+            'Content-Type': 'application/json',\\n+          };\\n+        }\\n+      }\\n+\\n+      const response = await fetch(url, requestOptions);\\n+      clearTimeout(timeoutId);\\n+\\n+      if (!response.ok) {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_error',\\n+              message: `Failed to download file: HTTP ${response.status}: ${response.statusText}`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      // Create parent directory if it doesn't exist\\n+      const parentDir = path.dirname(outputFile);\\n+      if (parentDir && !fs.existsSync(parentDir)) {\\n+        fs.mkdirSync(parentDir, { recursive: true });\\n+      }\\n+\\n+      // Get the response as an ArrayBuffer and write to file\\n+      const arrayBuffer = await response.arrayBuffer();\\n+      const buffer = Buffer.from(arrayBuffer);\\n+      fs.writeFileSync(outputFile, buffer);\\n+\\n+      const contentType = response.headers.get('content-type') || 'application/octet-stream';\\n+      logger.verbose(`[http_client] Downloaded: ${outputFile} (${buffer.length} bytes)`);\\n+\\n+      return {\\n+        issues: [],\\n+        file_path: outputFile,\\n+        size: buffer.length,\\n+        content_type: contentType,\\n+        cached: false,\\n+      } as unknown as ReviewSummary;\\n+    } catch (error: unknown) {\\n+      clearTimeout(timeoutId);\\n+\\n+      if (error instanceof Error && error.name === 'AbortError') {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_timeout',\\n+              message: `Download timed out after ${timeout}ms`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      return {\\n+        issues: [\\n+          {\\n+            file: 'http_client',\\n+            line: 0,\\n+            ruleId: 'http_client/download_error',\\n+            message: `Failed to download file: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            severity: 'error',\\n+            category: 'logic',\\n+          },\\n+        ],\\n+      };\\n+    }\\n+  }\\n+\\n   getSupportedConfigKeys(): string[] {\\n     return [\\n       'type',\\n@@ -241,7 +480,10 @@ export class HttpClientProvider extends CheckProvider {\\n       'headers',\\n       'body',\\n       'transform',\\n+      'transform_js',\\n       'timeout',\\n+      'output_file',\\n+      'skip_if_exists',\\n       'depends_on',\\n       'on',\\n       'if',\\n@@ -261,6 +503,8 @@ export class HttpClientProvider extends CheckProvider {\\n       'Network access to the endpoint',\\n       'Optional: Transform template for processing response data',\\n       'Optional: Body template for POST/PUT requests',\\n+      'Optional: output_file path to download response to a file',\\n+      'Optional: skip_if_exists (default: true) to enable caching for file downloads',\\n     ];\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/log-check-provider.ts\",\"additions\":19,\"deletions\":8,\"changes\":27,\"patch\":\"diff --git a/src/providers/log-check-provider.ts b/src/providers/log-check-provider.ts\\nindex 9c6332d1..b57f632c 100644\\n--- a/src/providers/log-check-provider.ts\\n+++ b/src/providers/log-check-provider.ts\\n@@ -78,7 +78,8 @@ export class LogCheckProvider extends CheckProvider {\\n       includeDependencies,\\n       includeMetadata,\\n       config.__outputHistory as Map<string, unknown[]> | undefined,\\n-      context\\n+      context,\\n+      config\\n     );\\n \\n     // Render the log message template\\n@@ -100,12 +101,20 @@ export class LogCheckProvider extends CheckProvider {\\n     else if (level === 'debug') logger.debug(logOutput);\\n     else logger.info(logOutput);\\n \\n-    // Return with the log content as custom data for dependent checks\\n-    return {\\n+    // Return with the log content as custom data for dependent checks.\\n+    // For chat-style logs (group: chat), also expose a structured\\n+    // `output.text` field so frontends like Slack can post a clean\\n+    // human-facing message without the level prefix.\\n+    const summary: ReviewSummary & { logOutput: string; output?: unknown } = {\\n       issues: [],\\n-      // Add log output as custom field\\n       logOutput,\\n-    } as ReviewSummary & { logOutput: string };\\n+    };\\n+\\n+    if ((config as any).group === 'chat') {\\n+      (summary as any).output = { text: renderedMessage };\\n+    }\\n+\\n+    return summary;\\n   }\\n \\n   private buildTemplateContext(\\n@@ -115,7 +124,8 @@ export class LogCheckProvider extends CheckProvider {\\n     _includeDependencies: boolean = true,\\n     includeMetadata: boolean = true,\\n     outputHistory?: Map<string, unknown[]>,\\n-    executionContext?: ExecutionContext\\n+    executionContext?: ExecutionContext,\\n+    config?: CheckProviderConfig\\n   ): Record<string, unknown> {\\n     const context: Record<string, unknown> = {};\\n \\n@@ -197,7 +207,8 @@ export class LogCheckProvider extends CheckProvider {\\n     }\\n \\n     // Add workflow inputs if available\\n-    const workflowInputs = executionContext?.workflowInputs || {};\\n+    // Check config first (set by projectWorkflowToGraph), then fall back to executionContext\\n+    const workflowInputs = (config as any)?.workflowInputs || executionContext?.workflowInputs || {};\\n     logger.debug(\\n       `[LogProvider] Adding ${Object.keys(workflowInputs).length} workflow inputs to context`\\n     );\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/script-check-provider.ts\",\"additions\":37,\"deletions\":1,\"changes\":38,\"patch\":\"diff --git a/src/providers/script-check-provider.ts b/src/providers/script-check-provider.ts\\nindex 9825bb69..947dbd73 100644\\n--- a/src/providers/script-check-provider.ts\\n+++ b/src/providers/script-check-provider.ts\\n@@ -59,6 +59,16 @@ export class ScriptCheckProvider extends CheckProvider {\\n       reuseSession?: boolean;\\n     } & import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n+    // Test hook: mock output for this step (short-circuit execution)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = _sessionInfo?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        // Return mock directly as step output\\n+        return { issues: [], output: mock } as ReviewSummary & { output: unknown };\\n+      }\\n+    } catch {}\\n+\\n     const script = String(config.content || '');\\n     const memoryStore = MemoryStore.getInstance();\\n     const ctx = buildProviderTemplateContext(\\n@@ -72,10 +82,36 @@ export class ScriptCheckProvider extends CheckProvider {\\n     // Keep provider quiet by default; no step-specific debug\\n     // (historical ad-hoc logs removed to avoid hardcoding step names).\\n \\n+    // Add workflow inputs to the context\\n+    const inputs = (config as any).workflowInputs || _sessionInfo?.workflowInputs || {};\\n+    (ctx as any).inputs = inputs;\\n+\\n+    // Add environment variables to context (consistent with http-client-provider)\\n+    (ctx as any).env = process.env;\\n+\\n     // Attach synchronous memory ops consistent with memory provider\\n     const { ops, needsSave } = createSyncMemoryOps(memoryStore);\\n     (ctx as any).memory = ops as unknown as Record<string, unknown>;\\n \\n+    // Add helper functions to the context\\n+    (ctx as any).escapeXml = (str: unknown): string => {\\n+      if (str == null) return '';\\n+      return String(str)\\n+        .replace(/&/g, '&amp;')\\n+        .replace(/</g, '&lt;')\\n+        .replace(/>/g, '&gt;')\\n+        .replace(/\\\"/g, '&quot;')\\n+        .replace(/'/g, '&apos;');\\n+    };\\n+\\n+    // Add btoa/atob for base64 encoding/decoding (browser API polyfill)\\n+    (ctx as any).btoa = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'binary').toString('base64');\\n+    };\\n+    (ctx as any).atob = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'base64').toString('binary');\\n+    };\\n+\\n     // Evaluate the script in a secure sandbox (per-execution instance)\\n     const sandbox = this.createSecureSandbox();\\n     let result: unknown;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/workflow-check-provider.ts\",\"additions\":164,\"deletions\":43,\"changes\":207,\"patch\":\"diff --git a/src/providers/workflow-check-provider.ts b/src/providers/workflow-check-provider.ts\\nindex 116d4dff..681703ce 100644\\n--- a/src/providers/workflow-check-provider.ts\\n+++ b/src/providers/workflow-check-provider.ts\\n@@ -67,34 +67,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n   ): Promise<ReviewSummary> {\\n     const cfg = config as CheckProviderConfig & { workflow?: string; config?: string };\\n     const isConfigPathMode = !!cfg.config && !cfg.workflow;\\n+    const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n \\n-    // Test harness support: allow mocking workflow checks as a black box.\\n-    // If a mock is provided for this step, short-circuit nested execution and\\n-    // return a ReviewSummary with optional output field.\\n-    try {\\n-      const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n-      // test-runner passes hooks on execution context\\n-      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n-      if (mock !== undefined) {\\n-        const ms = mock as any;\\n-        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n-        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n-        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n-        const summary: ReviewSummary & { output?: unknown } = {\\n-          issues: issuesArr,\\n-          output: out,\\n-          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n-        } as any;\\n-        return summary;\\n-      }\\n-    } catch {}\\n-\\n-    // Resolve workflow definition\\n+    // Resolve workflow definition FIRST (needed for input preparation and validation)\\n     let workflow: WorkflowDefinition | undefined;\\n     let workflowId = cfg.workflow as string | undefined;\\n \\n     if (isConfigPathMode) {\\n-      const parentCwd = ((context as any)?._parentContext?.workingDirectory ||\\n+      // Use originalWorkingDirectory for config file resolution - workflow configs\\n+      // should be loaded from the original project path, not the sandbox\\n+      const parentCwd = ((context as any)?._parentContext?.originalWorkingDirectory ||\\n+        (context as any)?._parentContext?.workingDirectory ||\\n+        (context as any)?.originalWorkingDirectory ||\\n         (context as any)?.workingDirectory ||\\n         process.cwd()) as string;\\n       workflow = await this.loadWorkflowFromConfigPath(String(cfg.config), parentCwd);\\n@@ -109,9 +93,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       logger.info(`Executing workflow '${workflowId}'`);\\n     }\\n \\n-    // Prepare inputs\\n+    // Prepare inputs - do this BEFORE mock check so we can validate inputs even when mocked\\n+    // This allows tests to assert that the correct inputs were passed to workflow steps\\n     const inputs = await this.prepareInputs(workflow, config, prInfo, dependencyResults);\\n \\n+    // Capture resolved workflow inputs for testing assertions (reuse prompt capture infrastructure)\\n+    // This allows using `prompts` assertions with `provider: 'workflow'` to verify inputs\\n+    try {\\n+      // Serialize inputs to capture for assertions - specifically the context field\\n+      // which contains the rendered template with dependency outputs\\n+      const inputsCapture = Object.entries(inputs)\\n+        .map(([k, v]) => `${k}: ${typeof v === 'string' ? v : JSON.stringify(v)}`)\\n+        .join('\\\\n\\\\n');\\n+      (context as any)?.hooks?.onPromptCaptured?.({\\n+        step: String(stepName),\\n+        provider: 'workflow',\\n+        prompt: inputsCapture,\\n+      });\\n+    } catch {\\n+      // Ignore capture errors - this is only for testing\\n+    }\\n+\\n     // Validate inputs\\n     const validation = this.registry.validateInputs(workflow, inputs);\\n     if (!validation.valid) {\\n@@ -119,6 +121,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       throw new Error(`Invalid workflow inputs: ${errors}`);\\n     }\\n \\n+    // Test harness support: allow mocking workflow checks as a black box.\\n+    // If a mock is provided for this step, short-circuit nested execution and\\n+    // return a ReviewSummary with optional output field.\\n+    // NOTE: This happens AFTER input preparation/validation so tests can assert inputs are correct\\n+    try {\\n+      // test-runner passes hooks on execution context\\n+      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        const ms = mock as any;\\n+        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n+        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n+        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n+        const summary: ReviewSummary & { output?: unknown } = {\\n+          issues: issuesArr,\\n+          output: out,\\n+          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n+        } as any;\\n+        return summary;\\n+      }\\n+    } catch {}\\n+\\n     // Apply overrides to workflow steps if specified\\n     const modifiedWorkflow = this.applyOverrides(workflow, config);\\n \\n@@ -211,6 +234,80 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Extract eventContext for slack/conversation\\n+    const eventContext = config.eventContext || {};\\n+\\n+    // Debug logging for conversation context\\n+    logger.debug(`[WorkflowProvider] prepareInputs for ${workflow.id}`);\\n+    logger.debug(`[WorkflowProvider] eventContext keys: ${Object.keys(eventContext).join(', ') || 'none'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.slack: ${eventContext.slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.conversation: ${(eventContext as any).conversation ? 'present' : 'absent'}`);\\n+\\n+    // Extract slack context (if provided via eventContext.slack)\\n+    const slack = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        const slackCtx = anyCtx?.slack;\\n+        if (slackCtx && typeof slackCtx === 'object') return slackCtx;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Extract unified conversation context across transports (Slack & GitHub)\\n+    const conversation = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        if (anyCtx?.slack?.conversation) return anyCtx.slack.conversation;\\n+        if (anyCtx?.github?.conversation) return anyCtx.github.conversation;\\n+        if (anyCtx?.conversation) return anyCtx.conversation;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Debug logging for extracted context\\n+    logger.debug(`[WorkflowProvider] slack extracted: ${slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] conversation extracted: ${conversation ? 'present' : 'absent'}`);\\n+    if (conversation) {\\n+      logger.debug(`[WorkflowProvider] conversation.messages count: ${Array.isArray((conversation as any).messages) ? (conversation as any).messages.length : 0}`);\\n+    }\\n+\\n+    // Extract output history from config (passed via __outputHistory)\\n+    const outputHistory = (config as any).__outputHistory as Map<string, unknown[]> | undefined;\\n+    const outputs_history: Record<string, unknown[]> = {};\\n+    if (outputHistory) {\\n+      for (const [k, v] of outputHistory.entries()) {\\n+        outputs_history[k] = v;\\n+      }\\n+    }\\n+\\n+    // Build template context with all available data\\n+    // Extract .output from each dependency result so that outputs['step-name'].field works naturally\\n+    // (not outputs['step-name'].output.field)\\n+    const outputsMap: Record<string, unknown> = {};\\n+    logger.debug(`[WorkflowProvider] dependencyResults: ${dependencyResults ? dependencyResults.size : 'undefined'} entries`);\\n+    if (dependencyResults) {\\n+      for (const [key, result] of dependencyResults.entries()) {\\n+        // Extract the output property, or use the whole result if output is undefined\\n+        const extracted = (result as any).output ?? result;\\n+        outputsMap[key] = extracted;\\n+        // Debug: log what we extracted for each dependency\\n+        const extractedKeys = extracted && typeof extracted === 'object' ? Object.keys(extracted).join(', ') : 'not-object';\\n+        logger.debug(`[WorkflowProvider] outputs['${key}']: keys=[${extractedKeys}]`);\\n+      }\\n+    }\\n+    const templateContext = {\\n+      pr: prInfo,\\n+      outputs: outputsMap,\\n+      env: process.env,\\n+      slack,\\n+      conversation,\\n+      outputs_history,\\n+    };\\n+\\n     // Apply user-provided inputs (args)\\n     const userInputs = config.args || config.workflow_inputs; // Support both for compatibility\\n     if (userInputs) {\\n@@ -219,11 +316,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         if (typeof value === 'string') {\\n           // Check if it's a Liquid template\\n           if (value.includes('{{') || value.includes('{%')) {\\n-            inputs[key] = await this.liquid.parseAndRender(value, {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            });\\n+            inputs[key] = await this.liquid.parseAndRender(value, templateContext);\\n+            // Debug: log rendered template value for important inputs\\n+            if (key === 'text' || key === 'question' || key === 'context') {\\n+              const rendered = String(inputs[key]);\\n+              logger.info(`[WorkflowProvider] Rendered '${key}' input (${rendered.length} chars): ${rendered.substring(0, 500)}${rendered.length > 500 ? '...' : ''}`);\\n+            }\\n           } else {\\n             inputs[key] = value;\\n           }\\n@@ -234,11 +332,7 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           inputs[key] = compileAndRun(\\n             sandbox,\\n             exprValue.expression,\\n-            {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            },\\n+            templateContext,\\n             { injectLog: true, logPrefix: `workflow.input.${key}` }\\n           );\\n         } else {\\n@@ -367,6 +461,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n     try {\\n       await childMemory.initialize();\\n     } catch {}\\n+    // Enhanced debug logging for workspace propagation diagnosis\\n+    const parentWorkspace = parentContext?.workspace;\\n+    logger.info(`[WorkflowProvider] Workspace propagation for nested workflow '${workflow.id}':`);\\n+    logger.info(`[WorkflowProvider]   parentContext exists: ${!!parentContext}`);\\n+    logger.info(`[WorkflowProvider]   parentContext.workspace exists: ${!!parentWorkspace}`);\\n+    if (parentWorkspace) {\\n+      logger.info(`[WorkflowProvider]   parentWorkspace.isEnabled(): ${parentWorkspace.isEnabled?.() ?? 'N/A'}`);\\n+      const projectCount = parentWorkspace.listProjects?.()?.length ?? 'N/A';\\n+      logger.info(`[WorkflowProvider]   parentWorkspace project count: ${projectCount}`);\\n+    } else {\\n+      logger.warn(`[WorkflowProvider]   NO WORKSPACE from parent - nested checkouts won't be added to workspace!`);\\n+    }\\n \\n     const childContext = {\\n       mode: 'state-machine' as const,\\n@@ -374,7 +480,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       checks: checksMetadata,\\n       journal: childJournal,\\n       memory: childMemory,\\n+      // For nested workflows we continue to execute inside the same logical\\n+      // working directory as the parent run. When workspace isolation is\\n+      // enabled on the parent engine, its WorkspaceManager is also propagated\\n+      // so that nested checks (AI, git-checkout, etc.) see the same isolated\\n+      // workspace and project symlinks instead of falling back to the Visor\\n+      // repository root.\\n       workingDirectory: parentContext?.workingDirectory || process.cwd(),\\n+      originalWorkingDirectory:\\n+        parentContext?.originalWorkingDirectory ||\\n+        parentContext?.workingDirectory ||\\n+        process.cwd(),\\n+      workspace: parentWorkspace,\\n       // Always use a fresh session for nested workflows to isolate history\\n       sessionId: uuidv4(),\\n       event: parentContext?.event || prInfo.eventType,\\n@@ -500,6 +617,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     } catch {}\\n \\n+    // Create outputs map that directly exposes .output values (not wrapped in { output, issues })\\n+    // This makes {{ outputs['check-name'] }} work naturally without needing .output\\n+    const outputsMap = Object.fromEntries(\\n+      Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n+    );\\n+\\n     for (const output of workflow.outputs) {\\n       if (output.value_js) {\\n         // JavaScript expression\\n@@ -508,10 +631,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           output.value_js,\\n           {\\n             inputs,\\n-            steps: Object.fromEntries(\\n-              Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-            ),\\n-            outputs: flat,\\n+            outputs: outputsMap,\\n+            // Keep 'steps' as alias for backwards compatibility\\n+            steps: outputsMap,\\n             pr: prInfo,\\n           },\\n           { injectLog: true, logPrefix: `workflow.output.${output.name}` }\\n@@ -520,10 +642,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         // Liquid template\\n         outputs[output.name] = await this.liquid.parseAndRender(output.value, {\\n           inputs,\\n-          steps: Object.fromEntries(\\n-            Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-          ),\\n-          outputs: flat,\\n+          outputs: outputsMap,\\n+          // Keep 'steps' as alias for backwards compatibility\\n+          steps: outputsMap,\\n           pr: prInfo,\\n         });\\n       }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/client.ts\",\"additions\":65,\"deletions\":1,\"changes\":66,\"patch\":\"diff --git a/src/slack/client.ts b/src/slack/client.ts\\nindex 292cc358..435f8b0d 100644\\n--- a/src/slack/client.ts\\n+++ b/src/slack/client.ts\\n@@ -142,6 +142,70 @@ export class SlackClient {\\n     }\\n   }\\n \\n+  public readonly files = {\\n+    /**\\n+     * Upload a file to Slack using files.uploadV2 API\\n+     * @param options Upload options including file content, filename, channel, and thread_ts\\n+     */\\n+    uploadV2: async ({\\n+      content,\\n+      filename,\\n+      channel,\\n+      thread_ts,\\n+      title,\\n+      initial_comment,\\n+    }: {\\n+      content: Buffer;\\n+      filename: string;\\n+      channel: string;\\n+      thread_ts?: string;\\n+      title?: string;\\n+      initial_comment?: string;\\n+    }): Promise<{ ok: boolean; file?: { id: string; permalink?: string } }> => {\\n+      try {\\n+        // Step 1: Get upload URL\\n+        const getUrlResp: any = await this.api('files.getUploadURLExternal', {\\n+          filename,\\n+          length: content.length,\\n+        });\\n+        if (!getUrlResp || getUrlResp.ok !== true || !getUrlResp.upload_url) {\\n+          console.warn(`Slack files.getUploadURLExternal failed: ${getUrlResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 2: Upload file content to the URL\\n+        const uploadResp = await fetch(getUrlResp.upload_url, {\\n+          method: 'POST',\\n+          body: content,\\n+        });\\n+        if (!uploadResp.ok) {\\n+          console.warn(`Slack file upload to URL failed: ${uploadResp.status}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 3: Complete the upload and share to channel\\n+        const completeResp: any = await this.api('files.completeUploadExternal', {\\n+          files: [{ id: getUrlResp.file_id, title: title || filename }],\\n+          channel_id: channel,\\n+          thread_ts,\\n+          initial_comment,\\n+        });\\n+        if (!completeResp || completeResp.ok !== true) {\\n+          console.warn(`Slack files.completeUploadExternal failed: ${completeResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        return {\\n+          ok: true,\\n+          file: completeResp.files?.[0] || { id: getUrlResp.file_id },\\n+        };\\n+      } catch (e) {\\n+        console.warn(`Slack file upload failed: ${e instanceof Error ? e.message : String(e)}`);\\n+        return { ok: false };\\n+      }\\n+    },\\n+  };\\n+\\n   getWebClient(): any {\\n     return {\\n       conversations: {\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/markdown.ts\",\"additions\":204,\"deletions\":5,\"changes\":209,\"patch\":\"diff --git a/src/slack/markdown.ts b/src/slack/markdown.ts\\nindex cc63fb5f..9f91c25f 100644\\n--- a/src/slack/markdown.ts\\n+++ b/src/slack/markdown.ts\\n@@ -3,14 +3,191 @@\\n // without pulling in a full Markdown parser.\\n //\\n // Supported conversions:\\n+// - # Header / ## Header  ‚Üí *Header* (bold with visual separation)\\n // - **bold** / __bold__   ‚Üí *bold*\\n // - [label](url)          ‚Üí <url|label>\\n // - ![alt](url)           ‚Üí <url|alt>\\n // - *italic* (inline)     ‚Üí _italic_\\n+// - ```mermaid blocks     ‚Üí rendered to PNG and uploaded to Slack\\n //\\n // Everything else is passed through unchanged; Slack will still render many\\n // Markdown-like constructs (lists, code fences, etc.) natively.\\n \\n+import { spawn } from 'child_process';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n+import * as os from 'os';\\n+\\n+/**\\n+ * Represents an extracted mermaid diagram\\n+ */\\n+export interface MermaidDiagram {\\n+  /** The full match including ```mermaid and ``` */\\n+  fullMatch: string;\\n+  /** The mermaid code content */\\n+  code: string;\\n+  /** Start index in the original text */\\n+  startIndex: number;\\n+  /** End index in the original text */\\n+  endIndex: number;\\n+}\\n+\\n+/**\\n+ * Extract all mermaid code blocks from text\\n+ */\\n+export function extractMermaidDiagrams(text: string): MermaidDiagram[] {\\n+  const diagrams: MermaidDiagram[] = [];\\n+  // Match ```mermaid followed by newline, content, and closing ```\\n+  const regex = /```mermaid\\\\s*\\\\n([\\\\s\\\\S]*?)```/g;\\n+  let match;\\n+  while ((match = regex.exec(text)) !== null) {\\n+    diagrams.push({\\n+      fullMatch: match[0],\\n+      code: match[1].trim(),\\n+      startIndex: match.index,\\n+      endIndex: match.index + match[0].length,\\n+    });\\n+  }\\n+  return diagrams;\\n+}\\n+\\n+/**\\n+ * Render a mermaid diagram to PNG using mmdc CLI (@mermaid-js/mermaid-cli).\\n+ *\\n+ * Requirements:\\n+ * - Node.js and npx must be available in PATH\\n+ * - Network access on first run (npx downloads the package)\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ *\\n+ * On Linux, you may need to install chromium dependencies:\\n+ *   apt-get install -y chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ *\\n+ * On Docker/CI, consider using a base image with puppeteer support or\\n+ * pre-installing @mermaid-js/mermaid-cli globally.\\n+ *\\n+ * @param mermaidCode The mermaid diagram code\\n+ * @returns Buffer containing PNG data, or null if rendering failed\\n+ */\\n+export async function renderMermaidToPng(mermaidCode: string): Promise<Buffer | null> {\\n+  // Create temp files for input and output\\n+  const tmpDir = os.tmpdir();\\n+  const inputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.mmd`);\\n+  const outputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.png`);\\n+\\n+  try {\\n+    // Write mermaid code to temp file\\n+    fs.writeFileSync(inputFile, mermaidCode, 'utf-8');\\n+\\n+    // Detect system chromium for puppeteer (mermaid-cli dependency)\\n+    // Without this, puppeteer may hang trying to download its own chromium\\n+    const chromiumPaths = [\\n+      '/usr/bin/chromium',\\n+      '/usr/bin/chromium-browser',\\n+      '/usr/bin/google-chrome',\\n+      '/usr/bin/chrome',\\n+    ];\\n+    let chromiumPath: string | undefined;\\n+    for (const p of chromiumPaths) {\\n+      if (fs.existsSync(p)) {\\n+        chromiumPath = p;\\n+        break;\\n+      }\\n+    }\\n+\\n+    // Build environment with chromium path if found\\n+    const env = { ...process.env };\\n+    if (chromiumPath) {\\n+      env.PUPPETEER_EXECUTABLE_PATH = chromiumPath;\\n+    }\\n+\\n+    // Run mmdc to render PNG\\n+    const result = await new Promise<{ success: boolean; error?: string }>((resolve) => {\\n+      const proc = spawn('npx', [\\n+        '--yes',\\n+        '@mermaid-js/mermaid-cli',\\n+        '-i', inputFile,\\n+        '-o', outputFile,\\n+        '-e', 'png',\\n+        '-b', 'white',\\n+        '-w', '1200',\\n+      ], {\\n+        timeout: 60000, // 60 second timeout (first run may download packages)\\n+        stdio: ['pipe', 'pipe', 'pipe'],\\n+        env,\\n+      });\\n+\\n+      let stderr = '';\\n+      proc.stderr?.on('data', (data) => {\\n+        stderr += data.toString();\\n+      });\\n+\\n+      proc.on('close', (code) => {\\n+        if (code === 0) {\\n+          resolve({ success: true });\\n+        } else {\\n+          resolve({ success: false, error: stderr || `Exit code ${code}` });\\n+        }\\n+      });\\n+\\n+      proc.on('error', (err) => {\\n+        resolve({ success: false, error: err.message });\\n+      });\\n+    });\\n+\\n+    if (!result.success) {\\n+      console.warn(`Mermaid rendering failed: ${result.error}`);\\n+      return null;\\n+    }\\n+\\n+    // Read the output PNG\\n+    if (!fs.existsSync(outputFile)) {\\n+      console.warn('Mermaid output file not created');\\n+      return null;\\n+    }\\n+\\n+    const pngBuffer = fs.readFileSync(outputFile);\\n+    return pngBuffer;\\n+  } catch (e) {\\n+    console.warn(`Mermaid rendering error: ${e instanceof Error ? e.message : String(e)}`);\\n+    return null;\\n+  } finally {\\n+    // Cleanup temp files\\n+    try {\\n+      if (fs.existsSync(inputFile)) fs.unlinkSync(inputFile);\\n+      if (fs.existsSync(outputFile)) fs.unlinkSync(outputFile);\\n+    } catch {\\n+      // Ignore cleanup errors\\n+    }\\n+  }\\n+}\\n+\\n+/**\\n+ * Replace mermaid blocks in text with a placeholder message\\n+ * @param text Original text\\n+ * @param diagrams Extracted diagrams\\n+ * @param replacement Text to replace each diagram with (or a function that returns replacement for each index)\\n+ */\\n+export function replaceMermaidBlocks(\\n+  text: string,\\n+  diagrams: MermaidDiagram[],\\n+  replacement: string | ((index: number) => string) = '_(See diagram above)_'\\n+): string {\\n+  if (diagrams.length === 0) return text;\\n+\\n+  // Sort by start index descending to replace from end to start (preserves indices)\\n+  const sorted = [...diagrams].sort((a, b) => b.startIndex - a.startIndex);\\n+\\n+  let result = text;\\n+  sorted.forEach((diagram, sortedIndex) => {\\n+    // Calculate original index (since we sorted in reverse)\\n+    const originalIndex = diagrams.length - 1 - sortedIndex;\\n+    const rep = typeof replacement === 'function' ? replacement(originalIndex) : replacement;\\n+    result = result.slice(0, diagram.startIndex) + rep + result.slice(diagram.endIndex);\\n+  });\\n+\\n+  return result;\\n+}\\n+\\n export function markdownToSlack(text: string): string {\\n   if (!text || typeof text !== 'string') return '';\\n \\n@@ -33,7 +210,7 @@ export function markdownToSlack(text: string): string {\\n   out = out.replace(/\\\\*\\\\*([^*]+)\\\\*\\\\*/g, (_m, inner: string) => `*${inner}*`);\\n   out = out.replace(/__([^_]+)__/g, (_m, inner: string) => `*${inner}*`);\\n \\n-  // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation).\\n+  // Process lines for headers and bullet lists.\\n   // Slack's mrkdwn handles \\\"‚Ä¢\\\" bullets more naturally than raw \\\"-\\\" Markdown.\\n   const lines = out.split(/\\\\r?\\\\n/);\\n   let inCodeBlock = false;\\n@@ -46,9 +223,31 @@ export function markdownToSlack(text: string): string {\\n       continue;\\n     }\\n     if (inCodeBlock) continue;\\n-    const match = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n-    if (match) {\\n-      const [, indent, , rest] = match;\\n+\\n+    // Headers: # Header ‚Üí *Header* (Slack doesn't have native headers)\\n+    // Match 1-6 # at start of line, followed by space and text\\n+    const headerMatch = /^(#{1,6})\\\\s+(.+)$/.exec(trimmed);\\n+    if (headerMatch) {\\n+      const [, hashes, headerText] = headerMatch;\\n+      // For h1/h2, add extra emphasis with newline before (if not first line\\n+      // and previous line is not empty/header/code-fence)\\n+      const prevLine = i > 0 ? lines[i - 1].trim() : '';\\n+      const prevIsHeaderOrFence =\\n+        /^#{1,6}\\\\s+/.test(prevLine) ||\\n+        /^\\\\*[^*]+\\\\*$/.test(prevLine) ||\\n+        /^```/.test(prevLine);\\n+      if (hashes.length <= 2 && i > 0 && prevLine !== '' && !prevIsHeaderOrFence) {\\n+        lines[i] = `\\\\n*${headerText.trim()}*`;\\n+      } else {\\n+        lines[i] = `*${headerText.trim()}*`;\\n+      }\\n+      continue;\\n+    }\\n+\\n+    // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation)\\n+    const bulletMatch = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n+    if (bulletMatch) {\\n+      const [, indent, , rest] = bulletMatch;\\n       lines[i] = `${indent}‚Ä¢ ${rest}`;\\n     }\\n   }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/prompt-state.ts\",\"additions\":19,\"deletions\":2,\"changes\":21,\"patch\":\"diff --git a/src/slack/prompt-state.ts b/src/slack/prompt-state.ts\\nindex a13233b2..544777c6 100644\\n--- a/src/slack/prompt-state.ts\\n+++ b/src/slack/prompt-state.ts\\n@@ -95,7 +95,10 @@ export class PromptStateManager {\\n   setFirstMessage(channel: string, threadTs: string, text: string): void {\\n     const key = this.key(channel, threadTs);\\n     if (!text || !text.trim()) return;\\n-    if (!this.firstMessage.has(key)) {\\n+    const existing = this.firstMessage.get(key);\\n+    // Only set if: no entry exists OR the existing entry was already consumed\\n+    // This allows new messages to be captured after a resume cycle\\n+    if (!existing || existing.consumed) {\\n       this.firstMessage.set(key, { text, consumed: false });\\n     }\\n   }\\n@@ -130,6 +133,20 @@ export class PromptStateManager {\\n         removed++;\\n       }\\n     }\\n+    // Also clean up stale firstMessage entries (consumed entries older than TTL)\\n+    // Keep unconsumed entries to avoid losing user messages\\n+    for (const [key] of this.firstMessage.entries()) {\\n+      const waitingInfo = this.waiting.get(key);\\n+      // If no corresponding waiting entry exists and the firstMessage was consumed,\\n+      // the conversation is likely complete - safe to remove\\n+      if (!waitingInfo) {\\n+        const entry = this.firstMessage.get(key);\\n+        if (entry?.consumed) {\\n+          this.firstMessage.delete(key);\\n+          removed++;\\n+        }\\n+      }\\n+    }\\n     if (removed) {\\n       try {\\n         logger.info(`[prompt-state] cleanup removed ${removed} entries`);\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/socket-runner.ts\",\"additions\":56,\"deletions\":32,\"changes\":88,\"patch\":\"diff --git a/src/slack/socket-runner.ts b/src/slack/socket-runner.ts\\nindex d1681a03..5b461d22 100644\\n--- a/src/slack/socket-runner.ts\\n+++ b/src/slack/socket-runner.ts\\n@@ -7,6 +7,7 @@ import { SlackAdapter } from './adapter';\\n import { CachePrewarmer } from './cache-prewarmer';\\n import { RateLimiter, type RateLimitConfig } from './rate-limiter';\\n import type { SlackBotConfig } from '../types/bot';\\n+import { withActiveSpan } from '../telemetry/trace-helpers';\\n \\n type SlackSocketConfig = {\\n   appToken?: string; // xapp- token\\n@@ -30,6 +31,7 @@ export class SlackSocketRunner {\\n   private botUserId?: string;\\n   private processedKeys: Map<string, number> = new Map();\\n   private adapter?: SlackAdapter;\\n+  private retryCount = 0;\\n \\n   constructor(engine: StateMachineExecutionEngine, cfg: VisorConfig, opts: SlackSocketConfig) {\\n     const app = opts.appToken || process.env.SLACK_APP_TOKEN || '';\\n@@ -107,7 +109,10 @@ export class SlackSocketRunner {\\n \\n   private async connect(url: string): Promise<void> {\\n     this.ws = new WebSocket(url);\\n-    this.ws.on('open', () => logger.info('[SlackSocket] WebSocket connected'));\\n+    this.ws.on('open', () => {\\n+      this.retryCount = 0; // Reset on successful connection\\n+      logger.info('[SlackSocket] WebSocket connected');\\n+    });\\n     this.ws.on('close', (code, reason) => {\\n       logger.warn(`[SlackSocket] WebSocket closed: ${code} ${reason}`);\\n       setTimeout(() => this.restart().catch(() => {}), 1000);\\n@@ -123,7 +128,13 @@ export class SlackSocketRunner {\\n       const url = await this.openConnection();\\n       await this.connect(url);\\n     } catch (e) {\\n-      logger.error(`[SlackSocket] Restart failed: ${e instanceof Error ? e.message : e}`);\\n+      this.retryCount++;\\n+      // Exponential backoff: 2s, 4s, 8s, 16s, 32s, capped at 60s\\n+      const delay = Math.min(2000 * Math.pow(2, this.retryCount - 1), 60000);\\n+      logger.error(\\n+        `[SlackSocket] Restart failed (attempt ${this.retryCount}), retrying in ${Math.round(delay / 1000)}s: ${e instanceof Error ? e.message : e}`\\n+      );\\n+      setTimeout(() => this.restart().catch(() => {}), delay);\\n     }\\n   }\\n \\n@@ -296,36 +307,49 @@ export class SlackSocketRunner {\\n           });\\n         }\\n       } catch {}\\n-      if (path) {\\n-        try {\\n-          const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n-          const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n-          logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n-          await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n-            webhookContext: { webhookData: map, eventType: 'manual' },\\n-            debug: process.env.VISOR_DEBUG === 'true',\\n-          });\\n-          if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n-          return; // resume path handled\\n-        } catch (e) {\\n-          logger.warn(\\n-            `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n-              e instanceof Error ? e.message : String(e)\\n-            }`\\n-          );\\n-        }\\n-      }\\n+      try {\\n+        await withActiveSpan(\\n+          'visor.run',\\n+          {\\n+            'visor.run.source': 'slack',\\n+            'slack.event.type': String(type || ''),\\n+            'slack.channel': channelId,\\n+            'slack.thread_ts': threadTs,\\n+          },\\n+          async () => {\\n+            if (path) {\\n+              try {\\n+                const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n+                const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n+                logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n+                await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n+                  webhookContext: { webhookData: map, eventType: 'manual' },\\n+                  debug: process.env.VISOR_DEBUG === 'true',\\n+                });\\n+                return;\\n+              } catch (e) {\\n+                logger.warn(\\n+                  `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n+                    e instanceof Error ? e.message : String(e)\\n+                  }`\\n+                );\\n+              }\\n+            }\\n \\n-      // Cold run (no snapshot)\\n-      await runEngine.executeChecks({\\n-        checks: allChecks,\\n-        showDetails: true,\\n-        outputFormat: 'json',\\n-        config: cfgForRun,\\n-        webhookContext: { webhookData: map, eventType: 'manual' },\\n-        debug: process.env.VISOR_DEBUG === 'true',\\n-      } as any);\\n-      if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+            // Cold run (no snapshot)\\n+            await runEngine.executeChecks({\\n+              checks: allChecks,\\n+              showDetails: true,\\n+              outputFormat: 'json',\\n+              config: cfgForRun,\\n+              webhookContext: { webhookData: map, eventType: 'manual' },\\n+              debug: process.env.VISOR_DEBUG === 'true',\\n+            } as any);\\n+          }\\n+        );\\n+      } finally {\\n+        if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+      }\\n     } catch (e) {\\n       logger.error(\\n         `[SlackSocket] Engine execution failed: ${e instanceof Error ? e.message : String(e)}`\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/context/build-engine-context.ts\",\"additions\":11,\"deletions\":4,\"changes\":15,\"patch\":\"diff --git a/src/state-machine/context/build-engine-context.ts b/src/state-machine/context/build-engine-context.ts\\nindex 149c73bf..45e63b6a 100644\\n--- a/src/state-machine/context/build-engine-context.ts\\n+++ b/src/state-machine/context/build-engine-context.ts\\n@@ -57,7 +57,8 @@ export function buildEngineContextForRun(\\n       ) as EventTrigger[],\\n       group: checkConfig.group,\\n       providerType: checkConfig.type || 'ai',\\n-      dependencies: checkConfig.depends_on || [],\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: Array.isArray(checkConfig.depends_on) ? checkConfig.depends_on : checkConfig.depends_on ? [checkConfig.depends_on] : [],\\n     };\\n   }\\n \\n@@ -134,11 +135,14 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n   const originalPath = context.workingDirectory || process.cwd();\\n \\n   try {\\n+    // Check if workspace should be kept (for debugging)\\n+    const keepWorkspace = process.env.VISOR_KEEP_WORKSPACE === 'true';\\n+\\n     // Create workspace manager\\n     const workspace = WorkspaceManager.getInstance(context.sessionId, originalPath, {\\n       enabled: true,\\n-      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH,\\n-      cleanupOnExit: workspaceConfig?.cleanup_on_exit !== false,\\n+      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH || '/tmp/visor-workspaces',\\n+      cleanupOnExit: keepWorkspace ? false : workspaceConfig?.cleanup_on_exit !== false,\\n     });\\n \\n     // Initialize workspace (creates main project worktree)\\n@@ -151,6 +155,9 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n \\n     logger.info(`[Workspace] Initialized workspace: ${info.workspacePath}`);\\n     logger.debug(`[Workspace] Main project at: ${info.mainProjectPath}`);\\n+    if (keepWorkspace) {\\n+      logger.info(`[Workspace] Keeping workspace after execution (--keep-workspace)`);\\n+    }\\n \\n     return context;\\n   } catch (error) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/execution-invoker.ts\",\"additions\":84,\"deletions\":4,\"changes\":88,\"patch\":\"diff --git a/src/state-machine/dispatch/execution-invoker.ts b/src/state-machine/dispatch/execution-invoker.ts\\nindex eb7587d3..798ca8e4 100644\\n--- a/src/state-machine/dispatch/execution-invoker.ts\\n+++ b/src/state-machine/dispatch/execution-invoker.ts\\n@@ -356,10 +356,17 @@ export async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -413,6 +420,31 @@ export async function executeSingleCheck(\\n     }\\n   }\\n \\n+  // Banner-style log when a check actually starts executing. This is emitted\\n+  // after all gating (if/depends_on) has passed so it only appears for real\\n+  // runs, and gives a clear visual separator in the logs between checks.\\n+  try {\\n+    const wave = state.wave;\\n+    const level = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${level}) ‚îÅ‚îÅ‚îÅ`;\\n+\\n+    // When running in a TTY, colour the entire banner line for extra\\n+    // visibility; keep plain text for JSON/SARIF or non-TTY environments.\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   let forEachParent: string | undefined;\\n   let forEachItems: unknown[] | undefined;\\n   for (const depId of depList) {\\n@@ -563,13 +595,56 @@ export async function executeSingleCheck(\\n       files: [],\\n       commits: [],\\n     };\\n+    // Derive AI session reuse context for this check (self-mode only for now).\\n+    // When reuse_ai_session: 'self' is configured, look up the last root-scope\\n+    // journal entry for this check in the current engine session and expose its\\n+    // sessionId via ExecutionContext so ai-check-provider can call\\n+    // executeReviewWithSessionReuse() against the same ProbeAgent session.\\n+    let parentSessionId: string | undefined;\\n+    let reuseSession = false;\\n+    try {\\n+      const reuseCfg: unknown = (checkConfig as any).reuse_ai_session;\\n+      if (reuseCfg === 'self') {\\n+        const snapshotId = context.journal.beginSnapshot();\\n+        const visible = context.journal.readVisible(\\n+          context.sessionId,\\n+          snapshotId,\\n+          context.event as any\\n+        );\\n+        // Prefer the most recent root-scope result for this check\\n+        const prior = visible.filter(\\n+          e => e.checkId === checkId && (!e.scope || e.scope.length === 0)\\n+        );\\n+        if (prior.length > 0) {\\n+          const last = prior[prior.length - 1];\\n+          const sess = (last.result as any)?.sessionId;\\n+          if (typeof sess === 'string' && sess.length > 0) {\\n+            parentSessionId = sess;\\n+            reuseSession = true;\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only ‚Äì fall back to normal (non-reuse) execution on error.\\n+      parentSessionId = undefined;\\n+      reuseSession = false;\\n+    }\\n+\\n     const executionContext = {\\n       ...context.executionContext,\\n       _engineMode: context.mode,\\n       _parentContext: context,\\n       _parentState: state,\\n+      // Explicitly propagate workspace reference for nested workflows\\n+      workspace: context.workspace,\\n     };\\n \\n+    // Attach session reuse hints for providers that support them (AI, Claude Code, etc).\\n+    if (reuseSession && parentSessionId) {\\n+      (executionContext as any).parentSessionId = parentSessionId;\\n+      (executionContext as any).reuseSession = true;\\n+    }\\n+\\n     // Handle on_init lifecycle hook BEFORE main execution\\n     if (checkConfig.on_init) {\\n       try {\\n@@ -613,7 +688,12 @@ export async function executeSingleCheck(\\n \\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/foreach-processor.ts\",\"additions\":27,\"deletions\":2,\"changes\":29,\"patch\":\"diff --git a/src/state-machine/dispatch/foreach-processor.ts b/src/state-machine/dispatch/foreach-processor.ts\\nindex 336bf9df..ab630986 100644\\n--- a/src/state-machine/dispatch/foreach-processor.ts\\n+++ b/src/state-machine/dispatch/foreach-processor.ts\\n@@ -77,6 +77,26 @@ export async function executeCheckWithForEachItems(\\n   const allContents: string[] = [];\\n   const perIterationDurations: number[] = [];\\n \\n+  // Emit a banner for the forEach parent so logs clearly show when we are\\n+  // entering the aggregated execution for that step.\\n+  try {\\n+    const wave = state.wave;\\n+    const lvl = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${lvl}, forEach parent) ‚îÅ‚îÅ‚îÅ`;\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   for (let itemIndex = 0; itemIndex < forEachItems.length; itemIndex++) {\\n     const iterationStartMs = Date.now();\\n     const scope: Array<{ check: string; index: number }> = [\\n@@ -248,7 +268,12 @@ export async function executeCheckWithForEachItems(\\n \\n       const result = await withActiveSpan(\\n         `visor.check.${checkId}`,\\n-        { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+        {\\n+          'visor.check.id': checkId,\\n+          'visor.check.type': providerType,\\n+          session_id: context.sessionId,\\n+          wave: state.wave,\\n+        },\\n         async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n       );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/stats-manager.ts\",\"additions\":21,\"deletions\":1,\"changes\":22,\"patch\":\"diff --git a/src/state-machine/dispatch/stats-manager.ts b/src/state-machine/dispatch/stats-manager.ts\\nindex 8d04436e..2f317b59 100644\\n--- a/src/state-machine/dispatch/stats-manager.ts\\n+++ b/src/state-machine/dispatch/stats-manager.ts\\n@@ -44,6 +44,26 @@ export function updateStats(\\n       issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n     };\\n \\n+    // Respect explicit skip markers on the result (set by LevelDispatch).\\n+    const skippedMarker = (result as any).__skipped;\\n+    if (skippedMarker) {\\n+      stats.skipped = true;\\n+      stats.skipReason =\\n+        typeof skippedMarker === 'string'\\n+          ? (skippedMarker as any)\\n+          : (stats.skipReason as any) || 'if_condition';\\n+      // A skipped result should not contribute to run counters; reset\\n+      // them explicitly to guard against any prior values lingering\\n+      // from earlier runs of the same check.\\n+      stats.totalRuns = 0;\\n+      stats.successfulRuns = 0;\\n+      stats.failedRuns = 0;\\n+      stats.skippedRuns++;\\n+      // Do not count this as a run; keep totalRuns/failed/success unchanged.\\n+      state.stats.set(checkId, stats);\\n+      continue;\\n+    }\\n+\\n     // If this check was previously marked as skipped and now executed,\\n     // clear the skipped flag and any skip counters to ensure the run is visible.\\n     if (stats.skipped) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/runner.ts\",\"additions\":10,\"deletions\":7,\"changes\":17,\"patch\":\"diff --git a/src/state-machine/runner.ts b/src/state-machine/runner.ts\\nindex ee86b876..a13a5efd 100644\\n--- a/src/state-machine/runner.ts\\n+++ b/src/state-machine/runner.ts\\n@@ -124,14 +124,17 @@ export class StateMachineRunner {\\n    */\\n   private async executeState(state: EngineState): Promise<void> {\\n     // M4: Wrap state execution in OTEL span\\n+    const attrs: Record<string, unknown> = {\\n+      state: state,\\n+      engine_mode: this.context.mode,\\n+      wave: this.state.wave,\\n+      session_id: this.context.sessionId,\\n+    };\\n+    const waveKind = (this.state as any)?.flags?.waveKind;\\n+    if (waveKind) attrs.wave_kind = waveKind;\\n     return withActiveSpan(\\n       `engine.state.${state.toLowerCase()}`,\\n-      {\\n-        state: state,\\n-        engine_mode: this.context.mode,\\n-        wave: this.state.wave,\\n-        session_id: this.context.sessionId,\\n-      },\\n+      attrs,\\n       async () => {\\n         try {\\n           switch (state) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/level-dispatch.ts\",\"additions\":152,\"deletions\":15,\"changes\":167,\"patch\":\"diff --git a/src/state-machine/states/level-dispatch.ts b/src/state-machine/states/level-dispatch.ts\\nindex 5588b0f5..95392197 100644\\n--- a/src/state-machine/states/level-dispatch.ts\\n+++ b/src/state-machine/states/level-dispatch.ts\\n@@ -26,7 +26,7 @@ import type { CheckExecutionStats } from '../../types/execution';\\n import type { CheckProviderConfig } from '../../providers/check-provider.interface';\\n import type { CheckConfig } from '../../types/config';\\n import { handleRouting, checkLoopBudget } from './routing';\\n-import { withActiveSpan } from '../../telemetry/trace-helpers';\\n+import { withActiveSpan, setSpanAttributes, addEvent } from '../../telemetry/trace-helpers';\\n import { emitMermaidFromMarkdown } from '../../utils/mermaid-telemetry';\\n import { emitNdjsonSpanWithEvents, emitNdjsonFallback } from '../../telemetry/fallback-ndjson';\\n import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n@@ -46,6 +46,32 @@ function mapCheckNameToFocus(checkName: string): string {\\n   return focusMap[checkName] || 'all';\\n }\\n \\n+function formatScopeLabel(scope: Array<{ check: string; index: number }> | undefined): string {\\n+  if (!scope || scope.length === 0) return '';\\n+  return scope.map(item => `${item.check}:${item.index}`).join('|');\\n+}\\n+\\n+function recordOnFinishRoutingEvent(args: {\\n+  checkId: string;\\n+  action: 'run' | 'goto';\\n+  target: string;\\n+  source: 'run' | 'goto' | 'goto_js' | 'transitions';\\n+  scope?: Array<{ check: string; index: number }>;\\n+  gotoEvent?: string;\\n+}): void {\\n+  const attrs: Record<string, unknown> = {\\n+    check_id: args.checkId,\\n+    trigger: 'on_finish',\\n+    action: args.action,\\n+    target: args.target,\\n+    source: args.source,\\n+  };\\n+  const scopeLabel = formatScopeLabel(args.scope);\\n+  if (scopeLabel) attrs.scope = scopeLabel;\\n+  if (args.gotoEvent) attrs.goto_event = args.gotoEvent;\\n+  addEvent('visor.routing', attrs);\\n+}\\n+\\n /**\\n  * Build output history Map from journal for template rendering\\n  * This matches the format expected by AI providers\\n@@ -121,12 +147,13 @@ async function evaluateIfCondition(\\n         return false;\\n       }\\n     })();\\n+    // Steps with dependencies should always see outputs from all completed steps.\\n     // In forward-run waves (from on_success/on_fail goto), guards should see the\\n     // latest global outputs even if the check has no explicit dependencies.\\n     // In wave-retry (from on_finish), restrict to checks with dependencies to\\n     // avoid wrongly skipping top-level prompts like 'ask'.\\n     const useGlobalOutputs =\\n-      (useGlobalOutputsFlag && waveKind === 'forward') || (useGlobalOutputsFlag && hasDeps);\\n+      hasDeps || (useGlobalOutputsFlag && waveKind === 'forward');\\n \\n     if (useGlobalOutputs) {\\n       // Forward-run wave: allow guards to consult latest outputs from the entire\\n@@ -199,6 +226,7 @@ async function evaluateIfCondition(\\n       baseBranch: (context.prInfo as any)?.baseBranch,\\n       filesChanged: context.prInfo?.files?.map(f => f.filename),\\n       environment: envSnapshot,\\n+      workflowInputs: (context.config as any).workflow_inputs || {},\\n     };\\n \\n     const shouldRun = await evaluator.evaluateIfCondition(checkId, ifExpression, contextData);\\n@@ -238,6 +266,11 @@ export async function handleLevelDispatch(\\n   // Update current level tracking\\n   state.currentLevel = level.level;\\n   state.currentLevelChecks = new Set(level.parallel);\\n+  const levelChecksPreview = level.parallel.slice(0, 5).join(',');\\n+  setSpanAttributes({\\n+    level_size: level.parallel.length,\\n+    level_checks_preview: levelChecksPreview,\\n+  });\\n \\n   // Emit level ready event\\n   emitEvent({ type: 'LevelReady', level, wave: state.wave });\\n@@ -689,6 +722,39 @@ async function executeCheckWithForEachItems(\\n         }\\n       } catch {}\\n \\n+      // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+      // The socket-runner stores conversation data in webhookData under the endpoint key\\n+      try {\\n+        const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+        const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+        if (context.debug) {\\n+          logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+        }\\n+        if (webhookData && webhookData.size > 0) {\\n+          // Find the payload with slack_conversation\\n+          for (const payload of webhookData.values()) {\\n+            const slackConv = (payload as any)?.slack_conversation;\\n+            if (slackConv) {\\n+              // Build slack context with event and conversation\\n+              const event = (payload as any)?.event;\\n+              const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+              if (context.debug) {\\n+                logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+              }\\n+              (providerConfig as any).eventContext = {\\n+                ...(providerConfig as any).eventContext,\\n+                slack: {\\n+                  event: event || {},\\n+                  conversation: slackConv,\\n+                },\\n+                conversation: slackConv, // Also expose at top level for convenience\\n+              };\\n+              break;\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n+\\n       // Build dependency results with scope\\n       const dependencyResults = buildDependencyResultsWithScope(\\n         checkId,\\n@@ -820,15 +886,17 @@ async function executeCheckWithForEachItems(\\n       } catch {}\\n \\n       // Execute provider with telemetry\\n-      const itemResult = await withActiveSpan(\\n-        `visor.check.${checkId}`,\\n-        {\\n-          'visor.check.id': checkId,\\n-          'visor.check.type': providerType,\\n-          'visor.foreach.index': itemIndex,\\n-        },\\n-        async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n-      );\\n+    const itemResult = await withActiveSpan(\\n+      `visor.check.${checkId}`,\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        'visor.foreach.index': itemIndex,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n+      async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n+    );\\n \\n       // Enrich issues\\n       const enrichedIssues = (itemResult.issues || []).map((issue: ReviewIssue) => ({\\n@@ -942,6 +1010,7 @@ async function executeCheckWithForEachItems(\\n             const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n               previousResults: dependencyResults as any,\\n               event: context.event || 'manual',\\n+              output: output, // Pass the iteration output for guarantee evaluation\\n             } as any);\\n             if (!holds) {\\n               const issue: ReviewIssue = {\\n@@ -1311,6 +1380,13 @@ async function executeCheckWithForEachItems(\\n             // Increment loop count\\n             state.routingLoopCount++;\\n \\n+            recordOnFinishRoutingEvent({\\n+              checkId: forEachParent,\\n+              action: 'run',\\n+              target: targetCheck,\\n+              source: 'run',\\n+              scope: [],\\n+            });\\n             emitEvent({\\n               type: 'ForwardRunRequested',\\n               target: targetCheck,\\n@@ -1358,6 +1434,14 @@ async function executeCheckWithForEachItems(\\n                 return aggregatedResult; // abort further routing\\n               }\\n               state.routingLoopCount++;\\n+              recordOnFinishRoutingEvent({\\n+                checkId: forEachParent,\\n+                action: 'goto',\\n+                target: transTarget.to,\\n+                source: 'transitions',\\n+                scope: [],\\n+                gotoEvent: (transTarget as any).goto_event,\\n+              });\\n               emitEvent({\\n                 type: 'ForwardRunRequested',\\n                 target: transTarget.to,\\n@@ -1464,6 +1548,13 @@ async function executeCheckWithForEachItems(\\n           // Increment loop count\\n           state.routingLoopCount++;\\n \\n+          recordOnFinishRoutingEvent({\\n+            checkId: forEachParent,\\n+            action: 'goto',\\n+            target: gotoTarget,\\n+            source: onFinish.goto_js ? 'goto_js' : 'goto',\\n+            scope: [],\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: gotoTarget,\\n@@ -1618,10 +1709,17 @@ async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -1923,6 +2021,39 @@ async function executeSingleCheck(\\n       }\\n     } catch {}\\n \\n+    // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+    // The socket-runner stores conversation data in webhookData under the endpoint key\\n+    try {\\n+      const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+      const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+      if (context.debug) {\\n+        logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+      }\\n+      if (webhookData && webhookData.size > 0) {\\n+        // Find the payload with slack_conversation\\n+        for (const payload of webhookData.values()) {\\n+          const slackConv = (payload as any)?.slack_conversation;\\n+          if (slackConv) {\\n+            // Build slack context with event and conversation\\n+            const event = (payload as any)?.event;\\n+            const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+            if (context.debug) {\\n+              logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+            }\\n+            (providerConfig as any).eventContext = {\\n+              ...(providerConfig as any).eventContext,\\n+              slack: {\\n+                event: event || {},\\n+                conversation: slackConv,\\n+              },\\n+              conversation: slackConv, // Also expose at top level for convenience\\n+            };\\n+            break;\\n+          }\\n+        }\\n+      }\\n+    } catch {}\\n+\\n     // Build dependency results\\n     const dependencyResults = buildDependencyResults(checkId, checkConfig, context, state);\\n \\n@@ -2016,7 +2147,12 @@ async function executeSingleCheck(\\n     // Execute provider with telemetry\\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n@@ -2098,6 +2234,7 @@ async function executeSingleCheck(\\n           const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n             previousResults: dependencyResults as any,\\n             event: context.event || 'manual',\\n+            output: enrichedResult.output,\\n           } as any);\\n           if (!holds) {\\n             const issue: ReviewIssue = {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/routing.ts\",\"additions\":165,\"deletions\":1,\"changes\":166,\"patch\":\"diff --git a/src/state-machine/states/routing.ts b/src/state-machine/states/routing.ts\\nindex 5752a5bb..b1fd8654 100644\\n--- a/src/state-machine/states/routing.ts\\n+++ b/src/state-machine/states/routing.ts\\n@@ -15,6 +15,7 @@ import type { EngineContext, RunState, EngineState, EngineEvent } from '../../ty\\n import type { ReviewSummary, ReviewIssue } from '../../reviewer';\\n import type { CheckConfig, OnFailConfig, TransitionRule } from '../../types/config';\\n import { logger } from '../../logger';\\n+import { addEvent } from '../../telemetry/trace-helpers';\\n import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n import { createSecureSandbox, compileAndRun } from '../../utils/sandbox';\\n import { MemoryStore } from '../../memory-store';\\n@@ -153,6 +154,37 @@ function createMemoryHelpers() {\\n   };\\n }\\n \\n+type RoutingTrigger = 'on_success' | 'on_fail' | 'on_finish';\\n+type RoutingAction = 'run' | 'goto' | 'retry';\\n+type RoutingSource = 'run' | 'run_js' | 'goto' | 'goto_js' | 'transitions' | 'retry';\\n+\\n+function formatScopeLabel(scope: Array<{ check: string; index: number }> | undefined): string {\\n+  if (!scope || scope.length === 0) return '';\\n+  return scope.map(item => `${item.check}:${item.index}`).join('|');\\n+}\\n+\\n+function recordRoutingEvent(args: {\\n+  checkId: string;\\n+  trigger: RoutingTrigger;\\n+  action: RoutingAction;\\n+  target?: string;\\n+  source?: RoutingSource;\\n+  scope?: Array<{ check: string; index: number }>;\\n+  gotoEvent?: string;\\n+}): void {\\n+  const attrs: Record<string, unknown> = {\\n+    check_id: args.checkId,\\n+    trigger: args.trigger,\\n+    action: args.action,\\n+  };\\n+  if (args.target) attrs.target = args.target;\\n+  if (args.source) attrs.source = args.source;\\n+  const scopeLabel = formatScopeLabel(args.scope);\\n+  if (scopeLabel) attrs.scope = scopeLabel;\\n+  if (args.gotoEvent) attrs.goto_event = args.gotoEvent;\\n+  addEvent('visor.routing', attrs);\\n+}\\n+\\n /**\\n  * Handle routing state - evaluate conditions and decide next actions\\n  */\\n@@ -271,6 +303,14 @@ async function processOnFinish(\\n             { check: checkId, index: itemIndex },\\n           ];\\n \\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_finish',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -284,6 +324,14 @@ async function processOnFinish(\\n         // Increment loop count\\n         state.routingLoopCount++;\\n \\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_finish',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope: [],\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -328,6 +376,14 @@ async function processOnFinish(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_finish',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -362,6 +418,15 @@ async function processOnFinish(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_finish',\\n+        action: 'goto',\\n+        target: finishTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: finishTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: finishTransTarget.to,\\n@@ -406,6 +471,14 @@ async function processOnFinish(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_finish',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onFinish.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+    });\\n     // Enqueue forward run event\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n@@ -640,6 +713,14 @@ async function processOnSuccess(\\n             { check: checkId, index: itemIndex },\\n           ];\\n \\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_success',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -652,6 +733,14 @@ async function processOnSuccess(\\n         // Increment loop count\\n         state.routingLoopCount++;\\n \\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_success',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope,\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -695,6 +784,14 @@ async function processOnSuccess(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_success',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -728,6 +825,15 @@ async function processOnSuccess(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_success',\\n+        action: 'goto',\\n+        target: successTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: successTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: successTransTarget.to,\\n@@ -773,6 +879,15 @@ async function processOnSuccess(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_success',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onSuccess.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+      gotoEvent: onSuccess.goto_event,\\n+    });\\n     // Enqueue forward run event with optional event override\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n@@ -864,6 +979,14 @@ async function processOnFail(\\n           const itemScope: Array<{ check: string; index: number }> = [\\n             { check: checkId, index: itemIndex },\\n           ];\\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_fail',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -874,6 +997,14 @@ async function processOnFail(\\n       } else {\\n         // No forEach context: preserve current scope (if any)\\n         state.routingLoopCount++;\\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_fail',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope,\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -917,6 +1048,14 @@ async function processOnFail(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_fail',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -958,6 +1097,13 @@ async function processOnFail(\\n \\n         // Increment loop count and schedule forward run for the same check\\n         state.routingLoopCount++;\\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_fail',\\n+          action: 'retry',\\n+          source: 'retry',\\n+          scope: sc || [],\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: checkId,\\n@@ -1013,6 +1159,15 @@ async function processOnFail(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_fail',\\n+        action: 'goto',\\n+        target: failTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: failTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: failTransTarget.to,\\n@@ -1058,6 +1213,15 @@ async function processOnFail(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_fail',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onFail.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+      gotoEvent: onFail.goto_event,\\n+    });\\n     // Enqueue forward run event with optional event override\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/wave-planning.ts\",\"additions\":10,\"deletions\":3,\"changes\":13,\"patch\":\"diff --git a/src/state-machine/states/wave-planning.ts b/src/state-machine/states/wave-planning.ts\\nindex 7551d55c..7b42f043 100644\\n--- a/src/state-machine/states/wave-planning.ts\\n+++ b/src/state-machine/states/wave-planning.ts\\n@@ -351,8 +351,11 @@ export async function handleWavePlanning(\\n     for (const id of checksToRun) {\\n       // Only include dependencies that are within the same subset (often none).\\n       const cfg = context.config.checks?.[id];\\n-      const deps = (cfg?.depends_on || []).filter((d: string) => checksToRun.includes(d));\\n-      subDeps[id] = deps as string[];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = cfg?.depends_on;\\n+      const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n+      const deps = depsArray.filter((d: string) => checksToRun.includes(d));\\n+      subDeps[id] = deps;\\n     }\\n \\n     const subGraph = DependencyResolver.buildDependencyGraph(subDeps);\\n@@ -410,6 +413,10 @@ export async function handleWavePlanning(\\n     // Initialize current wave state\\n     (state as any).currentWaveCompletions = new Set<string>();\\n     (state as any).failedChecks = new Set<string>();\\n+    try {\\n+      (state as any).flags = (state as any).flags || {};\\n+      (state as any).flags.waveKind = 'initial';\\n+    } catch {}\\n   }\\n \\n   // Check if there are levels to execute\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/workflow-projection.ts\",\"additions\":3,\"deletions\":2,\"changes\":5,\"patch\":\"diff --git a/src/state-machine/workflow-projection.ts b/src/state-machine/workflow-projection.ts\\nindex b2d5eb24..2bcfd858 100644\\n--- a/src/state-machine/workflow-projection.ts\\n+++ b/src/state-machine/workflow-projection.ts\\n@@ -53,7 +53,8 @@ export function projectWorkflowToGraph(\\n       triggers: step.on || workflow.on || [],\\n       group: step.group,\\n       providerType: step.type || 'ai',\\n-      dependencies: (step.depends_on || []).map(dep => dep),\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: (Array.isArray(step.depends_on) ? step.depends_on : step.depends_on ? [step.depends_on] : []).map((dep: string) => dep),\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/flow-stage.ts\",\"additions\":37,\"deletions\":4,\"changes\":41,\"patch\":\"diff --git a/src/test-runner/core/flow-stage.ts b/src/test-runner/core/flow-stage.ts\\nindex a14256c2..a587f9f1 100644\\n--- a/src/test-runner/core/flow-stage.ts\\n+++ b/src/test-runner/core/flow-stage.ts\\n@@ -352,10 +352,30 @@ export class FlowStage {\\n       } catch {}\\n       // Prefer authoritative counts from res.statistics when available, then fall back to deltas/inferred\\n       const fromResStats: Record<string, number> = {};\\n+      const resSkipped: Record<string, boolean> = {};\\n       try {\\n         for (const s of (res.statistics?.checks || []) as any[]) {\\n           const n = (s && s.checkName) || '';\\n-          if (typeof n === 'string' && n) fromResStats[n] = (s.totalRuns || 0) as number;\\n+          if (typeof n === 'string' && n) {\\n+            fromResStats[n] = (s.totalRuns || 0) as number;\\n+            resSkipped[n] = !!(s as any).skipped || !!(s as any).skipReason;\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      // Steps that have explicit expectations in this stage. We will use\\n+      // this to avoid inflating run counts (via parent-alignment heuristics)\\n+      // for checks that are not even expected in this stage.\\n+      const expectedSteps = new Set<string>();\\n+      const expectedZero = new Set<string>();\\n+      try {\\n+        const expCalls = ((stage.expect || {}).calls || []) as Array<{ step?: string }>;\\n+        for (const c of expCalls) {\\n+          if (c && typeof c.step === 'string' && c.step) expectedSteps.add(c.step);\\n+          try {\\n+            if (c && typeof c.step === 'string' && c.step && (c as any).exactly === 0)\\n+              expectedZero.add(c.step);\\n+          } catch {}\\n         }\\n       } catch {}\\n \\n@@ -410,10 +430,15 @@ export class FlowStage {\\n           const base = stageOnly ? 0 : statBase[name] || 0;\\n           const d = Math.max(0, resTotal - base);\\n           runs = d > 0 ? d : 0;\\n+          // If the engine marked this check as skipped, force runs to 0\\n+          // so parent-alignment heuristics below do not inflate it.\\n+          if (resSkipped[name]) {\\n+            runs = 0;\\n+          }\\n         } else {\\n           runs = deltaMap[name] !== undefined ? deltaMap[name] : inferred;\\n         }\\n-        if (runs === 0 && presentInResults.has(name)) runs = 1;\\n+        if (runs === 0 && presentInResults.has(name) && !resSkipped[name]) runs = 1;\\n         if (!isForEachLike && histRuns > 0) runs = histRuns;\\n         // Only use per-item history counts for non-forEach checks. For forEach parents,\\n         // use aggregated totals from res.statistics to reflect number of parent executions.\\n@@ -442,7 +467,15 @@ export class FlowStage {\\n             parentMax = Math.max(parentMax, dP);\\n           }\\n           // Apply only for non-forEach checks with no observable history in this stage\\n-          if (!isForEachLike && histRuns === 0 && histPerItemRuns === 0 && parentMax > 0) {\\n+          if (\\n+            !isForEachLike &&\\n+            histRuns === 0 &&\\n+            histPerItemRuns === 0 &&\\n+            parentMax > 0 &&\\n+            !resSkipped[name] &&\\n+            (expectedSteps.size === 0 || expectedSteps.has(name)) &&\\n+            !expectedZero.has(name)\\n+          ) {\\n             runs = Math.max(runs, parentMax);\\n           }\\n         } catch {}\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/test-execution-wrapper.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/test-runner/core/test-execution-wrapper.ts b/src/test-runner/core/test-execution-wrapper.ts\\nindex 254d0970..d0333152 100644\\n--- a/src/test-runner/core/test-execution-wrapper.ts\\n+++ b/src/test-runner/core/test-execution-wrapper.ts\\n@@ -31,6 +31,8 @@ export class TestExecutionWrapper {\\n       const prev: any = (this.engine as any).executionContext || {};\\n       const merged = {\\n         ...prev,\\n+        // Inject workflow inputs for template access via {{ inputs.* }}\\n+        workflowInputs: cfg.workflow_inputs || prev.workflowInputs || {},\\n         mode: {\\n           ...(prev.mode || {}),\\n           test: true,\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/evaluators.ts\",\"additions\":67,\"deletions\":2,\"changes\":69,\"patch\":\"diff --git a/src/test-runner/evaluators.ts b/src/test-runner/evaluators.ts\\nindex 4843163a..75e9c174 100644\\n--- a/src/test-runner/evaluators.ts\\n+++ b/src/test-runner/evaluators.ts\\n@@ -252,6 +252,69 @@ export function evaluatePrompts(\\n   }\\n }\\n \\n+/**\\n+ * Evaluate workflow_output assertions against computed workflow outputs.\\n+ * Similar to evaluateOutputs but tests workflow-level outputs (defined in outputs: section)\\n+ * rather than step outputs.\\n+ */\\n+export function evaluateWorkflowOutputs(\\n+  errors: string[],\\n+  expect: ExpectBlock,\\n+  workflowOutputs: Record<string, unknown> | undefined\\n+): void {\\n+  const expectations = (expect as any).workflow_output;\\n+  if (!Array.isArray(expectations) || expectations.length === 0) return;\\n+  if (!workflowOutputs) {\\n+    errors.push('workflow_output assertions present but no workflow outputs computed');\\n+    return;\\n+  }\\n+\\n+  for (const o of expectations) {\\n+    const path = o.path as string;\\n+    if (!path) {\\n+      errors.push('workflow_output assertion missing path');\\n+      continue;\\n+    }\\n+    const v = deepGet(workflowOutputs, path);\\n+    if (o.equals !== undefined && !deepEqual(v, o.equals)) {\\n+      errors.push(\\n+        `Workflow output ${path} expected ${JSON.stringify(o.equals)} but got ${JSON.stringify(v)}`\\n+      );\\n+    }\\n+    if (o.equalsDeep !== undefined && !deepEqual(v, o.equalsDeep)) {\\n+      errors.push(`Workflow output ${path} deepEquals failed`);\\n+    }\\n+    if (o.matches && !parseRegex(o.matches).test(String(v))) {\\n+      errors.push(`Workflow output ${path} does not match ${o.matches}`);\\n+    }\\n+    if (o.contains) {\\n+      const contents = Array.isArray(o.contains) ? o.contains : [o.contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (!strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} expected to contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.not_contains) {\\n+      const contents = Array.isArray(o.not_contains) ? o.not_contains : [o.not_contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} should not contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.contains_unordered) {\\n+      if (!Array.isArray(v)) {\\n+        errors.push(`Workflow output ${path} not an array for contains_unordered`);\\n+      } else if (!containsUnordered(v as unknown[], o.contains_unordered)) {\\n+        errors.push(`Workflow output ${path} missing elements (unordered)`);\\n+      }\\n+    }\\n+  }\\n+}\\n+\\n export function evaluateOutputs(\\n   errors: string[],\\n   expect: ExpectBlock,\\n@@ -332,7 +395,8 @@ export function evaluateCase(\\n   strict: boolean,\\n   promptsByStep: Record<string, string[]>,\\n   _results: GroupedResults,\\n-  outputHistory: Record<string, unknown[]>\\n+  outputHistory: Record<string, unknown[]>,\\n+  workflowOutputs?: Record<string, unknown>\\n ): string[] {\\n   const errors: string[] = [];\\n   const executed = buildExecutedMap(stats);\\n@@ -350,5 +414,6 @@ export function evaluateCase(\\n   evaluateNoCalls(errors, expect, executed, recorder, slackRecorder);\\n   evaluatePrompts(errors, expect, promptsByStep);\\n   evaluateOutputs(errors, expect, outputHistory);\\n+  evaluateWorkflowOutputs(errors, expect, workflowOutputs);\\n   return errors;\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/index.ts\",\"additions\":112,\"deletions\":7,\"changes\":119,\"patch\":\"diff --git a/src/test-runner/index.ts b/src/test-runner/index.ts\\nindex 29916a16..4dcc3331 100644\\n--- a/src/test-runner/index.ts\\n+++ b/src/test-runner/index.ts\\n@@ -159,6 +159,7 @@ export async function runSuites(\\n   options: {\\n     only?: string;\\n     bail?: boolean;\\n+    noMocks?: boolean;\\n     maxParallelSuites?: number;\\n     maxParallel?: number;\\n     promptMaxChars?: number;\\n@@ -225,6 +226,7 @@ export async function runSuites(\\n       const r = await runner.runCases(fp, suite as TestSuite, {\\n         only: options.only,\\n         bail: options.bail,\\n+        noMocks: options.noMocks,\\n         maxParallel: options.maxParallel,\\n         promptMaxChars: options.promptMaxChars,\\n       });\\n@@ -285,7 +287,8 @@ export class VisorTestRunner {\\n     defaultPromptCap?: number,\\n     ghRec?: { error_code?: number; timeout_ms?: number },\\n     defaultIncludeTags?: string[] | undefined,\\n-    defaultExcludeTags?: string[] | undefined\\n+    defaultExcludeTags?: string[] | undefined,\\n+    noMocks?: boolean\\n   ): {\\n     name: string;\\n     strict: boolean;\\n@@ -399,9 +402,11 @@ export class VisorTestRunner {\\n               : info.prompt;\\n           prompts[k].push(p);\\n         },\\n-        mockForStep: (step: string) => mockMgr.get(step),\\n+        // In noMocks mode, always return undefined to let real providers execute\\n+        mockForStep: (step: string) => noMocks ? undefined : mockMgr.get(step),\\n         // Ensure human-input never blocks tests: prefer case mock, then default value\\n         onHumanInput: async (req: { checkId: string; default?: string }) => {\\n+          if (noMocks) return (req.default ?? '').toString();\\n           const m = mockMgr.get(req.checkId);\\n           if (m !== undefined && m !== null) return String(m);\\n           return (req.default ?? '').toString();\\n@@ -481,6 +486,60 @@ export class VisorTestRunner {\\n     if (!checks || checks.length === 0) return;\\n     console.log(`  checks: ${checks.join(', ')}`);\\n   }\\n+  /**\\n+   * Compute workflow outputs from output definitions and step results.\\n+   * Evaluates value_js and value (Liquid) expressions to produce output values.\\n+   */\\n+  private computeWorkflowOutputs(\\n+    outputDefs: Array<{ name: string; value?: string; value_js?: string }>,\\n+    outputHistory: Record<string, unknown[]>,\\n+    results: any[]\\n+  ): Record<string, unknown> {\\n+    const computed: Record<string, unknown> = {};\\n+    // Build a steps map from outputHistory (last output for each step)\\n+    const steps: Record<string, unknown> = {};\\n+    for (const [stepId, hist] of Object.entries(outputHistory)) {\\n+      if (Array.isArray(hist) && hist.length > 0) {\\n+        steps[stepId] = hist[hist.length - 1];\\n+      }\\n+    }\\n+    // Build outputs map from history (for {{ outputs[\\\"step\\\"] }} syntax)\\n+    const outputs: Record<string, unknown> = { ...steps };\\n+\\n+    for (const outputDef of outputDefs) {\\n+      if (!outputDef.name) continue;\\n+      try {\\n+        if (outputDef.value_js) {\\n+          // Evaluate JavaScript expression\\n+          const fn = new Function('steps', 'outputs', 'results', `\\n+            try {\\n+              ${outputDef.value_js}\\n+            } catch (e) {\\n+              return undefined;\\n+            }\\n+          `);\\n+          computed[outputDef.name] = fn(steps, outputs, results);\\n+        } else if (outputDef.value) {\\n+          // Evaluate Liquid template\\n+          const { Liquid } = require('liquidjs');\\n+          const engine = new Liquid();\\n+          const rendered = engine.parseAndRenderSync(outputDef.value, { steps, outputs, results });\\n+          // Try to parse as JSON, otherwise keep as string\\n+          try {\\n+            computed[outputDef.name] = JSON.parse(rendered);\\n+          } catch {\\n+            computed[outputDef.name] = rendered.trim();\\n+          }\\n+        }\\n+      } catch (e) {\\n+        // Skip outputs that fail to compute\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.log(`  ‚ö†Ô∏è Failed to compute output '${outputDef.name}': ${e}`);\\n+        }\\n+      }\\n+    }\\n+    return computed;\\n+  }\\n   /**\\n    * Locate a tests file: explicit path > ./.visor.tests.yaml > defaults/visor.tests.yaml\\n    */\\n@@ -606,7 +665,7 @@ export class VisorTestRunner {\\n   public async runCases(\\n     testsPath: string,\\n     suite: TestSuite,\\n-    options: { only?: string; bail?: boolean; maxParallel?: number; promptMaxChars?: number }\\n+    options: { only?: string; bail?: boolean; noMocks?: boolean; maxParallel?: number; promptMaxChars?: number }\\n   ): Promise<{\\n     failures: number;\\n     results: Array<{\\n@@ -748,9 +807,14 @@ export class VisorTestRunner {\\n     const __keepAlive = setInterval(() => {}, 1000);\\n     // Header: show suite path for clarity\\n     let __suiteRel = testsPath;\\n+    const noMocksMode = options.noMocks || false;\\n     try {\\n       __suiteRel = path.relative(this.cwd, testsPath) || testsPath;\\n       console.log(`Suite: ${__suiteRel}`);\\n+      if (noMocksMode) {\\n+        console.log(this.color('üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)', '33'));\\n+        console.log(this.gray('   Step outputs will be captured and printed as suggested mocks\\\\n'));\\n+      }\\n     } catch {}\\n     const runOne = async (_case: any): Promise<{ name: string; failed: number }> => {\\n       // Case header for clarity\\n@@ -805,6 +869,11 @@ export class VisorTestRunner {\\n           cfgLocal.checks[name] = chk;\\n         }\\n       }\\n+      // Workflow testing: inject workflow_input into config for template access\\n+      const workflowInput = (_case as any).workflow_input;\\n+      if (workflowInput && typeof workflowInput === 'object') {\\n+        (cfgLocal as any).workflow_inputs = workflowInput;\\n+      }\\n       const setup = this.setupTestCase(\\n         _case,\\n         cfgLocal,\\n@@ -812,7 +881,8 @@ export class VisorTestRunner {\\n         defaultPromptCap,\\n         ghRec,\\n         defaultIncludeTags,\\n-        defaultExcludeTags\\n+        defaultExcludeTags,\\n+        noMocksMode\\n       );\\n       try {\\n         this.printSelectedChecks(setup.checksToRun);\\n@@ -826,6 +896,24 @@ export class VisorTestRunner {\\n         } catch {}\\n         const exec = await this.executeTestCase(setup, cfgLocal);\\n         const res = exec.res;\\n+\\n+        // In no-mocks mode, print captured outputs as suggested mocks\\n+        if (noMocksMode && Object.keys(exec.outHistory).length > 0) {\\n+          console.log(this.color('\\\\nüìã Suggested mocks (copy to your test case):', '36'));\\n+          console.log(this.gray('mocks:'));\\n+          for (const [stepName, outputs] of Object.entries(exec.outHistory)) {\\n+            if (!Array.isArray(outputs) || outputs.length === 0) continue;\\n+            // Use the last output for the step\\n+            const lastOutput = outputs[outputs.length - 1];\\n+            // Format as YAML with proper indentation\\n+            const yamlOutput = yaml.dump({ [stepName]: lastOutput }, { indent: 2, lineWidth: 120, noRefs: true });\\n+            // Indent the YAML output for proper nesting under 'mocks:'\\n+            const indented = yamlOutput.split('\\\\n').map(line => '  ' + line).join('\\\\n');\\n+            console.log(indented);\\n+          }\\n+          console.log('');\\n+        }\\n+\\n         if (process.env.VISOR_DEBUG === 'true') {\\n           try {\\n             const names = (res.statistics.checks || []).map(\\n@@ -836,6 +924,21 @@ export class VisorTestRunner {\\n         }\\n         // avoid printing raw history keys each case\\n         // (fallback for on_finish static targets handled inside executeTestCase)\\n+        // Workflow testing: compute workflow outputs if workflow has outputs defined\\n+        let workflowOutputs: Record<string, unknown> | undefined;\\n+        if (Array.isArray((cfgLocal as any).outputs)) {\\n+          try {\\n+            workflowOutputs = this.computeWorkflowOutputs(\\n+              (cfgLocal as any).outputs,\\n+              exec.outHistory,\\n+              res.results\\n+            );\\n+          } catch (e) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ‚ö†Ô∏è Error computing workflow outputs: ${e}`);\\n+            }\\n+          }\\n+        }\\n         const caseFailures = require('./evaluators').evaluateCase(\\n           _case.name,\\n           res.statistics,\\n@@ -845,7 +948,8 @@ export class VisorTestRunner {\\n           setup.strict,\\n           setup.prompts,\\n           res.results,\\n-          exec.outHistory\\n+          exec.outHistory,\\n+          workflowOutputs\\n         );\\n         // Warn about unmocked AI/command steps that executed\\n         try {\\n@@ -1280,7 +1384,8 @@ export class VisorTestRunner {\\n   ): void {\\n     const executed: Record<string, number> = {};\\n     for (const s of stats.checks) {\\n-      if (!s.skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n+      const skipped = (s as any).skipped === true || !!(s as any).skipReason;\\n+      if (!skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n     }\\n     const expCalls = (expect.calls || []).filter(c => c.step);\\n     const expectedSteps = new Map<\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/validator.ts\",\"additions\":48,\"deletions\":2,\"changes\":50,\"patch\":\"diff --git a/src/test-runner/validator.ts b/src/test-runner/validator.ts\\nindex dbb890c5..5ffff5ee 100644\\n--- a/src/test-runner/validator.ts\\n+++ b/src/test-runner/validator.ts\\n@@ -25,6 +25,16 @@ const schema: any = {\\n     hooks: { type: 'object' },\\n     slack: { type: 'object' },\\n     frontends: { type: 'array' },\\n+    workspace: { type: 'object' },\\n+    // Workflow definition fields (for workflow files with co-located tests)\\n+    id: { type: 'string' },\\n+    name: { type: 'string' },\\n+    description: { type: 'string' },\\n+    inputs: { type: 'array' },\\n+    outputs: { type: 'array' },\\n+    ai_mcp_servers: { type: 'object' },\\n+    ai_provider: { type: 'string' },\\n+    ai_model: { type: 'string' },\\n     tests: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -136,6 +146,8 @@ const schema: any = {\\n             oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n           },\\n         },\\n+        // Workflow testing: input values to pass to the workflow\\n+        workflow_input: { type: 'object' },\\n         expect: { $ref: '#/$defs/expectBlock' },\\n         // Flow cases\\n         flow: {\\n@@ -270,6 +282,35 @@ const schema: any = {\\n       },\\n       required: ['step', 'path'],\\n     },\\n+    // Workflow output expectation: like outputsExpectation but without step (tests workflow-level outputs)\\n+    workflowOutputExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        path: { type: 'string' },\\n+        equals: {},\\n+        equalsDeep: {},\\n+        matches: { type: 'string' },\\n+        contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        not_contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            path: { type: 'string' },\\n+            equals: {},\\n+            matches: { type: 'string' },\\n+          },\\n+          required: ['path'],\\n+        },\\n+        contains_unordered: { type: 'array' },\\n+      },\\n+      required: ['path'],\\n+    },\\n     expectBlock: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -278,6 +319,8 @@ const schema: any = {\\n         calls: { type: 'array', items: { $ref: '#/$defs/callsExpectation' } },\\n         prompts: { type: 'array', items: { $ref: '#/$defs/promptsExpectation' } },\\n         outputs: { type: 'array', items: { $ref: '#/$defs/outputsExpectation' } },\\n+        // Workflow testing: assert on workflow-level outputs (defined in workflow's outputs: section)\\n+        workflow_output: { type: 'array', items: { $ref: '#/$defs/workflowOutputExpectation' } },\\n         no_calls: {\\n           type: 'array',\\n           items: {\\n@@ -343,6 +386,7 @@ const knownKeys = new Set([\\n   'version',\\n   'extends',\\n   'tests',\\n+  'workspace',\\n   // tests\\n   'tests.defaults',\\n   'tests.fixtures',\\n@@ -363,6 +407,7 @@ const knownKeys = new Set([\\n   'env',\\n   'routing',\\n   'mocks',\\n+  'workflow_input',\\n   'expect',\\n   'flow',\\n   // expect\\n@@ -370,6 +415,7 @@ const knownKeys = new Set([\\n   'expect.calls',\\n   'expect.prompts',\\n   'expect.outputs',\\n+  'expect.workflow_output',\\n   'expect.no_calls',\\n   'expect.fail',\\n   'expect.strict_violation',\\n@@ -424,7 +470,7 @@ function formatError(e: ErrorObject): string {\\n       if (hint) msg += ` (${hint})`;\\n       // Small curated allow-list for frequent nodes to reduce guesswork\\n       if (path.endsWith('expect')) {\\n-        msg += ` (allowed: use, calls, prompts, outputs, no_calls, fail, strict_violation)`;\\n+        msg += ` (allowed: use, calls, prompts, outputs, workflow_output, no_calls, fail, strict_violation)`;\\n       } else if (path.endsWith('env')) {\\n         msg += ` (values must be strings)`;\\n       } else if (path.endsWith('tests')) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/cli.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/cli.ts b/src/types/cli.ts\\nindex a7d9ca30..f06f93d5 100644\\n--- a/src/types/cli.ts\\n+++ b/src/types/cli.ts\\n@@ -61,6 +61,8 @@ export interface CliOptions {\\n   githubV2?: boolean;\\n   /** Enable Slack Socket Mode runner */\\n   slack?: boolean;\\n+  /** Keep workspace folders after execution (for debugging) */\\n+  keepWorkspace?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/config.ts\",\"additions\":114,\"deletions\":5,\"changes\":119,\"patch\":\"diff --git a/src/types/config.ts b/src/types/config.ts\\nindex ed239959..dd363de1 100644\\n--- a/src/types/config.ts\\n+++ b/src/types/config.ts\\n@@ -122,6 +122,9 @@ export interface FailureConditionContext {\\n   /** Environment variables */\\n   env?: Record<string, string>;\\n \\n+  /** Workflow inputs - values passed to the workflow */\\n+  inputs?: Record<string, unknown>;\\n+\\n   /** Memory accessor for accessing memory store in expressions */\\n   memory?: {\\n     get: (key: string, namespace?: string) => unknown;\\n@@ -356,6 +359,8 @@ export interface AIProviderConfig {\\n   allowBash?: boolean;\\n   /** Advanced bash command execution configuration */\\n   bashConfig?: BashConfig;\\n+  /** Completion prompt for post-completion validation/review (runs after attempt_completion) */\\n+  completion_prompt?: string;\\n }\\n \\n /**\\n@@ -461,8 +466,8 @@ export interface CheckConfig {\\n   env?: EnvConfig;\\n   /** Timeout in seconds for command execution (default: 60) */\\n   timeout?: number;\\n-  /** Check IDs that this check depends on (optional) */\\n-  depends_on?: string[];\\n+  /** Check IDs that this check depends on (optional). Accepts single string or array. */\\n+  depends_on?: string | string[];\\n   /** Group name for comment separation (e.g., \\\"code-review\\\", \\\"pr-overview\\\") - optional */\\n   group?: string;\\n   /** Schema type for template rendering (e.g., \\\"code-review\\\", \\\"markdown\\\") or inline JSON schema object - optional */\\n@@ -621,6 +626,46 @@ export interface CheckConfig {\\n   overrides?: Record<string, Partial<CheckConfig>>;\\n   /** Map workflow outputs to check outputs */\\n   output_mapping?: Record<string, string>;\\n+\\n+  // Workflow aliases for backward compatibility\\n+  /** Alias for args - workflow inputs (backward compatibility) */\\n+  workflow_inputs?: Record<string, unknown>;\\n+  /** Config file path - alternative to workflow ID (loads a Visor config file as workflow) */\\n+  config?: string;\\n+  /** Alias for overrides - workflow step overrides (backward compatibility) */\\n+  workflow_overrides?: Record<string, Partial<CheckConfig>>;\\n+\\n+  /**\\n+   * Git-checkout provider specific options (optional, only used when type === 'git-checkout').\\n+   */\\n+  /** Git reference to checkout (branch, tag, commit SHA) - supports templates */\\n+  ref?: string;\\n+  /** Repository URL or owner/repo format (defaults to current repository) */\\n+  repository?: string;\\n+  /** GitHub token for private repositories (defaults to GITHUB_TOKEN env) */\\n+  token?: string;\\n+  /** Number of commits to fetch (0 for full history, default: 1) */\\n+  fetch_depth?: number;\\n+  /** Whether to fetch tags (default: false) */\\n+  fetch_tags?: boolean;\\n+  /** Checkout submodules: false, true, or 'recursive' */\\n+  submodules?: boolean | 'recursive';\\n+  /** Working directory for the checkout (defaults to temp directory) */\\n+  working_directory?: string;\\n+  /** Use git worktree for efficient parallel checkouts (default: true) */\\n+  use_worktree?: boolean;\\n+  /** Clean the working directory before checkout (default: true) */\\n+  clean?: boolean;\\n+  /** Sparse checkout paths - only checkout specific directories/files */\\n+  sparse_checkout?: string[];\\n+  /** Enable Git LFS (Large File Storage) */\\n+  lfs?: boolean;\\n+  /** Timeout in ms for cloning the bare repository (default: 300000 = 5 min) */\\n+  clone_timeout_ms?: number;\\n+  /** Clean up worktree on failure (default: true) */\\n+  cleanup_on_failure?: boolean;\\n+  /** Keep worktree after workflow completion (default: false) */\\n+  persist_worktree?: boolean;\\n }\\n \\n /**\\n@@ -1037,6 +1082,36 @@ export interface CustomToolDefinition {\\n   outputSchema?: Record<string, unknown>;\\n }\\n \\n+/**\\n+ * Workflow input definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowInput {\\n+  /** Input parameter name */\\n+  name: string;\\n+  /** JSON Schema for the input */\\n+  schema?: Record<string, unknown>;\\n+  /** Whether this input is required */\\n+  required?: boolean;\\n+  /** Default value if not provided */\\n+  default?: unknown;\\n+  /** Human-readable description */\\n+  description?: string;\\n+}\\n+\\n+/**\\n+ * Workflow output definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowOutput {\\n+  /** Output name */\\n+  name: string;\\n+  /** Human-readable description */\\n+  description?: string;\\n+  /** Value using Liquid template syntax (references step outputs) */\\n+  value?: string;\\n+  /** Value using JavaScript expression (alternative to value) */\\n+  value_js?: string;\\n+}\\n+\\n /**\\n  * Main Visor configuration\\n  */\\n@@ -1051,12 +1126,16 @@ export interface VisorConfig {\\n   tools?: Record<string, CustomToolDefinition>;\\n   /** Import workflow definitions from external files or URLs */\\n   imports?: string[];\\n+  /** Workflow inputs (for standalone reusable workflows) */\\n+  inputs?: WorkflowInput[];\\n+  /** Workflow outputs (for standalone reusable workflows) */\\n+  outputs?: WorkflowOutput[];\\n   /** Step configurations (recommended) */\\n   steps?: Record<string, CheckConfig>;\\n   /** Check configurations (legacy, use 'steps' instead) - always populated after normalization */\\n   checks?: Record<string, CheckConfig>;\\n-  /** Output configuration */\\n-  output: OutputConfig;\\n+  /** Output configuration (optional - defaults provided) */\\n+  output?: OutputConfig;\\n   /** HTTP server configuration for receiving webhooks */\\n   http_server?: HttpServerConfig;\\n   /** Memory storage configuration */\\n@@ -1092,6 +1171,36 @@ export interface VisorConfig {\\n     /** Frontend-specific configuration */\\n     config?: unknown;\\n   }>;\\n+  /** Workspace isolation configuration for sandboxed execution */\\n+  workspace?: WorkspaceConfig;\\n+  /** Slack configuration */\\n+  slack?: SlackConfig;\\n+}\\n+\\n+/**\\n+ * Workspace isolation configuration\\n+ */\\n+export interface WorkspaceConfig {\\n+  /** Enable workspace isolation (default: true when config present) */\\n+  enabled?: boolean;\\n+  /** Base path for workspaces (default: /tmp/visor-workspaces) */\\n+  base_path?: string;\\n+  /** Clean up workspace on exit (default: true) */\\n+  cleanup_on_exit?: boolean;\\n+}\\n+\\n+/**\\n+ * Slack configuration\\n+ */\\n+export interface SlackConfig {\\n+  /** Slack API version */\\n+  version?: string;\\n+  /** Mention handling: 'all', 'direct', etc. */\\n+  mentions?: string;\\n+  /** Thread handling: 'required', 'optional', etc. */\\n+  threads?: string;\\n+  /** Show raw output in Slack responses */\\n+  show_raw_output?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/git-checkout.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/git-checkout.ts b/src/types/git-checkout.ts\\nindex eb1c5ebc..4d27855d 100644\\n--- a/src/types/git-checkout.ts\\n+++ b/src/types/git-checkout.ts\\n@@ -16,6 +16,8 @@ export interface GitCheckoutConfig {\\n   fetch_depth?: number;\\n   fetch_tags?: boolean;\\n   submodules?: boolean | 'recursive';\\n+  /** Timeout (ms) for cloning the bare repository; defaults to 300000 (5 minutes) */\\n+  clone_timeout_ms?: number;\\n \\n   // Worktree configuration\\n   working_directory?: string;\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/sandbox.ts\",\"additions\":46,\"deletions\":1,\"changes\":47,\"patch\":\"diff --git a/src/utils/sandbox.ts b/src/utils/sandbox.ts\\nindex 9914a3a5..12e27c30 100644\\n--- a/src/utils/sandbox.ts\\n+++ b/src/utils/sandbox.ts\\n@@ -14,6 +14,51 @@ export interface CompileOptions {\\n   wrapFunction?: boolean;\\n }\\n \\n+export interface JsSyntaxValidationResult {\\n+  valid: boolean;\\n+  error?: string;\\n+}\\n+\\n+/**\\n+ * Validate JavaScript syntax without executing it.\\n+ * Uses the sandbox's compile method to check for syntax errors.\\n+ * Returns validation result with error message if invalid.\\n+ */\\n+export function validateJsSyntax(code: string): JsSyntaxValidationResult {\\n+  if (!code || typeof code !== 'string') {\\n+    return { valid: false, error: 'Code must be a non-empty string' };\\n+  }\\n+\\n+  const trimmed = code.trim();\\n+  if (trimmed.length === 0) {\\n+    return { valid: false, error: 'Code cannot be empty' };\\n+  }\\n+\\n+  // Create a minimal sandbox instance for syntax checking\\n+  const sandbox = createSecureSandbox();\\n+\\n+  // Wrap code similar to compileAndRun to catch the same syntax issues\\n+  const looksLikeBlock = /\\\\breturn\\\\b/.test(trimmed) || /;/.test(trimmed) || /\\\\n/.test(trimmed);\\n+  const looksLikeIife = /\\\\)\\\\s*\\\\(\\\\s*\\\\)\\\\s*;?$/.test(trimmed);\\n+  const body = looksLikeBlock\\n+    ? looksLikeIife\\n+      ? `return (\\\\n${trimmed}\\\\n);\\\\n`\\n+      : `return (() => {\\\\n${trimmed}\\\\n})();\\\\n`\\n+    : `return (\\\\n${trimmed}\\\\n);\\\\n`;\\n+\\n+  // Add log injection header (same as compileAndRun)\\n+  const header = `const __lp = \\\"[syntax-check]\\\"; const log = (...a) => { try { console.log(__lp, ...a); } catch {} };\\\\n`;\\n+  const fullCode = `${header}${body}`;\\n+\\n+  try {\\n+    sandbox.compile(fullCode);\\n+    return { valid: true };\\n+  } catch (e) {\\n+    const msg = e instanceof Error ? e.message : String(e);\\n+    return { valid: false, error: msg };\\n+  }\\n+}\\n+\\n /**\\n  * Create a hardened Sandbox with a consistent set of globals and prototype\\n  * whitelists. This is a superset of the sets previously used by individual\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/tracer-init.ts\",\"additions\":19,\"deletions\":1,\"changes\":20,\"patch\":\"diff --git a/src/utils/tracer-init.ts b/src/utils/tracer-init.ts\\nindex b42995d9..4edf1ffc 100644\\n--- a/src/utils/tracer-init.ts\\n+++ b/src/utils/tracer-init.ts\\n@@ -76,6 +76,24 @@ export async function initializeTracer(\\n \\n       const tracer = new SimpleAppTracer(telemetry, sessionId);\\n \\n+      // WORKAROUND: Add missing recordEvent method for completionPrompt feature (probe #321)\\n+      // SimpleAppTracer doesn't have recordEvent but completionPrompt requires it\\n+      if (typeof (tracer as any).recordEvent !== 'function') {\\n+        (tracer as any).recordEvent = (\\n+          name: string,\\n+          attributes?: Record<string, unknown>\\n+        ) => {\\n+          // Log completion events to telemetry for debugging\\n+          try {\\n+            if ((telemetry as any).record) {\\n+              (telemetry as any).record({ event: name, ...attributes });\\n+            }\\n+          } catch {\\n+            // Best-effort only\\n+          }\\n+        };\\n+      }\\n+\\n       console.error(`üìä Simple tracing enabled, will save to: ${traceFilePath}`);\\n \\n       // If in GitHub Actions, log the path for artifact upload\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/worktree-manager.ts\",\"additions\":142,\"deletions\":27,\"changes\":169,\"patch\":\"diff --git a/src/utils/worktree-manager.ts b/src/utils/worktree-manager.ts\\nindex f3cebf3d..2de7a9f4 100644\\n--- a/src/utils/worktree-manager.ts\\n+++ b/src/utils/worktree-manager.ts\\n@@ -6,6 +6,7 @@\\n  */\\n \\n import * as fs from 'fs';\\n+import * as fsp from 'fs/promises';\\n import * as path from 'path';\\n import * as crypto from 'crypto';\\n import { commandExecutor } from './command-executor';\\n@@ -119,7 +120,8 @@ export class WorktreeManager {\\n     repository: string,\\n     repoUrl: string,\\n     token?: string,\\n-    fetchDepth?: number\\n+    fetchDepth?: number,\\n+    cloneTimeoutMs?: number\\n   ): Promise<string> {\\n     const reposDir = this.getReposDir();\\n     const repoName = repository.replace(/\\\\//g, '-');\\n@@ -129,9 +131,21 @@ export class WorktreeManager {\\n     if (fs.existsSync(bareRepoPath)) {\\n       logger.debug(`Bare repository already exists: ${bareRepoPath}`);\\n \\n-      // Update remote refs\\n-      await this.updateBareRepo(bareRepoPath);\\n-      return bareRepoPath;\\n+      // Verify the bare repo has the correct remote URL to prevent using corrupted repos\\n+      const verifyResult = await this.verifyBareRepoRemote(bareRepoPath, repoUrl);\\n+      if (verifyResult === 'timeout') {\\n+        // Timeout during verification - use stale cache to avoid hanging on re-clone\\n+        logger.info(`Using stale bare repository (verification timed out): ${bareRepoPath}`);\\n+        return bareRepoPath;\\n+      } else if (verifyResult === false) {\\n+        logger.warn(`Bare repository at ${bareRepoPath} has incorrect remote, removing and re-cloning`);\\n+        await fsp.rm(bareRepoPath, { recursive: true, force: true });\\n+        // Fall through to clone below\\n+      } else {\\n+        // Update remote refs\\n+        await this.updateBareRepo(bareRepoPath);\\n+        return bareRepoPath;\\n+      }\\n     }\\n \\n     // Clone as bare repository\\n@@ -153,7 +167,9 @@ export class WorktreeManager {\\n     }\\n     cloneCmd += ` ${this.escapeShellArg(cloneUrl)} ${this.escapeShellArg(bareRepoPath)}`;\\n \\n-    const result = await this.executeGitCommand(cloneCmd, { timeout: 300000 }); // 5 minute timeout\\n+    const result = await this.executeGitCommand(cloneCmd, {\\n+      timeout: cloneTimeoutMs || 300000, // default 5 minutes\\n+    });\\n \\n     if (result.exitCode !== 0) {\\n       // Redact tokens from error messages\\n@@ -171,19 +187,98 @@ export class WorktreeManager {\\n   private async updateBareRepo(bareRepoPath: string): Promise<void> {\\n     logger.debug(`Updating bare repository: ${bareRepoPath}`);\\n \\n-    const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n-    const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+    try {\\n+      const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n+      const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n+        // Don't throw - we can continue with stale refs\\n+      } else {\\n+        logger.debug(`Successfully updated bare repository`);\\n+      }\\n+    } catch (error) {\\n+      // Handle timeout or other errors gracefully - we can continue with stale refs\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      logger.warn(`Failed to update bare repository (will use stale refs): ${errorMessage}`);\\n+    }\\n+  }\\n \\n-    if (result.exitCode !== 0) {\\n-      logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n-      // Don't throw - we can continue with stale refs\\n-    } else {\\n-      logger.debug(`Successfully updated bare repository`);\\n+  /**\\n+   * Verify that a bare repository has the correct remote URL.\\n+   * This prevents reusing corrupted repos that were cloned from a different repository.\\n+   * Returns: true (valid), false (invalid - should re-clone), or 'timeout' (use stale cache)\\n+   */\\n+  private async verifyBareRepoRemote(\\n+    bareRepoPath: string,\\n+    expectedUrl: string\\n+  ): Promise<boolean | 'timeout'> {\\n+    try {\\n+      const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote get-url origin`;\\n+      const result = await this.executeGitCommand(cmd, { timeout: 10000 });\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to get remote URL for ${bareRepoPath}: ${result.stderr}`);\\n+        return false;\\n+      }\\n+\\n+      const actualUrl = result.stdout.trim();\\n+\\n+      // Normalize URLs for comparison:\\n+      // - remove credentials (tokens/username)\\n+      // - remove .git suffix if present\\n+      // - trim trailing slash\\n+      // - lowercase for case‚Äëinsensitive match\\n+      const normalizeUrl = (url: string): string => {\\n+        // Convert common SSH form to https for comparison\\n+        if (url.startsWith('git@github.com:')) {\\n+          url = url.replace('git@github.com:', 'https://github.com/');\\n+        }\\n+        return url\\n+          // strip userinfo part to avoid mismatches when the cached bare\\n+          // repo was cloned with an access token but the expected URL is\\n+          // tokenless (or vice‚Äëversa)\\n+          .replace(/:\\\\/\\\\/[^@]+@/, '://')\\n+          .replace(/\\\\.git$/, '')\\n+          .replace(/\\\\/$/, '')\\n+          .toLowerCase();\\n+      };\\n+\\n+      const normalizedExpected = normalizeUrl(expectedUrl);\\n+      const normalizedActual = normalizeUrl(actualUrl);\\n+\\n+      if (normalizedExpected !== normalizedActual) {\\n+        logger.warn(\\n+          `Bare repo remote mismatch: expected ${expectedUrl}, got ${actualUrl}`\\n+        );\\n+        return false;\\n+      }\\n+\\n+      logger.debug(`Bare repo remote verified: ${actualUrl}`);\\n+      return true;\\n+    } catch (error) {\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      // If it's a timeout, return 'timeout' so caller can use stale cache instead of re-cloning\\n+      if (errorMessage.includes('timed out')) {\\n+        logger.warn(`Timeout verifying bare repo remote (will use stale cache): ${errorMessage}`);\\n+        return 'timeout';\\n+      }\\n+      logger.warn(`Error verifying bare repo remote: ${error}`);\\n+      return false;\\n     }\\n   }\\n \\n   /**\\n-   * Create a new worktree\\n+   * Create a new worktree for the given repository/ref.\\n+   *\\n+   * Important: we always create worktrees in a detached HEAD state pinned\\n+   * to a specific commit SHA rather than a named branch like \\\"main\\\". Git\\n+   * only allows a branch to be checked out in a single worktree at a time;\\n+   * using the raw commit (plus --detach) lets multiple workflows safely\\n+   * create independent worktrees for the same branch without hitting\\n+   * errors like:\\n+   *\\n+   *   fatal: 'main' is already used by worktree at '.../TykTechnologies-tyk-docs-main-XXXX'\\n    */\\n   async createWorktree(\\n     repository: string,\\n@@ -195,6 +290,7 @@ export class WorktreeManager {\\n       clean?: boolean;\\n       workflowId?: string;\\n       fetchDepth?: number;\\n+      cloneTimeoutMs?: number;\\n     } = {}\\n   ): Promise<WorktreeInfo> {\\n     // Validate ref to prevent command injection\\n@@ -205,7 +301,8 @@ export class WorktreeManager {\\n       repository,\\n       repoUrl,\\n       options.token,\\n-      options.fetchDepth\\n+      options.fetchDepth,\\n+      options.cloneTimeoutMs\\n     );\\n \\n     // Generate worktree ID and path\\n@@ -241,21 +338,24 @@ export class WorktreeManager {\\n       }\\n     }\\n \\n-    // Fetch the ref if needed\\n+    // Fetch the ref if needed, then resolve it to a concrete commit SHA.\\n+    // We use the commit (detached HEAD) instead of the branch name so we\\n+    // can have multiple worktrees for the same branch without git refusing\\n+    // with \\\"branch X is already checked out\\\".\\n     await this.fetchRef(bareRepoPath, ref);\\n+    const commit = await this.getCommitShaForRef(bareRepoPath, ref);\\n \\n-    // Create worktree\\n-    logger.info(`Creating worktree for ${repository}@${ref}`);\\n-    const createCmd = `git -C ${this.escapeShellArg(bareRepoPath)} worktree add ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(ref)}`;\\n+    // Create worktree in detached HEAD state at the resolved commit\\n+    logger.info(`Creating worktree for ${repository}@${ref} (${commit})`);\\n+    const createCmd = `git -C ${this.escapeShellArg(\\n+      bareRepoPath\\n+    )} worktree add --detach ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(commit)}`;\\n     const result = await this.executeGitCommand(createCmd, { timeout: 60000 });\\n \\n     if (result.exitCode !== 0) {\\n       throw new Error(`Failed to create worktree: ${result.stderr}`);\\n     }\\n \\n-    // Get commit SHA\\n-    const commit = await this.getCommitSha(worktreePath);\\n-\\n     // Create metadata\\n     const metadata: WorktreeMetadata = {\\n       worktree_id: worktreeId,\\n@@ -316,14 +416,17 @@ export class WorktreeManager {\\n   }\\n \\n   /**\\n-   * Get commit SHA for worktree\\n+   * Get commit SHA for a given ref inside a bare repository.\\n+   *\\n+   * This runs after fetchRef so that <ref> should resolve to either a\\n+   * local branch, tag, or remote-tracking ref.\\n    */\\n-  private async getCommitSha(worktreePath: string): Promise<string> {\\n-    const cmd = `git -C ${this.escapeShellArg(worktreePath)} rev-parse HEAD`;\\n+  private async getCommitShaForRef(bareRepoPath: string, ref: string): Promise<string> {\\n+    const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} rev-parse ${this.escapeShellArg(ref)}`;\\n     const result = await this.executeGitCommand(cmd);\\n \\n     if (result.exitCode !== 0) {\\n-      throw new Error(`Failed to get commit SHA: ${result.stderr}`);\\n+      throw new Error(`Failed to get commit SHA for ref ${ref}: ${result.stderr}`);\\n     }\\n \\n     return result.stdout.trim();\\n@@ -621,9 +724,21 @@ export class WorktreeManager {\\n     command: string,\\n     options: { timeout?: number; env?: Record<string, string> } = {}\\n   ): Promise<GitCommandResult> {\\n+    // Merge provided env with process.env and add git-specific settings\\n+    // These settings prevent git from hanging on interactive prompts while\\n+    // still allowing OS-level credential helpers to work:\\n+    // - GIT_TERMINAL_PROMPT=0: Prevents terminal credential prompts\\n+    // - GIT_SSH_COMMAND: Disables SSH password prompts (BatchMode)\\n+    const gitEnv = {\\n+      ...process.env,\\n+      ...options.env,\\n+      GIT_TERMINAL_PROMPT: '0',\\n+      GIT_SSH_COMMAND: 'ssh -o BatchMode=yes -o StrictHostKeyChecking=no',\\n+    };\\n+\\n     const result = await commandExecutor.execute(command, {\\n       timeout: options.timeout || 30000,\\n-      env: options.env || process.env,\\n+      env: gitEnv,\\n     } as any);\\n \\n     return {\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-executor.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-executor.ts b/src/workflow-executor.ts\\nindex 751e7f16..df76762f 100644\\n--- a/src/workflow-executor.ts\\n+++ b/src/workflow-executor.ts\\n@@ -219,7 +219,9 @@ export class WorkflowExecutor {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps)) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     // Use static DependencyResolver\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-registry.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-registry.ts b/src/workflow-registry.ts\\nindex 2cbe3a89..6bd855fb 100644\\n--- a/src/workflow-registry.ts\\n+++ b/src/workflow-registry.ts\\n@@ -410,7 +410,9 @@ export class WorkflowRegistry {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps || {})) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     try {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/ai-review-service.test.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/tests/unit/ai-review-service.test.ts b/tests/unit/ai-review-service.test.ts\\nindex ec2b998a..2c616c99 100644\\n--- a/tests/unit/ai-review-service.test.ts\\n+++ b/tests/unit/ai-review-service.test.ts\\n@@ -482,6 +482,37 @@ describe('AIReviewService', () => {\\n       expect(context).toContain('<additions>10</additions>');\\n       expect(context).toContain('<deletions>5</deletions>');\\n     });\\n+\\n+    it('should exclude diffs by default when in Slack mode', async () => {\\n+      // Slack mode is detected by presence of slackConversation property\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // In Slack mode, diffs should be excluded by default\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+      // Should still include file summary\\n+      expect(context).toContain('<files_summary>');\\n+    });\\n+\\n+    it('should include diffs in Slack mode when explicitly requested', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      (mockPRInfo as any).includeCodeContext = true;\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // With explicit includeCodeContext: true, diffs should be included\\n+      expect(context).toContain('<full_diff>');\\n+      expect(context).toContain('--- test.ts');\\n+    });\\n+\\n+    it('should exclude diffs in Slack mode even when includeCodeContext is undefined', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      // Deliberately not setting includeCodeContext to test default behavior\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+    });\\n   });\\n \\n   describe('Comment Filtering for Code Review', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config-extends.test.ts\",\"additions\":8,\"deletions\":8,\"changes\":16,\"patch\":\"diff --git a/tests/unit/config-extends.test.ts b/tests/unit/config-extends.test.ts\\nindex b112a869..7ee69ce6 100644\\n--- a/tests/unit/config-extends.test.ts\\n+++ b/tests/unit/config-extends.test.ts\\n@@ -564,7 +564,7 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.security).toBeDefined();\\n       expect(config.checks!.performance).toBeDefined();\\n       expect(config.checks!.custom).toBeDefined();\\n-      expect(config.output.pr_comment.format).toBe('json');\\n+      expect(config.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should extend from default configuration', async () => {\\n@@ -1002,7 +1002,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.base).toBeDefined(); // From base\\n       expect(loaded.checks!.middle).toBeDefined(); // From middle\\n       expect(loaded.checks!.top).toBeDefined(); // From top\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From top\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From top\\n     });\\n \\n     it('should handle mixed local and remote extends', async () => {\\n@@ -1079,7 +1079,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.local).toBeDefined();\\n       expect(loaded.checks!.remote).toBeDefined();\\n       expect(loaded.checks!.child).toBeDefined();\\n-      expect(loaded.output.pr_comment.format).toBe('json');\\n+      expect(loaded.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should handle extends with relative path resolution', async () => {\\n@@ -1593,9 +1593,9 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.performance).toBeDefined(); // Added in child\\n \\n       // Verify output\\n-      expect(config.output.pr_comment.format).toBe('json'); // Overridden\\n-      expect(config.output.pr_comment.group_by).toBe('file'); // Overridden\\n-      expect(config.output.pr_comment.collapse).toBe(false); // Overridden\\n+      expect(config.output!.pr_comment.format).toBe('json'); // Overridden\\n+      expect(config.output!.pr_comment.group_by).toBe('file'); // Overridden\\n+      expect(config.output!.pr_comment.collapse).toBe(false); // Overridden\\n     });\\n \\n     it('should handle configuration with disabled checks removal', async () => {\\n@@ -1814,7 +1814,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.root).toBeDefined(); // From root\\n       expect(loaded.checks!.level1).toBeDefined(); // From level1\\n       expect(loaded.checks!.level2).toBeDefined(); // From level2\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From level2\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From level2\\n     });\\n \\n     it('should handle mixed absolute and relative paths in extends array', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config.test.ts\",\"additions\":7,\"deletions\":7,\"changes\":14,\"patch\":\"diff --git a/tests/unit/config.test.ts b/tests/unit/config.test.ts\\nindex 67ce61fd..0677a3d4 100644\\n--- a/tests/unit/config.test.ts\\n+++ b/tests/unit/config.test.ts\\n@@ -48,7 +48,7 @@ output:\\n       expect(config.version).toBe('1.0');\\n       expect(config.checks).toHaveProperty('performance');\\n       expect(config.checks).toHaveProperty('security');\\n-      expect(config.output.pr_comment.format).toBe('table');\\n+      expect(config.output!.pr_comment.format).toBe('table');\\n     });\\n \\n     it('should handle missing config file gracefully', async () => {\\n@@ -264,9 +264,9 @@ checks:\\n       const config = await configManager.loadConfig('/path/to/minimal.yaml');\\n \\n       // Should have default output configuration\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.group_by).toBe('check');\\n-      expect(config.output.pr_comment.collapse).toBe(true);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.group_by).toBe('check');\\n+      expect(config.output!.pr_comment.collapse).toBe(true);\\n     });\\n   });\\n \\n@@ -483,8 +483,8 @@ output:\\n       expect(config.checks!.performance.prompt).toContain('N+1 database queries');\\n       expect(config.checks!.security.prompt).toContain('SQL injection');\\n       expect(config.checks!.architecture.prompt).toContain('SOLID principles');\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.collapse).toBe(false);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.collapse).toBe(false);\\n     });\\n   });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/foreach-empty-skip.test.ts\",\"additions\":5,\"deletions\":3,\"changes\":8,\"patch\":\"diff --git a/tests/unit/foreach-empty-skip.test.ts b/tests/unit/foreach-empty-skip.test.ts\\nindex 1ea8e1e5..2640ed14 100644\\n--- a/tests/unit/foreach-empty-skip.test.ts\\n+++ b/tests/unit/foreach-empty-skip.test.ts\\n@@ -90,11 +90,13 @@ describe('forEach Empty Array Skip', () => {\\n     );\\n     expect(processStats?.totalRuns).toBe(0);\\n \\n-    // notify-ticket should also NOT run because process-ticket returned empty\\n+    // notify-ticket SHOULD run even though process-ticket had 0 runs from empty forEach\\n+    // This is the expected behavior: downstream steps that depend on forEach children\\n+    // should still execute (with empty/null inputs) to handle the \\\"no items\\\" case\\n     const notifyStats = result.executionStatistics?.checks!.find(\\n       c => c.checkName === 'notify-ticket'\\n     );\\n-    expect(notifyStats?.totalRuns).toBe(0);\\n+    expect(notifyStats?.totalRuns).toBe(1);\\n   });\\n \\n   it('should handle empty array with transform_js', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/ai-check-provider.test.ts\",\"additions\":61,\"deletions\":1,\"changes\":62,\"patch\":\"diff --git a/tests/unit/providers/ai-check-provider.test.ts b/tests/unit/providers/ai-check-provider.test.ts\\nindex d0b94086..a858ddae 100644\\n--- a/tests/unit/providers/ai-check-provider.test.ts\\n+++ b/tests/unit/providers/ai-check-provider.test.ts\\n@@ -399,6 +399,66 @@ describe('AICheckProvider', () => {\\n         },\\n       });\\n     });\\n+\\n+    it('derives allowedFolders and path from workspace when present on parent context', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const workspace = {\\n+        isEnabled: () => true,\\n+        getWorkspaceInfo: () => ({\\n+          workspacePath: '/tmp/ws-123',\\n+          mainProjectPath: '/tmp/ws-123/main-project',\\n+        }),\\n+        listProjects: () => [\\n+          { name: 'tyk', path: '/tmp/ws-123/tyk' },\\n+          { name: 'tyk-docs', path: '/tmp/ws-123/tyk-docs' },\\n+        ],\\n+      };\\n+\\n+      const execContext: any = {\\n+        _parentContext: {\\n+          workspace,\\n+        },\\n+      };\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'code_help',\\n+        ai: {\\n+          provider: 'google',\\n+          model: 'gemini-2.5-pro',\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, execContext);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'google',\\n+        model: 'gemini-2.5-pro',\\n+        // Workspace root should be used as primary working directory for tools\\n+        path: '/tmp/ws-123',\\n+      });\\n+\\n+      // allowedFolders should contain the workspace plus all project paths, de-duped\\n+      expect(capturedConfig.allowedFolders).toEqual(\\n+        expect.arrayContaining(['/tmp/ws-123', '/tmp/ws-123/tyk', '/tmp/ws-123/tyk-docs'])\\n+      );\\n+    });\\n   });\\n \\n   describe('getSupportedConfigKeys', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/command-check-provider.test.ts\",\"additions\":105,\"deletions\":1,\"changes\":106,\"patch\":\"diff --git a/tests/unit/providers/command-check-provider.test.ts b/tests/unit/providers/command-check-provider.test.ts\\nindex 3bf11a84..0a821bbd 100644\\n--- a/tests/unit/providers/command-check-provider.test.ts\\n+++ b/tests/unit/providers/command-check-provider.test.ts\\n@@ -666,4 +666,108 @@ describe('CommandCheckProvider', () => {\\n       expect((result as any).output).toBe('content'); // CommandCheckProvider trims whitespace\\n     });\\n   });\\n+\\n+  describe('Workflow Inputs', () => {\\n+    it('should use workflowInputs from config when available', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Hello from config.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from config.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with inputs from config.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from config.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should fall back to context.workflowInputs when config.workflowInputs is not set', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Hello from context.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from context.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify the command was rendered with inputs from context.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from context.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should prefer config.workflowInputs over context.workflowInputs', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Config takes precedence',\\n+        },\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Context should be ignored',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Config takes precedence\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify config.workflowInputs takes precedence\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Config takes precedence\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should provide empty inputs when neither config nor context has workflowInputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with empty inputs (undefined renders as empty)\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+  });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/git-checkout-workspace.test.ts\",\"additions\":4,\"deletions\":5,\"changes\":9,\"patch\":\"diff --git a/tests/unit/providers/git-checkout-workspace.test.ts b/tests/unit/providers/git-checkout-workspace.test.ts\\nindex 4eb625b7..4a0fbb6e 100644\\n--- a/tests/unit/providers/git-checkout-workspace.test.ts\\n+++ b/tests/unit/providers/git-checkout-workspace.test.ts\\n@@ -129,13 +129,12 @@ describe('GitCheckoutProvider Workspace Integration', () => {\\n         const output = (result as any).output as GitCheckoutOutput;\\n         expect(output.success).toBe(true);\\n         expect(output.workspace_path).toBeDefined();\\n-        // When checkName is provided, it's used as the project name\\n-        expect(output.workspace_path).toContain('test-checkout');\\n \\n-        // Verify project was added to workspace\\n+        // Verify project was added to workspace and the name is derived\\n+        // from the repository/description, not the checkName.\\n         const projects = workspace.listProjects();\\n         expect(projects.length).toBe(1);\\n-        expect(projects[0].name).toBe('test-checkout');\\n+        expect(projects[0].name).toBe('external-repo');\\n       } finally {\\n         await workspace.cleanup();\\n         fs.rmSync(mainProjectDir, { recursive: true, force: true });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/http-client-provider.test.ts\",\"additions\":15,\"deletions\":10,\"changes\":25,\"patch\":\"diff --git a/tests/unit/providers/http-client-provider.test.ts b/tests/unit/providers/http-client-provider.test.ts\\nindex b759ad75..caa73417 100644\\n--- a/tests/unit/providers/http-client-provider.test.ts\\n+++ b/tests/unit/providers/http-client-provider.test.ts\\n@@ -120,15 +120,17 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect(result).toEqual<ReviewSummary & { data: unknown }>({\\n+      // The provider returns data in the 'output' property (consistent with other providers)\\n+      expect(result).toEqual({\\n         issues: [],\\n-        data: responseData,\\n+        output: { status: 'ok', data: { value: 123 } },\\n       });\\n     });\\n \\n     it('should handle POST request with body', async () => {\\n       mockConfig.method = 'POST';\\n-      mockConfig.body = '{\\\"request\\\": \\\"data\\\"}';\\n+      // Use a body with Liquid template to trigger parseAndRender\\n+      mockConfig.body = '{\\\"request\\\": \\\"{{ pr.title }}\\\"}';\\n \\n       const responseData = { result: 'success' };\\n       const mockResponse = {\\n@@ -143,15 +145,16 @@ describe('HttpClientProvider', () => {\\n       };\\n \\n       mockFetch.mockResolvedValue(mockResponse);\\n-      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"data\\\"}');\\n+      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"Test PR\\\"}');\\n \\n       const result = await provider.execute(mockPRInfo, mockConfig, new Map());\\n \\n+      // Verify the body template was parsed by Liquid with the correct template\\n       expect(mockLiquid.parseAndRender).toHaveBeenCalledWith(\\n-        '{\\\"request\\\": \\\"data\\\"}',\\n+        '{\\\"request\\\": \\\"{{ pr.title }}\\\"}',\\n         expect.objectContaining({\\n           pr: expect.any(Object),\\n-          outputs: {},\\n+          outputs: expect.any(Object),\\n         })\\n       );\\n \\n@@ -159,7 +162,7 @@ describe('HttpClientProvider', () => {\\n         'https://api.example.com/data',\\n         expect.objectContaining({\\n           method: 'POST',\\n-          body: '{\\\"request\\\": \\\"data\\\"}',\\n+          body: '{\\\"request\\\": \\\"Test PR\\\"}',\\n           headers: expect.objectContaining({\\n             Authorization: 'Bearer token',\\n             'Content-Type': 'application/json',\\n@@ -167,7 +170,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(responseData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { result: string } }).output.result).toEqual('success');\\n     });\\n \\n     it('should handle HTTP errors', async () => {\\n@@ -235,7 +239,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(transformedData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { transformed: string } }).output.transformed).toEqual('result');\\n     });\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/session-reuse-config.test.ts\",\"additions\":20,\"deletions\":1,\"changes\":21,\"patch\":\"diff --git a/tests/unit/session-reuse-config.test.ts b/tests/unit/session-reuse-config.test.ts\\nindex a2e2ee26..18fc42e6 100644\\n--- a/tests/unit/session-reuse-config.test.ts\\n+++ b/tests/unit/session-reuse-config.test.ts\\n@@ -152,6 +152,25 @@ describe('Session Reuse Configuration Validation', () => {\\n       }).not.toThrow();\\n     });\\n \\n+    it('should accept reuse_ai_session=\\\\\\\"self\\\\\\\" without depends_on', () => {\\n+      const config: Partial<VisorConfig> = {\\n+        version: '1.0',\\n+        checks: {\\n+          'loop-check': {\\n+            type: 'ai',\\n+            prompt: 'Chat-like loop that reuses its own session',\\n+            on: ['pr_opened'],\\n+            // Special self-reuse mode does not require depends_on\\n+            reuse_ai_session: 'self',\\n+          },\\n+        },\\n+      };\\n+\\n+      expect(() => {\\n+        (configManager as any).validateConfig(config);\\n+      }).not.toThrow();\\n+    });\\n+\\n     it('should accept reuse_ai_session=true with multiple dependencies', () => {\\n       const config: Partial<VisorConfig> = {\\n         version: '1.0',\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/slack-markdown-format.test.ts\",\"additions\":25,\"deletions\":1,\"changes\":26,\"patch\":\"diff --git a/tests/unit/slack-markdown-format.test.ts b/tests/unit/slack-markdown-format.test.ts\\nindex 5e57ccae..39d0baf3 100644\\n--- a/tests/unit/slack-markdown-format.test.ts\\n+++ b/tests/unit/slack-markdown-format.test.ts\\n@@ -39,4 +39,28 @@ describe('markdownToSlack', () => {\\n       ['‚Ä¢ parent', '  ‚Ä¢ child', '```', '- not-a-bullet inside code', '```', '‚Ä¢ after'].join('\\\\n')\\n     );\\n   });\\n+\\n+  it('converts markdown headers to bold text', () => {\\n+    const input = '# Main Title\\\\n## Subtitle\\\\n### Section';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*Main Title*\\\\n*Subtitle*\\\\n*Section*');\\n+  });\\n+\\n+  it('adds newline before h1/h2 headers when preceded by content', () => {\\n+    const input = 'Some content\\\\n## New Section\\\\nMore content';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('Some content\\\\n\\\\n*New Section*\\\\nMore content');\\n+  });\\n+\\n+  it('does not add newline before h1 if it is the first line', () => {\\n+    const input = '# First Header\\\\nContent here';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*First Header*\\\\nContent here');\\n+  });\\n+\\n+  it('ignores headers inside code blocks', () => {\\n+    const input = '```\\\\n# This is a comment\\\\n```\\\\n# Real Header';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('```\\\\n# This is a comment\\\\n```\\\\n*Real Header*');\\n+  });\\n });\\n\",\"status\":\"modified\"}],\"outputs\":{\"ask\":{\"text\":\"what is the root cause of this bug:\\n<https://tyktech.atlassian.net/browse/TT-16304>\",\"ts\":1767899989365},\"jira-context\":{\"jira_context_xml\":\"<jira_context>\\n  <issue_count>1</issue_count>\\n  <issue key=\\\"TT-16304\\\">\\n    <summary>Valid OAS API cannot be used by GW due to validation failed error</summary>\\n    <description></description>\\n    <status>Open</status>\\n    <priority>Medium</priority>\\n    <assignee></assignee>\\n    <reporter>Radoslaw Krawczyk</reporter>\\n    <components>Tyk Gateway</components>\\n  </issue>\\n</jira_context>\",\"issues\":[{\"key\":\"TT-16304\",\"summary\":\"Valid OAS API cannot be used by GW due to validation failed error\",\"description\":\"\",\"status\":\"Open\",\"priority\":\"Medium\",\"assignee\":\"\",\"reporter\":\"Radoslaw Krawczyk\",\"labels\":[],\"components\":[\"Tyk Gateway\"],\"custom_fields\":{},\"parent\":null,\"subtasks\":[],\"comments\":[]}],\"issue_count\":1,\"ts\":1767899990143},\"zendesk-context\":{\"zendesk_context_xml\":\"<zendesk_context>\\n  <ticket_count>1</ticket_count>\\n  <ticket id=\\\"16304\\\">\\n    <subject>Authorization Header : starts with a space </subject>\\n    <description>\\n\\nWe have a WAF Compliance Issue whereby the WAF asserts a Header for Authorization as such the Value is leading by a space ; Hence a 403 is recieved\\n\\nurl -v --cert qa-ndex.enstreamidentity.com.cer --key bell-preprod.key \\\\\\n\\n-H &quot;enstream-authorization: 8087656ca224433466c00c75847ac248&quot; \\\\\\n\\n&quot;https://apistg.nbd.bell.ca/ndex/getAttributesFromId?appId=3049&amp;idType=mdn&amp;idList=4162160839&amp;attbList=*&quot;\\n\\n*   Trying 184.150.80.90:443...\\n\\n* Connected to apistg.nbd.bell.ca (184.150.80.90) port 443 (#0)\\n\\n* ALPN, offering h2\\n\\n* ALPN, offering http/1.1\\n\\n* successfully set certificate verify locations:\\n\\n*  CAfile: /etc/ssl/cert.pem\\n\\n*  CApath: none\\n\\n* (304) (OUT), TLS handshake, Client hello (1):\\n\\n* (304) (IN), TLS handshake, Server hello (2):\\n\\n* TLSv1.2 (IN), TLS handshake, Certificate (11):\\n\\n* TLSv1.2 (IN), TLS handshake, Server key exchange (12):\\n\\n* TLSv1.2 (IN), TLS handshake, Request CERT (13):\\n\\n* TLSv1.2 (IN), TLS handshake, Server finished (14):\\n\\n* TLSv1.2 (OUT), TLS handshake, Certificate (11):\\n\\n* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):\\n\\n* TLSv1.2 (OUT), TLS handshake, CERT verify (15):\\n\\n* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):\\n\\n* TLSv1.2 (OUT), TLS handshake, Finished (20):\\n\\n* TLSv1.2 (IN), TLS change cipher, Change cipher spec (1):\\n\\n* TLSv1.2 (IN), TLS handshake, Finished (20):\\n\\n* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256\\n\\n* ALPN, server did not agree to a protocol\\n\\n* Server certificate:\\n\\n*  subject: C=CA; ST=Ontario; L=Ottawa; O=Bell Canada; CN=apistg.nbd.bell.ca\\n\\n*  start date: Jan 20 20:32:24 2023 GMT\\n\\n*  expire date: Feb 19 20:32:21 2024 GMT\\n\\n*  subjectAltName: host &quot;apistg.nbd.bell.ca&quot; matched cert&apos;s &quot;apistg.nbd.bell.ca&quot;\\n\\n*  issuer: C=US; O=Entrust, Inc.; OU=See www.entrust.net/legal-terms; OU=(c) 2012 Entrust, Inc. - for authorized use only; CN=Entrust Certification Authority - L1K\\n\\n*  SSL certificate verify ok.\\n\\n&gt; GET /ndex/getAttributesFromId?appId=3049&amp;idType=mdn&amp;idList=4162160839&amp;attbList=* HTTP/1.1\\n\\n&gt; Host: apistg.nbd.bell.ca\\n\\n&gt; User-Agent: curl/7.79.1\\n\\n&gt; Accept: */*\\n\\n&gt; enstream-authorization: 2c760a6ca224433466c00c75847ac248\\n\\n&gt;\\n\\n* Mark bundle as not supporting multiuse\\n\\n&lt; HTTP/1.1 403 Forbidden\\n\\n&lt; Content-Type: application/json\\n\\n&lt; Date: Tue, 07 Mar 2023 16:35:42 GMT\\n\\n&lt; Content-Length: 57\\n\\n&lt; Set-Cookie: TS01ab8fe2=017e5ad8c4423a1f0c76c854eb18d47723f266acc5bb34020cbc2b9674adb0c3412fc32496f3685aa3b22cf1c284a800cc2f29fdcd; Path=/; Secure; HTTPOnly\\n\\n&lt;\\n\\n{\\n\\n    &quot;error&quot;: &quot;Access to this API has been disallowed&quot;\\n\\n* Connection #0 to host apistg.nbd.bell.ca left intact\\n\\n}</description>\\n    <status>closed</status>\\n    <priority>normal</priority>\\n    <created_at>2023-03-13T14:33:03Z</created_at>\\n    <updated_at>2023-03-21T18:02:27Z</updated_at>\\n    <tags>amer, gold, mongodb, on-prem, q_a, sla, us, wes</tags>\\n  </ticket>\\n</zendesk_context>\",\"tickets\":\"[{\\\"id\\\":\\\"16304\\\",\\\"subject\\\":\\\"Authorization Header : starts with a space \\\",\\\"description\\\":\\\"\\\\n\\\\nWe have a WAF Compliance Issue whereby the WAF asserts a Header for Authorization as such the Value is leading by a space ; Hence a 403 is recieved\\\\n\\\\nurl -v --cert qa-ndex.enstreamidentity.com.cer --key bell-preprod.key \\\\\\\\\\\\n\\\\n-H \\\\\\\"enstream-authorization: 8087656ca224433466c00c75847ac248\\\\\\\" \\\\\\\\\\\\n\\\\n\\\\\\\"https://apistg.nbd.bell.ca/ndex/getAttributesFromId?appId=3049&idType=mdn&idList=4162160839&attbList=*\\\\\\\"\\\\n\\\\n*   Trying 184.150.80.90:443...\\\\n\\\\n* Connected to apistg.nbd.bell.ca (184.150.80.90) port 443 (#0)\\\\n\\\\n* ALPN, offering h2\\\\n\\\\n* ALPN, offering http/1.1\\\\n\\\\n* successfully set certificate verify locations:\\\\n\\\\n*  CAfile: /etc/ssl/cert.pem\\\\n\\\\n*  CApath: none\\\\n\\\\n* (304) (OUT), TLS handshake, Client hello (1):\\\\n\\\\n* (304) (IN), TLS handshake, Server hello (2):\\\\n\\\\n* TLSv1.2 (IN), TLS handshake, Certificate (11):\\\\n\\\\n* TLSv1.2 (IN), TLS handshake, Server key exchange (12):\\\\n\\\\n* TLSv1.2 (IN), TLS handshake, Request CERT (13):\\\\n\\\\n* TLSv1.2 (IN), TLS handshake, Server finished (14):\\\\n\\\\n* TLSv1.2 (OUT), TLS handshake, Certificate (11):\\\\n\\\\n* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):\\\\n\\\\n* TLSv1.2 (OUT), TLS handshake, CERT verify (15):\\\\n\\\\n* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):\\\\n\\\\n* TLSv1.2 (OUT), TLS handshake, Finished (20):\\\\n\\\\n* TLSv1.2 (IN), TLS change cipher, Change cipher spec (1):\\\\n\\\\n* TLSv1.2 (IN), TLS handshake, Finished (20):\\\\n\\\\n* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256\\\\n\\\\n* ALPN, server did not agree to a protocol\\\\n\\\\n* Server certificate:\\\\n\\\\n*  subject: C=CA; ST=Ontario; L=Ottawa; O=Bell Canada; CN=apistg.nbd.bell.ca\\\\n\\\\n*  start date: Jan 20 20:32:24 2023 GMT\\\\n\\\\n*  expire date: Feb 19 20:32:21 2024 GMT\\\\n\\\\n*  subjectAltName: host \\\\\\\"apistg.nbd.bell.ca\\\\\\\" matched cert's \\\\\\\"apistg.nbd.bell.ca\\\\\\\"\\\\n\\\\n*  issuer: C=US; O=Entrust, Inc.; OU=See www.entrust.net/legal-terms; OU=(c) 2012 Entrust, Inc. - for authorized use only; CN=Entrust Certification Authority - L1K\\\\n\\\\n*  SSL certificate verify ok.\\\\n\\\\n> GET /ndex/getAttributesFromId?appId=3049&idType=mdn&idList=4162160839&attbList=* HTTP/1.1\\\\n\\\\n> Host: apistg.nbd.bell.ca\\\\n\\\\n> User-Agent: curl/7.79.1\\\\n\\\\n> Accept: */*\\\\n\\\\n> enstream-authorization: 2c760a6ca224433466c00c75847ac248\\\\n\\\\n>\\\\n\\\\n* Mark bundle as not supporting multiuse\\\\n\\\\n< HTTP/1.1 403 Forbidden\\\\n\\\\n< Content-Type: application/json\\\\n\\\\n< Date: Tue, 07 Mar 2023 16:35:42 GMT\\\\n\\\\n< Content-Length: 57\\\\n\\\\n< Set-Cookie: TS01ab8fe2=017e5ad8c4423a1f0c76c854eb18d47723f266acc5bb34020cbc2b9674adb0c3412fc32496f3685aa3b22cf1c284a800cc2f29fdcd; Path=/; Secure; HTTPOnly\\\\n\\\\n<\\\\n\\\\n{\\\\n\\\\n    \\\\\\\"error\\\\\\\": \\\\\\\"Access to this API has been disallowed\\\\\\\"\\\\n\\\\n* Connection #0 to host apistg.nbd.bell.ca left intact\\\\n\\\\n}\\\",\\\"status\\\":\\\"closed\\\",\\\"priority\\\":\\\"normal\\\",\\\"requester_id\\\":5942781629980,\\\"assignee_id\\\":389488497399,\\\"tags\\\":[\\\"amer\\\",\\\"gold\\\",\\\"mongodb\\\",\\\"on-prem\\\",\\\"q_a\\\",\\\"sla\\\",\\\"us\\\",\\\"wes\\\"],\\\"created_at\\\":\\\"2023-03-13T14:33:03Z\\\",\\\"updated_at\\\":\\\"2023-03-21T18:02:27Z\\\"}]\",\"ticket_count\":1,\"attachments\":\"{}\",\"ts\":1767899990429},\"log-request\":{\"issues\":[],\"logOutput\":\"‚ÑπÔ∏è **INFO**: üì• Request from user: unknown\\nChannel: unknown\\nThread: unknown\\nMessage: what is the root cause of this bug:\\n<https://tyktech.atlassian.net/browse/TT-16304>\\n\"},\"code-help-error\":{\"issues\":[]},\"customer-insights-error\":{\"issues\":[]},\"release-notes-error\":{\"issues\":[]}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.output":"{\"intent\":\"code_help\",\"topic\":\"What is the root cause of the bug in Jira ticket TT-16304, where a valid OAS API fails validation in the Gateway?\",\"text\":\"{\\n  \\\"intent\\\": \\\"code_help\\\",\\n  \\\"topic\\\": \\\"What is the root cause of the bug in Jira ticket TT-16304, where a valid OAS API fails validation in the Gateway?\\\"\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":2,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"code-help","visor.provider.type":"workflow"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"6cb247d8-9f68-469f-9a04-0e37725a7a22"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"6cb247d8-9f68-469f-9a04-0e37725a7a22"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"6cb247d8-9f68-469f-9a04-0e37725a7a22"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-tyk-docs","visor.provider.type":"git-checkout"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"6cb247d8-9f68-469f-9a04-0e37725a7a22"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"6cb247d8-9f68-469f-9a04-0e37725a7a22"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"6cb247d8-9f68-469f-9a04-0e37725a7a22"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ensure-code-plan","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"ensure-code-plan","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"outputs\":{\"checkout-tyk-docs\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-docs-main-bada06aa\",\"ref\":\"main\",\"commit\":\"a022bb7c96ca4f48bcdac27cae3f7c4a24171ee9\",\"worktree_id\":\"TykTechnologies-tyk-docs-main-bada06aa\",\"repository\":\"TykTechnologies/tyk-docs\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d/tyk-docs\",\"ts\":1767900006651}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"ensure-code-plan","visor.check.output":"{\"projects\":[{\"project_id\":\"tyk\",\"reason\":\"The Jira ticket ID was not found in the docs. The Tyk Gateway is the core component responsible for processing APIs, making it a primary location for potential bugs.\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Dashboard backend is responsible for configuring the Gateway. Bugs often originate from misconfigurations or issues in how the Dashboard generates and sends API definitions to the Gateway.\"}],\"notes\":\"The Jira ticket ID was not found in the documentation, so I cannot determine the specific feature area. The plan includes the two most central components of the Tyk stack (Gateway and Dashboard backend) as the most probable locations for a bug's root cause.\",\"text\":\"{\\n  \\\"projects\\\": [\\n    {\\n      \\\"project_id\\\": \\\"tyk\\\",\\n      \\\"reason\\\": \\\"The Jira ticket ID was not found in the docs. The Tyk Gateway is the core component responsible for processing APIs, making it a primary location for potential bugs.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-analytics\\\",\\n      \\\"reason\\\": \\\"The Tyk Dashboard backend is responsible for configuring the Gateway. Bugs often originate from misconfigurations or issues in how the Dashboard generates and sends API definitions to the Gateway.\\\"\\n    }\\n  ],\\n  \\\"notes\\\": \\\"The Jira ticket ID was not found in the documentation, so I cannot determine the specific feature area. The plan includes the two most central components of the Tyk stack (Gateway and Dashboard backend) as the most probable locations for a bug's root cause.\\\"\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"6cb247d8-9f68-469f-9a04-0e37725a7a22"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"6cb247d8-9f68-469f-9a04-0e37725a7a22"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"6cb247d8-9f68-469f-9a04-0e37725a7a22"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"project-items","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"6cb247d8-9f68-469f-9a04-0e37725a7a22"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"6cb247d8-9f68-469f-9a04-0e37725a7a22"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"6cb247d8-9f68-469f-9a04-0e37725a7a22"}}]}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":0,"visor.foreach.total":2},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":1,"visor.foreach.total":2},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"6cb247d8-9f68-469f-9a04-0e37725a7a22"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"6cb247d8-9f68-469f-9a04-0e37725a7a22"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"6cb247d8-9f68-469f-9a04-0e37725a7a22"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"project-code-query","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"project-code-query","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"outputs\":{\"ensure-code-plan\":{\"projects\":[{\"project_id\":\"tyk\",\"reason\":\"The Jira ticket ID was not found in the docs. The Tyk Gateway is the core component responsible for processing APIs, making it a primary location for potential bugs.\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Dashboard backend is responsible for configuring the Gateway. Bugs often originate from misconfigurations or issues in how the Dashboard generates and sends API definitions to the Gateway.\"}],\"notes\":\"The Jira ticket ID was not found in the documentation, so I cannot determine the specific feature area. The plan includes the two most central components of the Tyk stack (Gateway and Dashboard backend) as the most probable locations for a bug's root cause.\",\"text\":\"{\\n  \\\"projects\\\": [\\n    {\\n      \\\"project_id\\\": \\\"tyk\\\",\\n      \\\"reason\\\": \\\"The Jira ticket ID was not found in the docs. The Tyk Gateway is the core component responsible for processing APIs, making it a primary location for potential bugs.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-analytics\\\",\\n      \\\"reason\\\": \\\"The Tyk Dashboard backend is responsible for configuring the Gateway. Bugs often originate from misconfigurations or issues in how the Dashboard generates and sends API definitions to the Gateway.\\\"\\n    }\\n  ],\\n  \\\"notes\\\": \\\"The Jira ticket ID was not found in the documentation, so I cannot determine the specific feature area. The plan includes the two most central components of the Tyk stack (Gateway and Dashboard backend) as the most probable locations for a bug's root cause.\\\"\\n}\",\"ts\":1767900030552},\"project-items\":[{\"project_id\":\"tyk\",\"reason\":\"The Jira ticket ID was not found in the docs. The Tyk Gateway is the core component responsible for processing APIs, making it a primary location for potential bugs.\",\"repository\":\"TykTechnologies/tyk\",\"description\":\"Tyk Gateway\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Dashboard backend is responsible for configuring the Gateway. Bugs often originate from misconfigurations or issues in how the Dashboard generates and sends API definitions to the Gateway.\",\"repository\":\"TykTechnologies/tyk-analytics\",\"description\":\"Tyk Dashboard backend\"}],\"checkout-projects\":{\"issues\":[],\"isForEach\":true,\"forEachItems\":[{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-HEAD-38ad7b02\",\"ref\":\"HEAD\",\"commit\":\"bb67e9e7f9382a09ff152c6942d00023c463feef\",\"worktree_id\":\"TykTechnologies-tyk-HEAD-38ad7b02\",\"repository\":\"TykTechnologies/tyk\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d/tyk\"},{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-analytics-HEAD-d3ffa7b2\",\"ref\":\"HEAD\",\"commit\":\"b7ed8000a93e1284b28100938363a312d2d1fad1\",\"worktree_id\":\"TykTechnologies-tyk-analytics-HEAD-d3ffa7b2\",\"repository\":\"TykTechnologies/tyk-analytics\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d/tyk-analytics\"}],\"forEachItemResults\":[{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-HEAD-38ad7b02\",\"ref\":\"HEAD\",\"commit\":\"bb67e9e7f9382a09ff152c6942d00023c463feef\",\"worktree_id\":\"TykTechnologies-tyk-HEAD-38ad7b02\",\"repository\":\"TykTechnologies/tyk\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d/tyk\"}},{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-analytics-HEAD-d3ffa7b2\",\"ref\":\"HEAD\",\"commit\":\"b7ed8000a93e1284b28100938363a312d2d1fad1\",\"worktree_id\":\"TykTechnologies-tyk-analytics-HEAD-d3ffa7b2\",\"repository\":\"TykTechnologies/tyk-analytics\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d/tyk-analytics\"}}]},\"checkout-tyk-docs\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-docs-main-bada06aa\",\"ref\":\"main\",\"commit\":\"a022bb7c96ca4f48bcdac27cae3f7c4a24171ee9\",\"worktree_id\":\"TykTechnologies-tyk-docs-main-bada06aa\",\"repository\":\"TykTechnologies/tyk-docs\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d/tyk-docs\",\"ts\":1767900006651},\"project-items-raw\":[{\"project_id\":\"tyk\",\"reason\":\"The Jira ticket ID was not found in the docs. The Tyk Gateway is the core component responsible for processing APIs, making it a primary location for potential bugs.\",\"repository\":\"TykTechnologies/tyk\",\"description\":\"Tyk Gateway\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Dashboard backend is responsible for configuring the Gateway. Bugs often originate from misconfigurations or issues in how the Dashboard generates and sends API definitions to the Gateway.\",\"repository\":\"TykTechnologies/tyk-analytics\",\"description\":\"Tyk Dashboard backend\"}]},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"project-code-query","visor.check.output":"{\"answer\":{\"text\":\"I was unable to determine the root cause of the bug described in Jira ticket TT-16304. I searched for the ticket number in both the `tyk` and `tyk-analytics` repositories, but found no references to it. I also searched for the phrase \\\"state machine\\\" and the keywords \\\"state\\\" and \\\"machine\\\" in the code and the documentation, but again found no results. Without access to the Jira ticket itself or more information about the feature, I am unable to locate the relevant code and investigate the bug.\"},\"text\":\"{\\n  \\\"answer\\\": {\\n    \\\"text\\\": \\\"I was unable to determine the root cause of the bug described in Jira ticket TT-16304. I searched for the ticket number in both the `tyk` and `tyk-analytics` repositories, but found no references to it. I also searched for the phrase \\\\\\\"state machine\\\\\\\" and the keywords \\\\\\\"state\\\\\\\" and \\\\\\\"machine\\\\\\\" in the code and the documentation, but again found no results. Without access to the Jira ticket itself or more information about the feature, I am unable to locate the relevant code and investigate the bug.\\\"\\n  }\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"6cb247d8-9f68-469f-9a04-0e37725a7a22"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"6cb247d8-9f68-469f-9a04-0e37725a7a22"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"6cb247d8-9f68-469f-9a04-0e37725a7a22"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":2,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"code-help-reply","visor.provider.type":"log"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":3,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":3,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":3,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":3,"session_id":"14d27ad5-8c79-46f8-b6d9-bd4b7cd1fd0d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"jira-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"zendesk-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"log-request","visor.provider.type":"log"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"13b17c99-8c44-4caa-89a3-6a9b1168c803"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"2b953423-829c-4120-8f8e-d2017c9538aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"13b17c99-8c44-4caa-89a3-6a9b1168c803"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"2b953423-829c-4120-8f8e-d2017c9538aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"13b17c99-8c44-4caa-89a3-6a9b1168c803"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"2b953423-829c-4120-8f8e-d2017c9538aa"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-keys","visor.provider.type":"script"}}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-ids","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"history\":{}},\"outputs_history\":{},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"include_comments\":true,\"max_tickets\":5,\"download_assets\":true,\"cache_dir\":\".cache/zendesk\",\"text\":\"<@U09T5KRLMPU> If using the Tyk Operator with OperatorContext to manage multiple environments within a cluster, how would I go about a blue-green deployment for a given environment?\\nA blue-green deployment with the Tyk Operator using `OperatorContext` is achieved by managing two distinct API definitions‚Äîone for the \\\"blue\\\" environment and one for the \\\"green\\\" environment‚Äîand manipulating their `listen_path` or domain to control traffic flow. The process relies on the Tyk Gateway's hot-reload capability to switch traffic seamlessly without downtime.\\n\\nHere‚Äôs a step-by-step breakdown of how this works:\\n\\n*1. Initial State (Blue Version Live)*\\n\\n‚Ä¢ You start with a `ApiDefinition` custom resource for your stable \\\"blue\\\" version. This definition points to your blue upstream service and has a public-facing `listen_path`, for example, `/my-service`.\\n‚Ä¢ The Tyk Operator detects this resource and creates the corresponding API definition in the Tyk Dashboard.\\n‚Ä¢ The Tyk Gateway loads this definition and routes all production traffic for `/my-service` to your blue environment.\\n\\n*2. Deploying the New Version (Green)*\\n\\n‚Ä¢ To deploy a new version, you create a second `ApiDefinition` resource for your \\\"green\\\" version. This definition will point to your new, green upstream service.\\n‚Ä¢ Initially, you configure this green version with a non-production `listen_path`, such as `/my-service-green`. This keeps it isolated from live traffic and allows for testing.\\n‚Ä¢ The Tyk Operator creates this second API definition in the Dashboard, and the Gateway loads it, making the green version accessible at `/my-service-green` for verification.\\n\\n*3. Switching Traffic to Green*\\n\\nWhen you are ready to switch production traffic to the new version, you perform the following steps by updating the `ApiDefinition` resources in Kubernetes:\\n\\n1.  *Update the Green API:* Change the `listen_path` of the green `ApiDefinition` from `/my-service-green` to the production path `/my-service`.\\n2.  *Update the Blue API:* Change the `listen_path` of the blue `ApiDefinition` from `/my-service` to a non-production path like `/my-service-blue`. Alternatively, you can deactivate the blue API by setting its `active` flag to `false`.\\n\\n‚Ä¢ The Tyk Operator applies these changes to the API definitions in the Tyk Dashboard.\\n‚Ä¢ The Dashboard then pushes these updates to the Tyk Gateways.\\n‚Ä¢ The Gateways perform a hot-reload of the configuration. This is a zero-downtime operation that atomically updates the routing rules. All new traffic to `/my-service` will now be directed to your green upstream service.\\n\\n*4. Rollback*\\n\\nIf you need to roll back to the blue version, you simply reverse the process: update the `ApiDefinition` resources to swap the `listen_path` values back to their original states. The Gateway will hot-reload again, and traffic will be routed to the blue environment.\\n\\nIn summary, the Tyk Operator orchestrates the blue-green deployment by managing two separate API definitions. The traffic switch is executed by updating the `listen_path` of these definitions, which triggers a seamless hot-reload in the Tyk Gateway, ensuring a zero-downtime transition.\\n\\n*References:*\\n\\n‚Ä¢ *Tyk Docs on Blue-Green Upgrades:* <https://github.com/TykTechnologies/tyk-docs/blob/main/developer-support/upgrading.mdx#L133-L139|https://github.com/TykTechnologies/tyk-docs/blob/main/developer-support/upgrading.mdx#L133-L139>\\n‚Ä¢ *Tyk Docs on API Versioning:* <https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/gateway-config-managing-oas.mdx#L628|https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/gateway-config-managing-oas.mdx#L628>\\n‚Ä¢ *Tyk Operator API Definition:* <https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-stack/tyk-operator/create-an-api.mdx#L1111-L1118|https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-stack/tyk-operator/create-an-api.mdx#L1111-L1118>\\nCan this be done without having to modify the APIDefinition custom resource?\\n<@U09T5KRLMPU>  Can this be done without having to modify the APIDefinition custom resource?\\nWhether a blue-green deployment can be achieved without modifying the `APIDefinition` custom resource depends entirely on the Tyk Gateway's DNS resolution and caching behavior for upstream targets. The proposed strategy of pointing the `target_url` to a stable Kubernetes service and then switching traffic by modifying the service's selectors is a common pattern, but its success hinges on how quickly the gateway detects the change in the underlying endpoints.\\n\\nHere are the two possibilities:\\n\\n1.  *This approach will work if* the Tyk Gateway is configured to have a very short Time-To-Live (TTL) for its DNS cache, or if it resolves the DNS for each incoming request. In this scenario, when you update the Kubernetes service to point to the 'green' deployment's pods, the gateway would quickly pick up the new IP addresses and start routing traffic to them.\\n\\n2.  *This approach will NOT work if* the Tyk Gateway caches DNS lookups for an extended period. If the gateway resolves the service's DNS name to a set of IP addresses and caches that result, it will continue to send traffic to the old 'blue' pods, even after the service's selectors have been updated. The traffic would not shift to the 'green' deployment until the gateway's DNS cache expires or it is forced to reload.\\n\\nThe reason that modifying the `APIDefinition` (e.g., changing the `listen_path`) is the recommended and guaranteed method is because this action triggers a hot reload of the gateway's configuration. A hot reload forces the gateway to discard its old configuration and caches, including any cached DNS entries for upstream targets, and build a new routing map. This ensures that the change in traffic routing is immediate and predictable.\\n\\n*In summary:* while it is theoretically possible to perform a blue-green deployment using DNS switching, it is not a guaranteed method and depends on the gateway's specific, and often undocumented, DNS caching implementation. The reliable and officially supported method is to manage two distinct `APIDefinition` resources and manipulate their configurations to control traffic flow, as this leverages the gateway's hot reload capability for a seamless transition.\\n<@U09T5KRLMPU> - In my scenario, the upstream targets are the same between the blue and green deployments.  How does this change the approach?\\nYes, a blue-green deployment can be achieved without modifying the `APIDefinition` custom resource by using Tyk's *service discovery* feature.\\n\\nThis approach allows you to dynamically control traffic flow by updating an external endpoint rather than changing the static API configuration in Kubernetes.\\n\\n*How It Works:*\\n\\n1.  *Configure Service Discovery*: In your API definition, instead of providing a static list of upstream targets, you enable the service discovery feature. This is configured in the `proxy.service_discovery` section of the API definition. The most important field is `query_endpoint`, which points to an HTTP endpoint that you control.\\n\\n2.  *Externalize Upstream Targets*: The `query_endpoint` is responsible for returning a JSON response containing the list of upstream targets (e.g., the IP addresses or hostnames of your services). The Tyk Gateway will periodically call this endpoint to refresh its target list.\\n\\n3.  *Perform the Blue-Green Switch*: To switch traffic from your \\\"blue\\\" environment to your \\\"green\\\" environment, you simply update the response of your `query_endpoint`. \\n    ‚Ä¢ Initially, the endpoint returns the addresses of your \\\"blue\\\" services.\\n    ‚Ä¢ When you're ready to switch, you change the endpoint to return the addresses of your \\\"green\\\" services.\\n\\nThe Tyk Gateway will pick up this change on its next refresh cycle and start routing all new traffic to the green environment, achieving a seamless, zero-downtime deployment.\\n\\n*Why This Works Without Modifying the `APIDefinition`:*\\n\\nThe `APIDefinition` custom resource only contains the configuration for *how* to discover the upstream targets (i.e., the URL of the `query_endpoint`). The actual list of targets is managed externally. This separation allows you to change where traffic is routed without needing to apply any changes to your Kubernetes resources, thus avoiding a reload of the API definition.\\n\\n*Alternative Considered: Weighted Load Balancing*\\n\\nWhile Tyk also supports weighted load balancing, it is not suitable for this specific use case. The weights for each upstream target are defined directly within the `APIDefinition`. Therefore, shifting traffic by adjusting weights would require modifying the custom resource, which is what you want to avoid.\\n\\n*Code References:*\\n\\n‚Ä¢ *Service Discovery Configuration Struct (`tyk` repository)*: <https://github.com/TykTechnologies/tyk/blob/master/apidef/api_definitions.go#L615-L628|`tyk/apidef/api_definitions.go#L615-L628`>\\n‚Ä¢ *Service Discovery Fetch Logic (`tyk` repository)*: <https://github.com/TykTechnologies/tyk/blob/master/gateway/service_discovery.go#L300-L309|`tyk/gateway/service_discovery.go#L300-L309`>\\n‚Ä¢ *Weighted Load Balancing Implementation (`tyk` repository)*: <https://github.com/TykTechnologies/tyk/blob/master/apidef/oas/upstream.go#L1354-L1372|`tyk/apidef/oas/upstream.go#L1354-L1372`>\\n‚Ä¢ *Dashboard GraphQL Schema for Service Discovery (`tyk-analytics` repository)*: <https://github.com/TykTechnologies/tyk-analytics/blob/master/graphql/schema.graphql|`tyk-analytics/graphql/schema.graphql`>\\n‚Ä¢ *Dashboard UI Test for Service Discovery (`tyk-analytics` repository)*: <https://github.com/TykTechnologies/tyk-analytics/blob/master/tests/ui/tests/specs/api_designer/oas_api_service_discovery_test.ts|`tyk-analytics/tests/ui/tests/specs/api_designer/oas_api_service_discovery_test.ts`>\\nWhen the upstream targets are the same for blue and green deployments, the core strategy for a blue-green deployment in Tyk remains similar, but the focus shifts from changing the upstream URL to changing other aspects of the API definition, such as middleware, plugins, or security policies. The goal is to have two different configurations for the same API and switch between them.\\n\\nHere are two primary approaches to achieve this, along with considerations for your requirement of not modifying the `ApiDefinition` custom resource:\\n\\n*Approach 1: Using Two Separate `ApiDefinition` Custom Resources*\\n\\nThis is the most common and straightforward approach, as described in the initial response. Even with the same upstream, you would have two distinct `ApiDefinition` resources:\\n\\n‚Ä¢ *Blue `ApiDefinition`*: Contains the current, stable configuration (e.g., a specific set of middleware). It uses the production `listen_path`, for example, `/my-service`.\\n‚Ä¢ *Green `ApiDefinition`*: Contains the new configuration you want to deploy. The `proxy.target_url` would be the same as the blue definition, but it would have different middleware, a new plugin, or other configuration changes. Initially, it would use a non-production `listen_path` like `/my-service-green` for testing.\\n\\nTo switch traffic, you would modify the `listen_path` in both `ApiDefinition` custom resources, swapping the production path to the green definition.\\n\\n*Regarding your question about not modifying the custom resource:* This approach *requires* modifying the `ApiDefinition` CRs to change the `listen_path`. If your goal is to avoid `kubectl apply` or CI/CD pipeline runs for the switch, this method is not ideal.\\n\\n*Approach 2: Using API Versioning within a Single `ApiDefinition`*\\n\\nThis approach is more aligned with your goal of not modifying the `ApiDefinition` custom resource to perform the switch. It involves using Tyk's built-in API versioning capabilities.\\n\\nHere's how it would work conceptually:\\n\\n1.  *Define Versions in a Single `ApiDefinition`*: You would create a single `ApiDefinition` that contains multiple versions. For example:\\n    ‚Ä¢ *Version \\\"blue\\\"*: This would be your stable version, marked as the `default` version.\\n    ‚Ä¢ *Version \\\"green\\\"*: This would be your new version with the updated configuration (e.g., different middleware).\\n\\n    Both versions would point to the same `proxy.target_url` but would have different configurations for other parts of the API definition.\\n\\n2.  *Switch the Default Version via the Dashboard API*: The key to this approach is that the switch from blue to green is not done by changing the Kubernetes resource. Instead, you would make an API call to the Tyk Dashboard to change the `default` version of the API from \\\"blue\\\" to \\\"green\\\".\\n\\n    ‚Ä¢ When \\\"blue\\\" is the default, traffic to `/my-service` is handled by the blue version's configuration.\\n    ‚Ä¢ When you switch the default to \\\"green\\\" via the API, the Tyk Gateway will perform a hot reload, and all new traffic to `/my-service` will be processed using the green version's configuration.\\n\\n*How this meets your requirement:*\\n\\n‚Ä¢ The `ApiDefinition` custom resource in your Git repository remains unchanged.\\n‚Ä¢ The switch is a runtime operation performed by calling the Tyk Dashboard API, which can be triggered from a script, a CI/CD pipeline, or even a manual process without changing any Kubernetes manifests.\\n\\n*Limitations and Confirmation:*\\n\\nUnfortunately, due to the limitations of the current environment, I cannot provide the exact API endpoint from the `tyk-analytics` (Dashboard) codebase to perform this switch. The provided environment only contains the `visor2` repository, not the source code for `tyk` or `tyk-analytics`.\\n\\nHowever, based on the design of the Tyk platform, this functionality should be available through the Dashboard's API. I recommend you consult the official Tyk documentation on API versioning or use a tool like Postman to inspect the Tyk Dashboard API for an endpoint that allows you to update an API definition, specifically to change the `default_version` field.\\n\\nIn summary, if you want to perform a blue-green deployment with the same upstream and without modifying the `ApiDefinition` custom resource, the recommended approach is to use *API versioning* and trigger the switch by *calling the Tyk Dashboard API* to change the default version.\\nCan I use kubernetes labels to redirect traffic instead of service discovery?\\nCan I use kubernetes labels to redirect traffic instead of service discovery?\\n\"},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"13b17c99-8c44-4caa-89a3-6a9b1168c803"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"13b17c99-8c44-4caa-89a3-6a9b1168c803"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"13b17c99-8c44-4caa-89a3-6a9b1168c803"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"no-issues-fallback","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"no-issues-fallback","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"extract-keys\":{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767903775113},\"fetch-issues\":{\"issues\":[]},\"history\":{\"extract-keys\":[{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767903775113}],\"fetch-issues\":[{\"issues\":[]}]}},\"outputs_history\":{\"extract-keys\":[{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767903775113}],\"fetch-issues\":[{\"issues\":[]}]},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"text\":\"<@U09T5KRLMPU> If using the Tyk Operator with OperatorContext to manage multiple environments within a cluster, how would I go about a blue-green deployment for a given environment?\\nA blue-green deployment with the Tyk Operator using `OperatorContext` is achieved by managing two distinct API definitions‚Äîone for the \\\"blue\\\" environment and one for the \\\"green\\\" environment‚Äîand manipulating their `listen_path` or domain to control traffic flow. The process relies on the Tyk Gateway's hot-reload capability to switch traffic seamlessly without downtime.\\n\\nHere‚Äôs a step-by-step breakdown of how this works:\\n\\n*1. Initial State (Blue Version Live)*\\n\\n‚Ä¢ You start with a `ApiDefinition` custom resource for your stable \\\"blue\\\" version. This definition points to your blue upstream service and has a public-facing `listen_path`, for example, `/my-service`.\\n‚Ä¢ The Tyk Operator detects this resource and creates the corresponding API definition in the Tyk Dashboard.\\n‚Ä¢ The Tyk Gateway loads this definition and routes all production traffic for `/my-service` to your blue environment.\\n\\n*2. Deploying the New Version (Green)*\\n\\n‚Ä¢ To deploy a new version, you create a second `ApiDefinition` resource for your \\\"green\\\" version. This definition will point to your new, green upstream service.\\n‚Ä¢ Initially, you configure this green version with a non-production `listen_path`, such as `/my-service-green`. This keeps it isolated from live traffic and allows for testing.\\n‚Ä¢ The Tyk Operator creates this second API definition in the Dashboard, and the Gateway loads it, making the green version accessible at `/my-service-green` for verification.\\n\\n*3. Switching Traffic to Green*\\n\\nWhen you are ready to switch production traffic to the new version, you perform the following steps by updating the `ApiDefinition` resources in Kubernetes:\\n\\n1.  *Update the Green API:* Change the `listen_path` of the green `ApiDefinition` from `/my-service-green` to the production path `/my-service`.\\n2.  *Update the Blue API:* Change the `listen_path` of the blue `ApiDefinition` from `/my-service` to a non-production path like `/my-service-blue`. Alternatively, you can deactivate the blue API by setting its `active` flag to `false`.\\n\\n‚Ä¢ The Tyk Operator applies these changes to the API definitions in the Tyk Dashboard.\\n‚Ä¢ The Dashboard then pushes these updates to the Tyk Gateways.\\n‚Ä¢ The Gateways perform a hot-reload of the configuration. This is a zero-downtime operation that atomically updates the routing rules. All new traffic to `/my-service` will now be directed to your green upstream service.\\n\\n*4. Rollback*\\n\\nIf you need to roll back to the blue version, you simply reverse the process: update the `ApiDefinition` resources to swap the `listen_path` values back to their original states. The Gateway will hot-reload again, and traffic will be routed to the blue environment.\\n\\nIn summary, the Tyk Operator orchestrates the blue-green deployment by managing two separate API definitions. The traffic switch is executed by updating the `listen_path` of these definitions, which triggers a seamless hot-reload in the Tyk Gateway, ensuring a zero-downtime transition.\\n\\n*References:*\\n\\n‚Ä¢ *Tyk Docs on Blue-Green Upgrades:* <https://github.com/TykTechnologies/tyk-docs/blob/main/developer-support/upgrading.mdx#L133-L139|https://github.com/TykTechnologies/tyk-docs/blob/main/developer-support/upgrading.mdx#L133-L139>\\n‚Ä¢ *Tyk Docs on API Versioning:* <https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/gateway-config-managing-oas.mdx#L628|https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/gateway-config-managing-oas.mdx#L628>\\n‚Ä¢ *Tyk Operator API Definition:* <https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-stack/tyk-operator/create-an-api.mdx#L1111-L1118|https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-stack/tyk-operator/create-an-api.mdx#L1111-L1118>\\nCan this be done without having to modify the APIDefinition custom resource?\\n<@U09T5KRLMPU>  Can this be done without having to modify the APIDefinition custom resource?\\nWhether a blue-green deployment can be achieved without modifying the `APIDefinition` custom resource depends entirely on the Tyk Gateway's DNS resolution and caching behavior for upstream targets. The proposed strategy of pointing the `target_url` to a stable Kubernetes service and then switching traffic by modifying the service's selectors is a common pattern, but its success hinges on how quickly the gateway detects the change in the underlying endpoints.\\n\\nHere are the two possibilities:\\n\\n1.  *This approach will work if* the Tyk Gateway is configured to have a very short Time-To-Live (TTL) for its DNS cache, or if it resolves the DNS for each incoming request. In this scenario, when you update the Kubernetes service to point to the 'green' deployment's pods, the gateway would quickly pick up the new IP addresses and start routing traffic to them.\\n\\n2.  *This approach will NOT work if* the Tyk Gateway caches DNS lookups for an extended period. If the gateway resolves the service's DNS name to a set of IP addresses and caches that result, it will continue to send traffic to the old 'blue' pods, even after the service's selectors have been updated. The traffic would not shift to the 'green' deployment until the gateway's DNS cache expires or it is forced to reload.\\n\\nThe reason that modifying the `APIDefinition` (e.g., changing the `listen_path`) is the recommended and guaranteed method is because this action triggers a hot reload of the gateway's configuration. A hot reload forces the gateway to discard its old configuration and caches, including any cached DNS entries for upstream targets, and build a new routing map. This ensures that the change in traffic routing is immediate and predictable.\\n\\n*In summary:* while it is theoretically possible to perform a blue-green deployment using DNS switching, it is not a guaranteed method and depends on the gateway's specific, and often undocumented, DNS caching implementation. The reliable and officially supported method is to manage two distinct `APIDefinition` resources and manipulate their configurations to control traffic flow, as this leverages the gateway's hot reload capability for a seamless transition.\\n<@U09T5KRLMPU> - In my scenario, the upstream targets are the same between the blue and green deployments.  How does this change the approach?\\nYes, a blue-green deployment can be achieved without modifying the `APIDefinition` custom resource by using Tyk's *service discovery* feature.\\n\\nThis approach allows you to dynamically control traffic flow by updating an external endpoint rather than changing the static API configuration in Kubernetes.\\n\\n*How It Works:*\\n\\n1.  *Configure Service Discovery*: In your API definition, instead of providing a static list of upstream targets, you enable the service discovery feature. This is configured in the `proxy.service_discovery` section of the API definition. The most important field is `query_endpoint`, which points to an HTTP endpoint that you control.\\n\\n2.  *Externalize Upstream Targets*: The `query_endpoint` is responsible for returning a JSON response containing the list of upstream targets (e.g., the IP addresses or hostnames of your services). The Tyk Gateway will periodically call this endpoint to refresh its target list.\\n\\n3.  *Perform the Blue-Green Switch*: To switch traffic from your \\\"blue\\\" environment to your \\\"green\\\" environment, you simply update the response of your `query_endpoint`. \\n    ‚Ä¢ Initially, the endpoint returns the addresses of your \\\"blue\\\" services.\\n    ‚Ä¢ When you're ready to switch, you change the endpoint to return the addresses of your \\\"green\\\" services.\\n\\nThe Tyk Gateway will pick up this change on its next refresh cycle and start routing all new traffic to the green environment, achieving a seamless, zero-downtime deployment.\\n\\n*Why This Works Without Modifying the `APIDefinition`:*\\n\\nThe `APIDefinition` custom resource only contains the configuration for *how* to discover the upstream targets (i.e., the URL of the `query_endpoint`). The actual list of targets is managed externally. This separation allows you to change where traffic is routed without needing to apply any changes to your Kubernetes resources, thus avoiding a reload of the API definition.\\n\\n*Alternative Considered: Weighted Load Balancing*\\n\\nWhile Tyk also supports weighted load balancing, it is not suitable for this specific use case. The weights for each upstream target are defined directly within the `APIDefinition`. Therefore, shifting traffic by adjusting weights would require modifying the custom resource, which is what you want to avoid.\\n\\n*Code References:*\\n\\n‚Ä¢ *Service Discovery Configuration Struct (`tyk` repository)*: <https://github.com/TykTechnologies/tyk/blob/master/apidef/api_definitions.go#L615-L628|`tyk/apidef/api_definitions.go#L615-L628`>\\n‚Ä¢ *Service Discovery Fetch Logic (`tyk` repository)*: <https://github.com/TykTechnologies/tyk/blob/master/gateway/service_discovery.go#L300-L309|`tyk/gateway/service_discovery.go#L300-L309`>\\n‚Ä¢ *Weighted Load Balancing Implementation (`tyk` repository)*: <https://github.com/TykTechnologies/tyk/blob/master/apidef/oas/upstream.go#L1354-L1372|`tyk/apidef/oas/upstream.go#L1354-L1372`>\\n‚Ä¢ *Dashboard GraphQL Schema for Service Discovery (`tyk-analytics` repository)*: <https://github.com/TykTechnologies/tyk-analytics/blob/master/graphql/schema.graphql|`tyk-analytics/graphql/schema.graphql`>\\n‚Ä¢ *Dashboard UI Test for Service Discovery (`tyk-analytics` repository)*: <https://github.com/TykTechnologies/tyk-analytics/blob/master/tests/ui/tests/specs/api_designer/oas_api_service_discovery_test.ts|`tyk-analytics/tests/ui/tests/specs/api_designer/oas_api_service_discovery_test.ts`>\\nWhen the upstream targets are the same for blue and green deployments, the core strategy for a blue-green deployment in Tyk remains similar, but the focus shifts from changing the upstream URL to changing other aspects of the API definition, such as middleware, plugins, or security policies. The goal is to have two different configurations for the same API and switch between them.\\n\\nHere are two primary approaches to achieve this, along with considerations for your requirement of not modifying the `ApiDefinition` custom resource:\\n\\n*Approach 1: Using Two Separate `ApiDefinition` Custom Resources*\\n\\nThis is the most common and straightforward approach, as described in the initial response. Even with the same upstream, you would have two distinct `ApiDefinition` resources:\\n\\n‚Ä¢ *Blue `ApiDefinition`*: Contains the current, stable configuration (e.g., a specific set of middleware). It uses the production `listen_path`, for example, `/my-service`.\\n‚Ä¢ *Green `ApiDefinition`*: Contains the new configuration you want to deploy. The `proxy.target_url` would be the same as the blue definition, but it would have different middleware, a new plugin, or other configuration changes. Initially, it would use a non-production `listen_path` like `/my-service-green` for testing.\\n\\nTo switch traffic, you would modify the `listen_path` in both `ApiDefinition` custom resources, swapping the production path to the green definition.\\n\\n*Regarding your question about not modifying the custom resource:* This approach *requires* modifying the `ApiDefinition` CRs to change the `listen_path`. If your goal is to avoid `kubectl apply` or CI/CD pipeline runs for the switch, this method is not ideal.\\n\\n*Approach 2: Using API Versioning within a Single `ApiDefinition`*\\n\\nThis approach is more aligned with your goal of not modifying the `ApiDefinition` custom resource to perform the switch. It involves using Tyk's built-in API versioning capabilities.\\n\\nHere's how it would work conceptually:\\n\\n1.  *Define Versions in a Single `ApiDefinition`*: You would create a single `ApiDefinition` that contains multiple versions. For example:\\n    ‚Ä¢ *Version \\\"blue\\\"*: This would be your stable version, marked as the `default` version.\\n    ‚Ä¢ *Version \\\"green\\\"*: This would be your new version with the updated configuration (e.g., different middleware).\\n\\n    Both versions would point to the same `proxy.target_url` but would have different configurations for other parts of the API definition.\\n\\n2.  *Switch the Default Version via the Dashboard API*: The key to this approach is that the switch from blue to green is not done by changing the Kubernetes resource. Instead, you would make an API call to the Tyk Dashboard to change the `default` version of the API from \\\"blue\\\" to \\\"green\\\".\\n\\n    ‚Ä¢ When \\\"blue\\\" is the default, traffic to `/my-service` is handled by the blue version's configuration.\\n    ‚Ä¢ When you switch the default to \\\"green\\\" via the API, the Tyk Gateway will perform a hot reload, and all new traffic to `/my-service` will be processed using the green version's configuration.\\n\\n*How this meets your requirement:*\\n\\n‚Ä¢ The `ApiDefinition` custom resource in your Git repository remains unchanged.\\n‚Ä¢ The switch is a runtime operation performed by calling the Tyk Dashboard API, which can be triggered from a script, a CI/CD pipeline, or even a manual process without changing any Kubernetes manifests.\\n\\n*Limitations and Confirmation:*\\n\\nUnfortunately, due to the limitations of the current environment, I cannot provide the exact API endpoint from the `tyk-analytics` (Dashboard) codebase to perform this switch. The provided environment only contains the `visor2` repository, not the source code for `tyk` or `tyk-analytics`.\\n\\nHowever, based on the design of the Tyk platform, this functionality should be available through the Dashboard's API. I recommend you consult the official Tyk documentation on API versioning or use a tool like Postman to inspect the Tyk Dashboard API for an endpoint that allows you to update an API definition, specifically to change the `default_version` field.\\n\\nIn summary, if you want to perform a blue-green deployment with the same upstream and without modifying the `ApiDefinition` custom resource, the recommended approach is to use *API versioning* and trigger the switch by *calling the Tyk Dashboard API* to change the default version.\\nCan I use kubernetes labels to redirect traffic instead of service discovery?\\nCan I use kubernetes labels to redirect traffic instead of service discovery?\\n\",\"jql\":\"\",\"custom_field_aliases\":{},\"include_comments\":true,\"max_issues\":3,\"ticket_prefixes\":[\"TT\",\"DX\"]},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.check","attributes":{"visor.check.id":"no-issues-fallback","visor.check.output":"\"<jira_context><issue_count>0</issue_count><message>No Jira issue keys found in the provided text</message></jira_context>\""},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"13b17c99-8c44-4caa-89a3-6a9b1168c803"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"13b17c99-8c44-4caa-89a3-6a9b1168c803"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"13b17c99-8c44-4caa-89a3-6a9b1168c803"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"13b17c99-8c44-4caa-89a3-6a9b1168c803"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"13b17c99-8c44-4caa-89a3-6a9b1168c803"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.output":"{\"data\":[],\"count\":0}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"2b953423-829c-4120-8f8e-d2017c9538aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"2b953423-829c-4120-8f8e-d2017c9538aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"2b953423-829c-4120-8f8e-d2017c9538aa"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"no-tickets-fallback","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"no-tickets-fallback","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"extract-ids\":{\"data\":[],\"count\":0,\"ts\":1767903775298},\"fetch-tickets\":{\"issues\":[]},\"history\":{\"extract-ids\":[{\"data\":[],\"count\":0,\"ts\":1767903775298}],\"fetch-tickets\":[{\"issues\":[]}]}},\"outputs_history\":{\"extract-ids\":[{\"data\":[],\"count\":0,\"ts\":1767903775298}],\"fetch-tickets\":[{\"issues\":[]}]},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"include_comments\":true,\"max_tickets\":5,\"download_assets\":true,\"cache_dir\":\".cache/zendesk\",\"text\":\"<@U09T5KRLMPU> If using the Tyk Operator with OperatorContext to manage multiple environments within a cluster, how would I go about a blue-green deployment for a given environment?\\nA blue-green deployment with the Tyk Operator using `OperatorContext` is achieved by managing two distinct API definitions‚Äîone for the \\\"blue\\\" environment and one for the \\\"green\\\" environment‚Äîand manipulating their `listen_path` or domain to control traffic flow. The process relies on the Tyk Gateway's hot-reload capability to switch traffic seamlessly without downtime.\\n\\nHere‚Äôs a step-by-step breakdown of how this works:\\n\\n*1. Initial State (Blue Version Live)*\\n\\n‚Ä¢ You start with a `ApiDefinition` custom resource for your stable \\\"blue\\\" version. This definition points to your blue upstream service and has a public-facing `listen_path`, for example, `/my-service`.\\n‚Ä¢ The Tyk Operator detects this resource and creates the corresponding API definition in the Tyk Dashboard.\\n‚Ä¢ The Tyk Gateway loads this definition and routes all production traffic for `/my-service` to your blue environment.\\n\\n*2. Deploying the New Version (Green)*\\n\\n‚Ä¢ To deploy a new version, you create a second `ApiDefinition` resource for your \\\"green\\\" version. This definition will point to your new, green upstream service.\\n‚Ä¢ Initially, you configure this green version with a non-production `listen_path`, such as `/my-service-green`. This keeps it isolated from live traffic and allows for testing.\\n‚Ä¢ The Tyk Operator creates this second API definition in the Dashboard, and the Gateway loads it, making the green version accessible at `/my-service-green` for verification.\\n\\n*3. Switching Traffic to Green*\\n\\nWhen you are ready to switch production traffic to the new version, you perform the following steps by updating the `ApiDefinition` resources in Kubernetes:\\n\\n1.  *Update the Green API:* Change the `listen_path` of the green `ApiDefinition` from `/my-service-green` to the production path `/my-service`.\\n2.  *Update the Blue API:* Change the `listen_path` of the blue `ApiDefinition` from `/my-service` to a non-production path like `/my-service-blue`. Alternatively, you can deactivate the blue API by setting its `active` flag to `false`.\\n\\n‚Ä¢ The Tyk Operator applies these changes to the API definitions in the Tyk Dashboard.\\n‚Ä¢ The Dashboard then pushes these updates to the Tyk Gateways.\\n‚Ä¢ The Gateways perform a hot-reload of the configuration. This is a zero-downtime operation that atomically updates the routing rules. All new traffic to `/my-service` will now be directed to your green upstream service.\\n\\n*4. Rollback*\\n\\nIf you need to roll back to the blue version, you simply reverse the process: update the `ApiDefinition` resources to swap the `listen_path` values back to their original states. The Gateway will hot-reload again, and traffic will be routed to the blue environment.\\n\\nIn summary, the Tyk Operator orchestrates the blue-green deployment by managing two separate API definitions. The traffic switch is executed by updating the `listen_path` of these definitions, which triggers a seamless hot-reload in the Tyk Gateway, ensuring a zero-downtime transition.\\n\\n*References:*\\n\\n‚Ä¢ *Tyk Docs on Blue-Green Upgrades:* <https://github.com/TykTechnologies/tyk-docs/blob/main/developer-support/upgrading.mdx#L133-L139|https://github.com/TykTechnologies/tyk-docs/blob/main/developer-support/upgrading.mdx#L133-L139>\\n‚Ä¢ *Tyk Docs on API Versioning:* <https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/gateway-config-managing-oas.mdx#L628|https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/gateway-config-managing-oas.mdx#L628>\\n‚Ä¢ *Tyk Operator API Definition:* <https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-stack/tyk-operator/create-an-api.mdx#L1111-L1118|https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-stack/tyk-operator/create-an-api.mdx#L1111-L1118>\\nCan this be done without having to modify the APIDefinition custom resource?\\n<@U09T5KRLMPU>  Can this be done without having to modify the APIDefinition custom resource?\\nWhether a blue-green deployment can be achieved without modifying the `APIDefinition` custom resource depends entirely on the Tyk Gateway's DNS resolution and caching behavior for upstream targets. The proposed strategy of pointing the `target_url` to a stable Kubernetes service and then switching traffic by modifying the service's selectors is a common pattern, but its success hinges on how quickly the gateway detects the change in the underlying endpoints.\\n\\nHere are the two possibilities:\\n\\n1.  *This approach will work if* the Tyk Gateway is configured to have a very short Time-To-Live (TTL) for its DNS cache, or if it resolves the DNS for each incoming request. In this scenario, when you update the Kubernetes service to point to the 'green' deployment's pods, the gateway would quickly pick up the new IP addresses and start routing traffic to them.\\n\\n2.  *This approach will NOT work if* the Tyk Gateway caches DNS lookups for an extended period. If the gateway resolves the service's DNS name to a set of IP addresses and caches that result, it will continue to send traffic to the old 'blue' pods, even after the service's selectors have been updated. The traffic would not shift to the 'green' deployment until the gateway's DNS cache expires or it is forced to reload.\\n\\nThe reason that modifying the `APIDefinition` (e.g., changing the `listen_path`) is the recommended and guaranteed method is because this action triggers a hot reload of the gateway's configuration. A hot reload forces the gateway to discard its old configuration and caches, including any cached DNS entries for upstream targets, and build a new routing map. This ensures that the change in traffic routing is immediate and predictable.\\n\\n*In summary:* while it is theoretically possible to perform a blue-green deployment using DNS switching, it is not a guaranteed method and depends on the gateway's specific, and often undocumented, DNS caching implementation. The reliable and officially supported method is to manage two distinct `APIDefinition` resources and manipulate their configurations to control traffic flow, as this leverages the gateway's hot reload capability for a seamless transition.\\n<@U09T5KRLMPU> - In my scenario, the upstream targets are the same between the blue and green deployments.  How does this change the approach?\\nYes, a blue-green deployment can be achieved without modifying the `APIDefinition` custom resource by using Tyk's *service discovery* feature.\\n\\nThis approach allows you to dynamically control traffic flow by updating an external endpoint rather than changing the static API configuration in Kubernetes.\\n\\n*How It Works:*\\n\\n1.  *Configure Service Discovery*: In your API definition, instead of providing a static list of upstream targets, you enable the service discovery feature. This is configured in the `proxy.service_discovery` section of the API definition. The most important field is `query_endpoint`, which points to an HTTP endpoint that you control.\\n\\n2.  *Externalize Upstream Targets*: The `query_endpoint` is responsible for returning a JSON response containing the list of upstream targets (e.g., the IP addresses or hostnames of your services). The Tyk Gateway will periodically call this endpoint to refresh its target list.\\n\\n3.  *Perform the Blue-Green Switch*: To switch traffic from your \\\"blue\\\" environment to your \\\"green\\\" environment, you simply update the response of your `query_endpoint`. \\n    ‚Ä¢ Initially, the endpoint returns the addresses of your \\\"blue\\\" services.\\n    ‚Ä¢ When you're ready to switch, you change the endpoint to return the addresses of your \\\"green\\\" services.\\n\\nThe Tyk Gateway will pick up this change on its next refresh cycle and start routing all new traffic to the green environment, achieving a seamless, zero-downtime deployment.\\n\\n*Why This Works Without Modifying the `APIDefinition`:*\\n\\nThe `APIDefinition` custom resource only contains the configuration for *how* to discover the upstream targets (i.e., the URL of the `query_endpoint`). The actual list of targets is managed externally. This separation allows you to change where traffic is routed without needing to apply any changes to your Kubernetes resources, thus avoiding a reload of the API definition.\\n\\n*Alternative Considered: Weighted Load Balancing*\\n\\nWhile Tyk also supports weighted load balancing, it is not suitable for this specific use case. The weights for each upstream target are defined directly within the `APIDefinition`. Therefore, shifting traffic by adjusting weights would require modifying the custom resource, which is what you want to avoid.\\n\\n*Code References:*\\n\\n‚Ä¢ *Service Discovery Configuration Struct (`tyk` repository)*: <https://github.com/TykTechnologies/tyk/blob/master/apidef/api_definitions.go#L615-L628|`tyk/apidef/api_definitions.go#L615-L628`>\\n‚Ä¢ *Service Discovery Fetch Logic (`tyk` repository)*: <https://github.com/TykTechnologies/tyk/blob/master/gateway/service_discovery.go#L300-L309|`tyk/gateway/service_discovery.go#L300-L309`>\\n‚Ä¢ *Weighted Load Balancing Implementation (`tyk` repository)*: <https://github.com/TykTechnologies/tyk/blob/master/apidef/oas/upstream.go#L1354-L1372|`tyk/apidef/oas/upstream.go#L1354-L1372`>\\n‚Ä¢ *Dashboard GraphQL Schema for Service Discovery (`tyk-analytics` repository)*: <https://github.com/TykTechnologies/tyk-analytics/blob/master/graphql/schema.graphql|`tyk-analytics/graphql/schema.graphql`>\\n‚Ä¢ *Dashboard UI Test for Service Discovery (`tyk-analytics` repository)*: <https://github.com/TykTechnologies/tyk-analytics/blob/master/tests/ui/tests/specs/api_designer/oas_api_service_discovery_test.ts|`tyk-analytics/tests/ui/tests/specs/api_designer/oas_api_service_discovery_test.ts`>\\nWhen the upstream targets are the same for blue and green deployments, the core strategy for a blue-green deployment in Tyk remains similar, but the focus shifts from changing the upstream URL to changing other aspects of the API definition, such as middleware, plugins, or security policies. The goal is to have two different configurations for the same API and switch between them.\\n\\nHere are two primary approaches to achieve this, along with considerations for your requirement of not modifying the `ApiDefinition` custom resource:\\n\\n*Approach 1: Using Two Separate `ApiDefinition` Custom Resources*\\n\\nThis is the most common and straightforward approach, as described in the initial response. Even with the same upstream, you would have two distinct `ApiDefinition` resources:\\n\\n‚Ä¢ *Blue `ApiDefinition`*: Contains the current, stable configuration (e.g., a specific set of middleware). It uses the production `listen_path`, for example, `/my-service`.\\n‚Ä¢ *Green `ApiDefinition`*: Contains the new configuration you want to deploy. The `proxy.target_url` would be the same as the blue definition, but it would have different middleware, a new plugin, or other configuration changes. Initially, it would use a non-production `listen_path` like `/my-service-green` for testing.\\n\\nTo switch traffic, you would modify the `listen_path` in both `ApiDefinition` custom resources, swapping the production path to the green definition.\\n\\n*Regarding your question about not modifying the custom resource:* This approach *requires* modifying the `ApiDefinition` CRs to change the `listen_path`. If your goal is to avoid `kubectl apply` or CI/CD pipeline runs for the switch, this method is not ideal.\\n\\n*Approach 2: Using API Versioning within a Single `ApiDefinition`*\\n\\nThis approach is more aligned with your goal of not modifying the `ApiDefinition` custom resource to perform the switch. It involves using Tyk's built-in API versioning capabilities.\\n\\nHere's how it would work conceptually:\\n\\n1.  *Define Versions in a Single `ApiDefinition`*: You would create a single `ApiDefinition` that contains multiple versions. For example:\\n    ‚Ä¢ *Version \\\"blue\\\"*: This would be your stable version, marked as the `default` version.\\n    ‚Ä¢ *Version \\\"green\\\"*: This would be your new version with the updated configuration (e.g., different middleware).\\n\\n    Both versions would point to the same `proxy.target_url` but would have different configurations for other parts of the API definition.\\n\\n2.  *Switch the Default Version via the Dashboard API*: The key to this approach is that the switch from blue to green is not done by changing the Kubernetes resource. Instead, you would make an API call to the Tyk Dashboard to change the `default` version of the API from \\\"blue\\\" to \\\"green\\\".\\n\\n    ‚Ä¢ When \\\"blue\\\" is the default, traffic to `/my-service` is handled by the blue version's configuration.\\n    ‚Ä¢ When you switch the default to \\\"green\\\" via the API, the Tyk Gateway will perform a hot reload, and all new traffic to `/my-service` will be processed using the green version's configuration.\\n\\n*How this meets your requirement:*\\n\\n‚Ä¢ The `ApiDefinition` custom resource in your Git repository remains unchanged.\\n‚Ä¢ The switch is a runtime operation performed by calling the Tyk Dashboard API, which can be triggered from a script, a CI/CD pipeline, or even a manual process without changing any Kubernetes manifests.\\n\\n*Limitations and Confirmation:*\\n\\nUnfortunately, due to the limitations of the current environment, I cannot provide the exact API endpoint from the `tyk-analytics` (Dashboard) codebase to perform this switch. The provided environment only contains the `visor2` repository, not the source code for `tyk` or `tyk-analytics`.\\n\\nHowever, based on the design of the Tyk platform, this functionality should be available through the Dashboard's API. I recommend you consult the official Tyk documentation on API versioning or use a tool like Postman to inspect the Tyk Dashboard API for an endpoint that allows you to update an API definition, specifically to change the `default_version` field.\\n\\nIn summary, if you want to perform a blue-green deployment with the same upstream and without modifying the `ApiDefinition` custom resource, the recommended approach is to use *API versioning* and trigger the switch by *calling the Tyk Dashboard API* to change the default version.\\nCan I use kubernetes labels to redirect traffic instead of service discovery?\\nCan I use kubernetes labels to redirect traffic instead of service discovery?\\n\"},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.check","attributes":{"visor.check.id":"no-tickets-fallback","visor.check.output":"\"<zendesk_context><ticket_count>0</ticket_count><message>No Zendesk ticket IDs found in the provided text</message></zendesk_context>\""},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"2b953423-829c-4120-8f8e-d2017c9538aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"2b953423-829c-4120-8f8e-d2017c9538aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"2b953423-829c-4120-8f8e-d2017c9538aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"2b953423-829c-4120-8f8e-d2017c9538aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"2b953423-829c-4120-8f8e-d2017c9538aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"2b953423-829c-4120-8f8e-d2017c9538aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"2b953423-829c-4120-8f8e-d2017c9538aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"2b953423-829c-4120-8f8e-d2017c9538aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"2b953423-829c-4120-8f8e-d2017c9538aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"2b953423-829c-4120-8f8e-d2017c9538aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"2b953423-829c-4120-8f8e-d2017c9538aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"route-intent","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.input.context":"{\"pr\":{\"number\":0,\"title\":\"Local Analysis: feat/slack-frontend-v2 (72 modified)\",\"author\":\"Leonid Bugaev\",\"branch\":\"feat/slack-frontend-v2\",\"base\":\"main\"},\"files\":[{\"filename\":\"CLAUDE.md\",\"additions\":14,\"deletions\":1,\"changes\":15,\"patch\":\"diff --git a/CLAUDE.md b/CLAUDE.md\\nindex 009271f9..ae68c57e 100644\\n--- a/CLAUDE.md\\n+++ b/CLAUDE.md\\n@@ -150,4 +150,17 @@ Configuration supports:\\n    - Safe JSON parsing: `try { JSON.parse(output) } catch(e) { log(\\\"Error:\\\", e) }`\\n    - Validate structure: `log(\\\"Is array?\\\", Array.isArray(outputs[\\\"check-name\\\"]));`\\n \\n+6. **Tracing with OTel/Jaeger**:\\n+   - Enable telemetry: `VISOR_TELEMETRY_ENABLED=true`, `VISOR_TELEMETRY_SINK=otlp`,\\n+     `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://localhost:4318/v1/traces`\\n+   - Root span: `visor.run` (one per CLI/Slack execution)\\n+   - State spans: `engine.state.*` with `wave`, `wave_kind`, `session_id`\\n+   - Check spans: `visor.check.<checkId>` with `visor.check.id`, `visor.check.type`,\\n+     `visor.foreach.index` (for map fanout)\\n+   - Routing decisions: `visor.routing` events attached to the active state span; fields\\n+     include `trigger`, `action`, `source`, `target`, `scope`, `goto_event` (repeats\\n+     across waves show routing loops)\\n+   - Wave visibility: `engine.state.level_dispatch` includes `level_size` and\\n+     `level_checks_preview` for the planned wave\\n+\\n See `docs/debugging.md` for comprehensive debugging guide.\\n\",\"status\":\"modified\"},{\"filename\":\"README.md\",\"additions\":7,\"deletions\":1,\"changes\":8,\"patch\":\"diff --git a/README.md b/README.md\\nindex 41131861..d92f02cc 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -534,6 +534,12 @@ steps:\\n     session_mode: append    # Shares history for full conversation\\n ```\\n \\n+You can also reuse the **same check‚Äôs** session when it loops back to itself, using:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` with `session_mode: append`\\n+\\n+See the standalone example at `examples/session-reuse-self.yaml` and the detailed guide in [docs/advanced-ai.md](docs/advanced-ai.md).\\n+\\n Learn more: [docs/advanced-ai.md](docs/advanced-ai.md)\\n \\n ## üìã Schema-Template System\\n\",\"status\":\"modified\"},{\"filename\":\"docs/advanced-ai.md\",\"additions\":61,\"deletions\":2,\"changes\":63,\"patch\":\"diff --git a/docs/advanced-ai.md b/docs/advanced-ai.md\\nindex bb8e684a..879285ed 100644\\n--- a/docs/advanced-ai.md\\n+++ b/docs/advanced-ai.md\\n@@ -1,7 +1,7 @@\\n ## üß† Advanced AI Features\\n \\n ### AI Session Reuse\\n-Use `reuse_ai_session: true` on dependent checks to continue conversation context with the AI across checks. This improves follow‚Äëups and consistency.\\n+Use `reuse_ai_session` on checks to continue conversation context with the AI across steps. This improves follow‚Äëups and consistency for follow‚Äëon analysis and chat‚Äëstyle flows.\\n \\n **Session Modes:**\\n - **`clone` (default)**: Creates a copy of the conversation history. Each check gets an independent session with the same starting context. Changes made by one check don't affect others.\\n@@ -26,6 +26,65 @@ steps:\\n     depends_on: [security-remediation]\\n     reuse_ai_session: true\\n     session_mode: append  # Share history - sees full conversation\\n+\\n+#### Reusing your own session: `reuse_ai_session: self`\\n+\\n+Sometimes the step you want to loop back into is the AI step itself (e.g. Slack assistants or multi‚Äëturn internal tools). For that case you can use:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` ‚Äì the step reuses its **own** Probe session when it runs again in the same engine run.\\n+- `session_mode: append` ‚Äì makes the follow‚Äëup behave like a normal conversation turn.\\n+\\n+On the first run of the step, Visor creates a new ProbeAgent session and registers it. If routing (`on_success.goto`, `goto_js`, etc.) later jumps back to the same step within the same run, the engine:\\n+\\n+- Finds the last result for that step in the current run.\\n+- Reads the `sessionId` stored in the result.\\n+- Calls the AI provider again using `executeReviewWithSessionReuse` with that session id.\\n+\\n+Simple example (no transport wiring, just CLI/tests):\\n+\\n+```yaml\\n+version: \\\"2.0\\\"\\n+\\n+steps:\\n+  seed:\\n+    type: script\\n+    content: |\\n+      return { text: \\\"hello from seed\\\" };\\n+\\n+  convo:\\n+    type: ai\\n+    depends_on: [seed]\\n+    reuse_ai_session: self\\n+    session_mode: append\\n+    ai:\\n+      provider: mock\\n+      model: mock\\n+      disableTools: true\\n+      allowedTools: []\\n+      system_prompt: \\\"You are a tiny echo assistant.\\\"\\n+    prompt: |\\n+      Seed message: {{ outputs['seed'].text }}\\n+\\n+      Past convo outputs in this run:\\n+      {% assign hist = outputs_history['convo'] | default: empty %}\\n+      {% if hist and hist.size > 0 %}\\n+      {% for h in hist %}\\n+      - Previous reply {{ forloop.index }}.\\n+      {% endfor %}\\n+      {% else %}\\n+      - No previous replies yet.\\n+      {% endif %}\\n+    on_success:\\n+      goto_js: |\\n+        // Example: re‚Äëenter this step up to 3 times in a single run\\n+        return attempt < 3 ? 'convo' : null;\\n+```\\n+\\n+The corresponding testable example lives at:\\n+\\n+- `examples/session-reuse-self.yaml`\\n+\\n+This keeps the configuration small but shows how to wire `reuse_ai_session: self` and `session_mode: append` without touching higher‚Äëlevel workflows like `tyk-assistant`.\\n ```\\n \\n **When to use each mode:**\\n\",\"status\":\"modified\"},{\"filename\":\"docs/recipes.md\",\"additions\":149,\"deletions\":3,\"changes\":152,\"patch\":\"diff --git a/docs/recipes.md b/docs/recipes.md\\nindex 1015bc6c..1089e9a5 100644\\n--- a/docs/recipes.md\\n+++ b/docs/recipes.md\\n@@ -321,8 +321,154 @@ Tip: When you define a JSON Schema, you generally do **not** need to tell the mo\\n - Avoid noisy fallbacks like `(outputs['x']?.kind ?? '') === 'status'` when `outputs['x']?.kind === 'status'` is equivalent.\\n - These conventions apply uniformly to any provider (`ai`, `command`, `script`, `github`, `http_client`, etc).\\n \\n+### Command step best practices\\n+\\n+When using `type: command` steps:\\n+\\n+**Avoid external tool dependencies** like `jq`, `yq`, `python`, etc.:\\n+- They may not be installed in all environments (GitHub Actions, Docker, CI)\\n+- Use `transform_js` to parse and transform output instead\\n+- Keep shell commands simple: `grep`, `sed`, `awk`, `sort`, `head` are universally available\\n+\\n+```yaml\\n+# Bad - requires jq\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | jq -R -s 'split(\\\"\\\\n\\\")'\\n+  parseJson: true\\n+\\n+# Good - use transform_js for parsing\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | sort -u\\n+  transform_js: |\\n+    const lines = (output || '').trim().split('\\\\n').filter(Boolean);\\n+    return { data: lines, count: lines.length };\\n+```\\n+\\n+**Prefer line-separated output** over JSON from shell:\\n+- Simple to parse with `transform_js`\\n+- No need for `parseJson: true`\\n+- More robust across different shells/environments\\n+\\n+**Use transform_js for structured output**:\\n+- The sandbox provides `output` (command stdout as string)\\n+- Return an object with the fields you need\\n+- Works consistently across all environments\\n+\\n+### Testing workflows with `--no-mocks`\\n+\\n+The `--no-mocks` flag runs your test cases with real providers instead of injecting mock responses. This is essential for:\\n+\\n+1. **Debugging integration issues** - See actual API responses and errors\\n+2. **Capturing realistic mock data** - Get real output to copy into your test cases\\n+3. **Validating credentials** - Verify environment variables are set correctly\\n+4. **Developing new workflows** - Build tests incrementally with real data\\n+\\n+#### Basic usage\\n+\\n+```bash\\n+# Run all test cases with real providers\\n+visor test --config my-workflow.yaml --no-mocks\\n+\\n+# Run a specific test case with real providers\\n+visor test --config my-workflow.yaml --no-mocks --only \\\"my-test-case\\\"\\n+```\\n+\\n+#### Suggested mocks output\\n+\\n+When running with `--no-mocks`, Visor captures each step's output and prints it as YAML you can copy directly into your test case:\\n+\\n+```\\n+üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)\\n+   Step outputs will be captured and printed as suggested mocks\\n+\\n+... test execution ...\\n+\\n+üìã Suggested mocks (copy to your test case):\\n+mocks:\\n+  extract-keys:\\n+    data:\\n+      - PROJ-123\\n+      - DEV-456\\n+    count: 2\\n+  fetch-issues:\\n+    data:\\n+      - key: PROJ-123\\n+        summary: Fix authentication bug\\n+        status: In Progress\\n+```\\n+\\n+Copy the YAML under `mocks:` into your test case's `mocks:` section.\\n+\\n+#### Workflow for building tests\\n+\\n+1. **Start with a minimal test case** (no mocks):\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+   ```\\n+\\n+2. **Run with `--no-mocks`** to capture real outputs:\\n+   ```bash\\n+   visor test --config workflow.yaml --no-mocks --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+3. **Copy the suggested mocks** into your test case:\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+         mocks:\\n+           extract-keys:\\n+             data: [\\\"PROJ-123\\\"]\\n+             count: 1\\n+           # ... rest of captured mocks\\n+   ```\\n+\\n+4. **Add assertions** based on the real data:\\n+   ```yaml\\n+         expect:\\n+           workflow_output:\\n+             - path: issue_count\\n+               equals: 1\\n+   ```\\n+\\n+5. **Run normally** to verify mocks work:\\n+   ```bash\\n+   visor test --config workflow.yaml --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+#### Debugging with `--no-mocks`\\n+\\n+When a test fails with mocks, use `--no-mocks` to see what's actually happening:\\n+\\n+```bash\\n+# See real API responses and errors\\n+visor test --config workflow.yaml --no-mocks --only \\\"failing-test\\\"\\n+\\n+# Common issues revealed:\\n+# - Missing or expired credentials\\n+# - API endpoint changes\\n+# - Unexpected response formats\\n+# - Network/timeout issues\\n+```\\n+\\n+The real error messages and responses help identify whether the issue is with your mocks or the actual integration.\\n+\\n ### More examples\\n \\n-- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags  \\n-- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration  \\n+- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags\\n+- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration\\n - `examples/` ‚Äì MCP, Jira, and advanced configs\\n\",\"status\":\"modified\"},{\"filename\":\"package-lock.json\",\"additions\":252,\"deletions\":9,\"changes\":261,\"patch\":\"diff --git a/package-lock.json b/package-lock.json\\nindex 5fb3ad89..e5b10d27 100644\\n--- a/package-lock.json\\n+++ b/package-lock.json\\n@@ -7,6 +7,7 @@\\n     \\\"\\\": {\\n       \\\"name\\\": \\\"@probelabs/visor\\\",\\n       \\\"version\\\": \\\"0.1.42\\\",\\n+      \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"MIT\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@actions/core\\\": \\\"^1.11.1\\\",\\n@@ -16,7 +17,7 @@\\n         \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n         \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n         \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n         \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n         \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -54,6 +55,7 @@\\n         \\\"husky\\\": \\\"^9.1.7\\\",\\n         \\\"jest\\\": \\\"^30.1.3\\\",\\n         \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+        \\\"patch-package\\\": \\\"^8.0.1\\\",\\n         \\\"prettier\\\": \\\"^3.6.2\\\",\\n         \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n         \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n@@ -5942,9 +5944,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/maid\\\": {\\n-      \\\"version\\\": \\\"0.0.21\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.21.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-H3EA2tDjWsAPgXZAwfHrNoEXnHmQFS9+JGDK/YpeiF3g3FxPgnWBgtgYPgtNuETQsEXxR1RzDgQVJL2JHWRRHw==\\\",\\n+      \\\"version\\\": \\\"0.0.22\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.22.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-FYcyyP4BlVrHQmBznXFPpMbvUqFI6wbecHUUe4S+yIeDvJ8W4bP0ndZ0cMoc2p7hekWYAG4RtlohQBr8e4TKOg==\\\",\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@types/dagre\\\": \\\"^0.7.53\\\",\\n@@ -5960,9 +5962,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/probe\\\": {\\n-      \\\"version\\\": \\\"0.6.0-rc169\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc169.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-/laV3+1TSjASru8gGSvGwyjQ7jalMkFuZETl35uv37J1Hb4eG95CbKGU5NihcCrLXR44q1lKJiQGHXcAav2mBA==\\\",\\n+      \\\"version\\\": \\\"0.6.0-rc176\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc176.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-w/pAvnz2/yXb4FQXZV2QazxuW02xsI/wRg36yxZoLSG0O/hIncjF4rquZe3O+emceI2Ks5zbrA7cgQEdpwT8vA==\\\",\\n       \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n@@ -5972,7 +5974,7 @@\\n         \\\"@ai-sdk/openai\\\": \\\"^2.0.10\\\",\\n         \\\"@anthropic-ai/claude-agent-sdk\\\": \\\"^0.1.46\\\",\\n         \\\"@modelcontextprotocol/sdk\\\": \\\"^1.0.0\\\",\\n-        \\\"@probelabs/maid\\\": \\\"^0.0.21\\\",\\n+        \\\"@probelabs/maid\\\": \\\"^0.0.22\\\",\\n         \\\"adm-zip\\\": \\\"^0.5.16\\\",\\n         \\\"ai\\\": \\\"^5.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -8192,6 +8194,13 @@\\n         \\\"node\\\": \\\">= 20\\\"\\n       }\\n     },\\n+    \\\"node_modules/@yarnpkg/lockfile\\\": {\\n+      \\\"version\\\": \\\"1.1.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@yarnpkg/lockfile/-/lockfile-1.1.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-GpSwvyXOcOOlV70vbnzjj4fW5xW/FdUF6nQEt1ENy7m4ZCczi1+/buVUPAqmGfqznsORNFzUMjctTIp8a9tuCQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"BSD-2-Clause\\\"\\n+    },\\n     \\\"node_modules/accepts\\\": {\\n       \\\"version\\\": \\\"1.3.8\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/accepts/-/accepts-1.3.8.tgz\\\",\\n@@ -9180,6 +9189,25 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/call-bind\\\": {\\n+      \\\"version\\\": \\\"1.0.8\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind/-/call-bind-1.0.8.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-oKlSFMcMwpUg2ednkhQ454wfWiU/ul3CkJe/PEHcTKuiX6RpbehUiFMXu13HalGZxfUwCQzZG747YXBn1im9ww==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind-apply-helpers\\\": \\\"^1.0.0\\\",\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"set-function-length\\\": \\\"^1.2.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/call-bind-apply-helpers\\\": {\\n       \\\"version\\\": \\\"1.0.2\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind-apply-helpers/-/call-bind-apply-helpers-1.0.2.tgz\\\",\\n@@ -10613,6 +10641,24 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/define-data-property\\\": {\\n+      \\\"version\\\": \\\"1.1.4\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/define-data-property/-/define-data-property-1.1.4.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-rBMvIzlpA8v6E+SJZoo++HAYqsLrkg7MSfIinMPFhmkorw7X+dOXVJQs+QT69zGkzMyfDnIMN2Wid1+NbL3T+A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/define-lazy-prop\\\": {\\n       \\\"version\\\": \\\"3.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/define-lazy-prop/-/define-lazy-prop-3.0.0.tgz\\\",\\n@@ -11794,6 +11840,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/find-yarn-workspace-root\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/find-yarn-workspace-root/-/find-yarn-workspace-root-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-1IMnbjt4KzsQfnhnzNd8wUEgXZ44IzZaZmnLYx7D5FZlaHt2gW20Cri8Q+E/t5tIj4+epTBub+2Zxu/vNILzqQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Apache-2.0\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"micromatch\\\": \\\"^4.0.2\\\"\\n+      }\\n+    },\\n     \\\"node_modules/fix-dts-default-cjs-exports\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/fix-dts-default-cjs-exports/-/fix-dts-default-cjs-exports-1.0.1.tgz\\\",\\n@@ -12267,6 +12323,19 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/has-property-descriptors\\\": {\\n+      \\\"version\\\": \\\"1.0.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/has-property-descriptors/-/has-property-descriptors-1.0.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-55JNKuIW+vq4Ke1BjOTjM2YctQIvCT7GFzHwmfZPGo5wnrgkid0YQtnAleFSqumZm4az3n2BS+erby5ipJdgrg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/has-symbols\\\": {\\n       \\\"version\\\": \\\"1.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/has-symbols/-/has-symbols-1.1.0.tgz\\\",\\n@@ -12784,6 +12853,13 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/isarray\\\": {\\n+      \\\"version\\\": \\\"2.0.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/isarray/-/isarray-2.0.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-xHjhDr3cNBK0BzdUJSPXZntQUx/mwMS5Rw4A7lPJ90XGAO6ISP/ePDNuo0vhqOZU+UD5JoodwCAAoZQd3FeAKw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\"\\n+    },\\n     \\\"node_modules/isexe\\\": {\\n       \\\"version\\\": \\\"2.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz\\\",\\n@@ -13594,6 +13670,26 @@\\n       \\\"integrity\\\": \\\"sha512-NM8/P9n3XjXhIZn1lLhkFaACTOURQXjWhV4BA/RnOv8xvgqtqpAX9IO4mRQxSx1Rlo4tqzeqb0sOlruaOy3dug==\\\",\\n       \\\"license\\\": \\\"MIT\\\"\\n     },\\n+    \\\"node_modules/json-stable-stringify\\\": {\\n+      \\\"version\\\": \\\"1.3.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify/-/json-stable-stringify-1.3.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-qtYiSSFlwot9XHtF9bD9c7rwKjr+RecWT//ZnPvSmEjpV5mmPOCN4j8UjY5hbjNkOwZ/jQv3J6R1/pL7RwgMsg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind\\\": \\\"^1.0.8\\\",\\n+        \\\"call-bound\\\": \\\"^1.0.4\\\",\\n+        \\\"isarray\\\": \\\"^2.0.5\\\",\\n+        \\\"jsonify\\\": \\\"^0.0.1\\\",\\n+        \\\"object-keys\\\": \\\"^1.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/json-stable-stringify-without-jsonify\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify-without-jsonify/-/json-stable-stringify-without-jsonify-1.0.1.tgz\\\",\\n@@ -13633,6 +13729,16 @@\\n         \\\"graceful-fs\\\": \\\"^4.1.6\\\"\\n       }\\n     },\\n+    \\\"node_modules/jsonify\\\": {\\n+      \\\"version\\\": \\\"0.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/jsonify/-/jsonify-0.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-2/Ki0GcmuqSrgFyelQq9M05y7PS0mEwuIzrf3f1fPqkVDVRvZrPZtVSMHxdgo8Aq0sxAOb/cr2aqqA3LeWHVPg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Public Domain\\\",\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/katex\\\": {\\n       \\\"version\\\": \\\"0.16.25\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/katex/-/katex-0.16.25.tgz\\\",\\n@@ -13676,6 +13782,16 @@\\n       \\\"integrity\\\": \\\"sha512-Ls993zuzfayK269Svk9hzpeGUKob/sIgZzyHYdjQoAdQetRKpOLj+k/QQQ/6Qi0Yz65mlROrfd+Ev+1+7dz9Kw==\\\",\\n       \\\"dev\\\": true\\n     },\\n+    \\\"node_modules/klaw-sync\\\": {\\n+      \\\"version\\\": \\\"6.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/klaw-sync/-/klaw-sync-6.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-nIeuVSzdCCs6TDPTqI8w1Yre34sSq7AkZ4B3sfOBbI2CgVSB4Du4aLQijFU2+lhAFCwt9+42Hel6lQNIv6AntQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"graceful-fs\\\": \\\"^4.1.11\\\"\\n+      }\\n+    },\\n     \\\"node_modules/kolorist\\\": {\\n       \\\"version\\\": \\\"1.8.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/kolorist/-/kolorist-1.8.0.tgz\\\",\\n@@ -14785,6 +14901,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n       }\\n     },\\n+    \\\"node_modules/object-keys\\\": {\\n+      \\\"version\\\": \\\"1.1.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/object-keys/-/object-keys-1.1.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NuAESUOUMrlIXOfHKzD6bpPu3tYt3xvjNdRIQ+FeT0lNb4K8WR70CaDxhuNguS2XG+GjkyMwOzsN5ZktImfhLA==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/ohash\\\": {\\n       \\\"version\\\": \\\"2.0.11\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/ohash/-/ohash-2.0.11.tgz\\\",\\n@@ -15022,6 +15148,95 @@\\n         \\\"node\\\": \\\">= 0.8\\\"\\n       }\\n     },\\n+    \\\"node_modules/patch-package\\\": {\\n+      \\\"version\\\": \\\"8.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/patch-package/-/patch-package-8.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-VsKRIA8f5uqHQ7NGhwIna6Bx6D9s/1iXlA1hthBVBEbkq+t4kXD0HHt+rJhf/Z+Ci0F/HCB2hvn0qLdLG+Qxlw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"@yarnpkg/lockfile\\\": \\\"^1.1.0\\\",\\n+        \\\"chalk\\\": \\\"^4.1.2\\\",\\n+        \\\"ci-info\\\": \\\"^3.7.0\\\",\\n+        \\\"cross-spawn\\\": \\\"^7.0.3\\\",\\n+        \\\"find-yarn-workspace-root\\\": \\\"^2.0.0\\\",\\n+        \\\"fs-extra\\\": \\\"^10.0.0\\\",\\n+        \\\"json-stable-stringify\\\": \\\"^1.0.2\\\",\\n+        \\\"klaw-sync\\\": \\\"^6.0.0\\\",\\n+        \\\"minimist\\\": \\\"^1.2.6\\\",\\n+        \\\"open\\\": \\\"^7.4.2\\\",\\n+        \\\"semver\\\": \\\"^7.5.3\\\",\\n+        \\\"slash\\\": \\\"^2.0.0\\\",\\n+        \\\"tmp\\\": \\\"^0.2.4\\\",\\n+        \\\"yaml\\\": \\\"^2.2.2\\\"\\n+      },\\n+      \\\"bin\\\": {\\n+        \\\"patch-package\\\": \\\"index.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14\\\",\\n+        \\\"npm\\\": \\\">5\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/ci-info\\\": {\\n+      \\\"version\\\": \\\"3.9.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/ci-info/-/ci-info-3.9.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NIxF55hv4nSqQswkAeiOi1r83xy8JldOFDTWiug55KBu9Jnblncd2U6ViHmYgHf01TPZS77NJBhBMKdWj9HQMQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"funding\\\": [\\n+        {\\n+          \\\"type\\\": \\\"github\\\",\\n+          \\\"url\\\": \\\"https://github.com/sponsors/sibiraj-s\\\"\\n+        }\\n+      ],\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/is-docker\\\": {\\n+      \\\"version\\\": \\\"2.2.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/is-docker/-/is-docker-2.2.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-F+i2BKsFrH66iaUFc0woD8sLy8getkwTwtOBjvs56Cx4CgJDeKQeqfz8wAYiSb8JOprWhHH5p77PbmYCvvUuXQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"bin\\\": {\\n+        \\\"is-docker\\\": \\\"cli.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/open\\\": {\\n+      \\\"version\\\": \\\"7.4.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/open/-/open-7.4.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-MVHddDVweXZF3awtlAS+6pgKLlm/JgxZ90+/NBurBoQctVOOB/zDdVjcyPzQ+0laDGbsWgrRkflI65sQeOgT9Q==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"is-docker\\\": \\\"^2.0.0\\\",\\n+        \\\"is-wsl\\\": \\\"^2.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/slash\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/slash/-/slash-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-ZYKh3Wh2z1PpEXWr0MpSBZ0V6mZHAQfYevttO11c51CaWjGTaadiKZ+wVt1PbMlDV5qhMFslpZCemhwOK7C89A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=6\\\"\\n+      }\\n+    },\\n     \\\"node_modules/path-data-parser\\\": {\\n       \\\"version\\\": \\\"0.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/path-data-parser/-/path-data-parser-0.1.0.tgz\\\",\\n@@ -16473,6 +16688,24 @@\\n         \\\"node\\\": \\\">= 0.8.0\\\"\\n       }\\n     },\\n+    \\\"node_modules/set-function-length\\\": {\\n+      \\\"version\\\": \\\"1.2.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/set-function-length/-/set-function-length-1.2.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-pgRc4hJ4/sNjWCSS9AmnS40x3bNMDTknHgL5UaMBTMyJnU90EgWh1Rz+MC9eFu4BuN/UwZjKQuY/1v3rM7HMfg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"define-data-property\\\": \\\"^1.1.4\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"function-bind\\\": \\\"^1.1.2\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\",\\n+        \\\"has-property-descriptors\\\": \\\"^1.0.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/setprototypeof\\\": {\\n       \\\"version\\\": \\\"1.2.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/setprototypeof/-/setprototypeof-1.2.0.tgz\\\",\\n@@ -17384,6 +17617,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/tmp\\\": {\\n+      \\\"version\\\": \\\"0.2.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/tmp/-/tmp-0.2.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-voyz6MApa1rQGUxT3E+BK7/ROe8itEx7vD8/HEvt4xwXucvQ5G5oeEiHkmHZJuBO21RpOf+YYm9MOivj709jow==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14.14\\\"\\n+      }\\n+    },\\n     \\\"node_modules/tmpl\\\": {\\n       \\\"version\\\": \\\"1.0.5\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/tmpl/-/tmpl-1.0.5.tgz\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"package.json\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/package.json b/package.json\\nindex f9ccbb6c..302d728f 100644\\n--- a/package.json\\n+++ b/package.json\\n@@ -49,6 +49,7 @@\\n     \\\"test:with-build\\\": \\\"npm run build:cli && jest\\\",\\n     \\\"test:yaml\\\": \\\"node dist/index.js test --progress compact\\\",\\n     \\\"prepare\\\": \\\"husky\\\",\\n+    \\\"postinstall\\\": \\\"patch-package\\\",\\n     \\\"pre-commit\\\": \\\"lint-staged\\\",\\n     \\\"deploy:site\\\": \\\"cd site && npx wrangler pages deploy . --project-name=visor-site --commit-dirty=true\\\",\\n     \\\"deploy:worker\\\": \\\"npx wrangler deploy\\\",\\n@@ -96,7 +97,7 @@\\n     \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n     \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n     \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n     \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n     \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n     \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -143,6 +144,7 @@\\n     \\\"husky\\\": \\\"^9.1.7\\\",\\n     \\\"jest\\\": \\\"^30.1.3\\\",\\n     \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+    \\\"patch-package\\\": \\\"^8.0.1\\\",\\n     \\\"prettier\\\": \\\"^3.6.2\\\",\\n     \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n     \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"src/ai-review-service.ts\",\"additions\":101,\"deletions\":151,\"changes\":252,\"patch\":\"diff --git a/src/ai-review-service.ts b/src/ai-review-service.ts\\nindex 6b9c5b2e..f18accea 100644\\n--- a/src/ai-review-service.ts\\n+++ b/src/ai-review-service.ts\\n@@ -65,6 +65,13 @@ export interface AIReviewConfig {\\n   allowBash?: boolean;\\n   // Advanced bash command execution configuration\\n   bashConfig?: import('./types/config').BashConfig;\\n+  // Optional workspace root and allowed folders for ProbeAgent.\\n+  // When provided, these are forwarded to ProbeAgent so tools like search/query\\n+  // operate inside the isolated workspace/projects instead of the Visor repo root.\\n+  path?: string;\\n+  allowedFolders?: string[];\\n+  // Completion prompt for post-completion validation/review (runs after attempt_completion)\\n+  completionPrompt?: string;\\n }\\n \\n export interface AIDebugInfo {\\n@@ -278,7 +285,7 @@ export class AIReviewService {\\n     try {\\n       const call = this.callProbeAgent(prompt, schema, debugInfo, checkName, sessionId);\\n       const timeoutMs = Math.max(0, this.config.timeout || 0);\\n-      const { response, effectiveSchema } =\\n+      const { response, effectiveSchema, sessionId: usedSessionId } =\\n         timeoutMs > 0 ? await this.withTimeout(call, timeoutMs, 'AI review') : await call;\\n       const processingTime = Date.now() - startTime;\\n \\n@@ -290,6 +297,11 @@ export class AIReviewService {\\n \\n       const result = this.parseAIResponse(response, debugInfo, effectiveSchema);\\n \\n+      // Expose the session ID used for this call so the engine can reuse it later\\n+      try {\\n+        (result as any).sessionId = usedSessionId;\\n+      } catch {}\\n+\\n       if (debugInfo) {\\n         result.debug = debugInfo;\\n       }\\n@@ -651,17 +663,34 @@ ${prContext}${slackContextXml}\\n     const prContextInfo = prInfo as PRInfo & {\\n       isPRContext?: boolean;\\n       includeCodeContext?: boolean;\\n+      slackConversation?: unknown;\\n     };\\n     const isIssue = prContextInfo.isIssue === true;\\n \\n     // Check if we should include code context (diffs)\\n     const isPRContext = prContextInfo.isPRContext === true;\\n-    // In PR context, always include diffs. Otherwise check the flag.\\n-    const includeCodeContext = isPRContext || prContextInfo.includeCodeContext !== false;\\n+    const isSlackMode = prContextInfo.slackConversation !== undefined;\\n+\\n+    // Determine whether to include code context:\\n+    // - In explicit PR context (GitHub PR events), always include diffs\\n+    // - In Slack mode, default to NO code context unless explicitly requested\\n+    // - Otherwise, include code context unless explicitly disabled\\n+    let includeCodeContext: boolean;\\n+    if (isPRContext) {\\n+      includeCodeContext = true;\\n+    } else if (isSlackMode) {\\n+      // In Slack mode, only include code context if explicitly set to true\\n+      includeCodeContext = prContextInfo.includeCodeContext === true;\\n+    } else {\\n+      // Default: include unless explicitly disabled\\n+      includeCodeContext = prContextInfo.includeCodeContext !== false;\\n+    }\\n \\n     // Log the decision for transparency (debug level)\\n     if (isPRContext) {\\n       log('üîç Including full code diffs in AI context (PR mode)');\\n+    } else if (isSlackMode && !includeCodeContext) {\\n+      log('üí¨ Slack mode: excluding code diffs (use includeCodeContext: true to enable)');\\n     } else if (!includeCodeContext) {\\n       log('üìä Including only file summary in AI context (no diffs)');\\n     } else {\\n@@ -1537,7 +1566,15 @@ ${'='.repeat(60)}\\n     debugInfo?: AIDebugInfo,\\n     _checkName?: string,\\n     providedSessionId?: string\\n-  ): Promise<{ response: string; effectiveSchema?: string }> {\\n+  ): Promise<{ response: string; effectiveSchema?: string; sessionId: string }> {\\n+    // Derive a stable session ID for this call so the engine can reuse it later\\n+    const sessionId =\\n+      providedSessionId ||\\n+      (() => {\\n+        const timestamp = new Date().toISOString();\\n+        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n+      })();\\n+\\n     // Handle mock model/provider\\n     if (this.config.model === 'mock' || this.config.provider === 'mock') {\\n       const inJest = !!process.env.JEST_WORKER_ID;\\n@@ -1545,19 +1582,17 @@ ${'='.repeat(60)}\\n       if (!inJest) {\\n         // Fast path for CLI/integration: synthesize a mock response without invoking ProbeAgent\\n         const response = await this.generateMockResponse(prompt, _checkName, schema);\\n-        return { response, effectiveSchema: typeof schema === 'object' ? 'custom' : schema };\\n+        return {\\n+          response,\\n+          effectiveSchema: typeof schema === 'object' ? 'custom' : schema,\\n+          sessionId,\\n+        };\\n       }\\n       // In unit tests, still invoke ProbeAgent so tests can assert on options (schema) passed in\\n       // Fall through to normal flow below\\n     }\\n \\n     // Create ProbeAgent instance with proper options\\n-    const sessionId =\\n-      providedSessionId ||\\n-      (() => {\\n-        const timestamp = new Date().toISOString();\\n-        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n-      })();\\n \\n     log('ü§ñ Creating ProbeAgent for AI review...');\\n     log(`üÜî Session ID: ${sessionId}`);\\n@@ -1674,6 +1709,35 @@ ${'='.repeat(60)}\\n         (options as any).bashConfig = this.config.bashConfig;\\n       }\\n \\n+      // Pass completion prompt for post-completion validation/review\\n+      if (this.config.completionPrompt !== undefined) {\\n+        (options as any).completionPrompt = this.config.completionPrompt;\\n+      }\\n+\\n+      // Propagate workspace / allowed folders to ProbeAgent so that tools\\n+      // operate inside the isolated workspace and project checkouts instead\\n+      // of the Visor repository root.\\n+      try {\\n+        const cfgAny: any = this.config as any;\\n+        const allowedFolders = cfgAny.allowedFolders as string[] | undefined;\\n+        const workspacePath =\\n+          cfgAny.workspacePath || cfgAny.path || (Array.isArray(allowedFolders) && allowedFolders[0]);\\n+        if (Array.isArray(allowedFolders) && allowedFolders.length > 0) {\\n+          (options as any).allowedFolders = allowedFolders;\\n+          if (!options.path && workspacePath) {\\n+            (options as any).path = workspacePath;\\n+          }\\n+          log(`üóÇÔ∏è ProbeAgent workspace config:`);\\n+          log(`   path (cwd): ${(options as any).path}`);\\n+          log(`   allowedFolders[0]: ${allowedFolders[0]}`);\\n+        } else if (workspacePath) {\\n+          (options as any).path = workspacePath;\\n+          log(`üóÇÔ∏è ProbeAgent path: ${workspacePath} (no allowedFolders)`);\\n+        }\\n+      } catch {\\n+        // Best-effort only; fall back to ProbeAgent defaults on error.\\n+      }\\n+\\n       // Add provider-specific options if configured\\n       if (this.config.provider) {\\n         // Map claude-code to anthropic for ProbeAgent compatibility\\n@@ -2037,7 +2101,7 @@ ${'='.repeat(60)}\\n         log(`üîß Debug: Registered AI session for potential reuse: ${sessionId}`);\\n       }\\n \\n-      return { response, effectiveSchema };\\n+      return { response, effectiveSchema, sessionId };\\n     } catch (error) {\\n       console.error('‚ùå ProbeAgent failed:', error);\\n       throw new Error(\\n@@ -2191,13 +2255,23 @@ ${'='.repeat(60)}\\n         // For other schemas (code-review, etc.), extract and parse JSON with boundary detection\\n         log('üîç Extracting JSON from AI response...');\\n \\n-        // Try direct parsing first - if AI returned pure JSON\\n+        // Sanitize response: strip BOM, zero-width chars, and other invisible characters\\n+        // that can cause JSON parsing to fail even when the text looks valid\\n+        const sanitizedResponse = response\\n+          .replace(/^\\\\uFEFF/, '') // BOM\\n+          .replace(/[\\\\u200B-\\\\u200D\\\\uFEFF\\\\u00A0]/g, '') // Zero-width chars, NBSP\\n+          .replace(/[\\\\x00-\\\\x08\\\\x0B\\\\x0C\\\\x0E-\\\\x1F\\\\x7F]/g, '') // Control chars (except \\\\t \\\\n \\\\r)\\n+          .trim();\\n+\\n+        // Try direct JSON parsing - no bracket-matching extraction\\n+        // JSON validation is offloaded to Probe agent when schema is provided\\n         try {\\n-          reviewData = JSON.parse(response.trim());\\n+          reviewData = JSON.parse(sanitizedResponse);\\n           log('‚úÖ Successfully parsed direct JSON response');\\n           if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-        } catch {\\n-          log('üîç Direct parsing failed, trying to extract JSON from response...');\\n+        } catch (parseErr) {\\n+          const errMsg = parseErr instanceof Error ? parseErr.message : String(parseErr);\\n+          log(`üîç Direct JSON parsing failed: ${errMsg}`);\\n \\n           // If the response starts with \\\"I cannot\\\" or similar, it's likely a refusal\\n           if (\\n@@ -2210,66 +2284,16 @@ ${'='.repeat(60)}\\n             };\\n           }\\n \\n-          // Try to extract JSON using improved method with proper bracket matching\\n-          const jsonString = this.extractJsonFromResponse(response);\\n-\\n-          if (jsonString) {\\n-            try {\\n-              reviewData = JSON.parse(jsonString);\\n-              log('‚úÖ Successfully parsed extracted JSON');\\n-              if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-            } catch {\\n-              log('üîß Extracted JSON parsing failed, falling back to plain text handling...');\\n-\\n-              // Check if response is plain text and doesn't contain structured data\\n-              if (!response.includes('{') && !response.includes('}')) {\\n-                log('üîß Plain text response detected, creating structured fallback...');\\n-\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              } else {\\n-                // Fallback: treat the entire response as an issue\\n-                log('üîß Creating fallback response from non-JSON content...');\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              }\\n-            }\\n-          } else {\\n-            // No JSON found at all - treat as plain text response\\n-            log('üîß No JSON found in response, treating as plain text...');\\n-            reviewData = {\\n-              issues: [\\n-                {\\n-                  file: 'AI_RESPONSE',\\n-                  line: 1,\\n-                  ruleId: 'ai/raw_response',\\n-                  message: response,\\n-                  severity: 'info',\\n-                  category: 'documentation',\\n-                },\\n-              ],\\n-            };\\n-          }\\n+          // Not valid JSON - treat entire response as text output\\n+          // This allows Probe (or other AI providers) to handle JSON validation\\n+          // and avoids false positives from bracket-matching (e.g., mermaid diagrams)\\n+          log('üîß Treating response as plain text (no JSON extraction)');\\n+          const trimmed = response.trim();\\n+          return {\\n+            issues: [],\\n+            output: { text: trimmed },\\n+            debug: debugInfo,\\n+          };\\n         }\\n       }\\n \\n@@ -2457,80 +2481,6 @@ ${'='.repeat(60)}\\n     }\\n   }\\n \\n-  /**\\n-   * Extract JSON from a response that might contain surrounding text\\n-   * Uses proper bracket matching to find valid JSON objects or arrays\\n-   */\\n-  private extractJsonFromResponse(response: string): string | null {\\n-    const text = response.trim();\\n-\\n-    // Try to find JSON objects first (higher priority)\\n-    let bestJson = this.findJsonWithBracketMatching(text, '{', '}');\\n-\\n-    // If no object found, try arrays\\n-    if (!bestJson) {\\n-      bestJson = this.findJsonWithBracketMatching(text, '[', ']');\\n-    }\\n-\\n-    return bestJson;\\n-  }\\n-\\n-  /**\\n-   * Find JSON with proper bracket matching to avoid false positives\\n-   */\\n-  private findJsonWithBracketMatching(\\n-    text: string,\\n-    openChar: string,\\n-    closeChar: string\\n-  ): string | null {\\n-    const firstIndex = text.indexOf(openChar);\\n-    if (firstIndex === -1) return null;\\n-\\n-    let depth = 0;\\n-    let inString = false;\\n-    let escaping = false;\\n-\\n-    for (let i = firstIndex; i < text.length; i++) {\\n-      const char = text[i];\\n-\\n-      if (escaping) {\\n-        escaping = false;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\\\\\\' && inString) {\\n-        escaping = true;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\"' && !escaping) {\\n-        inString = !inString;\\n-        continue;\\n-      }\\n-\\n-      if (!inString) {\\n-        if (char === openChar) {\\n-          depth++;\\n-        } else if (char === closeChar) {\\n-          depth--;\\n-          if (depth === 0) {\\n-            // Found matching closing bracket\\n-            const candidate = text.substring(firstIndex, i + 1);\\n-            try {\\n-              JSON.parse(candidate); // Validate it's actually valid JSON\\n-              return candidate;\\n-            } catch {\\n-              // This wasn't valid JSON, keep looking\\n-              break;\\n-            }\\n-          }\\n-        }\\n-      }\\n-    }\\n-\\n-    return null;\\n-  }\\n-\\n   /**\\n    * Generate mock response for testing\\n    */\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli-main.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/src/cli-main.ts b/src/cli-main.ts\\nindex 7dbef501..e671fbd8 100644\\n--- a/src/cli-main.ts\\n+++ b/src/cli-main.ts\\n@@ -158,6 +158,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n   }\\n   const only = getArg('--only');\\n   const bail = hasFlag('--bail');\\n+  const noMocks = hasFlag('--no-mocks');\\n   const listOnly = hasFlag('--list');\\n   const validateOnly = hasFlag('--validate');\\n   const progress = (getArg('--progress') as 'compact' | 'detailed' | undefined) || 'compact';\\n@@ -224,6 +225,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       const agg = await runSuites(multiFiles, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallelSuites: maxParallelSuites || Math.max(1, require('os').cpus()?.length || 2),\\n         maxParallel,\\n         promptMaxChars,\\n@@ -340,6 +342,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       runRes = await runner.runCases(tpath, suite, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallel,\\n         promptMaxChars,\\n         engineMode: 'state-machine',\\n@@ -609,6 +612,9 @@ export async function main(): Promise<void> {\\n     // Set environment variables early for proper logging in all modules\\n     process.env.VISOR_OUTPUT_FORMAT = options.output;\\n     process.env.VISOR_DEBUG = options.debug ? 'true' : 'false';\\n+    if (options.keepWorkspace) {\\n+      process.env.VISOR_KEEP_WORKSPACE = 'true';\\n+    }\\n     // Configure centralized logger\\n     configureLoggerFromCli({\\n       output: options.output,\\n@@ -716,6 +722,31 @@ export async function main(): Promise<void> {\\n       const threads = slackAny.threads || 'any';\\n       const allow = Array.isArray(slackAny.channel_allowlist) ? slackAny.channel_allowlist : [];\\n       const appToken = slackAny.app_token || process.env.SLACK_APP_TOKEN;\\n+\\n+      // Initialize telemetry for Slack mode (normally done later for CLI runs).\\n+      if ((config as any)?.telemetry) {\\n+        const t = (config as any).telemetry as {\\n+          enabled?: boolean;\\n+          sink?: 'otlp' | 'file' | 'console';\\n+          file?: { dir?: string; ndjson?: boolean };\\n+          tracing?: { auto_instrumentations?: boolean; trace_report?: { enabled?: boolean } };\\n+        };\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true' || !!t?.enabled,\\n+          sink:\\n+            (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || t?.sink || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR || t?.file?.dir, ndjson: !!t?.file?.ndjson },\\n+          autoInstrument: !!t?.tracing?.auto_instrumentations,\\n+          traceReport: !!t?.tracing?.trace_report?.enabled,\\n+        });\\n+      } else {\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true',\\n+          sink: (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR },\\n+        });\\n+      }\\n+\\n       const runner = new SlackSocketRunner(engine, config, {\\n         appToken,\\n         endpoint,\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli.ts\",\"additions\":5,\"deletions\":1,\"changes\":6,\"patch\":\"diff --git a/src/cli.ts b/src/cli.ts\\nindex 134429b2..b1c46213 100644\\n--- a/src/cli.ts\\n+++ b/src/cli.ts\\n@@ -76,6 +76,7 @@ export class CLI {\\n         parseInt(value, 10)\\n       )\\n       .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText())\\n       .exitOverride(); // Prevent automatic process.exit for better error handling\\n \\n@@ -154,6 +155,7 @@ export class CLI {\\n           parseInt(value, 10)\\n         )\\n         .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+        .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n         .allowUnknownOption(false)\\n         .allowExcessArguments(false) // Don't allow positional arguments\\n         .addHelpText('after', this.getExamplesText())\\n@@ -224,6 +226,7 @@ export class CLI {\\n         message: options.message,\\n         githubV2: false,\\n         slack: Boolean(options.slack),\\n+        keepWorkspace: Boolean(options.keepWorkspace),\\n       };\\n     } catch (error: unknown) {\\n       // Handle commander.js exit overrides for help/version ONLY\\n@@ -348,6 +351,7 @@ export class CLI {\\n       .option('--debug-port <port>', 'Port for debug server (default: 3456)', value =>\\n         parseInt(value, 10)\\n       )\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText());\\n \\n     // Get the basic help and append examples manually if addHelpText doesn't work\\n\",\"status\":\"modified\"},{\"filename\":\"src/config.ts\",\"additions\":67,\"deletions\":17,\"changes\":84,\"patch\":\"diff --git a/src/config.ts b/src/config.ts\\nindex 407efa2d..02abe9df 100644\\n--- a/src/config.ts\\n+++ b/src/config.ts\\n@@ -20,6 +20,7 @@ import { ConfigLoader, ConfigLoaderOptions } from './utils/config-loader';\\n import { ConfigMerger } from './utils/config-merger';\\n import Ajv from 'ajv';\\n import addFormats from 'ajv-formats';\\n+import { validateJsSyntax } from './utils/sandbox';\\n \\n /**\\n  * Valid event triggers for checks\\n@@ -477,16 +478,29 @@ export class ConfigManager {\\n \\n     logger.info(`Registered workflow '${workflowId}' for standalone execution`);\\n \\n-    // Create a COMPLETELY NEW visor config with ONLY the test checks\\n-    // This prevents any workflow fields from leaking into the config\\n-    const visorConfig: Partial<VisorConfig> = {\\n+    // For standalone workflow testing, use the workflow's steps as checks\\n+    // This allows the test runner to execute the workflow steps directly\\n+    const workflowSteps = workflowData.steps || {};\\n+\\n+    // Create a config that includes the workflow steps as checks,\\n+    // plus the tests section and workflow metadata for output computation\\n+    const visorConfig: Partial<VisorConfig> & { tests?: any; outputs?: any; inputs?: any } = {\\n       version: '1.0',\\n-      steps: tests,\\n-      checks: tests, // Backward compatibility\\n+      steps: workflowSteps,\\n+      checks: workflowSteps,\\n+      tests: tests, // Preserve test harness config (may be empty if stripped by test runner)\\n     };\\n \\n-    logger.debug(`Standalone workflow config has ${Object.keys(tests).length} test checks`);\\n-    logger.debug(`Test check names: ${Object.keys(tests).join(', ')}`);\\n+    // Preserve workflow metadata for output computation during tests\\n+    if (workflowData.outputs) {\\n+      visorConfig.outputs = workflowData.outputs;\\n+    }\\n+    if (workflowData.inputs) {\\n+      visorConfig.inputs = workflowData.inputs;\\n+    }\\n+\\n+    logger.debug(`Standalone workflow config has ${Object.keys(workflowSteps).length} workflow steps as checks`);\\n+    logger.debug(`Workflow step names: ${Object.keys(workflowSteps).join(', ')}`);\\n     logger.debug(`Config keys after conversion: ${Object.keys(visorConfig).join(', ')}`);\\n \\n     return visorConfig;\\n@@ -982,26 +996,29 @@ export class ConfigManager {\\n \\n     // Validate reuse_ai_session configuration\\n     if (checkConfig.reuse_ai_session !== undefined) {\\n-      const isString = typeof checkConfig.reuse_ai_session === 'string';\\n-      const isBoolean = typeof checkConfig.reuse_ai_session === 'boolean';\\n+      const reuseValue = checkConfig.reuse_ai_session as unknown;\\n+      const isString = typeof reuseValue === 'string';\\n+      const isBoolean = typeof reuseValue === 'boolean';\\n+      const isSelf = reuseValue === 'self';\\n \\n       if (!isString && !isBoolean) {\\n         errors.push({\\n           field: `checks.${checkName}.reuse_ai_session`,\\n           message: `Invalid reuse_ai_session value for \\\"${checkName}\\\": must be string (check name) or boolean`,\\n-          value: checkConfig.reuse_ai_session,\\n+          value: reuseValue,\\n         });\\n-      } else if (isString) {\\n-        // When reuse_ai_session is a string, it must refer to a valid check\\n-        const targetCheckName = checkConfig.reuse_ai_session as string;\\n+      } else if (isString && !isSelf) {\\n+        // When reuse_ai_session is a string (other than the special 'self' value),\\n+        // it must refer to a valid check name\\n+        const targetCheckName = reuseValue as string;\\n         if (!config?.checks || !config.checks[targetCheckName]) {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" references non-existent check \\\"${targetCheckName}\\\" for session reuse`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n-      } else if (checkConfig.reuse_ai_session === true) {\\n+      } else if (reuseValue === true) {\\n         // When reuse_ai_session is true, depends_on must be specified and non-empty\\n         if (\\n           !checkConfig.depends_on ||\\n@@ -1011,7 +1028,7 @@ export class ConfigManager {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" has reuse_ai_session=true but missing or empty depends_on. Session reuse requires dependency on another check.`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n       }\\n@@ -1076,6 +1093,39 @@ export class ConfigManager {\\n         });\\n       }\\n     }\\n+\\n+    // Validate JavaScript syntax in transform_js and script content\\n+    try {\\n+      // Validate transform_js if present\\n+      const transformJs = (checkConfig as any).transform_js;\\n+      if (typeof transformJs === 'string' && transformJs.trim().length > 0) {\\n+        const result = validateJsSyntax(transformJs);\\n+        if (!result.valid) {\\n+          errors.push({\\n+            field: `checks.${checkName}.transform_js`,\\n+            message: `JavaScript syntax error in \\\"${checkName}\\\" transform_js: ${result.error}`,\\n+            value: transformJs.slice(0, 100) + (transformJs.length > 100 ? '...' : ''),\\n+          });\\n+        }\\n+      }\\n+\\n+      // Validate script content if type is 'script'\\n+      if (checkConfig.type === 'script') {\\n+        const content = (checkConfig as any).content;\\n+        if (typeof content === 'string' && content.trim().length > 0) {\\n+          const result = validateJsSyntax(content);\\n+          if (!result.valid) {\\n+            errors.push({\\n+              field: `checks.${checkName}.content`,\\n+              message: `JavaScript syntax error in \\\"${checkName}\\\" script: ${result.error}`,\\n+              value: content.slice(0, 100) + (content.length > 100 ? '...' : ''),\\n+            });\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Syntax validation is best-effort; don't fail the whole config on validation errors\\n+    }\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/failure-condition-evaluator.ts\",\"additions\":25,\"deletions\":9,\"changes\":34,\"patch\":\"diff --git a/src/failure-condition-evaluator.ts b/src/failure-condition-evaluator.ts\\nindex 3b54a015..5800df0a 100644\\n--- a/src/failure-condition-evaluator.ts\\n+++ b/src/failure-condition-evaluator.ts\\n@@ -137,6 +137,8 @@ export class FailureConditionEvaluator {\\n       previousResults?: Map<string, ReviewSummary>;\\n       authorAssociation?: string;\\n       workflowInputs?: Record<string, unknown>;\\n+      /** Current step's output for guarantee evaluation */\\n+      output?: unknown;\\n     }\\n   ): Promise<boolean> {\\n     // Build context for if evaluation\\n@@ -177,10 +179,10 @@ export class FailureConditionEvaluator {\\n       // Workflow inputs (for workflows)\\n       inputs: contextData?.workflowInputs || {},\\n \\n-      // Required output property (empty for if conditions)\\n-      output: {\\n-        issues: [],\\n-      },\\n+      // Output property: use provided output for guarantee evaluation, or empty for if conditions\\n+      output: contextData?.output !== undefined && contextData.output !== null && typeof contextData.output === 'object'\\n+        ? (contextData.output as Record<string, unknown>)\\n+        : { issues: [] },\\n       // Author association (used by permission helpers)\\n       authorAssociation: contextData?.authorAssociation,\\n \\n@@ -213,10 +215,10 @@ export class FailureConditionEvaluator {\\n               m && typeof m.get === 'function' ? m.get('all_valid', 'fact-validation') : undefined;\\n             memStr = ` mem.fact-validation.all_valid=${String(v)}`;\\n           } catch {}\\n+          // Debug if-eval output (only when VISOR_DEBUG enabled)\\n+          const outputKeys = Object.keys(context.outputs || {});\\n           console.error(\\n-            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" env.ENABLE_FACT_VALIDATION=${String(\\n-              (envMap as any).ENABLE_FACT_VALIDATION\\n-            )} event=${context.event?.event_name} result=${String(res)}${memStr}`\\n+            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" result=${String(res)} outputKeys=[${outputKeys.join(',')}]`\\n           );\\n         }\\n       } catch {}\\n@@ -480,7 +482,18 @@ export class FailureConditionEvaluator {\\n \\n       // Extract context variables\\n       const output = context.output || {};\\n-      const issues = output.issues || [];\\n+      // Ensure issues is an array - it might be a JSON string from workflow outputs\\n+      let issues = output.issues || [];\\n+      if (typeof issues === 'string') {\\n+        try {\\n+          issues = JSON.parse(issues);\\n+        } catch {\\n+          issues = [];\\n+        }\\n+      }\\n+      if (!Array.isArray(issues)) {\\n+        issues = [];\\n+      }\\n \\n       // Backward compatibility: provide metadata for transition period\\n       // TODO: Remove after all configurations are updated\\n@@ -518,6 +531,7 @@ export class FailureConditionEvaluator {\\n       const event = context.event || 'manual';\\n       const env = context.env || {};\\n       const outputs = context.outputs || {};\\n+      const inputs = context.inputs || {};\\n       const debugData = context.debug || null;\\n \\n       // Get memory store and create accessor for fail_if expressions\\n@@ -555,6 +569,7 @@ export class FailureConditionEvaluator {\\n         filesCount,\\n         event,\\n         env,\\n+        inputs,\\n         // Helper functions\\n         contains,\\n         startsWith,\\n@@ -621,6 +636,7 @@ export class FailureConditionEvaluator {\\n             filesCount,\\n             event,\\n             env,\\n+            inputs,\\n             // Helpers\\n             contains,\\n             startsWith,\\n\",\"status\":\"modified\"},{\"filename\":\"src/frontends/slack-frontend.ts\",\"additions\":107,\"deletions\":6,\"changes\":113,\"patch\":\"diff --git a/src/frontends/slack-frontend.ts b/src/frontends/slack-frontend.ts\\nindex 97161a7d..41735aa2 100644\\n--- a/src/frontends/slack-frontend.ts\\n+++ b/src/frontends/slack-frontend.ts\\n@@ -1,6 +1,32 @@\\n+/**\\n+ * Slack Frontend for Visor workflows.\\n+ *\\n+ * Features:\\n+ * - Posts AI replies to Slack threads\\n+ * - Converts Markdown to Slack mrkdwn format\\n+ * - Renders mermaid diagrams to PNG and uploads as images\\n+ * - Manages üëÄ/üëç reactions for acknowledgement\\n+ * - Handles human input prompts via prompt-state\\n+ *\\n+ * Mermaid Diagram Rendering:\\n+ * - Detects ```mermaid code blocks in AI responses\\n+ * - Renders to PNG using @mermaid-js/mermaid-cli (mmdc)\\n+ * - Uploads rendered images to Slack thread\\n+ * - Replaces mermaid blocks with \\\"_(See diagram above)_\\\" placeholder\\n+ *\\n+ * Requirements for mermaid rendering:\\n+ * - Node.js and npx in PATH\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ * - On Linux: apt-get install chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ */\\n import type { Frontend, FrontendContext } from './host';\\n import { SlackClient } from '../slack/client';\\n-import { formatSlackText } from '../slack/markdown';\\n+import {\\n+  formatSlackText,\\n+  extractMermaidDiagrams,\\n+  renderMermaidToPng,\\n+  replaceMermaidBlocks,\\n+} from '../slack/markdown';\\n \\n type SlackFrontendConfig = {\\n   defaultChannel?: string;\\n@@ -284,6 +310,9 @@ export class SlackFrontend implements Frontend {\\n       const isLogChat = providerType === 'log' && checkCfg.group === 'chat';\\n       if (!isAi && !isLogChat) return;\\n \\n+      // Skip internal steps - they're intermediate processing and shouldn't post to Slack\\n+      if (checkCfg.criticality === 'internal') return;\\n+\\n       // For AI checks, only post when using simple/unstructured schemas (or none).\\n       if (isAi) {\\n         const schema = checkCfg.schema;\\n@@ -316,6 +345,13 @@ export class SlackFrontend implements Frontend {\\n         ) {\\n           text = (result as any).content.trim();\\n         }\\n+      } else if (isLogChat && typeof (result as any)?.logOutput === 'string') {\\n+        // For log-based chat checks, render the formatted log output as the\\n+        // Slack message when no structured text field is present.\\n+        const raw = (result as any).logOutput;\\n+        if (raw.trim().length > 0) {\\n+          text = raw.trim();\\n+        }\\n       } else if (isAi && showRawOutput && out !== undefined) {\\n         try {\\n           text = JSON.stringify(out, null, 2);\\n@@ -325,13 +361,78 @@ export class SlackFrontend implements Frontend {\\n       }\\n       if (!text) return;\\n \\n-      const formattedText = formatSlackText(text);\\n+      // Extract and render mermaid diagrams before posting\\n+      const diagrams = extractMermaidDiagrams(text);\\n+      let processedText = text;\\n+\\n+      if (diagrams.length > 0) {\\n+        try {\\n+          ctx.logger.info(\\n+            `[slack-frontend] found ${diagrams.length} mermaid diagram(s) to render for ${checkId}`\\n+          );\\n+        } catch {}\\n+\\n+        // Render and upload each diagram\\n+        const uploadedCount: number[] = [];\\n+        for (let i = 0; i < diagrams.length; i++) {\\n+          const diagram = diagrams[i];\\n+          try {\\n+            ctx.logger.info(`[slack-frontend] rendering mermaid diagram ${i + 1}...`);\\n+            const pngBuffer = await renderMermaidToPng(diagram.code);\\n+            if (pngBuffer) {\\n+              ctx.logger.info(\\n+                `[slack-frontend] rendered diagram ${i + 1}, size=${pngBuffer.length} bytes, uploading...`\\n+              );\\n+              const filename = `diagram-${i + 1}.png`;\\n+              const uploadResult = await slack.files.uploadV2({\\n+                content: pngBuffer,\\n+                filename,\\n+                channel,\\n+                thread_ts: threadTs,\\n+                title: `Diagram ${i + 1}`,\\n+              });\\n+              if (uploadResult.ok) {\\n+                uploadedCount.push(i);\\n+                ctx.logger.info(`[slack-frontend] uploaded mermaid diagram ${i + 1} to ${channel}`);\\n+              } else {\\n+                ctx.logger.warn(`[slack-frontend] upload failed for diagram ${i + 1}`);\\n+              }\\n+            } else {\\n+              ctx.logger.warn(\\n+                `[slack-frontend] mermaid rendering returned null for diagram ${i + 1} (mmdc failed or not installed)`\\n+              );\\n+            }\\n+          } catch (e) {\\n+            ctx.logger.warn(\\n+              `[slack-frontend] failed to render/upload mermaid diagram ${i + 1}: ${\\n+                e instanceof Error ? e.message : String(e)\\n+              }`\\n+            );\\n+          }\\n+        }\\n+\\n+        // Replace mermaid blocks with placeholder text\\n+        if (uploadedCount.length > 0) {\\n+          processedText = replaceMermaidBlocks(text, diagrams, (idx) =>\\n+            uploadedCount.includes(idx) ? '_(See diagram above)_' : '_(Diagram rendering failed)_'\\n+          );\\n+        }\\n+      }\\n+\\n+      const formattedText = formatSlackText(processedText);\\n       await slack.chat.postMessage({ channel, text: formattedText, thread_ts: threadTs });\\n+      ctx.logger.info(\\n+        `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+      );\\n+    } catch (outerErr) {\\n+      // Log errors instead of silently swallowing them\\n       try {\\n-        ctx.logger.info(\\n-          `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+        ctx.logger.warn(\\n+          `[slack-frontend] maybePostDirectReply failed for ${checkId}: ${\\n+            outerErr instanceof Error ? outerErr.message : String(outerErr)\\n+          }`\\n         );\\n       } catch {}\\n-    } catch {}\\n+    }\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/generated/config-schema.ts\",\"additions\":233,\"deletions\":13,\"changes\":246,\"patch\":\"diff --git a/src/generated/config-schema.ts b/src/generated/config-schema.ts\\nindex b3caca4f..ecab390d 100644\\n--- a/src/generated/config-schema.ts\\n+++ b/src/generated/config-schema.ts\\n@@ -56,6 +56,20 @@ export const configSchema = {\\n           },\\n           description: 'Import workflow definitions from external files or URLs',\\n         },\\n+        inputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowInput',\\n+          },\\n+          description: 'Workflow inputs (for standalone reusable workflows)',\\n+        },\\n+        outputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowOutput',\\n+          },\\n+          description: 'Workflow outputs (for standalone reusable workflows)',\\n+        },\\n         steps: {\\n           $ref: '#/definitions/Record%3Cstring%2CCheckConfig%3E',\\n           description: 'Step configurations (recommended)',\\n@@ -67,7 +81,7 @@ export const configSchema = {\\n         },\\n         output: {\\n           $ref: '#/definitions/OutputConfig',\\n-          description: 'Output configuration',\\n+          description: 'Output configuration (optional - defaults provided)',\\n         },\\n         http_server: {\\n           $ref: '#/definitions/HttpServerConfig',\\n@@ -139,8 +153,16 @@ export const configSchema = {\\n           },\\n           description: 'Optional integrations: event-driven frontends (e.g., ndjson-sink, github)',\\n         },\\n+        workspace: {\\n+          $ref: '#/definitions/WorkspaceConfig',\\n+          description: 'Workspace isolation configuration for sandboxed execution',\\n+        },\\n+        slack: {\\n+          $ref: '#/definitions/SlackConfig',\\n+          description: 'Slack configuration',\\n+        },\\n       },\\n-      required: ['output', 'version'],\\n+      required: ['version'],\\n       patternProperties: {\\n         '^x-': {},\\n       },\\n@@ -243,6 +265,63 @@ export const configSchema = {\\n         type: 'string',\\n       },\\n     },\\n+    WorkflowInput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Input parameter name',\\n+        },\\n+        schema: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'JSON Schema for the input',\\n+        },\\n+        required: {\\n+          type: 'boolean',\\n+          description: 'Whether this input is required',\\n+        },\\n+        default: {\\n+          description: 'Default value if not provided',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow input definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    WorkflowOutput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Output name',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+        value: {\\n+          type: 'string',\\n+          description: 'Value using Liquid template syntax (references step outputs)',\\n+        },\\n+        value_js: {\\n+          type: 'string',\\n+          description: 'Value using JavaScript expression (alternative to value)',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow output definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n     'Record<string,CheckConfig>': {\\n       type: 'object',\\n       additionalProperties: {\\n@@ -390,11 +469,19 @@ export const configSchema = {\\n           description: 'Timeout in seconds for command execution (default: 60)',\\n         },\\n         depends_on: {\\n-          type: 'array',\\n-          items: {\\n-            type: 'string',\\n-          },\\n-          description: 'Check IDs that this check depends on (optional)',\\n+          anyOf: [\\n+            {\\n+              type: 'string',\\n+            },\\n+            {\\n+              type: 'array',\\n+              items: {\\n+                type: 'string',\\n+              },\\n+            },\\n+          ],\\n+          description:\\n+            'Check IDs that this check depends on (optional). Accepts single string or array.',\\n         },\\n         group: {\\n           type: 'string',\\n@@ -653,13 +740,93 @@ export const configSchema = {\\n           description: 'Arguments/inputs for the workflow',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Override specific step configurations in the workflow',\\n         },\\n         output_mapping: {\\n           $ref: '#/definitions/Record%3Cstring%2Cstring%3E',\\n           description: 'Map workflow outputs to check outputs',\\n         },\\n+        workflow_inputs: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'Alias for args - workflow inputs (backward compatibility)',\\n+        },\\n+        config: {\\n+          type: 'string',\\n+          description:\\n+            'Config file path - alternative to workflow ID (loads a Visor config file as workflow)',\\n+        },\\n+        workflow_overrides: {\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n+          description: 'Alias for overrides - workflow step overrides (backward compatibility)',\\n+        },\\n+        ref: {\\n+          type: 'string',\\n+          description: 'Git reference to checkout (branch, tag, commit SHA) - supports templates',\\n+        },\\n+        repository: {\\n+          type: 'string',\\n+          description: 'Repository URL or owner/repo format (defaults to current repository)',\\n+        },\\n+        token: {\\n+          type: 'string',\\n+          description: 'GitHub token for private repositories (defaults to GITHUB_TOKEN env)',\\n+        },\\n+        fetch_depth: {\\n+          type: 'number',\\n+          description: 'Number of commits to fetch (0 for full history, default: 1)',\\n+        },\\n+        fetch_tags: {\\n+          type: 'boolean',\\n+          description: 'Whether to fetch tags (default: false)',\\n+        },\\n+        submodules: {\\n+          anyOf: [\\n+            {\\n+              type: 'boolean',\\n+            },\\n+            {\\n+              type: 'string',\\n+              const: 'recursive',\\n+            },\\n+          ],\\n+          description: \\\"Checkout submodules: false, true, or 'recursive'\\\",\\n+        },\\n+        working_directory: {\\n+          type: 'string',\\n+          description: 'Working directory for the checkout (defaults to temp directory)',\\n+        },\\n+        use_worktree: {\\n+          type: 'boolean',\\n+          description: 'Use git worktree for efficient parallel checkouts (default: true)',\\n+        },\\n+        clean: {\\n+          type: 'boolean',\\n+          description: 'Clean the working directory before checkout (default: true)',\\n+        },\\n+        sparse_checkout: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description: 'Sparse checkout paths - only checkout specific directories/files',\\n+        },\\n+        lfs: {\\n+          type: 'boolean',\\n+          description: 'Enable Git LFS (Large File Storage)',\\n+        },\\n+        clone_timeout_ms: {\\n+          type: 'number',\\n+          description: 'Timeout in ms for cloning the bare repository (default: 300000 = 5 min)',\\n+        },\\n+        cleanup_on_failure: {\\n+          type: 'boolean',\\n+          description: 'Clean up worktree on failure (default: true)',\\n+        },\\n+        persist_worktree: {\\n+          type: 'boolean',\\n+          description: 'Keep worktree after workflow completion (default: false)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'Configuration for a single check',\\n@@ -794,6 +961,11 @@ export const configSchema = {\\n           $ref: '#/definitions/BashConfig',\\n           description: 'Advanced bash command execution configuration',\\n         },\\n+        completion_prompt: {\\n+          type: 'string',\\n+          description:\\n+            'Completion prompt for post-completion validation/review (runs after attempt_completion)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'AI provider configuration',\\n@@ -1223,7 +1395,7 @@ export const configSchema = {\\n           description: 'Custom output name (defaults to workflow name)',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Step overrides',\\n         },\\n         output_mapping: {\\n@@ -1238,14 +1410,14 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n-    'Record<string,Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>>':\\n+    'Record<string,Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>>':\\n       {\\n         type: 'object',\\n         additionalProperties: {\\n-          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E',\\n+          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E',\\n         },\\n       },\\n-    'Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>': {\\n+    'Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>': {\\n       type: 'object',\\n       additionalProperties: false,\\n     },\\n@@ -1811,6 +1983,54 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n+    WorkspaceConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        enabled: {\\n+          type: 'boolean',\\n+          description: 'Enable workspace isolation (default: true when config present)',\\n+        },\\n+        base_path: {\\n+          type: 'string',\\n+          description: 'Base path for workspaces (default: /tmp/visor-workspaces)',\\n+        },\\n+        cleanup_on_exit: {\\n+          type: 'boolean',\\n+          description: 'Clean up workspace on exit (default: true)',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Workspace isolation configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    SlackConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        version: {\\n+          type: 'string',\\n+          description: 'Slack API version',\\n+        },\\n+        mentions: {\\n+          type: 'string',\\n+          description: \\\"Mention handling: 'all', 'direct', etc.\\\",\\n+        },\\n+        threads: {\\n+          type: 'string',\\n+          description: \\\"Thread handling: 'required', 'optional', etc.\\\",\\n+        },\\n+        show_raw_output: {\\n+          type: 'boolean',\\n+          description: 'Show raw output in Slack responses',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Slack configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n   },\\n } as const;\\n export default configSchema;\\n\",\"status\":\"modified\"},{\"filename\":\"src/index.ts\",\"additions\":5,\"deletions\":2,\"changes\":7,\"patch\":\"diff --git a/src/index.ts b/src/index.ts\\nindex 50b89df3..9621e733 100644\\n--- a/src/index.ts\\n+++ b/src/index.ts\\n@@ -628,8 +628,11 @@ function resolveDependencies(\\n \\n     // Get dependencies for this check\\n     const checkConfig = config?.checks?.[checkId];\\n+    // Normalize depends_on to array (supports string | string[])\\n+    const rawDeps = checkConfig?.depends_on;\\n+    const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     // Expand OR groups (pipe syntax) for dependency closure discovery\\n-    const dependencies = (checkConfig?.depends_on || []).flatMap(d =>\\n+    const dependencies = depsArray.flatMap((d: string) =>\\n       typeof d === 'string' && d.includes('|')\\n         ? d\\n             .split('|')\\n\",\"status\":\"modified\"},{\"filename\":\"src/liquid-extensions.ts\",\"additions\":68,\"deletions\":1,\"changes\":69,\"patch\":\"diff --git a/src/liquid-extensions.ts b/src/liquid-extensions.ts\\nindex e9efaf68..dbf9c6ed 100644\\n--- a/src/liquid-extensions.ts\\n+++ b/src/liquid-extensions.ts\\n@@ -124,6 +124,26 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     }\\n   });\\n \\n+  // Register base64 filter for encoding strings\\n+  // Usage: {{ \\\"user:password\\\" | base64 }}\\n+  liquid.registerFilter('base64', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    return Buffer.from(str).toString('base64');\\n+  });\\n+\\n+  // Register base64_decode filter for decoding base64 strings\\n+  // Usage: {{ encoded_value | base64_decode }}\\n+  liquid.registerFilter('base64_decode', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    try {\\n+      return Buffer.from(str, 'base64').toString('utf-8');\\n+    } catch {\\n+      return '[Error: Invalid base64 string]';\\n+    }\\n+  });\\n+\\n   // Sanitize a label to allowed characters only: [A-Za-z0-9:/]\\n   liquid.registerFilter('safe_label', (value: unknown) => sanitizeLabel(value));\\n \\n@@ -137,6 +157,53 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     return s.replace(/\\\\\\\\n/g, '\\\\n').replace(/\\\\\\\\r/g, '\\\\r').replace(/\\\\\\\\t/g, '\\\\t');\\n   });\\n \\n+  // JSON escape filter - escapes a string for use inside a JSON string value\\n+  // This escapes special characters like quotes, backslashes, and control characters\\n+  // Usage: \\\"jql\\\": \\\"{{ myValue | json_escape }}\\\"\\n+  liquid.registerFilter('json_escape', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const s = String(value);\\n+    // Use JSON.stringify which handles all escaping, then strip the surrounding quotes\\n+    const jsonStr = JSON.stringify(s);\\n+    // Remove the first and last character (the quotes added by JSON.stringify)\\n+    return jsonStr.slice(1, -1);\\n+  });\\n+\\n+  // Shell escape filter - wraps value in single quotes with proper escaping\\n+  // Usage: {{ value | shell_escape }}\\n+  // Example: \\\"hello'world\\\" becomes \\\"'hello'\\\\''world'\\\"\\n+  // This is POSIX-compliant and safe for arbitrary text including mermaid diagrams\\n+  liquid.registerFilter('shell_escape', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    // Replace single quotes with: end quote, escaped quote, start quote\\n+    // Then wrap the entire thing in single quotes\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Alias for shell_escape\\n+  liquid.registerFilter('escape_shell', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Shell escape for double quotes (less safe but sometimes needed)\\n+  // Usage: {{ value | shell_escape_double }}\\n+  // Escapes: $, `, \\\\, \\\", and !\\n+  liquid.registerFilter('shell_escape_double', (value: unknown) => {\\n+    if (value == null) return '\\\"\\\"';\\n+    const s = String(value);\\n+    // Escape characters that have special meaning inside double quotes\\n+    const escaped = s\\n+      .replace(/\\\\\\\\/g, '\\\\\\\\\\\\\\\\')  // backslash first\\n+      .replace(/\\\\$/g, '\\\\\\\\$')   // dollar sign\\n+      .replace(/`/g, '\\\\\\\\`')    // backticks\\n+      .replace(/\\\"/g, '\\\\\\\\\\\"')    // double quotes\\n+      .replace(/!/g, '\\\\\\\\!');   // history expansion\\n+    return '\\\"' + escaped + '\\\"';\\n+  });\\n+\\n   // Register author permission filters (from main)\\n   // These filters check the author's permission level; detect local mode for tests\\n   const isLocal = detectLocalMode();\\n\",\"status\":\"modified\"},{\"filename\":\"src/logger.ts\",\"additions\":31,\"deletions\":2,\"changes\":33,\"patch\":\"diff --git a/src/logger.ts b/src/logger.ts\\nindex 6148b77c..aee7136e 100644\\n--- a/src/logger.ts\\n+++ b/src/logger.ts\\n@@ -85,7 +85,36 @@ class Logger {\\n       if (this.showTimestamps) {\\n         const ts = new Date().toISOString();\\n         const lvl = level ? level : undefined;\\n-        const prefix = lvl ? `[${ts}] [${lvl}]` : `[${ts}]`;\\n+\\n+        let tsToken = `[${ts}]`;\\n+        let lvlToken = lvl ? `[${lvl}]` : '';\\n+\\n+        // Add simple ANSI colour when running in a TTY and not emitting\\n+        // JSON/SARIF. Colours are intentionally minimal and only applied\\n+        // to the prefix markers, not the full line.\\n+        if (this.isTTY && !this.isJsonLike) {\\n+          const reset = '\\\\x1b[0m';\\n+          const dim = '\\\\x1b[2m';\\n+          const colours: Record<LogLevel, string> = {\\n+            silent: '',\\n+            error: '\\\\x1b[31m', // red\\n+            warn: '\\\\x1b[33m', // yellow\\n+            info: '\\\\x1b[36m', // cyan\\n+            verbose: '\\\\x1b[35m', // magenta\\n+            debug: '\\\\x1b[90m', // bright black / gray\\n+          };\\n+\\n+          tsToken = `${dim}${tsToken}${reset}`;\\n+\\n+          if (lvl) {\\n+            const colour = colours[lvl] || '';\\n+            if (colour) {\\n+              lvlToken = `${colour}${lvlToken}${reset}`;\\n+            }\\n+          }\\n+        }\\n+\\n+        const prefix = lvl ? `${tsToken} ${lvlToken}` : tsToken;\\n         process.stderr.write(`${prefix} ${msg}\\\\n`);\\n       } else {\\n         process.stderr.write(msg + '\\\\n');\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/ai-check-provider.ts\",\"additions\":100,\"deletions\":6,\"changes\":106,\"patch\":\"diff --git a/src/providers/ai-check-provider.ts b/src/providers/ai-check-provider.ts\\nindex 53f1a350..b41d6af1 100644\\n--- a/src/providers/ai-check-provider.ts\\n+++ b/src/providers/ai-check-provider.ts\\n@@ -186,7 +186,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     let promptContent: string;\\n \\n@@ -204,7 +205,8 @@ export class AICheckProvider extends CheckProvider {\\n       eventContext,\\n       dependencyResults,\\n       outputHistory,\\n-      args\\n+      args,\\n+      workflowInputs\\n     );\\n   }\\n \\n@@ -335,7 +337,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     // Build outputs_raw from -raw keys (aggregate parent values)\\n     const outputsRaw: Record<string, unknown> = {};\\n@@ -535,6 +538,8 @@ export class AICheckProvider extends CheckProvider {\\n       outputs_raw: outputsRaw,\\n       // Custom arguments from on_init 'with' directive\\n       args: args || {},\\n+      // Workflow inputs (for nested workflow steps to access parent inputs like {{ inputs.context }})\\n+      inputs: workflowInputs || {},\\n     };\\n \\n     try {\\n@@ -621,7 +626,8 @@ export class AICheckProvider extends CheckProvider {\\n         console.error(`[ai-exec] step=${String((config as any).checkName || 'unknown')}`);\\n       }\\n     } catch {}\\n-    // Extract AI configuration - only set properties that are explicitly provided\\n+    // Extract AI configuration - only set properties that are explicitly provided.\\n+    // Workspace / allowedFolders will be derived below from the execution context.\\n     const aiConfig: AIReviewConfig = {};\\n \\n     // Check-level AI configuration (ai object)\\n@@ -671,6 +677,9 @@ export class AICheckProvider extends CheckProvider {\\n       if (aiAny.bashConfig !== undefined) {\\n         aiConfig.bashConfig = aiAny.bashConfig as import('../types/config').BashConfig;\\n       }\\n+      if (aiAny.completion_prompt !== undefined) {\\n+        aiConfig.completionPrompt = aiAny.completion_prompt as string;\\n+      }\\n       if (aiAny.skip_code_context !== undefined) {\\n         // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n         (aiConfig as any).skip_code_context = aiAny.skip_code_context as boolean;\\n@@ -691,6 +700,90 @@ export class AICheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Derive workspace-aware allowedFolders for ProbeAgent when workspace\\n+    // isolation is enabled. This ensures tools like search/query operate\\n+    // inside the isolated workspace (and its project symlinks) instead of\\n+    // the Visor repository root.\\n+    // Folder names are human-readable (tyk-docs, visor2) thanks to WorkspaceManager.\\n+    try {\\n+      const ctxAny: any = sessionInfo as any;\\n+      const parentCtx = ctxAny?._parentContext;\\n+      const workspace = parentCtx?.workspace;\\n+\\n+      // Enhanced debug logging for workspace propagation diagnosis\\n+      logger.debug(`[AI Provider] Workspace detection for check '${(config as any).checkName || 'unknown'}':`);\\n+      logger.debug(`[AI Provider]   sessionInfo exists: ${!!sessionInfo}`);\\n+      logger.debug(`[AI Provider]   _parentContext exists: ${!!parentCtx}`);\\n+      logger.debug(`[AI Provider]   workspace exists: ${!!workspace}`);\\n+      if (workspace) {\\n+        logger.debug(`[AI Provider]   workspace.isEnabled exists: ${typeof workspace.isEnabled === 'function'}`);\\n+        logger.debug(`[AI Provider]   workspace.isEnabled(): ${typeof workspace.isEnabled === 'function' ? workspace.isEnabled() : 'N/A'}`);\\n+        const projectCount = typeof workspace.listProjects === 'function' ? workspace.listProjects()?.length : 'N/A';\\n+        logger.debug(`[AI Provider]   workspace.listProjects() count: ${projectCount}`);\\n+      }\\n+\\n+      if (workspace && typeof workspace.isEnabled === 'function' && workspace.isEnabled()) {\\n+        const folders: string[] = [];\\n+        let workspaceRoot: string | undefined;\\n+        let mainProjectPath: string | undefined;\\n+        try {\\n+          const info = workspace.getWorkspaceInfo?.();\\n+          if (info && typeof info.workspacePath === 'string') {\\n+            workspaceRoot = info.workspacePath;\\n+            mainProjectPath = info.mainProjectPath;\\n+            // Add workspace root\\n+            folders.push(info.workspacePath);\\n+            // Include main project path (has human-readable name like \\\"visor2\\\")\\n+            if (mainProjectPath) {\\n+              folders.push(mainProjectPath);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore workspace info errors\\n+        }\\n+        try {\\n+          const projects = workspace.listProjects?.() || [];\\n+          for (const proj of projects as any[]) {\\n+            if (proj && typeof proj.path === 'string') {\\n+              // Project paths have human-readable names (tyk-docs, not checkout-tyk-docs)\\n+              folders.push(proj.path);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore project listing errors\\n+        }\\n+        const unique = Array.from(new Set(folders.filter(p => typeof p === 'string' && p)));\\n+        if (unique.length > 0 && workspaceRoot) {\\n+          (aiConfig as any).allowedFolders = unique;\\n+          // Set path and cwd to workspace root - AI will run with cwd set to workspace\\n+          // Both are set for compatibility: path for older probe versions, cwd for rc175+\\n+          (aiConfig as any).path = workspaceRoot;\\n+          (aiConfig as any).cwd = workspaceRoot;\\n+          // Also set workspacePath for the AI to know the root\\n+          (aiConfig as any).workspacePath = workspaceRoot;\\n+          logger.debug(`[AI Provider] Workspace isolation enabled:`);\\n+          logger.debug(`[AI Provider]   workspaceRoot (cwd): ${workspaceRoot}`);\\n+          logger.debug(`[AI Provider]   allowedFolders: ${JSON.stringify(unique)}`);\\n+        }\\n+      } else if (parentCtx && typeof parentCtx.workingDirectory === 'string') {\\n+        // Fallback: when workspace is not available (or disabled), still\\n+        // constrain tools to the engine's working directory so ProbeAgent\\n+        // operates inside the same logical root as the state machine. This\\n+        // also ensures nested workflows (e.g. code-question-helper) use the\\n+        // workspace main project path once initializeWorkspace has updated\\n+        // the parent context.\\n+        if (!(aiConfig as any).allowedFolders) {\\n+          (aiConfig as any).allowedFolders = [parentCtx.workingDirectory];\\n+        }\\n+        if (!(aiConfig as any).path) {\\n+          (aiConfig as any).path = parentCtx.workingDirectory;\\n+          (aiConfig as any).cwd = parentCtx.workingDirectory;\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only; fall back to defaults on error.\\n+    }\\n+\\n     // Check-level AI model and provider (top-level properties)\\n     if (config.ai_model !== undefined) {\\n       aiConfig.model = config.ai_model as string;\\n@@ -876,7 +969,8 @@ export class AICheckProvider extends CheckProvider {\\n       ctxWithStage,\\n       _dependencyResults,\\n       (config as any).__outputHistory as Map<string, unknown[]> | undefined,\\n-      (sessionInfo as any)?.args\\n+      (sessionInfo as any)?.args,\\n+      (config as any).workflowInputs as Record<string, unknown> | undefined\\n     );\\n \\n     // Optional persona (vendor extension): ai.ai_persona or ai_persona.\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/command-check-provider.ts\",\"additions\":50,\"deletions\":16,\"changes\":66,\"patch\":\"diff --git a/src/providers/command-check-provider.ts b/src/providers/command-check-provider.ts\\nindex e0c9d634..bf7ffd43 100644\\n--- a/src/providers/command-check-provider.ts\\n+++ b/src/providers/command-check-provider.ts\\n@@ -131,7 +131,8 @@ export class CommandCheckProvider extends CheckProvider {\\n       // New: outputs_raw exposes aggregate values (e.g., full arrays for forEach parents)\\n       outputs_raw: outputsRaw,\\n       // Workflow inputs (when executing within a workflow)\\n-      inputs: context?.workflowInputs || {},\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any).workflowInputs || context?.workflowInputs || {},\\n       // Custom arguments from on_init 'with' directive\\n       args: context?.args || {},\\n       env: this.getSafeEnvironmentVariables(),\\n@@ -178,17 +179,31 @@ export class CommandCheckProvider extends CheckProvider {\\n           mock = rawMock as Record<string, unknown>;\\n         }\\n         const m = mock as { stdout?: string; stderr?: string; exit_code?: number; exit?: number };\\n-        let out: unknown = m.stdout ?? '';\\n-        try {\\n-          if (\\n-            typeof out === 'string' &&\\n-            (out.trim().startsWith('{') || out.trim().startsWith('['))\\n-          ) {\\n-            out = JSON.parse(out);\\n-          }\\n-        } catch {}\\n-        const code =\\n-          typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0;\\n+\\n+        // Check if this looks like a command mock (has stdout/stderr/exit_code) or direct data output\\n+        const isCommandMock = m.stdout !== undefined || m.stderr !== undefined ||\\n+          m.exit_code !== undefined || m.exit !== undefined;\\n+\\n+        let out: unknown;\\n+        if (isCommandMock) {\\n+          // Traditional command mock format: { stdout: \\\"...\\\", exit_code: 0 }\\n+          out = m.stdout ?? '';\\n+          try {\\n+            if (\\n+              typeof out === 'string' &&\\n+              (out.trim().startsWith('{') || out.trim().startsWith('['))\\n+            ) {\\n+              out = JSON.parse(out);\\n+            }\\n+          } catch {}\\n+        } else {\\n+          // Direct data mock format: { data: [...], count: 1 } - use as-is\\n+          out = mock;\\n+        }\\n+\\n+        const code = isCommandMock\\n+          ? (typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0)\\n+          : 0;\\n         if (code !== 0) {\\n           return {\\n             issues: [\\n@@ -231,9 +246,8 @@ export class CommandCheckProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Get timeout from config (in seconds) or use default (60 seconds)\\n-      const timeoutSeconds = (config.timeout as number) || 60;\\n-      const timeoutMs = timeoutSeconds * 1000;\\n+      // Get timeout from config (in milliseconds) or use default (60 seconds = 60000ms)\\n+      const timeoutMs = (config.timeout as number) || 60000;\\n \\n       // Normalize only the eval payload for `node -e|--eval` invocations that may contain\\n       // literal newlines due to YAML processing (\\\"\\\\n\\\" -> newline). We re-escape newlines\\n@@ -1533,6 +1547,26 @@ ${bodyWithReturn}\\n       return null;\\n     }\\n \\n+    // Require at least one issue-specific field beyond just a message-like field.\\n+    // This prevents objects like {text: \\\"...\\\"} from being treated as issues.\\n+    const hasIssueField =\\n+      data.file !== undefined ||\\n+      data.path !== undefined ||\\n+      data.filename !== undefined ||\\n+      data.line !== undefined ||\\n+      data.startLine !== undefined ||\\n+      data.lineNumber !== undefined ||\\n+      data.severity !== undefined ||\\n+      data.level !== undefined ||\\n+      data.priority !== undefined ||\\n+      data.ruleId !== undefined ||\\n+      data.rule !== undefined ||\\n+      data.category !== undefined ||\\n+      data.type !== undefined;\\n+    if (!hasIssueField) {\\n+      return null;\\n+    }\\n+\\n     const allowedSeverities = new Set(['info', 'warning', 'error', 'critical']);\\n     const severityRaw = this.toTrimmedString(data.severity || data.level || data.priority);\\n     let severity: ReviewIssue['severity'] = 'warning';\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/git-checkout-provider.ts\",\"additions\":41,\"deletions\":6,\"changes\":47,\"patch\":\"diff --git a/src/providers/git-checkout-provider.ts b/src/providers/git-checkout-provider.ts\\nindex baac0001..6d3a2852 100644\\n--- a/src/providers/git-checkout-provider.ts\\n+++ b/src/providers/git-checkout-provider.ts\\n@@ -69,6 +69,16 @@ export class GitCheckoutProvider extends CheckProvider {\\n       return false;\\n     }\\n \\n+    if (checkoutConfig.clone_timeout_ms !== undefined) {\\n+      if (\\n+        typeof checkoutConfig.clone_timeout_ms !== 'number' ||\\n+        checkoutConfig.clone_timeout_ms <= 0\\n+      ) {\\n+        logger.error('Invalid config: clone_timeout_ms must be a positive number (milliseconds)');\\n+        return false;\\n+      }\\n+    }\\n+\\n     return true;\\n   }\\n \\n@@ -91,7 +101,13 @@ export class GitCheckoutProvider extends CheckProvider {\\n       );\\n \\n       // Resolve dynamic variables\\n-      const resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      let resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      // If ref resolves to an empty string, fall back to HEAD so we rely on\\n+      // the repository's default branch. This keeps downstream git commands\\n+      // well-defined and matches existing expectations/tests.\\n+      if (!resolvedRef || resolvedRef.trim().length === 0) {\\n+        resolvedRef = 'HEAD';\\n+      }\\n       const resolvedRepository = checkoutConfig.repository\\n         ? await this.liquid.parseAndRender(checkoutConfig.repository, templateContext)\\n         : process.env.GITHUB_REPOSITORY || 'unknown/unknown';\\n@@ -118,6 +134,7 @@ export class GitCheckoutProvider extends CheckProvider {\\n           clean: checkoutConfig.clean !== false, // Default: true\\n           workflowId: (context as any)?.workflowId,\\n           fetchDepth: checkoutConfig.fetch_depth,\\n+          cloneTimeoutMs: checkoutConfig.clone_timeout_ms,\\n         }\\n       );\\n \\n@@ -134,18 +151,35 @@ export class GitCheckoutProvider extends CheckProvider {\\n \\n       // Add project to workspace if workspace isolation is enabled\\n       const workspace = (context as any)?._parentContext?.workspace;\\n+\\n+      // Enhanced debug logging for workspace project addition diagnosis\\n+      const checkName = (config as any)?.checkName || 'unknown';\\n+      logger.info(`[GitCheckout] Workspace check for '${checkName}':`);\\n+      logger.info(`[GitCheckout]   _parentContext exists: ${!!(context as any)?._parentContext}`);\\n+      logger.info(`[GitCheckout]   workspace exists: ${!!workspace}`);\\n+      logger.info(`[GitCheckout]   workspace.isEnabled(): ${workspace?.isEnabled?.() ?? 'N/A'}`);\\n+      if (workspace) {\\n+        const projectCountBefore = workspace.listProjects?.()?.length ?? 'N/A';\\n+        logger.debug(`[GitCheckout]   projects before addProject: ${projectCountBefore}`);\\n+      }\\n+\\n       if (workspace?.isEnabled()) {\\n         try {\\n+          // Don't pass checkName as description - let workspace use repo name\\n+          // This results in human-readable names like \\\"tyk-docs\\\" instead of \\\"checkout-tyk-docs\\\"\\n           const workspacePath = await workspace.addProject(\\n             resolvedRepository,\\n-            worktree.path,\\n-            checkoutConfig.checkName\\n+            worktree.path\\n           );\\n           output.workspace_path = workspacePath;\\n-          logger.debug(`Added project to workspace: ${workspacePath}`);\\n+          const projectCountAfter = workspace.listProjects?.()?.length ?? 'N/A';\\n+          logger.debug(`[GitCheckout] Added project to workspace: ${workspacePath}`);\\n+          logger.debug(`[GitCheckout]   projects after addProject: ${projectCountAfter}`);\\n         } catch (error) {\\n           logger.warn(`Failed to add project to workspace: ${error}`);\\n         }\\n+      } else {\\n+        logger.debug(`[GitCheckout] Workspace not enabled, skipping addProject`);\\n       }\\n \\n       logger.info(\\n@@ -228,7 +262,8 @@ export class GitCheckoutProvider extends CheckProvider {\\n       outputs: outputsObj,\\n       outputs_history: historyObj,\\n       env: safeEnv,\\n-      inputs: context?.workflowInputs,\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any)?.workflowInputs || context?.workflowInputs,\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/http-client-provider.ts\",\"additions\":279,\"deletions\":35,\"changes\":314,\"patch\":\"diff --git a/src/providers/http-client-provider.ts b/src/providers/http-client-provider.ts\\nindex 4d8c41be..6db81591 100644\\n--- a/src/providers/http-client-provider.ts\\n+++ b/src/providers/http-client-provider.ts\\n@@ -4,18 +4,29 @@ import { ReviewSummary } from '../reviewer';\\n import { Liquid } from 'liquidjs';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n import { EnvironmentResolver } from '../utils/env-resolver';\\n+import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n+import { buildProviderTemplateContext } from '../utils/template-context';\\n+import Sandbox from '@nyariv/sandboxjs';\\n+import { logger } from '../logger';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n \\n /**\\n  * Check provider that fetches data from HTTP endpoints\\n  */\\n export class HttpClientProvider extends CheckProvider {\\n   private liquid: Liquid;\\n+  private sandbox?: Sandbox;\\n \\n   constructor() {\\n     super();\\n     this.liquid = createExtendedLiquid();\\n   }\\n \\n+  private createSecureSandbox(): Sandbox {\\n+    return createSecureSandbox();\\n+  }\\n+\\n   getName(): string {\\n     return 'http_client';\\n   }\\n@@ -61,50 +72,124 @@ export class HttpClientProvider extends CheckProvider {\\n     const headers = (config.headers as Record<string, string>) || {};\\n     const timeout = (config.timeout as number) || 30000;\\n     const transform = config.transform as string | undefined;\\n+    const transformJs = config.transform_js as string | undefined;\\n     const bodyTemplate = config.body as string | undefined;\\n+    const outputFileTemplate = config.output_file as string | undefined;\\n+    const skipIfExists = config.skip_if_exists !== false; // Default true for caching\\n \\n-    try {\\n-      // Prepare template context for URL and body\\n-      const templateContext = {\\n-        pr: {\\n-          number: prInfo.number,\\n-          title: prInfo.title,\\n-          body: prInfo.body,\\n-          author: prInfo.author,\\n-          base: prInfo.base,\\n-          head: prInfo.head,\\n-          totalAdditions: prInfo.totalAdditions,\\n-          totalDeletions: prInfo.totalDeletions,\\n-        },\\n-        outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-        env: process.env,\\n-      };\\n+    // Track resolved URL for error messages\\n+    let resolvedUrlForErrors = url;\\n \\n-      // Render URL with template if it contains liquid syntax\\n-      let renderedUrl = url;\\n-      if (url.includes('{{') || url.includes('{%')) {\\n-        renderedUrl = await this.liquid.parseAndRender(url, templateContext);\\n+    try {\\n+      // Use shared template context builder for consistent output extraction\\n+      const templateContext = buildProviderTemplateContext(\\n+        prInfo,\\n+        dependencyResults,\\n+        undefined, // memoryStore\\n+        undefined, // outputHistory\\n+        undefined, // stageHistoryBase\\n+        { attachMemoryReadHelpers: false }\\n+      );\\n+      // Add env to context for shell-style variable resolution\\n+      (templateContext as Record<string, unknown>).env = process.env;\\n+\\n+      // First resolve shell-style environment variables (${VAR}, $VAR, ${{ env.VAR }})\\n+      let renderedUrl = String(EnvironmentResolver.resolveValue(url));\\n+      resolvedUrlForErrors = renderedUrl; // Track for error messages\\n+\\n+      // Then render Liquid templates if present\\n+      if (renderedUrl.includes('{{') || renderedUrl.includes('{%')) {\\n+        renderedUrl = await this.liquid.parseAndRender(renderedUrl, templateContext);\\n+        resolvedUrlForErrors = renderedUrl; // Update after Liquid rendering\\n       }\\n \\n       // Prepare request body if provided\\n       let requestBody: string | undefined;\\n       if (bodyTemplate) {\\n-        const renderedBody = await this.liquid.parseAndRender(bodyTemplate, templateContext);\\n-        requestBody = renderedBody;\\n+        // First resolve shell-style environment variables\\n+        let resolvedBody = String(EnvironmentResolver.resolveValue(bodyTemplate));\\n+        // Then render Liquid templates if present\\n+        if (resolvedBody.includes('{{') || resolvedBody.includes('{%')) {\\n+          resolvedBody = await this.liquid.parseAndRender(resolvedBody, templateContext);\\n+        }\\n+        requestBody = resolvedBody;\\n       }\\n \\n-      // Resolve environment variables in headers\\n-      const resolvedHeaders = EnvironmentResolver.resolveHeaders(headers);\\n+      // Resolve environment variables and Liquid templates in headers\\n+      const resolvedHeaders: Record<string, string> = {};\\n+      for (const [key, value] of Object.entries(headers)) {\\n+        let resolvedValue = String(EnvironmentResolver.resolveValue(value));\\n+        // Render Liquid templates if present\\n+        if (resolvedValue.includes('{{') || resolvedValue.includes('{%')) {\\n+          resolvedValue = await this.liquid.parseAndRender(resolvedValue, templateContext);\\n+        }\\n+        resolvedHeaders[key] = resolvedValue;\\n+        // Debug auth header (mask most of the value for security)\\n+        if (key.toLowerCase() === 'authorization') {\\n+          const maskedValue = resolvedValue.length > 20\\n+            ? `${resolvedValue.substring(0, 15)}...${resolvedValue.substring(resolvedValue.length - 5)}`\\n+            : resolvedValue;\\n+          logger.verbose(`[http_client] ${key}: ${maskedValue}`);\\n+        }\\n+      }\\n+\\n+      // Resolve output_file path if specified\\n+      let resolvedOutputFile: string | undefined;\\n+      if (outputFileTemplate) {\\n+        let outputPath = String(EnvironmentResolver.resolveValue(outputFileTemplate));\\n+        if (outputPath.includes('{{') || outputPath.includes('{%')) {\\n+          outputPath = await this.liquid.parseAndRender(outputPath, templateContext);\\n+        }\\n+        resolvedOutputFile = outputPath.trim();\\n+\\n+        // Check if file already exists (caching)\\n+        if (skipIfExists && fs.existsSync(resolvedOutputFile)) {\\n+          const stats = fs.statSync(resolvedOutputFile);\\n+          logger.verbose(`[http_client] File cached: ${resolvedOutputFile} (${stats.size} bytes)`);\\n+          return {\\n+            issues: [],\\n+            file_path: resolvedOutputFile,\\n+            size: stats.size,\\n+            cached: true,\\n+          } as unknown as ReviewSummary;\\n+        }\\n+      }\\n \\n       // Test hook: mock HTTP response for this step\\n       const stepName = (config as any).checkName || 'unknown';\\n       const mock = context?.hooks?.mockForStep?.(String(stepName));\\n-      const data =\\n-        mock !== undefined\\n-          ? mock\\n-          : await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n \\n-      // Apply transformation if specified\\n+      // If mock is provided, return it directly as the step output (with issues: [])\\n+      if (mock !== undefined) {\\n+        const mockObj = typeof mock === 'object' && mock !== null ? mock : { data: mock };\\n+        return {\\n+          issues: [],\\n+          ...mockObj,\\n+        } as unknown as ReviewSummary & { data: unknown };\\n+      }\\n+\\n+      // Debug log the request for troubleshooting\\n+      logger.verbose(`[http_client] ${method} ${renderedUrl}`);\\n+      if (requestBody) {\\n+        logger.verbose(`[http_client] Body: ${requestBody.substring(0, 500)}${requestBody.length > 500 ? '...' : ''}`);\\n+      }\\n+\\n+      // If output_file is specified, download to file instead of returning data\\n+      if (resolvedOutputFile) {\\n+        const fileResult = await this.downloadToFile(\\n+          renderedUrl,\\n+          method,\\n+          resolvedHeaders,\\n+          requestBody,\\n+          timeout,\\n+          resolvedOutputFile\\n+        );\\n+        return fileResult;\\n+      }\\n+\\n+      const data = await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n+\\n+      // Apply Liquid transformation if specified\\n       let processedData = data;\\n       if (transform) {\\n         try {\\n@@ -136,12 +221,53 @@ export class HttpClientProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Return the fetched data as a custom field for dependent checks to access\\n+      // Apply JavaScript transformation if specified\\n+      if (transformJs) {\\n+        try {\\n+          if (!this.sandbox) {\\n+            this.sandbox = this.createSecureSandbox();\\n+          }\\n+\\n+          // Create scope for JavaScript transform (scope, not context)\\n+          const jsScope: Record<string, unknown> = {\\n+            output: data,\\n+            pr: templateContext.pr,\\n+            outputs: templateContext.outputs,\\n+            env: process.env,\\n+          };\\n+\\n+          const result = compileAndRun(this.sandbox, transformJs, jsScope, {\\n+            injectLog: true,\\n+            logPrefix: 'üîç [transform_js]',\\n+            wrapFunction: true,\\n+          });\\n+          processedData = result;\\n+          logger.verbose(`‚úì Applied JavaScript transform successfully`);\\n+        } catch (error) {\\n+          logger.error(\\n+            `‚úó Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`\\n+          );\\n+          return {\\n+            issues: [\\n+              {\\n+                file: 'http_client',\\n+                line: 0,\\n+                ruleId: 'http_client/transform_js_error',\\n+                message: `Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              },\\n+            ],\\n+          };\\n+        }\\n+      }\\n+\\n+      // Return the fetched data in the standard `output` property for guarantee evaluation\\n+      // This is consistent with other providers (script, command, ai, etc.)\\n       return {\\n         issues: [],\\n-        // Add custom data field that will be passed through to dependent checks\\n-        data: processedData,\\n-      } as ReviewSummary & { data: unknown };\\n+        output: processedData,\\n+      } as unknown as ReviewSummary;\\n     } catch (error) {\\n       return {\\n         issues: [\\n@@ -149,7 +275,7 @@ export class HttpClientProvider extends CheckProvider {\\n             file: 'http_client',\\n             line: 0,\\n             ruleId: 'http_client/fetch_error',\\n-            message: `Failed to fetch from ${url}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            message: `Failed to fetch from ${resolvedUrlForErrors}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n             severity: 'error',\\n             category: 'logic',\\n           },\\n@@ -198,7 +324,14 @@ export class HttpClientProvider extends CheckProvider {\\n \\n       clearTimeout(timeoutId);\\n \\n+      logger.verbose(`[http_client] Response: ${response.status} ${response.statusText}`);\\n+\\n       if (!response.ok) {\\n+        // Log response body for debugging auth failures\\n+        try {\\n+          const errorBody = await response.text();\\n+          logger.warn(`[http_client] Error body: ${errorBody.substring(0, 500)}`);\\n+        } catch {}\\n         throw new Error(`HTTP ${response.status}: ${response.statusText}`);\\n       }\\n \\n@@ -233,6 +366,112 @@ export class HttpClientProvider extends CheckProvider {\\n     }\\n   }\\n \\n+  private async downloadToFile(\\n+    url: string,\\n+    method: string,\\n+    headers: Record<string, string>,\\n+    body: string | undefined,\\n+    timeout: number,\\n+    outputFile: string\\n+  ): Promise<ReviewSummary> {\\n+    // Check if fetch is available (Node 18+)\\n+    if (typeof fetch === 'undefined') {\\n+      throw new Error('HTTP client provider requires Node.js 18+ or node-fetch package');\\n+    }\\n+\\n+    const controller = new AbortController();\\n+    const timeoutId = setTimeout(() => controller.abort(), timeout);\\n+\\n+    try {\\n+      const requestOptions: RequestInit = {\\n+        method,\\n+        headers: { ...headers },\\n+        signal: controller.signal,\\n+      };\\n+\\n+      // Add body for non-GET requests\\n+      if (method !== 'GET' && body) {\\n+        requestOptions.body = body;\\n+        if (!headers['Content-Type'] && !headers['content-type']) {\\n+          requestOptions.headers = {\\n+            ...requestOptions.headers,\\n+            'Content-Type': 'application/json',\\n+          };\\n+        }\\n+      }\\n+\\n+      const response = await fetch(url, requestOptions);\\n+      clearTimeout(timeoutId);\\n+\\n+      if (!response.ok) {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_error',\\n+              message: `Failed to download file: HTTP ${response.status}: ${response.statusText}`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      // Create parent directory if it doesn't exist\\n+      const parentDir = path.dirname(outputFile);\\n+      if (parentDir && !fs.existsSync(parentDir)) {\\n+        fs.mkdirSync(parentDir, { recursive: true });\\n+      }\\n+\\n+      // Get the response as an ArrayBuffer and write to file\\n+      const arrayBuffer = await response.arrayBuffer();\\n+      const buffer = Buffer.from(arrayBuffer);\\n+      fs.writeFileSync(outputFile, buffer);\\n+\\n+      const contentType = response.headers.get('content-type') || 'application/octet-stream';\\n+      logger.verbose(`[http_client] Downloaded: ${outputFile} (${buffer.length} bytes)`);\\n+\\n+      return {\\n+        issues: [],\\n+        file_path: outputFile,\\n+        size: buffer.length,\\n+        content_type: contentType,\\n+        cached: false,\\n+      } as unknown as ReviewSummary;\\n+    } catch (error: unknown) {\\n+      clearTimeout(timeoutId);\\n+\\n+      if (error instanceof Error && error.name === 'AbortError') {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_timeout',\\n+              message: `Download timed out after ${timeout}ms`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      return {\\n+        issues: [\\n+          {\\n+            file: 'http_client',\\n+            line: 0,\\n+            ruleId: 'http_client/download_error',\\n+            message: `Failed to download file: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            severity: 'error',\\n+            category: 'logic',\\n+          },\\n+        ],\\n+      };\\n+    }\\n+  }\\n+\\n   getSupportedConfigKeys(): string[] {\\n     return [\\n       'type',\\n@@ -241,7 +480,10 @@ export class HttpClientProvider extends CheckProvider {\\n       'headers',\\n       'body',\\n       'transform',\\n+      'transform_js',\\n       'timeout',\\n+      'output_file',\\n+      'skip_if_exists',\\n       'depends_on',\\n       'on',\\n       'if',\\n@@ -261,6 +503,8 @@ export class HttpClientProvider extends CheckProvider {\\n       'Network access to the endpoint',\\n       'Optional: Transform template for processing response data',\\n       'Optional: Body template for POST/PUT requests',\\n+      'Optional: output_file path to download response to a file',\\n+      'Optional: skip_if_exists (default: true) to enable caching for file downloads',\\n     ];\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/log-check-provider.ts\",\"additions\":19,\"deletions\":8,\"changes\":27,\"patch\":\"diff --git a/src/providers/log-check-provider.ts b/src/providers/log-check-provider.ts\\nindex 9c6332d1..b57f632c 100644\\n--- a/src/providers/log-check-provider.ts\\n+++ b/src/providers/log-check-provider.ts\\n@@ -78,7 +78,8 @@ export class LogCheckProvider extends CheckProvider {\\n       includeDependencies,\\n       includeMetadata,\\n       config.__outputHistory as Map<string, unknown[]> | undefined,\\n-      context\\n+      context,\\n+      config\\n     );\\n \\n     // Render the log message template\\n@@ -100,12 +101,20 @@ export class LogCheckProvider extends CheckProvider {\\n     else if (level === 'debug') logger.debug(logOutput);\\n     else logger.info(logOutput);\\n \\n-    // Return with the log content as custom data for dependent checks\\n-    return {\\n+    // Return with the log content as custom data for dependent checks.\\n+    // For chat-style logs (group: chat), also expose a structured\\n+    // `output.text` field so frontends like Slack can post a clean\\n+    // human-facing message without the level prefix.\\n+    const summary: ReviewSummary & { logOutput: string; output?: unknown } = {\\n       issues: [],\\n-      // Add log output as custom field\\n       logOutput,\\n-    } as ReviewSummary & { logOutput: string };\\n+    };\\n+\\n+    if ((config as any).group === 'chat') {\\n+      (summary as any).output = { text: renderedMessage };\\n+    }\\n+\\n+    return summary;\\n   }\\n \\n   private buildTemplateContext(\\n@@ -115,7 +124,8 @@ export class LogCheckProvider extends CheckProvider {\\n     _includeDependencies: boolean = true,\\n     includeMetadata: boolean = true,\\n     outputHistory?: Map<string, unknown[]>,\\n-    executionContext?: ExecutionContext\\n+    executionContext?: ExecutionContext,\\n+    config?: CheckProviderConfig\\n   ): Record<string, unknown> {\\n     const context: Record<string, unknown> = {};\\n \\n@@ -197,7 +207,8 @@ export class LogCheckProvider extends CheckProvider {\\n     }\\n \\n     // Add workflow inputs if available\\n-    const workflowInputs = executionContext?.workflowInputs || {};\\n+    // Check config first (set by projectWorkflowToGraph), then fall back to executionContext\\n+    const workflowInputs = (config as any)?.workflowInputs || executionContext?.workflowInputs || {};\\n     logger.debug(\\n       `[LogProvider] Adding ${Object.keys(workflowInputs).length} workflow inputs to context`\\n     );\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/script-check-provider.ts\",\"additions\":37,\"deletions\":1,\"changes\":38,\"patch\":\"diff --git a/src/providers/script-check-provider.ts b/src/providers/script-check-provider.ts\\nindex 9825bb69..947dbd73 100644\\n--- a/src/providers/script-check-provider.ts\\n+++ b/src/providers/script-check-provider.ts\\n@@ -59,6 +59,16 @@ export class ScriptCheckProvider extends CheckProvider {\\n       reuseSession?: boolean;\\n     } & import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n+    // Test hook: mock output for this step (short-circuit execution)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = _sessionInfo?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        // Return mock directly as step output\\n+        return { issues: [], output: mock } as ReviewSummary & { output: unknown };\\n+      }\\n+    } catch {}\\n+\\n     const script = String(config.content || '');\\n     const memoryStore = MemoryStore.getInstance();\\n     const ctx = buildProviderTemplateContext(\\n@@ -72,10 +82,36 @@ export class ScriptCheckProvider extends CheckProvider {\\n     // Keep provider quiet by default; no step-specific debug\\n     // (historical ad-hoc logs removed to avoid hardcoding step names).\\n \\n+    // Add workflow inputs to the context\\n+    const inputs = (config as any).workflowInputs || _sessionInfo?.workflowInputs || {};\\n+    (ctx as any).inputs = inputs;\\n+\\n+    // Add environment variables to context (consistent with http-client-provider)\\n+    (ctx as any).env = process.env;\\n+\\n     // Attach synchronous memory ops consistent with memory provider\\n     const { ops, needsSave } = createSyncMemoryOps(memoryStore);\\n     (ctx as any).memory = ops as unknown as Record<string, unknown>;\\n \\n+    // Add helper functions to the context\\n+    (ctx as any).escapeXml = (str: unknown): string => {\\n+      if (str == null) return '';\\n+      return String(str)\\n+        .replace(/&/g, '&amp;')\\n+        .replace(/</g, '&lt;')\\n+        .replace(/>/g, '&gt;')\\n+        .replace(/\\\"/g, '&quot;')\\n+        .replace(/'/g, '&apos;');\\n+    };\\n+\\n+    // Add btoa/atob for base64 encoding/decoding (browser API polyfill)\\n+    (ctx as any).btoa = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'binary').toString('base64');\\n+    };\\n+    (ctx as any).atob = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'base64').toString('binary');\\n+    };\\n+\\n     // Evaluate the script in a secure sandbox (per-execution instance)\\n     const sandbox = this.createSecureSandbox();\\n     let result: unknown;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/workflow-check-provider.ts\",\"additions\":164,\"deletions\":43,\"changes\":207,\"patch\":\"diff --git a/src/providers/workflow-check-provider.ts b/src/providers/workflow-check-provider.ts\\nindex 116d4dff..681703ce 100644\\n--- a/src/providers/workflow-check-provider.ts\\n+++ b/src/providers/workflow-check-provider.ts\\n@@ -67,34 +67,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n   ): Promise<ReviewSummary> {\\n     const cfg = config as CheckProviderConfig & { workflow?: string; config?: string };\\n     const isConfigPathMode = !!cfg.config && !cfg.workflow;\\n+    const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n \\n-    // Test harness support: allow mocking workflow checks as a black box.\\n-    // If a mock is provided for this step, short-circuit nested execution and\\n-    // return a ReviewSummary with optional output field.\\n-    try {\\n-      const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n-      // test-runner passes hooks on execution context\\n-      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n-      if (mock !== undefined) {\\n-        const ms = mock as any;\\n-        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n-        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n-        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n-        const summary: ReviewSummary & { output?: unknown } = {\\n-          issues: issuesArr,\\n-          output: out,\\n-          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n-        } as any;\\n-        return summary;\\n-      }\\n-    } catch {}\\n-\\n-    // Resolve workflow definition\\n+    // Resolve workflow definition FIRST (needed for input preparation and validation)\\n     let workflow: WorkflowDefinition | undefined;\\n     let workflowId = cfg.workflow as string | undefined;\\n \\n     if (isConfigPathMode) {\\n-      const parentCwd = ((context as any)?._parentContext?.workingDirectory ||\\n+      // Use originalWorkingDirectory for config file resolution - workflow configs\\n+      // should be loaded from the original project path, not the sandbox\\n+      const parentCwd = ((context as any)?._parentContext?.originalWorkingDirectory ||\\n+        (context as any)?._parentContext?.workingDirectory ||\\n+        (context as any)?.originalWorkingDirectory ||\\n         (context as any)?.workingDirectory ||\\n         process.cwd()) as string;\\n       workflow = await this.loadWorkflowFromConfigPath(String(cfg.config), parentCwd);\\n@@ -109,9 +93,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       logger.info(`Executing workflow '${workflowId}'`);\\n     }\\n \\n-    // Prepare inputs\\n+    // Prepare inputs - do this BEFORE mock check so we can validate inputs even when mocked\\n+    // This allows tests to assert that the correct inputs were passed to workflow steps\\n     const inputs = await this.prepareInputs(workflow, config, prInfo, dependencyResults);\\n \\n+    // Capture resolved workflow inputs for testing assertions (reuse prompt capture infrastructure)\\n+    // This allows using `prompts` assertions with `provider: 'workflow'` to verify inputs\\n+    try {\\n+      // Serialize inputs to capture for assertions - specifically the context field\\n+      // which contains the rendered template with dependency outputs\\n+      const inputsCapture = Object.entries(inputs)\\n+        .map(([k, v]) => `${k}: ${typeof v === 'string' ? v : JSON.stringify(v)}`)\\n+        .join('\\\\n\\\\n');\\n+      (context as any)?.hooks?.onPromptCaptured?.({\\n+        step: String(stepName),\\n+        provider: 'workflow',\\n+        prompt: inputsCapture,\\n+      });\\n+    } catch {\\n+      // Ignore capture errors - this is only for testing\\n+    }\\n+\\n     // Validate inputs\\n     const validation = this.registry.validateInputs(workflow, inputs);\\n     if (!validation.valid) {\\n@@ -119,6 +121,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       throw new Error(`Invalid workflow inputs: ${errors}`);\\n     }\\n \\n+    // Test harness support: allow mocking workflow checks as a black box.\\n+    // If a mock is provided for this step, short-circuit nested execution and\\n+    // return a ReviewSummary with optional output field.\\n+    // NOTE: This happens AFTER input preparation/validation so tests can assert inputs are correct\\n+    try {\\n+      // test-runner passes hooks on execution context\\n+      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        const ms = mock as any;\\n+        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n+        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n+        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n+        const summary: ReviewSummary & { output?: unknown } = {\\n+          issues: issuesArr,\\n+          output: out,\\n+          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n+        } as any;\\n+        return summary;\\n+      }\\n+    } catch {}\\n+\\n     // Apply overrides to workflow steps if specified\\n     const modifiedWorkflow = this.applyOverrides(workflow, config);\\n \\n@@ -211,6 +234,80 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Extract eventContext for slack/conversation\\n+    const eventContext = config.eventContext || {};\\n+\\n+    // Debug logging for conversation context\\n+    logger.debug(`[WorkflowProvider] prepareInputs for ${workflow.id}`);\\n+    logger.debug(`[WorkflowProvider] eventContext keys: ${Object.keys(eventContext).join(', ') || 'none'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.slack: ${eventContext.slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.conversation: ${(eventContext as any).conversation ? 'present' : 'absent'}`);\\n+\\n+    // Extract slack context (if provided via eventContext.slack)\\n+    const slack = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        const slackCtx = anyCtx?.slack;\\n+        if (slackCtx && typeof slackCtx === 'object') return slackCtx;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Extract unified conversation context across transports (Slack & GitHub)\\n+    const conversation = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        if (anyCtx?.slack?.conversation) return anyCtx.slack.conversation;\\n+        if (anyCtx?.github?.conversation) return anyCtx.github.conversation;\\n+        if (anyCtx?.conversation) return anyCtx.conversation;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Debug logging for extracted context\\n+    logger.debug(`[WorkflowProvider] slack extracted: ${slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] conversation extracted: ${conversation ? 'present' : 'absent'}`);\\n+    if (conversation) {\\n+      logger.debug(`[WorkflowProvider] conversation.messages count: ${Array.isArray((conversation as any).messages) ? (conversation as any).messages.length : 0}`);\\n+    }\\n+\\n+    // Extract output history from config (passed via __outputHistory)\\n+    const outputHistory = (config as any).__outputHistory as Map<string, unknown[]> | undefined;\\n+    const outputs_history: Record<string, unknown[]> = {};\\n+    if (outputHistory) {\\n+      for (const [k, v] of outputHistory.entries()) {\\n+        outputs_history[k] = v;\\n+      }\\n+    }\\n+\\n+    // Build template context with all available data\\n+    // Extract .output from each dependency result so that outputs['step-name'].field works naturally\\n+    // (not outputs['step-name'].output.field)\\n+    const outputsMap: Record<string, unknown> = {};\\n+    logger.debug(`[WorkflowProvider] dependencyResults: ${dependencyResults ? dependencyResults.size : 'undefined'} entries`);\\n+    if (dependencyResults) {\\n+      for (const [key, result] of dependencyResults.entries()) {\\n+        // Extract the output property, or use the whole result if output is undefined\\n+        const extracted = (result as any).output ?? result;\\n+        outputsMap[key] = extracted;\\n+        // Debug: log what we extracted for each dependency\\n+        const extractedKeys = extracted && typeof extracted === 'object' ? Object.keys(extracted).join(', ') : 'not-object';\\n+        logger.debug(`[WorkflowProvider] outputs['${key}']: keys=[${extractedKeys}]`);\\n+      }\\n+    }\\n+    const templateContext = {\\n+      pr: prInfo,\\n+      outputs: outputsMap,\\n+      env: process.env,\\n+      slack,\\n+      conversation,\\n+      outputs_history,\\n+    };\\n+\\n     // Apply user-provided inputs (args)\\n     const userInputs = config.args || config.workflow_inputs; // Support both for compatibility\\n     if (userInputs) {\\n@@ -219,11 +316,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         if (typeof value === 'string') {\\n           // Check if it's a Liquid template\\n           if (value.includes('{{') || value.includes('{%')) {\\n-            inputs[key] = await this.liquid.parseAndRender(value, {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            });\\n+            inputs[key] = await this.liquid.parseAndRender(value, templateContext);\\n+            // Debug: log rendered template value for important inputs\\n+            if (key === 'text' || key === 'question' || key === 'context') {\\n+              const rendered = String(inputs[key]);\\n+              logger.info(`[WorkflowProvider] Rendered '${key}' input (${rendered.length} chars): ${rendered.substring(0, 500)}${rendered.length > 500 ? '...' : ''}`);\\n+            }\\n           } else {\\n             inputs[key] = value;\\n           }\\n@@ -234,11 +332,7 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           inputs[key] = compileAndRun(\\n             sandbox,\\n             exprValue.expression,\\n-            {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            },\\n+            templateContext,\\n             { injectLog: true, logPrefix: `workflow.input.${key}` }\\n           );\\n         } else {\\n@@ -367,6 +461,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n     try {\\n       await childMemory.initialize();\\n     } catch {}\\n+    // Enhanced debug logging for workspace propagation diagnosis\\n+    const parentWorkspace = parentContext?.workspace;\\n+    logger.info(`[WorkflowProvider] Workspace propagation for nested workflow '${workflow.id}':`);\\n+    logger.info(`[WorkflowProvider]   parentContext exists: ${!!parentContext}`);\\n+    logger.info(`[WorkflowProvider]   parentContext.workspace exists: ${!!parentWorkspace}`);\\n+    if (parentWorkspace) {\\n+      logger.info(`[WorkflowProvider]   parentWorkspace.isEnabled(): ${parentWorkspace.isEnabled?.() ?? 'N/A'}`);\\n+      const projectCount = parentWorkspace.listProjects?.()?.length ?? 'N/A';\\n+      logger.info(`[WorkflowProvider]   parentWorkspace project count: ${projectCount}`);\\n+    } else {\\n+      logger.warn(`[WorkflowProvider]   NO WORKSPACE from parent - nested checkouts won't be added to workspace!`);\\n+    }\\n \\n     const childContext = {\\n       mode: 'state-machine' as const,\\n@@ -374,7 +480,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       checks: checksMetadata,\\n       journal: childJournal,\\n       memory: childMemory,\\n+      // For nested workflows we continue to execute inside the same logical\\n+      // working directory as the parent run. When workspace isolation is\\n+      // enabled on the parent engine, its WorkspaceManager is also propagated\\n+      // so that nested checks (AI, git-checkout, etc.) see the same isolated\\n+      // workspace and project symlinks instead of falling back to the Visor\\n+      // repository root.\\n       workingDirectory: parentContext?.workingDirectory || process.cwd(),\\n+      originalWorkingDirectory:\\n+        parentContext?.originalWorkingDirectory ||\\n+        parentContext?.workingDirectory ||\\n+        process.cwd(),\\n+      workspace: parentWorkspace,\\n       // Always use a fresh session for nested workflows to isolate history\\n       sessionId: uuidv4(),\\n       event: parentContext?.event || prInfo.eventType,\\n@@ -500,6 +617,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     } catch {}\\n \\n+    // Create outputs map that directly exposes .output values (not wrapped in { output, issues })\\n+    // This makes {{ outputs['check-name'] }} work naturally without needing .output\\n+    const outputsMap = Object.fromEntries(\\n+      Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n+    );\\n+\\n     for (const output of workflow.outputs) {\\n       if (output.value_js) {\\n         // JavaScript expression\\n@@ -508,10 +631,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           output.value_js,\\n           {\\n             inputs,\\n-            steps: Object.fromEntries(\\n-              Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-            ),\\n-            outputs: flat,\\n+            outputs: outputsMap,\\n+            // Keep 'steps' as alias for backwards compatibility\\n+            steps: outputsMap,\\n             pr: prInfo,\\n           },\\n           { injectLog: true, logPrefix: `workflow.output.${output.name}` }\\n@@ -520,10 +642,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         // Liquid template\\n         outputs[output.name] = await this.liquid.parseAndRender(output.value, {\\n           inputs,\\n-          steps: Object.fromEntries(\\n-            Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-          ),\\n-          outputs: flat,\\n+          outputs: outputsMap,\\n+          // Keep 'steps' as alias for backwards compatibility\\n+          steps: outputsMap,\\n           pr: prInfo,\\n         });\\n       }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/client.ts\",\"additions\":65,\"deletions\":1,\"changes\":66,\"patch\":\"diff --git a/src/slack/client.ts b/src/slack/client.ts\\nindex 292cc358..435f8b0d 100644\\n--- a/src/slack/client.ts\\n+++ b/src/slack/client.ts\\n@@ -142,6 +142,70 @@ export class SlackClient {\\n     }\\n   }\\n \\n+  public readonly files = {\\n+    /**\\n+     * Upload a file to Slack using files.uploadV2 API\\n+     * @param options Upload options including file content, filename, channel, and thread_ts\\n+     */\\n+    uploadV2: async ({\\n+      content,\\n+      filename,\\n+      channel,\\n+      thread_ts,\\n+      title,\\n+      initial_comment,\\n+    }: {\\n+      content: Buffer;\\n+      filename: string;\\n+      channel: string;\\n+      thread_ts?: string;\\n+      title?: string;\\n+      initial_comment?: string;\\n+    }): Promise<{ ok: boolean; file?: { id: string; permalink?: string } }> => {\\n+      try {\\n+        // Step 1: Get upload URL\\n+        const getUrlResp: any = await this.api('files.getUploadURLExternal', {\\n+          filename,\\n+          length: content.length,\\n+        });\\n+        if (!getUrlResp || getUrlResp.ok !== true || !getUrlResp.upload_url) {\\n+          console.warn(`Slack files.getUploadURLExternal failed: ${getUrlResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 2: Upload file content to the URL\\n+        const uploadResp = await fetch(getUrlResp.upload_url, {\\n+          method: 'POST',\\n+          body: content,\\n+        });\\n+        if (!uploadResp.ok) {\\n+          console.warn(`Slack file upload to URL failed: ${uploadResp.status}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 3: Complete the upload and share to channel\\n+        const completeResp: any = await this.api('files.completeUploadExternal', {\\n+          files: [{ id: getUrlResp.file_id, title: title || filename }],\\n+          channel_id: channel,\\n+          thread_ts,\\n+          initial_comment,\\n+        });\\n+        if (!completeResp || completeResp.ok !== true) {\\n+          console.warn(`Slack files.completeUploadExternal failed: ${completeResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        return {\\n+          ok: true,\\n+          file: completeResp.files?.[0] || { id: getUrlResp.file_id },\\n+        };\\n+      } catch (e) {\\n+        console.warn(`Slack file upload failed: ${e instanceof Error ? e.message : String(e)}`);\\n+        return { ok: false };\\n+      }\\n+    },\\n+  };\\n+\\n   getWebClient(): any {\\n     return {\\n       conversations: {\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/markdown.ts\",\"additions\":204,\"deletions\":5,\"changes\":209,\"patch\":\"diff --git a/src/slack/markdown.ts b/src/slack/markdown.ts\\nindex cc63fb5f..9f91c25f 100644\\n--- a/src/slack/markdown.ts\\n+++ b/src/slack/markdown.ts\\n@@ -3,14 +3,191 @@\\n // without pulling in a full Markdown parser.\\n //\\n // Supported conversions:\\n+// - # Header / ## Header  ‚Üí *Header* (bold with visual separation)\\n // - **bold** / __bold__   ‚Üí *bold*\\n // - [label](url)          ‚Üí <url|label>\\n // - ![alt](url)           ‚Üí <url|alt>\\n // - *italic* (inline)     ‚Üí _italic_\\n+// - ```mermaid blocks     ‚Üí rendered to PNG and uploaded to Slack\\n //\\n // Everything else is passed through unchanged; Slack will still render many\\n // Markdown-like constructs (lists, code fences, etc.) natively.\\n \\n+import { spawn } from 'child_process';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n+import * as os from 'os';\\n+\\n+/**\\n+ * Represents an extracted mermaid diagram\\n+ */\\n+export interface MermaidDiagram {\\n+  /** The full match including ```mermaid and ``` */\\n+  fullMatch: string;\\n+  /** The mermaid code content */\\n+  code: string;\\n+  /** Start index in the original text */\\n+  startIndex: number;\\n+  /** End index in the original text */\\n+  endIndex: number;\\n+}\\n+\\n+/**\\n+ * Extract all mermaid code blocks from text\\n+ */\\n+export function extractMermaidDiagrams(text: string): MermaidDiagram[] {\\n+  const diagrams: MermaidDiagram[] = [];\\n+  // Match ```mermaid followed by newline, content, and closing ```\\n+  const regex = /```mermaid\\\\s*\\\\n([\\\\s\\\\S]*?)```/g;\\n+  let match;\\n+  while ((match = regex.exec(text)) !== null) {\\n+    diagrams.push({\\n+      fullMatch: match[0],\\n+      code: match[1].trim(),\\n+      startIndex: match.index,\\n+      endIndex: match.index + match[0].length,\\n+    });\\n+  }\\n+  return diagrams;\\n+}\\n+\\n+/**\\n+ * Render a mermaid diagram to PNG using mmdc CLI (@mermaid-js/mermaid-cli).\\n+ *\\n+ * Requirements:\\n+ * - Node.js and npx must be available in PATH\\n+ * - Network access on first run (npx downloads the package)\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ *\\n+ * On Linux, you may need to install chromium dependencies:\\n+ *   apt-get install -y chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ *\\n+ * On Docker/CI, consider using a base image with puppeteer support or\\n+ * pre-installing @mermaid-js/mermaid-cli globally.\\n+ *\\n+ * @param mermaidCode The mermaid diagram code\\n+ * @returns Buffer containing PNG data, or null if rendering failed\\n+ */\\n+export async function renderMermaidToPng(mermaidCode: string): Promise<Buffer | null> {\\n+  // Create temp files for input and output\\n+  const tmpDir = os.tmpdir();\\n+  const inputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.mmd`);\\n+  const outputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.png`);\\n+\\n+  try {\\n+    // Write mermaid code to temp file\\n+    fs.writeFileSync(inputFile, mermaidCode, 'utf-8');\\n+\\n+    // Detect system chromium for puppeteer (mermaid-cli dependency)\\n+    // Without this, puppeteer may hang trying to download its own chromium\\n+    const chromiumPaths = [\\n+      '/usr/bin/chromium',\\n+      '/usr/bin/chromium-browser',\\n+      '/usr/bin/google-chrome',\\n+      '/usr/bin/chrome',\\n+    ];\\n+    let chromiumPath: string | undefined;\\n+    for (const p of chromiumPaths) {\\n+      if (fs.existsSync(p)) {\\n+        chromiumPath = p;\\n+        break;\\n+      }\\n+    }\\n+\\n+    // Build environment with chromium path if found\\n+    const env = { ...process.env };\\n+    if (chromiumPath) {\\n+      env.PUPPETEER_EXECUTABLE_PATH = chromiumPath;\\n+    }\\n+\\n+    // Run mmdc to render PNG\\n+    const result = await new Promise<{ success: boolean; error?: string }>((resolve) => {\\n+      const proc = spawn('npx', [\\n+        '--yes',\\n+        '@mermaid-js/mermaid-cli',\\n+        '-i', inputFile,\\n+        '-o', outputFile,\\n+        '-e', 'png',\\n+        '-b', 'white',\\n+        '-w', '1200',\\n+      ], {\\n+        timeout: 60000, // 60 second timeout (first run may download packages)\\n+        stdio: ['pipe', 'pipe', 'pipe'],\\n+        env,\\n+      });\\n+\\n+      let stderr = '';\\n+      proc.stderr?.on('data', (data) => {\\n+        stderr += data.toString();\\n+      });\\n+\\n+      proc.on('close', (code) => {\\n+        if (code === 0) {\\n+          resolve({ success: true });\\n+        } else {\\n+          resolve({ success: false, error: stderr || `Exit code ${code}` });\\n+        }\\n+      });\\n+\\n+      proc.on('error', (err) => {\\n+        resolve({ success: false, error: err.message });\\n+      });\\n+    });\\n+\\n+    if (!result.success) {\\n+      console.warn(`Mermaid rendering failed: ${result.error}`);\\n+      return null;\\n+    }\\n+\\n+    // Read the output PNG\\n+    if (!fs.existsSync(outputFile)) {\\n+      console.warn('Mermaid output file not created');\\n+      return null;\\n+    }\\n+\\n+    const pngBuffer = fs.readFileSync(outputFile);\\n+    return pngBuffer;\\n+  } catch (e) {\\n+    console.warn(`Mermaid rendering error: ${e instanceof Error ? e.message : String(e)}`);\\n+    return null;\\n+  } finally {\\n+    // Cleanup temp files\\n+    try {\\n+      if (fs.existsSync(inputFile)) fs.unlinkSync(inputFile);\\n+      if (fs.existsSync(outputFile)) fs.unlinkSync(outputFile);\\n+    } catch {\\n+      // Ignore cleanup errors\\n+    }\\n+  }\\n+}\\n+\\n+/**\\n+ * Replace mermaid blocks in text with a placeholder message\\n+ * @param text Original text\\n+ * @param diagrams Extracted diagrams\\n+ * @param replacement Text to replace each diagram with (or a function that returns replacement for each index)\\n+ */\\n+export function replaceMermaidBlocks(\\n+  text: string,\\n+  diagrams: MermaidDiagram[],\\n+  replacement: string | ((index: number) => string) = '_(See diagram above)_'\\n+): string {\\n+  if (diagrams.length === 0) return text;\\n+\\n+  // Sort by start index descending to replace from end to start (preserves indices)\\n+  const sorted = [...diagrams].sort((a, b) => b.startIndex - a.startIndex);\\n+\\n+  let result = text;\\n+  sorted.forEach((diagram, sortedIndex) => {\\n+    // Calculate original index (since we sorted in reverse)\\n+    const originalIndex = diagrams.length - 1 - sortedIndex;\\n+    const rep = typeof replacement === 'function' ? replacement(originalIndex) : replacement;\\n+    result = result.slice(0, diagram.startIndex) + rep + result.slice(diagram.endIndex);\\n+  });\\n+\\n+  return result;\\n+}\\n+\\n export function markdownToSlack(text: string): string {\\n   if (!text || typeof text !== 'string') return '';\\n \\n@@ -33,7 +210,7 @@ export function markdownToSlack(text: string): string {\\n   out = out.replace(/\\\\*\\\\*([^*]+)\\\\*\\\\*/g, (_m, inner: string) => `*${inner}*`);\\n   out = out.replace(/__([^_]+)__/g, (_m, inner: string) => `*${inner}*`);\\n \\n-  // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation).\\n+  // Process lines for headers and bullet lists.\\n   // Slack's mrkdwn handles \\\"‚Ä¢\\\" bullets more naturally than raw \\\"-\\\" Markdown.\\n   const lines = out.split(/\\\\r?\\\\n/);\\n   let inCodeBlock = false;\\n@@ -46,9 +223,31 @@ export function markdownToSlack(text: string): string {\\n       continue;\\n     }\\n     if (inCodeBlock) continue;\\n-    const match = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n-    if (match) {\\n-      const [, indent, , rest] = match;\\n+\\n+    // Headers: # Header ‚Üí *Header* (Slack doesn't have native headers)\\n+    // Match 1-6 # at start of line, followed by space and text\\n+    const headerMatch = /^(#{1,6})\\\\s+(.+)$/.exec(trimmed);\\n+    if (headerMatch) {\\n+      const [, hashes, headerText] = headerMatch;\\n+      // For h1/h2, add extra emphasis with newline before (if not first line\\n+      // and previous line is not empty/header/code-fence)\\n+      const prevLine = i > 0 ? lines[i - 1].trim() : '';\\n+      const prevIsHeaderOrFence =\\n+        /^#{1,6}\\\\s+/.test(prevLine) ||\\n+        /^\\\\*[^*]+\\\\*$/.test(prevLine) ||\\n+        /^```/.test(prevLine);\\n+      if (hashes.length <= 2 && i > 0 && prevLine !== '' && !prevIsHeaderOrFence) {\\n+        lines[i] = `\\\\n*${headerText.trim()}*`;\\n+      } else {\\n+        lines[i] = `*${headerText.trim()}*`;\\n+      }\\n+      continue;\\n+    }\\n+\\n+    // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation)\\n+    const bulletMatch = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n+    if (bulletMatch) {\\n+      const [, indent, , rest] = bulletMatch;\\n       lines[i] = `${indent}‚Ä¢ ${rest}`;\\n     }\\n   }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/prompt-state.ts\",\"additions\":19,\"deletions\":2,\"changes\":21,\"patch\":\"diff --git a/src/slack/prompt-state.ts b/src/slack/prompt-state.ts\\nindex a13233b2..544777c6 100644\\n--- a/src/slack/prompt-state.ts\\n+++ b/src/slack/prompt-state.ts\\n@@ -95,7 +95,10 @@ export class PromptStateManager {\\n   setFirstMessage(channel: string, threadTs: string, text: string): void {\\n     const key = this.key(channel, threadTs);\\n     if (!text || !text.trim()) return;\\n-    if (!this.firstMessage.has(key)) {\\n+    const existing = this.firstMessage.get(key);\\n+    // Only set if: no entry exists OR the existing entry was already consumed\\n+    // This allows new messages to be captured after a resume cycle\\n+    if (!existing || existing.consumed) {\\n       this.firstMessage.set(key, { text, consumed: false });\\n     }\\n   }\\n@@ -130,6 +133,20 @@ export class PromptStateManager {\\n         removed++;\\n       }\\n     }\\n+    // Also clean up stale firstMessage entries (consumed entries older than TTL)\\n+    // Keep unconsumed entries to avoid losing user messages\\n+    for (const [key] of this.firstMessage.entries()) {\\n+      const waitingInfo = this.waiting.get(key);\\n+      // If no corresponding waiting entry exists and the firstMessage was consumed,\\n+      // the conversation is likely complete - safe to remove\\n+      if (!waitingInfo) {\\n+        const entry = this.firstMessage.get(key);\\n+        if (entry?.consumed) {\\n+          this.firstMessage.delete(key);\\n+          removed++;\\n+        }\\n+      }\\n+    }\\n     if (removed) {\\n       try {\\n         logger.info(`[prompt-state] cleanup removed ${removed} entries`);\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/socket-runner.ts\",\"additions\":56,\"deletions\":32,\"changes\":88,\"patch\":\"diff --git a/src/slack/socket-runner.ts b/src/slack/socket-runner.ts\\nindex d1681a03..5b461d22 100644\\n--- a/src/slack/socket-runner.ts\\n+++ b/src/slack/socket-runner.ts\\n@@ -7,6 +7,7 @@ import { SlackAdapter } from './adapter';\\n import { CachePrewarmer } from './cache-prewarmer';\\n import { RateLimiter, type RateLimitConfig } from './rate-limiter';\\n import type { SlackBotConfig } from '../types/bot';\\n+import { withActiveSpan } from '../telemetry/trace-helpers';\\n \\n type SlackSocketConfig = {\\n   appToken?: string; // xapp- token\\n@@ -30,6 +31,7 @@ export class SlackSocketRunner {\\n   private botUserId?: string;\\n   private processedKeys: Map<string, number> = new Map();\\n   private adapter?: SlackAdapter;\\n+  private retryCount = 0;\\n \\n   constructor(engine: StateMachineExecutionEngine, cfg: VisorConfig, opts: SlackSocketConfig) {\\n     const app = opts.appToken || process.env.SLACK_APP_TOKEN || '';\\n@@ -107,7 +109,10 @@ export class SlackSocketRunner {\\n \\n   private async connect(url: string): Promise<void> {\\n     this.ws = new WebSocket(url);\\n-    this.ws.on('open', () => logger.info('[SlackSocket] WebSocket connected'));\\n+    this.ws.on('open', () => {\\n+      this.retryCount = 0; // Reset on successful connection\\n+      logger.info('[SlackSocket] WebSocket connected');\\n+    });\\n     this.ws.on('close', (code, reason) => {\\n       logger.warn(`[SlackSocket] WebSocket closed: ${code} ${reason}`);\\n       setTimeout(() => this.restart().catch(() => {}), 1000);\\n@@ -123,7 +128,13 @@ export class SlackSocketRunner {\\n       const url = await this.openConnection();\\n       await this.connect(url);\\n     } catch (e) {\\n-      logger.error(`[SlackSocket] Restart failed: ${e instanceof Error ? e.message : e}`);\\n+      this.retryCount++;\\n+      // Exponential backoff: 2s, 4s, 8s, 16s, 32s, capped at 60s\\n+      const delay = Math.min(2000 * Math.pow(2, this.retryCount - 1), 60000);\\n+      logger.error(\\n+        `[SlackSocket] Restart failed (attempt ${this.retryCount}), retrying in ${Math.round(delay / 1000)}s: ${e instanceof Error ? e.message : e}`\\n+      );\\n+      setTimeout(() => this.restart().catch(() => {}), delay);\\n     }\\n   }\\n \\n@@ -296,36 +307,49 @@ export class SlackSocketRunner {\\n           });\\n         }\\n       } catch {}\\n-      if (path) {\\n-        try {\\n-          const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n-          const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n-          logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n-          await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n-            webhookContext: { webhookData: map, eventType: 'manual' },\\n-            debug: process.env.VISOR_DEBUG === 'true',\\n-          });\\n-          if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n-          return; // resume path handled\\n-        } catch (e) {\\n-          logger.warn(\\n-            `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n-              e instanceof Error ? e.message : String(e)\\n-            }`\\n-          );\\n-        }\\n-      }\\n+      try {\\n+        await withActiveSpan(\\n+          'visor.run',\\n+          {\\n+            'visor.run.source': 'slack',\\n+            'slack.event.type': String(type || ''),\\n+            'slack.channel': channelId,\\n+            'slack.thread_ts': threadTs,\\n+          },\\n+          async () => {\\n+            if (path) {\\n+              try {\\n+                const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n+                const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n+                logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n+                await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n+                  webhookContext: { webhookData: map, eventType: 'manual' },\\n+                  debug: process.env.VISOR_DEBUG === 'true',\\n+                });\\n+                return;\\n+              } catch (e) {\\n+                logger.warn(\\n+                  `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n+                    e instanceof Error ? e.message : String(e)\\n+                  }`\\n+                );\\n+              }\\n+            }\\n \\n-      // Cold run (no snapshot)\\n-      await runEngine.executeChecks({\\n-        checks: allChecks,\\n-        showDetails: true,\\n-        outputFormat: 'json',\\n-        config: cfgForRun,\\n-        webhookContext: { webhookData: map, eventType: 'manual' },\\n-        debug: process.env.VISOR_DEBUG === 'true',\\n-      } as any);\\n-      if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+            // Cold run (no snapshot)\\n+            await runEngine.executeChecks({\\n+              checks: allChecks,\\n+              showDetails: true,\\n+              outputFormat: 'json',\\n+              config: cfgForRun,\\n+              webhookContext: { webhookData: map, eventType: 'manual' },\\n+              debug: process.env.VISOR_DEBUG === 'true',\\n+            } as any);\\n+          }\\n+        );\\n+      } finally {\\n+        if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+      }\\n     } catch (e) {\\n       logger.error(\\n         `[SlackSocket] Engine execution failed: ${e instanceof Error ? e.message : String(e)}`\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/context/build-engine-context.ts\",\"additions\":11,\"deletions\":4,\"changes\":15,\"patch\":\"diff --git a/src/state-machine/context/build-engine-context.ts b/src/state-machine/context/build-engine-context.ts\\nindex 149c73bf..45e63b6a 100644\\n--- a/src/state-machine/context/build-engine-context.ts\\n+++ b/src/state-machine/context/build-engine-context.ts\\n@@ -57,7 +57,8 @@ export function buildEngineContextForRun(\\n       ) as EventTrigger[],\\n       group: checkConfig.group,\\n       providerType: checkConfig.type || 'ai',\\n-      dependencies: checkConfig.depends_on || [],\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: Array.isArray(checkConfig.depends_on) ? checkConfig.depends_on : checkConfig.depends_on ? [checkConfig.depends_on] : [],\\n     };\\n   }\\n \\n@@ -134,11 +135,14 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n   const originalPath = context.workingDirectory || process.cwd();\\n \\n   try {\\n+    // Check if workspace should be kept (for debugging)\\n+    const keepWorkspace = process.env.VISOR_KEEP_WORKSPACE === 'true';\\n+\\n     // Create workspace manager\\n     const workspace = WorkspaceManager.getInstance(context.sessionId, originalPath, {\\n       enabled: true,\\n-      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH,\\n-      cleanupOnExit: workspaceConfig?.cleanup_on_exit !== false,\\n+      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH || '/tmp/visor-workspaces',\\n+      cleanupOnExit: keepWorkspace ? false : workspaceConfig?.cleanup_on_exit !== false,\\n     });\\n \\n     // Initialize workspace (creates main project worktree)\\n@@ -151,6 +155,9 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n \\n     logger.info(`[Workspace] Initialized workspace: ${info.workspacePath}`);\\n     logger.debug(`[Workspace] Main project at: ${info.mainProjectPath}`);\\n+    if (keepWorkspace) {\\n+      logger.info(`[Workspace] Keeping workspace after execution (--keep-workspace)`);\\n+    }\\n \\n     return context;\\n   } catch (error) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/execution-invoker.ts\",\"additions\":84,\"deletions\":4,\"changes\":88,\"patch\":\"diff --git a/src/state-machine/dispatch/execution-invoker.ts b/src/state-machine/dispatch/execution-invoker.ts\\nindex eb7587d3..798ca8e4 100644\\n--- a/src/state-machine/dispatch/execution-invoker.ts\\n+++ b/src/state-machine/dispatch/execution-invoker.ts\\n@@ -356,10 +356,17 @@ export async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -413,6 +420,31 @@ export async function executeSingleCheck(\\n     }\\n   }\\n \\n+  // Banner-style log when a check actually starts executing. This is emitted\\n+  // after all gating (if/depends_on) has passed so it only appears for real\\n+  // runs, and gives a clear visual separator in the logs between checks.\\n+  try {\\n+    const wave = state.wave;\\n+    const level = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${level}) ‚îÅ‚îÅ‚îÅ`;\\n+\\n+    // When running in a TTY, colour the entire banner line for extra\\n+    // visibility; keep plain text for JSON/SARIF or non-TTY environments.\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   let forEachParent: string | undefined;\\n   let forEachItems: unknown[] | undefined;\\n   for (const depId of depList) {\\n@@ -563,13 +595,56 @@ export async function executeSingleCheck(\\n       files: [],\\n       commits: [],\\n     };\\n+    // Derive AI session reuse context for this check (self-mode only for now).\\n+    // When reuse_ai_session: 'self' is configured, look up the last root-scope\\n+    // journal entry for this check in the current engine session and expose its\\n+    // sessionId via ExecutionContext so ai-check-provider can call\\n+    // executeReviewWithSessionReuse() against the same ProbeAgent session.\\n+    let parentSessionId: string | undefined;\\n+    let reuseSession = false;\\n+    try {\\n+      const reuseCfg: unknown = (checkConfig as any).reuse_ai_session;\\n+      if (reuseCfg === 'self') {\\n+        const snapshotId = context.journal.beginSnapshot();\\n+        const visible = context.journal.readVisible(\\n+          context.sessionId,\\n+          snapshotId,\\n+          context.event as any\\n+        );\\n+        // Prefer the most recent root-scope result for this check\\n+        const prior = visible.filter(\\n+          e => e.checkId === checkId && (!e.scope || e.scope.length === 0)\\n+        );\\n+        if (prior.length > 0) {\\n+          const last = prior[prior.length - 1];\\n+          const sess = (last.result as any)?.sessionId;\\n+          if (typeof sess === 'string' && sess.length > 0) {\\n+            parentSessionId = sess;\\n+            reuseSession = true;\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only ‚Äì fall back to normal (non-reuse) execution on error.\\n+      parentSessionId = undefined;\\n+      reuseSession = false;\\n+    }\\n+\\n     const executionContext = {\\n       ...context.executionContext,\\n       _engineMode: context.mode,\\n       _parentContext: context,\\n       _parentState: state,\\n+      // Explicitly propagate workspace reference for nested workflows\\n+      workspace: context.workspace,\\n     };\\n \\n+    // Attach session reuse hints for providers that support them (AI, Claude Code, etc).\\n+    if (reuseSession && parentSessionId) {\\n+      (executionContext as any).parentSessionId = parentSessionId;\\n+      (executionContext as any).reuseSession = true;\\n+    }\\n+\\n     // Handle on_init lifecycle hook BEFORE main execution\\n     if (checkConfig.on_init) {\\n       try {\\n@@ -613,7 +688,12 @@ export async function executeSingleCheck(\\n \\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/foreach-processor.ts\",\"additions\":27,\"deletions\":2,\"changes\":29,\"patch\":\"diff --git a/src/state-machine/dispatch/foreach-processor.ts b/src/state-machine/dispatch/foreach-processor.ts\\nindex 336bf9df..ab630986 100644\\n--- a/src/state-machine/dispatch/foreach-processor.ts\\n+++ b/src/state-machine/dispatch/foreach-processor.ts\\n@@ -77,6 +77,26 @@ export async function executeCheckWithForEachItems(\\n   const allContents: string[] = [];\\n   const perIterationDurations: number[] = [];\\n \\n+  // Emit a banner for the forEach parent so logs clearly show when we are\\n+  // entering the aggregated execution for that step.\\n+  try {\\n+    const wave = state.wave;\\n+    const lvl = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${lvl}, forEach parent) ‚îÅ‚îÅ‚îÅ`;\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   for (let itemIndex = 0; itemIndex < forEachItems.length; itemIndex++) {\\n     const iterationStartMs = Date.now();\\n     const scope: Array<{ check: string; index: number }> = [\\n@@ -248,7 +268,12 @@ export async function executeCheckWithForEachItems(\\n \\n       const result = await withActiveSpan(\\n         `visor.check.${checkId}`,\\n-        { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+        {\\n+          'visor.check.id': checkId,\\n+          'visor.check.type': providerType,\\n+          session_id: context.sessionId,\\n+          wave: state.wave,\\n+        },\\n         async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n       );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/stats-manager.ts\",\"additions\":21,\"deletions\":1,\"changes\":22,\"patch\":\"diff --git a/src/state-machine/dispatch/stats-manager.ts b/src/state-machine/dispatch/stats-manager.ts\\nindex 8d04436e..2f317b59 100644\\n--- a/src/state-machine/dispatch/stats-manager.ts\\n+++ b/src/state-machine/dispatch/stats-manager.ts\\n@@ -44,6 +44,26 @@ export function updateStats(\\n       issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n     };\\n \\n+    // Respect explicit skip markers on the result (set by LevelDispatch).\\n+    const skippedMarker = (result as any).__skipped;\\n+    if (skippedMarker) {\\n+      stats.skipped = true;\\n+      stats.skipReason =\\n+        typeof skippedMarker === 'string'\\n+          ? (skippedMarker as any)\\n+          : (stats.skipReason as any) || 'if_condition';\\n+      // A skipped result should not contribute to run counters; reset\\n+      // them explicitly to guard against any prior values lingering\\n+      // from earlier runs of the same check.\\n+      stats.totalRuns = 0;\\n+      stats.successfulRuns = 0;\\n+      stats.failedRuns = 0;\\n+      stats.skippedRuns++;\\n+      // Do not count this as a run; keep totalRuns/failed/success unchanged.\\n+      state.stats.set(checkId, stats);\\n+      continue;\\n+    }\\n+\\n     // If this check was previously marked as skipped and now executed,\\n     // clear the skipped flag and any skip counters to ensure the run is visible.\\n     if (stats.skipped) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/runner.ts\",\"additions\":10,\"deletions\":7,\"changes\":17,\"patch\":\"diff --git a/src/state-machine/runner.ts b/src/state-machine/runner.ts\\nindex ee86b876..a13a5efd 100644\\n--- a/src/state-machine/runner.ts\\n+++ b/src/state-machine/runner.ts\\n@@ -124,14 +124,17 @@ export class StateMachineRunner {\\n    */\\n   private async executeState(state: EngineState): Promise<void> {\\n     // M4: Wrap state execution in OTEL span\\n+    const attrs: Record<string, unknown> = {\\n+      state: state,\\n+      engine_mode: this.context.mode,\\n+      wave: this.state.wave,\\n+      session_id: this.context.sessionId,\\n+    };\\n+    const waveKind = (this.state as any)?.flags?.waveKind;\\n+    if (waveKind) attrs.wave_kind = waveKind;\\n     return withActiveSpan(\\n       `engine.state.${state.toLowerCase()}`,\\n-      {\\n-        state: state,\\n-        engine_mode: this.context.mode,\\n-        wave: this.state.wave,\\n-        session_id: this.context.sessionId,\\n-      },\\n+      attrs,\\n       async () => {\\n         try {\\n           switch (state) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/level-dispatch.ts\",\"additions\":152,\"deletions\":15,\"changes\":167,\"patch\":\"diff --git a/src/state-machine/states/level-dispatch.ts b/src/state-machine/states/level-dispatch.ts\\nindex 5588b0f5..95392197 100644\\n--- a/src/state-machine/states/level-dispatch.ts\\n+++ b/src/state-machine/states/level-dispatch.ts\\n@@ -26,7 +26,7 @@ import type { CheckExecutionStats } from '../../types/execution';\\n import type { CheckProviderConfig } from '../../providers/check-provider.interface';\\n import type { CheckConfig } from '../../types/config';\\n import { handleRouting, checkLoopBudget } from './routing';\\n-import { withActiveSpan } from '../../telemetry/trace-helpers';\\n+import { withActiveSpan, setSpanAttributes, addEvent } from '../../telemetry/trace-helpers';\\n import { emitMermaidFromMarkdown } from '../../utils/mermaid-telemetry';\\n import { emitNdjsonSpanWithEvents, emitNdjsonFallback } from '../../telemetry/fallback-ndjson';\\n import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n@@ -46,6 +46,32 @@ function mapCheckNameToFocus(checkName: string): string {\\n   return focusMap[checkName] || 'all';\\n }\\n \\n+function formatScopeLabel(scope: Array<{ check: string; index: number }> | undefined): string {\\n+  if (!scope || scope.length === 0) return '';\\n+  return scope.map(item => `${item.check}:${item.index}`).join('|');\\n+}\\n+\\n+function recordOnFinishRoutingEvent(args: {\\n+  checkId: string;\\n+  action: 'run' | 'goto';\\n+  target: string;\\n+  source: 'run' | 'goto' | 'goto_js' | 'transitions';\\n+  scope?: Array<{ check: string; index: number }>;\\n+  gotoEvent?: string;\\n+}): void {\\n+  const attrs: Record<string, unknown> = {\\n+    check_id: args.checkId,\\n+    trigger: 'on_finish',\\n+    action: args.action,\\n+    target: args.target,\\n+    source: args.source,\\n+  };\\n+  const scopeLabel = formatScopeLabel(args.scope);\\n+  if (scopeLabel) attrs.scope = scopeLabel;\\n+  if (args.gotoEvent) attrs.goto_event = args.gotoEvent;\\n+  addEvent('visor.routing', attrs);\\n+}\\n+\\n /**\\n  * Build output history Map from journal for template rendering\\n  * This matches the format expected by AI providers\\n@@ -121,12 +147,13 @@ async function evaluateIfCondition(\\n         return false;\\n       }\\n     })();\\n+    // Steps with dependencies should always see outputs from all completed steps.\\n     // In forward-run waves (from on_success/on_fail goto), guards should see the\\n     // latest global outputs even if the check has no explicit dependencies.\\n     // In wave-retry (from on_finish), restrict to checks with dependencies to\\n     // avoid wrongly skipping top-level prompts like 'ask'.\\n     const useGlobalOutputs =\\n-      (useGlobalOutputsFlag && waveKind === 'forward') || (useGlobalOutputsFlag && hasDeps);\\n+      hasDeps || (useGlobalOutputsFlag && waveKind === 'forward');\\n \\n     if (useGlobalOutputs) {\\n       // Forward-run wave: allow guards to consult latest outputs from the entire\\n@@ -199,6 +226,7 @@ async function evaluateIfCondition(\\n       baseBranch: (context.prInfo as any)?.baseBranch,\\n       filesChanged: context.prInfo?.files?.map(f => f.filename),\\n       environment: envSnapshot,\\n+      workflowInputs: (context.config as any).workflow_inputs || {},\\n     };\\n \\n     const shouldRun = await evaluator.evaluateIfCondition(checkId, ifExpression, contextData);\\n@@ -238,6 +266,11 @@ export async function handleLevelDispatch(\\n   // Update current level tracking\\n   state.currentLevel = level.level;\\n   state.currentLevelChecks = new Set(level.parallel);\\n+  const levelChecksPreview = level.parallel.slice(0, 5).join(',');\\n+  setSpanAttributes({\\n+    level_size: level.parallel.length,\\n+    level_checks_preview: levelChecksPreview,\\n+  });\\n \\n   // Emit level ready event\\n   emitEvent({ type: 'LevelReady', level, wave: state.wave });\\n@@ -689,6 +722,39 @@ async function executeCheckWithForEachItems(\\n         }\\n       } catch {}\\n \\n+      // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+      // The socket-runner stores conversation data in webhookData under the endpoint key\\n+      try {\\n+        const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+        const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+        if (context.debug) {\\n+          logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+        }\\n+        if (webhookData && webhookData.size > 0) {\\n+          // Find the payload with slack_conversation\\n+          for (const payload of webhookData.values()) {\\n+            const slackConv = (payload as any)?.slack_conversation;\\n+            if (slackConv) {\\n+              // Build slack context with event and conversation\\n+              const event = (payload as any)?.event;\\n+              const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+              if (context.debug) {\\n+                logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+              }\\n+              (providerConfig as any).eventContext = {\\n+                ...(providerConfig as any).eventContext,\\n+                slack: {\\n+                  event: event || {},\\n+                  conversation: slackConv,\\n+                },\\n+                conversation: slackConv, // Also expose at top level for convenience\\n+              };\\n+              break;\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n+\\n       // Build dependency results with scope\\n       const dependencyResults = buildDependencyResultsWithScope(\\n         checkId,\\n@@ -820,15 +886,17 @@ async function executeCheckWithForEachItems(\\n       } catch {}\\n \\n       // Execute provider with telemetry\\n-      const itemResult = await withActiveSpan(\\n-        `visor.check.${checkId}`,\\n-        {\\n-          'visor.check.id': checkId,\\n-          'visor.check.type': providerType,\\n-          'visor.foreach.index': itemIndex,\\n-        },\\n-        async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n-      );\\n+    const itemResult = await withActiveSpan(\\n+      `visor.check.${checkId}`,\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        'visor.foreach.index': itemIndex,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n+      async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n+    );\\n \\n       // Enrich issues\\n       const enrichedIssues = (itemResult.issues || []).map((issue: ReviewIssue) => ({\\n@@ -942,6 +1010,7 @@ async function executeCheckWithForEachItems(\\n             const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n               previousResults: dependencyResults as any,\\n               event: context.event || 'manual',\\n+              output: output, // Pass the iteration output for guarantee evaluation\\n             } as any);\\n             if (!holds) {\\n               const issue: ReviewIssue = {\\n@@ -1311,6 +1380,13 @@ async function executeCheckWithForEachItems(\\n             // Increment loop count\\n             state.routingLoopCount++;\\n \\n+            recordOnFinishRoutingEvent({\\n+              checkId: forEachParent,\\n+              action: 'run',\\n+              target: targetCheck,\\n+              source: 'run',\\n+              scope: [],\\n+            });\\n             emitEvent({\\n               type: 'ForwardRunRequested',\\n               target: targetCheck,\\n@@ -1358,6 +1434,14 @@ async function executeCheckWithForEachItems(\\n                 return aggregatedResult; // abort further routing\\n               }\\n               state.routingLoopCount++;\\n+              recordOnFinishRoutingEvent({\\n+                checkId: forEachParent,\\n+                action: 'goto',\\n+                target: transTarget.to,\\n+                source: 'transitions',\\n+                scope: [],\\n+                gotoEvent: (transTarget as any).goto_event,\\n+              });\\n               emitEvent({\\n                 type: 'ForwardRunRequested',\\n                 target: transTarget.to,\\n@@ -1464,6 +1548,13 @@ async function executeCheckWithForEachItems(\\n           // Increment loop count\\n           state.routingLoopCount++;\\n \\n+          recordOnFinishRoutingEvent({\\n+            checkId: forEachParent,\\n+            action: 'goto',\\n+            target: gotoTarget,\\n+            source: onFinish.goto_js ? 'goto_js' : 'goto',\\n+            scope: [],\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: gotoTarget,\\n@@ -1618,10 +1709,17 @@ async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -1923,6 +2021,39 @@ async function executeSingleCheck(\\n       }\\n     } catch {}\\n \\n+    // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+    // The socket-runner stores conversation data in webhookData under the endpoint key\\n+    try {\\n+      const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+      const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+      if (context.debug) {\\n+        logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+      }\\n+      if (webhookData && webhookData.size > 0) {\\n+        // Find the payload with slack_conversation\\n+        for (const payload of webhookData.values()) {\\n+          const slackConv = (payload as any)?.slack_conversation;\\n+          if (slackConv) {\\n+            // Build slack context with event and conversation\\n+            const event = (payload as any)?.event;\\n+            const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+            if (context.debug) {\\n+              logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+            }\\n+            (providerConfig as any).eventContext = {\\n+              ...(providerConfig as any).eventContext,\\n+              slack: {\\n+                event: event || {},\\n+                conversation: slackConv,\\n+              },\\n+              conversation: slackConv, // Also expose at top level for convenience\\n+            };\\n+            break;\\n+          }\\n+        }\\n+      }\\n+    } catch {}\\n+\\n     // Build dependency results\\n     const dependencyResults = buildDependencyResults(checkId, checkConfig, context, state);\\n \\n@@ -2016,7 +2147,12 @@ async function executeSingleCheck(\\n     // Execute provider with telemetry\\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n@@ -2098,6 +2234,7 @@ async function executeSingleCheck(\\n           const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n             previousResults: dependencyResults as any,\\n             event: context.event || 'manual',\\n+            output: enrichedResult.output,\\n           } as any);\\n           if (!holds) {\\n             const issue: ReviewIssue = {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/routing.ts\",\"additions\":165,\"deletions\":1,\"changes\":166,\"patch\":\"diff --git a/src/state-machine/states/routing.ts b/src/state-machine/states/routing.ts\\nindex 5752a5bb..b1fd8654 100644\\n--- a/src/state-machine/states/routing.ts\\n+++ b/src/state-machine/states/routing.ts\\n@@ -15,6 +15,7 @@ import type { EngineContext, RunState, EngineState, EngineEvent } from '../../ty\\n import type { ReviewSummary, ReviewIssue } from '../../reviewer';\\n import type { CheckConfig, OnFailConfig, TransitionRule } from '../../types/config';\\n import { logger } from '../../logger';\\n+import { addEvent } from '../../telemetry/trace-helpers';\\n import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n import { createSecureSandbox, compileAndRun } from '../../utils/sandbox';\\n import { MemoryStore } from '../../memory-store';\\n@@ -153,6 +154,37 @@ function createMemoryHelpers() {\\n   };\\n }\\n \\n+type RoutingTrigger = 'on_success' | 'on_fail' | 'on_finish';\\n+type RoutingAction = 'run' | 'goto' | 'retry';\\n+type RoutingSource = 'run' | 'run_js' | 'goto' | 'goto_js' | 'transitions' | 'retry';\\n+\\n+function formatScopeLabel(scope: Array<{ check: string; index: number }> | undefined): string {\\n+  if (!scope || scope.length === 0) return '';\\n+  return scope.map(item => `${item.check}:${item.index}`).join('|');\\n+}\\n+\\n+function recordRoutingEvent(args: {\\n+  checkId: string;\\n+  trigger: RoutingTrigger;\\n+  action: RoutingAction;\\n+  target?: string;\\n+  source?: RoutingSource;\\n+  scope?: Array<{ check: string; index: number }>;\\n+  gotoEvent?: string;\\n+}): void {\\n+  const attrs: Record<string, unknown> = {\\n+    check_id: args.checkId,\\n+    trigger: args.trigger,\\n+    action: args.action,\\n+  };\\n+  if (args.target) attrs.target = args.target;\\n+  if (args.source) attrs.source = args.source;\\n+  const scopeLabel = formatScopeLabel(args.scope);\\n+  if (scopeLabel) attrs.scope = scopeLabel;\\n+  if (args.gotoEvent) attrs.goto_event = args.gotoEvent;\\n+  addEvent('visor.routing', attrs);\\n+}\\n+\\n /**\\n  * Handle routing state - evaluate conditions and decide next actions\\n  */\\n@@ -271,6 +303,14 @@ async function processOnFinish(\\n             { check: checkId, index: itemIndex },\\n           ];\\n \\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_finish',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -284,6 +324,14 @@ async function processOnFinish(\\n         // Increment loop count\\n         state.routingLoopCount++;\\n \\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_finish',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope: [],\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -328,6 +376,14 @@ async function processOnFinish(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_finish',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -362,6 +418,15 @@ async function processOnFinish(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_finish',\\n+        action: 'goto',\\n+        target: finishTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: finishTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: finishTransTarget.to,\\n@@ -406,6 +471,14 @@ async function processOnFinish(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_finish',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onFinish.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+    });\\n     // Enqueue forward run event\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n@@ -640,6 +713,14 @@ async function processOnSuccess(\\n             { check: checkId, index: itemIndex },\\n           ];\\n \\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_success',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -652,6 +733,14 @@ async function processOnSuccess(\\n         // Increment loop count\\n         state.routingLoopCount++;\\n \\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_success',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope,\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -695,6 +784,14 @@ async function processOnSuccess(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_success',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -728,6 +825,15 @@ async function processOnSuccess(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_success',\\n+        action: 'goto',\\n+        target: successTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: successTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: successTransTarget.to,\\n@@ -773,6 +879,15 @@ async function processOnSuccess(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_success',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onSuccess.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+      gotoEvent: onSuccess.goto_event,\\n+    });\\n     // Enqueue forward run event with optional event override\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n@@ -864,6 +979,14 @@ async function processOnFail(\\n           const itemScope: Array<{ check: string; index: number }> = [\\n             { check: checkId, index: itemIndex },\\n           ];\\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_fail',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -874,6 +997,14 @@ async function processOnFail(\\n       } else {\\n         // No forEach context: preserve current scope (if any)\\n         state.routingLoopCount++;\\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_fail',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope,\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -917,6 +1048,14 @@ async function processOnFail(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_fail',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -958,6 +1097,13 @@ async function processOnFail(\\n \\n         // Increment loop count and schedule forward run for the same check\\n         state.routingLoopCount++;\\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_fail',\\n+          action: 'retry',\\n+          source: 'retry',\\n+          scope: sc || [],\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: checkId,\\n@@ -1013,6 +1159,15 @@ async function processOnFail(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_fail',\\n+        action: 'goto',\\n+        target: failTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: failTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: failTransTarget.to,\\n@@ -1058,6 +1213,15 @@ async function processOnFail(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_fail',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onFail.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+      gotoEvent: onFail.goto_event,\\n+    });\\n     // Enqueue forward run event with optional event override\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/wave-planning.ts\",\"additions\":10,\"deletions\":3,\"changes\":13,\"patch\":\"diff --git a/src/state-machine/states/wave-planning.ts b/src/state-machine/states/wave-planning.ts\\nindex 7551d55c..7b42f043 100644\\n--- a/src/state-machine/states/wave-planning.ts\\n+++ b/src/state-machine/states/wave-planning.ts\\n@@ -351,8 +351,11 @@ export async function handleWavePlanning(\\n     for (const id of checksToRun) {\\n       // Only include dependencies that are within the same subset (often none).\\n       const cfg = context.config.checks?.[id];\\n-      const deps = (cfg?.depends_on || []).filter((d: string) => checksToRun.includes(d));\\n-      subDeps[id] = deps as string[];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = cfg?.depends_on;\\n+      const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n+      const deps = depsArray.filter((d: string) => checksToRun.includes(d));\\n+      subDeps[id] = deps;\\n     }\\n \\n     const subGraph = DependencyResolver.buildDependencyGraph(subDeps);\\n@@ -410,6 +413,10 @@ export async function handleWavePlanning(\\n     // Initialize current wave state\\n     (state as any).currentWaveCompletions = new Set<string>();\\n     (state as any).failedChecks = new Set<string>();\\n+    try {\\n+      (state as any).flags = (state as any).flags || {};\\n+      (state as any).flags.waveKind = 'initial';\\n+    } catch {}\\n   }\\n \\n   // Check if there are levels to execute\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/workflow-projection.ts\",\"additions\":3,\"deletions\":2,\"changes\":5,\"patch\":\"diff --git a/src/state-machine/workflow-projection.ts b/src/state-machine/workflow-projection.ts\\nindex b2d5eb24..2bcfd858 100644\\n--- a/src/state-machine/workflow-projection.ts\\n+++ b/src/state-machine/workflow-projection.ts\\n@@ -53,7 +53,8 @@ export function projectWorkflowToGraph(\\n       triggers: step.on || workflow.on || [],\\n       group: step.group,\\n       providerType: step.type || 'ai',\\n-      dependencies: (step.depends_on || []).map(dep => dep),\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: (Array.isArray(step.depends_on) ? step.depends_on : step.depends_on ? [step.depends_on] : []).map((dep: string) => dep),\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/flow-stage.ts\",\"additions\":37,\"deletions\":4,\"changes\":41,\"patch\":\"diff --git a/src/test-runner/core/flow-stage.ts b/src/test-runner/core/flow-stage.ts\\nindex a14256c2..a587f9f1 100644\\n--- a/src/test-runner/core/flow-stage.ts\\n+++ b/src/test-runner/core/flow-stage.ts\\n@@ -352,10 +352,30 @@ export class FlowStage {\\n       } catch {}\\n       // Prefer authoritative counts from res.statistics when available, then fall back to deltas/inferred\\n       const fromResStats: Record<string, number> = {};\\n+      const resSkipped: Record<string, boolean> = {};\\n       try {\\n         for (const s of (res.statistics?.checks || []) as any[]) {\\n           const n = (s && s.checkName) || '';\\n-          if (typeof n === 'string' && n) fromResStats[n] = (s.totalRuns || 0) as number;\\n+          if (typeof n === 'string' && n) {\\n+            fromResStats[n] = (s.totalRuns || 0) as number;\\n+            resSkipped[n] = !!(s as any).skipped || !!(s as any).skipReason;\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      // Steps that have explicit expectations in this stage. We will use\\n+      // this to avoid inflating run counts (via parent-alignment heuristics)\\n+      // for checks that are not even expected in this stage.\\n+      const expectedSteps = new Set<string>();\\n+      const expectedZero = new Set<string>();\\n+      try {\\n+        const expCalls = ((stage.expect || {}).calls || []) as Array<{ step?: string }>;\\n+        for (const c of expCalls) {\\n+          if (c && typeof c.step === 'string' && c.step) expectedSteps.add(c.step);\\n+          try {\\n+            if (c && typeof c.step === 'string' && c.step && (c as any).exactly === 0)\\n+              expectedZero.add(c.step);\\n+          } catch {}\\n         }\\n       } catch {}\\n \\n@@ -410,10 +430,15 @@ export class FlowStage {\\n           const base = stageOnly ? 0 : statBase[name] || 0;\\n           const d = Math.max(0, resTotal - base);\\n           runs = d > 0 ? d : 0;\\n+          // If the engine marked this check as skipped, force runs to 0\\n+          // so parent-alignment heuristics below do not inflate it.\\n+          if (resSkipped[name]) {\\n+            runs = 0;\\n+          }\\n         } else {\\n           runs = deltaMap[name] !== undefined ? deltaMap[name] : inferred;\\n         }\\n-        if (runs === 0 && presentInResults.has(name)) runs = 1;\\n+        if (runs === 0 && presentInResults.has(name) && !resSkipped[name]) runs = 1;\\n         if (!isForEachLike && histRuns > 0) runs = histRuns;\\n         // Only use per-item history counts for non-forEach checks. For forEach parents,\\n         // use aggregated totals from res.statistics to reflect number of parent executions.\\n@@ -442,7 +467,15 @@ export class FlowStage {\\n             parentMax = Math.max(parentMax, dP);\\n           }\\n           // Apply only for non-forEach checks with no observable history in this stage\\n-          if (!isForEachLike && histRuns === 0 && histPerItemRuns === 0 && parentMax > 0) {\\n+          if (\\n+            !isForEachLike &&\\n+            histRuns === 0 &&\\n+            histPerItemRuns === 0 &&\\n+            parentMax > 0 &&\\n+            !resSkipped[name] &&\\n+            (expectedSteps.size === 0 || expectedSteps.has(name)) &&\\n+            !expectedZero.has(name)\\n+          ) {\\n             runs = Math.max(runs, parentMax);\\n           }\\n         } catch {}\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/test-execution-wrapper.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/test-runner/core/test-execution-wrapper.ts b/src/test-runner/core/test-execution-wrapper.ts\\nindex 254d0970..d0333152 100644\\n--- a/src/test-runner/core/test-execution-wrapper.ts\\n+++ b/src/test-runner/core/test-execution-wrapper.ts\\n@@ -31,6 +31,8 @@ export class TestExecutionWrapper {\\n       const prev: any = (this.engine as any).executionContext || {};\\n       const merged = {\\n         ...prev,\\n+        // Inject workflow inputs for template access via {{ inputs.* }}\\n+        workflowInputs: cfg.workflow_inputs || prev.workflowInputs || {},\\n         mode: {\\n           ...(prev.mode || {}),\\n           test: true,\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/evaluators.ts\",\"additions\":67,\"deletions\":2,\"changes\":69,\"patch\":\"diff --git a/src/test-runner/evaluators.ts b/src/test-runner/evaluators.ts\\nindex 4843163a..75e9c174 100644\\n--- a/src/test-runner/evaluators.ts\\n+++ b/src/test-runner/evaluators.ts\\n@@ -252,6 +252,69 @@ export function evaluatePrompts(\\n   }\\n }\\n \\n+/**\\n+ * Evaluate workflow_output assertions against computed workflow outputs.\\n+ * Similar to evaluateOutputs but tests workflow-level outputs (defined in outputs: section)\\n+ * rather than step outputs.\\n+ */\\n+export function evaluateWorkflowOutputs(\\n+  errors: string[],\\n+  expect: ExpectBlock,\\n+  workflowOutputs: Record<string, unknown> | undefined\\n+): void {\\n+  const expectations = (expect as any).workflow_output;\\n+  if (!Array.isArray(expectations) || expectations.length === 0) return;\\n+  if (!workflowOutputs) {\\n+    errors.push('workflow_output assertions present but no workflow outputs computed');\\n+    return;\\n+  }\\n+\\n+  for (const o of expectations) {\\n+    const path = o.path as string;\\n+    if (!path) {\\n+      errors.push('workflow_output assertion missing path');\\n+      continue;\\n+    }\\n+    const v = deepGet(workflowOutputs, path);\\n+    if (o.equals !== undefined && !deepEqual(v, o.equals)) {\\n+      errors.push(\\n+        `Workflow output ${path} expected ${JSON.stringify(o.equals)} but got ${JSON.stringify(v)}`\\n+      );\\n+    }\\n+    if (o.equalsDeep !== undefined && !deepEqual(v, o.equalsDeep)) {\\n+      errors.push(`Workflow output ${path} deepEquals failed`);\\n+    }\\n+    if (o.matches && !parseRegex(o.matches).test(String(v))) {\\n+      errors.push(`Workflow output ${path} does not match ${o.matches}`);\\n+    }\\n+    if (o.contains) {\\n+      const contents = Array.isArray(o.contains) ? o.contains : [o.contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (!strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} expected to contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.not_contains) {\\n+      const contents = Array.isArray(o.not_contains) ? o.not_contains : [o.not_contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} should not contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.contains_unordered) {\\n+      if (!Array.isArray(v)) {\\n+        errors.push(`Workflow output ${path} not an array for contains_unordered`);\\n+      } else if (!containsUnordered(v as unknown[], o.contains_unordered)) {\\n+        errors.push(`Workflow output ${path} missing elements (unordered)`);\\n+      }\\n+    }\\n+  }\\n+}\\n+\\n export function evaluateOutputs(\\n   errors: string[],\\n   expect: ExpectBlock,\\n@@ -332,7 +395,8 @@ export function evaluateCase(\\n   strict: boolean,\\n   promptsByStep: Record<string, string[]>,\\n   _results: GroupedResults,\\n-  outputHistory: Record<string, unknown[]>\\n+  outputHistory: Record<string, unknown[]>,\\n+  workflowOutputs?: Record<string, unknown>\\n ): string[] {\\n   const errors: string[] = [];\\n   const executed = buildExecutedMap(stats);\\n@@ -350,5 +414,6 @@ export function evaluateCase(\\n   evaluateNoCalls(errors, expect, executed, recorder, slackRecorder);\\n   evaluatePrompts(errors, expect, promptsByStep);\\n   evaluateOutputs(errors, expect, outputHistory);\\n+  evaluateWorkflowOutputs(errors, expect, workflowOutputs);\\n   return errors;\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/index.ts\",\"additions\":112,\"deletions\":7,\"changes\":119,\"patch\":\"diff --git a/src/test-runner/index.ts b/src/test-runner/index.ts\\nindex 29916a16..4dcc3331 100644\\n--- a/src/test-runner/index.ts\\n+++ b/src/test-runner/index.ts\\n@@ -159,6 +159,7 @@ export async function runSuites(\\n   options: {\\n     only?: string;\\n     bail?: boolean;\\n+    noMocks?: boolean;\\n     maxParallelSuites?: number;\\n     maxParallel?: number;\\n     promptMaxChars?: number;\\n@@ -225,6 +226,7 @@ export async function runSuites(\\n       const r = await runner.runCases(fp, suite as TestSuite, {\\n         only: options.only,\\n         bail: options.bail,\\n+        noMocks: options.noMocks,\\n         maxParallel: options.maxParallel,\\n         promptMaxChars: options.promptMaxChars,\\n       });\\n@@ -285,7 +287,8 @@ export class VisorTestRunner {\\n     defaultPromptCap?: number,\\n     ghRec?: { error_code?: number; timeout_ms?: number },\\n     defaultIncludeTags?: string[] | undefined,\\n-    defaultExcludeTags?: string[] | undefined\\n+    defaultExcludeTags?: string[] | undefined,\\n+    noMocks?: boolean\\n   ): {\\n     name: string;\\n     strict: boolean;\\n@@ -399,9 +402,11 @@ export class VisorTestRunner {\\n               : info.prompt;\\n           prompts[k].push(p);\\n         },\\n-        mockForStep: (step: string) => mockMgr.get(step),\\n+        // In noMocks mode, always return undefined to let real providers execute\\n+        mockForStep: (step: string) => noMocks ? undefined : mockMgr.get(step),\\n         // Ensure human-input never blocks tests: prefer case mock, then default value\\n         onHumanInput: async (req: { checkId: string; default?: string }) => {\\n+          if (noMocks) return (req.default ?? '').toString();\\n           const m = mockMgr.get(req.checkId);\\n           if (m !== undefined && m !== null) return String(m);\\n           return (req.default ?? '').toString();\\n@@ -481,6 +486,60 @@ export class VisorTestRunner {\\n     if (!checks || checks.length === 0) return;\\n     console.log(`  checks: ${checks.join(', ')}`);\\n   }\\n+  /**\\n+   * Compute workflow outputs from output definitions and step results.\\n+   * Evaluates value_js and value (Liquid) expressions to produce output values.\\n+   */\\n+  private computeWorkflowOutputs(\\n+    outputDefs: Array<{ name: string; value?: string; value_js?: string }>,\\n+    outputHistory: Record<string, unknown[]>,\\n+    results: any[]\\n+  ): Record<string, unknown> {\\n+    const computed: Record<string, unknown> = {};\\n+    // Build a steps map from outputHistory (last output for each step)\\n+    const steps: Record<string, unknown> = {};\\n+    for (const [stepId, hist] of Object.entries(outputHistory)) {\\n+      if (Array.isArray(hist) && hist.length > 0) {\\n+        steps[stepId] = hist[hist.length - 1];\\n+      }\\n+    }\\n+    // Build outputs map from history (for {{ outputs[\\\"step\\\"] }} syntax)\\n+    const outputs: Record<string, unknown> = { ...steps };\\n+\\n+    for (const outputDef of outputDefs) {\\n+      if (!outputDef.name) continue;\\n+      try {\\n+        if (outputDef.value_js) {\\n+          // Evaluate JavaScript expression\\n+          const fn = new Function('steps', 'outputs', 'results', `\\n+            try {\\n+              ${outputDef.value_js}\\n+            } catch (e) {\\n+              return undefined;\\n+            }\\n+          `);\\n+          computed[outputDef.name] = fn(steps, outputs, results);\\n+        } else if (outputDef.value) {\\n+          // Evaluate Liquid template\\n+          const { Liquid } = require('liquidjs');\\n+          const engine = new Liquid();\\n+          const rendered = engine.parseAndRenderSync(outputDef.value, { steps, outputs, results });\\n+          // Try to parse as JSON, otherwise keep as string\\n+          try {\\n+            computed[outputDef.name] = JSON.parse(rendered);\\n+          } catch {\\n+            computed[outputDef.name] = rendered.trim();\\n+          }\\n+        }\\n+      } catch (e) {\\n+        // Skip outputs that fail to compute\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.log(`  ‚ö†Ô∏è Failed to compute output '${outputDef.name}': ${e}`);\\n+        }\\n+      }\\n+    }\\n+    return computed;\\n+  }\\n   /**\\n    * Locate a tests file: explicit path > ./.visor.tests.yaml > defaults/visor.tests.yaml\\n    */\\n@@ -606,7 +665,7 @@ export class VisorTestRunner {\\n   public async runCases(\\n     testsPath: string,\\n     suite: TestSuite,\\n-    options: { only?: string; bail?: boolean; maxParallel?: number; promptMaxChars?: number }\\n+    options: { only?: string; bail?: boolean; noMocks?: boolean; maxParallel?: number; promptMaxChars?: number }\\n   ): Promise<{\\n     failures: number;\\n     results: Array<{\\n@@ -748,9 +807,14 @@ export class VisorTestRunner {\\n     const __keepAlive = setInterval(() => {}, 1000);\\n     // Header: show suite path for clarity\\n     let __suiteRel = testsPath;\\n+    const noMocksMode = options.noMocks || false;\\n     try {\\n       __suiteRel = path.relative(this.cwd, testsPath) || testsPath;\\n       console.log(`Suite: ${__suiteRel}`);\\n+      if (noMocksMode) {\\n+        console.log(this.color('üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)', '33'));\\n+        console.log(this.gray('   Step outputs will be captured and printed as suggested mocks\\\\n'));\\n+      }\\n     } catch {}\\n     const runOne = async (_case: any): Promise<{ name: string; failed: number }> => {\\n       // Case header for clarity\\n@@ -805,6 +869,11 @@ export class VisorTestRunner {\\n           cfgLocal.checks[name] = chk;\\n         }\\n       }\\n+      // Workflow testing: inject workflow_input into config for template access\\n+      const workflowInput = (_case as any).workflow_input;\\n+      if (workflowInput && typeof workflowInput === 'object') {\\n+        (cfgLocal as any).workflow_inputs = workflowInput;\\n+      }\\n       const setup = this.setupTestCase(\\n         _case,\\n         cfgLocal,\\n@@ -812,7 +881,8 @@ export class VisorTestRunner {\\n         defaultPromptCap,\\n         ghRec,\\n         defaultIncludeTags,\\n-        defaultExcludeTags\\n+        defaultExcludeTags,\\n+        noMocksMode\\n       );\\n       try {\\n         this.printSelectedChecks(setup.checksToRun);\\n@@ -826,6 +896,24 @@ export class VisorTestRunner {\\n         } catch {}\\n         const exec = await this.executeTestCase(setup, cfgLocal);\\n         const res = exec.res;\\n+\\n+        // In no-mocks mode, print captured outputs as suggested mocks\\n+        if (noMocksMode && Object.keys(exec.outHistory).length > 0) {\\n+          console.log(this.color('\\\\nüìã Suggested mocks (copy to your test case):', '36'));\\n+          console.log(this.gray('mocks:'));\\n+          for (const [stepName, outputs] of Object.entries(exec.outHistory)) {\\n+            if (!Array.isArray(outputs) || outputs.length === 0) continue;\\n+            // Use the last output for the step\\n+            const lastOutput = outputs[outputs.length - 1];\\n+            // Format as YAML with proper indentation\\n+            const yamlOutput = yaml.dump({ [stepName]: lastOutput }, { indent: 2, lineWidth: 120, noRefs: true });\\n+            // Indent the YAML output for proper nesting under 'mocks:'\\n+            const indented = yamlOutput.split('\\\\n').map(line => '  ' + line).join('\\\\n');\\n+            console.log(indented);\\n+          }\\n+          console.log('');\\n+        }\\n+\\n         if (process.env.VISOR_DEBUG === 'true') {\\n           try {\\n             const names = (res.statistics.checks || []).map(\\n@@ -836,6 +924,21 @@ export class VisorTestRunner {\\n         }\\n         // avoid printing raw history keys each case\\n         // (fallback for on_finish static targets handled inside executeTestCase)\\n+        // Workflow testing: compute workflow outputs if workflow has outputs defined\\n+        let workflowOutputs: Record<string, unknown> | undefined;\\n+        if (Array.isArray((cfgLocal as any).outputs)) {\\n+          try {\\n+            workflowOutputs = this.computeWorkflowOutputs(\\n+              (cfgLocal as any).outputs,\\n+              exec.outHistory,\\n+              res.results\\n+            );\\n+          } catch (e) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ‚ö†Ô∏è Error computing workflow outputs: ${e}`);\\n+            }\\n+          }\\n+        }\\n         const caseFailures = require('./evaluators').evaluateCase(\\n           _case.name,\\n           res.statistics,\\n@@ -845,7 +948,8 @@ export class VisorTestRunner {\\n           setup.strict,\\n           setup.prompts,\\n           res.results,\\n-          exec.outHistory\\n+          exec.outHistory,\\n+          workflowOutputs\\n         );\\n         // Warn about unmocked AI/command steps that executed\\n         try {\\n@@ -1280,7 +1384,8 @@ export class VisorTestRunner {\\n   ): void {\\n     const executed: Record<string, number> = {};\\n     for (const s of stats.checks) {\\n-      if (!s.skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n+      const skipped = (s as any).skipped === true || !!(s as any).skipReason;\\n+      if (!skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n     }\\n     const expCalls = (expect.calls || []).filter(c => c.step);\\n     const expectedSteps = new Map<\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/validator.ts\",\"additions\":48,\"deletions\":2,\"changes\":50,\"patch\":\"diff --git a/src/test-runner/validator.ts b/src/test-runner/validator.ts\\nindex dbb890c5..5ffff5ee 100644\\n--- a/src/test-runner/validator.ts\\n+++ b/src/test-runner/validator.ts\\n@@ -25,6 +25,16 @@ const schema: any = {\\n     hooks: { type: 'object' },\\n     slack: { type: 'object' },\\n     frontends: { type: 'array' },\\n+    workspace: { type: 'object' },\\n+    // Workflow definition fields (for workflow files with co-located tests)\\n+    id: { type: 'string' },\\n+    name: { type: 'string' },\\n+    description: { type: 'string' },\\n+    inputs: { type: 'array' },\\n+    outputs: { type: 'array' },\\n+    ai_mcp_servers: { type: 'object' },\\n+    ai_provider: { type: 'string' },\\n+    ai_model: { type: 'string' },\\n     tests: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -136,6 +146,8 @@ const schema: any = {\\n             oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n           },\\n         },\\n+        // Workflow testing: input values to pass to the workflow\\n+        workflow_input: { type: 'object' },\\n         expect: { $ref: '#/$defs/expectBlock' },\\n         // Flow cases\\n         flow: {\\n@@ -270,6 +282,35 @@ const schema: any = {\\n       },\\n       required: ['step', 'path'],\\n     },\\n+    // Workflow output expectation: like outputsExpectation but without step (tests workflow-level outputs)\\n+    workflowOutputExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        path: { type: 'string' },\\n+        equals: {},\\n+        equalsDeep: {},\\n+        matches: { type: 'string' },\\n+        contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        not_contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            path: { type: 'string' },\\n+            equals: {},\\n+            matches: { type: 'string' },\\n+          },\\n+          required: ['path'],\\n+        },\\n+        contains_unordered: { type: 'array' },\\n+      },\\n+      required: ['path'],\\n+    },\\n     expectBlock: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -278,6 +319,8 @@ const schema: any = {\\n         calls: { type: 'array', items: { $ref: '#/$defs/callsExpectation' } },\\n         prompts: { type: 'array', items: { $ref: '#/$defs/promptsExpectation' } },\\n         outputs: { type: 'array', items: { $ref: '#/$defs/outputsExpectation' } },\\n+        // Workflow testing: assert on workflow-level outputs (defined in workflow's outputs: section)\\n+        workflow_output: { type: 'array', items: { $ref: '#/$defs/workflowOutputExpectation' } },\\n         no_calls: {\\n           type: 'array',\\n           items: {\\n@@ -343,6 +386,7 @@ const knownKeys = new Set([\\n   'version',\\n   'extends',\\n   'tests',\\n+  'workspace',\\n   // tests\\n   'tests.defaults',\\n   'tests.fixtures',\\n@@ -363,6 +407,7 @@ const knownKeys = new Set([\\n   'env',\\n   'routing',\\n   'mocks',\\n+  'workflow_input',\\n   'expect',\\n   'flow',\\n   // expect\\n@@ -370,6 +415,7 @@ const knownKeys = new Set([\\n   'expect.calls',\\n   'expect.prompts',\\n   'expect.outputs',\\n+  'expect.workflow_output',\\n   'expect.no_calls',\\n   'expect.fail',\\n   'expect.strict_violation',\\n@@ -424,7 +470,7 @@ function formatError(e: ErrorObject): string {\\n       if (hint) msg += ` (${hint})`;\\n       // Small curated allow-list for frequent nodes to reduce guesswork\\n       if (path.endsWith('expect')) {\\n-        msg += ` (allowed: use, calls, prompts, outputs, no_calls, fail, strict_violation)`;\\n+        msg += ` (allowed: use, calls, prompts, outputs, workflow_output, no_calls, fail, strict_violation)`;\\n       } else if (path.endsWith('env')) {\\n         msg += ` (values must be strings)`;\\n       } else if (path.endsWith('tests')) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/cli.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/cli.ts b/src/types/cli.ts\\nindex a7d9ca30..f06f93d5 100644\\n--- a/src/types/cli.ts\\n+++ b/src/types/cli.ts\\n@@ -61,6 +61,8 @@ export interface CliOptions {\\n   githubV2?: boolean;\\n   /** Enable Slack Socket Mode runner */\\n   slack?: boolean;\\n+  /** Keep workspace folders after execution (for debugging) */\\n+  keepWorkspace?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/config.ts\",\"additions\":114,\"deletions\":5,\"changes\":119,\"patch\":\"diff --git a/src/types/config.ts b/src/types/config.ts\\nindex ed239959..dd363de1 100644\\n--- a/src/types/config.ts\\n+++ b/src/types/config.ts\\n@@ -122,6 +122,9 @@ export interface FailureConditionContext {\\n   /** Environment variables */\\n   env?: Record<string, string>;\\n \\n+  /** Workflow inputs - values passed to the workflow */\\n+  inputs?: Record<string, unknown>;\\n+\\n   /** Memory accessor for accessing memory store in expressions */\\n   memory?: {\\n     get: (key: string, namespace?: string) => unknown;\\n@@ -356,6 +359,8 @@ export interface AIProviderConfig {\\n   allowBash?: boolean;\\n   /** Advanced bash command execution configuration */\\n   bashConfig?: BashConfig;\\n+  /** Completion prompt for post-completion validation/review (runs after attempt_completion) */\\n+  completion_prompt?: string;\\n }\\n \\n /**\\n@@ -461,8 +466,8 @@ export interface CheckConfig {\\n   env?: EnvConfig;\\n   /** Timeout in seconds for command execution (default: 60) */\\n   timeout?: number;\\n-  /** Check IDs that this check depends on (optional) */\\n-  depends_on?: string[];\\n+  /** Check IDs that this check depends on (optional). Accepts single string or array. */\\n+  depends_on?: string | string[];\\n   /** Group name for comment separation (e.g., \\\"code-review\\\", \\\"pr-overview\\\") - optional */\\n   group?: string;\\n   /** Schema type for template rendering (e.g., \\\"code-review\\\", \\\"markdown\\\") or inline JSON schema object - optional */\\n@@ -621,6 +626,46 @@ export interface CheckConfig {\\n   overrides?: Record<string, Partial<CheckConfig>>;\\n   /** Map workflow outputs to check outputs */\\n   output_mapping?: Record<string, string>;\\n+\\n+  // Workflow aliases for backward compatibility\\n+  /** Alias for args - workflow inputs (backward compatibility) */\\n+  workflow_inputs?: Record<string, unknown>;\\n+  /** Config file path - alternative to workflow ID (loads a Visor config file as workflow) */\\n+  config?: string;\\n+  /** Alias for overrides - workflow step overrides (backward compatibility) */\\n+  workflow_overrides?: Record<string, Partial<CheckConfig>>;\\n+\\n+  /**\\n+   * Git-checkout provider specific options (optional, only used when type === 'git-checkout').\\n+   */\\n+  /** Git reference to checkout (branch, tag, commit SHA) - supports templates */\\n+  ref?: string;\\n+  /** Repository URL or owner/repo format (defaults to current repository) */\\n+  repository?: string;\\n+  /** GitHub token for private repositories (defaults to GITHUB_TOKEN env) */\\n+  token?: string;\\n+  /** Number of commits to fetch (0 for full history, default: 1) */\\n+  fetch_depth?: number;\\n+  /** Whether to fetch tags (default: false) */\\n+  fetch_tags?: boolean;\\n+  /** Checkout submodules: false, true, or 'recursive' */\\n+  submodules?: boolean | 'recursive';\\n+  /** Working directory for the checkout (defaults to temp directory) */\\n+  working_directory?: string;\\n+  /** Use git worktree for efficient parallel checkouts (default: true) */\\n+  use_worktree?: boolean;\\n+  /** Clean the working directory before checkout (default: true) */\\n+  clean?: boolean;\\n+  /** Sparse checkout paths - only checkout specific directories/files */\\n+  sparse_checkout?: string[];\\n+  /** Enable Git LFS (Large File Storage) */\\n+  lfs?: boolean;\\n+  /** Timeout in ms for cloning the bare repository (default: 300000 = 5 min) */\\n+  clone_timeout_ms?: number;\\n+  /** Clean up worktree on failure (default: true) */\\n+  cleanup_on_failure?: boolean;\\n+  /** Keep worktree after workflow completion (default: false) */\\n+  persist_worktree?: boolean;\\n }\\n \\n /**\\n@@ -1037,6 +1082,36 @@ export interface CustomToolDefinition {\\n   outputSchema?: Record<string, unknown>;\\n }\\n \\n+/**\\n+ * Workflow input definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowInput {\\n+  /** Input parameter name */\\n+  name: string;\\n+  /** JSON Schema for the input */\\n+  schema?: Record<string, unknown>;\\n+  /** Whether this input is required */\\n+  required?: boolean;\\n+  /** Default value if not provided */\\n+  default?: unknown;\\n+  /** Human-readable description */\\n+  description?: string;\\n+}\\n+\\n+/**\\n+ * Workflow output definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowOutput {\\n+  /** Output name */\\n+  name: string;\\n+  /** Human-readable description */\\n+  description?: string;\\n+  /** Value using Liquid template syntax (references step outputs) */\\n+  value?: string;\\n+  /** Value using JavaScript expression (alternative to value) */\\n+  value_js?: string;\\n+}\\n+\\n /**\\n  * Main Visor configuration\\n  */\\n@@ -1051,12 +1126,16 @@ export interface VisorConfig {\\n   tools?: Record<string, CustomToolDefinition>;\\n   /** Import workflow definitions from external files or URLs */\\n   imports?: string[];\\n+  /** Workflow inputs (for standalone reusable workflows) */\\n+  inputs?: WorkflowInput[];\\n+  /** Workflow outputs (for standalone reusable workflows) */\\n+  outputs?: WorkflowOutput[];\\n   /** Step configurations (recommended) */\\n   steps?: Record<string, CheckConfig>;\\n   /** Check configurations (legacy, use 'steps' instead) - always populated after normalization */\\n   checks?: Record<string, CheckConfig>;\\n-  /** Output configuration */\\n-  output: OutputConfig;\\n+  /** Output configuration (optional - defaults provided) */\\n+  output?: OutputConfig;\\n   /** HTTP server configuration for receiving webhooks */\\n   http_server?: HttpServerConfig;\\n   /** Memory storage configuration */\\n@@ -1092,6 +1171,36 @@ export interface VisorConfig {\\n     /** Frontend-specific configuration */\\n     config?: unknown;\\n   }>;\\n+  /** Workspace isolation configuration for sandboxed execution */\\n+  workspace?: WorkspaceConfig;\\n+  /** Slack configuration */\\n+  slack?: SlackConfig;\\n+}\\n+\\n+/**\\n+ * Workspace isolation configuration\\n+ */\\n+export interface WorkspaceConfig {\\n+  /** Enable workspace isolation (default: true when config present) */\\n+  enabled?: boolean;\\n+  /** Base path for workspaces (default: /tmp/visor-workspaces) */\\n+  base_path?: string;\\n+  /** Clean up workspace on exit (default: true) */\\n+  cleanup_on_exit?: boolean;\\n+}\\n+\\n+/**\\n+ * Slack configuration\\n+ */\\n+export interface SlackConfig {\\n+  /** Slack API version */\\n+  version?: string;\\n+  /** Mention handling: 'all', 'direct', etc. */\\n+  mentions?: string;\\n+  /** Thread handling: 'required', 'optional', etc. */\\n+  threads?: string;\\n+  /** Show raw output in Slack responses */\\n+  show_raw_output?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/git-checkout.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/git-checkout.ts b/src/types/git-checkout.ts\\nindex eb1c5ebc..4d27855d 100644\\n--- a/src/types/git-checkout.ts\\n+++ b/src/types/git-checkout.ts\\n@@ -16,6 +16,8 @@ export interface GitCheckoutConfig {\\n   fetch_depth?: number;\\n   fetch_tags?: boolean;\\n   submodules?: boolean | 'recursive';\\n+  /** Timeout (ms) for cloning the bare repository; defaults to 300000 (5 minutes) */\\n+  clone_timeout_ms?: number;\\n \\n   // Worktree configuration\\n   working_directory?: string;\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/sandbox.ts\",\"additions\":46,\"deletions\":1,\"changes\":47,\"patch\":\"diff --git a/src/utils/sandbox.ts b/src/utils/sandbox.ts\\nindex 9914a3a5..12e27c30 100644\\n--- a/src/utils/sandbox.ts\\n+++ b/src/utils/sandbox.ts\\n@@ -14,6 +14,51 @@ export interface CompileOptions {\\n   wrapFunction?: boolean;\\n }\\n \\n+export interface JsSyntaxValidationResult {\\n+  valid: boolean;\\n+  error?: string;\\n+}\\n+\\n+/**\\n+ * Validate JavaScript syntax without executing it.\\n+ * Uses the sandbox's compile method to check for syntax errors.\\n+ * Returns validation result with error message if invalid.\\n+ */\\n+export function validateJsSyntax(code: string): JsSyntaxValidationResult {\\n+  if (!code || typeof code !== 'string') {\\n+    return { valid: false, error: 'Code must be a non-empty string' };\\n+  }\\n+\\n+  const trimmed = code.trim();\\n+  if (trimmed.length === 0) {\\n+    return { valid: false, error: 'Code cannot be empty' };\\n+  }\\n+\\n+  // Create a minimal sandbox instance for syntax checking\\n+  const sandbox = createSecureSandbox();\\n+\\n+  // Wrap code similar to compileAndRun to catch the same syntax issues\\n+  const looksLikeBlock = /\\\\breturn\\\\b/.test(trimmed) || /;/.test(trimmed) || /\\\\n/.test(trimmed);\\n+  const looksLikeIife = /\\\\)\\\\s*\\\\(\\\\s*\\\\)\\\\s*;?$/.test(trimmed);\\n+  const body = looksLikeBlock\\n+    ? looksLikeIife\\n+      ? `return (\\\\n${trimmed}\\\\n);\\\\n`\\n+      : `return (() => {\\\\n${trimmed}\\\\n})();\\\\n`\\n+    : `return (\\\\n${trimmed}\\\\n);\\\\n`;\\n+\\n+  // Add log injection header (same as compileAndRun)\\n+  const header = `const __lp = \\\"[syntax-check]\\\"; const log = (...a) => { try { console.log(__lp, ...a); } catch {} };\\\\n`;\\n+  const fullCode = `${header}${body}`;\\n+\\n+  try {\\n+    sandbox.compile(fullCode);\\n+    return { valid: true };\\n+  } catch (e) {\\n+    const msg = e instanceof Error ? e.message : String(e);\\n+    return { valid: false, error: msg };\\n+  }\\n+}\\n+\\n /**\\n  * Create a hardened Sandbox with a consistent set of globals and prototype\\n  * whitelists. This is a superset of the sets previously used by individual\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/tracer-init.ts\",\"additions\":19,\"deletions\":1,\"changes\":20,\"patch\":\"diff --git a/src/utils/tracer-init.ts b/src/utils/tracer-init.ts\\nindex b42995d9..4edf1ffc 100644\\n--- a/src/utils/tracer-init.ts\\n+++ b/src/utils/tracer-init.ts\\n@@ -76,6 +76,24 @@ export async function initializeTracer(\\n \\n       const tracer = new SimpleAppTracer(telemetry, sessionId);\\n \\n+      // WORKAROUND: Add missing recordEvent method for completionPrompt feature (probe #321)\\n+      // SimpleAppTracer doesn't have recordEvent but completionPrompt requires it\\n+      if (typeof (tracer as any).recordEvent !== 'function') {\\n+        (tracer as any).recordEvent = (\\n+          name: string,\\n+          attributes?: Record<string, unknown>\\n+        ) => {\\n+          // Log completion events to telemetry for debugging\\n+          try {\\n+            if ((telemetry as any).record) {\\n+              (telemetry as any).record({ event: name, ...attributes });\\n+            }\\n+          } catch {\\n+            // Best-effort only\\n+          }\\n+        };\\n+      }\\n+\\n       console.error(`üìä Simple tracing enabled, will save to: ${traceFilePath}`);\\n \\n       // If in GitHub Actions, log the path for artifact upload\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/worktree-manager.ts\",\"additions\":142,\"deletions\":27,\"changes\":169,\"patch\":\"diff --git a/src/utils/worktree-manager.ts b/src/utils/worktree-manager.ts\\nindex f3cebf3d..2de7a9f4 100644\\n--- a/src/utils/worktree-manager.ts\\n+++ b/src/utils/worktree-manager.ts\\n@@ -6,6 +6,7 @@\\n  */\\n \\n import * as fs from 'fs';\\n+import * as fsp from 'fs/promises';\\n import * as path from 'path';\\n import * as crypto from 'crypto';\\n import { commandExecutor } from './command-executor';\\n@@ -119,7 +120,8 @@ export class WorktreeManager {\\n     repository: string,\\n     repoUrl: string,\\n     token?: string,\\n-    fetchDepth?: number\\n+    fetchDepth?: number,\\n+    cloneTimeoutMs?: number\\n   ): Promise<string> {\\n     const reposDir = this.getReposDir();\\n     const repoName = repository.replace(/\\\\//g, '-');\\n@@ -129,9 +131,21 @@ export class WorktreeManager {\\n     if (fs.existsSync(bareRepoPath)) {\\n       logger.debug(`Bare repository already exists: ${bareRepoPath}`);\\n \\n-      // Update remote refs\\n-      await this.updateBareRepo(bareRepoPath);\\n-      return bareRepoPath;\\n+      // Verify the bare repo has the correct remote URL to prevent using corrupted repos\\n+      const verifyResult = await this.verifyBareRepoRemote(bareRepoPath, repoUrl);\\n+      if (verifyResult === 'timeout') {\\n+        // Timeout during verification - use stale cache to avoid hanging on re-clone\\n+        logger.info(`Using stale bare repository (verification timed out): ${bareRepoPath}`);\\n+        return bareRepoPath;\\n+      } else if (verifyResult === false) {\\n+        logger.warn(`Bare repository at ${bareRepoPath} has incorrect remote, removing and re-cloning`);\\n+        await fsp.rm(bareRepoPath, { recursive: true, force: true });\\n+        // Fall through to clone below\\n+      } else {\\n+        // Update remote refs\\n+        await this.updateBareRepo(bareRepoPath);\\n+        return bareRepoPath;\\n+      }\\n     }\\n \\n     // Clone as bare repository\\n@@ -153,7 +167,9 @@ export class WorktreeManager {\\n     }\\n     cloneCmd += ` ${this.escapeShellArg(cloneUrl)} ${this.escapeShellArg(bareRepoPath)}`;\\n \\n-    const result = await this.executeGitCommand(cloneCmd, { timeout: 300000 }); // 5 minute timeout\\n+    const result = await this.executeGitCommand(cloneCmd, {\\n+      timeout: cloneTimeoutMs || 300000, // default 5 minutes\\n+    });\\n \\n     if (result.exitCode !== 0) {\\n       // Redact tokens from error messages\\n@@ -171,19 +187,98 @@ export class WorktreeManager {\\n   private async updateBareRepo(bareRepoPath: string): Promise<void> {\\n     logger.debug(`Updating bare repository: ${bareRepoPath}`);\\n \\n-    const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n-    const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+    try {\\n+      const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n+      const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n+        // Don't throw - we can continue with stale refs\\n+      } else {\\n+        logger.debug(`Successfully updated bare repository`);\\n+      }\\n+    } catch (error) {\\n+      // Handle timeout or other errors gracefully - we can continue with stale refs\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      logger.warn(`Failed to update bare repository (will use stale refs): ${errorMessage}`);\\n+    }\\n+  }\\n \\n-    if (result.exitCode !== 0) {\\n-      logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n-      // Don't throw - we can continue with stale refs\\n-    } else {\\n-      logger.debug(`Successfully updated bare repository`);\\n+  /**\\n+   * Verify that a bare repository has the correct remote URL.\\n+   * This prevents reusing corrupted repos that were cloned from a different repository.\\n+   * Returns: true (valid), false (invalid - should re-clone), or 'timeout' (use stale cache)\\n+   */\\n+  private async verifyBareRepoRemote(\\n+    bareRepoPath: string,\\n+    expectedUrl: string\\n+  ): Promise<boolean | 'timeout'> {\\n+    try {\\n+      const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote get-url origin`;\\n+      const result = await this.executeGitCommand(cmd, { timeout: 10000 });\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to get remote URL for ${bareRepoPath}: ${result.stderr}`);\\n+        return false;\\n+      }\\n+\\n+      const actualUrl = result.stdout.trim();\\n+\\n+      // Normalize URLs for comparison:\\n+      // - remove credentials (tokens/username)\\n+      // - remove .git suffix if present\\n+      // - trim trailing slash\\n+      // - lowercase for case‚Äëinsensitive match\\n+      const normalizeUrl = (url: string): string => {\\n+        // Convert common SSH form to https for comparison\\n+        if (url.startsWith('git@github.com:')) {\\n+          url = url.replace('git@github.com:', 'https://github.com/');\\n+        }\\n+        return url\\n+          // strip userinfo part to avoid mismatches when the cached bare\\n+          // repo was cloned with an access token but the expected URL is\\n+          // tokenless (or vice‚Äëversa)\\n+          .replace(/:\\\\/\\\\/[^@]+@/, '://')\\n+          .replace(/\\\\.git$/, '')\\n+          .replace(/\\\\/$/, '')\\n+          .toLowerCase();\\n+      };\\n+\\n+      const normalizedExpected = normalizeUrl(expectedUrl);\\n+      const normalizedActual = normalizeUrl(actualUrl);\\n+\\n+      if (normalizedExpected !== normalizedActual) {\\n+        logger.warn(\\n+          `Bare repo remote mismatch: expected ${expectedUrl}, got ${actualUrl}`\\n+        );\\n+        return false;\\n+      }\\n+\\n+      logger.debug(`Bare repo remote verified: ${actualUrl}`);\\n+      return true;\\n+    } catch (error) {\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      // If it's a timeout, return 'timeout' so caller can use stale cache instead of re-cloning\\n+      if (errorMessage.includes('timed out')) {\\n+        logger.warn(`Timeout verifying bare repo remote (will use stale cache): ${errorMessage}`);\\n+        return 'timeout';\\n+      }\\n+      logger.warn(`Error verifying bare repo remote: ${error}`);\\n+      return false;\\n     }\\n   }\\n \\n   /**\\n-   * Create a new worktree\\n+   * Create a new worktree for the given repository/ref.\\n+   *\\n+   * Important: we always create worktrees in a detached HEAD state pinned\\n+   * to a specific commit SHA rather than a named branch like \\\"main\\\". Git\\n+   * only allows a branch to be checked out in a single worktree at a time;\\n+   * using the raw commit (plus --detach) lets multiple workflows safely\\n+   * create independent worktrees for the same branch without hitting\\n+   * errors like:\\n+   *\\n+   *   fatal: 'main' is already used by worktree at '.../TykTechnologies-tyk-docs-main-XXXX'\\n    */\\n   async createWorktree(\\n     repository: string,\\n@@ -195,6 +290,7 @@ export class WorktreeManager {\\n       clean?: boolean;\\n       workflowId?: string;\\n       fetchDepth?: number;\\n+      cloneTimeoutMs?: number;\\n     } = {}\\n   ): Promise<WorktreeInfo> {\\n     // Validate ref to prevent command injection\\n@@ -205,7 +301,8 @@ export class WorktreeManager {\\n       repository,\\n       repoUrl,\\n       options.token,\\n-      options.fetchDepth\\n+      options.fetchDepth,\\n+      options.cloneTimeoutMs\\n     );\\n \\n     // Generate worktree ID and path\\n@@ -241,21 +338,24 @@ export class WorktreeManager {\\n       }\\n     }\\n \\n-    // Fetch the ref if needed\\n+    // Fetch the ref if needed, then resolve it to a concrete commit SHA.\\n+    // We use the commit (detached HEAD) instead of the branch name so we\\n+    // can have multiple worktrees for the same branch without git refusing\\n+    // with \\\"branch X is already checked out\\\".\\n     await this.fetchRef(bareRepoPath, ref);\\n+    const commit = await this.getCommitShaForRef(bareRepoPath, ref);\\n \\n-    // Create worktree\\n-    logger.info(`Creating worktree for ${repository}@${ref}`);\\n-    const createCmd = `git -C ${this.escapeShellArg(bareRepoPath)} worktree add ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(ref)}`;\\n+    // Create worktree in detached HEAD state at the resolved commit\\n+    logger.info(`Creating worktree for ${repository}@${ref} (${commit})`);\\n+    const createCmd = `git -C ${this.escapeShellArg(\\n+      bareRepoPath\\n+    )} worktree add --detach ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(commit)}`;\\n     const result = await this.executeGitCommand(createCmd, { timeout: 60000 });\\n \\n     if (result.exitCode !== 0) {\\n       throw new Error(`Failed to create worktree: ${result.stderr}`);\\n     }\\n \\n-    // Get commit SHA\\n-    const commit = await this.getCommitSha(worktreePath);\\n-\\n     // Create metadata\\n     const metadata: WorktreeMetadata = {\\n       worktree_id: worktreeId,\\n@@ -316,14 +416,17 @@ export class WorktreeManager {\\n   }\\n \\n   /**\\n-   * Get commit SHA for worktree\\n+   * Get commit SHA for a given ref inside a bare repository.\\n+   *\\n+   * This runs after fetchRef so that <ref> should resolve to either a\\n+   * local branch, tag, or remote-tracking ref.\\n    */\\n-  private async getCommitSha(worktreePath: string): Promise<string> {\\n-    const cmd = `git -C ${this.escapeShellArg(worktreePath)} rev-parse HEAD`;\\n+  private async getCommitShaForRef(bareRepoPath: string, ref: string): Promise<string> {\\n+    const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} rev-parse ${this.escapeShellArg(ref)}`;\\n     const result = await this.executeGitCommand(cmd);\\n \\n     if (result.exitCode !== 0) {\\n-      throw new Error(`Failed to get commit SHA: ${result.stderr}`);\\n+      throw new Error(`Failed to get commit SHA for ref ${ref}: ${result.stderr}`);\\n     }\\n \\n     return result.stdout.trim();\\n@@ -621,9 +724,21 @@ export class WorktreeManager {\\n     command: string,\\n     options: { timeout?: number; env?: Record<string, string> } = {}\\n   ): Promise<GitCommandResult> {\\n+    // Merge provided env with process.env and add git-specific settings\\n+    // These settings prevent git from hanging on interactive prompts while\\n+    // still allowing OS-level credential helpers to work:\\n+    // - GIT_TERMINAL_PROMPT=0: Prevents terminal credential prompts\\n+    // - GIT_SSH_COMMAND: Disables SSH password prompts (BatchMode)\\n+    const gitEnv = {\\n+      ...process.env,\\n+      ...options.env,\\n+      GIT_TERMINAL_PROMPT: '0',\\n+      GIT_SSH_COMMAND: 'ssh -o BatchMode=yes -o StrictHostKeyChecking=no',\\n+    };\\n+\\n     const result = await commandExecutor.execute(command, {\\n       timeout: options.timeout || 30000,\\n-      env: options.env || process.env,\\n+      env: gitEnv,\\n     } as any);\\n \\n     return {\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-executor.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-executor.ts b/src/workflow-executor.ts\\nindex 751e7f16..df76762f 100644\\n--- a/src/workflow-executor.ts\\n+++ b/src/workflow-executor.ts\\n@@ -219,7 +219,9 @@ export class WorkflowExecutor {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps)) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     // Use static DependencyResolver\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-registry.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-registry.ts b/src/workflow-registry.ts\\nindex 2cbe3a89..6bd855fb 100644\\n--- a/src/workflow-registry.ts\\n+++ b/src/workflow-registry.ts\\n@@ -410,7 +410,9 @@ export class WorkflowRegistry {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps || {})) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     try {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/ai-review-service.test.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/tests/unit/ai-review-service.test.ts b/tests/unit/ai-review-service.test.ts\\nindex ec2b998a..2c616c99 100644\\n--- a/tests/unit/ai-review-service.test.ts\\n+++ b/tests/unit/ai-review-service.test.ts\\n@@ -482,6 +482,37 @@ describe('AIReviewService', () => {\\n       expect(context).toContain('<additions>10</additions>');\\n       expect(context).toContain('<deletions>5</deletions>');\\n     });\\n+\\n+    it('should exclude diffs by default when in Slack mode', async () => {\\n+      // Slack mode is detected by presence of slackConversation property\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // In Slack mode, diffs should be excluded by default\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+      // Should still include file summary\\n+      expect(context).toContain('<files_summary>');\\n+    });\\n+\\n+    it('should include diffs in Slack mode when explicitly requested', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      (mockPRInfo as any).includeCodeContext = true;\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // With explicit includeCodeContext: true, diffs should be included\\n+      expect(context).toContain('<full_diff>');\\n+      expect(context).toContain('--- test.ts');\\n+    });\\n+\\n+    it('should exclude diffs in Slack mode even when includeCodeContext is undefined', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      // Deliberately not setting includeCodeContext to test default behavior\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+    });\\n   });\\n \\n   describe('Comment Filtering for Code Review', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config-extends.test.ts\",\"additions\":8,\"deletions\":8,\"changes\":16,\"patch\":\"diff --git a/tests/unit/config-extends.test.ts b/tests/unit/config-extends.test.ts\\nindex b112a869..7ee69ce6 100644\\n--- a/tests/unit/config-extends.test.ts\\n+++ b/tests/unit/config-extends.test.ts\\n@@ -564,7 +564,7 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.security).toBeDefined();\\n       expect(config.checks!.performance).toBeDefined();\\n       expect(config.checks!.custom).toBeDefined();\\n-      expect(config.output.pr_comment.format).toBe('json');\\n+      expect(config.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should extend from default configuration', async () => {\\n@@ -1002,7 +1002,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.base).toBeDefined(); // From base\\n       expect(loaded.checks!.middle).toBeDefined(); // From middle\\n       expect(loaded.checks!.top).toBeDefined(); // From top\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From top\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From top\\n     });\\n \\n     it('should handle mixed local and remote extends', async () => {\\n@@ -1079,7 +1079,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.local).toBeDefined();\\n       expect(loaded.checks!.remote).toBeDefined();\\n       expect(loaded.checks!.child).toBeDefined();\\n-      expect(loaded.output.pr_comment.format).toBe('json');\\n+      expect(loaded.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should handle extends with relative path resolution', async () => {\\n@@ -1593,9 +1593,9 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.performance).toBeDefined(); // Added in child\\n \\n       // Verify output\\n-      expect(config.output.pr_comment.format).toBe('json'); // Overridden\\n-      expect(config.output.pr_comment.group_by).toBe('file'); // Overridden\\n-      expect(config.output.pr_comment.collapse).toBe(false); // Overridden\\n+      expect(config.output!.pr_comment.format).toBe('json'); // Overridden\\n+      expect(config.output!.pr_comment.group_by).toBe('file'); // Overridden\\n+      expect(config.output!.pr_comment.collapse).toBe(false); // Overridden\\n     });\\n \\n     it('should handle configuration with disabled checks removal', async () => {\\n@@ -1814,7 +1814,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.root).toBeDefined(); // From root\\n       expect(loaded.checks!.level1).toBeDefined(); // From level1\\n       expect(loaded.checks!.level2).toBeDefined(); // From level2\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From level2\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From level2\\n     });\\n \\n     it('should handle mixed absolute and relative paths in extends array', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config.test.ts\",\"additions\":7,\"deletions\":7,\"changes\":14,\"patch\":\"diff --git a/tests/unit/config.test.ts b/tests/unit/config.test.ts\\nindex 67ce61fd..0677a3d4 100644\\n--- a/tests/unit/config.test.ts\\n+++ b/tests/unit/config.test.ts\\n@@ -48,7 +48,7 @@ output:\\n       expect(config.version).toBe('1.0');\\n       expect(config.checks).toHaveProperty('performance');\\n       expect(config.checks).toHaveProperty('security');\\n-      expect(config.output.pr_comment.format).toBe('table');\\n+      expect(config.output!.pr_comment.format).toBe('table');\\n     });\\n \\n     it('should handle missing config file gracefully', async () => {\\n@@ -264,9 +264,9 @@ checks:\\n       const config = await configManager.loadConfig('/path/to/minimal.yaml');\\n \\n       // Should have default output configuration\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.group_by).toBe('check');\\n-      expect(config.output.pr_comment.collapse).toBe(true);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.group_by).toBe('check');\\n+      expect(config.output!.pr_comment.collapse).toBe(true);\\n     });\\n   });\\n \\n@@ -483,8 +483,8 @@ output:\\n       expect(config.checks!.performance.prompt).toContain('N+1 database queries');\\n       expect(config.checks!.security.prompt).toContain('SQL injection');\\n       expect(config.checks!.architecture.prompt).toContain('SOLID principles');\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.collapse).toBe(false);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.collapse).toBe(false);\\n     });\\n   });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/foreach-empty-skip.test.ts\",\"additions\":5,\"deletions\":3,\"changes\":8,\"patch\":\"diff --git a/tests/unit/foreach-empty-skip.test.ts b/tests/unit/foreach-empty-skip.test.ts\\nindex 1ea8e1e5..2640ed14 100644\\n--- a/tests/unit/foreach-empty-skip.test.ts\\n+++ b/tests/unit/foreach-empty-skip.test.ts\\n@@ -90,11 +90,13 @@ describe('forEach Empty Array Skip', () => {\\n     );\\n     expect(processStats?.totalRuns).toBe(0);\\n \\n-    // notify-ticket should also NOT run because process-ticket returned empty\\n+    // notify-ticket SHOULD run even though process-ticket had 0 runs from empty forEach\\n+    // This is the expected behavior: downstream steps that depend on forEach children\\n+    // should still execute (with empty/null inputs) to handle the \\\"no items\\\" case\\n     const notifyStats = result.executionStatistics?.checks!.find(\\n       c => c.checkName === 'notify-ticket'\\n     );\\n-    expect(notifyStats?.totalRuns).toBe(0);\\n+    expect(notifyStats?.totalRuns).toBe(1);\\n   });\\n \\n   it('should handle empty array with transform_js', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/ai-check-provider.test.ts\",\"additions\":61,\"deletions\":1,\"changes\":62,\"patch\":\"diff --git a/tests/unit/providers/ai-check-provider.test.ts b/tests/unit/providers/ai-check-provider.test.ts\\nindex d0b94086..a858ddae 100644\\n--- a/tests/unit/providers/ai-check-provider.test.ts\\n+++ b/tests/unit/providers/ai-check-provider.test.ts\\n@@ -399,6 +399,66 @@ describe('AICheckProvider', () => {\\n         },\\n       });\\n     });\\n+\\n+    it('derives allowedFolders and path from workspace when present on parent context', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const workspace = {\\n+        isEnabled: () => true,\\n+        getWorkspaceInfo: () => ({\\n+          workspacePath: '/tmp/ws-123',\\n+          mainProjectPath: '/tmp/ws-123/main-project',\\n+        }),\\n+        listProjects: () => [\\n+          { name: 'tyk', path: '/tmp/ws-123/tyk' },\\n+          { name: 'tyk-docs', path: '/tmp/ws-123/tyk-docs' },\\n+        ],\\n+      };\\n+\\n+      const execContext: any = {\\n+        _parentContext: {\\n+          workspace,\\n+        },\\n+      };\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'code_help',\\n+        ai: {\\n+          provider: 'google',\\n+          model: 'gemini-2.5-pro',\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, execContext);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'google',\\n+        model: 'gemini-2.5-pro',\\n+        // Workspace root should be used as primary working directory for tools\\n+        path: '/tmp/ws-123',\\n+      });\\n+\\n+      // allowedFolders should contain the workspace plus all project paths, de-duped\\n+      expect(capturedConfig.allowedFolders).toEqual(\\n+        expect.arrayContaining(['/tmp/ws-123', '/tmp/ws-123/tyk', '/tmp/ws-123/tyk-docs'])\\n+      );\\n+    });\\n   });\\n \\n   describe('getSupportedConfigKeys', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/command-check-provider.test.ts\",\"additions\":105,\"deletions\":1,\"changes\":106,\"patch\":\"diff --git a/tests/unit/providers/command-check-provider.test.ts b/tests/unit/providers/command-check-provider.test.ts\\nindex 3bf11a84..0a821bbd 100644\\n--- a/tests/unit/providers/command-check-provider.test.ts\\n+++ b/tests/unit/providers/command-check-provider.test.ts\\n@@ -666,4 +666,108 @@ describe('CommandCheckProvider', () => {\\n       expect((result as any).output).toBe('content'); // CommandCheckProvider trims whitespace\\n     });\\n   });\\n+\\n+  describe('Workflow Inputs', () => {\\n+    it('should use workflowInputs from config when available', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Hello from config.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from config.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with inputs from config.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from config.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should fall back to context.workflowInputs when config.workflowInputs is not set', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Hello from context.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from context.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify the command was rendered with inputs from context.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from context.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should prefer config.workflowInputs over context.workflowInputs', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Config takes precedence',\\n+        },\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Context should be ignored',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Config takes precedence\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify config.workflowInputs takes precedence\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Config takes precedence\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should provide empty inputs when neither config nor context has workflowInputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with empty inputs (undefined renders as empty)\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+  });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/git-checkout-workspace.test.ts\",\"additions\":4,\"deletions\":5,\"changes\":9,\"patch\":\"diff --git a/tests/unit/providers/git-checkout-workspace.test.ts b/tests/unit/providers/git-checkout-workspace.test.ts\\nindex 4eb625b7..4a0fbb6e 100644\\n--- a/tests/unit/providers/git-checkout-workspace.test.ts\\n+++ b/tests/unit/providers/git-checkout-workspace.test.ts\\n@@ -129,13 +129,12 @@ describe('GitCheckoutProvider Workspace Integration', () => {\\n         const output = (result as any).output as GitCheckoutOutput;\\n         expect(output.success).toBe(true);\\n         expect(output.workspace_path).toBeDefined();\\n-        // When checkName is provided, it's used as the project name\\n-        expect(output.workspace_path).toContain('test-checkout');\\n \\n-        // Verify project was added to workspace\\n+        // Verify project was added to workspace and the name is derived\\n+        // from the repository/description, not the checkName.\\n         const projects = workspace.listProjects();\\n         expect(projects.length).toBe(1);\\n-        expect(projects[0].name).toBe('test-checkout');\\n+        expect(projects[0].name).toBe('external-repo');\\n       } finally {\\n         await workspace.cleanup();\\n         fs.rmSync(mainProjectDir, { recursive: true, force: true });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/http-client-provider.test.ts\",\"additions\":15,\"deletions\":10,\"changes\":25,\"patch\":\"diff --git a/tests/unit/providers/http-client-provider.test.ts b/tests/unit/providers/http-client-provider.test.ts\\nindex b759ad75..caa73417 100644\\n--- a/tests/unit/providers/http-client-provider.test.ts\\n+++ b/tests/unit/providers/http-client-provider.test.ts\\n@@ -120,15 +120,17 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect(result).toEqual<ReviewSummary & { data: unknown }>({\\n+      // The provider returns data in the 'output' property (consistent with other providers)\\n+      expect(result).toEqual({\\n         issues: [],\\n-        data: responseData,\\n+        output: { status: 'ok', data: { value: 123 } },\\n       });\\n     });\\n \\n     it('should handle POST request with body', async () => {\\n       mockConfig.method = 'POST';\\n-      mockConfig.body = '{\\\"request\\\": \\\"data\\\"}';\\n+      // Use a body with Liquid template to trigger parseAndRender\\n+      mockConfig.body = '{\\\"request\\\": \\\"{{ pr.title }}\\\"}';\\n \\n       const responseData = { result: 'success' };\\n       const mockResponse = {\\n@@ -143,15 +145,16 @@ describe('HttpClientProvider', () => {\\n       };\\n \\n       mockFetch.mockResolvedValue(mockResponse);\\n-      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"data\\\"}');\\n+      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"Test PR\\\"}');\\n \\n       const result = await provider.execute(mockPRInfo, mockConfig, new Map());\\n \\n+      // Verify the body template was parsed by Liquid with the correct template\\n       expect(mockLiquid.parseAndRender).toHaveBeenCalledWith(\\n-        '{\\\"request\\\": \\\"data\\\"}',\\n+        '{\\\"request\\\": \\\"{{ pr.title }}\\\"}',\\n         expect.objectContaining({\\n           pr: expect.any(Object),\\n-          outputs: {},\\n+          outputs: expect.any(Object),\\n         })\\n       );\\n \\n@@ -159,7 +162,7 @@ describe('HttpClientProvider', () => {\\n         'https://api.example.com/data',\\n         expect.objectContaining({\\n           method: 'POST',\\n-          body: '{\\\"request\\\": \\\"data\\\"}',\\n+          body: '{\\\"request\\\": \\\"Test PR\\\"}',\\n           headers: expect.objectContaining({\\n             Authorization: 'Bearer token',\\n             'Content-Type': 'application/json',\\n@@ -167,7 +170,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(responseData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { result: string } }).output.result).toEqual('success');\\n     });\\n \\n     it('should handle HTTP errors', async () => {\\n@@ -235,7 +239,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(transformedData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { transformed: string } }).output.transformed).toEqual('result');\\n     });\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/session-reuse-config.test.ts\",\"additions\":20,\"deletions\":1,\"changes\":21,\"patch\":\"diff --git a/tests/unit/session-reuse-config.test.ts b/tests/unit/session-reuse-config.test.ts\\nindex a2e2ee26..18fc42e6 100644\\n--- a/tests/unit/session-reuse-config.test.ts\\n+++ b/tests/unit/session-reuse-config.test.ts\\n@@ -152,6 +152,25 @@ describe('Session Reuse Configuration Validation', () => {\\n       }).not.toThrow();\\n     });\\n \\n+    it('should accept reuse_ai_session=\\\\\\\"self\\\\\\\" without depends_on', () => {\\n+      const config: Partial<VisorConfig> = {\\n+        version: '1.0',\\n+        checks: {\\n+          'loop-check': {\\n+            type: 'ai',\\n+            prompt: 'Chat-like loop that reuses its own session',\\n+            on: ['pr_opened'],\\n+            // Special self-reuse mode does not require depends_on\\n+            reuse_ai_session: 'self',\\n+          },\\n+        },\\n+      };\\n+\\n+      expect(() => {\\n+        (configManager as any).validateConfig(config);\\n+      }).not.toThrow();\\n+    });\\n+\\n     it('should accept reuse_ai_session=true with multiple dependencies', () => {\\n       const config: Partial<VisorConfig> = {\\n         version: '1.0',\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/slack-markdown-format.test.ts\",\"additions\":25,\"deletions\":1,\"changes\":26,\"patch\":\"diff --git a/tests/unit/slack-markdown-format.test.ts b/tests/unit/slack-markdown-format.test.ts\\nindex 5e57ccae..39d0baf3 100644\\n--- a/tests/unit/slack-markdown-format.test.ts\\n+++ b/tests/unit/slack-markdown-format.test.ts\\n@@ -39,4 +39,28 @@ describe('markdownToSlack', () => {\\n       ['‚Ä¢ parent', '  ‚Ä¢ child', '```', '- not-a-bullet inside code', '```', '‚Ä¢ after'].join('\\\\n')\\n     );\\n   });\\n+\\n+  it('converts markdown headers to bold text', () => {\\n+    const input = '# Main Title\\\\n## Subtitle\\\\n### Section';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*Main Title*\\\\n*Subtitle*\\\\n*Section*');\\n+  });\\n+\\n+  it('adds newline before h1/h2 headers when preceded by content', () => {\\n+    const input = 'Some content\\\\n## New Section\\\\nMore content';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('Some content\\\\n\\\\n*New Section*\\\\nMore content');\\n+  });\\n+\\n+  it('does not add newline before h1 if it is the first line', () => {\\n+    const input = '# First Header\\\\nContent here';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*First Header*\\\\nContent here');\\n+  });\\n+\\n+  it('ignores headers inside code blocks', () => {\\n+    const input = '```\\\\n# This is a comment\\\\n```\\\\n# Real Header';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('```\\\\n# This is a comment\\\\n```\\\\n*Real Header*');\\n+  });\\n });\\n\",\"status\":\"modified\"}],\"outputs\":{\"ask\":{\"text\":\"Can I use kubernetes labels to redirect traffic instead of service discovery?\",\"ts\":1767903775048},\"jira-context\":{\"jira_context_xml\":\"\",\"issues\":[],\"issue_count\":0,\"ts\":1767903775148},\"zendesk-context\":{\"zendesk_context_xml\":\"\",\"tickets\":\"\",\"ticket_count\":0,\"attachments\":\"\",\"ts\":1767903775310},\"log-request\":{\"issues\":[],\"logOutput\":\"‚ÑπÔ∏è **INFO**: üì• Request from user: unknown\\nChannel: unknown\\nThread: unknown\\nMessage: Can I use kubernetes labels to redirect traffic instead of service discovery?\\n\"},\"code-help-error\":{\"issues\":[]},\"customer-insights-error\":{\"issues\":[]},\"release-notes-error\":{\"issues\":[]}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.output":"{\"intent\":\"code_help\",\"topic\":\"Can I use Kubernetes labels to redirect traffic for a blue-green deployment with Tyk Operator instead of using service discovery?\",\"text\":\"{\\n  \\\"intent\\\": \\\"code_help\\\",\\n  \\\"topic\\\": \\\"Can I use Kubernetes labels to redirect traffic for a blue-green deployment with Tyk Operator instead of using service discovery?\\\"\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":2,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"code-help","visor.provider.type":"workflow"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"baefc644-d2f0-41a9-9a64-f41ad68343aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"baefc644-d2f0-41a9-9a64-f41ad68343aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"baefc644-d2f0-41a9-9a64-f41ad68343aa"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-tyk-docs","visor.provider.type":"git-checkout"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"baefc644-d2f0-41a9-9a64-f41ad68343aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"baefc644-d2f0-41a9-9a64-f41ad68343aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"baefc644-d2f0-41a9-9a64-f41ad68343aa"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ensure-code-plan","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"ensure-code-plan","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"outputs\":{\"checkout-tyk-docs\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-docs-main-00277b35\",\"ref\":\"main\",\"commit\":\"a022bb7c96ca4f48bcdac27cae3f7c4a24171ee9\",\"worktree_id\":\"TykTechnologies-tyk-docs-main-00277b35\",\"repository\":\"TykTechnologies/tyk-docs\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/41b88553-88e2-4059-ae27-a32139effc83/tyk-docs\",\"ts\":1767903798415}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"ensure-code-plan","visor.check.output":"{\"projects\":[{\"project_id\":\"tyk\",\"reason\":\"The core question is about how the Tyk Gateway discovers and resolves upstream services in a Kubernetes environment. The docs describe a service discovery mechanism based on polling an HTTP endpoint, which is a Gateway-level feature.\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The documentation shows that service discovery is configured within the API Definition, which is managed and configured via the Tyk Dashboard. The Dashboard acts as the control plane for the Gateway.\"}],\"notes\":\"The user is asking if Tyk Gateway can use Kubernetes labels for traffic redirection, as an alternative to its standard service discovery. The documentation strongly suggests that Tyk's service discovery polls a generic HTTP endpoint for a list of targets and does not have a native integration for watching Kubernetes service labels directly. The investigation should focus on confirming this behavior in the Gateway's source code and how it's configured via the Dashboard.\",\"text\":\"{\\n  \\\"projects\\\": [\\n    {\\n      \\\"project_id\\\": \\\"tyk\\\",\\n      \\\"reason\\\": \\\"The core question is about how the Tyk Gateway discovers and resolves upstream services in a Kubernetes environment. The docs describe a service discovery mechanism based on polling an HTTP endpoint, which is a Gateway-level feature.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-analytics\\\",\\n      \\\"reason\\\": \\\"The documentation shows that service discovery is configured within the API Definition, which is managed and configured via the Tyk Dashboard. The Dashboard acts as the control plane for the Gateway.\\\"\\n    }\\n  ],\\n  \\\"notes\\\": \\\"The user is asking if Tyk Gateway can use Kubernetes labels for traffic redirection, as an alternative to its standard service discovery. The documentation strongly suggests that Tyk's service discovery polls a generic HTTP endpoint for a list of targets and does not have a native integration for watching Kubernetes service labels directly. The investigation should focus on confirming this behavior in the Gateway's source code and how it's configured via the Dashboard.\\\"\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"baefc644-d2f0-41a9-9a64-f41ad68343aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"baefc644-d2f0-41a9-9a64-f41ad68343aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"baefc644-d2f0-41a9-9a64-f41ad68343aa"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"project-items","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"baefc644-d2f0-41a9-9a64-f41ad68343aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"baefc644-d2f0-41a9-9a64-f41ad68343aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"baefc644-d2f0-41a9-9a64-f41ad68343aa"}}]}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":0,"visor.foreach.total":2},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":1,"visor.foreach.total":2},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"baefc644-d2f0-41a9-9a64-f41ad68343aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"baefc644-d2f0-41a9-9a64-f41ad68343aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"baefc644-d2f0-41a9-9a64-f41ad68343aa"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"project-code-query","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"project-code-query","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"outputs\":{\"ensure-code-plan\":{\"projects\":[{\"project_id\":\"tyk\",\"reason\":\"The core question is about how the Tyk Gateway discovers and resolves upstream services in a Kubernetes environment. The docs describe a service discovery mechanism based on polling an HTTP endpoint, which is a Gateway-level feature.\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The documentation shows that service discovery is configured within the API Definition, which is managed and configured via the Tyk Dashboard. The Dashboard acts as the control plane for the Gateway.\"}],\"notes\":\"The user is asking if Tyk Gateway can use Kubernetes labels for traffic redirection, as an alternative to its standard service discovery. The documentation strongly suggests that Tyk's service discovery polls a generic HTTP endpoint for a list of targets and does not have a native integration for watching Kubernetes service labels directly. The investigation should focus on confirming this behavior in the Gateway's source code and how it's configured via the Dashboard.\",\"text\":\"{\\n  \\\"projects\\\": [\\n    {\\n      \\\"project_id\\\": \\\"tyk\\\",\\n      \\\"reason\\\": \\\"The core question is about how the Tyk Gateway discovers and resolves upstream services in a Kubernetes environment. The docs describe a service discovery mechanism based on polling an HTTP endpoint, which is a Gateway-level feature.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-analytics\\\",\\n      \\\"reason\\\": \\\"The documentation shows that service discovery is configured within the API Definition, which is managed and configured via the Tyk Dashboard. The Dashboard acts as the control plane for the Gateway.\\\"\\n    }\\n  ],\\n  \\\"notes\\\": \\\"The user is asking if Tyk Gateway can use Kubernetes labels for traffic redirection, as an alternative to its standard service discovery. The documentation strongly suggests that Tyk's service discovery polls a generic HTTP endpoint for a list of targets and does not have a native integration for watching Kubernetes service labels directly. The investigation should focus on confirming this behavior in the Gateway's source code and how it's configured via the Dashboard.\\\"\\n}\",\"ts\":1767903817995},\"project-items\":[{\"project_id\":\"tyk\",\"reason\":\"The core question is about how the Tyk Gateway discovers and resolves upstream services in a Kubernetes environment. The docs describe a service discovery mechanism based on polling an HTTP endpoint, which is a Gateway-level feature.\",\"repository\":\"TykTechnologies/tyk\",\"description\":\"Tyk Gateway\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The documentation shows that service discovery is configured within the API Definition, which is managed and configured via the Tyk Dashboard. The Dashboard acts as the control plane for the Gateway.\",\"repository\":\"TykTechnologies/tyk-analytics\",\"description\":\"Tyk Dashboard backend\"}],\"checkout-projects\":{\"issues\":[],\"isForEach\":true,\"forEachItems\":[{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-HEAD-59399fd3\",\"ref\":\"HEAD\",\"commit\":\"bb67e9e7f9382a09ff152c6942d00023c463feef\",\"worktree_id\":\"TykTechnologies-tyk-HEAD-59399fd3\",\"repository\":\"TykTechnologies/tyk\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/41b88553-88e2-4059-ae27-a32139effc83/tyk\"},{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-analytics-HEAD-5a54c529\",\"ref\":\"HEAD\",\"commit\":\"b7ed8000a93e1284b28100938363a312d2d1fad1\",\"worktree_id\":\"TykTechnologies-tyk-analytics-HEAD-5a54c529\",\"repository\":\"TykTechnologies/tyk-analytics\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/41b88553-88e2-4059-ae27-a32139effc83/tyk-analytics\"}],\"forEachItemResults\":[{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-HEAD-59399fd3\",\"ref\":\"HEAD\",\"commit\":\"bb67e9e7f9382a09ff152c6942d00023c463feef\",\"worktree_id\":\"TykTechnologies-tyk-HEAD-59399fd3\",\"repository\":\"TykTechnologies/tyk\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/41b88553-88e2-4059-ae27-a32139effc83/tyk\"}},{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-analytics-HEAD-5a54c529\",\"ref\":\"HEAD\",\"commit\":\"b7ed8000a93e1284b28100938363a312d2d1fad1\",\"worktree_id\":\"TykTechnologies-tyk-analytics-HEAD-5a54c529\",\"repository\":\"TykTechnologies/tyk-analytics\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/41b88553-88e2-4059-ae27-a32139effc83/tyk-analytics\"}}]},\"checkout-tyk-docs\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-docs-main-00277b35\",\"ref\":\"main\",\"commit\":\"a022bb7c96ca4f48bcdac27cae3f7c4a24171ee9\",\"worktree_id\":\"TykTechnologies-tyk-docs-main-00277b35\",\"repository\":\"TykTechnologies/tyk-docs\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/41b88553-88e2-4059-ae27-a32139effc83/tyk-docs\",\"ts\":1767903798415},\"project-items-raw\":[{\"project_id\":\"tyk\",\"reason\":\"The core question is about how the Tyk Gateway discovers and resolves upstream services in a Kubernetes environment. The docs describe a service discovery mechanism based on polling an HTTP endpoint, which is a Gateway-level feature.\",\"repository\":\"TykTechnologies/tyk\",\"description\":\"Tyk Gateway\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The documentation shows that service discovery is configured within the API Definition, which is managed and configured via the Tyk Dashboard. The Dashboard acts as the control plane for the Gateway.\",\"repository\":\"TykTechnologies/tyk-analytics\",\"description\":\"Tyk Dashboard backend\"}]},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"project-code-query","visor.check.output":"{\"answer\":{\"text\":\"No, you cannot use Kubernetes labels directly within the Tyk Gateway to redirect traffic. The Gateway does not have a native integration with the Kubernetes API to watch for label changes on services or pods.\\n\\nThe investigation of the Tyk Gateway's source code confirms that it does not include any Kubernetes client libraries, which would be necessary for such an integration. The documentation also supports this, showing that the intended way to integrate with Kubernetes is by using a Kubernetes service's DNS name (e.g., `http://my-service.my-namespace.svc.cluster.local`) as the `proxy.target_url` in the API definition.\\n\\nIn this standard setup, Tyk relies on Kubernetes' own internal DNS and service discovery. When you point Tyk to a Kubernetes service, it's Kubernetes that uses labels and selectors to route traffic to the correct pods. Tyk itself is simply resolving a DNS name and is unaware of the underlying labels.\\n\\nIf your goal is to switch traffic between different sets of pods (e.g., for a blue-green deployment) without modifying the `ApiDefinition`, the recommended approach is to use Tyk's service discovery feature. You would need to create a small, custom HTTP endpoint that can be queried by the Tyk Gateway. This endpoint would be responsible for querying the Kubernetes API, finding pods with a specific label (e.g., `live=true`), and returning their IP addresses to the Gateway. By changing the labels on your pods, you can control which upstreams the Gateway discovers, thus redirecting traffic without a configuration reload.\"},\"text\":\"{\\n  \\\"answer\\\": {\\n    \\\"text\\\": \\\"No, you cannot use Kubernetes labels directly within the Tyk Gateway to redirect traffic. The Gateway does not have a native integration with the Kubernetes API to watch for label changes on services or pods.\\\\n\\\\nThe investigation of the Tyk Gateway's source code confirms that it does not include any Kubernetes client libraries, which would be necessary for such an integration. The documentation also supports this, showing that the intended way to integrate with Kubernetes is by using a Kubernetes service's DNS name (e.g., `http://my-service.my-namespace.svc.cluster.local`) as the `proxy.target_url` in the API definition.\\\\n\\\\nIn this standard setup, Tyk relies on Kubernetes' own internal DNS and service discovery. When you point Tyk to a Kubernetes service, it's Kubernetes that uses labels and selectors to route traffic to the correct pods. Tyk itself is simply resolving a DNS name and is unaware of the underlying labels.\\\\n\\\\nIf your goal is to switch traffic between different sets of pods (e.g., for a blue-green deployment) without modifying the `ApiDefinition`, the recommended approach is to use Tyk's service discovery feature. You would need to create a small, custom HTTP endpoint that can be queried by the Tyk Gateway. This endpoint would be responsible for querying the Kubernetes API, finding pods with a specific label (e.g., `live=true`), and returning their IP addresses to the Gateway. By changing the labels on your pods, you can control which upstreams the Gateway discovers, thus redirecting traffic without a configuration reload.\\\"\\n  }\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"baefc644-d2f0-41a9-9a64-f41ad68343aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"baefc644-d2f0-41a9-9a64-f41ad68343aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"baefc644-d2f0-41a9-9a64-f41ad68343aa"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":2,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"code-help-reply","visor.provider.type":"log"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":3,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":3,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":3,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":3,"session_id":"41b88553-88e2-4059-ae27-a32139effc83"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"jira-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"zendesk-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"log-request","visor.provider.type":"log"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"a01e1de2-f5dd-44eb-bb02-fe21df7c3f25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"760ac094-7231-447d-995d-25f8235f5c14"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"a01e1de2-f5dd-44eb-bb02-fe21df7c3f25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"760ac094-7231-447d-995d-25f8235f5c14"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"a01e1de2-f5dd-44eb-bb02-fe21df7c3f25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"760ac094-7231-447d-995d-25f8235f5c14"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-keys","visor.provider.type":"script"}}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-ids","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"history\":{}},\"outputs_history\":{},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"include_comments\":true,\"max_tickets\":5,\"download_assets\":true,\"cache_dir\":\".cache/zendesk\",\"text\":\"Hey <@U09T5KRLMPU> I'm using the Tyk Operator to manage multiple environments in my kubernetes cluster using OperatorContext.  I want to be able to use a blue-green deployment approach to upgrade one of the environments.  My green environment should have all of the same APIDefinition and SecurityPolicy CRDs as the blue environment.  I want to be able to validate the green environment before cutting traffic over to it.  What's the best way to go about achieving this?\\nHey  I'm using the Tyk Operator to manage multiple environments in my kubernetes cluster using OperatorContext.  I want to be able to use a blue-green deployment approach to upgrade one of the environments.  My green environment should have all of the same APIDefinition and SecurityPolicy CRDs as the blue environment.  I want to be able to validate the green environment before cutting traffic over to it.  What's the best way to go about achieving this?\\n\"},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.output":"{\"data\":[],\"count\":0}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"a01e1de2-f5dd-44eb-bb02-fe21df7c3f25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"a01e1de2-f5dd-44eb-bb02-fe21df7c3f25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"a01e1de2-f5dd-44eb-bb02-fe21df7c3f25"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"no-issues-fallback","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"no-issues-fallback","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"extract-keys\":{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767904440069},\"fetch-issues\":{\"issues\":[]},\"history\":{\"extract-keys\":[{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767904440069}],\"fetch-issues\":[{\"issues\":[]}]}},\"outputs_history\":{\"extract-keys\":[{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767904440069}],\"fetch-issues\":[{\"issues\":[]}]},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"text\":\"Hey <@U09T5KRLMPU> I'm using the Tyk Operator to manage multiple environments in my kubernetes cluster using OperatorContext.  I want to be able to use a blue-green deployment approach to upgrade one of the environments.  My green environment should have all of the same APIDefinition and SecurityPolicy CRDs as the blue environment.  I want to be able to validate the green environment before cutting traffic over to it.  What's the best way to go about achieving this?\\nHey  I'm using the Tyk Operator to manage multiple environments in my kubernetes cluster using OperatorContext.  I want to be able to use a blue-green deployment approach to upgrade one of the environments.  My green environment should have all of the same APIDefinition and SecurityPolicy CRDs as the blue environment.  I want to be able to validate the green environment before cutting traffic over to it.  What's the best way to go about achieving this?\\n\",\"jql\":\"\",\"custom_field_aliases\":{},\"include_comments\":true,\"max_issues\":3,\"ticket_prefixes\":[\"TT\",\"DX\"]},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.check","attributes":{"visor.check.id":"no-issues-fallback","visor.check.output":"\"<jira_context><issue_count>0</issue_count><message>No Jira issue keys found in the provided text</message></jira_context>\""},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"a01e1de2-f5dd-44eb-bb02-fe21df7c3f25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"a01e1de2-f5dd-44eb-bb02-fe21df7c3f25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"a01e1de2-f5dd-44eb-bb02-fe21df7c3f25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"a01e1de2-f5dd-44eb-bb02-fe21df7c3f25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"a01e1de2-f5dd-44eb-bb02-fe21df7c3f25"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"760ac094-7231-447d-995d-25f8235f5c14"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"760ac094-7231-447d-995d-25f8235f5c14"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"760ac094-7231-447d-995d-25f8235f5c14"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"no-tickets-fallback","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"no-tickets-fallback","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"extract-ids\":{\"data\":[],\"count\":0,\"ts\":1767904440096},\"fetch-tickets\":{\"issues\":[]},\"history\":{\"extract-ids\":[{\"data\":[],\"count\":0,\"ts\":1767904440096}],\"fetch-tickets\":[{\"issues\":[]}]}},\"outputs_history\":{\"extract-ids\":[{\"data\":[],\"count\":0,\"ts\":1767904440096}],\"fetch-tickets\":[{\"issues\":[]}]},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"include_comments\":true,\"max_tickets\":5,\"download_assets\":true,\"cache_dir\":\".cache/zendesk\",\"text\":\"Hey <@U09T5KRLMPU> I'm using the Tyk Operator to manage multiple environments in my kubernetes cluster using OperatorContext.  I want to be able to use a blue-green deployment approach to upgrade one of the environments.  My green environment should have all of the same APIDefinition and SecurityPolicy CRDs as the blue environment.  I want to be able to validate the green environment before cutting traffic over to it.  What's the best way to go about achieving this?\\nHey  I'm using the Tyk Operator to manage multiple environments in my kubernetes cluster using OperatorContext.  I want to be able to use a blue-green deployment approach to upgrade one of the environments.  My green environment should have all of the same APIDefinition and SecurityPolicy CRDs as the blue environment.  I want to be able to validate the green environment before cutting traffic over to it.  What's the best way to go about achieving this?\\n\"},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.check","attributes":{"visor.check.id":"no-tickets-fallback","visor.check.output":"\"<zendesk_context><ticket_count>0</ticket_count><message>No Zendesk ticket IDs found in the provided text</message></zendesk_context>\""},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"760ac094-7231-447d-995d-25f8235f5c14"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"760ac094-7231-447d-995d-25f8235f5c14"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"760ac094-7231-447d-995d-25f8235f5c14"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"760ac094-7231-447d-995d-25f8235f5c14"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"760ac094-7231-447d-995d-25f8235f5c14"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"760ac094-7231-447d-995d-25f8235f5c14"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"760ac094-7231-447d-995d-25f8235f5c14"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"760ac094-7231-447d-995d-25f8235f5c14"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"760ac094-7231-447d-995d-25f8235f5c14"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"760ac094-7231-447d-995d-25f8235f5c14"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"760ac094-7231-447d-995d-25f8235f5c14"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"route-intent","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.input.context":"{\"pr\":{\"number\":0,\"title\":\"Local Analysis: feat/slack-frontend-v2 (72 modified)\",\"author\":\"Leonid Bugaev\",\"branch\":\"feat/slack-frontend-v2\",\"base\":\"main\"},\"files\":[{\"filename\":\"CLAUDE.md\",\"additions\":14,\"deletions\":1,\"changes\":15,\"patch\":\"diff --git a/CLAUDE.md b/CLAUDE.md\\nindex 009271f9..ae68c57e 100644\\n--- a/CLAUDE.md\\n+++ b/CLAUDE.md\\n@@ -150,4 +150,17 @@ Configuration supports:\\n    - Safe JSON parsing: `try { JSON.parse(output) } catch(e) { log(\\\"Error:\\\", e) }`\\n    - Validate structure: `log(\\\"Is array?\\\", Array.isArray(outputs[\\\"check-name\\\"]));`\\n \\n+6. **Tracing with OTel/Jaeger**:\\n+   - Enable telemetry: `VISOR_TELEMETRY_ENABLED=true`, `VISOR_TELEMETRY_SINK=otlp`,\\n+     `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://localhost:4318/v1/traces`\\n+   - Root span: `visor.run` (one per CLI/Slack execution)\\n+   - State spans: `engine.state.*` with `wave`, `wave_kind`, `session_id`\\n+   - Check spans: `visor.check.<checkId>` with `visor.check.id`, `visor.check.type`,\\n+     `visor.foreach.index` (for map fanout)\\n+   - Routing decisions: `visor.routing` events attached to the active state span; fields\\n+     include `trigger`, `action`, `source`, `target`, `scope`, `goto_event` (repeats\\n+     across waves show routing loops)\\n+   - Wave visibility: `engine.state.level_dispatch` includes `level_size` and\\n+     `level_checks_preview` for the planned wave\\n+\\n See `docs/debugging.md` for comprehensive debugging guide.\\n\",\"status\":\"modified\"},{\"filename\":\"README.md\",\"additions\":7,\"deletions\":1,\"changes\":8,\"patch\":\"diff --git a/README.md b/README.md\\nindex 41131861..d92f02cc 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -534,6 +534,12 @@ steps:\\n     session_mode: append    # Shares history for full conversation\\n ```\\n \\n+You can also reuse the **same check‚Äôs** session when it loops back to itself, using:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` with `session_mode: append`\\n+\\n+See the standalone example at `examples/session-reuse-self.yaml` and the detailed guide in [docs/advanced-ai.md](docs/advanced-ai.md).\\n+\\n Learn more: [docs/advanced-ai.md](docs/advanced-ai.md)\\n \\n ## üìã Schema-Template System\\n\",\"status\":\"modified\"},{\"filename\":\"docs/advanced-ai.md\",\"additions\":61,\"deletions\":2,\"changes\":63,\"patch\":\"diff --git a/docs/advanced-ai.md b/docs/advanced-ai.md\\nindex bb8e684a..879285ed 100644\\n--- a/docs/advanced-ai.md\\n+++ b/docs/advanced-ai.md\\n@@ -1,7 +1,7 @@\\n ## üß† Advanced AI Features\\n \\n ### AI Session Reuse\\n-Use `reuse_ai_session: true` on dependent checks to continue conversation context with the AI across checks. This improves follow‚Äëups and consistency.\\n+Use `reuse_ai_session` on checks to continue conversation context with the AI across steps. This improves follow‚Äëups and consistency for follow‚Äëon analysis and chat‚Äëstyle flows.\\n \\n **Session Modes:**\\n - **`clone` (default)**: Creates a copy of the conversation history. Each check gets an independent session with the same starting context. Changes made by one check don't affect others.\\n@@ -26,6 +26,65 @@ steps:\\n     depends_on: [security-remediation]\\n     reuse_ai_session: true\\n     session_mode: append  # Share history - sees full conversation\\n+\\n+#### Reusing your own session: `reuse_ai_session: self`\\n+\\n+Sometimes the step you want to loop back into is the AI step itself (e.g. Slack assistants or multi‚Äëturn internal tools). For that case you can use:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` ‚Äì the step reuses its **own** Probe session when it runs again in the same engine run.\\n+- `session_mode: append` ‚Äì makes the follow‚Äëup behave like a normal conversation turn.\\n+\\n+On the first run of the step, Visor creates a new ProbeAgent session and registers it. If routing (`on_success.goto`, `goto_js`, etc.) later jumps back to the same step within the same run, the engine:\\n+\\n+- Finds the last result for that step in the current run.\\n+- Reads the `sessionId` stored in the result.\\n+- Calls the AI provider again using `executeReviewWithSessionReuse` with that session id.\\n+\\n+Simple example (no transport wiring, just CLI/tests):\\n+\\n+```yaml\\n+version: \\\"2.0\\\"\\n+\\n+steps:\\n+  seed:\\n+    type: script\\n+    content: |\\n+      return { text: \\\"hello from seed\\\" };\\n+\\n+  convo:\\n+    type: ai\\n+    depends_on: [seed]\\n+    reuse_ai_session: self\\n+    session_mode: append\\n+    ai:\\n+      provider: mock\\n+      model: mock\\n+      disableTools: true\\n+      allowedTools: []\\n+      system_prompt: \\\"You are a tiny echo assistant.\\\"\\n+    prompt: |\\n+      Seed message: {{ outputs['seed'].text }}\\n+\\n+      Past convo outputs in this run:\\n+      {% assign hist = outputs_history['convo'] | default: empty %}\\n+      {% if hist and hist.size > 0 %}\\n+      {% for h in hist %}\\n+      - Previous reply {{ forloop.index }}.\\n+      {% endfor %}\\n+      {% else %}\\n+      - No previous replies yet.\\n+      {% endif %}\\n+    on_success:\\n+      goto_js: |\\n+        // Example: re‚Äëenter this step up to 3 times in a single run\\n+        return attempt < 3 ? 'convo' : null;\\n+```\\n+\\n+The corresponding testable example lives at:\\n+\\n+- `examples/session-reuse-self.yaml`\\n+\\n+This keeps the configuration small but shows how to wire `reuse_ai_session: self` and `session_mode: append` without touching higher‚Äëlevel workflows like `tyk-assistant`.\\n ```\\n \\n **When to use each mode:**\\n\",\"status\":\"modified\"},{\"filename\":\"docs/recipes.md\",\"additions\":149,\"deletions\":3,\"changes\":152,\"patch\":\"diff --git a/docs/recipes.md b/docs/recipes.md\\nindex 1015bc6c..1089e9a5 100644\\n--- a/docs/recipes.md\\n+++ b/docs/recipes.md\\n@@ -321,8 +321,154 @@ Tip: When you define a JSON Schema, you generally do **not** need to tell the mo\\n - Avoid noisy fallbacks like `(outputs['x']?.kind ?? '') === 'status'` when `outputs['x']?.kind === 'status'` is equivalent.\\n - These conventions apply uniformly to any provider (`ai`, `command`, `script`, `github`, `http_client`, etc).\\n \\n+### Command step best practices\\n+\\n+When using `type: command` steps:\\n+\\n+**Avoid external tool dependencies** like `jq`, `yq`, `python`, etc.:\\n+- They may not be installed in all environments (GitHub Actions, Docker, CI)\\n+- Use `transform_js` to parse and transform output instead\\n+- Keep shell commands simple: `grep`, `sed`, `awk`, `sort`, `head` are universally available\\n+\\n+```yaml\\n+# Bad - requires jq\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | jq -R -s 'split(\\\"\\\\n\\\")'\\n+  parseJson: true\\n+\\n+# Good - use transform_js for parsing\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | sort -u\\n+  transform_js: |\\n+    const lines = (output || '').trim().split('\\\\n').filter(Boolean);\\n+    return { data: lines, count: lines.length };\\n+```\\n+\\n+**Prefer line-separated output** over JSON from shell:\\n+- Simple to parse with `transform_js`\\n+- No need for `parseJson: true`\\n+- More robust across different shells/environments\\n+\\n+**Use transform_js for structured output**:\\n+- The sandbox provides `output` (command stdout as string)\\n+- Return an object with the fields you need\\n+- Works consistently across all environments\\n+\\n+### Testing workflows with `--no-mocks`\\n+\\n+The `--no-mocks` flag runs your test cases with real providers instead of injecting mock responses. This is essential for:\\n+\\n+1. **Debugging integration issues** - See actual API responses and errors\\n+2. **Capturing realistic mock data** - Get real output to copy into your test cases\\n+3. **Validating credentials** - Verify environment variables are set correctly\\n+4. **Developing new workflows** - Build tests incrementally with real data\\n+\\n+#### Basic usage\\n+\\n+```bash\\n+# Run all test cases with real providers\\n+visor test --config my-workflow.yaml --no-mocks\\n+\\n+# Run a specific test case with real providers\\n+visor test --config my-workflow.yaml --no-mocks --only \\\"my-test-case\\\"\\n+```\\n+\\n+#### Suggested mocks output\\n+\\n+When running with `--no-mocks`, Visor captures each step's output and prints it as YAML you can copy directly into your test case:\\n+\\n+```\\n+üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)\\n+   Step outputs will be captured and printed as suggested mocks\\n+\\n+... test execution ...\\n+\\n+üìã Suggested mocks (copy to your test case):\\n+mocks:\\n+  extract-keys:\\n+    data:\\n+      - PROJ-123\\n+      - DEV-456\\n+    count: 2\\n+  fetch-issues:\\n+    data:\\n+      - key: PROJ-123\\n+        summary: Fix authentication bug\\n+        status: In Progress\\n+```\\n+\\n+Copy the YAML under `mocks:` into your test case's `mocks:` section.\\n+\\n+#### Workflow for building tests\\n+\\n+1. **Start with a minimal test case** (no mocks):\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+   ```\\n+\\n+2. **Run with `--no-mocks`** to capture real outputs:\\n+   ```bash\\n+   visor test --config workflow.yaml --no-mocks --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+3. **Copy the suggested mocks** into your test case:\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+         mocks:\\n+           extract-keys:\\n+             data: [\\\"PROJ-123\\\"]\\n+             count: 1\\n+           # ... rest of captured mocks\\n+   ```\\n+\\n+4. **Add assertions** based on the real data:\\n+   ```yaml\\n+         expect:\\n+           workflow_output:\\n+             - path: issue_count\\n+               equals: 1\\n+   ```\\n+\\n+5. **Run normally** to verify mocks work:\\n+   ```bash\\n+   visor test --config workflow.yaml --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+#### Debugging with `--no-mocks`\\n+\\n+When a test fails with mocks, use `--no-mocks` to see what's actually happening:\\n+\\n+```bash\\n+# See real API responses and errors\\n+visor test --config workflow.yaml --no-mocks --only \\\"failing-test\\\"\\n+\\n+# Common issues revealed:\\n+# - Missing or expired credentials\\n+# - API endpoint changes\\n+# - Unexpected response formats\\n+# - Network/timeout issues\\n+```\\n+\\n+The real error messages and responses help identify whether the issue is with your mocks or the actual integration.\\n+\\n ### More examples\\n \\n-- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags  \\n-- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration  \\n+- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags\\n+- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration\\n - `examples/` ‚Äì MCP, Jira, and advanced configs\\n\",\"status\":\"modified\"},{\"filename\":\"package-lock.json\",\"additions\":252,\"deletions\":9,\"changes\":261,\"patch\":\"diff --git a/package-lock.json b/package-lock.json\\nindex 5fb3ad89..e5b10d27 100644\\n--- a/package-lock.json\\n+++ b/package-lock.json\\n@@ -7,6 +7,7 @@\\n     \\\"\\\": {\\n       \\\"name\\\": \\\"@probelabs/visor\\\",\\n       \\\"version\\\": \\\"0.1.42\\\",\\n+      \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"MIT\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@actions/core\\\": \\\"^1.11.1\\\",\\n@@ -16,7 +17,7 @@\\n         \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n         \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n         \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n         \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n         \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -54,6 +55,7 @@\\n         \\\"husky\\\": \\\"^9.1.7\\\",\\n         \\\"jest\\\": \\\"^30.1.3\\\",\\n         \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+        \\\"patch-package\\\": \\\"^8.0.1\\\",\\n         \\\"prettier\\\": \\\"^3.6.2\\\",\\n         \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n         \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n@@ -5942,9 +5944,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/maid\\\": {\\n-      \\\"version\\\": \\\"0.0.21\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.21.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-H3EA2tDjWsAPgXZAwfHrNoEXnHmQFS9+JGDK/YpeiF3g3FxPgnWBgtgYPgtNuETQsEXxR1RzDgQVJL2JHWRRHw==\\\",\\n+      \\\"version\\\": \\\"0.0.22\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.22.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-FYcyyP4BlVrHQmBznXFPpMbvUqFI6wbecHUUe4S+yIeDvJ8W4bP0ndZ0cMoc2p7hekWYAG4RtlohQBr8e4TKOg==\\\",\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@types/dagre\\\": \\\"^0.7.53\\\",\\n@@ -5960,9 +5962,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/probe\\\": {\\n-      \\\"version\\\": \\\"0.6.0-rc169\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc169.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-/laV3+1TSjASru8gGSvGwyjQ7jalMkFuZETl35uv37J1Hb4eG95CbKGU5NihcCrLXR44q1lKJiQGHXcAav2mBA==\\\",\\n+      \\\"version\\\": \\\"0.6.0-rc176\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc176.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-w/pAvnz2/yXb4FQXZV2QazxuW02xsI/wRg36yxZoLSG0O/hIncjF4rquZe3O+emceI2Ks5zbrA7cgQEdpwT8vA==\\\",\\n       \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n@@ -5972,7 +5974,7 @@\\n         \\\"@ai-sdk/openai\\\": \\\"^2.0.10\\\",\\n         \\\"@anthropic-ai/claude-agent-sdk\\\": \\\"^0.1.46\\\",\\n         \\\"@modelcontextprotocol/sdk\\\": \\\"^1.0.0\\\",\\n-        \\\"@probelabs/maid\\\": \\\"^0.0.21\\\",\\n+        \\\"@probelabs/maid\\\": \\\"^0.0.22\\\",\\n         \\\"adm-zip\\\": \\\"^0.5.16\\\",\\n         \\\"ai\\\": \\\"^5.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -8192,6 +8194,13 @@\\n         \\\"node\\\": \\\">= 20\\\"\\n       }\\n     },\\n+    \\\"node_modules/@yarnpkg/lockfile\\\": {\\n+      \\\"version\\\": \\\"1.1.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@yarnpkg/lockfile/-/lockfile-1.1.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-GpSwvyXOcOOlV70vbnzjj4fW5xW/FdUF6nQEt1ENy7m4ZCczi1+/buVUPAqmGfqznsORNFzUMjctTIp8a9tuCQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"BSD-2-Clause\\\"\\n+    },\\n     \\\"node_modules/accepts\\\": {\\n       \\\"version\\\": \\\"1.3.8\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/accepts/-/accepts-1.3.8.tgz\\\",\\n@@ -9180,6 +9189,25 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/call-bind\\\": {\\n+      \\\"version\\\": \\\"1.0.8\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind/-/call-bind-1.0.8.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-oKlSFMcMwpUg2ednkhQ454wfWiU/ul3CkJe/PEHcTKuiX6RpbehUiFMXu13HalGZxfUwCQzZG747YXBn1im9ww==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind-apply-helpers\\\": \\\"^1.0.0\\\",\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"set-function-length\\\": \\\"^1.2.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/call-bind-apply-helpers\\\": {\\n       \\\"version\\\": \\\"1.0.2\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind-apply-helpers/-/call-bind-apply-helpers-1.0.2.tgz\\\",\\n@@ -10613,6 +10641,24 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/define-data-property\\\": {\\n+      \\\"version\\\": \\\"1.1.4\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/define-data-property/-/define-data-property-1.1.4.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-rBMvIzlpA8v6E+SJZoo++HAYqsLrkg7MSfIinMPFhmkorw7X+dOXVJQs+QT69zGkzMyfDnIMN2Wid1+NbL3T+A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/define-lazy-prop\\\": {\\n       \\\"version\\\": \\\"3.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/define-lazy-prop/-/define-lazy-prop-3.0.0.tgz\\\",\\n@@ -11794,6 +11840,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/find-yarn-workspace-root\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/find-yarn-workspace-root/-/find-yarn-workspace-root-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-1IMnbjt4KzsQfnhnzNd8wUEgXZ44IzZaZmnLYx7D5FZlaHt2gW20Cri8Q+E/t5tIj4+epTBub+2Zxu/vNILzqQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Apache-2.0\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"micromatch\\\": \\\"^4.0.2\\\"\\n+      }\\n+    },\\n     \\\"node_modules/fix-dts-default-cjs-exports\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/fix-dts-default-cjs-exports/-/fix-dts-default-cjs-exports-1.0.1.tgz\\\",\\n@@ -12267,6 +12323,19 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/has-property-descriptors\\\": {\\n+      \\\"version\\\": \\\"1.0.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/has-property-descriptors/-/has-property-descriptors-1.0.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-55JNKuIW+vq4Ke1BjOTjM2YctQIvCT7GFzHwmfZPGo5wnrgkid0YQtnAleFSqumZm4az3n2BS+erby5ipJdgrg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/has-symbols\\\": {\\n       \\\"version\\\": \\\"1.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/has-symbols/-/has-symbols-1.1.0.tgz\\\",\\n@@ -12784,6 +12853,13 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/isarray\\\": {\\n+      \\\"version\\\": \\\"2.0.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/isarray/-/isarray-2.0.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-xHjhDr3cNBK0BzdUJSPXZntQUx/mwMS5Rw4A7lPJ90XGAO6ISP/ePDNuo0vhqOZU+UD5JoodwCAAoZQd3FeAKw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\"\\n+    },\\n     \\\"node_modules/isexe\\\": {\\n       \\\"version\\\": \\\"2.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz\\\",\\n@@ -13594,6 +13670,26 @@\\n       \\\"integrity\\\": \\\"sha512-NM8/P9n3XjXhIZn1lLhkFaACTOURQXjWhV4BA/RnOv8xvgqtqpAX9IO4mRQxSx1Rlo4tqzeqb0sOlruaOy3dug==\\\",\\n       \\\"license\\\": \\\"MIT\\\"\\n     },\\n+    \\\"node_modules/json-stable-stringify\\\": {\\n+      \\\"version\\\": \\\"1.3.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify/-/json-stable-stringify-1.3.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-qtYiSSFlwot9XHtF9bD9c7rwKjr+RecWT//ZnPvSmEjpV5mmPOCN4j8UjY5hbjNkOwZ/jQv3J6R1/pL7RwgMsg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind\\\": \\\"^1.0.8\\\",\\n+        \\\"call-bound\\\": \\\"^1.0.4\\\",\\n+        \\\"isarray\\\": \\\"^2.0.5\\\",\\n+        \\\"jsonify\\\": \\\"^0.0.1\\\",\\n+        \\\"object-keys\\\": \\\"^1.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/json-stable-stringify-without-jsonify\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify-without-jsonify/-/json-stable-stringify-without-jsonify-1.0.1.tgz\\\",\\n@@ -13633,6 +13729,16 @@\\n         \\\"graceful-fs\\\": \\\"^4.1.6\\\"\\n       }\\n     },\\n+    \\\"node_modules/jsonify\\\": {\\n+      \\\"version\\\": \\\"0.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/jsonify/-/jsonify-0.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-2/Ki0GcmuqSrgFyelQq9M05y7PS0mEwuIzrf3f1fPqkVDVRvZrPZtVSMHxdgo8Aq0sxAOb/cr2aqqA3LeWHVPg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Public Domain\\\",\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/katex\\\": {\\n       \\\"version\\\": \\\"0.16.25\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/katex/-/katex-0.16.25.tgz\\\",\\n@@ -13676,6 +13782,16 @@\\n       \\\"integrity\\\": \\\"sha512-Ls993zuzfayK269Svk9hzpeGUKob/sIgZzyHYdjQoAdQetRKpOLj+k/QQQ/6Qi0Yz65mlROrfd+Ev+1+7dz9Kw==\\\",\\n       \\\"dev\\\": true\\n     },\\n+    \\\"node_modules/klaw-sync\\\": {\\n+      \\\"version\\\": \\\"6.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/klaw-sync/-/klaw-sync-6.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-nIeuVSzdCCs6TDPTqI8w1Yre34sSq7AkZ4B3sfOBbI2CgVSB4Du4aLQijFU2+lhAFCwt9+42Hel6lQNIv6AntQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"graceful-fs\\\": \\\"^4.1.11\\\"\\n+      }\\n+    },\\n     \\\"node_modules/kolorist\\\": {\\n       \\\"version\\\": \\\"1.8.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/kolorist/-/kolorist-1.8.0.tgz\\\",\\n@@ -14785,6 +14901,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n       }\\n     },\\n+    \\\"node_modules/object-keys\\\": {\\n+      \\\"version\\\": \\\"1.1.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/object-keys/-/object-keys-1.1.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NuAESUOUMrlIXOfHKzD6bpPu3tYt3xvjNdRIQ+FeT0lNb4K8WR70CaDxhuNguS2XG+GjkyMwOzsN5ZktImfhLA==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/ohash\\\": {\\n       \\\"version\\\": \\\"2.0.11\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/ohash/-/ohash-2.0.11.tgz\\\",\\n@@ -15022,6 +15148,95 @@\\n         \\\"node\\\": \\\">= 0.8\\\"\\n       }\\n     },\\n+    \\\"node_modules/patch-package\\\": {\\n+      \\\"version\\\": \\\"8.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/patch-package/-/patch-package-8.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-VsKRIA8f5uqHQ7NGhwIna6Bx6D9s/1iXlA1hthBVBEbkq+t4kXD0HHt+rJhf/Z+Ci0F/HCB2hvn0qLdLG+Qxlw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"@yarnpkg/lockfile\\\": \\\"^1.1.0\\\",\\n+        \\\"chalk\\\": \\\"^4.1.2\\\",\\n+        \\\"ci-info\\\": \\\"^3.7.0\\\",\\n+        \\\"cross-spawn\\\": \\\"^7.0.3\\\",\\n+        \\\"find-yarn-workspace-root\\\": \\\"^2.0.0\\\",\\n+        \\\"fs-extra\\\": \\\"^10.0.0\\\",\\n+        \\\"json-stable-stringify\\\": \\\"^1.0.2\\\",\\n+        \\\"klaw-sync\\\": \\\"^6.0.0\\\",\\n+        \\\"minimist\\\": \\\"^1.2.6\\\",\\n+        \\\"open\\\": \\\"^7.4.2\\\",\\n+        \\\"semver\\\": \\\"^7.5.3\\\",\\n+        \\\"slash\\\": \\\"^2.0.0\\\",\\n+        \\\"tmp\\\": \\\"^0.2.4\\\",\\n+        \\\"yaml\\\": \\\"^2.2.2\\\"\\n+      },\\n+      \\\"bin\\\": {\\n+        \\\"patch-package\\\": \\\"index.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14\\\",\\n+        \\\"npm\\\": \\\">5\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/ci-info\\\": {\\n+      \\\"version\\\": \\\"3.9.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/ci-info/-/ci-info-3.9.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NIxF55hv4nSqQswkAeiOi1r83xy8JldOFDTWiug55KBu9Jnblncd2U6ViHmYgHf01TPZS77NJBhBMKdWj9HQMQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"funding\\\": [\\n+        {\\n+          \\\"type\\\": \\\"github\\\",\\n+          \\\"url\\\": \\\"https://github.com/sponsors/sibiraj-s\\\"\\n+        }\\n+      ],\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/is-docker\\\": {\\n+      \\\"version\\\": \\\"2.2.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/is-docker/-/is-docker-2.2.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-F+i2BKsFrH66iaUFc0woD8sLy8getkwTwtOBjvs56Cx4CgJDeKQeqfz8wAYiSb8JOprWhHH5p77PbmYCvvUuXQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"bin\\\": {\\n+        \\\"is-docker\\\": \\\"cli.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/open\\\": {\\n+      \\\"version\\\": \\\"7.4.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/open/-/open-7.4.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-MVHddDVweXZF3awtlAS+6pgKLlm/JgxZ90+/NBurBoQctVOOB/zDdVjcyPzQ+0laDGbsWgrRkflI65sQeOgT9Q==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"is-docker\\\": \\\"^2.0.0\\\",\\n+        \\\"is-wsl\\\": \\\"^2.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/slash\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/slash/-/slash-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-ZYKh3Wh2z1PpEXWr0MpSBZ0V6mZHAQfYevttO11c51CaWjGTaadiKZ+wVt1PbMlDV5qhMFslpZCemhwOK7C89A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=6\\\"\\n+      }\\n+    },\\n     \\\"node_modules/path-data-parser\\\": {\\n       \\\"version\\\": \\\"0.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/path-data-parser/-/path-data-parser-0.1.0.tgz\\\",\\n@@ -16473,6 +16688,24 @@\\n         \\\"node\\\": \\\">= 0.8.0\\\"\\n       }\\n     },\\n+    \\\"node_modules/set-function-length\\\": {\\n+      \\\"version\\\": \\\"1.2.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/set-function-length/-/set-function-length-1.2.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-pgRc4hJ4/sNjWCSS9AmnS40x3bNMDTknHgL5UaMBTMyJnU90EgWh1Rz+MC9eFu4BuN/UwZjKQuY/1v3rM7HMfg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"define-data-property\\\": \\\"^1.1.4\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"function-bind\\\": \\\"^1.1.2\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\",\\n+        \\\"has-property-descriptors\\\": \\\"^1.0.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/setprototypeof\\\": {\\n       \\\"version\\\": \\\"1.2.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/setprototypeof/-/setprototypeof-1.2.0.tgz\\\",\\n@@ -17384,6 +17617,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/tmp\\\": {\\n+      \\\"version\\\": \\\"0.2.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/tmp/-/tmp-0.2.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-voyz6MApa1rQGUxT3E+BK7/ROe8itEx7vD8/HEvt4xwXucvQ5G5oeEiHkmHZJuBO21RpOf+YYm9MOivj709jow==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14.14\\\"\\n+      }\\n+    },\\n     \\\"node_modules/tmpl\\\": {\\n       \\\"version\\\": \\\"1.0.5\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/tmpl/-/tmpl-1.0.5.tgz\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"package.json\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/package.json b/package.json\\nindex f9ccbb6c..302d728f 100644\\n--- a/package.json\\n+++ b/package.json\\n@@ -49,6 +49,7 @@\\n     \\\"test:with-build\\\": \\\"npm run build:cli && jest\\\",\\n     \\\"test:yaml\\\": \\\"node dist/index.js test --progress compact\\\",\\n     \\\"prepare\\\": \\\"husky\\\",\\n+    \\\"postinstall\\\": \\\"patch-package\\\",\\n     \\\"pre-commit\\\": \\\"lint-staged\\\",\\n     \\\"deploy:site\\\": \\\"cd site && npx wrangler pages deploy . --project-name=visor-site --commit-dirty=true\\\",\\n     \\\"deploy:worker\\\": \\\"npx wrangler deploy\\\",\\n@@ -96,7 +97,7 @@\\n     \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n     \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n     \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n     \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n     \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n     \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -143,6 +144,7 @@\\n     \\\"husky\\\": \\\"^9.1.7\\\",\\n     \\\"jest\\\": \\\"^30.1.3\\\",\\n     \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+    \\\"patch-package\\\": \\\"^8.0.1\\\",\\n     \\\"prettier\\\": \\\"^3.6.2\\\",\\n     \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n     \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"src/ai-review-service.ts\",\"additions\":101,\"deletions\":151,\"changes\":252,\"patch\":\"diff --git a/src/ai-review-service.ts b/src/ai-review-service.ts\\nindex 6b9c5b2e..f18accea 100644\\n--- a/src/ai-review-service.ts\\n+++ b/src/ai-review-service.ts\\n@@ -65,6 +65,13 @@ export interface AIReviewConfig {\\n   allowBash?: boolean;\\n   // Advanced bash command execution configuration\\n   bashConfig?: import('./types/config').BashConfig;\\n+  // Optional workspace root and allowed folders for ProbeAgent.\\n+  // When provided, these are forwarded to ProbeAgent so tools like search/query\\n+  // operate inside the isolated workspace/projects instead of the Visor repo root.\\n+  path?: string;\\n+  allowedFolders?: string[];\\n+  // Completion prompt for post-completion validation/review (runs after attempt_completion)\\n+  completionPrompt?: string;\\n }\\n \\n export interface AIDebugInfo {\\n@@ -278,7 +285,7 @@ export class AIReviewService {\\n     try {\\n       const call = this.callProbeAgent(prompt, schema, debugInfo, checkName, sessionId);\\n       const timeoutMs = Math.max(0, this.config.timeout || 0);\\n-      const { response, effectiveSchema } =\\n+      const { response, effectiveSchema, sessionId: usedSessionId } =\\n         timeoutMs > 0 ? await this.withTimeout(call, timeoutMs, 'AI review') : await call;\\n       const processingTime = Date.now() - startTime;\\n \\n@@ -290,6 +297,11 @@ export class AIReviewService {\\n \\n       const result = this.parseAIResponse(response, debugInfo, effectiveSchema);\\n \\n+      // Expose the session ID used for this call so the engine can reuse it later\\n+      try {\\n+        (result as any).sessionId = usedSessionId;\\n+      } catch {}\\n+\\n       if (debugInfo) {\\n         result.debug = debugInfo;\\n       }\\n@@ -651,17 +663,34 @@ ${prContext}${slackContextXml}\\n     const prContextInfo = prInfo as PRInfo & {\\n       isPRContext?: boolean;\\n       includeCodeContext?: boolean;\\n+      slackConversation?: unknown;\\n     };\\n     const isIssue = prContextInfo.isIssue === true;\\n \\n     // Check if we should include code context (diffs)\\n     const isPRContext = prContextInfo.isPRContext === true;\\n-    // In PR context, always include diffs. Otherwise check the flag.\\n-    const includeCodeContext = isPRContext || prContextInfo.includeCodeContext !== false;\\n+    const isSlackMode = prContextInfo.slackConversation !== undefined;\\n+\\n+    // Determine whether to include code context:\\n+    // - In explicit PR context (GitHub PR events), always include diffs\\n+    // - In Slack mode, default to NO code context unless explicitly requested\\n+    // - Otherwise, include code context unless explicitly disabled\\n+    let includeCodeContext: boolean;\\n+    if (isPRContext) {\\n+      includeCodeContext = true;\\n+    } else if (isSlackMode) {\\n+      // In Slack mode, only include code context if explicitly set to true\\n+      includeCodeContext = prContextInfo.includeCodeContext === true;\\n+    } else {\\n+      // Default: include unless explicitly disabled\\n+      includeCodeContext = prContextInfo.includeCodeContext !== false;\\n+    }\\n \\n     // Log the decision for transparency (debug level)\\n     if (isPRContext) {\\n       log('üîç Including full code diffs in AI context (PR mode)');\\n+    } else if (isSlackMode && !includeCodeContext) {\\n+      log('üí¨ Slack mode: excluding code diffs (use includeCodeContext: true to enable)');\\n     } else if (!includeCodeContext) {\\n       log('üìä Including only file summary in AI context (no diffs)');\\n     } else {\\n@@ -1537,7 +1566,15 @@ ${'='.repeat(60)}\\n     debugInfo?: AIDebugInfo,\\n     _checkName?: string,\\n     providedSessionId?: string\\n-  ): Promise<{ response: string; effectiveSchema?: string }> {\\n+  ): Promise<{ response: string; effectiveSchema?: string; sessionId: string }> {\\n+    // Derive a stable session ID for this call so the engine can reuse it later\\n+    const sessionId =\\n+      providedSessionId ||\\n+      (() => {\\n+        const timestamp = new Date().toISOString();\\n+        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n+      })();\\n+\\n     // Handle mock model/provider\\n     if (this.config.model === 'mock' || this.config.provider === 'mock') {\\n       const inJest = !!process.env.JEST_WORKER_ID;\\n@@ -1545,19 +1582,17 @@ ${'='.repeat(60)}\\n       if (!inJest) {\\n         // Fast path for CLI/integration: synthesize a mock response without invoking ProbeAgent\\n         const response = await this.generateMockResponse(prompt, _checkName, schema);\\n-        return { response, effectiveSchema: typeof schema === 'object' ? 'custom' : schema };\\n+        return {\\n+          response,\\n+          effectiveSchema: typeof schema === 'object' ? 'custom' : schema,\\n+          sessionId,\\n+        };\\n       }\\n       // In unit tests, still invoke ProbeAgent so tests can assert on options (schema) passed in\\n       // Fall through to normal flow below\\n     }\\n \\n     // Create ProbeAgent instance with proper options\\n-    const sessionId =\\n-      providedSessionId ||\\n-      (() => {\\n-        const timestamp = new Date().toISOString();\\n-        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n-      })();\\n \\n     log('ü§ñ Creating ProbeAgent for AI review...');\\n     log(`üÜî Session ID: ${sessionId}`);\\n@@ -1674,6 +1709,35 @@ ${'='.repeat(60)}\\n         (options as any).bashConfig = this.config.bashConfig;\\n       }\\n \\n+      // Pass completion prompt for post-completion validation/review\\n+      if (this.config.completionPrompt !== undefined) {\\n+        (options as any).completionPrompt = this.config.completionPrompt;\\n+      }\\n+\\n+      // Propagate workspace / allowed folders to ProbeAgent so that tools\\n+      // operate inside the isolated workspace and project checkouts instead\\n+      // of the Visor repository root.\\n+      try {\\n+        const cfgAny: any = this.config as any;\\n+        const allowedFolders = cfgAny.allowedFolders as string[] | undefined;\\n+        const workspacePath =\\n+          cfgAny.workspacePath || cfgAny.path || (Array.isArray(allowedFolders) && allowedFolders[0]);\\n+        if (Array.isArray(allowedFolders) && allowedFolders.length > 0) {\\n+          (options as any).allowedFolders = allowedFolders;\\n+          if (!options.path && workspacePath) {\\n+            (options as any).path = workspacePath;\\n+          }\\n+          log(`üóÇÔ∏è ProbeAgent workspace config:`);\\n+          log(`   path (cwd): ${(options as any).path}`);\\n+          log(`   allowedFolders[0]: ${allowedFolders[0]}`);\\n+        } else if (workspacePath) {\\n+          (options as any).path = workspacePath;\\n+          log(`üóÇÔ∏è ProbeAgent path: ${workspacePath} (no allowedFolders)`);\\n+        }\\n+      } catch {\\n+        // Best-effort only; fall back to ProbeAgent defaults on error.\\n+      }\\n+\\n       // Add provider-specific options if configured\\n       if (this.config.provider) {\\n         // Map claude-code to anthropic for ProbeAgent compatibility\\n@@ -2037,7 +2101,7 @@ ${'='.repeat(60)}\\n         log(`üîß Debug: Registered AI session for potential reuse: ${sessionId}`);\\n       }\\n \\n-      return { response, effectiveSchema };\\n+      return { response, effectiveSchema, sessionId };\\n     } catch (error) {\\n       console.error('‚ùå ProbeAgent failed:', error);\\n       throw new Error(\\n@@ -2191,13 +2255,23 @@ ${'='.repeat(60)}\\n         // For other schemas (code-review, etc.), extract and parse JSON with boundary detection\\n         log('üîç Extracting JSON from AI response...');\\n \\n-        // Try direct parsing first - if AI returned pure JSON\\n+        // Sanitize response: strip BOM, zero-width chars, and other invisible characters\\n+        // that can cause JSON parsing to fail even when the text looks valid\\n+        const sanitizedResponse = response\\n+          .replace(/^\\\\uFEFF/, '') // BOM\\n+          .replace(/[\\\\u200B-\\\\u200D\\\\uFEFF\\\\u00A0]/g, '') // Zero-width chars, NBSP\\n+          .replace(/[\\\\x00-\\\\x08\\\\x0B\\\\x0C\\\\x0E-\\\\x1F\\\\x7F]/g, '') // Control chars (except \\\\t \\\\n \\\\r)\\n+          .trim();\\n+\\n+        // Try direct JSON parsing - no bracket-matching extraction\\n+        // JSON validation is offloaded to Probe agent when schema is provided\\n         try {\\n-          reviewData = JSON.parse(response.trim());\\n+          reviewData = JSON.parse(sanitizedResponse);\\n           log('‚úÖ Successfully parsed direct JSON response');\\n           if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-        } catch {\\n-          log('üîç Direct parsing failed, trying to extract JSON from response...');\\n+        } catch (parseErr) {\\n+          const errMsg = parseErr instanceof Error ? parseErr.message : String(parseErr);\\n+          log(`üîç Direct JSON parsing failed: ${errMsg}`);\\n \\n           // If the response starts with \\\"I cannot\\\" or similar, it's likely a refusal\\n           if (\\n@@ -2210,66 +2284,16 @@ ${'='.repeat(60)}\\n             };\\n           }\\n \\n-          // Try to extract JSON using improved method with proper bracket matching\\n-          const jsonString = this.extractJsonFromResponse(response);\\n-\\n-          if (jsonString) {\\n-            try {\\n-              reviewData = JSON.parse(jsonString);\\n-              log('‚úÖ Successfully parsed extracted JSON');\\n-              if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-            } catch {\\n-              log('üîß Extracted JSON parsing failed, falling back to plain text handling...');\\n-\\n-              // Check if response is plain text and doesn't contain structured data\\n-              if (!response.includes('{') && !response.includes('}')) {\\n-                log('üîß Plain text response detected, creating structured fallback...');\\n-\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              } else {\\n-                // Fallback: treat the entire response as an issue\\n-                log('üîß Creating fallback response from non-JSON content...');\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              }\\n-            }\\n-          } else {\\n-            // No JSON found at all - treat as plain text response\\n-            log('üîß No JSON found in response, treating as plain text...');\\n-            reviewData = {\\n-              issues: [\\n-                {\\n-                  file: 'AI_RESPONSE',\\n-                  line: 1,\\n-                  ruleId: 'ai/raw_response',\\n-                  message: response,\\n-                  severity: 'info',\\n-                  category: 'documentation',\\n-                },\\n-              ],\\n-            };\\n-          }\\n+          // Not valid JSON - treat entire response as text output\\n+          // This allows Probe (or other AI providers) to handle JSON validation\\n+          // and avoids false positives from bracket-matching (e.g., mermaid diagrams)\\n+          log('üîß Treating response as plain text (no JSON extraction)');\\n+          const trimmed = response.trim();\\n+          return {\\n+            issues: [],\\n+            output: { text: trimmed },\\n+            debug: debugInfo,\\n+          };\\n         }\\n       }\\n \\n@@ -2457,80 +2481,6 @@ ${'='.repeat(60)}\\n     }\\n   }\\n \\n-  /**\\n-   * Extract JSON from a response that might contain surrounding text\\n-   * Uses proper bracket matching to find valid JSON objects or arrays\\n-   */\\n-  private extractJsonFromResponse(response: string): string | null {\\n-    const text = response.trim();\\n-\\n-    // Try to find JSON objects first (higher priority)\\n-    let bestJson = this.findJsonWithBracketMatching(text, '{', '}');\\n-\\n-    // If no object found, try arrays\\n-    if (!bestJson) {\\n-      bestJson = this.findJsonWithBracketMatching(text, '[', ']');\\n-    }\\n-\\n-    return bestJson;\\n-  }\\n-\\n-  /**\\n-   * Find JSON with proper bracket matching to avoid false positives\\n-   */\\n-  private findJsonWithBracketMatching(\\n-    text: string,\\n-    openChar: string,\\n-    closeChar: string\\n-  ): string | null {\\n-    const firstIndex = text.indexOf(openChar);\\n-    if (firstIndex === -1) return null;\\n-\\n-    let depth = 0;\\n-    let inString = false;\\n-    let escaping = false;\\n-\\n-    for (let i = firstIndex; i < text.length; i++) {\\n-      const char = text[i];\\n-\\n-      if (escaping) {\\n-        escaping = false;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\\\\\\' && inString) {\\n-        escaping = true;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\"' && !escaping) {\\n-        inString = !inString;\\n-        continue;\\n-      }\\n-\\n-      if (!inString) {\\n-        if (char === openChar) {\\n-          depth++;\\n-        } else if (char === closeChar) {\\n-          depth--;\\n-          if (depth === 0) {\\n-            // Found matching closing bracket\\n-            const candidate = text.substring(firstIndex, i + 1);\\n-            try {\\n-              JSON.parse(candidate); // Validate it's actually valid JSON\\n-              return candidate;\\n-            } catch {\\n-              // This wasn't valid JSON, keep looking\\n-              break;\\n-            }\\n-          }\\n-        }\\n-      }\\n-    }\\n-\\n-    return null;\\n-  }\\n-\\n   /**\\n    * Generate mock response for testing\\n    */\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli-main.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/src/cli-main.ts b/src/cli-main.ts\\nindex 7dbef501..e671fbd8 100644\\n--- a/src/cli-main.ts\\n+++ b/src/cli-main.ts\\n@@ -158,6 +158,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n   }\\n   const only = getArg('--only');\\n   const bail = hasFlag('--bail');\\n+  const noMocks = hasFlag('--no-mocks');\\n   const listOnly = hasFlag('--list');\\n   const validateOnly = hasFlag('--validate');\\n   const progress = (getArg('--progress') as 'compact' | 'detailed' | undefined) || 'compact';\\n@@ -224,6 +225,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       const agg = await runSuites(multiFiles, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallelSuites: maxParallelSuites || Math.max(1, require('os').cpus()?.length || 2),\\n         maxParallel,\\n         promptMaxChars,\\n@@ -340,6 +342,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       runRes = await runner.runCases(tpath, suite, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallel,\\n         promptMaxChars,\\n         engineMode: 'state-machine',\\n@@ -609,6 +612,9 @@ export async function main(): Promise<void> {\\n     // Set environment variables early for proper logging in all modules\\n     process.env.VISOR_OUTPUT_FORMAT = options.output;\\n     process.env.VISOR_DEBUG = options.debug ? 'true' : 'false';\\n+    if (options.keepWorkspace) {\\n+      process.env.VISOR_KEEP_WORKSPACE = 'true';\\n+    }\\n     // Configure centralized logger\\n     configureLoggerFromCli({\\n       output: options.output,\\n@@ -716,6 +722,31 @@ export async function main(): Promise<void> {\\n       const threads = slackAny.threads || 'any';\\n       const allow = Array.isArray(slackAny.channel_allowlist) ? slackAny.channel_allowlist : [];\\n       const appToken = slackAny.app_token || process.env.SLACK_APP_TOKEN;\\n+\\n+      // Initialize telemetry for Slack mode (normally done later for CLI runs).\\n+      if ((config as any)?.telemetry) {\\n+        const t = (config as any).telemetry as {\\n+          enabled?: boolean;\\n+          sink?: 'otlp' | 'file' | 'console';\\n+          file?: { dir?: string; ndjson?: boolean };\\n+          tracing?: { auto_instrumentations?: boolean; trace_report?: { enabled?: boolean } };\\n+        };\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true' || !!t?.enabled,\\n+          sink:\\n+            (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || t?.sink || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR || t?.file?.dir, ndjson: !!t?.file?.ndjson },\\n+          autoInstrument: !!t?.tracing?.auto_instrumentations,\\n+          traceReport: !!t?.tracing?.trace_report?.enabled,\\n+        });\\n+      } else {\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true',\\n+          sink: (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR },\\n+        });\\n+      }\\n+\\n       const runner = new SlackSocketRunner(engine, config, {\\n         appToken,\\n         endpoint,\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli.ts\",\"additions\":5,\"deletions\":1,\"changes\":6,\"patch\":\"diff --git a/src/cli.ts b/src/cli.ts\\nindex 134429b2..b1c46213 100644\\n--- a/src/cli.ts\\n+++ b/src/cli.ts\\n@@ -76,6 +76,7 @@ export class CLI {\\n         parseInt(value, 10)\\n       )\\n       .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText())\\n       .exitOverride(); // Prevent automatic process.exit for better error handling\\n \\n@@ -154,6 +155,7 @@ export class CLI {\\n           parseInt(value, 10)\\n         )\\n         .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+        .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n         .allowUnknownOption(false)\\n         .allowExcessArguments(false) // Don't allow positional arguments\\n         .addHelpText('after', this.getExamplesText())\\n@@ -224,6 +226,7 @@ export class CLI {\\n         message: options.message,\\n         githubV2: false,\\n         slack: Boolean(options.slack),\\n+        keepWorkspace: Boolean(options.keepWorkspace),\\n       };\\n     } catch (error: unknown) {\\n       // Handle commander.js exit overrides for help/version ONLY\\n@@ -348,6 +351,7 @@ export class CLI {\\n       .option('--debug-port <port>', 'Port for debug server (default: 3456)', value =>\\n         parseInt(value, 10)\\n       )\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText());\\n \\n     // Get the basic help and append examples manually if addHelpText doesn't work\\n\",\"status\":\"modified\"},{\"filename\":\"src/config.ts\",\"additions\":67,\"deletions\":17,\"changes\":84,\"patch\":\"diff --git a/src/config.ts b/src/config.ts\\nindex 407efa2d..02abe9df 100644\\n--- a/src/config.ts\\n+++ b/src/config.ts\\n@@ -20,6 +20,7 @@ import { ConfigLoader, ConfigLoaderOptions } from './utils/config-loader';\\n import { ConfigMerger } from './utils/config-merger';\\n import Ajv from 'ajv';\\n import addFormats from 'ajv-formats';\\n+import { validateJsSyntax } from './utils/sandbox';\\n \\n /**\\n  * Valid event triggers for checks\\n@@ -477,16 +478,29 @@ export class ConfigManager {\\n \\n     logger.info(`Registered workflow '${workflowId}' for standalone execution`);\\n \\n-    // Create a COMPLETELY NEW visor config with ONLY the test checks\\n-    // This prevents any workflow fields from leaking into the config\\n-    const visorConfig: Partial<VisorConfig> = {\\n+    // For standalone workflow testing, use the workflow's steps as checks\\n+    // This allows the test runner to execute the workflow steps directly\\n+    const workflowSteps = workflowData.steps || {};\\n+\\n+    // Create a config that includes the workflow steps as checks,\\n+    // plus the tests section and workflow metadata for output computation\\n+    const visorConfig: Partial<VisorConfig> & { tests?: any; outputs?: any; inputs?: any } = {\\n       version: '1.0',\\n-      steps: tests,\\n-      checks: tests, // Backward compatibility\\n+      steps: workflowSteps,\\n+      checks: workflowSteps,\\n+      tests: tests, // Preserve test harness config (may be empty if stripped by test runner)\\n     };\\n \\n-    logger.debug(`Standalone workflow config has ${Object.keys(tests).length} test checks`);\\n-    logger.debug(`Test check names: ${Object.keys(tests).join(', ')}`);\\n+    // Preserve workflow metadata for output computation during tests\\n+    if (workflowData.outputs) {\\n+      visorConfig.outputs = workflowData.outputs;\\n+    }\\n+    if (workflowData.inputs) {\\n+      visorConfig.inputs = workflowData.inputs;\\n+    }\\n+\\n+    logger.debug(`Standalone workflow config has ${Object.keys(workflowSteps).length} workflow steps as checks`);\\n+    logger.debug(`Workflow step names: ${Object.keys(workflowSteps).join(', ')}`);\\n     logger.debug(`Config keys after conversion: ${Object.keys(visorConfig).join(', ')}`);\\n \\n     return visorConfig;\\n@@ -982,26 +996,29 @@ export class ConfigManager {\\n \\n     // Validate reuse_ai_session configuration\\n     if (checkConfig.reuse_ai_session !== undefined) {\\n-      const isString = typeof checkConfig.reuse_ai_session === 'string';\\n-      const isBoolean = typeof checkConfig.reuse_ai_session === 'boolean';\\n+      const reuseValue = checkConfig.reuse_ai_session as unknown;\\n+      const isString = typeof reuseValue === 'string';\\n+      const isBoolean = typeof reuseValue === 'boolean';\\n+      const isSelf = reuseValue === 'self';\\n \\n       if (!isString && !isBoolean) {\\n         errors.push({\\n           field: `checks.${checkName}.reuse_ai_session`,\\n           message: `Invalid reuse_ai_session value for \\\"${checkName}\\\": must be string (check name) or boolean`,\\n-          value: checkConfig.reuse_ai_session,\\n+          value: reuseValue,\\n         });\\n-      } else if (isString) {\\n-        // When reuse_ai_session is a string, it must refer to a valid check\\n-        const targetCheckName = checkConfig.reuse_ai_session as string;\\n+      } else if (isString && !isSelf) {\\n+        // When reuse_ai_session is a string (other than the special 'self' value),\\n+        // it must refer to a valid check name\\n+        const targetCheckName = reuseValue as string;\\n         if (!config?.checks || !config.checks[targetCheckName]) {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" references non-existent check \\\"${targetCheckName}\\\" for session reuse`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n-      } else if (checkConfig.reuse_ai_session === true) {\\n+      } else if (reuseValue === true) {\\n         // When reuse_ai_session is true, depends_on must be specified and non-empty\\n         if (\\n           !checkConfig.depends_on ||\\n@@ -1011,7 +1028,7 @@ export class ConfigManager {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" has reuse_ai_session=true but missing or empty depends_on. Session reuse requires dependency on another check.`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n       }\\n@@ -1076,6 +1093,39 @@ export class ConfigManager {\\n         });\\n       }\\n     }\\n+\\n+    // Validate JavaScript syntax in transform_js and script content\\n+    try {\\n+      // Validate transform_js if present\\n+      const transformJs = (checkConfig as any).transform_js;\\n+      if (typeof transformJs === 'string' && transformJs.trim().length > 0) {\\n+        const result = validateJsSyntax(transformJs);\\n+        if (!result.valid) {\\n+          errors.push({\\n+            field: `checks.${checkName}.transform_js`,\\n+            message: `JavaScript syntax error in \\\"${checkName}\\\" transform_js: ${result.error}`,\\n+            value: transformJs.slice(0, 100) + (transformJs.length > 100 ? '...' : ''),\\n+          });\\n+        }\\n+      }\\n+\\n+      // Validate script content if type is 'script'\\n+      if (checkConfig.type === 'script') {\\n+        const content = (checkConfig as any).content;\\n+        if (typeof content === 'string' && content.trim().length > 0) {\\n+          const result = validateJsSyntax(content);\\n+          if (!result.valid) {\\n+            errors.push({\\n+              field: `checks.${checkName}.content`,\\n+              message: `JavaScript syntax error in \\\"${checkName}\\\" script: ${result.error}`,\\n+              value: content.slice(0, 100) + (content.length > 100 ? '...' : ''),\\n+            });\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Syntax validation is best-effort; don't fail the whole config on validation errors\\n+    }\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/failure-condition-evaluator.ts\",\"additions\":25,\"deletions\":9,\"changes\":34,\"patch\":\"diff --git a/src/failure-condition-evaluator.ts b/src/failure-condition-evaluator.ts\\nindex 3b54a015..5800df0a 100644\\n--- a/src/failure-condition-evaluator.ts\\n+++ b/src/failure-condition-evaluator.ts\\n@@ -137,6 +137,8 @@ export class FailureConditionEvaluator {\\n       previousResults?: Map<string, ReviewSummary>;\\n       authorAssociation?: string;\\n       workflowInputs?: Record<string, unknown>;\\n+      /** Current step's output for guarantee evaluation */\\n+      output?: unknown;\\n     }\\n   ): Promise<boolean> {\\n     // Build context for if evaluation\\n@@ -177,10 +179,10 @@ export class FailureConditionEvaluator {\\n       // Workflow inputs (for workflows)\\n       inputs: contextData?.workflowInputs || {},\\n \\n-      // Required output property (empty for if conditions)\\n-      output: {\\n-        issues: [],\\n-      },\\n+      // Output property: use provided output for guarantee evaluation, or empty for if conditions\\n+      output: contextData?.output !== undefined && contextData.output !== null && typeof contextData.output === 'object'\\n+        ? (contextData.output as Record<string, unknown>)\\n+        : { issues: [] },\\n       // Author association (used by permission helpers)\\n       authorAssociation: contextData?.authorAssociation,\\n \\n@@ -213,10 +215,10 @@ export class FailureConditionEvaluator {\\n               m && typeof m.get === 'function' ? m.get('all_valid', 'fact-validation') : undefined;\\n             memStr = ` mem.fact-validation.all_valid=${String(v)}`;\\n           } catch {}\\n+          // Debug if-eval output (only when VISOR_DEBUG enabled)\\n+          const outputKeys = Object.keys(context.outputs || {});\\n           console.error(\\n-            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" env.ENABLE_FACT_VALIDATION=${String(\\n-              (envMap as any).ENABLE_FACT_VALIDATION\\n-            )} event=${context.event?.event_name} result=${String(res)}${memStr}`\\n+            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" result=${String(res)} outputKeys=[${outputKeys.join(',')}]`\\n           );\\n         }\\n       } catch {}\\n@@ -480,7 +482,18 @@ export class FailureConditionEvaluator {\\n \\n       // Extract context variables\\n       const output = context.output || {};\\n-      const issues = output.issues || [];\\n+      // Ensure issues is an array - it might be a JSON string from workflow outputs\\n+      let issues = output.issues || [];\\n+      if (typeof issues === 'string') {\\n+        try {\\n+          issues = JSON.parse(issues);\\n+        } catch {\\n+          issues = [];\\n+        }\\n+      }\\n+      if (!Array.isArray(issues)) {\\n+        issues = [];\\n+      }\\n \\n       // Backward compatibility: provide metadata for transition period\\n       // TODO: Remove after all configurations are updated\\n@@ -518,6 +531,7 @@ export class FailureConditionEvaluator {\\n       const event = context.event || 'manual';\\n       const env = context.env || {};\\n       const outputs = context.outputs || {};\\n+      const inputs = context.inputs || {};\\n       const debugData = context.debug || null;\\n \\n       // Get memory store and create accessor for fail_if expressions\\n@@ -555,6 +569,7 @@ export class FailureConditionEvaluator {\\n         filesCount,\\n         event,\\n         env,\\n+        inputs,\\n         // Helper functions\\n         contains,\\n         startsWith,\\n@@ -621,6 +636,7 @@ export class FailureConditionEvaluator {\\n             filesCount,\\n             event,\\n             env,\\n+            inputs,\\n             // Helpers\\n             contains,\\n             startsWith,\\n\",\"status\":\"modified\"},{\"filename\":\"src/frontends/slack-frontend.ts\",\"additions\":107,\"deletions\":6,\"changes\":113,\"patch\":\"diff --git a/src/frontends/slack-frontend.ts b/src/frontends/slack-frontend.ts\\nindex 97161a7d..41735aa2 100644\\n--- a/src/frontends/slack-frontend.ts\\n+++ b/src/frontends/slack-frontend.ts\\n@@ -1,6 +1,32 @@\\n+/**\\n+ * Slack Frontend for Visor workflows.\\n+ *\\n+ * Features:\\n+ * - Posts AI replies to Slack threads\\n+ * - Converts Markdown to Slack mrkdwn format\\n+ * - Renders mermaid diagrams to PNG and uploads as images\\n+ * - Manages üëÄ/üëç reactions for acknowledgement\\n+ * - Handles human input prompts via prompt-state\\n+ *\\n+ * Mermaid Diagram Rendering:\\n+ * - Detects ```mermaid code blocks in AI responses\\n+ * - Renders to PNG using @mermaid-js/mermaid-cli (mmdc)\\n+ * - Uploads rendered images to Slack thread\\n+ * - Replaces mermaid blocks with \\\"_(See diagram above)_\\\" placeholder\\n+ *\\n+ * Requirements for mermaid rendering:\\n+ * - Node.js and npx in PATH\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ * - On Linux: apt-get install chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ */\\n import type { Frontend, FrontendContext } from './host';\\n import { SlackClient } from '../slack/client';\\n-import { formatSlackText } from '../slack/markdown';\\n+import {\\n+  formatSlackText,\\n+  extractMermaidDiagrams,\\n+  renderMermaidToPng,\\n+  replaceMermaidBlocks,\\n+} from '../slack/markdown';\\n \\n type SlackFrontendConfig = {\\n   defaultChannel?: string;\\n@@ -284,6 +310,9 @@ export class SlackFrontend implements Frontend {\\n       const isLogChat = providerType === 'log' && checkCfg.group === 'chat';\\n       if (!isAi && !isLogChat) return;\\n \\n+      // Skip internal steps - they're intermediate processing and shouldn't post to Slack\\n+      if (checkCfg.criticality === 'internal') return;\\n+\\n       // For AI checks, only post when using simple/unstructured schemas (or none).\\n       if (isAi) {\\n         const schema = checkCfg.schema;\\n@@ -316,6 +345,13 @@ export class SlackFrontend implements Frontend {\\n         ) {\\n           text = (result as any).content.trim();\\n         }\\n+      } else if (isLogChat && typeof (result as any)?.logOutput === 'string') {\\n+        // For log-based chat checks, render the formatted log output as the\\n+        // Slack message when no structured text field is present.\\n+        const raw = (result as any).logOutput;\\n+        if (raw.trim().length > 0) {\\n+          text = raw.trim();\\n+        }\\n       } else if (isAi && showRawOutput && out !== undefined) {\\n         try {\\n           text = JSON.stringify(out, null, 2);\\n@@ -325,13 +361,78 @@ export class SlackFrontend implements Frontend {\\n       }\\n       if (!text) return;\\n \\n-      const formattedText = formatSlackText(text);\\n+      // Extract and render mermaid diagrams before posting\\n+      const diagrams = extractMermaidDiagrams(text);\\n+      let processedText = text;\\n+\\n+      if (diagrams.length > 0) {\\n+        try {\\n+          ctx.logger.info(\\n+            `[slack-frontend] found ${diagrams.length} mermaid diagram(s) to render for ${checkId}`\\n+          );\\n+        } catch {}\\n+\\n+        // Render and upload each diagram\\n+        const uploadedCount: number[] = [];\\n+        for (let i = 0; i < diagrams.length; i++) {\\n+          const diagram = diagrams[i];\\n+          try {\\n+            ctx.logger.info(`[slack-frontend] rendering mermaid diagram ${i + 1}...`);\\n+            const pngBuffer = await renderMermaidToPng(diagram.code);\\n+            if (pngBuffer) {\\n+              ctx.logger.info(\\n+                `[slack-frontend] rendered diagram ${i + 1}, size=${pngBuffer.length} bytes, uploading...`\\n+              );\\n+              const filename = `diagram-${i + 1}.png`;\\n+              const uploadResult = await slack.files.uploadV2({\\n+                content: pngBuffer,\\n+                filename,\\n+                channel,\\n+                thread_ts: threadTs,\\n+                title: `Diagram ${i + 1}`,\\n+              });\\n+              if (uploadResult.ok) {\\n+                uploadedCount.push(i);\\n+                ctx.logger.info(`[slack-frontend] uploaded mermaid diagram ${i + 1} to ${channel}`);\\n+              } else {\\n+                ctx.logger.warn(`[slack-frontend] upload failed for diagram ${i + 1}`);\\n+              }\\n+            } else {\\n+              ctx.logger.warn(\\n+                `[slack-frontend] mermaid rendering returned null for diagram ${i + 1} (mmdc failed or not installed)`\\n+              );\\n+            }\\n+          } catch (e) {\\n+            ctx.logger.warn(\\n+              `[slack-frontend] failed to render/upload mermaid diagram ${i + 1}: ${\\n+                e instanceof Error ? e.message : String(e)\\n+              }`\\n+            );\\n+          }\\n+        }\\n+\\n+        // Replace mermaid blocks with placeholder text\\n+        if (uploadedCount.length > 0) {\\n+          processedText = replaceMermaidBlocks(text, diagrams, (idx) =>\\n+            uploadedCount.includes(idx) ? '_(See diagram above)_' : '_(Diagram rendering failed)_'\\n+          );\\n+        }\\n+      }\\n+\\n+      const formattedText = formatSlackText(processedText);\\n       await slack.chat.postMessage({ channel, text: formattedText, thread_ts: threadTs });\\n+      ctx.logger.info(\\n+        `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+      );\\n+    } catch (outerErr) {\\n+      // Log errors instead of silently swallowing them\\n       try {\\n-        ctx.logger.info(\\n-          `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+        ctx.logger.warn(\\n+          `[slack-frontend] maybePostDirectReply failed for ${checkId}: ${\\n+            outerErr instanceof Error ? outerErr.message : String(outerErr)\\n+          }`\\n         );\\n       } catch {}\\n-    } catch {}\\n+    }\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/generated/config-schema.ts\",\"additions\":233,\"deletions\":13,\"changes\":246,\"patch\":\"diff --git a/src/generated/config-schema.ts b/src/generated/config-schema.ts\\nindex b3caca4f..ecab390d 100644\\n--- a/src/generated/config-schema.ts\\n+++ b/src/generated/config-schema.ts\\n@@ -56,6 +56,20 @@ export const configSchema = {\\n           },\\n           description: 'Import workflow definitions from external files or URLs',\\n         },\\n+        inputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowInput',\\n+          },\\n+          description: 'Workflow inputs (for standalone reusable workflows)',\\n+        },\\n+        outputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowOutput',\\n+          },\\n+          description: 'Workflow outputs (for standalone reusable workflows)',\\n+        },\\n         steps: {\\n           $ref: '#/definitions/Record%3Cstring%2CCheckConfig%3E',\\n           description: 'Step configurations (recommended)',\\n@@ -67,7 +81,7 @@ export const configSchema = {\\n         },\\n         output: {\\n           $ref: '#/definitions/OutputConfig',\\n-          description: 'Output configuration',\\n+          description: 'Output configuration (optional - defaults provided)',\\n         },\\n         http_server: {\\n           $ref: '#/definitions/HttpServerConfig',\\n@@ -139,8 +153,16 @@ export const configSchema = {\\n           },\\n           description: 'Optional integrations: event-driven frontends (e.g., ndjson-sink, github)',\\n         },\\n+        workspace: {\\n+          $ref: '#/definitions/WorkspaceConfig',\\n+          description: 'Workspace isolation configuration for sandboxed execution',\\n+        },\\n+        slack: {\\n+          $ref: '#/definitions/SlackConfig',\\n+          description: 'Slack configuration',\\n+        },\\n       },\\n-      required: ['output', 'version'],\\n+      required: ['version'],\\n       patternProperties: {\\n         '^x-': {},\\n       },\\n@@ -243,6 +265,63 @@ export const configSchema = {\\n         type: 'string',\\n       },\\n     },\\n+    WorkflowInput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Input parameter name',\\n+        },\\n+        schema: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'JSON Schema for the input',\\n+        },\\n+        required: {\\n+          type: 'boolean',\\n+          description: 'Whether this input is required',\\n+        },\\n+        default: {\\n+          description: 'Default value if not provided',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow input definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    WorkflowOutput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Output name',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+        value: {\\n+          type: 'string',\\n+          description: 'Value using Liquid template syntax (references step outputs)',\\n+        },\\n+        value_js: {\\n+          type: 'string',\\n+          description: 'Value using JavaScript expression (alternative to value)',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow output definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n     'Record<string,CheckConfig>': {\\n       type: 'object',\\n       additionalProperties: {\\n@@ -390,11 +469,19 @@ export const configSchema = {\\n           description: 'Timeout in seconds for command execution (default: 60)',\\n         },\\n         depends_on: {\\n-          type: 'array',\\n-          items: {\\n-            type: 'string',\\n-          },\\n-          description: 'Check IDs that this check depends on (optional)',\\n+          anyOf: [\\n+            {\\n+              type: 'string',\\n+            },\\n+            {\\n+              type: 'array',\\n+              items: {\\n+                type: 'string',\\n+              },\\n+            },\\n+          ],\\n+          description:\\n+            'Check IDs that this check depends on (optional). Accepts single string or array.',\\n         },\\n         group: {\\n           type: 'string',\\n@@ -653,13 +740,93 @@ export const configSchema = {\\n           description: 'Arguments/inputs for the workflow',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Override specific step configurations in the workflow',\\n         },\\n         output_mapping: {\\n           $ref: '#/definitions/Record%3Cstring%2Cstring%3E',\\n           description: 'Map workflow outputs to check outputs',\\n         },\\n+        workflow_inputs: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'Alias for args - workflow inputs (backward compatibility)',\\n+        },\\n+        config: {\\n+          type: 'string',\\n+          description:\\n+            'Config file path - alternative to workflow ID (loads a Visor config file as workflow)',\\n+        },\\n+        workflow_overrides: {\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n+          description: 'Alias for overrides - workflow step overrides (backward compatibility)',\\n+        },\\n+        ref: {\\n+          type: 'string',\\n+          description: 'Git reference to checkout (branch, tag, commit SHA) - supports templates',\\n+        },\\n+        repository: {\\n+          type: 'string',\\n+          description: 'Repository URL or owner/repo format (defaults to current repository)',\\n+        },\\n+        token: {\\n+          type: 'string',\\n+          description: 'GitHub token for private repositories (defaults to GITHUB_TOKEN env)',\\n+        },\\n+        fetch_depth: {\\n+          type: 'number',\\n+          description: 'Number of commits to fetch (0 for full history, default: 1)',\\n+        },\\n+        fetch_tags: {\\n+          type: 'boolean',\\n+          description: 'Whether to fetch tags (default: false)',\\n+        },\\n+        submodules: {\\n+          anyOf: [\\n+            {\\n+              type: 'boolean',\\n+            },\\n+            {\\n+              type: 'string',\\n+              const: 'recursive',\\n+            },\\n+          ],\\n+          description: \\\"Checkout submodules: false, true, or 'recursive'\\\",\\n+        },\\n+        working_directory: {\\n+          type: 'string',\\n+          description: 'Working directory for the checkout (defaults to temp directory)',\\n+        },\\n+        use_worktree: {\\n+          type: 'boolean',\\n+          description: 'Use git worktree for efficient parallel checkouts (default: true)',\\n+        },\\n+        clean: {\\n+          type: 'boolean',\\n+          description: 'Clean the working directory before checkout (default: true)',\\n+        },\\n+        sparse_checkout: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description: 'Sparse checkout paths - only checkout specific directories/files',\\n+        },\\n+        lfs: {\\n+          type: 'boolean',\\n+          description: 'Enable Git LFS (Large File Storage)',\\n+        },\\n+        clone_timeout_ms: {\\n+          type: 'number',\\n+          description: 'Timeout in ms for cloning the bare repository (default: 300000 = 5 min)',\\n+        },\\n+        cleanup_on_failure: {\\n+          type: 'boolean',\\n+          description: 'Clean up worktree on failure (default: true)',\\n+        },\\n+        persist_worktree: {\\n+          type: 'boolean',\\n+          description: 'Keep worktree after workflow completion (default: false)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'Configuration for a single check',\\n@@ -794,6 +961,11 @@ export const configSchema = {\\n           $ref: '#/definitions/BashConfig',\\n           description: 'Advanced bash command execution configuration',\\n         },\\n+        completion_prompt: {\\n+          type: 'string',\\n+          description:\\n+            'Completion prompt for post-completion validation/review (runs after attempt_completion)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'AI provider configuration',\\n@@ -1223,7 +1395,7 @@ export const configSchema = {\\n           description: 'Custom output name (defaults to workflow name)',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Step overrides',\\n         },\\n         output_mapping: {\\n@@ -1238,14 +1410,14 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n-    'Record<string,Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>>':\\n+    'Record<string,Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>>':\\n       {\\n         type: 'object',\\n         additionalProperties: {\\n-          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E',\\n+          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E',\\n         },\\n       },\\n-    'Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>': {\\n+    'Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>': {\\n       type: 'object',\\n       additionalProperties: false,\\n     },\\n@@ -1811,6 +1983,54 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n+    WorkspaceConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        enabled: {\\n+          type: 'boolean',\\n+          description: 'Enable workspace isolation (default: true when config present)',\\n+        },\\n+        base_path: {\\n+          type: 'string',\\n+          description: 'Base path for workspaces (default: /tmp/visor-workspaces)',\\n+        },\\n+        cleanup_on_exit: {\\n+          type: 'boolean',\\n+          description: 'Clean up workspace on exit (default: true)',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Workspace isolation configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    SlackConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        version: {\\n+          type: 'string',\\n+          description: 'Slack API version',\\n+        },\\n+        mentions: {\\n+          type: 'string',\\n+          description: \\\"Mention handling: 'all', 'direct', etc.\\\",\\n+        },\\n+        threads: {\\n+          type: 'string',\\n+          description: \\\"Thread handling: 'required', 'optional', etc.\\\",\\n+        },\\n+        show_raw_output: {\\n+          type: 'boolean',\\n+          description: 'Show raw output in Slack responses',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Slack configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n   },\\n } as const;\\n export default configSchema;\\n\",\"status\":\"modified\"},{\"filename\":\"src/index.ts\",\"additions\":5,\"deletions\":2,\"changes\":7,\"patch\":\"diff --git a/src/index.ts b/src/index.ts\\nindex 50b89df3..9621e733 100644\\n--- a/src/index.ts\\n+++ b/src/index.ts\\n@@ -628,8 +628,11 @@ function resolveDependencies(\\n \\n     // Get dependencies for this check\\n     const checkConfig = config?.checks?.[checkId];\\n+    // Normalize depends_on to array (supports string | string[])\\n+    const rawDeps = checkConfig?.depends_on;\\n+    const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     // Expand OR groups (pipe syntax) for dependency closure discovery\\n-    const dependencies = (checkConfig?.depends_on || []).flatMap(d =>\\n+    const dependencies = depsArray.flatMap((d: string) =>\\n       typeof d === 'string' && d.includes('|')\\n         ? d\\n             .split('|')\\n\",\"status\":\"modified\"},{\"filename\":\"src/liquid-extensions.ts\",\"additions\":68,\"deletions\":1,\"changes\":69,\"patch\":\"diff --git a/src/liquid-extensions.ts b/src/liquid-extensions.ts\\nindex e9efaf68..dbf9c6ed 100644\\n--- a/src/liquid-extensions.ts\\n+++ b/src/liquid-extensions.ts\\n@@ -124,6 +124,26 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     }\\n   });\\n \\n+  // Register base64 filter for encoding strings\\n+  // Usage: {{ \\\"user:password\\\" | base64 }}\\n+  liquid.registerFilter('base64', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    return Buffer.from(str).toString('base64');\\n+  });\\n+\\n+  // Register base64_decode filter for decoding base64 strings\\n+  // Usage: {{ encoded_value | base64_decode }}\\n+  liquid.registerFilter('base64_decode', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    try {\\n+      return Buffer.from(str, 'base64').toString('utf-8');\\n+    } catch {\\n+      return '[Error: Invalid base64 string]';\\n+    }\\n+  });\\n+\\n   // Sanitize a label to allowed characters only: [A-Za-z0-9:/]\\n   liquid.registerFilter('safe_label', (value: unknown) => sanitizeLabel(value));\\n \\n@@ -137,6 +157,53 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     return s.replace(/\\\\\\\\n/g, '\\\\n').replace(/\\\\\\\\r/g, '\\\\r').replace(/\\\\\\\\t/g, '\\\\t');\\n   });\\n \\n+  // JSON escape filter - escapes a string for use inside a JSON string value\\n+  // This escapes special characters like quotes, backslashes, and control characters\\n+  // Usage: \\\"jql\\\": \\\"{{ myValue | json_escape }}\\\"\\n+  liquid.registerFilter('json_escape', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const s = String(value);\\n+    // Use JSON.stringify which handles all escaping, then strip the surrounding quotes\\n+    const jsonStr = JSON.stringify(s);\\n+    // Remove the first and last character (the quotes added by JSON.stringify)\\n+    return jsonStr.slice(1, -1);\\n+  });\\n+\\n+  // Shell escape filter - wraps value in single quotes with proper escaping\\n+  // Usage: {{ value | shell_escape }}\\n+  // Example: \\\"hello'world\\\" becomes \\\"'hello'\\\\''world'\\\"\\n+  // This is POSIX-compliant and safe for arbitrary text including mermaid diagrams\\n+  liquid.registerFilter('shell_escape', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    // Replace single quotes with: end quote, escaped quote, start quote\\n+    // Then wrap the entire thing in single quotes\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Alias for shell_escape\\n+  liquid.registerFilter('escape_shell', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Shell escape for double quotes (less safe but sometimes needed)\\n+  // Usage: {{ value | shell_escape_double }}\\n+  // Escapes: $, `, \\\\, \\\", and !\\n+  liquid.registerFilter('shell_escape_double', (value: unknown) => {\\n+    if (value == null) return '\\\"\\\"';\\n+    const s = String(value);\\n+    // Escape characters that have special meaning inside double quotes\\n+    const escaped = s\\n+      .replace(/\\\\\\\\/g, '\\\\\\\\\\\\\\\\')  // backslash first\\n+      .replace(/\\\\$/g, '\\\\\\\\$')   // dollar sign\\n+      .replace(/`/g, '\\\\\\\\`')    // backticks\\n+      .replace(/\\\"/g, '\\\\\\\\\\\"')    // double quotes\\n+      .replace(/!/g, '\\\\\\\\!');   // history expansion\\n+    return '\\\"' + escaped + '\\\"';\\n+  });\\n+\\n   // Register author permission filters (from main)\\n   // These filters check the author's permission level; detect local mode for tests\\n   const isLocal = detectLocalMode();\\n\",\"status\":\"modified\"},{\"filename\":\"src/logger.ts\",\"additions\":31,\"deletions\":2,\"changes\":33,\"patch\":\"diff --git a/src/logger.ts b/src/logger.ts\\nindex 6148b77c..aee7136e 100644\\n--- a/src/logger.ts\\n+++ b/src/logger.ts\\n@@ -85,7 +85,36 @@ class Logger {\\n       if (this.showTimestamps) {\\n         const ts = new Date().toISOString();\\n         const lvl = level ? level : undefined;\\n-        const prefix = lvl ? `[${ts}] [${lvl}]` : `[${ts}]`;\\n+\\n+        let tsToken = `[${ts}]`;\\n+        let lvlToken = lvl ? `[${lvl}]` : '';\\n+\\n+        // Add simple ANSI colour when running in a TTY and not emitting\\n+        // JSON/SARIF. Colours are intentionally minimal and only applied\\n+        // to the prefix markers, not the full line.\\n+        if (this.isTTY && !this.isJsonLike) {\\n+          const reset = '\\\\x1b[0m';\\n+          const dim = '\\\\x1b[2m';\\n+          const colours: Record<LogLevel, string> = {\\n+            silent: '',\\n+            error: '\\\\x1b[31m', // red\\n+            warn: '\\\\x1b[33m', // yellow\\n+            info: '\\\\x1b[36m', // cyan\\n+            verbose: '\\\\x1b[35m', // magenta\\n+            debug: '\\\\x1b[90m', // bright black / gray\\n+          };\\n+\\n+          tsToken = `${dim}${tsToken}${reset}`;\\n+\\n+          if (lvl) {\\n+            const colour = colours[lvl] || '';\\n+            if (colour) {\\n+              lvlToken = `${colour}${lvlToken}${reset}`;\\n+            }\\n+          }\\n+        }\\n+\\n+        const prefix = lvl ? `${tsToken} ${lvlToken}` : tsToken;\\n         process.stderr.write(`${prefix} ${msg}\\\\n`);\\n       } else {\\n         process.stderr.write(msg + '\\\\n');\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/ai-check-provider.ts\",\"additions\":100,\"deletions\":6,\"changes\":106,\"patch\":\"diff --git a/src/providers/ai-check-provider.ts b/src/providers/ai-check-provider.ts\\nindex 53f1a350..b41d6af1 100644\\n--- a/src/providers/ai-check-provider.ts\\n+++ b/src/providers/ai-check-provider.ts\\n@@ -186,7 +186,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     let promptContent: string;\\n \\n@@ -204,7 +205,8 @@ export class AICheckProvider extends CheckProvider {\\n       eventContext,\\n       dependencyResults,\\n       outputHistory,\\n-      args\\n+      args,\\n+      workflowInputs\\n     );\\n   }\\n \\n@@ -335,7 +337,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     // Build outputs_raw from -raw keys (aggregate parent values)\\n     const outputsRaw: Record<string, unknown> = {};\\n@@ -535,6 +538,8 @@ export class AICheckProvider extends CheckProvider {\\n       outputs_raw: outputsRaw,\\n       // Custom arguments from on_init 'with' directive\\n       args: args || {},\\n+      // Workflow inputs (for nested workflow steps to access parent inputs like {{ inputs.context }})\\n+      inputs: workflowInputs || {},\\n     };\\n \\n     try {\\n@@ -621,7 +626,8 @@ export class AICheckProvider extends CheckProvider {\\n         console.error(`[ai-exec] step=${String((config as any).checkName || 'unknown')}`);\\n       }\\n     } catch {}\\n-    // Extract AI configuration - only set properties that are explicitly provided\\n+    // Extract AI configuration - only set properties that are explicitly provided.\\n+    // Workspace / allowedFolders will be derived below from the execution context.\\n     const aiConfig: AIReviewConfig = {};\\n \\n     // Check-level AI configuration (ai object)\\n@@ -671,6 +677,9 @@ export class AICheckProvider extends CheckProvider {\\n       if (aiAny.bashConfig !== undefined) {\\n         aiConfig.bashConfig = aiAny.bashConfig as import('../types/config').BashConfig;\\n       }\\n+      if (aiAny.completion_prompt !== undefined) {\\n+        aiConfig.completionPrompt = aiAny.completion_prompt as string;\\n+      }\\n       if (aiAny.skip_code_context !== undefined) {\\n         // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n         (aiConfig as any).skip_code_context = aiAny.skip_code_context as boolean;\\n@@ -691,6 +700,90 @@ export class AICheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Derive workspace-aware allowedFolders for ProbeAgent when workspace\\n+    // isolation is enabled. This ensures tools like search/query operate\\n+    // inside the isolated workspace (and its project symlinks) instead of\\n+    // the Visor repository root.\\n+    // Folder names are human-readable (tyk-docs, visor2) thanks to WorkspaceManager.\\n+    try {\\n+      const ctxAny: any = sessionInfo as any;\\n+      const parentCtx = ctxAny?._parentContext;\\n+      const workspace = parentCtx?.workspace;\\n+\\n+      // Enhanced debug logging for workspace propagation diagnosis\\n+      logger.debug(`[AI Provider] Workspace detection for check '${(config as any).checkName || 'unknown'}':`);\\n+      logger.debug(`[AI Provider]   sessionInfo exists: ${!!sessionInfo}`);\\n+      logger.debug(`[AI Provider]   _parentContext exists: ${!!parentCtx}`);\\n+      logger.debug(`[AI Provider]   workspace exists: ${!!workspace}`);\\n+      if (workspace) {\\n+        logger.debug(`[AI Provider]   workspace.isEnabled exists: ${typeof workspace.isEnabled === 'function'}`);\\n+        logger.debug(`[AI Provider]   workspace.isEnabled(): ${typeof workspace.isEnabled === 'function' ? workspace.isEnabled() : 'N/A'}`);\\n+        const projectCount = typeof workspace.listProjects === 'function' ? workspace.listProjects()?.length : 'N/A';\\n+        logger.debug(`[AI Provider]   workspace.listProjects() count: ${projectCount}`);\\n+      }\\n+\\n+      if (workspace && typeof workspace.isEnabled === 'function' && workspace.isEnabled()) {\\n+        const folders: string[] = [];\\n+        let workspaceRoot: string | undefined;\\n+        let mainProjectPath: string | undefined;\\n+        try {\\n+          const info = workspace.getWorkspaceInfo?.();\\n+          if (info && typeof info.workspacePath === 'string') {\\n+            workspaceRoot = info.workspacePath;\\n+            mainProjectPath = info.mainProjectPath;\\n+            // Add workspace root\\n+            folders.push(info.workspacePath);\\n+            // Include main project path (has human-readable name like \\\"visor2\\\")\\n+            if (mainProjectPath) {\\n+              folders.push(mainProjectPath);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore workspace info errors\\n+        }\\n+        try {\\n+          const projects = workspace.listProjects?.() || [];\\n+          for (const proj of projects as any[]) {\\n+            if (proj && typeof proj.path === 'string') {\\n+              // Project paths have human-readable names (tyk-docs, not checkout-tyk-docs)\\n+              folders.push(proj.path);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore project listing errors\\n+        }\\n+        const unique = Array.from(new Set(folders.filter(p => typeof p === 'string' && p)));\\n+        if (unique.length > 0 && workspaceRoot) {\\n+          (aiConfig as any).allowedFolders = unique;\\n+          // Set path and cwd to workspace root - AI will run with cwd set to workspace\\n+          // Both are set for compatibility: path for older probe versions, cwd for rc175+\\n+          (aiConfig as any).path = workspaceRoot;\\n+          (aiConfig as any).cwd = workspaceRoot;\\n+          // Also set workspacePath for the AI to know the root\\n+          (aiConfig as any).workspacePath = workspaceRoot;\\n+          logger.debug(`[AI Provider] Workspace isolation enabled:`);\\n+          logger.debug(`[AI Provider]   workspaceRoot (cwd): ${workspaceRoot}`);\\n+          logger.debug(`[AI Provider]   allowedFolders: ${JSON.stringify(unique)}`);\\n+        }\\n+      } else if (parentCtx && typeof parentCtx.workingDirectory === 'string') {\\n+        // Fallback: when workspace is not available (or disabled), still\\n+        // constrain tools to the engine's working directory so ProbeAgent\\n+        // operates inside the same logical root as the state machine. This\\n+        // also ensures nested workflows (e.g. code-question-helper) use the\\n+        // workspace main project path once initializeWorkspace has updated\\n+        // the parent context.\\n+        if (!(aiConfig as any).allowedFolders) {\\n+          (aiConfig as any).allowedFolders = [parentCtx.workingDirectory];\\n+        }\\n+        if (!(aiConfig as any).path) {\\n+          (aiConfig as any).path = parentCtx.workingDirectory;\\n+          (aiConfig as any).cwd = parentCtx.workingDirectory;\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only; fall back to defaults on error.\\n+    }\\n+\\n     // Check-level AI model and provider (top-level properties)\\n     if (config.ai_model !== undefined) {\\n       aiConfig.model = config.ai_model as string;\\n@@ -876,7 +969,8 @@ export class AICheckProvider extends CheckProvider {\\n       ctxWithStage,\\n       _dependencyResults,\\n       (config as any).__outputHistory as Map<string, unknown[]> | undefined,\\n-      (sessionInfo as any)?.args\\n+      (sessionInfo as any)?.args,\\n+      (config as any).workflowInputs as Record<string, unknown> | undefined\\n     );\\n \\n     // Optional persona (vendor extension): ai.ai_persona or ai_persona.\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/command-check-provider.ts\",\"additions\":50,\"deletions\":16,\"changes\":66,\"patch\":\"diff --git a/src/providers/command-check-provider.ts b/src/providers/command-check-provider.ts\\nindex e0c9d634..bf7ffd43 100644\\n--- a/src/providers/command-check-provider.ts\\n+++ b/src/providers/command-check-provider.ts\\n@@ -131,7 +131,8 @@ export class CommandCheckProvider extends CheckProvider {\\n       // New: outputs_raw exposes aggregate values (e.g., full arrays for forEach parents)\\n       outputs_raw: outputsRaw,\\n       // Workflow inputs (when executing within a workflow)\\n-      inputs: context?.workflowInputs || {},\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any).workflowInputs || context?.workflowInputs || {},\\n       // Custom arguments from on_init 'with' directive\\n       args: context?.args || {},\\n       env: this.getSafeEnvironmentVariables(),\\n@@ -178,17 +179,31 @@ export class CommandCheckProvider extends CheckProvider {\\n           mock = rawMock as Record<string, unknown>;\\n         }\\n         const m = mock as { stdout?: string; stderr?: string; exit_code?: number; exit?: number };\\n-        let out: unknown = m.stdout ?? '';\\n-        try {\\n-          if (\\n-            typeof out === 'string' &&\\n-            (out.trim().startsWith('{') || out.trim().startsWith('['))\\n-          ) {\\n-            out = JSON.parse(out);\\n-          }\\n-        } catch {}\\n-        const code =\\n-          typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0;\\n+\\n+        // Check if this looks like a command mock (has stdout/stderr/exit_code) or direct data output\\n+        const isCommandMock = m.stdout !== undefined || m.stderr !== undefined ||\\n+          m.exit_code !== undefined || m.exit !== undefined;\\n+\\n+        let out: unknown;\\n+        if (isCommandMock) {\\n+          // Traditional command mock format: { stdout: \\\"...\\\", exit_code: 0 }\\n+          out = m.stdout ?? '';\\n+          try {\\n+            if (\\n+              typeof out === 'string' &&\\n+              (out.trim().startsWith('{') || out.trim().startsWith('['))\\n+            ) {\\n+              out = JSON.parse(out);\\n+            }\\n+          } catch {}\\n+        } else {\\n+          // Direct data mock format: { data: [...], count: 1 } - use as-is\\n+          out = mock;\\n+        }\\n+\\n+        const code = isCommandMock\\n+          ? (typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0)\\n+          : 0;\\n         if (code !== 0) {\\n           return {\\n             issues: [\\n@@ -231,9 +246,8 @@ export class CommandCheckProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Get timeout from config (in seconds) or use default (60 seconds)\\n-      const timeoutSeconds = (config.timeout as number) || 60;\\n-      const timeoutMs = timeoutSeconds * 1000;\\n+      // Get timeout from config (in milliseconds) or use default (60 seconds = 60000ms)\\n+      const timeoutMs = (config.timeout as number) || 60000;\\n \\n       // Normalize only the eval payload for `node -e|--eval` invocations that may contain\\n       // literal newlines due to YAML processing (\\\"\\\\n\\\" -> newline). We re-escape newlines\\n@@ -1533,6 +1547,26 @@ ${bodyWithReturn}\\n       return null;\\n     }\\n \\n+    // Require at least one issue-specific field beyond just a message-like field.\\n+    // This prevents objects like {text: \\\"...\\\"} from being treated as issues.\\n+    const hasIssueField =\\n+      data.file !== undefined ||\\n+      data.path !== undefined ||\\n+      data.filename !== undefined ||\\n+      data.line !== undefined ||\\n+      data.startLine !== undefined ||\\n+      data.lineNumber !== undefined ||\\n+      data.severity !== undefined ||\\n+      data.level !== undefined ||\\n+      data.priority !== undefined ||\\n+      data.ruleId !== undefined ||\\n+      data.rule !== undefined ||\\n+      data.category !== undefined ||\\n+      data.type !== undefined;\\n+    if (!hasIssueField) {\\n+      return null;\\n+    }\\n+\\n     const allowedSeverities = new Set(['info', 'warning', 'error', 'critical']);\\n     const severityRaw = this.toTrimmedString(data.severity || data.level || data.priority);\\n     let severity: ReviewIssue['severity'] = 'warning';\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/git-checkout-provider.ts\",\"additions\":41,\"deletions\":6,\"changes\":47,\"patch\":\"diff --git a/src/providers/git-checkout-provider.ts b/src/providers/git-checkout-provider.ts\\nindex baac0001..6d3a2852 100644\\n--- a/src/providers/git-checkout-provider.ts\\n+++ b/src/providers/git-checkout-provider.ts\\n@@ -69,6 +69,16 @@ export class GitCheckoutProvider extends CheckProvider {\\n       return false;\\n     }\\n \\n+    if (checkoutConfig.clone_timeout_ms !== undefined) {\\n+      if (\\n+        typeof checkoutConfig.clone_timeout_ms !== 'number' ||\\n+        checkoutConfig.clone_timeout_ms <= 0\\n+      ) {\\n+        logger.error('Invalid config: clone_timeout_ms must be a positive number (milliseconds)');\\n+        return false;\\n+      }\\n+    }\\n+\\n     return true;\\n   }\\n \\n@@ -91,7 +101,13 @@ export class GitCheckoutProvider extends CheckProvider {\\n       );\\n \\n       // Resolve dynamic variables\\n-      const resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      let resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      // If ref resolves to an empty string, fall back to HEAD so we rely on\\n+      // the repository's default branch. This keeps downstream git commands\\n+      // well-defined and matches existing expectations/tests.\\n+      if (!resolvedRef || resolvedRef.trim().length === 0) {\\n+        resolvedRef = 'HEAD';\\n+      }\\n       const resolvedRepository = checkoutConfig.repository\\n         ? await this.liquid.parseAndRender(checkoutConfig.repository, templateContext)\\n         : process.env.GITHUB_REPOSITORY || 'unknown/unknown';\\n@@ -118,6 +134,7 @@ export class GitCheckoutProvider extends CheckProvider {\\n           clean: checkoutConfig.clean !== false, // Default: true\\n           workflowId: (context as any)?.workflowId,\\n           fetchDepth: checkoutConfig.fetch_depth,\\n+          cloneTimeoutMs: checkoutConfig.clone_timeout_ms,\\n         }\\n       );\\n \\n@@ -134,18 +151,35 @@ export class GitCheckoutProvider extends CheckProvider {\\n \\n       // Add project to workspace if workspace isolation is enabled\\n       const workspace = (context as any)?._parentContext?.workspace;\\n+\\n+      // Enhanced debug logging for workspace project addition diagnosis\\n+      const checkName = (config as any)?.checkName || 'unknown';\\n+      logger.info(`[GitCheckout] Workspace check for '${checkName}':`);\\n+      logger.info(`[GitCheckout]   _parentContext exists: ${!!(context as any)?._parentContext}`);\\n+      logger.info(`[GitCheckout]   workspace exists: ${!!workspace}`);\\n+      logger.info(`[GitCheckout]   workspace.isEnabled(): ${workspace?.isEnabled?.() ?? 'N/A'}`);\\n+      if (workspace) {\\n+        const projectCountBefore = workspace.listProjects?.()?.length ?? 'N/A';\\n+        logger.debug(`[GitCheckout]   projects before addProject: ${projectCountBefore}`);\\n+      }\\n+\\n       if (workspace?.isEnabled()) {\\n         try {\\n+          // Don't pass checkName as description - let workspace use repo name\\n+          // This results in human-readable names like \\\"tyk-docs\\\" instead of \\\"checkout-tyk-docs\\\"\\n           const workspacePath = await workspace.addProject(\\n             resolvedRepository,\\n-            worktree.path,\\n-            checkoutConfig.checkName\\n+            worktree.path\\n           );\\n           output.workspace_path = workspacePath;\\n-          logger.debug(`Added project to workspace: ${workspacePath}`);\\n+          const projectCountAfter = workspace.listProjects?.()?.length ?? 'N/A';\\n+          logger.debug(`[GitCheckout] Added project to workspace: ${workspacePath}`);\\n+          logger.debug(`[GitCheckout]   projects after addProject: ${projectCountAfter}`);\\n         } catch (error) {\\n           logger.warn(`Failed to add project to workspace: ${error}`);\\n         }\\n+      } else {\\n+        logger.debug(`[GitCheckout] Workspace not enabled, skipping addProject`);\\n       }\\n \\n       logger.info(\\n@@ -228,7 +262,8 @@ export class GitCheckoutProvider extends CheckProvider {\\n       outputs: outputsObj,\\n       outputs_history: historyObj,\\n       env: safeEnv,\\n-      inputs: context?.workflowInputs,\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any)?.workflowInputs || context?.workflowInputs,\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/http-client-provider.ts\",\"additions\":279,\"deletions\":35,\"changes\":314,\"patch\":\"diff --git a/src/providers/http-client-provider.ts b/src/providers/http-client-provider.ts\\nindex 4d8c41be..6db81591 100644\\n--- a/src/providers/http-client-provider.ts\\n+++ b/src/providers/http-client-provider.ts\\n@@ -4,18 +4,29 @@ import { ReviewSummary } from '../reviewer';\\n import { Liquid } from 'liquidjs';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n import { EnvironmentResolver } from '../utils/env-resolver';\\n+import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n+import { buildProviderTemplateContext } from '../utils/template-context';\\n+import Sandbox from '@nyariv/sandboxjs';\\n+import { logger } from '../logger';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n \\n /**\\n  * Check provider that fetches data from HTTP endpoints\\n  */\\n export class HttpClientProvider extends CheckProvider {\\n   private liquid: Liquid;\\n+  private sandbox?: Sandbox;\\n \\n   constructor() {\\n     super();\\n     this.liquid = createExtendedLiquid();\\n   }\\n \\n+  private createSecureSandbox(): Sandbox {\\n+    return createSecureSandbox();\\n+  }\\n+\\n   getName(): string {\\n     return 'http_client';\\n   }\\n@@ -61,50 +72,124 @@ export class HttpClientProvider extends CheckProvider {\\n     const headers = (config.headers as Record<string, string>) || {};\\n     const timeout = (config.timeout as number) || 30000;\\n     const transform = config.transform as string | undefined;\\n+    const transformJs = config.transform_js as string | undefined;\\n     const bodyTemplate = config.body as string | undefined;\\n+    const outputFileTemplate = config.output_file as string | undefined;\\n+    const skipIfExists = config.skip_if_exists !== false; // Default true for caching\\n \\n-    try {\\n-      // Prepare template context for URL and body\\n-      const templateContext = {\\n-        pr: {\\n-          number: prInfo.number,\\n-          title: prInfo.title,\\n-          body: prInfo.body,\\n-          author: prInfo.author,\\n-          base: prInfo.base,\\n-          head: prInfo.head,\\n-          totalAdditions: prInfo.totalAdditions,\\n-          totalDeletions: prInfo.totalDeletions,\\n-        },\\n-        outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-        env: process.env,\\n-      };\\n+    // Track resolved URL for error messages\\n+    let resolvedUrlForErrors = url;\\n \\n-      // Render URL with template if it contains liquid syntax\\n-      let renderedUrl = url;\\n-      if (url.includes('{{') || url.includes('{%')) {\\n-        renderedUrl = await this.liquid.parseAndRender(url, templateContext);\\n+    try {\\n+      // Use shared template context builder for consistent output extraction\\n+      const templateContext = buildProviderTemplateContext(\\n+        prInfo,\\n+        dependencyResults,\\n+        undefined, // memoryStore\\n+        undefined, // outputHistory\\n+        undefined, // stageHistoryBase\\n+        { attachMemoryReadHelpers: false }\\n+      );\\n+      // Add env to context for shell-style variable resolution\\n+      (templateContext as Record<string, unknown>).env = process.env;\\n+\\n+      // First resolve shell-style environment variables (${VAR}, $VAR, ${{ env.VAR }})\\n+      let renderedUrl = String(EnvironmentResolver.resolveValue(url));\\n+      resolvedUrlForErrors = renderedUrl; // Track for error messages\\n+\\n+      // Then render Liquid templates if present\\n+      if (renderedUrl.includes('{{') || renderedUrl.includes('{%')) {\\n+        renderedUrl = await this.liquid.parseAndRender(renderedUrl, templateContext);\\n+        resolvedUrlForErrors = renderedUrl; // Update after Liquid rendering\\n       }\\n \\n       // Prepare request body if provided\\n       let requestBody: string | undefined;\\n       if (bodyTemplate) {\\n-        const renderedBody = await this.liquid.parseAndRender(bodyTemplate, templateContext);\\n-        requestBody = renderedBody;\\n+        // First resolve shell-style environment variables\\n+        let resolvedBody = String(EnvironmentResolver.resolveValue(bodyTemplate));\\n+        // Then render Liquid templates if present\\n+        if (resolvedBody.includes('{{') || resolvedBody.includes('{%')) {\\n+          resolvedBody = await this.liquid.parseAndRender(resolvedBody, templateContext);\\n+        }\\n+        requestBody = resolvedBody;\\n       }\\n \\n-      // Resolve environment variables in headers\\n-      const resolvedHeaders = EnvironmentResolver.resolveHeaders(headers);\\n+      // Resolve environment variables and Liquid templates in headers\\n+      const resolvedHeaders: Record<string, string> = {};\\n+      for (const [key, value] of Object.entries(headers)) {\\n+        let resolvedValue = String(EnvironmentResolver.resolveValue(value));\\n+        // Render Liquid templates if present\\n+        if (resolvedValue.includes('{{') || resolvedValue.includes('{%')) {\\n+          resolvedValue = await this.liquid.parseAndRender(resolvedValue, templateContext);\\n+        }\\n+        resolvedHeaders[key] = resolvedValue;\\n+        // Debug auth header (mask most of the value for security)\\n+        if (key.toLowerCase() === 'authorization') {\\n+          const maskedValue = resolvedValue.length > 20\\n+            ? `${resolvedValue.substring(0, 15)}...${resolvedValue.substring(resolvedValue.length - 5)}`\\n+            : resolvedValue;\\n+          logger.verbose(`[http_client] ${key}: ${maskedValue}`);\\n+        }\\n+      }\\n+\\n+      // Resolve output_file path if specified\\n+      let resolvedOutputFile: string | undefined;\\n+      if (outputFileTemplate) {\\n+        let outputPath = String(EnvironmentResolver.resolveValue(outputFileTemplate));\\n+        if (outputPath.includes('{{') || outputPath.includes('{%')) {\\n+          outputPath = await this.liquid.parseAndRender(outputPath, templateContext);\\n+        }\\n+        resolvedOutputFile = outputPath.trim();\\n+\\n+        // Check if file already exists (caching)\\n+        if (skipIfExists && fs.existsSync(resolvedOutputFile)) {\\n+          const stats = fs.statSync(resolvedOutputFile);\\n+          logger.verbose(`[http_client] File cached: ${resolvedOutputFile} (${stats.size} bytes)`);\\n+          return {\\n+            issues: [],\\n+            file_path: resolvedOutputFile,\\n+            size: stats.size,\\n+            cached: true,\\n+          } as unknown as ReviewSummary;\\n+        }\\n+      }\\n \\n       // Test hook: mock HTTP response for this step\\n       const stepName = (config as any).checkName || 'unknown';\\n       const mock = context?.hooks?.mockForStep?.(String(stepName));\\n-      const data =\\n-        mock !== undefined\\n-          ? mock\\n-          : await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n \\n-      // Apply transformation if specified\\n+      // If mock is provided, return it directly as the step output (with issues: [])\\n+      if (mock !== undefined) {\\n+        const mockObj = typeof mock === 'object' && mock !== null ? mock : { data: mock };\\n+        return {\\n+          issues: [],\\n+          ...mockObj,\\n+        } as unknown as ReviewSummary & { data: unknown };\\n+      }\\n+\\n+      // Debug log the request for troubleshooting\\n+      logger.verbose(`[http_client] ${method} ${renderedUrl}`);\\n+      if (requestBody) {\\n+        logger.verbose(`[http_client] Body: ${requestBody.substring(0, 500)}${requestBody.length > 500 ? '...' : ''}`);\\n+      }\\n+\\n+      // If output_file is specified, download to file instead of returning data\\n+      if (resolvedOutputFile) {\\n+        const fileResult = await this.downloadToFile(\\n+          renderedUrl,\\n+          method,\\n+          resolvedHeaders,\\n+          requestBody,\\n+          timeout,\\n+          resolvedOutputFile\\n+        );\\n+        return fileResult;\\n+      }\\n+\\n+      const data = await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n+\\n+      // Apply Liquid transformation if specified\\n       let processedData = data;\\n       if (transform) {\\n         try {\\n@@ -136,12 +221,53 @@ export class HttpClientProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Return the fetched data as a custom field for dependent checks to access\\n+      // Apply JavaScript transformation if specified\\n+      if (transformJs) {\\n+        try {\\n+          if (!this.sandbox) {\\n+            this.sandbox = this.createSecureSandbox();\\n+          }\\n+\\n+          // Create scope for JavaScript transform (scope, not context)\\n+          const jsScope: Record<string, unknown> = {\\n+            output: data,\\n+            pr: templateContext.pr,\\n+            outputs: templateContext.outputs,\\n+            env: process.env,\\n+          };\\n+\\n+          const result = compileAndRun(this.sandbox, transformJs, jsScope, {\\n+            injectLog: true,\\n+            logPrefix: 'üîç [transform_js]',\\n+            wrapFunction: true,\\n+          });\\n+          processedData = result;\\n+          logger.verbose(`‚úì Applied JavaScript transform successfully`);\\n+        } catch (error) {\\n+          logger.error(\\n+            `‚úó Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`\\n+          );\\n+          return {\\n+            issues: [\\n+              {\\n+                file: 'http_client',\\n+                line: 0,\\n+                ruleId: 'http_client/transform_js_error',\\n+                message: `Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              },\\n+            ],\\n+          };\\n+        }\\n+      }\\n+\\n+      // Return the fetched data in the standard `output` property for guarantee evaluation\\n+      // This is consistent with other providers (script, command, ai, etc.)\\n       return {\\n         issues: [],\\n-        // Add custom data field that will be passed through to dependent checks\\n-        data: processedData,\\n-      } as ReviewSummary & { data: unknown };\\n+        output: processedData,\\n+      } as unknown as ReviewSummary;\\n     } catch (error) {\\n       return {\\n         issues: [\\n@@ -149,7 +275,7 @@ export class HttpClientProvider extends CheckProvider {\\n             file: 'http_client',\\n             line: 0,\\n             ruleId: 'http_client/fetch_error',\\n-            message: `Failed to fetch from ${url}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            message: `Failed to fetch from ${resolvedUrlForErrors}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n             severity: 'error',\\n             category: 'logic',\\n           },\\n@@ -198,7 +324,14 @@ export class HttpClientProvider extends CheckProvider {\\n \\n       clearTimeout(timeoutId);\\n \\n+      logger.verbose(`[http_client] Response: ${response.status} ${response.statusText}`);\\n+\\n       if (!response.ok) {\\n+        // Log response body for debugging auth failures\\n+        try {\\n+          const errorBody = await response.text();\\n+          logger.warn(`[http_client] Error body: ${errorBody.substring(0, 500)}`);\\n+        } catch {}\\n         throw new Error(`HTTP ${response.status}: ${response.statusText}`);\\n       }\\n \\n@@ -233,6 +366,112 @@ export class HttpClientProvider extends CheckProvider {\\n     }\\n   }\\n \\n+  private async downloadToFile(\\n+    url: string,\\n+    method: string,\\n+    headers: Record<string, string>,\\n+    body: string | undefined,\\n+    timeout: number,\\n+    outputFile: string\\n+  ): Promise<ReviewSummary> {\\n+    // Check if fetch is available (Node 18+)\\n+    if (typeof fetch === 'undefined') {\\n+      throw new Error('HTTP client provider requires Node.js 18+ or node-fetch package');\\n+    }\\n+\\n+    const controller = new AbortController();\\n+    const timeoutId = setTimeout(() => controller.abort(), timeout);\\n+\\n+    try {\\n+      const requestOptions: RequestInit = {\\n+        method,\\n+        headers: { ...headers },\\n+        signal: controller.signal,\\n+      };\\n+\\n+      // Add body for non-GET requests\\n+      if (method !== 'GET' && body) {\\n+        requestOptions.body = body;\\n+        if (!headers['Content-Type'] && !headers['content-type']) {\\n+          requestOptions.headers = {\\n+            ...requestOptions.headers,\\n+            'Content-Type': 'application/json',\\n+          };\\n+        }\\n+      }\\n+\\n+      const response = await fetch(url, requestOptions);\\n+      clearTimeout(timeoutId);\\n+\\n+      if (!response.ok) {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_error',\\n+              message: `Failed to download file: HTTP ${response.status}: ${response.statusText}`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      // Create parent directory if it doesn't exist\\n+      const parentDir = path.dirname(outputFile);\\n+      if (parentDir && !fs.existsSync(parentDir)) {\\n+        fs.mkdirSync(parentDir, { recursive: true });\\n+      }\\n+\\n+      // Get the response as an ArrayBuffer and write to file\\n+      const arrayBuffer = await response.arrayBuffer();\\n+      const buffer = Buffer.from(arrayBuffer);\\n+      fs.writeFileSync(outputFile, buffer);\\n+\\n+      const contentType = response.headers.get('content-type') || 'application/octet-stream';\\n+      logger.verbose(`[http_client] Downloaded: ${outputFile} (${buffer.length} bytes)`);\\n+\\n+      return {\\n+        issues: [],\\n+        file_path: outputFile,\\n+        size: buffer.length,\\n+        content_type: contentType,\\n+        cached: false,\\n+      } as unknown as ReviewSummary;\\n+    } catch (error: unknown) {\\n+      clearTimeout(timeoutId);\\n+\\n+      if (error instanceof Error && error.name === 'AbortError') {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_timeout',\\n+              message: `Download timed out after ${timeout}ms`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      return {\\n+        issues: [\\n+          {\\n+            file: 'http_client',\\n+            line: 0,\\n+            ruleId: 'http_client/download_error',\\n+            message: `Failed to download file: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            severity: 'error',\\n+            category: 'logic',\\n+          },\\n+        ],\\n+      };\\n+    }\\n+  }\\n+\\n   getSupportedConfigKeys(): string[] {\\n     return [\\n       'type',\\n@@ -241,7 +480,10 @@ export class HttpClientProvider extends CheckProvider {\\n       'headers',\\n       'body',\\n       'transform',\\n+      'transform_js',\\n       'timeout',\\n+      'output_file',\\n+      'skip_if_exists',\\n       'depends_on',\\n       'on',\\n       'if',\\n@@ -261,6 +503,8 @@ export class HttpClientProvider extends CheckProvider {\\n       'Network access to the endpoint',\\n       'Optional: Transform template for processing response data',\\n       'Optional: Body template for POST/PUT requests',\\n+      'Optional: output_file path to download response to a file',\\n+      'Optional: skip_if_exists (default: true) to enable caching for file downloads',\\n     ];\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/log-check-provider.ts\",\"additions\":19,\"deletions\":8,\"changes\":27,\"patch\":\"diff --git a/src/providers/log-check-provider.ts b/src/providers/log-check-provider.ts\\nindex 9c6332d1..b57f632c 100644\\n--- a/src/providers/log-check-provider.ts\\n+++ b/src/providers/log-check-provider.ts\\n@@ -78,7 +78,8 @@ export class LogCheckProvider extends CheckProvider {\\n       includeDependencies,\\n       includeMetadata,\\n       config.__outputHistory as Map<string, unknown[]> | undefined,\\n-      context\\n+      context,\\n+      config\\n     );\\n \\n     // Render the log message template\\n@@ -100,12 +101,20 @@ export class LogCheckProvider extends CheckProvider {\\n     else if (level === 'debug') logger.debug(logOutput);\\n     else logger.info(logOutput);\\n \\n-    // Return with the log content as custom data for dependent checks\\n-    return {\\n+    // Return with the log content as custom data for dependent checks.\\n+    // For chat-style logs (group: chat), also expose a structured\\n+    // `output.text` field so frontends like Slack can post a clean\\n+    // human-facing message without the level prefix.\\n+    const summary: ReviewSummary & { logOutput: string; output?: unknown } = {\\n       issues: [],\\n-      // Add log output as custom field\\n       logOutput,\\n-    } as ReviewSummary & { logOutput: string };\\n+    };\\n+\\n+    if ((config as any).group === 'chat') {\\n+      (summary as any).output = { text: renderedMessage };\\n+    }\\n+\\n+    return summary;\\n   }\\n \\n   private buildTemplateContext(\\n@@ -115,7 +124,8 @@ export class LogCheckProvider extends CheckProvider {\\n     _includeDependencies: boolean = true,\\n     includeMetadata: boolean = true,\\n     outputHistory?: Map<string, unknown[]>,\\n-    executionContext?: ExecutionContext\\n+    executionContext?: ExecutionContext,\\n+    config?: CheckProviderConfig\\n   ): Record<string, unknown> {\\n     const context: Record<string, unknown> = {};\\n \\n@@ -197,7 +207,8 @@ export class LogCheckProvider extends CheckProvider {\\n     }\\n \\n     // Add workflow inputs if available\\n-    const workflowInputs = executionContext?.workflowInputs || {};\\n+    // Check config first (set by projectWorkflowToGraph), then fall back to executionContext\\n+    const workflowInputs = (config as any)?.workflowInputs || executionContext?.workflowInputs || {};\\n     logger.debug(\\n       `[LogProvider] Adding ${Object.keys(workflowInputs).length} workflow inputs to context`\\n     );\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/script-check-provider.ts\",\"additions\":37,\"deletions\":1,\"changes\":38,\"patch\":\"diff --git a/src/providers/script-check-provider.ts b/src/providers/script-check-provider.ts\\nindex 9825bb69..947dbd73 100644\\n--- a/src/providers/script-check-provider.ts\\n+++ b/src/providers/script-check-provider.ts\\n@@ -59,6 +59,16 @@ export class ScriptCheckProvider extends CheckProvider {\\n       reuseSession?: boolean;\\n     } & import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n+    // Test hook: mock output for this step (short-circuit execution)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = _sessionInfo?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        // Return mock directly as step output\\n+        return { issues: [], output: mock } as ReviewSummary & { output: unknown };\\n+      }\\n+    } catch {}\\n+\\n     const script = String(config.content || '');\\n     const memoryStore = MemoryStore.getInstance();\\n     const ctx = buildProviderTemplateContext(\\n@@ -72,10 +82,36 @@ export class ScriptCheckProvider extends CheckProvider {\\n     // Keep provider quiet by default; no step-specific debug\\n     // (historical ad-hoc logs removed to avoid hardcoding step names).\\n \\n+    // Add workflow inputs to the context\\n+    const inputs = (config as any).workflowInputs || _sessionInfo?.workflowInputs || {};\\n+    (ctx as any).inputs = inputs;\\n+\\n+    // Add environment variables to context (consistent with http-client-provider)\\n+    (ctx as any).env = process.env;\\n+\\n     // Attach synchronous memory ops consistent with memory provider\\n     const { ops, needsSave } = createSyncMemoryOps(memoryStore);\\n     (ctx as any).memory = ops as unknown as Record<string, unknown>;\\n \\n+    // Add helper functions to the context\\n+    (ctx as any).escapeXml = (str: unknown): string => {\\n+      if (str == null) return '';\\n+      return String(str)\\n+        .replace(/&/g, '&amp;')\\n+        .replace(/</g, '&lt;')\\n+        .replace(/>/g, '&gt;')\\n+        .replace(/\\\"/g, '&quot;')\\n+        .replace(/'/g, '&apos;');\\n+    };\\n+\\n+    // Add btoa/atob for base64 encoding/decoding (browser API polyfill)\\n+    (ctx as any).btoa = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'binary').toString('base64');\\n+    };\\n+    (ctx as any).atob = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'base64').toString('binary');\\n+    };\\n+\\n     // Evaluate the script in a secure sandbox (per-execution instance)\\n     const sandbox = this.createSecureSandbox();\\n     let result: unknown;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/workflow-check-provider.ts\",\"additions\":164,\"deletions\":43,\"changes\":207,\"patch\":\"diff --git a/src/providers/workflow-check-provider.ts b/src/providers/workflow-check-provider.ts\\nindex 116d4dff..681703ce 100644\\n--- a/src/providers/workflow-check-provider.ts\\n+++ b/src/providers/workflow-check-provider.ts\\n@@ -67,34 +67,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n   ): Promise<ReviewSummary> {\\n     const cfg = config as CheckProviderConfig & { workflow?: string; config?: string };\\n     const isConfigPathMode = !!cfg.config && !cfg.workflow;\\n+    const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n \\n-    // Test harness support: allow mocking workflow checks as a black box.\\n-    // If a mock is provided for this step, short-circuit nested execution and\\n-    // return a ReviewSummary with optional output field.\\n-    try {\\n-      const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n-      // test-runner passes hooks on execution context\\n-      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n-      if (mock !== undefined) {\\n-        const ms = mock as any;\\n-        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n-        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n-        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n-        const summary: ReviewSummary & { output?: unknown } = {\\n-          issues: issuesArr,\\n-          output: out,\\n-          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n-        } as any;\\n-        return summary;\\n-      }\\n-    } catch {}\\n-\\n-    // Resolve workflow definition\\n+    // Resolve workflow definition FIRST (needed for input preparation and validation)\\n     let workflow: WorkflowDefinition | undefined;\\n     let workflowId = cfg.workflow as string | undefined;\\n \\n     if (isConfigPathMode) {\\n-      const parentCwd = ((context as any)?._parentContext?.workingDirectory ||\\n+      // Use originalWorkingDirectory for config file resolution - workflow configs\\n+      // should be loaded from the original project path, not the sandbox\\n+      const parentCwd = ((context as any)?._parentContext?.originalWorkingDirectory ||\\n+        (context as any)?._parentContext?.workingDirectory ||\\n+        (context as any)?.originalWorkingDirectory ||\\n         (context as any)?.workingDirectory ||\\n         process.cwd()) as string;\\n       workflow = await this.loadWorkflowFromConfigPath(String(cfg.config), parentCwd);\\n@@ -109,9 +93,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       logger.info(`Executing workflow '${workflowId}'`);\\n     }\\n \\n-    // Prepare inputs\\n+    // Prepare inputs - do this BEFORE mock check so we can validate inputs even when mocked\\n+    // This allows tests to assert that the correct inputs were passed to workflow steps\\n     const inputs = await this.prepareInputs(workflow, config, prInfo, dependencyResults);\\n \\n+    // Capture resolved workflow inputs for testing assertions (reuse prompt capture infrastructure)\\n+    // This allows using `prompts` assertions with `provider: 'workflow'` to verify inputs\\n+    try {\\n+      // Serialize inputs to capture for assertions - specifically the context field\\n+      // which contains the rendered template with dependency outputs\\n+      const inputsCapture = Object.entries(inputs)\\n+        .map(([k, v]) => `${k}: ${typeof v === 'string' ? v : JSON.stringify(v)}`)\\n+        .join('\\\\n\\\\n');\\n+      (context as any)?.hooks?.onPromptCaptured?.({\\n+        step: String(stepName),\\n+        provider: 'workflow',\\n+        prompt: inputsCapture,\\n+      });\\n+    } catch {\\n+      // Ignore capture errors - this is only for testing\\n+    }\\n+\\n     // Validate inputs\\n     const validation = this.registry.validateInputs(workflow, inputs);\\n     if (!validation.valid) {\\n@@ -119,6 +121,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       throw new Error(`Invalid workflow inputs: ${errors}`);\\n     }\\n \\n+    // Test harness support: allow mocking workflow checks as a black box.\\n+    // If a mock is provided for this step, short-circuit nested execution and\\n+    // return a ReviewSummary with optional output field.\\n+    // NOTE: This happens AFTER input preparation/validation so tests can assert inputs are correct\\n+    try {\\n+      // test-runner passes hooks on execution context\\n+      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        const ms = mock as any;\\n+        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n+        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n+        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n+        const summary: ReviewSummary & { output?: unknown } = {\\n+          issues: issuesArr,\\n+          output: out,\\n+          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n+        } as any;\\n+        return summary;\\n+      }\\n+    } catch {}\\n+\\n     // Apply overrides to workflow steps if specified\\n     const modifiedWorkflow = this.applyOverrides(workflow, config);\\n \\n@@ -211,6 +234,80 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Extract eventContext for slack/conversation\\n+    const eventContext = config.eventContext || {};\\n+\\n+    // Debug logging for conversation context\\n+    logger.debug(`[WorkflowProvider] prepareInputs for ${workflow.id}`);\\n+    logger.debug(`[WorkflowProvider] eventContext keys: ${Object.keys(eventContext).join(', ') || 'none'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.slack: ${eventContext.slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.conversation: ${(eventContext as any).conversation ? 'present' : 'absent'}`);\\n+\\n+    // Extract slack context (if provided via eventContext.slack)\\n+    const slack = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        const slackCtx = anyCtx?.slack;\\n+        if (slackCtx && typeof slackCtx === 'object') return slackCtx;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Extract unified conversation context across transports (Slack & GitHub)\\n+    const conversation = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        if (anyCtx?.slack?.conversation) return anyCtx.slack.conversation;\\n+        if (anyCtx?.github?.conversation) return anyCtx.github.conversation;\\n+        if (anyCtx?.conversation) return anyCtx.conversation;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Debug logging for extracted context\\n+    logger.debug(`[WorkflowProvider] slack extracted: ${slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] conversation extracted: ${conversation ? 'present' : 'absent'}`);\\n+    if (conversation) {\\n+      logger.debug(`[WorkflowProvider] conversation.messages count: ${Array.isArray((conversation as any).messages) ? (conversation as any).messages.length : 0}`);\\n+    }\\n+\\n+    // Extract output history from config (passed via __outputHistory)\\n+    const outputHistory = (config as any).__outputHistory as Map<string, unknown[]> | undefined;\\n+    const outputs_history: Record<string, unknown[]> = {};\\n+    if (outputHistory) {\\n+      for (const [k, v] of outputHistory.entries()) {\\n+        outputs_history[k] = v;\\n+      }\\n+    }\\n+\\n+    // Build template context with all available data\\n+    // Extract .output from each dependency result so that outputs['step-name'].field works naturally\\n+    // (not outputs['step-name'].output.field)\\n+    const outputsMap: Record<string, unknown> = {};\\n+    logger.debug(`[WorkflowProvider] dependencyResults: ${dependencyResults ? dependencyResults.size : 'undefined'} entries`);\\n+    if (dependencyResults) {\\n+      for (const [key, result] of dependencyResults.entries()) {\\n+        // Extract the output property, or use the whole result if output is undefined\\n+        const extracted = (result as any).output ?? result;\\n+        outputsMap[key] = extracted;\\n+        // Debug: log what we extracted for each dependency\\n+        const extractedKeys = extracted && typeof extracted === 'object' ? Object.keys(extracted).join(', ') : 'not-object';\\n+        logger.debug(`[WorkflowProvider] outputs['${key}']: keys=[${extractedKeys}]`);\\n+      }\\n+    }\\n+    const templateContext = {\\n+      pr: prInfo,\\n+      outputs: outputsMap,\\n+      env: process.env,\\n+      slack,\\n+      conversation,\\n+      outputs_history,\\n+    };\\n+\\n     // Apply user-provided inputs (args)\\n     const userInputs = config.args || config.workflow_inputs; // Support both for compatibility\\n     if (userInputs) {\\n@@ -219,11 +316,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         if (typeof value === 'string') {\\n           // Check if it's a Liquid template\\n           if (value.includes('{{') || value.includes('{%')) {\\n-            inputs[key] = await this.liquid.parseAndRender(value, {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            });\\n+            inputs[key] = await this.liquid.parseAndRender(value, templateContext);\\n+            // Debug: log rendered template value for important inputs\\n+            if (key === 'text' || key === 'question' || key === 'context') {\\n+              const rendered = String(inputs[key]);\\n+              logger.info(`[WorkflowProvider] Rendered '${key}' input (${rendered.length} chars): ${rendered.substring(0, 500)}${rendered.length > 500 ? '...' : ''}`);\\n+            }\\n           } else {\\n             inputs[key] = value;\\n           }\\n@@ -234,11 +332,7 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           inputs[key] = compileAndRun(\\n             sandbox,\\n             exprValue.expression,\\n-            {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            },\\n+            templateContext,\\n             { injectLog: true, logPrefix: `workflow.input.${key}` }\\n           );\\n         } else {\\n@@ -367,6 +461,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n     try {\\n       await childMemory.initialize();\\n     } catch {}\\n+    // Enhanced debug logging for workspace propagation diagnosis\\n+    const parentWorkspace = parentContext?.workspace;\\n+    logger.info(`[WorkflowProvider] Workspace propagation for nested workflow '${workflow.id}':`);\\n+    logger.info(`[WorkflowProvider]   parentContext exists: ${!!parentContext}`);\\n+    logger.info(`[WorkflowProvider]   parentContext.workspace exists: ${!!parentWorkspace}`);\\n+    if (parentWorkspace) {\\n+      logger.info(`[WorkflowProvider]   parentWorkspace.isEnabled(): ${parentWorkspace.isEnabled?.() ?? 'N/A'}`);\\n+      const projectCount = parentWorkspace.listProjects?.()?.length ?? 'N/A';\\n+      logger.info(`[WorkflowProvider]   parentWorkspace project count: ${projectCount}`);\\n+    } else {\\n+      logger.warn(`[WorkflowProvider]   NO WORKSPACE from parent - nested checkouts won't be added to workspace!`);\\n+    }\\n \\n     const childContext = {\\n       mode: 'state-machine' as const,\\n@@ -374,7 +480,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       checks: checksMetadata,\\n       journal: childJournal,\\n       memory: childMemory,\\n+      // For nested workflows we continue to execute inside the same logical\\n+      // working directory as the parent run. When workspace isolation is\\n+      // enabled on the parent engine, its WorkspaceManager is also propagated\\n+      // so that nested checks (AI, git-checkout, etc.) see the same isolated\\n+      // workspace and project symlinks instead of falling back to the Visor\\n+      // repository root.\\n       workingDirectory: parentContext?.workingDirectory || process.cwd(),\\n+      originalWorkingDirectory:\\n+        parentContext?.originalWorkingDirectory ||\\n+        parentContext?.workingDirectory ||\\n+        process.cwd(),\\n+      workspace: parentWorkspace,\\n       // Always use a fresh session for nested workflows to isolate history\\n       sessionId: uuidv4(),\\n       event: parentContext?.event || prInfo.eventType,\\n@@ -500,6 +617,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     } catch {}\\n \\n+    // Create outputs map that directly exposes .output values (not wrapped in { output, issues })\\n+    // This makes {{ outputs['check-name'] }} work naturally without needing .output\\n+    const outputsMap = Object.fromEntries(\\n+      Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n+    );\\n+\\n     for (const output of workflow.outputs) {\\n       if (output.value_js) {\\n         // JavaScript expression\\n@@ -508,10 +631,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           output.value_js,\\n           {\\n             inputs,\\n-            steps: Object.fromEntries(\\n-              Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-            ),\\n-            outputs: flat,\\n+            outputs: outputsMap,\\n+            // Keep 'steps' as alias for backwards compatibility\\n+            steps: outputsMap,\\n             pr: prInfo,\\n           },\\n           { injectLog: true, logPrefix: `workflow.output.${output.name}` }\\n@@ -520,10 +642,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         // Liquid template\\n         outputs[output.name] = await this.liquid.parseAndRender(output.value, {\\n           inputs,\\n-          steps: Object.fromEntries(\\n-            Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-          ),\\n-          outputs: flat,\\n+          outputs: outputsMap,\\n+          // Keep 'steps' as alias for backwards compatibility\\n+          steps: outputsMap,\\n           pr: prInfo,\\n         });\\n       }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/client.ts\",\"additions\":65,\"deletions\":1,\"changes\":66,\"patch\":\"diff --git a/src/slack/client.ts b/src/slack/client.ts\\nindex 292cc358..435f8b0d 100644\\n--- a/src/slack/client.ts\\n+++ b/src/slack/client.ts\\n@@ -142,6 +142,70 @@ export class SlackClient {\\n     }\\n   }\\n \\n+  public readonly files = {\\n+    /**\\n+     * Upload a file to Slack using files.uploadV2 API\\n+     * @param options Upload options including file content, filename, channel, and thread_ts\\n+     */\\n+    uploadV2: async ({\\n+      content,\\n+      filename,\\n+      channel,\\n+      thread_ts,\\n+      title,\\n+      initial_comment,\\n+    }: {\\n+      content: Buffer;\\n+      filename: string;\\n+      channel: string;\\n+      thread_ts?: string;\\n+      title?: string;\\n+      initial_comment?: string;\\n+    }): Promise<{ ok: boolean; file?: { id: string; permalink?: string } }> => {\\n+      try {\\n+        // Step 1: Get upload URL\\n+        const getUrlResp: any = await this.api('files.getUploadURLExternal', {\\n+          filename,\\n+          length: content.length,\\n+        });\\n+        if (!getUrlResp || getUrlResp.ok !== true || !getUrlResp.upload_url) {\\n+          console.warn(`Slack files.getUploadURLExternal failed: ${getUrlResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 2: Upload file content to the URL\\n+        const uploadResp = await fetch(getUrlResp.upload_url, {\\n+          method: 'POST',\\n+          body: content,\\n+        });\\n+        if (!uploadResp.ok) {\\n+          console.warn(`Slack file upload to URL failed: ${uploadResp.status}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 3: Complete the upload and share to channel\\n+        const completeResp: any = await this.api('files.completeUploadExternal', {\\n+          files: [{ id: getUrlResp.file_id, title: title || filename }],\\n+          channel_id: channel,\\n+          thread_ts,\\n+          initial_comment,\\n+        });\\n+        if (!completeResp || completeResp.ok !== true) {\\n+          console.warn(`Slack files.completeUploadExternal failed: ${completeResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        return {\\n+          ok: true,\\n+          file: completeResp.files?.[0] || { id: getUrlResp.file_id },\\n+        };\\n+      } catch (e) {\\n+        console.warn(`Slack file upload failed: ${e instanceof Error ? e.message : String(e)}`);\\n+        return { ok: false };\\n+      }\\n+    },\\n+  };\\n+\\n   getWebClient(): any {\\n     return {\\n       conversations: {\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/markdown.ts\",\"additions\":204,\"deletions\":5,\"changes\":209,\"patch\":\"diff --git a/src/slack/markdown.ts b/src/slack/markdown.ts\\nindex cc63fb5f..9f91c25f 100644\\n--- a/src/slack/markdown.ts\\n+++ b/src/slack/markdown.ts\\n@@ -3,14 +3,191 @@\\n // without pulling in a full Markdown parser.\\n //\\n // Supported conversions:\\n+// - # Header / ## Header  ‚Üí *Header* (bold with visual separation)\\n // - **bold** / __bold__   ‚Üí *bold*\\n // - [label](url)          ‚Üí <url|label>\\n // - ![alt](url)           ‚Üí <url|alt>\\n // - *italic* (inline)     ‚Üí _italic_\\n+// - ```mermaid blocks     ‚Üí rendered to PNG and uploaded to Slack\\n //\\n // Everything else is passed through unchanged; Slack will still render many\\n // Markdown-like constructs (lists, code fences, etc.) natively.\\n \\n+import { spawn } from 'child_process';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n+import * as os from 'os';\\n+\\n+/**\\n+ * Represents an extracted mermaid diagram\\n+ */\\n+export interface MermaidDiagram {\\n+  /** The full match including ```mermaid and ``` */\\n+  fullMatch: string;\\n+  /** The mermaid code content */\\n+  code: string;\\n+  /** Start index in the original text */\\n+  startIndex: number;\\n+  /** End index in the original text */\\n+  endIndex: number;\\n+}\\n+\\n+/**\\n+ * Extract all mermaid code blocks from text\\n+ */\\n+export function extractMermaidDiagrams(text: string): MermaidDiagram[] {\\n+  const diagrams: MermaidDiagram[] = [];\\n+  // Match ```mermaid followed by newline, content, and closing ```\\n+  const regex = /```mermaid\\\\s*\\\\n([\\\\s\\\\S]*?)```/g;\\n+  let match;\\n+  while ((match = regex.exec(text)) !== null) {\\n+    diagrams.push({\\n+      fullMatch: match[0],\\n+      code: match[1].trim(),\\n+      startIndex: match.index,\\n+      endIndex: match.index + match[0].length,\\n+    });\\n+  }\\n+  return diagrams;\\n+}\\n+\\n+/**\\n+ * Render a mermaid diagram to PNG using mmdc CLI (@mermaid-js/mermaid-cli).\\n+ *\\n+ * Requirements:\\n+ * - Node.js and npx must be available in PATH\\n+ * - Network access on first run (npx downloads the package)\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ *\\n+ * On Linux, you may need to install chromium dependencies:\\n+ *   apt-get install -y chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ *\\n+ * On Docker/CI, consider using a base image with puppeteer support or\\n+ * pre-installing @mermaid-js/mermaid-cli globally.\\n+ *\\n+ * @param mermaidCode The mermaid diagram code\\n+ * @returns Buffer containing PNG data, or null if rendering failed\\n+ */\\n+export async function renderMermaidToPng(mermaidCode: string): Promise<Buffer | null> {\\n+  // Create temp files for input and output\\n+  const tmpDir = os.tmpdir();\\n+  const inputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.mmd`);\\n+  const outputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.png`);\\n+\\n+  try {\\n+    // Write mermaid code to temp file\\n+    fs.writeFileSync(inputFile, mermaidCode, 'utf-8');\\n+\\n+    // Detect system chromium for puppeteer (mermaid-cli dependency)\\n+    // Without this, puppeteer may hang trying to download its own chromium\\n+    const chromiumPaths = [\\n+      '/usr/bin/chromium',\\n+      '/usr/bin/chromium-browser',\\n+      '/usr/bin/google-chrome',\\n+      '/usr/bin/chrome',\\n+    ];\\n+    let chromiumPath: string | undefined;\\n+    for (const p of chromiumPaths) {\\n+      if (fs.existsSync(p)) {\\n+        chromiumPath = p;\\n+        break;\\n+      }\\n+    }\\n+\\n+    // Build environment with chromium path if found\\n+    const env = { ...process.env };\\n+    if (chromiumPath) {\\n+      env.PUPPETEER_EXECUTABLE_PATH = chromiumPath;\\n+    }\\n+\\n+    // Run mmdc to render PNG\\n+    const result = await new Promise<{ success: boolean; error?: string }>((resolve) => {\\n+      const proc = spawn('npx', [\\n+        '--yes',\\n+        '@mermaid-js/mermaid-cli',\\n+        '-i', inputFile,\\n+        '-o', outputFile,\\n+        '-e', 'png',\\n+        '-b', 'white',\\n+        '-w', '1200',\\n+      ], {\\n+        timeout: 60000, // 60 second timeout (first run may download packages)\\n+        stdio: ['pipe', 'pipe', 'pipe'],\\n+        env,\\n+      });\\n+\\n+      let stderr = '';\\n+      proc.stderr?.on('data', (data) => {\\n+        stderr += data.toString();\\n+      });\\n+\\n+      proc.on('close', (code) => {\\n+        if (code === 0) {\\n+          resolve({ success: true });\\n+        } else {\\n+          resolve({ success: false, error: stderr || `Exit code ${code}` });\\n+        }\\n+      });\\n+\\n+      proc.on('error', (err) => {\\n+        resolve({ success: false, error: err.message });\\n+      });\\n+    });\\n+\\n+    if (!result.success) {\\n+      console.warn(`Mermaid rendering failed: ${result.error}`);\\n+      return null;\\n+    }\\n+\\n+    // Read the output PNG\\n+    if (!fs.existsSync(outputFile)) {\\n+      console.warn('Mermaid output file not created');\\n+      return null;\\n+    }\\n+\\n+    const pngBuffer = fs.readFileSync(outputFile);\\n+    return pngBuffer;\\n+  } catch (e) {\\n+    console.warn(`Mermaid rendering error: ${e instanceof Error ? e.message : String(e)}`);\\n+    return null;\\n+  } finally {\\n+    // Cleanup temp files\\n+    try {\\n+      if (fs.existsSync(inputFile)) fs.unlinkSync(inputFile);\\n+      if (fs.existsSync(outputFile)) fs.unlinkSync(outputFile);\\n+    } catch {\\n+      // Ignore cleanup errors\\n+    }\\n+  }\\n+}\\n+\\n+/**\\n+ * Replace mermaid blocks in text with a placeholder message\\n+ * @param text Original text\\n+ * @param diagrams Extracted diagrams\\n+ * @param replacement Text to replace each diagram with (or a function that returns replacement for each index)\\n+ */\\n+export function replaceMermaidBlocks(\\n+  text: string,\\n+  diagrams: MermaidDiagram[],\\n+  replacement: string | ((index: number) => string) = '_(See diagram above)_'\\n+): string {\\n+  if (diagrams.length === 0) return text;\\n+\\n+  // Sort by start index descending to replace from end to start (preserves indices)\\n+  const sorted = [...diagrams].sort((a, b) => b.startIndex - a.startIndex);\\n+\\n+  let result = text;\\n+  sorted.forEach((diagram, sortedIndex) => {\\n+    // Calculate original index (since we sorted in reverse)\\n+    const originalIndex = diagrams.length - 1 - sortedIndex;\\n+    const rep = typeof replacement === 'function' ? replacement(originalIndex) : replacement;\\n+    result = result.slice(0, diagram.startIndex) + rep + result.slice(diagram.endIndex);\\n+  });\\n+\\n+  return result;\\n+}\\n+\\n export function markdownToSlack(text: string): string {\\n   if (!text || typeof text !== 'string') return '';\\n \\n@@ -33,7 +210,7 @@ export function markdownToSlack(text: string): string {\\n   out = out.replace(/\\\\*\\\\*([^*]+)\\\\*\\\\*/g, (_m, inner: string) => `*${inner}*`);\\n   out = out.replace(/__([^_]+)__/g, (_m, inner: string) => `*${inner}*`);\\n \\n-  // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation).\\n+  // Process lines for headers and bullet lists.\\n   // Slack's mrkdwn handles \\\"‚Ä¢\\\" bullets more naturally than raw \\\"-\\\" Markdown.\\n   const lines = out.split(/\\\\r?\\\\n/);\\n   let inCodeBlock = false;\\n@@ -46,9 +223,31 @@ export function markdownToSlack(text: string): string {\\n       continue;\\n     }\\n     if (inCodeBlock) continue;\\n-    const match = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n-    if (match) {\\n-      const [, indent, , rest] = match;\\n+\\n+    // Headers: # Header ‚Üí *Header* (Slack doesn't have native headers)\\n+    // Match 1-6 # at start of line, followed by space and text\\n+    const headerMatch = /^(#{1,6})\\\\s+(.+)$/.exec(trimmed);\\n+    if (headerMatch) {\\n+      const [, hashes, headerText] = headerMatch;\\n+      // For h1/h2, add extra emphasis with newline before (if not first line\\n+      // and previous line is not empty/header/code-fence)\\n+      const prevLine = i > 0 ? lines[i - 1].trim() : '';\\n+      const prevIsHeaderOrFence =\\n+        /^#{1,6}\\\\s+/.test(prevLine) ||\\n+        /^\\\\*[^*]+\\\\*$/.test(prevLine) ||\\n+        /^```/.test(prevLine);\\n+      if (hashes.length <= 2 && i > 0 && prevLine !== '' && !prevIsHeaderOrFence) {\\n+        lines[i] = `\\\\n*${headerText.trim()}*`;\\n+      } else {\\n+        lines[i] = `*${headerText.trim()}*`;\\n+      }\\n+      continue;\\n+    }\\n+\\n+    // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation)\\n+    const bulletMatch = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n+    if (bulletMatch) {\\n+      const [, indent, , rest] = bulletMatch;\\n       lines[i] = `${indent}‚Ä¢ ${rest}`;\\n     }\\n   }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/prompt-state.ts\",\"additions\":19,\"deletions\":2,\"changes\":21,\"patch\":\"diff --git a/src/slack/prompt-state.ts b/src/slack/prompt-state.ts\\nindex a13233b2..544777c6 100644\\n--- a/src/slack/prompt-state.ts\\n+++ b/src/slack/prompt-state.ts\\n@@ -95,7 +95,10 @@ export class PromptStateManager {\\n   setFirstMessage(channel: string, threadTs: string, text: string): void {\\n     const key = this.key(channel, threadTs);\\n     if (!text || !text.trim()) return;\\n-    if (!this.firstMessage.has(key)) {\\n+    const existing = this.firstMessage.get(key);\\n+    // Only set if: no entry exists OR the existing entry was already consumed\\n+    // This allows new messages to be captured after a resume cycle\\n+    if (!existing || existing.consumed) {\\n       this.firstMessage.set(key, { text, consumed: false });\\n     }\\n   }\\n@@ -130,6 +133,20 @@ export class PromptStateManager {\\n         removed++;\\n       }\\n     }\\n+    // Also clean up stale firstMessage entries (consumed entries older than TTL)\\n+    // Keep unconsumed entries to avoid losing user messages\\n+    for (const [key] of this.firstMessage.entries()) {\\n+      const waitingInfo = this.waiting.get(key);\\n+      // If no corresponding waiting entry exists and the firstMessage was consumed,\\n+      // the conversation is likely complete - safe to remove\\n+      if (!waitingInfo) {\\n+        const entry = this.firstMessage.get(key);\\n+        if (entry?.consumed) {\\n+          this.firstMessage.delete(key);\\n+          removed++;\\n+        }\\n+      }\\n+    }\\n     if (removed) {\\n       try {\\n         logger.info(`[prompt-state] cleanup removed ${removed} entries`);\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/socket-runner.ts\",\"additions\":56,\"deletions\":32,\"changes\":88,\"patch\":\"diff --git a/src/slack/socket-runner.ts b/src/slack/socket-runner.ts\\nindex d1681a03..5b461d22 100644\\n--- a/src/slack/socket-runner.ts\\n+++ b/src/slack/socket-runner.ts\\n@@ -7,6 +7,7 @@ import { SlackAdapter } from './adapter';\\n import { CachePrewarmer } from './cache-prewarmer';\\n import { RateLimiter, type RateLimitConfig } from './rate-limiter';\\n import type { SlackBotConfig } from '../types/bot';\\n+import { withActiveSpan } from '../telemetry/trace-helpers';\\n \\n type SlackSocketConfig = {\\n   appToken?: string; // xapp- token\\n@@ -30,6 +31,7 @@ export class SlackSocketRunner {\\n   private botUserId?: string;\\n   private processedKeys: Map<string, number> = new Map();\\n   private adapter?: SlackAdapter;\\n+  private retryCount = 0;\\n \\n   constructor(engine: StateMachineExecutionEngine, cfg: VisorConfig, opts: SlackSocketConfig) {\\n     const app = opts.appToken || process.env.SLACK_APP_TOKEN || '';\\n@@ -107,7 +109,10 @@ export class SlackSocketRunner {\\n \\n   private async connect(url: string): Promise<void> {\\n     this.ws = new WebSocket(url);\\n-    this.ws.on('open', () => logger.info('[SlackSocket] WebSocket connected'));\\n+    this.ws.on('open', () => {\\n+      this.retryCount = 0; // Reset on successful connection\\n+      logger.info('[SlackSocket] WebSocket connected');\\n+    });\\n     this.ws.on('close', (code, reason) => {\\n       logger.warn(`[SlackSocket] WebSocket closed: ${code} ${reason}`);\\n       setTimeout(() => this.restart().catch(() => {}), 1000);\\n@@ -123,7 +128,13 @@ export class SlackSocketRunner {\\n       const url = await this.openConnection();\\n       await this.connect(url);\\n     } catch (e) {\\n-      logger.error(`[SlackSocket] Restart failed: ${e instanceof Error ? e.message : e}`);\\n+      this.retryCount++;\\n+      // Exponential backoff: 2s, 4s, 8s, 16s, 32s, capped at 60s\\n+      const delay = Math.min(2000 * Math.pow(2, this.retryCount - 1), 60000);\\n+      logger.error(\\n+        `[SlackSocket] Restart failed (attempt ${this.retryCount}), retrying in ${Math.round(delay / 1000)}s: ${e instanceof Error ? e.message : e}`\\n+      );\\n+      setTimeout(() => this.restart().catch(() => {}), delay);\\n     }\\n   }\\n \\n@@ -296,36 +307,49 @@ export class SlackSocketRunner {\\n           });\\n         }\\n       } catch {}\\n-      if (path) {\\n-        try {\\n-          const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n-          const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n-          logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n-          await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n-            webhookContext: { webhookData: map, eventType: 'manual' },\\n-            debug: process.env.VISOR_DEBUG === 'true',\\n-          });\\n-          if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n-          return; // resume path handled\\n-        } catch (e) {\\n-          logger.warn(\\n-            `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n-              e instanceof Error ? e.message : String(e)\\n-            }`\\n-          );\\n-        }\\n-      }\\n+      try {\\n+        await withActiveSpan(\\n+          'visor.run',\\n+          {\\n+            'visor.run.source': 'slack',\\n+            'slack.event.type': String(type || ''),\\n+            'slack.channel': channelId,\\n+            'slack.thread_ts': threadTs,\\n+          },\\n+          async () => {\\n+            if (path) {\\n+              try {\\n+                const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n+                const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n+                logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n+                await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n+                  webhookContext: { webhookData: map, eventType: 'manual' },\\n+                  debug: process.env.VISOR_DEBUG === 'true',\\n+                });\\n+                return;\\n+              } catch (e) {\\n+                logger.warn(\\n+                  `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n+                    e instanceof Error ? e.message : String(e)\\n+                  }`\\n+                );\\n+              }\\n+            }\\n \\n-      // Cold run (no snapshot)\\n-      await runEngine.executeChecks({\\n-        checks: allChecks,\\n-        showDetails: true,\\n-        outputFormat: 'json',\\n-        config: cfgForRun,\\n-        webhookContext: { webhookData: map, eventType: 'manual' },\\n-        debug: process.env.VISOR_DEBUG === 'true',\\n-      } as any);\\n-      if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+            // Cold run (no snapshot)\\n+            await runEngine.executeChecks({\\n+              checks: allChecks,\\n+              showDetails: true,\\n+              outputFormat: 'json',\\n+              config: cfgForRun,\\n+              webhookContext: { webhookData: map, eventType: 'manual' },\\n+              debug: process.env.VISOR_DEBUG === 'true',\\n+            } as any);\\n+          }\\n+        );\\n+      } finally {\\n+        if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+      }\\n     } catch (e) {\\n       logger.error(\\n         `[SlackSocket] Engine execution failed: ${e instanceof Error ? e.message : String(e)}`\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/context/build-engine-context.ts\",\"additions\":11,\"deletions\":4,\"changes\":15,\"patch\":\"diff --git a/src/state-machine/context/build-engine-context.ts b/src/state-machine/context/build-engine-context.ts\\nindex 149c73bf..45e63b6a 100644\\n--- a/src/state-machine/context/build-engine-context.ts\\n+++ b/src/state-machine/context/build-engine-context.ts\\n@@ -57,7 +57,8 @@ export function buildEngineContextForRun(\\n       ) as EventTrigger[],\\n       group: checkConfig.group,\\n       providerType: checkConfig.type || 'ai',\\n-      dependencies: checkConfig.depends_on || [],\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: Array.isArray(checkConfig.depends_on) ? checkConfig.depends_on : checkConfig.depends_on ? [checkConfig.depends_on] : [],\\n     };\\n   }\\n \\n@@ -134,11 +135,14 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n   const originalPath = context.workingDirectory || process.cwd();\\n \\n   try {\\n+    // Check if workspace should be kept (for debugging)\\n+    const keepWorkspace = process.env.VISOR_KEEP_WORKSPACE === 'true';\\n+\\n     // Create workspace manager\\n     const workspace = WorkspaceManager.getInstance(context.sessionId, originalPath, {\\n       enabled: true,\\n-      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH,\\n-      cleanupOnExit: workspaceConfig?.cleanup_on_exit !== false,\\n+      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH || '/tmp/visor-workspaces',\\n+      cleanupOnExit: keepWorkspace ? false : workspaceConfig?.cleanup_on_exit !== false,\\n     });\\n \\n     // Initialize workspace (creates main project worktree)\\n@@ -151,6 +155,9 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n \\n     logger.info(`[Workspace] Initialized workspace: ${info.workspacePath}`);\\n     logger.debug(`[Workspace] Main project at: ${info.mainProjectPath}`);\\n+    if (keepWorkspace) {\\n+      logger.info(`[Workspace] Keeping workspace after execution (--keep-workspace)`);\\n+    }\\n \\n     return context;\\n   } catch (error) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/execution-invoker.ts\",\"additions\":84,\"deletions\":4,\"changes\":88,\"patch\":\"diff --git a/src/state-machine/dispatch/execution-invoker.ts b/src/state-machine/dispatch/execution-invoker.ts\\nindex eb7587d3..798ca8e4 100644\\n--- a/src/state-machine/dispatch/execution-invoker.ts\\n+++ b/src/state-machine/dispatch/execution-invoker.ts\\n@@ -356,10 +356,17 @@ export async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -413,6 +420,31 @@ export async function executeSingleCheck(\\n     }\\n   }\\n \\n+  // Banner-style log when a check actually starts executing. This is emitted\\n+  // after all gating (if/depends_on) has passed so it only appears for real\\n+  // runs, and gives a clear visual separator in the logs between checks.\\n+  try {\\n+    const wave = state.wave;\\n+    const level = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${level}) ‚îÅ‚îÅ‚îÅ`;\\n+\\n+    // When running in a TTY, colour the entire banner line for extra\\n+    // visibility; keep plain text for JSON/SARIF or non-TTY environments.\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   let forEachParent: string | undefined;\\n   let forEachItems: unknown[] | undefined;\\n   for (const depId of depList) {\\n@@ -563,13 +595,56 @@ export async function executeSingleCheck(\\n       files: [],\\n       commits: [],\\n     };\\n+    // Derive AI session reuse context for this check (self-mode only for now).\\n+    // When reuse_ai_session: 'self' is configured, look up the last root-scope\\n+    // journal entry for this check in the current engine session and expose its\\n+    // sessionId via ExecutionContext so ai-check-provider can call\\n+    // executeReviewWithSessionReuse() against the same ProbeAgent session.\\n+    let parentSessionId: string | undefined;\\n+    let reuseSession = false;\\n+    try {\\n+      const reuseCfg: unknown = (checkConfig as any).reuse_ai_session;\\n+      if (reuseCfg === 'self') {\\n+        const snapshotId = context.journal.beginSnapshot();\\n+        const visible = context.journal.readVisible(\\n+          context.sessionId,\\n+          snapshotId,\\n+          context.event as any\\n+        );\\n+        // Prefer the most recent root-scope result for this check\\n+        const prior = visible.filter(\\n+          e => e.checkId === checkId && (!e.scope || e.scope.length === 0)\\n+        );\\n+        if (prior.length > 0) {\\n+          const last = prior[prior.length - 1];\\n+          const sess = (last.result as any)?.sessionId;\\n+          if (typeof sess === 'string' && sess.length > 0) {\\n+            parentSessionId = sess;\\n+            reuseSession = true;\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only ‚Äì fall back to normal (non-reuse) execution on error.\\n+      parentSessionId = undefined;\\n+      reuseSession = false;\\n+    }\\n+\\n     const executionContext = {\\n       ...context.executionContext,\\n       _engineMode: context.mode,\\n       _parentContext: context,\\n       _parentState: state,\\n+      // Explicitly propagate workspace reference for nested workflows\\n+      workspace: context.workspace,\\n     };\\n \\n+    // Attach session reuse hints for providers that support them (AI, Claude Code, etc).\\n+    if (reuseSession && parentSessionId) {\\n+      (executionContext as any).parentSessionId = parentSessionId;\\n+      (executionContext as any).reuseSession = true;\\n+    }\\n+\\n     // Handle on_init lifecycle hook BEFORE main execution\\n     if (checkConfig.on_init) {\\n       try {\\n@@ -613,7 +688,12 @@ export async function executeSingleCheck(\\n \\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/foreach-processor.ts\",\"additions\":27,\"deletions\":2,\"changes\":29,\"patch\":\"diff --git a/src/state-machine/dispatch/foreach-processor.ts b/src/state-machine/dispatch/foreach-processor.ts\\nindex 336bf9df..ab630986 100644\\n--- a/src/state-machine/dispatch/foreach-processor.ts\\n+++ b/src/state-machine/dispatch/foreach-processor.ts\\n@@ -77,6 +77,26 @@ export async function executeCheckWithForEachItems(\\n   const allContents: string[] = [];\\n   const perIterationDurations: number[] = [];\\n \\n+  // Emit a banner for the forEach parent so logs clearly show when we are\\n+  // entering the aggregated execution for that step.\\n+  try {\\n+    const wave = state.wave;\\n+    const lvl = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${lvl}, forEach parent) ‚îÅ‚îÅ‚îÅ`;\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   for (let itemIndex = 0; itemIndex < forEachItems.length; itemIndex++) {\\n     const iterationStartMs = Date.now();\\n     const scope: Array<{ check: string; index: number }> = [\\n@@ -248,7 +268,12 @@ export async function executeCheckWithForEachItems(\\n \\n       const result = await withActiveSpan(\\n         `visor.check.${checkId}`,\\n-        { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+        {\\n+          'visor.check.id': checkId,\\n+          'visor.check.type': providerType,\\n+          session_id: context.sessionId,\\n+          wave: state.wave,\\n+        },\\n         async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n       );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/stats-manager.ts\",\"additions\":21,\"deletions\":1,\"changes\":22,\"patch\":\"diff --git a/src/state-machine/dispatch/stats-manager.ts b/src/state-machine/dispatch/stats-manager.ts\\nindex 8d04436e..2f317b59 100644\\n--- a/src/state-machine/dispatch/stats-manager.ts\\n+++ b/src/state-machine/dispatch/stats-manager.ts\\n@@ -44,6 +44,26 @@ export function updateStats(\\n       issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n     };\\n \\n+    // Respect explicit skip markers on the result (set by LevelDispatch).\\n+    const skippedMarker = (result as any).__skipped;\\n+    if (skippedMarker) {\\n+      stats.skipped = true;\\n+      stats.skipReason =\\n+        typeof skippedMarker === 'string'\\n+          ? (skippedMarker as any)\\n+          : (stats.skipReason as any) || 'if_condition';\\n+      // A skipped result should not contribute to run counters; reset\\n+      // them explicitly to guard against any prior values lingering\\n+      // from earlier runs of the same check.\\n+      stats.totalRuns = 0;\\n+      stats.successfulRuns = 0;\\n+      stats.failedRuns = 0;\\n+      stats.skippedRuns++;\\n+      // Do not count this as a run; keep totalRuns/failed/success unchanged.\\n+      state.stats.set(checkId, stats);\\n+      continue;\\n+    }\\n+\\n     // If this check was previously marked as skipped and now executed,\\n     // clear the skipped flag and any skip counters to ensure the run is visible.\\n     if (stats.skipped) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/runner.ts\",\"additions\":10,\"deletions\":7,\"changes\":17,\"patch\":\"diff --git a/src/state-machine/runner.ts b/src/state-machine/runner.ts\\nindex ee86b876..a13a5efd 100644\\n--- a/src/state-machine/runner.ts\\n+++ b/src/state-machine/runner.ts\\n@@ -124,14 +124,17 @@ export class StateMachineRunner {\\n    */\\n   private async executeState(state: EngineState): Promise<void> {\\n     // M4: Wrap state execution in OTEL span\\n+    const attrs: Record<string, unknown> = {\\n+      state: state,\\n+      engine_mode: this.context.mode,\\n+      wave: this.state.wave,\\n+      session_id: this.context.sessionId,\\n+    };\\n+    const waveKind = (this.state as any)?.flags?.waveKind;\\n+    if (waveKind) attrs.wave_kind = waveKind;\\n     return withActiveSpan(\\n       `engine.state.${state.toLowerCase()}`,\\n-      {\\n-        state: state,\\n-        engine_mode: this.context.mode,\\n-        wave: this.state.wave,\\n-        session_id: this.context.sessionId,\\n-      },\\n+      attrs,\\n       async () => {\\n         try {\\n           switch (state) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/level-dispatch.ts\",\"additions\":152,\"deletions\":15,\"changes\":167,\"patch\":\"diff --git a/src/state-machine/states/level-dispatch.ts b/src/state-machine/states/level-dispatch.ts\\nindex 5588b0f5..95392197 100644\\n--- a/src/state-machine/states/level-dispatch.ts\\n+++ b/src/state-machine/states/level-dispatch.ts\\n@@ -26,7 +26,7 @@ import type { CheckExecutionStats } from '../../types/execution';\\n import type { CheckProviderConfig } from '../../providers/check-provider.interface';\\n import type { CheckConfig } from '../../types/config';\\n import { handleRouting, checkLoopBudget } from './routing';\\n-import { withActiveSpan } from '../../telemetry/trace-helpers';\\n+import { withActiveSpan, setSpanAttributes, addEvent } from '../../telemetry/trace-helpers';\\n import { emitMermaidFromMarkdown } from '../../utils/mermaid-telemetry';\\n import { emitNdjsonSpanWithEvents, emitNdjsonFallback } from '../../telemetry/fallback-ndjson';\\n import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n@@ -46,6 +46,32 @@ function mapCheckNameToFocus(checkName: string): string {\\n   return focusMap[checkName] || 'all';\\n }\\n \\n+function formatScopeLabel(scope: Array<{ check: string; index: number }> | undefined): string {\\n+  if (!scope || scope.length === 0) return '';\\n+  return scope.map(item => `${item.check}:${item.index}`).join('|');\\n+}\\n+\\n+function recordOnFinishRoutingEvent(args: {\\n+  checkId: string;\\n+  action: 'run' | 'goto';\\n+  target: string;\\n+  source: 'run' | 'goto' | 'goto_js' | 'transitions';\\n+  scope?: Array<{ check: string; index: number }>;\\n+  gotoEvent?: string;\\n+}): void {\\n+  const attrs: Record<string, unknown> = {\\n+    check_id: args.checkId,\\n+    trigger: 'on_finish',\\n+    action: args.action,\\n+    target: args.target,\\n+    source: args.source,\\n+  };\\n+  const scopeLabel = formatScopeLabel(args.scope);\\n+  if (scopeLabel) attrs.scope = scopeLabel;\\n+  if (args.gotoEvent) attrs.goto_event = args.gotoEvent;\\n+  addEvent('visor.routing', attrs);\\n+}\\n+\\n /**\\n  * Build output history Map from journal for template rendering\\n  * This matches the format expected by AI providers\\n@@ -121,12 +147,13 @@ async function evaluateIfCondition(\\n         return false;\\n       }\\n     })();\\n+    // Steps with dependencies should always see outputs from all completed steps.\\n     // In forward-run waves (from on_success/on_fail goto), guards should see the\\n     // latest global outputs even if the check has no explicit dependencies.\\n     // In wave-retry (from on_finish), restrict to checks with dependencies to\\n     // avoid wrongly skipping top-level prompts like 'ask'.\\n     const useGlobalOutputs =\\n-      (useGlobalOutputsFlag && waveKind === 'forward') || (useGlobalOutputsFlag && hasDeps);\\n+      hasDeps || (useGlobalOutputsFlag && waveKind === 'forward');\\n \\n     if (useGlobalOutputs) {\\n       // Forward-run wave: allow guards to consult latest outputs from the entire\\n@@ -199,6 +226,7 @@ async function evaluateIfCondition(\\n       baseBranch: (context.prInfo as any)?.baseBranch,\\n       filesChanged: context.prInfo?.files?.map(f => f.filename),\\n       environment: envSnapshot,\\n+      workflowInputs: (context.config as any).workflow_inputs || {},\\n     };\\n \\n     const shouldRun = await evaluator.evaluateIfCondition(checkId, ifExpression, contextData);\\n@@ -238,6 +266,11 @@ export async function handleLevelDispatch(\\n   // Update current level tracking\\n   state.currentLevel = level.level;\\n   state.currentLevelChecks = new Set(level.parallel);\\n+  const levelChecksPreview = level.parallel.slice(0, 5).join(',');\\n+  setSpanAttributes({\\n+    level_size: level.parallel.length,\\n+    level_checks_preview: levelChecksPreview,\\n+  });\\n \\n   // Emit level ready event\\n   emitEvent({ type: 'LevelReady', level, wave: state.wave });\\n@@ -689,6 +722,39 @@ async function executeCheckWithForEachItems(\\n         }\\n       } catch {}\\n \\n+      // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+      // The socket-runner stores conversation data in webhookData under the endpoint key\\n+      try {\\n+        const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+        const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+        if (context.debug) {\\n+          logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+        }\\n+        if (webhookData && webhookData.size > 0) {\\n+          // Find the payload with slack_conversation\\n+          for (const payload of webhookData.values()) {\\n+            const slackConv = (payload as any)?.slack_conversation;\\n+            if (slackConv) {\\n+              // Build slack context with event and conversation\\n+              const event = (payload as any)?.event;\\n+              const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+              if (context.debug) {\\n+                logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+              }\\n+              (providerConfig as any).eventContext = {\\n+                ...(providerConfig as any).eventContext,\\n+                slack: {\\n+                  event: event || {},\\n+                  conversation: slackConv,\\n+                },\\n+                conversation: slackConv, // Also expose at top level for convenience\\n+              };\\n+              break;\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n+\\n       // Build dependency results with scope\\n       const dependencyResults = buildDependencyResultsWithScope(\\n         checkId,\\n@@ -820,15 +886,17 @@ async function executeCheckWithForEachItems(\\n       } catch {}\\n \\n       // Execute provider with telemetry\\n-      const itemResult = await withActiveSpan(\\n-        `visor.check.${checkId}`,\\n-        {\\n-          'visor.check.id': checkId,\\n-          'visor.check.type': providerType,\\n-          'visor.foreach.index': itemIndex,\\n-        },\\n-        async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n-      );\\n+    const itemResult = await withActiveSpan(\\n+      `visor.check.${checkId}`,\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        'visor.foreach.index': itemIndex,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n+      async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n+    );\\n \\n       // Enrich issues\\n       const enrichedIssues = (itemResult.issues || []).map((issue: ReviewIssue) => ({\\n@@ -942,6 +1010,7 @@ async function executeCheckWithForEachItems(\\n             const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n               previousResults: dependencyResults as any,\\n               event: context.event || 'manual',\\n+              output: output, // Pass the iteration output for guarantee evaluation\\n             } as any);\\n             if (!holds) {\\n               const issue: ReviewIssue = {\\n@@ -1311,6 +1380,13 @@ async function executeCheckWithForEachItems(\\n             // Increment loop count\\n             state.routingLoopCount++;\\n \\n+            recordOnFinishRoutingEvent({\\n+              checkId: forEachParent,\\n+              action: 'run',\\n+              target: targetCheck,\\n+              source: 'run',\\n+              scope: [],\\n+            });\\n             emitEvent({\\n               type: 'ForwardRunRequested',\\n               target: targetCheck,\\n@@ -1358,6 +1434,14 @@ async function executeCheckWithForEachItems(\\n                 return aggregatedResult; // abort further routing\\n               }\\n               state.routingLoopCount++;\\n+              recordOnFinishRoutingEvent({\\n+                checkId: forEachParent,\\n+                action: 'goto',\\n+                target: transTarget.to,\\n+                source: 'transitions',\\n+                scope: [],\\n+                gotoEvent: (transTarget as any).goto_event,\\n+              });\\n               emitEvent({\\n                 type: 'ForwardRunRequested',\\n                 target: transTarget.to,\\n@@ -1464,6 +1548,13 @@ async function executeCheckWithForEachItems(\\n           // Increment loop count\\n           state.routingLoopCount++;\\n \\n+          recordOnFinishRoutingEvent({\\n+            checkId: forEachParent,\\n+            action: 'goto',\\n+            target: gotoTarget,\\n+            source: onFinish.goto_js ? 'goto_js' : 'goto',\\n+            scope: [],\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: gotoTarget,\\n@@ -1618,10 +1709,17 @@ async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -1923,6 +2021,39 @@ async function executeSingleCheck(\\n       }\\n     } catch {}\\n \\n+    // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+    // The socket-runner stores conversation data in webhookData under the endpoint key\\n+    try {\\n+      const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+      const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+      if (context.debug) {\\n+        logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+      }\\n+      if (webhookData && webhookData.size > 0) {\\n+        // Find the payload with slack_conversation\\n+        for (const payload of webhookData.values()) {\\n+          const slackConv = (payload as any)?.slack_conversation;\\n+          if (slackConv) {\\n+            // Build slack context with event and conversation\\n+            const event = (payload as any)?.event;\\n+            const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+            if (context.debug) {\\n+              logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+            }\\n+            (providerConfig as any).eventContext = {\\n+              ...(providerConfig as any).eventContext,\\n+              slack: {\\n+                event: event || {},\\n+                conversation: slackConv,\\n+              },\\n+              conversation: slackConv, // Also expose at top level for convenience\\n+            };\\n+            break;\\n+          }\\n+        }\\n+      }\\n+    } catch {}\\n+\\n     // Build dependency results\\n     const dependencyResults = buildDependencyResults(checkId, checkConfig, context, state);\\n \\n@@ -2016,7 +2147,12 @@ async function executeSingleCheck(\\n     // Execute provider with telemetry\\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n@@ -2098,6 +2234,7 @@ async function executeSingleCheck(\\n           const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n             previousResults: dependencyResults as any,\\n             event: context.event || 'manual',\\n+            output: enrichedResult.output,\\n           } as any);\\n           if (!holds) {\\n             const issue: ReviewIssue = {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/routing.ts\",\"additions\":165,\"deletions\":1,\"changes\":166,\"patch\":\"diff --git a/src/state-machine/states/routing.ts b/src/state-machine/states/routing.ts\\nindex 5752a5bb..b1fd8654 100644\\n--- a/src/state-machine/states/routing.ts\\n+++ b/src/state-machine/states/routing.ts\\n@@ -15,6 +15,7 @@ import type { EngineContext, RunState, EngineState, EngineEvent } from '../../ty\\n import type { ReviewSummary, ReviewIssue } from '../../reviewer';\\n import type { CheckConfig, OnFailConfig, TransitionRule } from '../../types/config';\\n import { logger } from '../../logger';\\n+import { addEvent } from '../../telemetry/trace-helpers';\\n import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n import { createSecureSandbox, compileAndRun } from '../../utils/sandbox';\\n import { MemoryStore } from '../../memory-store';\\n@@ -153,6 +154,37 @@ function createMemoryHelpers() {\\n   };\\n }\\n \\n+type RoutingTrigger = 'on_success' | 'on_fail' | 'on_finish';\\n+type RoutingAction = 'run' | 'goto' | 'retry';\\n+type RoutingSource = 'run' | 'run_js' | 'goto' | 'goto_js' | 'transitions' | 'retry';\\n+\\n+function formatScopeLabel(scope: Array<{ check: string; index: number }> | undefined): string {\\n+  if (!scope || scope.length === 0) return '';\\n+  return scope.map(item => `${item.check}:${item.index}`).join('|');\\n+}\\n+\\n+function recordRoutingEvent(args: {\\n+  checkId: string;\\n+  trigger: RoutingTrigger;\\n+  action: RoutingAction;\\n+  target?: string;\\n+  source?: RoutingSource;\\n+  scope?: Array<{ check: string; index: number }>;\\n+  gotoEvent?: string;\\n+}): void {\\n+  const attrs: Record<string, unknown> = {\\n+    check_id: args.checkId,\\n+    trigger: args.trigger,\\n+    action: args.action,\\n+  };\\n+  if (args.target) attrs.target = args.target;\\n+  if (args.source) attrs.source = args.source;\\n+  const scopeLabel = formatScopeLabel(args.scope);\\n+  if (scopeLabel) attrs.scope = scopeLabel;\\n+  if (args.gotoEvent) attrs.goto_event = args.gotoEvent;\\n+  addEvent('visor.routing', attrs);\\n+}\\n+\\n /**\\n  * Handle routing state - evaluate conditions and decide next actions\\n  */\\n@@ -271,6 +303,14 @@ async function processOnFinish(\\n             { check: checkId, index: itemIndex },\\n           ];\\n \\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_finish',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -284,6 +324,14 @@ async function processOnFinish(\\n         // Increment loop count\\n         state.routingLoopCount++;\\n \\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_finish',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope: [],\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -328,6 +376,14 @@ async function processOnFinish(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_finish',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -362,6 +418,15 @@ async function processOnFinish(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_finish',\\n+        action: 'goto',\\n+        target: finishTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: finishTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: finishTransTarget.to,\\n@@ -406,6 +471,14 @@ async function processOnFinish(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_finish',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onFinish.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+    });\\n     // Enqueue forward run event\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n@@ -640,6 +713,14 @@ async function processOnSuccess(\\n             { check: checkId, index: itemIndex },\\n           ];\\n \\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_success',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -652,6 +733,14 @@ async function processOnSuccess(\\n         // Increment loop count\\n         state.routingLoopCount++;\\n \\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_success',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope,\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -695,6 +784,14 @@ async function processOnSuccess(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_success',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -728,6 +825,15 @@ async function processOnSuccess(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_success',\\n+        action: 'goto',\\n+        target: successTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: successTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: successTransTarget.to,\\n@@ -773,6 +879,15 @@ async function processOnSuccess(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_success',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onSuccess.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+      gotoEvent: onSuccess.goto_event,\\n+    });\\n     // Enqueue forward run event with optional event override\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n@@ -864,6 +979,14 @@ async function processOnFail(\\n           const itemScope: Array<{ check: string; index: number }> = [\\n             { check: checkId, index: itemIndex },\\n           ];\\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_fail',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -874,6 +997,14 @@ async function processOnFail(\\n       } else {\\n         // No forEach context: preserve current scope (if any)\\n         state.routingLoopCount++;\\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_fail',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope,\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -917,6 +1048,14 @@ async function processOnFail(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_fail',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -958,6 +1097,13 @@ async function processOnFail(\\n \\n         // Increment loop count and schedule forward run for the same check\\n         state.routingLoopCount++;\\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_fail',\\n+          action: 'retry',\\n+          source: 'retry',\\n+          scope: sc || [],\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: checkId,\\n@@ -1013,6 +1159,15 @@ async function processOnFail(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_fail',\\n+        action: 'goto',\\n+        target: failTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: failTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: failTransTarget.to,\\n@@ -1058,6 +1213,15 @@ async function processOnFail(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_fail',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onFail.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+      gotoEvent: onFail.goto_event,\\n+    });\\n     // Enqueue forward run event with optional event override\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/wave-planning.ts\",\"additions\":10,\"deletions\":3,\"changes\":13,\"patch\":\"diff --git a/src/state-machine/states/wave-planning.ts b/src/state-machine/states/wave-planning.ts\\nindex 7551d55c..7b42f043 100644\\n--- a/src/state-machine/states/wave-planning.ts\\n+++ b/src/state-machine/states/wave-planning.ts\\n@@ -351,8 +351,11 @@ export async function handleWavePlanning(\\n     for (const id of checksToRun) {\\n       // Only include dependencies that are within the same subset (often none).\\n       const cfg = context.config.checks?.[id];\\n-      const deps = (cfg?.depends_on || []).filter((d: string) => checksToRun.includes(d));\\n-      subDeps[id] = deps as string[];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = cfg?.depends_on;\\n+      const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n+      const deps = depsArray.filter((d: string) => checksToRun.includes(d));\\n+      subDeps[id] = deps;\\n     }\\n \\n     const subGraph = DependencyResolver.buildDependencyGraph(subDeps);\\n@@ -410,6 +413,10 @@ export async function handleWavePlanning(\\n     // Initialize current wave state\\n     (state as any).currentWaveCompletions = new Set<string>();\\n     (state as any).failedChecks = new Set<string>();\\n+    try {\\n+      (state as any).flags = (state as any).flags || {};\\n+      (state as any).flags.waveKind = 'initial';\\n+    } catch {}\\n   }\\n \\n   // Check if there are levels to execute\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/workflow-projection.ts\",\"additions\":3,\"deletions\":2,\"changes\":5,\"patch\":\"diff --git a/src/state-machine/workflow-projection.ts b/src/state-machine/workflow-projection.ts\\nindex b2d5eb24..2bcfd858 100644\\n--- a/src/state-machine/workflow-projection.ts\\n+++ b/src/state-machine/workflow-projection.ts\\n@@ -53,7 +53,8 @@ export function projectWorkflowToGraph(\\n       triggers: step.on || workflow.on || [],\\n       group: step.group,\\n       providerType: step.type || 'ai',\\n-      dependencies: (step.depends_on || []).map(dep => dep),\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: (Array.isArray(step.depends_on) ? step.depends_on : step.depends_on ? [step.depends_on] : []).map((dep: string) => dep),\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/flow-stage.ts\",\"additions\":37,\"deletions\":4,\"changes\":41,\"patch\":\"diff --git a/src/test-runner/core/flow-stage.ts b/src/test-runner/core/flow-stage.ts\\nindex a14256c2..a587f9f1 100644\\n--- a/src/test-runner/core/flow-stage.ts\\n+++ b/src/test-runner/core/flow-stage.ts\\n@@ -352,10 +352,30 @@ export class FlowStage {\\n       } catch {}\\n       // Prefer authoritative counts from res.statistics when available, then fall back to deltas/inferred\\n       const fromResStats: Record<string, number> = {};\\n+      const resSkipped: Record<string, boolean> = {};\\n       try {\\n         for (const s of (res.statistics?.checks || []) as any[]) {\\n           const n = (s && s.checkName) || '';\\n-          if (typeof n === 'string' && n) fromResStats[n] = (s.totalRuns || 0) as number;\\n+          if (typeof n === 'string' && n) {\\n+            fromResStats[n] = (s.totalRuns || 0) as number;\\n+            resSkipped[n] = !!(s as any).skipped || !!(s as any).skipReason;\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      // Steps that have explicit expectations in this stage. We will use\\n+      // this to avoid inflating run counts (via parent-alignment heuristics)\\n+      // for checks that are not even expected in this stage.\\n+      const expectedSteps = new Set<string>();\\n+      const expectedZero = new Set<string>();\\n+      try {\\n+        const expCalls = ((stage.expect || {}).calls || []) as Array<{ step?: string }>;\\n+        for (const c of expCalls) {\\n+          if (c && typeof c.step === 'string' && c.step) expectedSteps.add(c.step);\\n+          try {\\n+            if (c && typeof c.step === 'string' && c.step && (c as any).exactly === 0)\\n+              expectedZero.add(c.step);\\n+          } catch {}\\n         }\\n       } catch {}\\n \\n@@ -410,10 +430,15 @@ export class FlowStage {\\n           const base = stageOnly ? 0 : statBase[name] || 0;\\n           const d = Math.max(0, resTotal - base);\\n           runs = d > 0 ? d : 0;\\n+          // If the engine marked this check as skipped, force runs to 0\\n+          // so parent-alignment heuristics below do not inflate it.\\n+          if (resSkipped[name]) {\\n+            runs = 0;\\n+          }\\n         } else {\\n           runs = deltaMap[name] !== undefined ? deltaMap[name] : inferred;\\n         }\\n-        if (runs === 0 && presentInResults.has(name)) runs = 1;\\n+        if (runs === 0 && presentInResults.has(name) && !resSkipped[name]) runs = 1;\\n         if (!isForEachLike && histRuns > 0) runs = histRuns;\\n         // Only use per-item history counts for non-forEach checks. For forEach parents,\\n         // use aggregated totals from res.statistics to reflect number of parent executions.\\n@@ -442,7 +467,15 @@ export class FlowStage {\\n             parentMax = Math.max(parentMax, dP);\\n           }\\n           // Apply only for non-forEach checks with no observable history in this stage\\n-          if (!isForEachLike && histRuns === 0 && histPerItemRuns === 0 && parentMax > 0) {\\n+          if (\\n+            !isForEachLike &&\\n+            histRuns === 0 &&\\n+            histPerItemRuns === 0 &&\\n+            parentMax > 0 &&\\n+            !resSkipped[name] &&\\n+            (expectedSteps.size === 0 || expectedSteps.has(name)) &&\\n+            !expectedZero.has(name)\\n+          ) {\\n             runs = Math.max(runs, parentMax);\\n           }\\n         } catch {}\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/test-execution-wrapper.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/test-runner/core/test-execution-wrapper.ts b/src/test-runner/core/test-execution-wrapper.ts\\nindex 254d0970..d0333152 100644\\n--- a/src/test-runner/core/test-execution-wrapper.ts\\n+++ b/src/test-runner/core/test-execution-wrapper.ts\\n@@ -31,6 +31,8 @@ export class TestExecutionWrapper {\\n       const prev: any = (this.engine as any).executionContext || {};\\n       const merged = {\\n         ...prev,\\n+        // Inject workflow inputs for template access via {{ inputs.* }}\\n+        workflowInputs: cfg.workflow_inputs || prev.workflowInputs || {},\\n         mode: {\\n           ...(prev.mode || {}),\\n           test: true,\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/evaluators.ts\",\"additions\":67,\"deletions\":2,\"changes\":69,\"patch\":\"diff --git a/src/test-runner/evaluators.ts b/src/test-runner/evaluators.ts\\nindex 4843163a..75e9c174 100644\\n--- a/src/test-runner/evaluators.ts\\n+++ b/src/test-runner/evaluators.ts\\n@@ -252,6 +252,69 @@ export function evaluatePrompts(\\n   }\\n }\\n \\n+/**\\n+ * Evaluate workflow_output assertions against computed workflow outputs.\\n+ * Similar to evaluateOutputs but tests workflow-level outputs (defined in outputs: section)\\n+ * rather than step outputs.\\n+ */\\n+export function evaluateWorkflowOutputs(\\n+  errors: string[],\\n+  expect: ExpectBlock,\\n+  workflowOutputs: Record<string, unknown> | undefined\\n+): void {\\n+  const expectations = (expect as any).workflow_output;\\n+  if (!Array.isArray(expectations) || expectations.length === 0) return;\\n+  if (!workflowOutputs) {\\n+    errors.push('workflow_output assertions present but no workflow outputs computed');\\n+    return;\\n+  }\\n+\\n+  for (const o of expectations) {\\n+    const path = o.path as string;\\n+    if (!path) {\\n+      errors.push('workflow_output assertion missing path');\\n+      continue;\\n+    }\\n+    const v = deepGet(workflowOutputs, path);\\n+    if (o.equals !== undefined && !deepEqual(v, o.equals)) {\\n+      errors.push(\\n+        `Workflow output ${path} expected ${JSON.stringify(o.equals)} but got ${JSON.stringify(v)}`\\n+      );\\n+    }\\n+    if (o.equalsDeep !== undefined && !deepEqual(v, o.equalsDeep)) {\\n+      errors.push(`Workflow output ${path} deepEquals failed`);\\n+    }\\n+    if (o.matches && !parseRegex(o.matches).test(String(v))) {\\n+      errors.push(`Workflow output ${path} does not match ${o.matches}`);\\n+    }\\n+    if (o.contains) {\\n+      const contents = Array.isArray(o.contains) ? o.contains : [o.contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (!strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} expected to contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.not_contains) {\\n+      const contents = Array.isArray(o.not_contains) ? o.not_contains : [o.not_contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} should not contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.contains_unordered) {\\n+      if (!Array.isArray(v)) {\\n+        errors.push(`Workflow output ${path} not an array for contains_unordered`);\\n+      } else if (!containsUnordered(v as unknown[], o.contains_unordered)) {\\n+        errors.push(`Workflow output ${path} missing elements (unordered)`);\\n+      }\\n+    }\\n+  }\\n+}\\n+\\n export function evaluateOutputs(\\n   errors: string[],\\n   expect: ExpectBlock,\\n@@ -332,7 +395,8 @@ export function evaluateCase(\\n   strict: boolean,\\n   promptsByStep: Record<string, string[]>,\\n   _results: GroupedResults,\\n-  outputHistory: Record<string, unknown[]>\\n+  outputHistory: Record<string, unknown[]>,\\n+  workflowOutputs?: Record<string, unknown>\\n ): string[] {\\n   const errors: string[] = [];\\n   const executed = buildExecutedMap(stats);\\n@@ -350,5 +414,6 @@ export function evaluateCase(\\n   evaluateNoCalls(errors, expect, executed, recorder, slackRecorder);\\n   evaluatePrompts(errors, expect, promptsByStep);\\n   evaluateOutputs(errors, expect, outputHistory);\\n+  evaluateWorkflowOutputs(errors, expect, workflowOutputs);\\n   return errors;\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/index.ts\",\"additions\":112,\"deletions\":7,\"changes\":119,\"patch\":\"diff --git a/src/test-runner/index.ts b/src/test-runner/index.ts\\nindex 29916a16..4dcc3331 100644\\n--- a/src/test-runner/index.ts\\n+++ b/src/test-runner/index.ts\\n@@ -159,6 +159,7 @@ export async function runSuites(\\n   options: {\\n     only?: string;\\n     bail?: boolean;\\n+    noMocks?: boolean;\\n     maxParallelSuites?: number;\\n     maxParallel?: number;\\n     promptMaxChars?: number;\\n@@ -225,6 +226,7 @@ export async function runSuites(\\n       const r = await runner.runCases(fp, suite as TestSuite, {\\n         only: options.only,\\n         bail: options.bail,\\n+        noMocks: options.noMocks,\\n         maxParallel: options.maxParallel,\\n         promptMaxChars: options.promptMaxChars,\\n       });\\n@@ -285,7 +287,8 @@ export class VisorTestRunner {\\n     defaultPromptCap?: number,\\n     ghRec?: { error_code?: number; timeout_ms?: number },\\n     defaultIncludeTags?: string[] | undefined,\\n-    defaultExcludeTags?: string[] | undefined\\n+    defaultExcludeTags?: string[] | undefined,\\n+    noMocks?: boolean\\n   ): {\\n     name: string;\\n     strict: boolean;\\n@@ -399,9 +402,11 @@ export class VisorTestRunner {\\n               : info.prompt;\\n           prompts[k].push(p);\\n         },\\n-        mockForStep: (step: string) => mockMgr.get(step),\\n+        // In noMocks mode, always return undefined to let real providers execute\\n+        mockForStep: (step: string) => noMocks ? undefined : mockMgr.get(step),\\n         // Ensure human-input never blocks tests: prefer case mock, then default value\\n         onHumanInput: async (req: { checkId: string; default?: string }) => {\\n+          if (noMocks) return (req.default ?? '').toString();\\n           const m = mockMgr.get(req.checkId);\\n           if (m !== undefined && m !== null) return String(m);\\n           return (req.default ?? '').toString();\\n@@ -481,6 +486,60 @@ export class VisorTestRunner {\\n     if (!checks || checks.length === 0) return;\\n     console.log(`  checks: ${checks.join(', ')}`);\\n   }\\n+  /**\\n+   * Compute workflow outputs from output definitions and step results.\\n+   * Evaluates value_js and value (Liquid) expressions to produce output values.\\n+   */\\n+  private computeWorkflowOutputs(\\n+    outputDefs: Array<{ name: string; value?: string; value_js?: string }>,\\n+    outputHistory: Record<string, unknown[]>,\\n+    results: any[]\\n+  ): Record<string, unknown> {\\n+    const computed: Record<string, unknown> = {};\\n+    // Build a steps map from outputHistory (last output for each step)\\n+    const steps: Record<string, unknown> = {};\\n+    for (const [stepId, hist] of Object.entries(outputHistory)) {\\n+      if (Array.isArray(hist) && hist.length > 0) {\\n+        steps[stepId] = hist[hist.length - 1];\\n+      }\\n+    }\\n+    // Build outputs map from history (for {{ outputs[\\\"step\\\"] }} syntax)\\n+    const outputs: Record<string, unknown> = { ...steps };\\n+\\n+    for (const outputDef of outputDefs) {\\n+      if (!outputDef.name) continue;\\n+      try {\\n+        if (outputDef.value_js) {\\n+          // Evaluate JavaScript expression\\n+          const fn = new Function('steps', 'outputs', 'results', `\\n+            try {\\n+              ${outputDef.value_js}\\n+            } catch (e) {\\n+              return undefined;\\n+            }\\n+          `);\\n+          computed[outputDef.name] = fn(steps, outputs, results);\\n+        } else if (outputDef.value) {\\n+          // Evaluate Liquid template\\n+          const { Liquid } = require('liquidjs');\\n+          const engine = new Liquid();\\n+          const rendered = engine.parseAndRenderSync(outputDef.value, { steps, outputs, results });\\n+          // Try to parse as JSON, otherwise keep as string\\n+          try {\\n+            computed[outputDef.name] = JSON.parse(rendered);\\n+          } catch {\\n+            computed[outputDef.name] = rendered.trim();\\n+          }\\n+        }\\n+      } catch (e) {\\n+        // Skip outputs that fail to compute\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.log(`  ‚ö†Ô∏è Failed to compute output '${outputDef.name}': ${e}`);\\n+        }\\n+      }\\n+    }\\n+    return computed;\\n+  }\\n   /**\\n    * Locate a tests file: explicit path > ./.visor.tests.yaml > defaults/visor.tests.yaml\\n    */\\n@@ -606,7 +665,7 @@ export class VisorTestRunner {\\n   public async runCases(\\n     testsPath: string,\\n     suite: TestSuite,\\n-    options: { only?: string; bail?: boolean; maxParallel?: number; promptMaxChars?: number }\\n+    options: { only?: string; bail?: boolean; noMocks?: boolean; maxParallel?: number; promptMaxChars?: number }\\n   ): Promise<{\\n     failures: number;\\n     results: Array<{\\n@@ -748,9 +807,14 @@ export class VisorTestRunner {\\n     const __keepAlive = setInterval(() => {}, 1000);\\n     // Header: show suite path for clarity\\n     let __suiteRel = testsPath;\\n+    const noMocksMode = options.noMocks || false;\\n     try {\\n       __suiteRel = path.relative(this.cwd, testsPath) || testsPath;\\n       console.log(`Suite: ${__suiteRel}`);\\n+      if (noMocksMode) {\\n+        console.log(this.color('üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)', '33'));\\n+        console.log(this.gray('   Step outputs will be captured and printed as suggested mocks\\\\n'));\\n+      }\\n     } catch {}\\n     const runOne = async (_case: any): Promise<{ name: string; failed: number }> => {\\n       // Case header for clarity\\n@@ -805,6 +869,11 @@ export class VisorTestRunner {\\n           cfgLocal.checks[name] = chk;\\n         }\\n       }\\n+      // Workflow testing: inject workflow_input into config for template access\\n+      const workflowInput = (_case as any).workflow_input;\\n+      if (workflowInput && typeof workflowInput === 'object') {\\n+        (cfgLocal as any).workflow_inputs = workflowInput;\\n+      }\\n       const setup = this.setupTestCase(\\n         _case,\\n         cfgLocal,\\n@@ -812,7 +881,8 @@ export class VisorTestRunner {\\n         defaultPromptCap,\\n         ghRec,\\n         defaultIncludeTags,\\n-        defaultExcludeTags\\n+        defaultExcludeTags,\\n+        noMocksMode\\n       );\\n       try {\\n         this.printSelectedChecks(setup.checksToRun);\\n@@ -826,6 +896,24 @@ export class VisorTestRunner {\\n         } catch {}\\n         const exec = await this.executeTestCase(setup, cfgLocal);\\n         const res = exec.res;\\n+\\n+        // In no-mocks mode, print captured outputs as suggested mocks\\n+        if (noMocksMode && Object.keys(exec.outHistory).length > 0) {\\n+          console.log(this.color('\\\\nüìã Suggested mocks (copy to your test case):', '36'));\\n+          console.log(this.gray('mocks:'));\\n+          for (const [stepName, outputs] of Object.entries(exec.outHistory)) {\\n+            if (!Array.isArray(outputs) || outputs.length === 0) continue;\\n+            // Use the last output for the step\\n+            const lastOutput = outputs[outputs.length - 1];\\n+            // Format as YAML with proper indentation\\n+            const yamlOutput = yaml.dump({ [stepName]: lastOutput }, { indent: 2, lineWidth: 120, noRefs: true });\\n+            // Indent the YAML output for proper nesting under 'mocks:'\\n+            const indented = yamlOutput.split('\\\\n').map(line => '  ' + line).join('\\\\n');\\n+            console.log(indented);\\n+          }\\n+          console.log('');\\n+        }\\n+\\n         if (process.env.VISOR_DEBUG === 'true') {\\n           try {\\n             const names = (res.statistics.checks || []).map(\\n@@ -836,6 +924,21 @@ export class VisorTestRunner {\\n         }\\n         // avoid printing raw history keys each case\\n         // (fallback for on_finish static targets handled inside executeTestCase)\\n+        // Workflow testing: compute workflow outputs if workflow has outputs defined\\n+        let workflowOutputs: Record<string, unknown> | undefined;\\n+        if (Array.isArray((cfgLocal as any).outputs)) {\\n+          try {\\n+            workflowOutputs = this.computeWorkflowOutputs(\\n+              (cfgLocal as any).outputs,\\n+              exec.outHistory,\\n+              res.results\\n+            );\\n+          } catch (e) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ‚ö†Ô∏è Error computing workflow outputs: ${e}`);\\n+            }\\n+          }\\n+        }\\n         const caseFailures = require('./evaluators').evaluateCase(\\n           _case.name,\\n           res.statistics,\\n@@ -845,7 +948,8 @@ export class VisorTestRunner {\\n           setup.strict,\\n           setup.prompts,\\n           res.results,\\n-          exec.outHistory\\n+          exec.outHistory,\\n+          workflowOutputs\\n         );\\n         // Warn about unmocked AI/command steps that executed\\n         try {\\n@@ -1280,7 +1384,8 @@ export class VisorTestRunner {\\n   ): void {\\n     const executed: Record<string, number> = {};\\n     for (const s of stats.checks) {\\n-      if (!s.skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n+      const skipped = (s as any).skipped === true || !!(s as any).skipReason;\\n+      if (!skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n     }\\n     const expCalls = (expect.calls || []).filter(c => c.step);\\n     const expectedSteps = new Map<\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/validator.ts\",\"additions\":48,\"deletions\":2,\"changes\":50,\"patch\":\"diff --git a/src/test-runner/validator.ts b/src/test-runner/validator.ts\\nindex dbb890c5..5ffff5ee 100644\\n--- a/src/test-runner/validator.ts\\n+++ b/src/test-runner/validator.ts\\n@@ -25,6 +25,16 @@ const schema: any = {\\n     hooks: { type: 'object' },\\n     slack: { type: 'object' },\\n     frontends: { type: 'array' },\\n+    workspace: { type: 'object' },\\n+    // Workflow definition fields (for workflow files with co-located tests)\\n+    id: { type: 'string' },\\n+    name: { type: 'string' },\\n+    description: { type: 'string' },\\n+    inputs: { type: 'array' },\\n+    outputs: { type: 'array' },\\n+    ai_mcp_servers: { type: 'object' },\\n+    ai_provider: { type: 'string' },\\n+    ai_model: { type: 'string' },\\n     tests: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -136,6 +146,8 @@ const schema: any = {\\n             oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n           },\\n         },\\n+        // Workflow testing: input values to pass to the workflow\\n+        workflow_input: { type: 'object' },\\n         expect: { $ref: '#/$defs/expectBlock' },\\n         // Flow cases\\n         flow: {\\n@@ -270,6 +282,35 @@ const schema: any = {\\n       },\\n       required: ['step', 'path'],\\n     },\\n+    // Workflow output expectation: like outputsExpectation but without step (tests workflow-level outputs)\\n+    workflowOutputExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        path: { type: 'string' },\\n+        equals: {},\\n+        equalsDeep: {},\\n+        matches: { type: 'string' },\\n+        contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        not_contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            path: { type: 'string' },\\n+            equals: {},\\n+            matches: { type: 'string' },\\n+          },\\n+          required: ['path'],\\n+        },\\n+        contains_unordered: { type: 'array' },\\n+      },\\n+      required: ['path'],\\n+    },\\n     expectBlock: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -278,6 +319,8 @@ const schema: any = {\\n         calls: { type: 'array', items: { $ref: '#/$defs/callsExpectation' } },\\n         prompts: { type: 'array', items: { $ref: '#/$defs/promptsExpectation' } },\\n         outputs: { type: 'array', items: { $ref: '#/$defs/outputsExpectation' } },\\n+        // Workflow testing: assert on workflow-level outputs (defined in workflow's outputs: section)\\n+        workflow_output: { type: 'array', items: { $ref: '#/$defs/workflowOutputExpectation' } },\\n         no_calls: {\\n           type: 'array',\\n           items: {\\n@@ -343,6 +386,7 @@ const knownKeys = new Set([\\n   'version',\\n   'extends',\\n   'tests',\\n+  'workspace',\\n   // tests\\n   'tests.defaults',\\n   'tests.fixtures',\\n@@ -363,6 +407,7 @@ const knownKeys = new Set([\\n   'env',\\n   'routing',\\n   'mocks',\\n+  'workflow_input',\\n   'expect',\\n   'flow',\\n   // expect\\n@@ -370,6 +415,7 @@ const knownKeys = new Set([\\n   'expect.calls',\\n   'expect.prompts',\\n   'expect.outputs',\\n+  'expect.workflow_output',\\n   'expect.no_calls',\\n   'expect.fail',\\n   'expect.strict_violation',\\n@@ -424,7 +470,7 @@ function formatError(e: ErrorObject): string {\\n       if (hint) msg += ` (${hint})`;\\n       // Small curated allow-list for frequent nodes to reduce guesswork\\n       if (path.endsWith('expect')) {\\n-        msg += ` (allowed: use, calls, prompts, outputs, no_calls, fail, strict_violation)`;\\n+        msg += ` (allowed: use, calls, prompts, outputs, workflow_output, no_calls, fail, strict_violation)`;\\n       } else if (path.endsWith('env')) {\\n         msg += ` (values must be strings)`;\\n       } else if (path.endsWith('tests')) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/cli.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/cli.ts b/src/types/cli.ts\\nindex a7d9ca30..f06f93d5 100644\\n--- a/src/types/cli.ts\\n+++ b/src/types/cli.ts\\n@@ -61,6 +61,8 @@ export interface CliOptions {\\n   githubV2?: boolean;\\n   /** Enable Slack Socket Mode runner */\\n   slack?: boolean;\\n+  /** Keep workspace folders after execution (for debugging) */\\n+  keepWorkspace?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/config.ts\",\"additions\":114,\"deletions\":5,\"changes\":119,\"patch\":\"diff --git a/src/types/config.ts b/src/types/config.ts\\nindex ed239959..dd363de1 100644\\n--- a/src/types/config.ts\\n+++ b/src/types/config.ts\\n@@ -122,6 +122,9 @@ export interface FailureConditionContext {\\n   /** Environment variables */\\n   env?: Record<string, string>;\\n \\n+  /** Workflow inputs - values passed to the workflow */\\n+  inputs?: Record<string, unknown>;\\n+\\n   /** Memory accessor for accessing memory store in expressions */\\n   memory?: {\\n     get: (key: string, namespace?: string) => unknown;\\n@@ -356,6 +359,8 @@ export interface AIProviderConfig {\\n   allowBash?: boolean;\\n   /** Advanced bash command execution configuration */\\n   bashConfig?: BashConfig;\\n+  /** Completion prompt for post-completion validation/review (runs after attempt_completion) */\\n+  completion_prompt?: string;\\n }\\n \\n /**\\n@@ -461,8 +466,8 @@ export interface CheckConfig {\\n   env?: EnvConfig;\\n   /** Timeout in seconds for command execution (default: 60) */\\n   timeout?: number;\\n-  /** Check IDs that this check depends on (optional) */\\n-  depends_on?: string[];\\n+  /** Check IDs that this check depends on (optional). Accepts single string or array. */\\n+  depends_on?: string | string[];\\n   /** Group name for comment separation (e.g., \\\"code-review\\\", \\\"pr-overview\\\") - optional */\\n   group?: string;\\n   /** Schema type for template rendering (e.g., \\\"code-review\\\", \\\"markdown\\\") or inline JSON schema object - optional */\\n@@ -621,6 +626,46 @@ export interface CheckConfig {\\n   overrides?: Record<string, Partial<CheckConfig>>;\\n   /** Map workflow outputs to check outputs */\\n   output_mapping?: Record<string, string>;\\n+\\n+  // Workflow aliases for backward compatibility\\n+  /** Alias for args - workflow inputs (backward compatibility) */\\n+  workflow_inputs?: Record<string, unknown>;\\n+  /** Config file path - alternative to workflow ID (loads a Visor config file as workflow) */\\n+  config?: string;\\n+  /** Alias for overrides - workflow step overrides (backward compatibility) */\\n+  workflow_overrides?: Record<string, Partial<CheckConfig>>;\\n+\\n+  /**\\n+   * Git-checkout provider specific options (optional, only used when type === 'git-checkout').\\n+   */\\n+  /** Git reference to checkout (branch, tag, commit SHA) - supports templates */\\n+  ref?: string;\\n+  /** Repository URL or owner/repo format (defaults to current repository) */\\n+  repository?: string;\\n+  /** GitHub token for private repositories (defaults to GITHUB_TOKEN env) */\\n+  token?: string;\\n+  /** Number of commits to fetch (0 for full history, default: 1) */\\n+  fetch_depth?: number;\\n+  /** Whether to fetch tags (default: false) */\\n+  fetch_tags?: boolean;\\n+  /** Checkout submodules: false, true, or 'recursive' */\\n+  submodules?: boolean | 'recursive';\\n+  /** Working directory for the checkout (defaults to temp directory) */\\n+  working_directory?: string;\\n+  /** Use git worktree for efficient parallel checkouts (default: true) */\\n+  use_worktree?: boolean;\\n+  /** Clean the working directory before checkout (default: true) */\\n+  clean?: boolean;\\n+  /** Sparse checkout paths - only checkout specific directories/files */\\n+  sparse_checkout?: string[];\\n+  /** Enable Git LFS (Large File Storage) */\\n+  lfs?: boolean;\\n+  /** Timeout in ms for cloning the bare repository (default: 300000 = 5 min) */\\n+  clone_timeout_ms?: number;\\n+  /** Clean up worktree on failure (default: true) */\\n+  cleanup_on_failure?: boolean;\\n+  /** Keep worktree after workflow completion (default: false) */\\n+  persist_worktree?: boolean;\\n }\\n \\n /**\\n@@ -1037,6 +1082,36 @@ export interface CustomToolDefinition {\\n   outputSchema?: Record<string, unknown>;\\n }\\n \\n+/**\\n+ * Workflow input definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowInput {\\n+  /** Input parameter name */\\n+  name: string;\\n+  /** JSON Schema for the input */\\n+  schema?: Record<string, unknown>;\\n+  /** Whether this input is required */\\n+  required?: boolean;\\n+  /** Default value if not provided */\\n+  default?: unknown;\\n+  /** Human-readable description */\\n+  description?: string;\\n+}\\n+\\n+/**\\n+ * Workflow output definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowOutput {\\n+  /** Output name */\\n+  name: string;\\n+  /** Human-readable description */\\n+  description?: string;\\n+  /** Value using Liquid template syntax (references step outputs) */\\n+  value?: string;\\n+  /** Value using JavaScript expression (alternative to value) */\\n+  value_js?: string;\\n+}\\n+\\n /**\\n  * Main Visor configuration\\n  */\\n@@ -1051,12 +1126,16 @@ export interface VisorConfig {\\n   tools?: Record<string, CustomToolDefinition>;\\n   /** Import workflow definitions from external files or URLs */\\n   imports?: string[];\\n+  /** Workflow inputs (for standalone reusable workflows) */\\n+  inputs?: WorkflowInput[];\\n+  /** Workflow outputs (for standalone reusable workflows) */\\n+  outputs?: WorkflowOutput[];\\n   /** Step configurations (recommended) */\\n   steps?: Record<string, CheckConfig>;\\n   /** Check configurations (legacy, use 'steps' instead) - always populated after normalization */\\n   checks?: Record<string, CheckConfig>;\\n-  /** Output configuration */\\n-  output: OutputConfig;\\n+  /** Output configuration (optional - defaults provided) */\\n+  output?: OutputConfig;\\n   /** HTTP server configuration for receiving webhooks */\\n   http_server?: HttpServerConfig;\\n   /** Memory storage configuration */\\n@@ -1092,6 +1171,36 @@ export interface VisorConfig {\\n     /** Frontend-specific configuration */\\n     config?: unknown;\\n   }>;\\n+  /** Workspace isolation configuration for sandboxed execution */\\n+  workspace?: WorkspaceConfig;\\n+  /** Slack configuration */\\n+  slack?: SlackConfig;\\n+}\\n+\\n+/**\\n+ * Workspace isolation configuration\\n+ */\\n+export interface WorkspaceConfig {\\n+  /** Enable workspace isolation (default: true when config present) */\\n+  enabled?: boolean;\\n+  /** Base path for workspaces (default: /tmp/visor-workspaces) */\\n+  base_path?: string;\\n+  /** Clean up workspace on exit (default: true) */\\n+  cleanup_on_exit?: boolean;\\n+}\\n+\\n+/**\\n+ * Slack configuration\\n+ */\\n+export interface SlackConfig {\\n+  /** Slack API version */\\n+  version?: string;\\n+  /** Mention handling: 'all', 'direct', etc. */\\n+  mentions?: string;\\n+  /** Thread handling: 'required', 'optional', etc. */\\n+  threads?: string;\\n+  /** Show raw output in Slack responses */\\n+  show_raw_output?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/git-checkout.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/git-checkout.ts b/src/types/git-checkout.ts\\nindex eb1c5ebc..4d27855d 100644\\n--- a/src/types/git-checkout.ts\\n+++ b/src/types/git-checkout.ts\\n@@ -16,6 +16,8 @@ export interface GitCheckoutConfig {\\n   fetch_depth?: number;\\n   fetch_tags?: boolean;\\n   submodules?: boolean | 'recursive';\\n+  /** Timeout (ms) for cloning the bare repository; defaults to 300000 (5 minutes) */\\n+  clone_timeout_ms?: number;\\n \\n   // Worktree configuration\\n   working_directory?: string;\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/sandbox.ts\",\"additions\":46,\"deletions\":1,\"changes\":47,\"patch\":\"diff --git a/src/utils/sandbox.ts b/src/utils/sandbox.ts\\nindex 9914a3a5..12e27c30 100644\\n--- a/src/utils/sandbox.ts\\n+++ b/src/utils/sandbox.ts\\n@@ -14,6 +14,51 @@ export interface CompileOptions {\\n   wrapFunction?: boolean;\\n }\\n \\n+export interface JsSyntaxValidationResult {\\n+  valid: boolean;\\n+  error?: string;\\n+}\\n+\\n+/**\\n+ * Validate JavaScript syntax without executing it.\\n+ * Uses the sandbox's compile method to check for syntax errors.\\n+ * Returns validation result with error message if invalid.\\n+ */\\n+export function validateJsSyntax(code: string): JsSyntaxValidationResult {\\n+  if (!code || typeof code !== 'string') {\\n+    return { valid: false, error: 'Code must be a non-empty string' };\\n+  }\\n+\\n+  const trimmed = code.trim();\\n+  if (trimmed.length === 0) {\\n+    return { valid: false, error: 'Code cannot be empty' };\\n+  }\\n+\\n+  // Create a minimal sandbox instance for syntax checking\\n+  const sandbox = createSecureSandbox();\\n+\\n+  // Wrap code similar to compileAndRun to catch the same syntax issues\\n+  const looksLikeBlock = /\\\\breturn\\\\b/.test(trimmed) || /;/.test(trimmed) || /\\\\n/.test(trimmed);\\n+  const looksLikeIife = /\\\\)\\\\s*\\\\(\\\\s*\\\\)\\\\s*;?$/.test(trimmed);\\n+  const body = looksLikeBlock\\n+    ? looksLikeIife\\n+      ? `return (\\\\n${trimmed}\\\\n);\\\\n`\\n+      : `return (() => {\\\\n${trimmed}\\\\n})();\\\\n`\\n+    : `return (\\\\n${trimmed}\\\\n);\\\\n`;\\n+\\n+  // Add log injection header (same as compileAndRun)\\n+  const header = `const __lp = \\\"[syntax-check]\\\"; const log = (...a) => { try { console.log(__lp, ...a); } catch {} };\\\\n`;\\n+  const fullCode = `${header}${body}`;\\n+\\n+  try {\\n+    sandbox.compile(fullCode);\\n+    return { valid: true };\\n+  } catch (e) {\\n+    const msg = e instanceof Error ? e.message : String(e);\\n+    return { valid: false, error: msg };\\n+  }\\n+}\\n+\\n /**\\n  * Create a hardened Sandbox with a consistent set of globals and prototype\\n  * whitelists. This is a superset of the sets previously used by individual\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/tracer-init.ts\",\"additions\":19,\"deletions\":1,\"changes\":20,\"patch\":\"diff --git a/src/utils/tracer-init.ts b/src/utils/tracer-init.ts\\nindex b42995d9..4edf1ffc 100644\\n--- a/src/utils/tracer-init.ts\\n+++ b/src/utils/tracer-init.ts\\n@@ -76,6 +76,24 @@ export async function initializeTracer(\\n \\n       const tracer = new SimpleAppTracer(telemetry, sessionId);\\n \\n+      // WORKAROUND: Add missing recordEvent method for completionPrompt feature (probe #321)\\n+      // SimpleAppTracer doesn't have recordEvent but completionPrompt requires it\\n+      if (typeof (tracer as any).recordEvent !== 'function') {\\n+        (tracer as any).recordEvent = (\\n+          name: string,\\n+          attributes?: Record<string, unknown>\\n+        ) => {\\n+          // Log completion events to telemetry for debugging\\n+          try {\\n+            if ((telemetry as any).record) {\\n+              (telemetry as any).record({ event: name, ...attributes });\\n+            }\\n+          } catch {\\n+            // Best-effort only\\n+          }\\n+        };\\n+      }\\n+\\n       console.error(`üìä Simple tracing enabled, will save to: ${traceFilePath}`);\\n \\n       // If in GitHub Actions, log the path for artifact upload\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/worktree-manager.ts\",\"additions\":142,\"deletions\":27,\"changes\":169,\"patch\":\"diff --git a/src/utils/worktree-manager.ts b/src/utils/worktree-manager.ts\\nindex f3cebf3d..2de7a9f4 100644\\n--- a/src/utils/worktree-manager.ts\\n+++ b/src/utils/worktree-manager.ts\\n@@ -6,6 +6,7 @@\\n  */\\n \\n import * as fs from 'fs';\\n+import * as fsp from 'fs/promises';\\n import * as path from 'path';\\n import * as crypto from 'crypto';\\n import { commandExecutor } from './command-executor';\\n@@ -119,7 +120,8 @@ export class WorktreeManager {\\n     repository: string,\\n     repoUrl: string,\\n     token?: string,\\n-    fetchDepth?: number\\n+    fetchDepth?: number,\\n+    cloneTimeoutMs?: number\\n   ): Promise<string> {\\n     const reposDir = this.getReposDir();\\n     const repoName = repository.replace(/\\\\//g, '-');\\n@@ -129,9 +131,21 @@ export class WorktreeManager {\\n     if (fs.existsSync(bareRepoPath)) {\\n       logger.debug(`Bare repository already exists: ${bareRepoPath}`);\\n \\n-      // Update remote refs\\n-      await this.updateBareRepo(bareRepoPath);\\n-      return bareRepoPath;\\n+      // Verify the bare repo has the correct remote URL to prevent using corrupted repos\\n+      const verifyResult = await this.verifyBareRepoRemote(bareRepoPath, repoUrl);\\n+      if (verifyResult === 'timeout') {\\n+        // Timeout during verification - use stale cache to avoid hanging on re-clone\\n+        logger.info(`Using stale bare repository (verification timed out): ${bareRepoPath}`);\\n+        return bareRepoPath;\\n+      } else if (verifyResult === false) {\\n+        logger.warn(`Bare repository at ${bareRepoPath} has incorrect remote, removing and re-cloning`);\\n+        await fsp.rm(bareRepoPath, { recursive: true, force: true });\\n+        // Fall through to clone below\\n+      } else {\\n+        // Update remote refs\\n+        await this.updateBareRepo(bareRepoPath);\\n+        return bareRepoPath;\\n+      }\\n     }\\n \\n     // Clone as bare repository\\n@@ -153,7 +167,9 @@ export class WorktreeManager {\\n     }\\n     cloneCmd += ` ${this.escapeShellArg(cloneUrl)} ${this.escapeShellArg(bareRepoPath)}`;\\n \\n-    const result = await this.executeGitCommand(cloneCmd, { timeout: 300000 }); // 5 minute timeout\\n+    const result = await this.executeGitCommand(cloneCmd, {\\n+      timeout: cloneTimeoutMs || 300000, // default 5 minutes\\n+    });\\n \\n     if (result.exitCode !== 0) {\\n       // Redact tokens from error messages\\n@@ -171,19 +187,98 @@ export class WorktreeManager {\\n   private async updateBareRepo(bareRepoPath: string): Promise<void> {\\n     logger.debug(`Updating bare repository: ${bareRepoPath}`);\\n \\n-    const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n-    const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+    try {\\n+      const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n+      const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n+        // Don't throw - we can continue with stale refs\\n+      } else {\\n+        logger.debug(`Successfully updated bare repository`);\\n+      }\\n+    } catch (error) {\\n+      // Handle timeout or other errors gracefully - we can continue with stale refs\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      logger.warn(`Failed to update bare repository (will use stale refs): ${errorMessage}`);\\n+    }\\n+  }\\n \\n-    if (result.exitCode !== 0) {\\n-      logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n-      // Don't throw - we can continue with stale refs\\n-    } else {\\n-      logger.debug(`Successfully updated bare repository`);\\n+  /**\\n+   * Verify that a bare repository has the correct remote URL.\\n+   * This prevents reusing corrupted repos that were cloned from a different repository.\\n+   * Returns: true (valid), false (invalid - should re-clone), or 'timeout' (use stale cache)\\n+   */\\n+  private async verifyBareRepoRemote(\\n+    bareRepoPath: string,\\n+    expectedUrl: string\\n+  ): Promise<boolean | 'timeout'> {\\n+    try {\\n+      const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote get-url origin`;\\n+      const result = await this.executeGitCommand(cmd, { timeout: 10000 });\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to get remote URL for ${bareRepoPath}: ${result.stderr}`);\\n+        return false;\\n+      }\\n+\\n+      const actualUrl = result.stdout.trim();\\n+\\n+      // Normalize URLs for comparison:\\n+      // - remove credentials (tokens/username)\\n+      // - remove .git suffix if present\\n+      // - trim trailing slash\\n+      // - lowercase for case‚Äëinsensitive match\\n+      const normalizeUrl = (url: string): string => {\\n+        // Convert common SSH form to https for comparison\\n+        if (url.startsWith('git@github.com:')) {\\n+          url = url.replace('git@github.com:', 'https://github.com/');\\n+        }\\n+        return url\\n+          // strip userinfo part to avoid mismatches when the cached bare\\n+          // repo was cloned with an access token but the expected URL is\\n+          // tokenless (or vice‚Äëversa)\\n+          .replace(/:\\\\/\\\\/[^@]+@/, '://')\\n+          .replace(/\\\\.git$/, '')\\n+          .replace(/\\\\/$/, '')\\n+          .toLowerCase();\\n+      };\\n+\\n+      const normalizedExpected = normalizeUrl(expectedUrl);\\n+      const normalizedActual = normalizeUrl(actualUrl);\\n+\\n+      if (normalizedExpected !== normalizedActual) {\\n+        logger.warn(\\n+          `Bare repo remote mismatch: expected ${expectedUrl}, got ${actualUrl}`\\n+        );\\n+        return false;\\n+      }\\n+\\n+      logger.debug(`Bare repo remote verified: ${actualUrl}`);\\n+      return true;\\n+    } catch (error) {\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      // If it's a timeout, return 'timeout' so caller can use stale cache instead of re-cloning\\n+      if (errorMessage.includes('timed out')) {\\n+        logger.warn(`Timeout verifying bare repo remote (will use stale cache): ${errorMessage}`);\\n+        return 'timeout';\\n+      }\\n+      logger.warn(`Error verifying bare repo remote: ${error}`);\\n+      return false;\\n     }\\n   }\\n \\n   /**\\n-   * Create a new worktree\\n+   * Create a new worktree for the given repository/ref.\\n+   *\\n+   * Important: we always create worktrees in a detached HEAD state pinned\\n+   * to a specific commit SHA rather than a named branch like \\\"main\\\". Git\\n+   * only allows a branch to be checked out in a single worktree at a time;\\n+   * using the raw commit (plus --detach) lets multiple workflows safely\\n+   * create independent worktrees for the same branch without hitting\\n+   * errors like:\\n+   *\\n+   *   fatal: 'main' is already used by worktree at '.../TykTechnologies-tyk-docs-main-XXXX'\\n    */\\n   async createWorktree(\\n     repository: string,\\n@@ -195,6 +290,7 @@ export class WorktreeManager {\\n       clean?: boolean;\\n       workflowId?: string;\\n       fetchDepth?: number;\\n+      cloneTimeoutMs?: number;\\n     } = {}\\n   ): Promise<WorktreeInfo> {\\n     // Validate ref to prevent command injection\\n@@ -205,7 +301,8 @@ export class WorktreeManager {\\n       repository,\\n       repoUrl,\\n       options.token,\\n-      options.fetchDepth\\n+      options.fetchDepth,\\n+      options.cloneTimeoutMs\\n     );\\n \\n     // Generate worktree ID and path\\n@@ -241,21 +338,24 @@ export class WorktreeManager {\\n       }\\n     }\\n \\n-    // Fetch the ref if needed\\n+    // Fetch the ref if needed, then resolve it to a concrete commit SHA.\\n+    // We use the commit (detached HEAD) instead of the branch name so we\\n+    // can have multiple worktrees for the same branch without git refusing\\n+    // with \\\"branch X is already checked out\\\".\\n     await this.fetchRef(bareRepoPath, ref);\\n+    const commit = await this.getCommitShaForRef(bareRepoPath, ref);\\n \\n-    // Create worktree\\n-    logger.info(`Creating worktree for ${repository}@${ref}`);\\n-    const createCmd = `git -C ${this.escapeShellArg(bareRepoPath)} worktree add ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(ref)}`;\\n+    // Create worktree in detached HEAD state at the resolved commit\\n+    logger.info(`Creating worktree for ${repository}@${ref} (${commit})`);\\n+    const createCmd = `git -C ${this.escapeShellArg(\\n+      bareRepoPath\\n+    )} worktree add --detach ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(commit)}`;\\n     const result = await this.executeGitCommand(createCmd, { timeout: 60000 });\\n \\n     if (result.exitCode !== 0) {\\n       throw new Error(`Failed to create worktree: ${result.stderr}`);\\n     }\\n \\n-    // Get commit SHA\\n-    const commit = await this.getCommitSha(worktreePath);\\n-\\n     // Create metadata\\n     const metadata: WorktreeMetadata = {\\n       worktree_id: worktreeId,\\n@@ -316,14 +416,17 @@ export class WorktreeManager {\\n   }\\n \\n   /**\\n-   * Get commit SHA for worktree\\n+   * Get commit SHA for a given ref inside a bare repository.\\n+   *\\n+   * This runs after fetchRef so that <ref> should resolve to either a\\n+   * local branch, tag, or remote-tracking ref.\\n    */\\n-  private async getCommitSha(worktreePath: string): Promise<string> {\\n-    const cmd = `git -C ${this.escapeShellArg(worktreePath)} rev-parse HEAD`;\\n+  private async getCommitShaForRef(bareRepoPath: string, ref: string): Promise<string> {\\n+    const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} rev-parse ${this.escapeShellArg(ref)}`;\\n     const result = await this.executeGitCommand(cmd);\\n \\n     if (result.exitCode !== 0) {\\n-      throw new Error(`Failed to get commit SHA: ${result.stderr}`);\\n+      throw new Error(`Failed to get commit SHA for ref ${ref}: ${result.stderr}`);\\n     }\\n \\n     return result.stdout.trim();\\n@@ -621,9 +724,21 @@ export class WorktreeManager {\\n     command: string,\\n     options: { timeout?: number; env?: Record<string, string> } = {}\\n   ): Promise<GitCommandResult> {\\n+    // Merge provided env with process.env and add git-specific settings\\n+    // These settings prevent git from hanging on interactive prompts while\\n+    // still allowing OS-level credential helpers to work:\\n+    // - GIT_TERMINAL_PROMPT=0: Prevents terminal credential prompts\\n+    // - GIT_SSH_COMMAND: Disables SSH password prompts (BatchMode)\\n+    const gitEnv = {\\n+      ...process.env,\\n+      ...options.env,\\n+      GIT_TERMINAL_PROMPT: '0',\\n+      GIT_SSH_COMMAND: 'ssh -o BatchMode=yes -o StrictHostKeyChecking=no',\\n+    };\\n+\\n     const result = await commandExecutor.execute(command, {\\n       timeout: options.timeout || 30000,\\n-      env: options.env || process.env,\\n+      env: gitEnv,\\n     } as any);\\n \\n     return {\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-executor.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-executor.ts b/src/workflow-executor.ts\\nindex 751e7f16..df76762f 100644\\n--- a/src/workflow-executor.ts\\n+++ b/src/workflow-executor.ts\\n@@ -219,7 +219,9 @@ export class WorkflowExecutor {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps)) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     // Use static DependencyResolver\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-registry.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-registry.ts b/src/workflow-registry.ts\\nindex 2cbe3a89..6bd855fb 100644\\n--- a/src/workflow-registry.ts\\n+++ b/src/workflow-registry.ts\\n@@ -410,7 +410,9 @@ export class WorkflowRegistry {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps || {})) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     try {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/ai-review-service.test.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/tests/unit/ai-review-service.test.ts b/tests/unit/ai-review-service.test.ts\\nindex ec2b998a..2c616c99 100644\\n--- a/tests/unit/ai-review-service.test.ts\\n+++ b/tests/unit/ai-review-service.test.ts\\n@@ -482,6 +482,37 @@ describe('AIReviewService', () => {\\n       expect(context).toContain('<additions>10</additions>');\\n       expect(context).toContain('<deletions>5</deletions>');\\n     });\\n+\\n+    it('should exclude diffs by default when in Slack mode', async () => {\\n+      // Slack mode is detected by presence of slackConversation property\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // In Slack mode, diffs should be excluded by default\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+      // Should still include file summary\\n+      expect(context).toContain('<files_summary>');\\n+    });\\n+\\n+    it('should include diffs in Slack mode when explicitly requested', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      (mockPRInfo as any).includeCodeContext = true;\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // With explicit includeCodeContext: true, diffs should be included\\n+      expect(context).toContain('<full_diff>');\\n+      expect(context).toContain('--- test.ts');\\n+    });\\n+\\n+    it('should exclude diffs in Slack mode even when includeCodeContext is undefined', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      // Deliberately not setting includeCodeContext to test default behavior\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+    });\\n   });\\n \\n   describe('Comment Filtering for Code Review', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config-extends.test.ts\",\"additions\":8,\"deletions\":8,\"changes\":16,\"patch\":\"diff --git a/tests/unit/config-extends.test.ts b/tests/unit/config-extends.test.ts\\nindex b112a869..7ee69ce6 100644\\n--- a/tests/unit/config-extends.test.ts\\n+++ b/tests/unit/config-extends.test.ts\\n@@ -564,7 +564,7 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.security).toBeDefined();\\n       expect(config.checks!.performance).toBeDefined();\\n       expect(config.checks!.custom).toBeDefined();\\n-      expect(config.output.pr_comment.format).toBe('json');\\n+      expect(config.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should extend from default configuration', async () => {\\n@@ -1002,7 +1002,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.base).toBeDefined(); // From base\\n       expect(loaded.checks!.middle).toBeDefined(); // From middle\\n       expect(loaded.checks!.top).toBeDefined(); // From top\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From top\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From top\\n     });\\n \\n     it('should handle mixed local and remote extends', async () => {\\n@@ -1079,7 +1079,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.local).toBeDefined();\\n       expect(loaded.checks!.remote).toBeDefined();\\n       expect(loaded.checks!.child).toBeDefined();\\n-      expect(loaded.output.pr_comment.format).toBe('json');\\n+      expect(loaded.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should handle extends with relative path resolution', async () => {\\n@@ -1593,9 +1593,9 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.performance).toBeDefined(); // Added in child\\n \\n       // Verify output\\n-      expect(config.output.pr_comment.format).toBe('json'); // Overridden\\n-      expect(config.output.pr_comment.group_by).toBe('file'); // Overridden\\n-      expect(config.output.pr_comment.collapse).toBe(false); // Overridden\\n+      expect(config.output!.pr_comment.format).toBe('json'); // Overridden\\n+      expect(config.output!.pr_comment.group_by).toBe('file'); // Overridden\\n+      expect(config.output!.pr_comment.collapse).toBe(false); // Overridden\\n     });\\n \\n     it('should handle configuration with disabled checks removal', async () => {\\n@@ -1814,7 +1814,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.root).toBeDefined(); // From root\\n       expect(loaded.checks!.level1).toBeDefined(); // From level1\\n       expect(loaded.checks!.level2).toBeDefined(); // From level2\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From level2\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From level2\\n     });\\n \\n     it('should handle mixed absolute and relative paths in extends array', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config.test.ts\",\"additions\":7,\"deletions\":7,\"changes\":14,\"patch\":\"diff --git a/tests/unit/config.test.ts b/tests/unit/config.test.ts\\nindex 67ce61fd..0677a3d4 100644\\n--- a/tests/unit/config.test.ts\\n+++ b/tests/unit/config.test.ts\\n@@ -48,7 +48,7 @@ output:\\n       expect(config.version).toBe('1.0');\\n       expect(config.checks).toHaveProperty('performance');\\n       expect(config.checks).toHaveProperty('security');\\n-      expect(config.output.pr_comment.format).toBe('table');\\n+      expect(config.output!.pr_comment.format).toBe('table');\\n     });\\n \\n     it('should handle missing config file gracefully', async () => {\\n@@ -264,9 +264,9 @@ checks:\\n       const config = await configManager.loadConfig('/path/to/minimal.yaml');\\n \\n       // Should have default output configuration\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.group_by).toBe('check');\\n-      expect(config.output.pr_comment.collapse).toBe(true);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.group_by).toBe('check');\\n+      expect(config.output!.pr_comment.collapse).toBe(true);\\n     });\\n   });\\n \\n@@ -483,8 +483,8 @@ output:\\n       expect(config.checks!.performance.prompt).toContain('N+1 database queries');\\n       expect(config.checks!.security.prompt).toContain('SQL injection');\\n       expect(config.checks!.architecture.prompt).toContain('SOLID principles');\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.collapse).toBe(false);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.collapse).toBe(false);\\n     });\\n   });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/foreach-empty-skip.test.ts\",\"additions\":5,\"deletions\":3,\"changes\":8,\"patch\":\"diff --git a/tests/unit/foreach-empty-skip.test.ts b/tests/unit/foreach-empty-skip.test.ts\\nindex 1ea8e1e5..2640ed14 100644\\n--- a/tests/unit/foreach-empty-skip.test.ts\\n+++ b/tests/unit/foreach-empty-skip.test.ts\\n@@ -90,11 +90,13 @@ describe('forEach Empty Array Skip', () => {\\n     );\\n     expect(processStats?.totalRuns).toBe(0);\\n \\n-    // notify-ticket should also NOT run because process-ticket returned empty\\n+    // notify-ticket SHOULD run even though process-ticket had 0 runs from empty forEach\\n+    // This is the expected behavior: downstream steps that depend on forEach children\\n+    // should still execute (with empty/null inputs) to handle the \\\"no items\\\" case\\n     const notifyStats = result.executionStatistics?.checks!.find(\\n       c => c.checkName === 'notify-ticket'\\n     );\\n-    expect(notifyStats?.totalRuns).toBe(0);\\n+    expect(notifyStats?.totalRuns).toBe(1);\\n   });\\n \\n   it('should handle empty array with transform_js', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/ai-check-provider.test.ts\",\"additions\":61,\"deletions\":1,\"changes\":62,\"patch\":\"diff --git a/tests/unit/providers/ai-check-provider.test.ts b/tests/unit/providers/ai-check-provider.test.ts\\nindex d0b94086..a858ddae 100644\\n--- a/tests/unit/providers/ai-check-provider.test.ts\\n+++ b/tests/unit/providers/ai-check-provider.test.ts\\n@@ -399,6 +399,66 @@ describe('AICheckProvider', () => {\\n         },\\n       });\\n     });\\n+\\n+    it('derives allowedFolders and path from workspace when present on parent context', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const workspace = {\\n+        isEnabled: () => true,\\n+        getWorkspaceInfo: () => ({\\n+          workspacePath: '/tmp/ws-123',\\n+          mainProjectPath: '/tmp/ws-123/main-project',\\n+        }),\\n+        listProjects: () => [\\n+          { name: 'tyk', path: '/tmp/ws-123/tyk' },\\n+          { name: 'tyk-docs', path: '/tmp/ws-123/tyk-docs' },\\n+        ],\\n+      };\\n+\\n+      const execContext: any = {\\n+        _parentContext: {\\n+          workspace,\\n+        },\\n+      };\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'code_help',\\n+        ai: {\\n+          provider: 'google',\\n+          model: 'gemini-2.5-pro',\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, execContext);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'google',\\n+        model: 'gemini-2.5-pro',\\n+        // Workspace root should be used as primary working directory for tools\\n+        path: '/tmp/ws-123',\\n+      });\\n+\\n+      // allowedFolders should contain the workspace plus all project paths, de-duped\\n+      expect(capturedConfig.allowedFolders).toEqual(\\n+        expect.arrayContaining(['/tmp/ws-123', '/tmp/ws-123/tyk', '/tmp/ws-123/tyk-docs'])\\n+      );\\n+    });\\n   });\\n \\n   describe('getSupportedConfigKeys', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/command-check-provider.test.ts\",\"additions\":105,\"deletions\":1,\"changes\":106,\"patch\":\"diff --git a/tests/unit/providers/command-check-provider.test.ts b/tests/unit/providers/command-check-provider.test.ts\\nindex 3bf11a84..0a821bbd 100644\\n--- a/tests/unit/providers/command-check-provider.test.ts\\n+++ b/tests/unit/providers/command-check-provider.test.ts\\n@@ -666,4 +666,108 @@ describe('CommandCheckProvider', () => {\\n       expect((result as any).output).toBe('content'); // CommandCheckProvider trims whitespace\\n     });\\n   });\\n+\\n+  describe('Workflow Inputs', () => {\\n+    it('should use workflowInputs from config when available', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Hello from config.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from config.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with inputs from config.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from config.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should fall back to context.workflowInputs when config.workflowInputs is not set', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Hello from context.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from context.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify the command was rendered with inputs from context.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from context.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should prefer config.workflowInputs over context.workflowInputs', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Config takes precedence',\\n+        },\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Context should be ignored',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Config takes precedence\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify config.workflowInputs takes precedence\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Config takes precedence\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should provide empty inputs when neither config nor context has workflowInputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with empty inputs (undefined renders as empty)\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+  });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/git-checkout-workspace.test.ts\",\"additions\":4,\"deletions\":5,\"changes\":9,\"patch\":\"diff --git a/tests/unit/providers/git-checkout-workspace.test.ts b/tests/unit/providers/git-checkout-workspace.test.ts\\nindex 4eb625b7..4a0fbb6e 100644\\n--- a/tests/unit/providers/git-checkout-workspace.test.ts\\n+++ b/tests/unit/providers/git-checkout-workspace.test.ts\\n@@ -129,13 +129,12 @@ describe('GitCheckoutProvider Workspace Integration', () => {\\n         const output = (result as any).output as GitCheckoutOutput;\\n         expect(output.success).toBe(true);\\n         expect(output.workspace_path).toBeDefined();\\n-        // When checkName is provided, it's used as the project name\\n-        expect(output.workspace_path).toContain('test-checkout');\\n \\n-        // Verify project was added to workspace\\n+        // Verify project was added to workspace and the name is derived\\n+        // from the repository/description, not the checkName.\\n         const projects = workspace.listProjects();\\n         expect(projects.length).toBe(1);\\n-        expect(projects[0].name).toBe('test-checkout');\\n+        expect(projects[0].name).toBe('external-repo');\\n       } finally {\\n         await workspace.cleanup();\\n         fs.rmSync(mainProjectDir, { recursive: true, force: true });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/http-client-provider.test.ts\",\"additions\":15,\"deletions\":10,\"changes\":25,\"patch\":\"diff --git a/tests/unit/providers/http-client-provider.test.ts b/tests/unit/providers/http-client-provider.test.ts\\nindex b759ad75..caa73417 100644\\n--- a/tests/unit/providers/http-client-provider.test.ts\\n+++ b/tests/unit/providers/http-client-provider.test.ts\\n@@ -120,15 +120,17 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect(result).toEqual<ReviewSummary & { data: unknown }>({\\n+      // The provider returns data in the 'output' property (consistent with other providers)\\n+      expect(result).toEqual({\\n         issues: [],\\n-        data: responseData,\\n+        output: { status: 'ok', data: { value: 123 } },\\n       });\\n     });\\n \\n     it('should handle POST request with body', async () => {\\n       mockConfig.method = 'POST';\\n-      mockConfig.body = '{\\\"request\\\": \\\"data\\\"}';\\n+      // Use a body with Liquid template to trigger parseAndRender\\n+      mockConfig.body = '{\\\"request\\\": \\\"{{ pr.title }}\\\"}';\\n \\n       const responseData = { result: 'success' };\\n       const mockResponse = {\\n@@ -143,15 +145,16 @@ describe('HttpClientProvider', () => {\\n       };\\n \\n       mockFetch.mockResolvedValue(mockResponse);\\n-      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"data\\\"}');\\n+      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"Test PR\\\"}');\\n \\n       const result = await provider.execute(mockPRInfo, mockConfig, new Map());\\n \\n+      // Verify the body template was parsed by Liquid with the correct template\\n       expect(mockLiquid.parseAndRender).toHaveBeenCalledWith(\\n-        '{\\\"request\\\": \\\"data\\\"}',\\n+        '{\\\"request\\\": \\\"{{ pr.title }}\\\"}',\\n         expect.objectContaining({\\n           pr: expect.any(Object),\\n-          outputs: {},\\n+          outputs: expect.any(Object),\\n         })\\n       );\\n \\n@@ -159,7 +162,7 @@ describe('HttpClientProvider', () => {\\n         'https://api.example.com/data',\\n         expect.objectContaining({\\n           method: 'POST',\\n-          body: '{\\\"request\\\": \\\"data\\\"}',\\n+          body: '{\\\"request\\\": \\\"Test PR\\\"}',\\n           headers: expect.objectContaining({\\n             Authorization: 'Bearer token',\\n             'Content-Type': 'application/json',\\n@@ -167,7 +170,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(responseData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { result: string } }).output.result).toEqual('success');\\n     });\\n \\n     it('should handle HTTP errors', async () => {\\n@@ -235,7 +239,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(transformedData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { transformed: string } }).output.transformed).toEqual('result');\\n     });\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/session-reuse-config.test.ts\",\"additions\":20,\"deletions\":1,\"changes\":21,\"patch\":\"diff --git a/tests/unit/session-reuse-config.test.ts b/tests/unit/session-reuse-config.test.ts\\nindex a2e2ee26..18fc42e6 100644\\n--- a/tests/unit/session-reuse-config.test.ts\\n+++ b/tests/unit/session-reuse-config.test.ts\\n@@ -152,6 +152,25 @@ describe('Session Reuse Configuration Validation', () => {\\n       }).not.toThrow();\\n     });\\n \\n+    it('should accept reuse_ai_session=\\\\\\\"self\\\\\\\" without depends_on', () => {\\n+      const config: Partial<VisorConfig> = {\\n+        version: '1.0',\\n+        checks: {\\n+          'loop-check': {\\n+            type: 'ai',\\n+            prompt: 'Chat-like loop that reuses its own session',\\n+            on: ['pr_opened'],\\n+            // Special self-reuse mode does not require depends_on\\n+            reuse_ai_session: 'self',\\n+          },\\n+        },\\n+      };\\n+\\n+      expect(() => {\\n+        (configManager as any).validateConfig(config);\\n+      }).not.toThrow();\\n+    });\\n+\\n     it('should accept reuse_ai_session=true with multiple dependencies', () => {\\n       const config: Partial<VisorConfig> = {\\n         version: '1.0',\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/slack-markdown-format.test.ts\",\"additions\":25,\"deletions\":1,\"changes\":26,\"patch\":\"diff --git a/tests/unit/slack-markdown-format.test.ts b/tests/unit/slack-markdown-format.test.ts\\nindex 5e57ccae..39d0baf3 100644\\n--- a/tests/unit/slack-markdown-format.test.ts\\n+++ b/tests/unit/slack-markdown-format.test.ts\\n@@ -39,4 +39,28 @@ describe('markdownToSlack', () => {\\n       ['‚Ä¢ parent', '  ‚Ä¢ child', '```', '- not-a-bullet inside code', '```', '‚Ä¢ after'].join('\\\\n')\\n     );\\n   });\\n+\\n+  it('converts markdown headers to bold text', () => {\\n+    const input = '# Main Title\\\\n## Subtitle\\\\n### Section';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*Main Title*\\\\n*Subtitle*\\\\n*Section*');\\n+  });\\n+\\n+  it('adds newline before h1/h2 headers when preceded by content', () => {\\n+    const input = 'Some content\\\\n## New Section\\\\nMore content';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('Some content\\\\n\\\\n*New Section*\\\\nMore content');\\n+  });\\n+\\n+  it('does not add newline before h1 if it is the first line', () => {\\n+    const input = '# First Header\\\\nContent here';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*First Header*\\\\nContent here');\\n+  });\\n+\\n+  it('ignores headers inside code blocks', () => {\\n+    const input = '```\\\\n# This is a comment\\\\n```\\\\n# Real Header';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('```\\\\n# This is a comment\\\\n```\\\\n*Real Header*');\\n+  });\\n });\\n\",\"status\":\"modified\"}],\"outputs\":{\"ask\":{\"text\":\"Hey  I'm using the Tyk Operator to manage multiple environments in my kubernetes cluster using OperatorContext.  I want to be able to use a blue-green deployment approach to upgrade one of the environments.  My green environment should have all of the same APIDefinition and SecurityPolicy CRDs as the blue environment.  I want to be able to validate the green environment before cutting traffic over to it.  What's the best way to go about achieving this?\",\"ts\":1767904439981},\"jira-context\":{\"jira_context_xml\":\"\",\"issues\":[],\"issue_count\":0,\"ts\":1767904440093},\"zendesk-context\":{\"zendesk_context_xml\":\"\",\"tickets\":\"\",\"ticket_count\":0,\"attachments\":\"\",\"ts\":1767904440108},\"log-request\":{\"issues\":[],\"logOutput\":\"‚ÑπÔ∏è **INFO**: üì• Request from user: unknown\\nChannel: unknown\\nThread: unknown\\nMessage: Hey  I'm using the Tyk Operator to manage multiple environments in my kubernetes cluster using OperatorContext.  I want to be able to use a blue-green deployment approach to upgrade one of the envi...\\n\"},\"code-help-error\":{\"issues\":[]},\"customer-insights-error\":{\"issues\":[]},\"release-notes-error\":{\"issues\":[]}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.output":"{\"intent\":\"code_help\",\"topic\":\"User is asking for the best way to implement a blue-green deployment strategy for an environment managed by Tyk Operator in Kubernetes using OperatorContext. They want to clone APIDefinition and SecurityPolicy CRDs from a 'blue' environment to a 'green' environment for validation before switching traffic.\",\"text\":\"{\\n  \\\"intent\\\": \\\"code_help\\\",\\n  \\\"topic\\\": \\\"User is asking for the best way to implement a blue-green deployment strategy for an environment managed by Tyk Operator in Kubernetes using OperatorContext. They want to clone APIDefinition and SecurityPolicy CRDs from a 'blue' environment to a 'green' environment for validation before switching traffic.\\\"\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":2,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"code-help","visor.provider.type":"workflow"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"61d506ff-fd15-42e3-8ff3-8c8e851809ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"61d506ff-fd15-42e3-8ff3-8c8e851809ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"61d506ff-fd15-42e3-8ff3-8c8e851809ba"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-tyk-docs","visor.provider.type":"git-checkout"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"61d506ff-fd15-42e3-8ff3-8c8e851809ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"61d506ff-fd15-42e3-8ff3-8c8e851809ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"61d506ff-fd15-42e3-8ff3-8c8e851809ba"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ensure-code-plan","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"ensure-code-plan","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"outputs\":{\"checkout-tyk-docs\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-docs-main-39f1fbf2\",\"ref\":\"main\",\"commit\":\"a022bb7c96ca4f48bcdac27cae3f7c4a24171ee9\",\"worktree_id\":\"TykTechnologies-tyk-docs-main-39f1fbf2\",\"repository\":\"TykTechnologies/tyk-docs\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/58bf29aa-f9ea-473f-9571-7c03523764ba/tyk-docs\",\"ts\":1767904452115}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"ensure-code-plan","visor.check.output":"{\"projects\":[{\"project_id\":\"tyk\",\"reason\":\"The Tyk Gateway is the component that enforces the APIDefinition and SecurityPolicy configurations. The user's goal is to validate a 'green' environment, which means validating the behavior of the Gateway with the new configuration.\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Operator translates Kubernetes CRDs into API calls to the Tyk Dashboard, which acts as the control plane for storing API definitions and security policies. The Dashboard is central to how the Operator manages configurations.\"},{\"project_id\":\"tyk-sink\",\"reason\":\"The user specifies managing multiple environments. In a typical multi-environment Tyk setup managed from a central point, MDCB (tyk-sink) is responsible for synchronizing configurations from the Dashboard to the distributed Gateway clusters in each environment (e.g., blue and green).\"}],\"notes\":\"The user's question is about implementing a blue-green deployment strategy using the Tyk Operator. This involves understanding how the Operator interacts with the Tyk stack to manage configurations across different environments. The core components involved are the Gateway (enforcement), the Dashboard (control plane/storage), and MDCB (config synchronization for multi-environment setups).\",\"text\":\"{\\n  \\\"projects\\\": [\\n    {\\n      \\\"project_id\\\": \\\"tyk\\\",\\n      \\\"reason\\\": \\\"The Tyk Gateway is the component that enforces the APIDefinition and SecurityPolicy configurations. The user's goal is to validate a 'green' environment, which means validating the behavior of the Gateway with the new configuration.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-analytics\\\",\\n      \\\"reason\\\": \\\"The Tyk Operator translates Kubernetes CRDs into API calls to the Tyk Dashboard, which acts as the control plane for storing API definitions and security policies. The Dashboard is central to how the Operator manages configurations.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-sink\\\",\\n      \\\"reason\\\": \\\"The user specifies managing multiple environments. In a typical multi-environment Tyk setup managed from a central point, MDCB (tyk-sink) is responsible for synchronizing configurations from the Dashboard to the distributed Gateway clusters in each environment (e.g., blue and green).\\\"\\n    }\\n  ],\\n  \\\"notes\\\": \\\"The user's question is about implementing a blue-green deployment strategy using the Tyk Operator. This involves understanding how the Operator interacts with the Tyk stack to manage configurations across different environments. The core components involved are the Gateway (enforcement), the Dashboard (control plane/storage), and MDCB (config synchronization for multi-environment setups).\\\"\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"61d506ff-fd15-42e3-8ff3-8c8e851809ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"61d506ff-fd15-42e3-8ff3-8c8e851809ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"61d506ff-fd15-42e3-8ff3-8c8e851809ba"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"project-items","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"61d506ff-fd15-42e3-8ff3-8c8e851809ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"61d506ff-fd15-42e3-8ff3-8c8e851809ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"61d506ff-fd15-42e3-8ff3-8c8e851809ba"}}]}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":0,"visor.foreach.total":3},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":1,"visor.foreach.total":3},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":2,"visor.foreach.total":3},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"61d506ff-fd15-42e3-8ff3-8c8e851809ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"61d506ff-fd15-42e3-8ff3-8c8e851809ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"61d506ff-fd15-42e3-8ff3-8c8e851809ba"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"project-code-query","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"project-code-query","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"outputs\":{\"ensure-code-plan\":{\"projects\":[{\"project_id\":\"tyk\",\"reason\":\"The Tyk Gateway is the component that enforces the APIDefinition and SecurityPolicy configurations. The user's goal is to validate a 'green' environment, which means validating the behavior of the Gateway with the new configuration.\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Operator translates Kubernetes CRDs into API calls to the Tyk Dashboard, which acts as the control plane for storing API definitions and security policies. The Dashboard is central to how the Operator manages configurations.\"},{\"project_id\":\"tyk-sink\",\"reason\":\"The user specifies managing multiple environments. In a typical multi-environment Tyk setup managed from a central point, MDCB (tyk-sink) is responsible for synchronizing configurations from the Dashboard to the distributed Gateway clusters in each environment (e.g., blue and green).\"}],\"notes\":\"The user's question is about implementing a blue-green deployment strategy using the Tyk Operator. This involves understanding how the Operator interacts with the Tyk stack to manage configurations across different environments. The core components involved are the Gateway (enforcement), the Dashboard (control plane/storage), and MDCB (config synchronization for multi-environment setups).\",\"text\":\"{\\n  \\\"projects\\\": [\\n    {\\n      \\\"project_id\\\": \\\"tyk\\\",\\n      \\\"reason\\\": \\\"The Tyk Gateway is the component that enforces the APIDefinition and SecurityPolicy configurations. The user's goal is to validate a 'green' environment, which means validating the behavior of the Gateway with the new configuration.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-analytics\\\",\\n      \\\"reason\\\": \\\"The Tyk Operator translates Kubernetes CRDs into API calls to the Tyk Dashboard, which acts as the control plane for storing API definitions and security policies. The Dashboard is central to how the Operator manages configurations.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk-sink\\\",\\n      \\\"reason\\\": \\\"The user specifies managing multiple environments. In a typical multi-environment Tyk setup managed from a central point, MDCB (tyk-sink) is responsible for synchronizing configurations from the Dashboard to the distributed Gateway clusters in each environment (e.g., blue and green).\\\"\\n    }\\n  ],\\n  \\\"notes\\\": \\\"The user's question is about implementing a blue-green deployment strategy using the Tyk Operator. This involves understanding how the Operator interacts with the Tyk stack to manage configurations across different environments. The core components involved are the Gateway (enforcement), the Dashboard (control plane/storage), and MDCB (config synchronization for multi-environment setups).\\\"\\n}\",\"ts\":1767904482191},\"project-items\":[{\"project_id\":\"tyk\",\"reason\":\"The Tyk Gateway is the component that enforces the APIDefinition and SecurityPolicy configurations. The user's goal is to validate a 'green' environment, which means validating the behavior of the Gateway with the new configuration.\",\"repository\":\"TykTechnologies/tyk\",\"description\":\"Tyk Gateway\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Operator translates Kubernetes CRDs into API calls to the Tyk Dashboard, which acts as the control plane for storing API definitions and security policies. The Dashboard is central to how the Operator manages configurations.\",\"repository\":\"TykTechnologies/tyk-analytics\",\"description\":\"Tyk Dashboard backend\"},{\"project_id\":\"tyk-sink\",\"reason\":\"The user specifies managing multiple environments. In a typical multi-environment Tyk setup managed from a central point, MDCB (tyk-sink) is responsible for synchronizing configurations from the Dashboard to the distributed Gateway clusters in each environment (e.g., blue and green).\",\"repository\":\"TykTechnologies/tyk-sink\",\"description\":\"Tyk MDCB\"}],\"checkout-projects\":{\"issues\":[],\"isForEach\":true,\"forEachItems\":[{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-HEAD-f8ad23cc\",\"ref\":\"HEAD\",\"commit\":\"bb67e9e7f9382a09ff152c6942d00023c463feef\",\"worktree_id\":\"TykTechnologies-tyk-HEAD-f8ad23cc\",\"repository\":\"TykTechnologies/tyk\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/58bf29aa-f9ea-473f-9571-7c03523764ba/tyk\"},{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-analytics-HEAD-e482b8f9\",\"ref\":\"HEAD\",\"commit\":\"b7ed8000a93e1284b28100938363a312d2d1fad1\",\"worktree_id\":\"TykTechnologies-tyk-analytics-HEAD-e482b8f9\",\"repository\":\"TykTechnologies/tyk-analytics\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/58bf29aa-f9ea-473f-9571-7c03523764ba/tyk-analytics\"},{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-sink-HEAD-a10f58fc\",\"ref\":\"HEAD\",\"commit\":\"d015f96a8988b5cad4dfc51ff4b48fd77e9aec8b\",\"worktree_id\":\"TykTechnologies-tyk-sink-HEAD-a10f58fc\",\"repository\":\"TykTechnologies/tyk-sink\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/58bf29aa-f9ea-473f-9571-7c03523764ba/tyk-sink\"}],\"forEachItemResults\":[{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-HEAD-f8ad23cc\",\"ref\":\"HEAD\",\"commit\":\"bb67e9e7f9382a09ff152c6942d00023c463feef\",\"worktree_id\":\"TykTechnologies-tyk-HEAD-f8ad23cc\",\"repository\":\"TykTechnologies/tyk\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/58bf29aa-f9ea-473f-9571-7c03523764ba/tyk\"}},{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-analytics-HEAD-e482b8f9\",\"ref\":\"HEAD\",\"commit\":\"b7ed8000a93e1284b28100938363a312d2d1fad1\",\"worktree_id\":\"TykTechnologies-tyk-analytics-HEAD-e482b8f9\",\"repository\":\"TykTechnologies/tyk-analytics\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/58bf29aa-f9ea-473f-9571-7c03523764ba/tyk-analytics\"}},{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-sink-HEAD-a10f58fc\",\"ref\":\"HEAD\",\"commit\":\"d015f96a8988b5cad4dfc51ff4b48fd77e9aec8b\",\"worktree_id\":\"TykTechnologies-tyk-sink-HEAD-a10f58fc\",\"repository\":\"TykTechnologies/tyk-sink\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/58bf29aa-f9ea-473f-9571-7c03523764ba/tyk-sink\"}}]},\"checkout-tyk-docs\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-docs-main-39f1fbf2\",\"ref\":\"main\",\"commit\":\"a022bb7c96ca4f48bcdac27cae3f7c4a24171ee9\",\"worktree_id\":\"TykTechnologies-tyk-docs-main-39f1fbf2\",\"repository\":\"TykTechnologies/tyk-docs\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/58bf29aa-f9ea-473f-9571-7c03523764ba/tyk-docs\",\"ts\":1767904452115},\"project-items-raw\":[{\"project_id\":\"tyk\",\"reason\":\"The Tyk Gateway is the component that enforces the APIDefinition and SecurityPolicy configurations. The user's goal is to validate a 'green' environment, which means validating the behavior of the Gateway with the new configuration.\",\"repository\":\"TykTechnologies/tyk\",\"description\":\"Tyk Gateway\"},{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Operator translates Kubernetes CRDs into API calls to the Tyk Dashboard, which acts as the control plane for storing API definitions and security policies. The Dashboard is central to how the Operator manages configurations.\",\"repository\":\"TykTechnologies/tyk-analytics\",\"description\":\"Tyk Dashboard backend\"},{\"project_id\":\"tyk-sink\",\"reason\":\"The user specifies managing multiple environments. In a typical multi-environment Tyk setup managed from a central point, MDCB (tyk-sink) is responsible for synchronizing configurations from the Dashboard to the distributed Gateway clusters in each environment (e.g., blue and green).\",\"repository\":\"TykTechnologies/tyk-sink\",\"description\":\"Tyk MDCB\"}]},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"project-code-query","visor.check.output":"{\"answer\":{\"text\":\"To achieve a blue-green deployment using the Tyk Operator and `OperatorContext`, you can follow these steps:\\n\\n1.  **Set up two separate Tyk stacks:** You will need to have two distinct Tyk environments, a \\\"blue\\\" one and a \\\"green\\\" one. Each environment should have its own Tyk Dashboard, Tyk Gateway, and Tyk Sink (MDCB).\\n\\n2.  **Create two `OperatorContext` resources:** In your Kubernetes cluster, you will define two `OperatorContext` custom resources. One will be configured to connect to your \\\"blue\\\" Tyk Dashboard's API, and the other will be configured to connect to your \\\"green\\\" Tyk Dashboard's API.\\n\\n3.  **Deploy your CRDs with `contextRef`:** You will deploy your `APIDefinition` and `SecurityPolicy` CRDs to your Kubernetes cluster. The CRDs intended for the blue environment should have a `contextRef` that points to the \\\"blue\\\" `OperatorContext`. Similarly, the CRDs for the green environment should have a `contextRef` that points to the \\\"green\\\" `OperatorContext`. This will ensure that the Tyk Operator applies the configurations to the correct Tyk environment.\\n\\n4.  **Validate the green environment:** Once the CRDs are applied, you can validate your \\\"green\\\" environment by sending traffic to the \\\"green\\\" Tyk Gateway's ingress. This allows you to test the new configuration without affecting your production traffic.\\n\\n5.  **Cut over traffic:** After you have validated the \\\"green\\\" environment, you can switch traffic from the \\\"blue\\\" environment to the \\\"green\\\" environment at your load balancer or ingress controller.\\n\\nThis approach provides a clean separation between your blue and green environments, allowing you to safely validate changes before directing production traffic to them. The Tyk Operator, through the use of `OperatorContext`, is the key to managing these multiple environments from a single Kubernetes cluster.\\n\\n**References:**\\n\\n*   **Tyk Operator Documentation:** [https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/automations/operator.mdx](https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/automations/operator.mdx)\\n*   **MDCB (Tyk Sink) Documentation:** [https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-mdcb-api.mdx](https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-mdcb-api.mdx)\"},\"text\":\"{\\n  \\\"answer\\\": {\\n    \\\"text\\\": \\\"To achieve a blue-green deployment using the Tyk Operator and `OperatorContext`, you can follow these steps:\\\\n\\\\n1.  **Set up two separate Tyk stacks:** You will need to have two distinct Tyk environments, a \\\\\\\"blue\\\\\\\" one and a \\\\\\\"green\\\\\\\" one. Each environment should have its own Tyk Dashboard, Tyk Gateway, and Tyk Sink (MDCB).\\\\n\\\\n2.  **Create two `OperatorContext` resources:** In your Kubernetes cluster, you will define two `OperatorContext` custom resources. One will be configured to connect to your \\\\\\\"blue\\\\\\\" Tyk Dashboard's API, and the other will be configured to connect to your \\\\\\\"green\\\\\\\" Tyk Dashboard's API.\\\\n\\\\n3.  **Deploy your CRDs with `contextRef`:** You will deploy your `APIDefinition` and `SecurityPolicy` CRDs to your Kubernetes cluster. The CRDs intended for the blue environment should have a `contextRef` that points to the \\\\\\\"blue\\\\\\\" `OperatorContext`. Similarly, the CRDs for the green environment should have a `contextRef` that points to the \\\\\\\"green\\\\\\\" `OperatorContext`. This will ensure that the Tyk Operator applies the configurations to the correct Tyk environment.\\\\n\\\\n4.  **Validate the green environment:** Once the CRDs are applied, you can validate your \\\\\\\"green\\\\\\\" environment by sending traffic to the \\\\\\\"green\\\\\\\" Tyk Gateway's ingress. This allows you to test the new configuration without affecting your production traffic.\\\\n\\\\n5.  **Cut over traffic:** After you have validated the \\\\\\\"green\\\\\\\" environment, you can switch traffic from the \\\\\\\"blue\\\\\\\" environment to the \\\\\\\"green\\\\\\\" environment at your load balancer or ingress controller.\\\\n\\\\nThis approach provides a clean separation between your blue and green environments, allowing you to safely validate changes before directing production traffic to them. The Tyk Operator, through the use of `OperatorContext`, is the key to managing these multiple environments from a single Kubernetes cluster.\\\\n\\\\n**References:**\\\\n\\\\n*   **Tyk Operator Documentation:** [https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/automations/operator.mdx](https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/automations/operator.mdx)\\\\n*   **MDCB (Tyk Sink) Documentation:** [https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-mdcb-api.mdx](https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-mdcb-api.mdx)\\\"\\n  }\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"61d506ff-fd15-42e3-8ff3-8c8e851809ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"61d506ff-fd15-42e3-8ff3-8c8e851809ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"61d506ff-fd15-42e3-8ff3-8c8e851809ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":2,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"code-help-reply","visor.provider.type":"log"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":3,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":3,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":3,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":3,"session_id":"58bf29aa-f9ea-473f-9571-7c03523764ba"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"jira-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"zendesk-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"log-request","visor.provider.type":"log"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"b4f77336-ae18-421f-9a0c-8fdd8c5ba7d8"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"be224a87-bbf7-4c93-a9d9-84d4b7df3451"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"b4f77336-ae18-421f-9a0c-8fdd8c5ba7d8"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"be224a87-bbf7-4c93-a9d9-84d4b7df3451"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"b4f77336-ae18-421f-9a0c-8fdd8c5ba7d8"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"be224a87-bbf7-4c93-a9d9-84d4b7df3451"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-keys","visor.provider.type":"script"}}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-ids","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"history\":{}},\"outputs_history\":{},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"include_comments\":true,\"max_tickets\":5,\"download_assets\":true,\"cache_dir\":\".cache/zendesk\",\"text\":\"Hey <@U09T5KRLMPU> I'm using the Tyk Operator to manage multiple environments in my kubernetes cluster using OperatorContext.  I want to be able to use a blue-green deployment approach to upgrade one of the environments.  My green environment should have all of the same APIDefinition and SecurityPolicy CRDs as the blue environment.  I want to be able to validate the green environment before cutting traffic over to it.  What's the best way to go about achieving this?\\nTo achieve a blue-green deployment using the Tyk Operator and `OperatorContext`, you can follow these steps:\\n\\n1.  *Set up two separate Tyk stacks:* You will need to have two distinct Tyk environments, a \\\"blue\\\" one and a \\\"green\\\" one. Each environment should have its own Tyk Dashboard, Tyk Gateway, and Tyk Sink (MDCB).\\n\\n2.  *Create two `OperatorContext` resources:* In your Kubernetes cluster, you will define two `OperatorContext` custom resources. One will be configured to connect to your \\\"blue\\\" Tyk Dashboard's API, and the other will be configured to connect to your \\\"green\\\" Tyk Dashboard's API.\\n\\n3.  *Deploy your CRDs with `contextRef`:* You will deploy your `APIDefinition` and `SecurityPolicy` CRDs to your Kubernetes cluster. The CRDs intended for the blue environment should have a `contextRef` that points to the \\\"blue\\\" `OperatorContext`. Similarly, the CRDs for the green environment should have a `contextRef` that points to the \\\"green\\\" `OperatorContext`. This will ensure that the Tyk Operator applies the configurations to the correct Tyk environment.\\n\\n4.  *Validate the green environment:* Once the CRDs are applied, you can validate your \\\"green\\\" environment by sending traffic to the \\\"green\\\" Tyk Gateway's ingress. This allows you to test the new configuration without affecting your production traffic.\\n\\n5.  *Cut over traffic:* After you have validated the \\\"green\\\" environment, you can switch traffic from the \\\"blue\\\" environment to the \\\"green\\\" environment at your load balancer or ingress controller.\\n\\nThis approach provides a clean separation between your blue and green environments, allowing you to safely validate changes before directing production traffic to them. The Tyk Operator, through the use of `OperatorContext`, is the key to managing these multiple environments from a single Kubernetes cluster.\\n\\n*References:*\\n\\n‚Ä¢ *Tyk Operator Documentation:* <https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/automations/operator.mdx|https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/automations/operator.mdx>\\n‚Ä¢ *MDCB (Tyk Sink) Documentation:* <https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-mdcb-api.mdx|https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-mdcb-api.mdx>\\nCan this be achieved with no modifications to the APIDefinition and SecurityPolicy CRDs?\\nCan this be achieved with no modifications to the APIDefinition and SecurityPolicy CRDs?\\n\"},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b4f77336-ae18-421f-9a0c-8fdd8c5ba7d8"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b4f77336-ae18-421f-9a0c-8fdd8c5ba7d8"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"b4f77336-ae18-421f-9a0c-8fdd8c5ba7d8"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"no-issues-fallback","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"no-issues-fallback","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"extract-keys\":{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767905289868},\"fetch-issues\":{\"issues\":[]},\"history\":{\"extract-keys\":[{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767905289868}],\"fetch-issues\":[{\"issues\":[]}]}},\"outputs_history\":{\"extract-keys\":[{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767905289868}],\"fetch-issues\":[{\"issues\":[]}]},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"text\":\"Hey <@U09T5KRLMPU> I'm using the Tyk Operator to manage multiple environments in my kubernetes cluster using OperatorContext.  I want to be able to use a blue-green deployment approach to upgrade one of the environments.  My green environment should have all of the same APIDefinition and SecurityPolicy CRDs as the blue environment.  I want to be able to validate the green environment before cutting traffic over to it.  What's the best way to go about achieving this?\\nTo achieve a blue-green deployment using the Tyk Operator and `OperatorContext`, you can follow these steps:\\n\\n1.  *Set up two separate Tyk stacks:* You will need to have two distinct Tyk environments, a \\\"blue\\\" one and a \\\"green\\\" one. Each environment should have its own Tyk Dashboard, Tyk Gateway, and Tyk Sink (MDCB).\\n\\n2.  *Create two `OperatorContext` resources:* In your Kubernetes cluster, you will define two `OperatorContext` custom resources. One will be configured to connect to your \\\"blue\\\" Tyk Dashboard's API, and the other will be configured to connect to your \\\"green\\\" Tyk Dashboard's API.\\n\\n3.  *Deploy your CRDs with `contextRef`:* You will deploy your `APIDefinition` and `SecurityPolicy` CRDs to your Kubernetes cluster. The CRDs intended for the blue environment should have a `contextRef` that points to the \\\"blue\\\" `OperatorContext`. Similarly, the CRDs for the green environment should have a `contextRef` that points to the \\\"green\\\" `OperatorContext`. This will ensure that the Tyk Operator applies the configurations to the correct Tyk environment.\\n\\n4.  *Validate the green environment:* Once the CRDs are applied, you can validate your \\\"green\\\" environment by sending traffic to the \\\"green\\\" Tyk Gateway's ingress. This allows you to test the new configuration without affecting your production traffic.\\n\\n5.  *Cut over traffic:* After you have validated the \\\"green\\\" environment, you can switch traffic from the \\\"blue\\\" environment to the \\\"green\\\" environment at your load balancer or ingress controller.\\n\\nThis approach provides a clean separation between your blue and green environments, allowing you to safely validate changes before directing production traffic to them. The Tyk Operator, through the use of `OperatorContext`, is the key to managing these multiple environments from a single Kubernetes cluster.\\n\\n*References:*\\n\\n‚Ä¢ *Tyk Operator Documentation:* <https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/automations/operator.mdx|https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/automations/operator.mdx>\\n‚Ä¢ *MDCB (Tyk Sink) Documentation:* <https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-mdcb-api.mdx|https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-mdcb-api.mdx>\\nCan this be achieved with no modifications to the APIDefinition and SecurityPolicy CRDs?\\nCan this be achieved with no modifications to the APIDefinition and SecurityPolicy CRDs?\\n\",\"jql\":\"\",\"custom_field_aliases\":{},\"include_comments\":true,\"max_issues\":3,\"ticket_prefixes\":[\"TT\",\"DX\"]},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.output":"{\"data\":[],\"count\":0}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.check","attributes":{"visor.check.id":"no-issues-fallback","visor.check.output":"\"<jira_context><issue_count>0</issue_count><message>No Jira issue keys found in the provided text</message></jira_context>\""},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b4f77336-ae18-421f-9a0c-8fdd8c5ba7d8"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b4f77336-ae18-421f-9a0c-8fdd8c5ba7d8"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"b4f77336-ae18-421f-9a0c-8fdd8c5ba7d8"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b4f77336-ae18-421f-9a0c-8fdd8c5ba7d8"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"b4f77336-ae18-421f-9a0c-8fdd8c5ba7d8"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"be224a87-bbf7-4c93-a9d9-84d4b7df3451"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"be224a87-bbf7-4c93-a9d9-84d4b7df3451"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"be224a87-bbf7-4c93-a9d9-84d4b7df3451"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"no-tickets-fallback","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"no-tickets-fallback","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"extract-ids\":{\"data\":[],\"count\":0,\"ts\":1767905289894},\"fetch-tickets\":{\"issues\":[]},\"history\":{\"extract-ids\":[{\"data\":[],\"count\":0,\"ts\":1767905289894}],\"fetch-tickets\":[{\"issues\":[]}]}},\"outputs_history\":{\"extract-ids\":[{\"data\":[],\"count\":0,\"ts\":1767905289894}],\"fetch-tickets\":[{\"issues\":[]}]},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"include_comments\":true,\"max_tickets\":5,\"download_assets\":true,\"cache_dir\":\".cache/zendesk\",\"text\":\"Hey <@U09T5KRLMPU> I'm using the Tyk Operator to manage multiple environments in my kubernetes cluster using OperatorContext.  I want to be able to use a blue-green deployment approach to upgrade one of the environments.  My green environment should have all of the same APIDefinition and SecurityPolicy CRDs as the blue environment.  I want to be able to validate the green environment before cutting traffic over to it.  What's the best way to go about achieving this?\\nTo achieve a blue-green deployment using the Tyk Operator and `OperatorContext`, you can follow these steps:\\n\\n1.  *Set up two separate Tyk stacks:* You will need to have two distinct Tyk environments, a \\\"blue\\\" one and a \\\"green\\\" one. Each environment should have its own Tyk Dashboard, Tyk Gateway, and Tyk Sink (MDCB).\\n\\n2.  *Create two `OperatorContext` resources:* In your Kubernetes cluster, you will define two `OperatorContext` custom resources. One will be configured to connect to your \\\"blue\\\" Tyk Dashboard's API, and the other will be configured to connect to your \\\"green\\\" Tyk Dashboard's API.\\n\\n3.  *Deploy your CRDs with `contextRef`:* You will deploy your `APIDefinition` and `SecurityPolicy` CRDs to your Kubernetes cluster. The CRDs intended for the blue environment should have a `contextRef` that points to the \\\"blue\\\" `OperatorContext`. Similarly, the CRDs for the green environment should have a `contextRef` that points to the \\\"green\\\" `OperatorContext`. This will ensure that the Tyk Operator applies the configurations to the correct Tyk environment.\\n\\n4.  *Validate the green environment:* Once the CRDs are applied, you can validate your \\\"green\\\" environment by sending traffic to the \\\"green\\\" Tyk Gateway's ingress. This allows you to test the new configuration without affecting your production traffic.\\n\\n5.  *Cut over traffic:* After you have validated the \\\"green\\\" environment, you can switch traffic from the \\\"blue\\\" environment to the \\\"green\\\" environment at your load balancer or ingress controller.\\n\\nThis approach provides a clean separation between your blue and green environments, allowing you to safely validate changes before directing production traffic to them. The Tyk Operator, through the use of `OperatorContext`, is the key to managing these multiple environments from a single Kubernetes cluster.\\n\\n*References:*\\n\\n‚Ä¢ *Tyk Operator Documentation:* <https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/automations/operator.mdx|https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/automations/operator.mdx>\\n‚Ä¢ *MDCB (Tyk Sink) Documentation:* <https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-mdcb-api.mdx|https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-mdcb-api.mdx>\\nCan this be achieved with no modifications to the APIDefinition and SecurityPolicy CRDs?\\nCan this be achieved with no modifications to the APIDefinition and SecurityPolicy CRDs?\\n\"},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.check","attributes":{"visor.check.id":"no-tickets-fallback","visor.check.output":"\"<zendesk_context><ticket_count>0</ticket_count><message>No Zendesk ticket IDs found in the provided text</message></zendesk_context>\""},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"be224a87-bbf7-4c93-a9d9-84d4b7df3451"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"be224a87-bbf7-4c93-a9d9-84d4b7df3451"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"be224a87-bbf7-4c93-a9d9-84d4b7df3451"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"be224a87-bbf7-4c93-a9d9-84d4b7df3451"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"be224a87-bbf7-4c93-a9d9-84d4b7df3451"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"be224a87-bbf7-4c93-a9d9-84d4b7df3451"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"be224a87-bbf7-4c93-a9d9-84d4b7df3451"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"be224a87-bbf7-4c93-a9d9-84d4b7df3451"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"be224a87-bbf7-4c93-a9d9-84d4b7df3451"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"be224a87-bbf7-4c93-a9d9-84d4b7df3451"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"be224a87-bbf7-4c93-a9d9-84d4b7df3451"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"route-intent","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.input.context":"{\"pr\":{\"number\":0,\"title\":\"Local Analysis: feat/slack-frontend-v2 (73 modified)\",\"author\":\"Leonid Bugaev\",\"branch\":\"feat/slack-frontend-v2\",\"base\":\"main\"},\"files\":[{\"filename\":\"CLAUDE.md\",\"additions\":14,\"deletions\":1,\"changes\":15,\"patch\":\"diff --git a/CLAUDE.md b/CLAUDE.md\\nindex 009271f9..ae68c57e 100644\\n--- a/CLAUDE.md\\n+++ b/CLAUDE.md\\n@@ -150,4 +150,17 @@ Configuration supports:\\n    - Safe JSON parsing: `try { JSON.parse(output) } catch(e) { log(\\\"Error:\\\", e) }`\\n    - Validate structure: `log(\\\"Is array?\\\", Array.isArray(outputs[\\\"check-name\\\"]));`\\n \\n+6. **Tracing with OTel/Jaeger**:\\n+   - Enable telemetry: `VISOR_TELEMETRY_ENABLED=true`, `VISOR_TELEMETRY_SINK=otlp`,\\n+     `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://localhost:4318/v1/traces`\\n+   - Root span: `visor.run` (one per CLI/Slack execution)\\n+   - State spans: `engine.state.*` with `wave`, `wave_kind`, `session_id`\\n+   - Check spans: `visor.check.<checkId>` with `visor.check.id`, `visor.check.type`,\\n+     `visor.foreach.index` (for map fanout)\\n+   - Routing decisions: `visor.routing` events attached to the active state span; fields\\n+     include `trigger`, `action`, `source`, `target`, `scope`, `goto_event` (repeats\\n+     across waves show routing loops)\\n+   - Wave visibility: `engine.state.level_dispatch` includes `level_size` and\\n+     `level_checks_preview` for the planned wave\\n+\\n See `docs/debugging.md` for comprehensive debugging guide.\\n\",\"status\":\"modified\"},{\"filename\":\"README.md\",\"additions\":7,\"deletions\":1,\"changes\":8,\"patch\":\"diff --git a/README.md b/README.md\\nindex 41131861..d92f02cc 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -534,6 +534,12 @@ steps:\\n     session_mode: append    # Shares history for full conversation\\n ```\\n \\n+You can also reuse the **same check‚Äôs** session when it loops back to itself, using:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` with `session_mode: append`\\n+\\n+See the standalone example at `examples/session-reuse-self.yaml` and the detailed guide in [docs/advanced-ai.md](docs/advanced-ai.md).\\n+\\n Learn more: [docs/advanced-ai.md](docs/advanced-ai.md)\\n \\n ## üìã Schema-Template System\\n\",\"status\":\"modified\"},{\"filename\":\"docs/advanced-ai.md\",\"additions\":61,\"deletions\":2,\"changes\":63,\"patch\":\"diff --git a/docs/advanced-ai.md b/docs/advanced-ai.md\\nindex bb8e684a..879285ed 100644\\n--- a/docs/advanced-ai.md\\n+++ b/docs/advanced-ai.md\\n@@ -1,7 +1,7 @@\\n ## üß† Advanced AI Features\\n \\n ### AI Session Reuse\\n-Use `reuse_ai_session: true` on dependent checks to continue conversation context with the AI across checks. This improves follow‚Äëups and consistency.\\n+Use `reuse_ai_session` on checks to continue conversation context with the AI across steps. This improves follow‚Äëups and consistency for follow‚Äëon analysis and chat‚Äëstyle flows.\\n \\n **Session Modes:**\\n - **`clone` (default)**: Creates a copy of the conversation history. Each check gets an independent session with the same starting context. Changes made by one check don't affect others.\\n@@ -26,6 +26,65 @@ steps:\\n     depends_on: [security-remediation]\\n     reuse_ai_session: true\\n     session_mode: append  # Share history - sees full conversation\\n+\\n+#### Reusing your own session: `reuse_ai_session: self`\\n+\\n+Sometimes the step you want to loop back into is the AI step itself (e.g. Slack assistants or multi‚Äëturn internal tools). For that case you can use:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` ‚Äì the step reuses its **own** Probe session when it runs again in the same engine run.\\n+- `session_mode: append` ‚Äì makes the follow‚Äëup behave like a normal conversation turn.\\n+\\n+On the first run of the step, Visor creates a new ProbeAgent session and registers it. If routing (`on_success.goto`, `goto_js`, etc.) later jumps back to the same step within the same run, the engine:\\n+\\n+- Finds the last result for that step in the current run.\\n+- Reads the `sessionId` stored in the result.\\n+- Calls the AI provider again using `executeReviewWithSessionReuse` with that session id.\\n+\\n+Simple example (no transport wiring, just CLI/tests):\\n+\\n+```yaml\\n+version: \\\"2.0\\\"\\n+\\n+steps:\\n+  seed:\\n+    type: script\\n+    content: |\\n+      return { text: \\\"hello from seed\\\" };\\n+\\n+  convo:\\n+    type: ai\\n+    depends_on: [seed]\\n+    reuse_ai_session: self\\n+    session_mode: append\\n+    ai:\\n+      provider: mock\\n+      model: mock\\n+      disableTools: true\\n+      allowedTools: []\\n+      system_prompt: \\\"You are a tiny echo assistant.\\\"\\n+    prompt: |\\n+      Seed message: {{ outputs['seed'].text }}\\n+\\n+      Past convo outputs in this run:\\n+      {% assign hist = outputs_history['convo'] | default: empty %}\\n+      {% if hist and hist.size > 0 %}\\n+      {% for h in hist %}\\n+      - Previous reply {{ forloop.index }}.\\n+      {% endfor %}\\n+      {% else %}\\n+      - No previous replies yet.\\n+      {% endif %}\\n+    on_success:\\n+      goto_js: |\\n+        // Example: re‚Äëenter this step up to 3 times in a single run\\n+        return attempt < 3 ? 'convo' : null;\\n+```\\n+\\n+The corresponding testable example lives at:\\n+\\n+- `examples/session-reuse-self.yaml`\\n+\\n+This keeps the configuration small but shows how to wire `reuse_ai_session: self` and `session_mode: append` without touching higher‚Äëlevel workflows like `tyk-assistant`.\\n ```\\n \\n **When to use each mode:**\\n\",\"status\":\"modified\"},{\"filename\":\"docs/recipes.md\",\"additions\":149,\"deletions\":3,\"changes\":152,\"patch\":\"diff --git a/docs/recipes.md b/docs/recipes.md\\nindex 1015bc6c..1089e9a5 100644\\n--- a/docs/recipes.md\\n+++ b/docs/recipes.md\\n@@ -321,8 +321,154 @@ Tip: When you define a JSON Schema, you generally do **not** need to tell the mo\\n - Avoid noisy fallbacks like `(outputs['x']?.kind ?? '') === 'status'` when `outputs['x']?.kind === 'status'` is equivalent.\\n - These conventions apply uniformly to any provider (`ai`, `command`, `script`, `github`, `http_client`, etc).\\n \\n+### Command step best practices\\n+\\n+When using `type: command` steps:\\n+\\n+**Avoid external tool dependencies** like `jq`, `yq`, `python`, etc.:\\n+- They may not be installed in all environments (GitHub Actions, Docker, CI)\\n+- Use `transform_js` to parse and transform output instead\\n+- Keep shell commands simple: `grep`, `sed`, `awk`, `sort`, `head` are universally available\\n+\\n+```yaml\\n+# Bad - requires jq\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | jq -R -s 'split(\\\"\\\\n\\\")'\\n+  parseJson: true\\n+\\n+# Good - use transform_js for parsing\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | sort -u\\n+  transform_js: |\\n+    const lines = (output || '').trim().split('\\\\n').filter(Boolean);\\n+    return { data: lines, count: lines.length };\\n+```\\n+\\n+**Prefer line-separated output** over JSON from shell:\\n+- Simple to parse with `transform_js`\\n+- No need for `parseJson: true`\\n+- More robust across different shells/environments\\n+\\n+**Use transform_js for structured output**:\\n+- The sandbox provides `output` (command stdout as string)\\n+- Return an object with the fields you need\\n+- Works consistently across all environments\\n+\\n+### Testing workflows with `--no-mocks`\\n+\\n+The `--no-mocks` flag runs your test cases with real providers instead of injecting mock responses. This is essential for:\\n+\\n+1. **Debugging integration issues** - See actual API responses and errors\\n+2. **Capturing realistic mock data** - Get real output to copy into your test cases\\n+3. **Validating credentials** - Verify environment variables are set correctly\\n+4. **Developing new workflows** - Build tests incrementally with real data\\n+\\n+#### Basic usage\\n+\\n+```bash\\n+# Run all test cases with real providers\\n+visor test --config my-workflow.yaml --no-mocks\\n+\\n+# Run a specific test case with real providers\\n+visor test --config my-workflow.yaml --no-mocks --only \\\"my-test-case\\\"\\n+```\\n+\\n+#### Suggested mocks output\\n+\\n+When running with `--no-mocks`, Visor captures each step's output and prints it as YAML you can copy directly into your test case:\\n+\\n+```\\n+üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)\\n+   Step outputs will be captured and printed as suggested mocks\\n+\\n+... test execution ...\\n+\\n+üìã Suggested mocks (copy to your test case):\\n+mocks:\\n+  extract-keys:\\n+    data:\\n+      - PROJ-123\\n+      - DEV-456\\n+    count: 2\\n+  fetch-issues:\\n+    data:\\n+      - key: PROJ-123\\n+        summary: Fix authentication bug\\n+        status: In Progress\\n+```\\n+\\n+Copy the YAML under `mocks:` into your test case's `mocks:` section.\\n+\\n+#### Workflow for building tests\\n+\\n+1. **Start with a minimal test case** (no mocks):\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+   ```\\n+\\n+2. **Run with `--no-mocks`** to capture real outputs:\\n+   ```bash\\n+   visor test --config workflow.yaml --no-mocks --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+3. **Copy the suggested mocks** into your test case:\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+         mocks:\\n+           extract-keys:\\n+             data: [\\\"PROJ-123\\\"]\\n+             count: 1\\n+           # ... rest of captured mocks\\n+   ```\\n+\\n+4. **Add assertions** based on the real data:\\n+   ```yaml\\n+         expect:\\n+           workflow_output:\\n+             - path: issue_count\\n+               equals: 1\\n+   ```\\n+\\n+5. **Run normally** to verify mocks work:\\n+   ```bash\\n+   visor test --config workflow.yaml --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+#### Debugging with `--no-mocks`\\n+\\n+When a test fails with mocks, use `--no-mocks` to see what's actually happening:\\n+\\n+```bash\\n+# See real API responses and errors\\n+visor test --config workflow.yaml --no-mocks --only \\\"failing-test\\\"\\n+\\n+# Common issues revealed:\\n+# - Missing or expired credentials\\n+# - API endpoint changes\\n+# - Unexpected response formats\\n+# - Network/timeout issues\\n+```\\n+\\n+The real error messages and responses help identify whether the issue is with your mocks or the actual integration.\\n+\\n ### More examples\\n \\n-- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags  \\n-- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration  \\n+- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags\\n+- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration\\n - `examples/` ‚Äì MCP, Jira, and advanced configs\\n\",\"status\":\"modified\"},{\"filename\":\"package-lock.json\",\"additions\":252,\"deletions\":9,\"changes\":261,\"patch\":\"diff --git a/package-lock.json b/package-lock.json\\nindex 5fb3ad89..e5b10d27 100644\\n--- a/package-lock.json\\n+++ b/package-lock.json\\n@@ -7,6 +7,7 @@\\n     \\\"\\\": {\\n       \\\"name\\\": \\\"@probelabs/visor\\\",\\n       \\\"version\\\": \\\"0.1.42\\\",\\n+      \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"MIT\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@actions/core\\\": \\\"^1.11.1\\\",\\n@@ -16,7 +17,7 @@\\n         \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n         \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n         \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n         \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n         \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -54,6 +55,7 @@\\n         \\\"husky\\\": \\\"^9.1.7\\\",\\n         \\\"jest\\\": \\\"^30.1.3\\\",\\n         \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+        \\\"patch-package\\\": \\\"^8.0.1\\\",\\n         \\\"prettier\\\": \\\"^3.6.2\\\",\\n         \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n         \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n@@ -5942,9 +5944,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/maid\\\": {\\n-      \\\"version\\\": \\\"0.0.21\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.21.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-H3EA2tDjWsAPgXZAwfHrNoEXnHmQFS9+JGDK/YpeiF3g3FxPgnWBgtgYPgtNuETQsEXxR1RzDgQVJL2JHWRRHw==\\\",\\n+      \\\"version\\\": \\\"0.0.22\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.22.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-FYcyyP4BlVrHQmBznXFPpMbvUqFI6wbecHUUe4S+yIeDvJ8W4bP0ndZ0cMoc2p7hekWYAG4RtlohQBr8e4TKOg==\\\",\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@types/dagre\\\": \\\"^0.7.53\\\",\\n@@ -5960,9 +5962,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/probe\\\": {\\n-      \\\"version\\\": \\\"0.6.0-rc169\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc169.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-/laV3+1TSjASru8gGSvGwyjQ7jalMkFuZETl35uv37J1Hb4eG95CbKGU5NihcCrLXR44q1lKJiQGHXcAav2mBA==\\\",\\n+      \\\"version\\\": \\\"0.6.0-rc176\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc176.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-w/pAvnz2/yXb4FQXZV2QazxuW02xsI/wRg36yxZoLSG0O/hIncjF4rquZe3O+emceI2Ks5zbrA7cgQEdpwT8vA==\\\",\\n       \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n@@ -5972,7 +5974,7 @@\\n         \\\"@ai-sdk/openai\\\": \\\"^2.0.10\\\",\\n         \\\"@anthropic-ai/claude-agent-sdk\\\": \\\"^0.1.46\\\",\\n         \\\"@modelcontextprotocol/sdk\\\": \\\"^1.0.0\\\",\\n-        \\\"@probelabs/maid\\\": \\\"^0.0.21\\\",\\n+        \\\"@probelabs/maid\\\": \\\"^0.0.22\\\",\\n         \\\"adm-zip\\\": \\\"^0.5.16\\\",\\n         \\\"ai\\\": \\\"^5.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -8192,6 +8194,13 @@\\n         \\\"node\\\": \\\">= 20\\\"\\n       }\\n     },\\n+    \\\"node_modules/@yarnpkg/lockfile\\\": {\\n+      \\\"version\\\": \\\"1.1.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@yarnpkg/lockfile/-/lockfile-1.1.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-GpSwvyXOcOOlV70vbnzjj4fW5xW/FdUF6nQEt1ENy7m4ZCczi1+/buVUPAqmGfqznsORNFzUMjctTIp8a9tuCQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"BSD-2-Clause\\\"\\n+    },\\n     \\\"node_modules/accepts\\\": {\\n       \\\"version\\\": \\\"1.3.8\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/accepts/-/accepts-1.3.8.tgz\\\",\\n@@ -9180,6 +9189,25 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/call-bind\\\": {\\n+      \\\"version\\\": \\\"1.0.8\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind/-/call-bind-1.0.8.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-oKlSFMcMwpUg2ednkhQ454wfWiU/ul3CkJe/PEHcTKuiX6RpbehUiFMXu13HalGZxfUwCQzZG747YXBn1im9ww==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind-apply-helpers\\\": \\\"^1.0.0\\\",\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"set-function-length\\\": \\\"^1.2.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/call-bind-apply-helpers\\\": {\\n       \\\"version\\\": \\\"1.0.2\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind-apply-helpers/-/call-bind-apply-helpers-1.0.2.tgz\\\",\\n@@ -10613,6 +10641,24 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/define-data-property\\\": {\\n+      \\\"version\\\": \\\"1.1.4\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/define-data-property/-/define-data-property-1.1.4.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-rBMvIzlpA8v6E+SJZoo++HAYqsLrkg7MSfIinMPFhmkorw7X+dOXVJQs+QT69zGkzMyfDnIMN2Wid1+NbL3T+A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/define-lazy-prop\\\": {\\n       \\\"version\\\": \\\"3.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/define-lazy-prop/-/define-lazy-prop-3.0.0.tgz\\\",\\n@@ -11794,6 +11840,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/find-yarn-workspace-root\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/find-yarn-workspace-root/-/find-yarn-workspace-root-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-1IMnbjt4KzsQfnhnzNd8wUEgXZ44IzZaZmnLYx7D5FZlaHt2gW20Cri8Q+E/t5tIj4+epTBub+2Zxu/vNILzqQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Apache-2.0\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"micromatch\\\": \\\"^4.0.2\\\"\\n+      }\\n+    },\\n     \\\"node_modules/fix-dts-default-cjs-exports\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/fix-dts-default-cjs-exports/-/fix-dts-default-cjs-exports-1.0.1.tgz\\\",\\n@@ -12267,6 +12323,19 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/has-property-descriptors\\\": {\\n+      \\\"version\\\": \\\"1.0.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/has-property-descriptors/-/has-property-descriptors-1.0.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-55JNKuIW+vq4Ke1BjOTjM2YctQIvCT7GFzHwmfZPGo5wnrgkid0YQtnAleFSqumZm4az3n2BS+erby5ipJdgrg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/has-symbols\\\": {\\n       \\\"version\\\": \\\"1.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/has-symbols/-/has-symbols-1.1.0.tgz\\\",\\n@@ -12784,6 +12853,13 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/isarray\\\": {\\n+      \\\"version\\\": \\\"2.0.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/isarray/-/isarray-2.0.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-xHjhDr3cNBK0BzdUJSPXZntQUx/mwMS5Rw4A7lPJ90XGAO6ISP/ePDNuo0vhqOZU+UD5JoodwCAAoZQd3FeAKw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\"\\n+    },\\n     \\\"node_modules/isexe\\\": {\\n       \\\"version\\\": \\\"2.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz\\\",\\n@@ -13594,6 +13670,26 @@\\n       \\\"integrity\\\": \\\"sha512-NM8/P9n3XjXhIZn1lLhkFaACTOURQXjWhV4BA/RnOv8xvgqtqpAX9IO4mRQxSx1Rlo4tqzeqb0sOlruaOy3dug==\\\",\\n       \\\"license\\\": \\\"MIT\\\"\\n     },\\n+    \\\"node_modules/json-stable-stringify\\\": {\\n+      \\\"version\\\": \\\"1.3.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify/-/json-stable-stringify-1.3.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-qtYiSSFlwot9XHtF9bD9c7rwKjr+RecWT//ZnPvSmEjpV5mmPOCN4j8UjY5hbjNkOwZ/jQv3J6R1/pL7RwgMsg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind\\\": \\\"^1.0.8\\\",\\n+        \\\"call-bound\\\": \\\"^1.0.4\\\",\\n+        \\\"isarray\\\": \\\"^2.0.5\\\",\\n+        \\\"jsonify\\\": \\\"^0.0.1\\\",\\n+        \\\"object-keys\\\": \\\"^1.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/json-stable-stringify-without-jsonify\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify-without-jsonify/-/json-stable-stringify-without-jsonify-1.0.1.tgz\\\",\\n@@ -13633,6 +13729,16 @@\\n         \\\"graceful-fs\\\": \\\"^4.1.6\\\"\\n       }\\n     },\\n+    \\\"node_modules/jsonify\\\": {\\n+      \\\"version\\\": \\\"0.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/jsonify/-/jsonify-0.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-2/Ki0GcmuqSrgFyelQq9M05y7PS0mEwuIzrf3f1fPqkVDVRvZrPZtVSMHxdgo8Aq0sxAOb/cr2aqqA3LeWHVPg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Public Domain\\\",\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/katex\\\": {\\n       \\\"version\\\": \\\"0.16.25\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/katex/-/katex-0.16.25.tgz\\\",\\n@@ -13676,6 +13782,16 @@\\n       \\\"integrity\\\": \\\"sha512-Ls993zuzfayK269Svk9hzpeGUKob/sIgZzyHYdjQoAdQetRKpOLj+k/QQQ/6Qi0Yz65mlROrfd+Ev+1+7dz9Kw==\\\",\\n       \\\"dev\\\": true\\n     },\\n+    \\\"node_modules/klaw-sync\\\": {\\n+      \\\"version\\\": \\\"6.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/klaw-sync/-/klaw-sync-6.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-nIeuVSzdCCs6TDPTqI8w1Yre34sSq7AkZ4B3sfOBbI2CgVSB4Du4aLQijFU2+lhAFCwt9+42Hel6lQNIv6AntQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"graceful-fs\\\": \\\"^4.1.11\\\"\\n+      }\\n+    },\\n     \\\"node_modules/kolorist\\\": {\\n       \\\"version\\\": \\\"1.8.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/kolorist/-/kolorist-1.8.0.tgz\\\",\\n@@ -14785,6 +14901,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n       }\\n     },\\n+    \\\"node_modules/object-keys\\\": {\\n+      \\\"version\\\": \\\"1.1.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/object-keys/-/object-keys-1.1.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NuAESUOUMrlIXOfHKzD6bpPu3tYt3xvjNdRIQ+FeT0lNb4K8WR70CaDxhuNguS2XG+GjkyMwOzsN5ZktImfhLA==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/ohash\\\": {\\n       \\\"version\\\": \\\"2.0.11\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/ohash/-/ohash-2.0.11.tgz\\\",\\n@@ -15022,6 +15148,95 @@\\n         \\\"node\\\": \\\">= 0.8\\\"\\n       }\\n     },\\n+    \\\"node_modules/patch-package\\\": {\\n+      \\\"version\\\": \\\"8.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/patch-package/-/patch-package-8.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-VsKRIA8f5uqHQ7NGhwIna6Bx6D9s/1iXlA1hthBVBEbkq+t4kXD0HHt+rJhf/Z+Ci0F/HCB2hvn0qLdLG+Qxlw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"@yarnpkg/lockfile\\\": \\\"^1.1.0\\\",\\n+        \\\"chalk\\\": \\\"^4.1.2\\\",\\n+        \\\"ci-info\\\": \\\"^3.7.0\\\",\\n+        \\\"cross-spawn\\\": \\\"^7.0.3\\\",\\n+        \\\"find-yarn-workspace-root\\\": \\\"^2.0.0\\\",\\n+        \\\"fs-extra\\\": \\\"^10.0.0\\\",\\n+        \\\"json-stable-stringify\\\": \\\"^1.0.2\\\",\\n+        \\\"klaw-sync\\\": \\\"^6.0.0\\\",\\n+        \\\"minimist\\\": \\\"^1.2.6\\\",\\n+        \\\"open\\\": \\\"^7.4.2\\\",\\n+        \\\"semver\\\": \\\"^7.5.3\\\",\\n+        \\\"slash\\\": \\\"^2.0.0\\\",\\n+        \\\"tmp\\\": \\\"^0.2.4\\\",\\n+        \\\"yaml\\\": \\\"^2.2.2\\\"\\n+      },\\n+      \\\"bin\\\": {\\n+        \\\"patch-package\\\": \\\"index.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14\\\",\\n+        \\\"npm\\\": \\\">5\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/ci-info\\\": {\\n+      \\\"version\\\": \\\"3.9.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/ci-info/-/ci-info-3.9.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NIxF55hv4nSqQswkAeiOi1r83xy8JldOFDTWiug55KBu9Jnblncd2U6ViHmYgHf01TPZS77NJBhBMKdWj9HQMQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"funding\\\": [\\n+        {\\n+          \\\"type\\\": \\\"github\\\",\\n+          \\\"url\\\": \\\"https://github.com/sponsors/sibiraj-s\\\"\\n+        }\\n+      ],\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/is-docker\\\": {\\n+      \\\"version\\\": \\\"2.2.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/is-docker/-/is-docker-2.2.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-F+i2BKsFrH66iaUFc0woD8sLy8getkwTwtOBjvs56Cx4CgJDeKQeqfz8wAYiSb8JOprWhHH5p77PbmYCvvUuXQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"bin\\\": {\\n+        \\\"is-docker\\\": \\\"cli.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/open\\\": {\\n+      \\\"version\\\": \\\"7.4.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/open/-/open-7.4.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-MVHddDVweXZF3awtlAS+6pgKLlm/JgxZ90+/NBurBoQctVOOB/zDdVjcyPzQ+0laDGbsWgrRkflI65sQeOgT9Q==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"is-docker\\\": \\\"^2.0.0\\\",\\n+        \\\"is-wsl\\\": \\\"^2.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/slash\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/slash/-/slash-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-ZYKh3Wh2z1PpEXWr0MpSBZ0V6mZHAQfYevttO11c51CaWjGTaadiKZ+wVt1PbMlDV5qhMFslpZCemhwOK7C89A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=6\\\"\\n+      }\\n+    },\\n     \\\"node_modules/path-data-parser\\\": {\\n       \\\"version\\\": \\\"0.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/path-data-parser/-/path-data-parser-0.1.0.tgz\\\",\\n@@ -16473,6 +16688,24 @@\\n         \\\"node\\\": \\\">= 0.8.0\\\"\\n       }\\n     },\\n+    \\\"node_modules/set-function-length\\\": {\\n+      \\\"version\\\": \\\"1.2.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/set-function-length/-/set-function-length-1.2.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-pgRc4hJ4/sNjWCSS9AmnS40x3bNMDTknHgL5UaMBTMyJnU90EgWh1Rz+MC9eFu4BuN/UwZjKQuY/1v3rM7HMfg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"define-data-property\\\": \\\"^1.1.4\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"function-bind\\\": \\\"^1.1.2\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\",\\n+        \\\"has-property-descriptors\\\": \\\"^1.0.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/setprototypeof\\\": {\\n       \\\"version\\\": \\\"1.2.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/setprototypeof/-/setprototypeof-1.2.0.tgz\\\",\\n@@ -17384,6 +17617,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/tmp\\\": {\\n+      \\\"version\\\": \\\"0.2.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/tmp/-/tmp-0.2.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-voyz6MApa1rQGUxT3E+BK7/ROe8itEx7vD8/HEvt4xwXucvQ5G5oeEiHkmHZJuBO21RpOf+YYm9MOivj709jow==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14.14\\\"\\n+      }\\n+    },\\n     \\\"node_modules/tmpl\\\": {\\n       \\\"version\\\": \\\"1.0.5\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/tmpl/-/tmpl-1.0.5.tgz\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"package.json\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/package.json b/package.json\\nindex f9ccbb6c..302d728f 100644\\n--- a/package.json\\n+++ b/package.json\\n@@ -49,6 +49,7 @@\\n     \\\"test:with-build\\\": \\\"npm run build:cli && jest\\\",\\n     \\\"test:yaml\\\": \\\"node dist/index.js test --progress compact\\\",\\n     \\\"prepare\\\": \\\"husky\\\",\\n+    \\\"postinstall\\\": \\\"patch-package\\\",\\n     \\\"pre-commit\\\": \\\"lint-staged\\\",\\n     \\\"deploy:site\\\": \\\"cd site && npx wrangler pages deploy . --project-name=visor-site --commit-dirty=true\\\",\\n     \\\"deploy:worker\\\": \\\"npx wrangler deploy\\\",\\n@@ -96,7 +97,7 @@\\n     \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n     \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n     \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n     \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n     \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n     \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -143,6 +144,7 @@\\n     \\\"husky\\\": \\\"^9.1.7\\\",\\n     \\\"jest\\\": \\\"^30.1.3\\\",\\n     \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+    \\\"patch-package\\\": \\\"^8.0.1\\\",\\n     \\\"prettier\\\": \\\"^3.6.2\\\",\\n     \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n     \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"src/ai-review-service.ts\",\"additions\":101,\"deletions\":151,\"changes\":252,\"patch\":\"diff --git a/src/ai-review-service.ts b/src/ai-review-service.ts\\nindex 6b9c5b2e..f18accea 100644\\n--- a/src/ai-review-service.ts\\n+++ b/src/ai-review-service.ts\\n@@ -65,6 +65,13 @@ export interface AIReviewConfig {\\n   allowBash?: boolean;\\n   // Advanced bash command execution configuration\\n   bashConfig?: import('./types/config').BashConfig;\\n+  // Optional workspace root and allowed folders for ProbeAgent.\\n+  // When provided, these are forwarded to ProbeAgent so tools like search/query\\n+  // operate inside the isolated workspace/projects instead of the Visor repo root.\\n+  path?: string;\\n+  allowedFolders?: string[];\\n+  // Completion prompt for post-completion validation/review (runs after attempt_completion)\\n+  completionPrompt?: string;\\n }\\n \\n export interface AIDebugInfo {\\n@@ -278,7 +285,7 @@ export class AIReviewService {\\n     try {\\n       const call = this.callProbeAgent(prompt, schema, debugInfo, checkName, sessionId);\\n       const timeoutMs = Math.max(0, this.config.timeout || 0);\\n-      const { response, effectiveSchema } =\\n+      const { response, effectiveSchema, sessionId: usedSessionId } =\\n         timeoutMs > 0 ? await this.withTimeout(call, timeoutMs, 'AI review') : await call;\\n       const processingTime = Date.now() - startTime;\\n \\n@@ -290,6 +297,11 @@ export class AIReviewService {\\n \\n       const result = this.parseAIResponse(response, debugInfo, effectiveSchema);\\n \\n+      // Expose the session ID used for this call so the engine can reuse it later\\n+      try {\\n+        (result as any).sessionId = usedSessionId;\\n+      } catch {}\\n+\\n       if (debugInfo) {\\n         result.debug = debugInfo;\\n       }\\n@@ -651,17 +663,34 @@ ${prContext}${slackContextXml}\\n     const prContextInfo = prInfo as PRInfo & {\\n       isPRContext?: boolean;\\n       includeCodeContext?: boolean;\\n+      slackConversation?: unknown;\\n     };\\n     const isIssue = prContextInfo.isIssue === true;\\n \\n     // Check if we should include code context (diffs)\\n     const isPRContext = prContextInfo.isPRContext === true;\\n-    // In PR context, always include diffs. Otherwise check the flag.\\n-    const includeCodeContext = isPRContext || prContextInfo.includeCodeContext !== false;\\n+    const isSlackMode = prContextInfo.slackConversation !== undefined;\\n+\\n+    // Determine whether to include code context:\\n+    // - In explicit PR context (GitHub PR events), always include diffs\\n+    // - In Slack mode, default to NO code context unless explicitly requested\\n+    // - Otherwise, include code context unless explicitly disabled\\n+    let includeCodeContext: boolean;\\n+    if (isPRContext) {\\n+      includeCodeContext = true;\\n+    } else if (isSlackMode) {\\n+      // In Slack mode, only include code context if explicitly set to true\\n+      includeCodeContext = prContextInfo.includeCodeContext === true;\\n+    } else {\\n+      // Default: include unless explicitly disabled\\n+      includeCodeContext = prContextInfo.includeCodeContext !== false;\\n+    }\\n \\n     // Log the decision for transparency (debug level)\\n     if (isPRContext) {\\n       log('üîç Including full code diffs in AI context (PR mode)');\\n+    } else if (isSlackMode && !includeCodeContext) {\\n+      log('üí¨ Slack mode: excluding code diffs (use includeCodeContext: true to enable)');\\n     } else if (!includeCodeContext) {\\n       log('üìä Including only file summary in AI context (no diffs)');\\n     } else {\\n@@ -1537,7 +1566,15 @@ ${'='.repeat(60)}\\n     debugInfo?: AIDebugInfo,\\n     _checkName?: string,\\n     providedSessionId?: string\\n-  ): Promise<{ response: string; effectiveSchema?: string }> {\\n+  ): Promise<{ response: string; effectiveSchema?: string; sessionId: string }> {\\n+    // Derive a stable session ID for this call so the engine can reuse it later\\n+    const sessionId =\\n+      providedSessionId ||\\n+      (() => {\\n+        const timestamp = new Date().toISOString();\\n+        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n+      })();\\n+\\n     // Handle mock model/provider\\n     if (this.config.model === 'mock' || this.config.provider === 'mock') {\\n       const inJest = !!process.env.JEST_WORKER_ID;\\n@@ -1545,19 +1582,17 @@ ${'='.repeat(60)}\\n       if (!inJest) {\\n         // Fast path for CLI/integration: synthesize a mock response without invoking ProbeAgent\\n         const response = await this.generateMockResponse(prompt, _checkName, schema);\\n-        return { response, effectiveSchema: typeof schema === 'object' ? 'custom' : schema };\\n+        return {\\n+          response,\\n+          effectiveSchema: typeof schema === 'object' ? 'custom' : schema,\\n+          sessionId,\\n+        };\\n       }\\n       // In unit tests, still invoke ProbeAgent so tests can assert on options (schema) passed in\\n       // Fall through to normal flow below\\n     }\\n \\n     // Create ProbeAgent instance with proper options\\n-    const sessionId =\\n-      providedSessionId ||\\n-      (() => {\\n-        const timestamp = new Date().toISOString();\\n-        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n-      })();\\n \\n     log('ü§ñ Creating ProbeAgent for AI review...');\\n     log(`üÜî Session ID: ${sessionId}`);\\n@@ -1674,6 +1709,35 @@ ${'='.repeat(60)}\\n         (options as any).bashConfig = this.config.bashConfig;\\n       }\\n \\n+      // Pass completion prompt for post-completion validation/review\\n+      if (this.config.completionPrompt !== undefined) {\\n+        (options as any).completionPrompt = this.config.completionPrompt;\\n+      }\\n+\\n+      // Propagate workspace / allowed folders to ProbeAgent so that tools\\n+      // operate inside the isolated workspace and project checkouts instead\\n+      // of the Visor repository root.\\n+      try {\\n+        const cfgAny: any = this.config as any;\\n+        const allowedFolders = cfgAny.allowedFolders as string[] | undefined;\\n+        const workspacePath =\\n+          cfgAny.workspacePath || cfgAny.path || (Array.isArray(allowedFolders) && allowedFolders[0]);\\n+        if (Array.isArray(allowedFolders) && allowedFolders.length > 0) {\\n+          (options as any).allowedFolders = allowedFolders;\\n+          if (!options.path && workspacePath) {\\n+            (options as any).path = workspacePath;\\n+          }\\n+          log(`üóÇÔ∏è ProbeAgent workspace config:`);\\n+          log(`   path (cwd): ${(options as any).path}`);\\n+          log(`   allowedFolders[0]: ${allowedFolders[0]}`);\\n+        } else if (workspacePath) {\\n+          (options as any).path = workspacePath;\\n+          log(`üóÇÔ∏è ProbeAgent path: ${workspacePath} (no allowedFolders)`);\\n+        }\\n+      } catch {\\n+        // Best-effort only; fall back to ProbeAgent defaults on error.\\n+      }\\n+\\n       // Add provider-specific options if configured\\n       if (this.config.provider) {\\n         // Map claude-code to anthropic for ProbeAgent compatibility\\n@@ -2037,7 +2101,7 @@ ${'='.repeat(60)}\\n         log(`üîß Debug: Registered AI session for potential reuse: ${sessionId}`);\\n       }\\n \\n-      return { response, effectiveSchema };\\n+      return { response, effectiveSchema, sessionId };\\n     } catch (error) {\\n       console.error('‚ùå ProbeAgent failed:', error);\\n       throw new Error(\\n@@ -2191,13 +2255,23 @@ ${'='.repeat(60)}\\n         // For other schemas (code-review, etc.), extract and parse JSON with boundary detection\\n         log('üîç Extracting JSON from AI response...');\\n \\n-        // Try direct parsing first - if AI returned pure JSON\\n+        // Sanitize response: strip BOM, zero-width chars, and other invisible characters\\n+        // that can cause JSON parsing to fail even when the text looks valid\\n+        const sanitizedResponse = response\\n+          .replace(/^\\\\uFEFF/, '') // BOM\\n+          .replace(/[\\\\u200B-\\\\u200D\\\\uFEFF\\\\u00A0]/g, '') // Zero-width chars, NBSP\\n+          .replace(/[\\\\x00-\\\\x08\\\\x0B\\\\x0C\\\\x0E-\\\\x1F\\\\x7F]/g, '') // Control chars (except \\\\t \\\\n \\\\r)\\n+          .trim();\\n+\\n+        // Try direct JSON parsing - no bracket-matching extraction\\n+        // JSON validation is offloaded to Probe agent when schema is provided\\n         try {\\n-          reviewData = JSON.parse(response.trim());\\n+          reviewData = JSON.parse(sanitizedResponse);\\n           log('‚úÖ Successfully parsed direct JSON response');\\n           if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-        } catch {\\n-          log('üîç Direct parsing failed, trying to extract JSON from response...');\\n+        } catch (parseErr) {\\n+          const errMsg = parseErr instanceof Error ? parseErr.message : String(parseErr);\\n+          log(`üîç Direct JSON parsing failed: ${errMsg}`);\\n \\n           // If the response starts with \\\"I cannot\\\" or similar, it's likely a refusal\\n           if (\\n@@ -2210,66 +2284,16 @@ ${'='.repeat(60)}\\n             };\\n           }\\n \\n-          // Try to extract JSON using improved method with proper bracket matching\\n-          const jsonString = this.extractJsonFromResponse(response);\\n-\\n-          if (jsonString) {\\n-            try {\\n-              reviewData = JSON.parse(jsonString);\\n-              log('‚úÖ Successfully parsed extracted JSON');\\n-              if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-            } catch {\\n-              log('üîß Extracted JSON parsing failed, falling back to plain text handling...');\\n-\\n-              // Check if response is plain text and doesn't contain structured data\\n-              if (!response.includes('{') && !response.includes('}')) {\\n-                log('üîß Plain text response detected, creating structured fallback...');\\n-\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              } else {\\n-                // Fallback: treat the entire response as an issue\\n-                log('üîß Creating fallback response from non-JSON content...');\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              }\\n-            }\\n-          } else {\\n-            // No JSON found at all - treat as plain text response\\n-            log('üîß No JSON found in response, treating as plain text...');\\n-            reviewData = {\\n-              issues: [\\n-                {\\n-                  file: 'AI_RESPONSE',\\n-                  line: 1,\\n-                  ruleId: 'ai/raw_response',\\n-                  message: response,\\n-                  severity: 'info',\\n-                  category: 'documentation',\\n-                },\\n-              ],\\n-            };\\n-          }\\n+          // Not valid JSON - treat entire response as text output\\n+          // This allows Probe (or other AI providers) to handle JSON validation\\n+          // and avoids false positives from bracket-matching (e.g., mermaid diagrams)\\n+          log('üîß Treating response as plain text (no JSON extraction)');\\n+          const trimmed = response.trim();\\n+          return {\\n+            issues: [],\\n+            output: { text: trimmed },\\n+            debug: debugInfo,\\n+          };\\n         }\\n       }\\n \\n@@ -2457,80 +2481,6 @@ ${'='.repeat(60)}\\n     }\\n   }\\n \\n-  /**\\n-   * Extract JSON from a response that might contain surrounding text\\n-   * Uses proper bracket matching to find valid JSON objects or arrays\\n-   */\\n-  private extractJsonFromResponse(response: string): string | null {\\n-    const text = response.trim();\\n-\\n-    // Try to find JSON objects first (higher priority)\\n-    let bestJson = this.findJsonWithBracketMatching(text, '{', '}');\\n-\\n-    // If no object found, try arrays\\n-    if (!bestJson) {\\n-      bestJson = this.findJsonWithBracketMatching(text, '[', ']');\\n-    }\\n-\\n-    return bestJson;\\n-  }\\n-\\n-  /**\\n-   * Find JSON with proper bracket matching to avoid false positives\\n-   */\\n-  private findJsonWithBracketMatching(\\n-    text: string,\\n-    openChar: string,\\n-    closeChar: string\\n-  ): string | null {\\n-    const firstIndex = text.indexOf(openChar);\\n-    if (firstIndex === -1) return null;\\n-\\n-    let depth = 0;\\n-    let inString = false;\\n-    let escaping = false;\\n-\\n-    for (let i = firstIndex; i < text.length; i++) {\\n-      const char = text[i];\\n-\\n-      if (escaping) {\\n-        escaping = false;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\\\\\\' && inString) {\\n-        escaping = true;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\"' && !escaping) {\\n-        inString = !inString;\\n-        continue;\\n-      }\\n-\\n-      if (!inString) {\\n-        if (char === openChar) {\\n-          depth++;\\n-        } else if (char === closeChar) {\\n-          depth--;\\n-          if (depth === 0) {\\n-            // Found matching closing bracket\\n-            const candidate = text.substring(firstIndex, i + 1);\\n-            try {\\n-              JSON.parse(candidate); // Validate it's actually valid JSON\\n-              return candidate;\\n-            } catch {\\n-              // This wasn't valid JSON, keep looking\\n-              break;\\n-            }\\n-          }\\n-        }\\n-      }\\n-    }\\n-\\n-    return null;\\n-  }\\n-\\n   /**\\n    * Generate mock response for testing\\n    */\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli-main.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/src/cli-main.ts b/src/cli-main.ts\\nindex 7dbef501..e671fbd8 100644\\n--- a/src/cli-main.ts\\n+++ b/src/cli-main.ts\\n@@ -158,6 +158,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n   }\\n   const only = getArg('--only');\\n   const bail = hasFlag('--bail');\\n+  const noMocks = hasFlag('--no-mocks');\\n   const listOnly = hasFlag('--list');\\n   const validateOnly = hasFlag('--validate');\\n   const progress = (getArg('--progress') as 'compact' | 'detailed' | undefined) || 'compact';\\n@@ -224,6 +225,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       const agg = await runSuites(multiFiles, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallelSuites: maxParallelSuites || Math.max(1, require('os').cpus()?.length || 2),\\n         maxParallel,\\n         promptMaxChars,\\n@@ -340,6 +342,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       runRes = await runner.runCases(tpath, suite, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallel,\\n         promptMaxChars,\\n         engineMode: 'state-machine',\\n@@ -609,6 +612,9 @@ export async function main(): Promise<void> {\\n     // Set environment variables early for proper logging in all modules\\n     process.env.VISOR_OUTPUT_FORMAT = options.output;\\n     process.env.VISOR_DEBUG = options.debug ? 'true' : 'false';\\n+    if (options.keepWorkspace) {\\n+      process.env.VISOR_KEEP_WORKSPACE = 'true';\\n+    }\\n     // Configure centralized logger\\n     configureLoggerFromCli({\\n       output: options.output,\\n@@ -716,6 +722,31 @@ export async function main(): Promise<void> {\\n       const threads = slackAny.threads || 'any';\\n       const allow = Array.isArray(slackAny.channel_allowlist) ? slackAny.channel_allowlist : [];\\n       const appToken = slackAny.app_token || process.env.SLACK_APP_TOKEN;\\n+\\n+      // Initialize telemetry for Slack mode (normally done later for CLI runs).\\n+      if ((config as any)?.telemetry) {\\n+        const t = (config as any).telemetry as {\\n+          enabled?: boolean;\\n+          sink?: 'otlp' | 'file' | 'console';\\n+          file?: { dir?: string; ndjson?: boolean };\\n+          tracing?: { auto_instrumentations?: boolean; trace_report?: { enabled?: boolean } };\\n+        };\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true' || !!t?.enabled,\\n+          sink:\\n+            (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || t?.sink || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR || t?.file?.dir, ndjson: !!t?.file?.ndjson },\\n+          autoInstrument: !!t?.tracing?.auto_instrumentations,\\n+          traceReport: !!t?.tracing?.trace_report?.enabled,\\n+        });\\n+      } else {\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true',\\n+          sink: (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR },\\n+        });\\n+      }\\n+\\n       const runner = new SlackSocketRunner(engine, config, {\\n         appToken,\\n         endpoint,\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli.ts\",\"additions\":5,\"deletions\":1,\"changes\":6,\"patch\":\"diff --git a/src/cli.ts b/src/cli.ts\\nindex 134429b2..b1c46213 100644\\n--- a/src/cli.ts\\n+++ b/src/cli.ts\\n@@ -76,6 +76,7 @@ export class CLI {\\n         parseInt(value, 10)\\n       )\\n       .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText())\\n       .exitOverride(); // Prevent automatic process.exit for better error handling\\n \\n@@ -154,6 +155,7 @@ export class CLI {\\n           parseInt(value, 10)\\n         )\\n         .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+        .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n         .allowUnknownOption(false)\\n         .allowExcessArguments(false) // Don't allow positional arguments\\n         .addHelpText('after', this.getExamplesText())\\n@@ -224,6 +226,7 @@ export class CLI {\\n         message: options.message,\\n         githubV2: false,\\n         slack: Boolean(options.slack),\\n+        keepWorkspace: Boolean(options.keepWorkspace),\\n       };\\n     } catch (error: unknown) {\\n       // Handle commander.js exit overrides for help/version ONLY\\n@@ -348,6 +351,7 @@ export class CLI {\\n       .option('--debug-port <port>', 'Port for debug server (default: 3456)', value =>\\n         parseInt(value, 10)\\n       )\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText());\\n \\n     // Get the basic help and append examples manually if addHelpText doesn't work\\n\",\"status\":\"modified\"},{\"filename\":\"src/config.ts\",\"additions\":67,\"deletions\":17,\"changes\":84,\"patch\":\"diff --git a/src/config.ts b/src/config.ts\\nindex 407efa2d..02abe9df 100644\\n--- a/src/config.ts\\n+++ b/src/config.ts\\n@@ -20,6 +20,7 @@ import { ConfigLoader, ConfigLoaderOptions } from './utils/config-loader';\\n import { ConfigMerger } from './utils/config-merger';\\n import Ajv from 'ajv';\\n import addFormats from 'ajv-formats';\\n+import { validateJsSyntax } from './utils/sandbox';\\n \\n /**\\n  * Valid event triggers for checks\\n@@ -477,16 +478,29 @@ export class ConfigManager {\\n \\n     logger.info(`Registered workflow '${workflowId}' for standalone execution`);\\n \\n-    // Create a COMPLETELY NEW visor config with ONLY the test checks\\n-    // This prevents any workflow fields from leaking into the config\\n-    const visorConfig: Partial<VisorConfig> = {\\n+    // For standalone workflow testing, use the workflow's steps as checks\\n+    // This allows the test runner to execute the workflow steps directly\\n+    const workflowSteps = workflowData.steps || {};\\n+\\n+    // Create a config that includes the workflow steps as checks,\\n+    // plus the tests section and workflow metadata for output computation\\n+    const visorConfig: Partial<VisorConfig> & { tests?: any; outputs?: any; inputs?: any } = {\\n       version: '1.0',\\n-      steps: tests,\\n-      checks: tests, // Backward compatibility\\n+      steps: workflowSteps,\\n+      checks: workflowSteps,\\n+      tests: tests, // Preserve test harness config (may be empty if stripped by test runner)\\n     };\\n \\n-    logger.debug(`Standalone workflow config has ${Object.keys(tests).length} test checks`);\\n-    logger.debug(`Test check names: ${Object.keys(tests).join(', ')}`);\\n+    // Preserve workflow metadata for output computation during tests\\n+    if (workflowData.outputs) {\\n+      visorConfig.outputs = workflowData.outputs;\\n+    }\\n+    if (workflowData.inputs) {\\n+      visorConfig.inputs = workflowData.inputs;\\n+    }\\n+\\n+    logger.debug(`Standalone workflow config has ${Object.keys(workflowSteps).length} workflow steps as checks`);\\n+    logger.debug(`Workflow step names: ${Object.keys(workflowSteps).join(', ')}`);\\n     logger.debug(`Config keys after conversion: ${Object.keys(visorConfig).join(', ')}`);\\n \\n     return visorConfig;\\n@@ -982,26 +996,29 @@ export class ConfigManager {\\n \\n     // Validate reuse_ai_session configuration\\n     if (checkConfig.reuse_ai_session !== undefined) {\\n-      const isString = typeof checkConfig.reuse_ai_session === 'string';\\n-      const isBoolean = typeof checkConfig.reuse_ai_session === 'boolean';\\n+      const reuseValue = checkConfig.reuse_ai_session as unknown;\\n+      const isString = typeof reuseValue === 'string';\\n+      const isBoolean = typeof reuseValue === 'boolean';\\n+      const isSelf = reuseValue === 'self';\\n \\n       if (!isString && !isBoolean) {\\n         errors.push({\\n           field: `checks.${checkName}.reuse_ai_session`,\\n           message: `Invalid reuse_ai_session value for \\\"${checkName}\\\": must be string (check name) or boolean`,\\n-          value: checkConfig.reuse_ai_session,\\n+          value: reuseValue,\\n         });\\n-      } else if (isString) {\\n-        // When reuse_ai_session is a string, it must refer to a valid check\\n-        const targetCheckName = checkConfig.reuse_ai_session as string;\\n+      } else if (isString && !isSelf) {\\n+        // When reuse_ai_session is a string (other than the special 'self' value),\\n+        // it must refer to a valid check name\\n+        const targetCheckName = reuseValue as string;\\n         if (!config?.checks || !config.checks[targetCheckName]) {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" references non-existent check \\\"${targetCheckName}\\\" for session reuse`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n-      } else if (checkConfig.reuse_ai_session === true) {\\n+      } else if (reuseValue === true) {\\n         // When reuse_ai_session is true, depends_on must be specified and non-empty\\n         if (\\n           !checkConfig.depends_on ||\\n@@ -1011,7 +1028,7 @@ export class ConfigManager {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" has reuse_ai_session=true but missing or empty depends_on. Session reuse requires dependency on another check.`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n       }\\n@@ -1076,6 +1093,39 @@ export class ConfigManager {\\n         });\\n       }\\n     }\\n+\\n+    // Validate JavaScript syntax in transform_js and script content\\n+    try {\\n+      // Validate transform_js if present\\n+      const transformJs = (checkConfig as any).transform_js;\\n+      if (typeof transformJs === 'string' && transformJs.trim().length > 0) {\\n+        const result = validateJsSyntax(transformJs);\\n+        if (!result.valid) {\\n+          errors.push({\\n+            field: `checks.${checkName}.transform_js`,\\n+            message: `JavaScript syntax error in \\\"${checkName}\\\" transform_js: ${result.error}`,\\n+            value: transformJs.slice(0, 100) + (transformJs.length > 100 ? '...' : ''),\\n+          });\\n+        }\\n+      }\\n+\\n+      // Validate script content if type is 'script'\\n+      if (checkConfig.type === 'script') {\\n+        const content = (checkConfig as any).content;\\n+        if (typeof content === 'string' && content.trim().length > 0) {\\n+          const result = validateJsSyntax(content);\\n+          if (!result.valid) {\\n+            errors.push({\\n+              field: `checks.${checkName}.content`,\\n+              message: `JavaScript syntax error in \\\"${checkName}\\\" script: ${result.error}`,\\n+              value: content.slice(0, 100) + (content.length > 100 ? '...' : ''),\\n+            });\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Syntax validation is best-effort; don't fail the whole config on validation errors\\n+    }\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/failure-condition-evaluator.ts\",\"additions\":25,\"deletions\":9,\"changes\":34,\"patch\":\"diff --git a/src/failure-condition-evaluator.ts b/src/failure-condition-evaluator.ts\\nindex 3b54a015..5800df0a 100644\\n--- a/src/failure-condition-evaluator.ts\\n+++ b/src/failure-condition-evaluator.ts\\n@@ -137,6 +137,8 @@ export class FailureConditionEvaluator {\\n       previousResults?: Map<string, ReviewSummary>;\\n       authorAssociation?: string;\\n       workflowInputs?: Record<string, unknown>;\\n+      /** Current step's output for guarantee evaluation */\\n+      output?: unknown;\\n     }\\n   ): Promise<boolean> {\\n     // Build context for if evaluation\\n@@ -177,10 +179,10 @@ export class FailureConditionEvaluator {\\n       // Workflow inputs (for workflows)\\n       inputs: contextData?.workflowInputs || {},\\n \\n-      // Required output property (empty for if conditions)\\n-      output: {\\n-        issues: [],\\n-      },\\n+      // Output property: use provided output for guarantee evaluation, or empty for if conditions\\n+      output: contextData?.output !== undefined && contextData.output !== null && typeof contextData.output === 'object'\\n+        ? (contextData.output as Record<string, unknown>)\\n+        : { issues: [] },\\n       // Author association (used by permission helpers)\\n       authorAssociation: contextData?.authorAssociation,\\n \\n@@ -213,10 +215,10 @@ export class FailureConditionEvaluator {\\n               m && typeof m.get === 'function' ? m.get('all_valid', 'fact-validation') : undefined;\\n             memStr = ` mem.fact-validation.all_valid=${String(v)}`;\\n           } catch {}\\n+          // Debug if-eval output (only when VISOR_DEBUG enabled)\\n+          const outputKeys = Object.keys(context.outputs || {});\\n           console.error(\\n-            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" env.ENABLE_FACT_VALIDATION=${String(\\n-              (envMap as any).ENABLE_FACT_VALIDATION\\n-            )} event=${context.event?.event_name} result=${String(res)}${memStr}`\\n+            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" result=${String(res)} outputKeys=[${outputKeys.join(',')}]`\\n           );\\n         }\\n       } catch {}\\n@@ -480,7 +482,18 @@ export class FailureConditionEvaluator {\\n \\n       // Extract context variables\\n       const output = context.output || {};\\n-      const issues = output.issues || [];\\n+      // Ensure issues is an array - it might be a JSON string from workflow outputs\\n+      let issues = output.issues || [];\\n+      if (typeof issues === 'string') {\\n+        try {\\n+          issues = JSON.parse(issues);\\n+        } catch {\\n+          issues = [];\\n+        }\\n+      }\\n+      if (!Array.isArray(issues)) {\\n+        issues = [];\\n+      }\\n \\n       // Backward compatibility: provide metadata for transition period\\n       // TODO: Remove after all configurations are updated\\n@@ -518,6 +531,7 @@ export class FailureConditionEvaluator {\\n       const event = context.event || 'manual';\\n       const env = context.env || {};\\n       const outputs = context.outputs || {};\\n+      const inputs = context.inputs || {};\\n       const debugData = context.debug || null;\\n \\n       // Get memory store and create accessor for fail_if expressions\\n@@ -555,6 +569,7 @@ export class FailureConditionEvaluator {\\n         filesCount,\\n         event,\\n         env,\\n+        inputs,\\n         // Helper functions\\n         contains,\\n         startsWith,\\n@@ -621,6 +636,7 @@ export class FailureConditionEvaluator {\\n             filesCount,\\n             event,\\n             env,\\n+            inputs,\\n             // Helpers\\n             contains,\\n             startsWith,\\n\",\"status\":\"modified\"},{\"filename\":\"src/frontends/slack-frontend.ts\",\"additions\":107,\"deletions\":6,\"changes\":113,\"patch\":\"diff --git a/src/frontends/slack-frontend.ts b/src/frontends/slack-frontend.ts\\nindex 97161a7d..41735aa2 100644\\n--- a/src/frontends/slack-frontend.ts\\n+++ b/src/frontends/slack-frontend.ts\\n@@ -1,6 +1,32 @@\\n+/**\\n+ * Slack Frontend for Visor workflows.\\n+ *\\n+ * Features:\\n+ * - Posts AI replies to Slack threads\\n+ * - Converts Markdown to Slack mrkdwn format\\n+ * - Renders mermaid diagrams to PNG and uploads as images\\n+ * - Manages üëÄ/üëç reactions for acknowledgement\\n+ * - Handles human input prompts via prompt-state\\n+ *\\n+ * Mermaid Diagram Rendering:\\n+ * - Detects ```mermaid code blocks in AI responses\\n+ * - Renders to PNG using @mermaid-js/mermaid-cli (mmdc)\\n+ * - Uploads rendered images to Slack thread\\n+ * - Replaces mermaid blocks with \\\"_(See diagram above)_\\\" placeholder\\n+ *\\n+ * Requirements for mermaid rendering:\\n+ * - Node.js and npx in PATH\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ * - On Linux: apt-get install chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ */\\n import type { Frontend, FrontendContext } from './host';\\n import { SlackClient } from '../slack/client';\\n-import { formatSlackText } from '../slack/markdown';\\n+import {\\n+  formatSlackText,\\n+  extractMermaidDiagrams,\\n+  renderMermaidToPng,\\n+  replaceMermaidBlocks,\\n+} from '../slack/markdown';\\n \\n type SlackFrontendConfig = {\\n   defaultChannel?: string;\\n@@ -284,6 +310,9 @@ export class SlackFrontend implements Frontend {\\n       const isLogChat = providerType === 'log' && checkCfg.group === 'chat';\\n       if (!isAi && !isLogChat) return;\\n \\n+      // Skip internal steps - they're intermediate processing and shouldn't post to Slack\\n+      if (checkCfg.criticality === 'internal') return;\\n+\\n       // For AI checks, only post when using simple/unstructured schemas (or none).\\n       if (isAi) {\\n         const schema = checkCfg.schema;\\n@@ -316,6 +345,13 @@ export class SlackFrontend implements Frontend {\\n         ) {\\n           text = (result as any).content.trim();\\n         }\\n+      } else if (isLogChat && typeof (result as any)?.logOutput === 'string') {\\n+        // For log-based chat checks, render the formatted log output as the\\n+        // Slack message when no structured text field is present.\\n+        const raw = (result as any).logOutput;\\n+        if (raw.trim().length > 0) {\\n+          text = raw.trim();\\n+        }\\n       } else if (isAi && showRawOutput && out !== undefined) {\\n         try {\\n           text = JSON.stringify(out, null, 2);\\n@@ -325,13 +361,78 @@ export class SlackFrontend implements Frontend {\\n       }\\n       if (!text) return;\\n \\n-      const formattedText = formatSlackText(text);\\n+      // Extract and render mermaid diagrams before posting\\n+      const diagrams = extractMermaidDiagrams(text);\\n+      let processedText = text;\\n+\\n+      if (diagrams.length > 0) {\\n+        try {\\n+          ctx.logger.info(\\n+            `[slack-frontend] found ${diagrams.length} mermaid diagram(s) to render for ${checkId}`\\n+          );\\n+        } catch {}\\n+\\n+        // Render and upload each diagram\\n+        const uploadedCount: number[] = [];\\n+        for (let i = 0; i < diagrams.length; i++) {\\n+          const diagram = diagrams[i];\\n+          try {\\n+            ctx.logger.info(`[slack-frontend] rendering mermaid diagram ${i + 1}...`);\\n+            const pngBuffer = await renderMermaidToPng(diagram.code);\\n+            if (pngBuffer) {\\n+              ctx.logger.info(\\n+                `[slack-frontend] rendered diagram ${i + 1}, size=${pngBuffer.length} bytes, uploading...`\\n+              );\\n+              const filename = `diagram-${i + 1}.png`;\\n+              const uploadResult = await slack.files.uploadV2({\\n+                content: pngBuffer,\\n+                filename,\\n+                channel,\\n+                thread_ts: threadTs,\\n+                title: `Diagram ${i + 1}`,\\n+              });\\n+              if (uploadResult.ok) {\\n+                uploadedCount.push(i);\\n+                ctx.logger.info(`[slack-frontend] uploaded mermaid diagram ${i + 1} to ${channel}`);\\n+              } else {\\n+                ctx.logger.warn(`[slack-frontend] upload failed for diagram ${i + 1}`);\\n+              }\\n+            } else {\\n+              ctx.logger.warn(\\n+                `[slack-frontend] mermaid rendering returned null for diagram ${i + 1} (mmdc failed or not installed)`\\n+              );\\n+            }\\n+          } catch (e) {\\n+            ctx.logger.warn(\\n+              `[slack-frontend] failed to render/upload mermaid diagram ${i + 1}: ${\\n+                e instanceof Error ? e.message : String(e)\\n+              }`\\n+            );\\n+          }\\n+        }\\n+\\n+        // Replace mermaid blocks with placeholder text\\n+        if (uploadedCount.length > 0) {\\n+          processedText = replaceMermaidBlocks(text, diagrams, (idx) =>\\n+            uploadedCount.includes(idx) ? '_(See diagram above)_' : '_(Diagram rendering failed)_'\\n+          );\\n+        }\\n+      }\\n+\\n+      const formattedText = formatSlackText(processedText);\\n       await slack.chat.postMessage({ channel, text: formattedText, thread_ts: threadTs });\\n+      ctx.logger.info(\\n+        `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+      );\\n+    } catch (outerErr) {\\n+      // Log errors instead of silently swallowing them\\n       try {\\n-        ctx.logger.info(\\n-          `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+        ctx.logger.warn(\\n+          `[slack-frontend] maybePostDirectReply failed for ${checkId}: ${\\n+            outerErr instanceof Error ? outerErr.message : String(outerErr)\\n+          }`\\n         );\\n       } catch {}\\n-    } catch {}\\n+    }\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/generated/config-schema.ts\",\"additions\":233,\"deletions\":13,\"changes\":246,\"patch\":\"diff --git a/src/generated/config-schema.ts b/src/generated/config-schema.ts\\nindex b3caca4f..ecab390d 100644\\n--- a/src/generated/config-schema.ts\\n+++ b/src/generated/config-schema.ts\\n@@ -56,6 +56,20 @@ export const configSchema = {\\n           },\\n           description: 'Import workflow definitions from external files or URLs',\\n         },\\n+        inputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowInput',\\n+          },\\n+          description: 'Workflow inputs (for standalone reusable workflows)',\\n+        },\\n+        outputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowOutput',\\n+          },\\n+          description: 'Workflow outputs (for standalone reusable workflows)',\\n+        },\\n         steps: {\\n           $ref: '#/definitions/Record%3Cstring%2CCheckConfig%3E',\\n           description: 'Step configurations (recommended)',\\n@@ -67,7 +81,7 @@ export const configSchema = {\\n         },\\n         output: {\\n           $ref: '#/definitions/OutputConfig',\\n-          description: 'Output configuration',\\n+          description: 'Output configuration (optional - defaults provided)',\\n         },\\n         http_server: {\\n           $ref: '#/definitions/HttpServerConfig',\\n@@ -139,8 +153,16 @@ export const configSchema = {\\n           },\\n           description: 'Optional integrations: event-driven frontends (e.g., ndjson-sink, github)',\\n         },\\n+        workspace: {\\n+          $ref: '#/definitions/WorkspaceConfig',\\n+          description: 'Workspace isolation configuration for sandboxed execution',\\n+        },\\n+        slack: {\\n+          $ref: '#/definitions/SlackConfig',\\n+          description: 'Slack configuration',\\n+        },\\n       },\\n-      required: ['output', 'version'],\\n+      required: ['version'],\\n       patternProperties: {\\n         '^x-': {},\\n       },\\n@@ -243,6 +265,63 @@ export const configSchema = {\\n         type: 'string',\\n       },\\n     },\\n+    WorkflowInput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Input parameter name',\\n+        },\\n+        schema: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'JSON Schema for the input',\\n+        },\\n+        required: {\\n+          type: 'boolean',\\n+          description: 'Whether this input is required',\\n+        },\\n+        default: {\\n+          description: 'Default value if not provided',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow input definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    WorkflowOutput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Output name',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+        value: {\\n+          type: 'string',\\n+          description: 'Value using Liquid template syntax (references step outputs)',\\n+        },\\n+        value_js: {\\n+          type: 'string',\\n+          description: 'Value using JavaScript expression (alternative to value)',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow output definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n     'Record<string,CheckConfig>': {\\n       type: 'object',\\n       additionalProperties: {\\n@@ -390,11 +469,19 @@ export const configSchema = {\\n           description: 'Timeout in seconds for command execution (default: 60)',\\n         },\\n         depends_on: {\\n-          type: 'array',\\n-          items: {\\n-            type: 'string',\\n-          },\\n-          description: 'Check IDs that this check depends on (optional)',\\n+          anyOf: [\\n+            {\\n+              type: 'string',\\n+            },\\n+            {\\n+              type: 'array',\\n+              items: {\\n+                type: 'string',\\n+              },\\n+            },\\n+          ],\\n+          description:\\n+            'Check IDs that this check depends on (optional). Accepts single string or array.',\\n         },\\n         group: {\\n           type: 'string',\\n@@ -653,13 +740,93 @@ export const configSchema = {\\n           description: 'Arguments/inputs for the workflow',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Override specific step configurations in the workflow',\\n         },\\n         output_mapping: {\\n           $ref: '#/definitions/Record%3Cstring%2Cstring%3E',\\n           description: 'Map workflow outputs to check outputs',\\n         },\\n+        workflow_inputs: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'Alias for args - workflow inputs (backward compatibility)',\\n+        },\\n+        config: {\\n+          type: 'string',\\n+          description:\\n+            'Config file path - alternative to workflow ID (loads a Visor config file as workflow)',\\n+        },\\n+        workflow_overrides: {\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n+          description: 'Alias for overrides - workflow step overrides (backward compatibility)',\\n+        },\\n+        ref: {\\n+          type: 'string',\\n+          description: 'Git reference to checkout (branch, tag, commit SHA) - supports templates',\\n+        },\\n+        repository: {\\n+          type: 'string',\\n+          description: 'Repository URL or owner/repo format (defaults to current repository)',\\n+        },\\n+        token: {\\n+          type: 'string',\\n+          description: 'GitHub token for private repositories (defaults to GITHUB_TOKEN env)',\\n+        },\\n+        fetch_depth: {\\n+          type: 'number',\\n+          description: 'Number of commits to fetch (0 for full history, default: 1)',\\n+        },\\n+        fetch_tags: {\\n+          type: 'boolean',\\n+          description: 'Whether to fetch tags (default: false)',\\n+        },\\n+        submodules: {\\n+          anyOf: [\\n+            {\\n+              type: 'boolean',\\n+            },\\n+            {\\n+              type: 'string',\\n+              const: 'recursive',\\n+            },\\n+          ],\\n+          description: \\\"Checkout submodules: false, true, or 'recursive'\\\",\\n+        },\\n+        working_directory: {\\n+          type: 'string',\\n+          description: 'Working directory for the checkout (defaults to temp directory)',\\n+        },\\n+        use_worktree: {\\n+          type: 'boolean',\\n+          description: 'Use git worktree for efficient parallel checkouts (default: true)',\\n+        },\\n+        clean: {\\n+          type: 'boolean',\\n+          description: 'Clean the working directory before checkout (default: true)',\\n+        },\\n+        sparse_checkout: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description: 'Sparse checkout paths - only checkout specific directories/files',\\n+        },\\n+        lfs: {\\n+          type: 'boolean',\\n+          description: 'Enable Git LFS (Large File Storage)',\\n+        },\\n+        clone_timeout_ms: {\\n+          type: 'number',\\n+          description: 'Timeout in ms for cloning the bare repository (default: 300000 = 5 min)',\\n+        },\\n+        cleanup_on_failure: {\\n+          type: 'boolean',\\n+          description: 'Clean up worktree on failure (default: true)',\\n+        },\\n+        persist_worktree: {\\n+          type: 'boolean',\\n+          description: 'Keep worktree after workflow completion (default: false)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'Configuration for a single check',\\n@@ -794,6 +961,11 @@ export const configSchema = {\\n           $ref: '#/definitions/BashConfig',\\n           description: 'Advanced bash command execution configuration',\\n         },\\n+        completion_prompt: {\\n+          type: 'string',\\n+          description:\\n+            'Completion prompt for post-completion validation/review (runs after attempt_completion)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'AI provider configuration',\\n@@ -1223,7 +1395,7 @@ export const configSchema = {\\n           description: 'Custom output name (defaults to workflow name)',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Step overrides',\\n         },\\n         output_mapping: {\\n@@ -1238,14 +1410,14 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n-    'Record<string,Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>>':\\n+    'Record<string,Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>>':\\n       {\\n         type: 'object',\\n         additionalProperties: {\\n-          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E',\\n+          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E',\\n         },\\n       },\\n-    'Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>': {\\n+    'Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>': {\\n       type: 'object',\\n       additionalProperties: false,\\n     },\\n@@ -1811,6 +1983,54 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n+    WorkspaceConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        enabled: {\\n+          type: 'boolean',\\n+          description: 'Enable workspace isolation (default: true when config present)',\\n+        },\\n+        base_path: {\\n+          type: 'string',\\n+          description: 'Base path for workspaces (default: /tmp/visor-workspaces)',\\n+        },\\n+        cleanup_on_exit: {\\n+          type: 'boolean',\\n+          description: 'Clean up workspace on exit (default: true)',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Workspace isolation configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    SlackConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        version: {\\n+          type: 'string',\\n+          description: 'Slack API version',\\n+        },\\n+        mentions: {\\n+          type: 'string',\\n+          description: \\\"Mention handling: 'all', 'direct', etc.\\\",\\n+        },\\n+        threads: {\\n+          type: 'string',\\n+          description: \\\"Thread handling: 'required', 'optional', etc.\\\",\\n+        },\\n+        show_raw_output: {\\n+          type: 'boolean',\\n+          description: 'Show raw output in Slack responses',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Slack configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n   },\\n } as const;\\n export default configSchema;\\n\",\"status\":\"modified\"},{\"filename\":\"src/index.ts\",\"additions\":5,\"deletions\":2,\"changes\":7,\"patch\":\"diff --git a/src/index.ts b/src/index.ts\\nindex 50b89df3..9621e733 100644\\n--- a/src/index.ts\\n+++ b/src/index.ts\\n@@ -628,8 +628,11 @@ function resolveDependencies(\\n \\n     // Get dependencies for this check\\n     const checkConfig = config?.checks?.[checkId];\\n+    // Normalize depends_on to array (supports string | string[])\\n+    const rawDeps = checkConfig?.depends_on;\\n+    const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     // Expand OR groups (pipe syntax) for dependency closure discovery\\n-    const dependencies = (checkConfig?.depends_on || []).flatMap(d =>\\n+    const dependencies = depsArray.flatMap((d: string) =>\\n       typeof d === 'string' && d.includes('|')\\n         ? d\\n             .split('|')\\n\",\"status\":\"modified\"},{\"filename\":\"src/liquid-extensions.ts\",\"additions\":68,\"deletions\":1,\"changes\":69,\"patch\":\"diff --git a/src/liquid-extensions.ts b/src/liquid-extensions.ts\\nindex e9efaf68..dbf9c6ed 100644\\n--- a/src/liquid-extensions.ts\\n+++ b/src/liquid-extensions.ts\\n@@ -124,6 +124,26 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     }\\n   });\\n \\n+  // Register base64 filter for encoding strings\\n+  // Usage: {{ \\\"user:password\\\" | base64 }}\\n+  liquid.registerFilter('base64', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    return Buffer.from(str).toString('base64');\\n+  });\\n+\\n+  // Register base64_decode filter for decoding base64 strings\\n+  // Usage: {{ encoded_value | base64_decode }}\\n+  liquid.registerFilter('base64_decode', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    try {\\n+      return Buffer.from(str, 'base64').toString('utf-8');\\n+    } catch {\\n+      return '[Error: Invalid base64 string]';\\n+    }\\n+  });\\n+\\n   // Sanitize a label to allowed characters only: [A-Za-z0-9:/]\\n   liquid.registerFilter('safe_label', (value: unknown) => sanitizeLabel(value));\\n \\n@@ -137,6 +157,53 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     return s.replace(/\\\\\\\\n/g, '\\\\n').replace(/\\\\\\\\r/g, '\\\\r').replace(/\\\\\\\\t/g, '\\\\t');\\n   });\\n \\n+  // JSON escape filter - escapes a string for use inside a JSON string value\\n+  // This escapes special characters like quotes, backslashes, and control characters\\n+  // Usage: \\\"jql\\\": \\\"{{ myValue | json_escape }}\\\"\\n+  liquid.registerFilter('json_escape', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const s = String(value);\\n+    // Use JSON.stringify which handles all escaping, then strip the surrounding quotes\\n+    const jsonStr = JSON.stringify(s);\\n+    // Remove the first and last character (the quotes added by JSON.stringify)\\n+    return jsonStr.slice(1, -1);\\n+  });\\n+\\n+  // Shell escape filter - wraps value in single quotes with proper escaping\\n+  // Usage: {{ value | shell_escape }}\\n+  // Example: \\\"hello'world\\\" becomes \\\"'hello'\\\\''world'\\\"\\n+  // This is POSIX-compliant and safe for arbitrary text including mermaid diagrams\\n+  liquid.registerFilter('shell_escape', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    // Replace single quotes with: end quote, escaped quote, start quote\\n+    // Then wrap the entire thing in single quotes\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Alias for shell_escape\\n+  liquid.registerFilter('escape_shell', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Shell escape for double quotes (less safe but sometimes needed)\\n+  // Usage: {{ value | shell_escape_double }}\\n+  // Escapes: $, `, \\\\, \\\", and !\\n+  liquid.registerFilter('shell_escape_double', (value: unknown) => {\\n+    if (value == null) return '\\\"\\\"';\\n+    const s = String(value);\\n+    // Escape characters that have special meaning inside double quotes\\n+    const escaped = s\\n+      .replace(/\\\\\\\\/g, '\\\\\\\\\\\\\\\\')  // backslash first\\n+      .replace(/\\\\$/g, '\\\\\\\\$')   // dollar sign\\n+      .replace(/`/g, '\\\\\\\\`')    // backticks\\n+      .replace(/\\\"/g, '\\\\\\\\\\\"')    // double quotes\\n+      .replace(/!/g, '\\\\\\\\!');   // history expansion\\n+    return '\\\"' + escaped + '\\\"';\\n+  });\\n+\\n   // Register author permission filters (from main)\\n   // These filters check the author's permission level; detect local mode for tests\\n   const isLocal = detectLocalMode();\\n\",\"status\":\"modified\"},{\"filename\":\"src/logger.ts\",\"additions\":31,\"deletions\":2,\"changes\":33,\"patch\":\"diff --git a/src/logger.ts b/src/logger.ts\\nindex 6148b77c..aee7136e 100644\\n--- a/src/logger.ts\\n+++ b/src/logger.ts\\n@@ -85,7 +85,36 @@ class Logger {\\n       if (this.showTimestamps) {\\n         const ts = new Date().toISOString();\\n         const lvl = level ? level : undefined;\\n-        const prefix = lvl ? `[${ts}] [${lvl}]` : `[${ts}]`;\\n+\\n+        let tsToken = `[${ts}]`;\\n+        let lvlToken = lvl ? `[${lvl}]` : '';\\n+\\n+        // Add simple ANSI colour when running in a TTY and not emitting\\n+        // JSON/SARIF. Colours are intentionally minimal and only applied\\n+        // to the prefix markers, not the full line.\\n+        if (this.isTTY && !this.isJsonLike) {\\n+          const reset = '\\\\x1b[0m';\\n+          const dim = '\\\\x1b[2m';\\n+          const colours: Record<LogLevel, string> = {\\n+            silent: '',\\n+            error: '\\\\x1b[31m', // red\\n+            warn: '\\\\x1b[33m', // yellow\\n+            info: '\\\\x1b[36m', // cyan\\n+            verbose: '\\\\x1b[35m', // magenta\\n+            debug: '\\\\x1b[90m', // bright black / gray\\n+          };\\n+\\n+          tsToken = `${dim}${tsToken}${reset}`;\\n+\\n+          if (lvl) {\\n+            const colour = colours[lvl] || '';\\n+            if (colour) {\\n+              lvlToken = `${colour}${lvlToken}${reset}`;\\n+            }\\n+          }\\n+        }\\n+\\n+        const prefix = lvl ? `${tsToken} ${lvlToken}` : tsToken;\\n         process.stderr.write(`${prefix} ${msg}\\\\n`);\\n       } else {\\n         process.stderr.write(msg + '\\\\n');\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/ai-check-provider.ts\",\"additions\":106,\"deletions\":8,\"changes\":114,\"patch\":\"diff --git a/src/providers/ai-check-provider.ts b/src/providers/ai-check-provider.ts\\nindex 53f1a350..987f2080 100644\\n--- a/src/providers/ai-check-provider.ts\\n+++ b/src/providers/ai-check-provider.ts\\n@@ -186,7 +186,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     let promptContent: string;\\n \\n@@ -204,7 +205,8 @@ export class AICheckProvider extends CheckProvider {\\n       eventContext,\\n       dependencyResults,\\n       outputHistory,\\n-      args\\n+      args,\\n+      workflowInputs\\n     );\\n   }\\n \\n@@ -335,7 +337,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     // Build outputs_raw from -raw keys (aggregate parent values)\\n     const outputsRaw: Record<string, unknown> = {};\\n@@ -535,6 +538,8 @@ export class AICheckProvider extends CheckProvider {\\n       outputs_raw: outputsRaw,\\n       // Custom arguments from on_init 'with' directive\\n       args: args || {},\\n+      // Workflow inputs (for nested workflow steps to access parent inputs like {{ inputs.context }})\\n+      inputs: workflowInputs || {},\\n     };\\n \\n     try {\\n@@ -621,7 +626,8 @@ export class AICheckProvider extends CheckProvider {\\n         console.error(`[ai-exec] step=${String((config as any).checkName || 'unknown')}`);\\n       }\\n     } catch {}\\n-    // Extract AI configuration - only set properties that are explicitly provided\\n+    // Extract AI configuration - only set properties that are explicitly provided.\\n+    // Workspace / allowedFolders will be derived below from the execution context.\\n     const aiConfig: AIReviewConfig = {};\\n \\n     // Check-level AI configuration (ai object)\\n@@ -671,6 +677,9 @@ export class AICheckProvider extends CheckProvider {\\n       if (aiAny.bashConfig !== undefined) {\\n         aiConfig.bashConfig = aiAny.bashConfig as import('../types/config').BashConfig;\\n       }\\n+      if (aiAny.completion_prompt !== undefined) {\\n+        aiConfig.completionPrompt = aiAny.completion_prompt as string;\\n+      }\\n       if (aiAny.skip_code_context !== undefined) {\\n         // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n         (aiConfig as any).skip_code_context = aiAny.skip_code_context as boolean;\\n@@ -691,6 +700,90 @@ export class AICheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Derive workspace-aware allowedFolders for ProbeAgent when workspace\\n+    // isolation is enabled. This ensures tools like search/query operate\\n+    // inside the isolated workspace (and its project symlinks) instead of\\n+    // the Visor repository root.\\n+    // Folder names are human-readable (tyk-docs, visor2) thanks to WorkspaceManager.\\n+    try {\\n+      const ctxAny: any = sessionInfo as any;\\n+      const parentCtx = ctxAny?._parentContext;\\n+      const workspace = parentCtx?.workspace;\\n+\\n+      // Enhanced debug logging for workspace propagation diagnosis\\n+      logger.debug(`[AI Provider] Workspace detection for check '${(config as any).checkName || 'unknown'}':`);\\n+      logger.debug(`[AI Provider]   sessionInfo exists: ${!!sessionInfo}`);\\n+      logger.debug(`[AI Provider]   _parentContext exists: ${!!parentCtx}`);\\n+      logger.debug(`[AI Provider]   workspace exists: ${!!workspace}`);\\n+      if (workspace) {\\n+        logger.debug(`[AI Provider]   workspace.isEnabled exists: ${typeof workspace.isEnabled === 'function'}`);\\n+        logger.debug(`[AI Provider]   workspace.isEnabled(): ${typeof workspace.isEnabled === 'function' ? workspace.isEnabled() : 'N/A'}`);\\n+        const projectCount = typeof workspace.listProjects === 'function' ? workspace.listProjects()?.length : 'N/A';\\n+        logger.debug(`[AI Provider]   workspace.listProjects() count: ${projectCount}`);\\n+      }\\n+\\n+      if (workspace && typeof workspace.isEnabled === 'function' && workspace.isEnabled()) {\\n+        const folders: string[] = [];\\n+        let workspaceRoot: string | undefined;\\n+        let mainProjectPath: string | undefined;\\n+        try {\\n+          const info = workspace.getWorkspaceInfo?.();\\n+          if (info && typeof info.workspacePath === 'string') {\\n+            workspaceRoot = info.workspacePath;\\n+            mainProjectPath = info.mainProjectPath;\\n+            // Add workspace root\\n+            folders.push(info.workspacePath);\\n+            // Include main project path (has human-readable name like \\\"visor2\\\")\\n+            if (mainProjectPath) {\\n+              folders.push(mainProjectPath);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore workspace info errors\\n+        }\\n+        try {\\n+          const projects = workspace.listProjects?.() || [];\\n+          for (const proj of projects as any[]) {\\n+            if (proj && typeof proj.path === 'string') {\\n+              // Project paths have human-readable names (tyk-docs, not checkout-tyk-docs)\\n+              folders.push(proj.path);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore project listing errors\\n+        }\\n+        const unique = Array.from(new Set(folders.filter(p => typeof p === 'string' && p)));\\n+        if (unique.length > 0 && workspaceRoot) {\\n+          (aiConfig as any).allowedFolders = unique;\\n+          // Set path and cwd to workspace root - AI will run with cwd set to workspace\\n+          // Both are set for compatibility: path for older probe versions, cwd for rc175+\\n+          (aiConfig as any).path = workspaceRoot;\\n+          (aiConfig as any).cwd = workspaceRoot;\\n+          // Also set workspacePath for the AI to know the root\\n+          (aiConfig as any).workspacePath = workspaceRoot;\\n+          logger.debug(`[AI Provider] Workspace isolation enabled:`);\\n+          logger.debug(`[AI Provider]   workspaceRoot (cwd): ${workspaceRoot}`);\\n+          logger.debug(`[AI Provider]   allowedFolders: ${JSON.stringify(unique)}`);\\n+        }\\n+      } else if (parentCtx && typeof parentCtx.workingDirectory === 'string') {\\n+        // Fallback: when workspace is not available (or disabled), still\\n+        // constrain tools to the engine's working directory so ProbeAgent\\n+        // operates inside the same logical root as the state machine. This\\n+        // also ensures nested workflows (e.g. code-question-helper) use the\\n+        // workspace main project path once initializeWorkspace has updated\\n+        // the parent context.\\n+        if (!(aiConfig as any).allowedFolders) {\\n+          (aiConfig as any).allowedFolders = [parentCtx.workingDirectory];\\n+        }\\n+        if (!(aiConfig as any).path) {\\n+          (aiConfig as any).path = parentCtx.workingDirectory;\\n+          (aiConfig as any).cwd = parentCtx.workingDirectory;\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only; fall back to defaults on error.\\n+    }\\n+\\n     // Check-level AI model and provider (top-level properties)\\n     if (config.ai_model !== undefined) {\\n       aiConfig.model = config.ai_model as string;\\n@@ -876,7 +969,8 @@ export class AICheckProvider extends CheckProvider {\\n       ctxWithStage,\\n       _dependencyResults,\\n       (config as any).__outputHistory as Map<string, unknown[]> | undefined,\\n-      (sessionInfo as any)?.args\\n+      (sessionInfo as any)?.args,\\n+      (config as any).workflowInputs as Record<string, unknown> | undefined\\n     );\\n \\n     // Optional persona (vendor extension): ai.ai_persona or ai_persona.\\n@@ -963,6 +1057,7 @@ export class AICheckProvider extends CheckProvider {\\n       const reuseEnabled =\\n         (config as any).reuse_ai_session === true ||\\n         typeof (config as any).reuse_ai_session === 'string';\\n+      let promptUsed = finalPrompt;\\n       if (sessionInfo?.reuseSession && sessionInfo.parentSessionId && reuseEnabled) {\\n         // Safety: only reuse if the parent session actually exists\\n         try {\\n@@ -975,6 +1070,7 @@ export class AICheckProvider extends CheckProvider {\\n               );\\n             }\\n             // Fall back to new session\\n+            promptUsed = processedPrompt;\\n             const fresh = await service.executeReview(\\n               prInfo,\\n               processedPrompt,\\n@@ -999,6 +1095,7 @@ export class AICheckProvider extends CheckProvider {\\n             `üîÑ Debug: Using session reuse with parent session: ${sessionInfo.parentSessionId} (mode: ${sessionMode})`\\n           );\\n         }\\n+        promptUsed = processedPrompt;\\n         result = await service.executeReviewWithSessionReuse(\\n           prInfo,\\n           processedPrompt,\\n@@ -1011,6 +1108,7 @@ export class AICheckProvider extends CheckProvider {\\n         if (aiConfig.debug) {\\n           console.error(`üÜï Debug: Creating new AI session for check: ${config.checkName}`);\\n         }\\n+        promptUsed = finalPrompt;\\n         result = await service.executeReview(\\n           prInfo,\\n           finalPrompt,\\n@@ -1038,11 +1136,11 @@ export class AICheckProvider extends CheckProvider {\\n             span,\\n             'ai',\\n             {\\n-              prompt: processedPrompt.substring(0, 500), // Preview only\\n+              prompt: promptUsed,\\n               model: aiConfig.model,\\n             },\\n             {\\n-              content: JSON.stringify(finalResult).substring(0, 500),\\n+              content: JSON.stringify(finalResult),\\n               tokens: (result as any).usage?.totalTokens,\\n             }\\n           );\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/command-check-provider.ts\",\"additions\":50,\"deletions\":16,\"changes\":66,\"patch\":\"diff --git a/src/providers/command-check-provider.ts b/src/providers/command-check-provider.ts\\nindex e0c9d634..bf7ffd43 100644\\n--- a/src/providers/command-check-provider.ts\\n+++ b/src/providers/command-check-provider.ts\\n@@ -131,7 +131,8 @@ export class CommandCheckProvider extends CheckProvider {\\n       // New: outputs_raw exposes aggregate values (e.g., full arrays for forEach parents)\\n       outputs_raw: outputsRaw,\\n       // Workflow inputs (when executing within a workflow)\\n-      inputs: context?.workflowInputs || {},\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any).workflowInputs || context?.workflowInputs || {},\\n       // Custom arguments from on_init 'with' directive\\n       args: context?.args || {},\\n       env: this.getSafeEnvironmentVariables(),\\n@@ -178,17 +179,31 @@ export class CommandCheckProvider extends CheckProvider {\\n           mock = rawMock as Record<string, unknown>;\\n         }\\n         const m = mock as { stdout?: string; stderr?: string; exit_code?: number; exit?: number };\\n-        let out: unknown = m.stdout ?? '';\\n-        try {\\n-          if (\\n-            typeof out === 'string' &&\\n-            (out.trim().startsWith('{') || out.trim().startsWith('['))\\n-          ) {\\n-            out = JSON.parse(out);\\n-          }\\n-        } catch {}\\n-        const code =\\n-          typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0;\\n+\\n+        // Check if this looks like a command mock (has stdout/stderr/exit_code) or direct data output\\n+        const isCommandMock = m.stdout !== undefined || m.stderr !== undefined ||\\n+          m.exit_code !== undefined || m.exit !== undefined;\\n+\\n+        let out: unknown;\\n+        if (isCommandMock) {\\n+          // Traditional command mock format: { stdout: \\\"...\\\", exit_code: 0 }\\n+          out = m.stdout ?? '';\\n+          try {\\n+            if (\\n+              typeof out === 'string' &&\\n+              (out.trim().startsWith('{') || out.trim().startsWith('['))\\n+            ) {\\n+              out = JSON.parse(out);\\n+            }\\n+          } catch {}\\n+        } else {\\n+          // Direct data mock format: { data: [...], count: 1 } - use as-is\\n+          out = mock;\\n+        }\\n+\\n+        const code = isCommandMock\\n+          ? (typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0)\\n+          : 0;\\n         if (code !== 0) {\\n           return {\\n             issues: [\\n@@ -231,9 +246,8 @@ export class CommandCheckProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Get timeout from config (in seconds) or use default (60 seconds)\\n-      const timeoutSeconds = (config.timeout as number) || 60;\\n-      const timeoutMs = timeoutSeconds * 1000;\\n+      // Get timeout from config (in milliseconds) or use default (60 seconds = 60000ms)\\n+      const timeoutMs = (config.timeout as number) || 60000;\\n \\n       // Normalize only the eval payload for `node -e|--eval` invocations that may contain\\n       // literal newlines due to YAML processing (\\\"\\\\n\\\" -> newline). We re-escape newlines\\n@@ -1533,6 +1547,26 @@ ${bodyWithReturn}\\n       return null;\\n     }\\n \\n+    // Require at least one issue-specific field beyond just a message-like field.\\n+    // This prevents objects like {text: \\\"...\\\"} from being treated as issues.\\n+    const hasIssueField =\\n+      data.file !== undefined ||\\n+      data.path !== undefined ||\\n+      data.filename !== undefined ||\\n+      data.line !== undefined ||\\n+      data.startLine !== undefined ||\\n+      data.lineNumber !== undefined ||\\n+      data.severity !== undefined ||\\n+      data.level !== undefined ||\\n+      data.priority !== undefined ||\\n+      data.ruleId !== undefined ||\\n+      data.rule !== undefined ||\\n+      data.category !== undefined ||\\n+      data.type !== undefined;\\n+    if (!hasIssueField) {\\n+      return null;\\n+    }\\n+\\n     const allowedSeverities = new Set(['info', 'warning', 'error', 'critical']);\\n     const severityRaw = this.toTrimmedString(data.severity || data.level || data.priority);\\n     let severity: ReviewIssue['severity'] = 'warning';\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/git-checkout-provider.ts\",\"additions\":41,\"deletions\":6,\"changes\":47,\"patch\":\"diff --git a/src/providers/git-checkout-provider.ts b/src/providers/git-checkout-provider.ts\\nindex baac0001..6d3a2852 100644\\n--- a/src/providers/git-checkout-provider.ts\\n+++ b/src/providers/git-checkout-provider.ts\\n@@ -69,6 +69,16 @@ export class GitCheckoutProvider extends CheckProvider {\\n       return false;\\n     }\\n \\n+    if (checkoutConfig.clone_timeout_ms !== undefined) {\\n+      if (\\n+        typeof checkoutConfig.clone_timeout_ms !== 'number' ||\\n+        checkoutConfig.clone_timeout_ms <= 0\\n+      ) {\\n+        logger.error('Invalid config: clone_timeout_ms must be a positive number (milliseconds)');\\n+        return false;\\n+      }\\n+    }\\n+\\n     return true;\\n   }\\n \\n@@ -91,7 +101,13 @@ export class GitCheckoutProvider extends CheckProvider {\\n       );\\n \\n       // Resolve dynamic variables\\n-      const resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      let resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      // If ref resolves to an empty string, fall back to HEAD so we rely on\\n+      // the repository's default branch. This keeps downstream git commands\\n+      // well-defined and matches existing expectations/tests.\\n+      if (!resolvedRef || resolvedRef.trim().length === 0) {\\n+        resolvedRef = 'HEAD';\\n+      }\\n       const resolvedRepository = checkoutConfig.repository\\n         ? await this.liquid.parseAndRender(checkoutConfig.repository, templateContext)\\n         : process.env.GITHUB_REPOSITORY || 'unknown/unknown';\\n@@ -118,6 +134,7 @@ export class GitCheckoutProvider extends CheckProvider {\\n           clean: checkoutConfig.clean !== false, // Default: true\\n           workflowId: (context as any)?.workflowId,\\n           fetchDepth: checkoutConfig.fetch_depth,\\n+          cloneTimeoutMs: checkoutConfig.clone_timeout_ms,\\n         }\\n       );\\n \\n@@ -134,18 +151,35 @@ export class GitCheckoutProvider extends CheckProvider {\\n \\n       // Add project to workspace if workspace isolation is enabled\\n       const workspace = (context as any)?._parentContext?.workspace;\\n+\\n+      // Enhanced debug logging for workspace project addition diagnosis\\n+      const checkName = (config as any)?.checkName || 'unknown';\\n+      logger.info(`[GitCheckout] Workspace check for '${checkName}':`);\\n+      logger.info(`[GitCheckout]   _parentContext exists: ${!!(context as any)?._parentContext}`);\\n+      logger.info(`[GitCheckout]   workspace exists: ${!!workspace}`);\\n+      logger.info(`[GitCheckout]   workspace.isEnabled(): ${workspace?.isEnabled?.() ?? 'N/A'}`);\\n+      if (workspace) {\\n+        const projectCountBefore = workspace.listProjects?.()?.length ?? 'N/A';\\n+        logger.debug(`[GitCheckout]   projects before addProject: ${projectCountBefore}`);\\n+      }\\n+\\n       if (workspace?.isEnabled()) {\\n         try {\\n+          // Don't pass checkName as description - let workspace use repo name\\n+          // This results in human-readable names like \\\"tyk-docs\\\" instead of \\\"checkout-tyk-docs\\\"\\n           const workspacePath = await workspace.addProject(\\n             resolvedRepository,\\n-            worktree.path,\\n-            checkoutConfig.checkName\\n+            worktree.path\\n           );\\n           output.workspace_path = workspacePath;\\n-          logger.debug(`Added project to workspace: ${workspacePath}`);\\n+          const projectCountAfter = workspace.listProjects?.()?.length ?? 'N/A';\\n+          logger.debug(`[GitCheckout] Added project to workspace: ${workspacePath}`);\\n+          logger.debug(`[GitCheckout]   projects after addProject: ${projectCountAfter}`);\\n         } catch (error) {\\n           logger.warn(`Failed to add project to workspace: ${error}`);\\n         }\\n+      } else {\\n+        logger.debug(`[GitCheckout] Workspace not enabled, skipping addProject`);\\n       }\\n \\n       logger.info(\\n@@ -228,7 +262,8 @@ export class GitCheckoutProvider extends CheckProvider {\\n       outputs: outputsObj,\\n       outputs_history: historyObj,\\n       env: safeEnv,\\n-      inputs: context?.workflowInputs,\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any)?.workflowInputs || context?.workflowInputs,\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/http-client-provider.ts\",\"additions\":279,\"deletions\":35,\"changes\":314,\"patch\":\"diff --git a/src/providers/http-client-provider.ts b/src/providers/http-client-provider.ts\\nindex 4d8c41be..6db81591 100644\\n--- a/src/providers/http-client-provider.ts\\n+++ b/src/providers/http-client-provider.ts\\n@@ -4,18 +4,29 @@ import { ReviewSummary } from '../reviewer';\\n import { Liquid } from 'liquidjs';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n import { EnvironmentResolver } from '../utils/env-resolver';\\n+import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n+import { buildProviderTemplateContext } from '../utils/template-context';\\n+import Sandbox from '@nyariv/sandboxjs';\\n+import { logger } from '../logger';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n \\n /**\\n  * Check provider that fetches data from HTTP endpoints\\n  */\\n export class HttpClientProvider extends CheckProvider {\\n   private liquid: Liquid;\\n+  private sandbox?: Sandbox;\\n \\n   constructor() {\\n     super();\\n     this.liquid = createExtendedLiquid();\\n   }\\n \\n+  private createSecureSandbox(): Sandbox {\\n+    return createSecureSandbox();\\n+  }\\n+\\n   getName(): string {\\n     return 'http_client';\\n   }\\n@@ -61,50 +72,124 @@ export class HttpClientProvider extends CheckProvider {\\n     const headers = (config.headers as Record<string, string>) || {};\\n     const timeout = (config.timeout as number) || 30000;\\n     const transform = config.transform as string | undefined;\\n+    const transformJs = config.transform_js as string | undefined;\\n     const bodyTemplate = config.body as string | undefined;\\n+    const outputFileTemplate = config.output_file as string | undefined;\\n+    const skipIfExists = config.skip_if_exists !== false; // Default true for caching\\n \\n-    try {\\n-      // Prepare template context for URL and body\\n-      const templateContext = {\\n-        pr: {\\n-          number: prInfo.number,\\n-          title: prInfo.title,\\n-          body: prInfo.body,\\n-          author: prInfo.author,\\n-          base: prInfo.base,\\n-          head: prInfo.head,\\n-          totalAdditions: prInfo.totalAdditions,\\n-          totalDeletions: prInfo.totalDeletions,\\n-        },\\n-        outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-        env: process.env,\\n-      };\\n+    // Track resolved URL for error messages\\n+    let resolvedUrlForErrors = url;\\n \\n-      // Render URL with template if it contains liquid syntax\\n-      let renderedUrl = url;\\n-      if (url.includes('{{') || url.includes('{%')) {\\n-        renderedUrl = await this.liquid.parseAndRender(url, templateContext);\\n+    try {\\n+      // Use shared template context builder for consistent output extraction\\n+      const templateContext = buildProviderTemplateContext(\\n+        prInfo,\\n+        dependencyResults,\\n+        undefined, // memoryStore\\n+        undefined, // outputHistory\\n+        undefined, // stageHistoryBase\\n+        { attachMemoryReadHelpers: false }\\n+      );\\n+      // Add env to context for shell-style variable resolution\\n+      (templateContext as Record<string, unknown>).env = process.env;\\n+\\n+      // First resolve shell-style environment variables (${VAR}, $VAR, ${{ env.VAR }})\\n+      let renderedUrl = String(EnvironmentResolver.resolveValue(url));\\n+      resolvedUrlForErrors = renderedUrl; // Track for error messages\\n+\\n+      // Then render Liquid templates if present\\n+      if (renderedUrl.includes('{{') || renderedUrl.includes('{%')) {\\n+        renderedUrl = await this.liquid.parseAndRender(renderedUrl, templateContext);\\n+        resolvedUrlForErrors = renderedUrl; // Update after Liquid rendering\\n       }\\n \\n       // Prepare request body if provided\\n       let requestBody: string | undefined;\\n       if (bodyTemplate) {\\n-        const renderedBody = await this.liquid.parseAndRender(bodyTemplate, templateContext);\\n-        requestBody = renderedBody;\\n+        // First resolve shell-style environment variables\\n+        let resolvedBody = String(EnvironmentResolver.resolveValue(bodyTemplate));\\n+        // Then render Liquid templates if present\\n+        if (resolvedBody.includes('{{') || resolvedBody.includes('{%')) {\\n+          resolvedBody = await this.liquid.parseAndRender(resolvedBody, templateContext);\\n+        }\\n+        requestBody = resolvedBody;\\n       }\\n \\n-      // Resolve environment variables in headers\\n-      const resolvedHeaders = EnvironmentResolver.resolveHeaders(headers);\\n+      // Resolve environment variables and Liquid templates in headers\\n+      const resolvedHeaders: Record<string, string> = {};\\n+      for (const [key, value] of Object.entries(headers)) {\\n+        let resolvedValue = String(EnvironmentResolver.resolveValue(value));\\n+        // Render Liquid templates if present\\n+        if (resolvedValue.includes('{{') || resolvedValue.includes('{%')) {\\n+          resolvedValue = await this.liquid.parseAndRender(resolvedValue, templateContext);\\n+        }\\n+        resolvedHeaders[key] = resolvedValue;\\n+        // Debug auth header (mask most of the value for security)\\n+        if (key.toLowerCase() === 'authorization') {\\n+          const maskedValue = resolvedValue.length > 20\\n+            ? `${resolvedValue.substring(0, 15)}...${resolvedValue.substring(resolvedValue.length - 5)}`\\n+            : resolvedValue;\\n+          logger.verbose(`[http_client] ${key}: ${maskedValue}`);\\n+        }\\n+      }\\n+\\n+      // Resolve output_file path if specified\\n+      let resolvedOutputFile: string | undefined;\\n+      if (outputFileTemplate) {\\n+        let outputPath = String(EnvironmentResolver.resolveValue(outputFileTemplate));\\n+        if (outputPath.includes('{{') || outputPath.includes('{%')) {\\n+          outputPath = await this.liquid.parseAndRender(outputPath, templateContext);\\n+        }\\n+        resolvedOutputFile = outputPath.trim();\\n+\\n+        // Check if file already exists (caching)\\n+        if (skipIfExists && fs.existsSync(resolvedOutputFile)) {\\n+          const stats = fs.statSync(resolvedOutputFile);\\n+          logger.verbose(`[http_client] File cached: ${resolvedOutputFile} (${stats.size} bytes)`);\\n+          return {\\n+            issues: [],\\n+            file_path: resolvedOutputFile,\\n+            size: stats.size,\\n+            cached: true,\\n+          } as unknown as ReviewSummary;\\n+        }\\n+      }\\n \\n       // Test hook: mock HTTP response for this step\\n       const stepName = (config as any).checkName || 'unknown';\\n       const mock = context?.hooks?.mockForStep?.(String(stepName));\\n-      const data =\\n-        mock !== undefined\\n-          ? mock\\n-          : await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n \\n-      // Apply transformation if specified\\n+      // If mock is provided, return it directly as the step output (with issues: [])\\n+      if (mock !== undefined) {\\n+        const mockObj = typeof mock === 'object' && mock !== null ? mock : { data: mock };\\n+        return {\\n+          issues: [],\\n+          ...mockObj,\\n+        } as unknown as ReviewSummary & { data: unknown };\\n+      }\\n+\\n+      // Debug log the request for troubleshooting\\n+      logger.verbose(`[http_client] ${method} ${renderedUrl}`);\\n+      if (requestBody) {\\n+        logger.verbose(`[http_client] Body: ${requestBody.substring(0, 500)}${requestBody.length > 500 ? '...' : ''}`);\\n+      }\\n+\\n+      // If output_file is specified, download to file instead of returning data\\n+      if (resolvedOutputFile) {\\n+        const fileResult = await this.downloadToFile(\\n+          renderedUrl,\\n+          method,\\n+          resolvedHeaders,\\n+          requestBody,\\n+          timeout,\\n+          resolvedOutputFile\\n+        );\\n+        return fileResult;\\n+      }\\n+\\n+      const data = await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n+\\n+      // Apply Liquid transformation if specified\\n       let processedData = data;\\n       if (transform) {\\n         try {\\n@@ -136,12 +221,53 @@ export class HttpClientProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Return the fetched data as a custom field for dependent checks to access\\n+      // Apply JavaScript transformation if specified\\n+      if (transformJs) {\\n+        try {\\n+          if (!this.sandbox) {\\n+            this.sandbox = this.createSecureSandbox();\\n+          }\\n+\\n+          // Create scope for JavaScript transform (scope, not context)\\n+          const jsScope: Record<string, unknown> = {\\n+            output: data,\\n+            pr: templateContext.pr,\\n+            outputs: templateContext.outputs,\\n+            env: process.env,\\n+          };\\n+\\n+          const result = compileAndRun(this.sandbox, transformJs, jsScope, {\\n+            injectLog: true,\\n+            logPrefix: 'üîç [transform_js]',\\n+            wrapFunction: true,\\n+          });\\n+          processedData = result;\\n+          logger.verbose(`‚úì Applied JavaScript transform successfully`);\\n+        } catch (error) {\\n+          logger.error(\\n+            `‚úó Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`\\n+          );\\n+          return {\\n+            issues: [\\n+              {\\n+                file: 'http_client',\\n+                line: 0,\\n+                ruleId: 'http_client/transform_js_error',\\n+                message: `Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              },\\n+            ],\\n+          };\\n+        }\\n+      }\\n+\\n+      // Return the fetched data in the standard `output` property for guarantee evaluation\\n+      // This is consistent with other providers (script, command, ai, etc.)\\n       return {\\n         issues: [],\\n-        // Add custom data field that will be passed through to dependent checks\\n-        data: processedData,\\n-      } as ReviewSummary & { data: unknown };\\n+        output: processedData,\\n+      } as unknown as ReviewSummary;\\n     } catch (error) {\\n       return {\\n         issues: [\\n@@ -149,7 +275,7 @@ export class HttpClientProvider extends CheckProvider {\\n             file: 'http_client',\\n             line: 0,\\n             ruleId: 'http_client/fetch_error',\\n-            message: `Failed to fetch from ${url}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            message: `Failed to fetch from ${resolvedUrlForErrors}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n             severity: 'error',\\n             category: 'logic',\\n           },\\n@@ -198,7 +324,14 @@ export class HttpClientProvider extends CheckProvider {\\n \\n       clearTimeout(timeoutId);\\n \\n+      logger.verbose(`[http_client] Response: ${response.status} ${response.statusText}`);\\n+\\n       if (!response.ok) {\\n+        // Log response body for debugging auth failures\\n+        try {\\n+          const errorBody = await response.text();\\n+          logger.warn(`[http_client] Error body: ${errorBody.substring(0, 500)}`);\\n+        } catch {}\\n         throw new Error(`HTTP ${response.status}: ${response.statusText}`);\\n       }\\n \\n@@ -233,6 +366,112 @@ export class HttpClientProvider extends CheckProvider {\\n     }\\n   }\\n \\n+  private async downloadToFile(\\n+    url: string,\\n+    method: string,\\n+    headers: Record<string, string>,\\n+    body: string | undefined,\\n+    timeout: number,\\n+    outputFile: string\\n+  ): Promise<ReviewSummary> {\\n+    // Check if fetch is available (Node 18+)\\n+    if (typeof fetch === 'undefined') {\\n+      throw new Error('HTTP client provider requires Node.js 18+ or node-fetch package');\\n+    }\\n+\\n+    const controller = new AbortController();\\n+    const timeoutId = setTimeout(() => controller.abort(), timeout);\\n+\\n+    try {\\n+      const requestOptions: RequestInit = {\\n+        method,\\n+        headers: { ...headers },\\n+        signal: controller.signal,\\n+      };\\n+\\n+      // Add body for non-GET requests\\n+      if (method !== 'GET' && body) {\\n+        requestOptions.body = body;\\n+        if (!headers['Content-Type'] && !headers['content-type']) {\\n+          requestOptions.headers = {\\n+            ...requestOptions.headers,\\n+            'Content-Type': 'application/json',\\n+          };\\n+        }\\n+      }\\n+\\n+      const response = await fetch(url, requestOptions);\\n+      clearTimeout(timeoutId);\\n+\\n+      if (!response.ok) {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_error',\\n+              message: `Failed to download file: HTTP ${response.status}: ${response.statusText}`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      // Create parent directory if it doesn't exist\\n+      const parentDir = path.dirname(outputFile);\\n+      if (parentDir && !fs.existsSync(parentDir)) {\\n+        fs.mkdirSync(parentDir, { recursive: true });\\n+      }\\n+\\n+      // Get the response as an ArrayBuffer and write to file\\n+      const arrayBuffer = await response.arrayBuffer();\\n+      const buffer = Buffer.from(arrayBuffer);\\n+      fs.writeFileSync(outputFile, buffer);\\n+\\n+      const contentType = response.headers.get('content-type') || 'application/octet-stream';\\n+      logger.verbose(`[http_client] Downloaded: ${outputFile} (${buffer.length} bytes)`);\\n+\\n+      return {\\n+        issues: [],\\n+        file_path: outputFile,\\n+        size: buffer.length,\\n+        content_type: contentType,\\n+        cached: false,\\n+      } as unknown as ReviewSummary;\\n+    } catch (error: unknown) {\\n+      clearTimeout(timeoutId);\\n+\\n+      if (error instanceof Error && error.name === 'AbortError') {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_timeout',\\n+              message: `Download timed out after ${timeout}ms`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      return {\\n+        issues: [\\n+          {\\n+            file: 'http_client',\\n+            line: 0,\\n+            ruleId: 'http_client/download_error',\\n+            message: `Failed to download file: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            severity: 'error',\\n+            category: 'logic',\\n+          },\\n+        ],\\n+      };\\n+    }\\n+  }\\n+\\n   getSupportedConfigKeys(): string[] {\\n     return [\\n       'type',\\n@@ -241,7 +480,10 @@ export class HttpClientProvider extends CheckProvider {\\n       'headers',\\n       'body',\\n       'transform',\\n+      'transform_js',\\n       'timeout',\\n+      'output_file',\\n+      'skip_if_exists',\\n       'depends_on',\\n       'on',\\n       'if',\\n@@ -261,6 +503,8 @@ export class HttpClientProvider extends CheckProvider {\\n       'Network access to the endpoint',\\n       'Optional: Transform template for processing response data',\\n       'Optional: Body template for POST/PUT requests',\\n+      'Optional: output_file path to download response to a file',\\n+      'Optional: skip_if_exists (default: true) to enable caching for file downloads',\\n     ];\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/log-check-provider.ts\",\"additions\":19,\"deletions\":8,\"changes\":27,\"patch\":\"diff --git a/src/providers/log-check-provider.ts b/src/providers/log-check-provider.ts\\nindex 9c6332d1..b57f632c 100644\\n--- a/src/providers/log-check-provider.ts\\n+++ b/src/providers/log-check-provider.ts\\n@@ -78,7 +78,8 @@ export class LogCheckProvider extends CheckProvider {\\n       includeDependencies,\\n       includeMetadata,\\n       config.__outputHistory as Map<string, unknown[]> | undefined,\\n-      context\\n+      context,\\n+      config\\n     );\\n \\n     // Render the log message template\\n@@ -100,12 +101,20 @@ export class LogCheckProvider extends CheckProvider {\\n     else if (level === 'debug') logger.debug(logOutput);\\n     else logger.info(logOutput);\\n \\n-    // Return with the log content as custom data for dependent checks\\n-    return {\\n+    // Return with the log content as custom data for dependent checks.\\n+    // For chat-style logs (group: chat), also expose a structured\\n+    // `output.text` field so frontends like Slack can post a clean\\n+    // human-facing message without the level prefix.\\n+    const summary: ReviewSummary & { logOutput: string; output?: unknown } = {\\n       issues: [],\\n-      // Add log output as custom field\\n       logOutput,\\n-    } as ReviewSummary & { logOutput: string };\\n+    };\\n+\\n+    if ((config as any).group === 'chat') {\\n+      (summary as any).output = { text: renderedMessage };\\n+    }\\n+\\n+    return summary;\\n   }\\n \\n   private buildTemplateContext(\\n@@ -115,7 +124,8 @@ export class LogCheckProvider extends CheckProvider {\\n     _includeDependencies: boolean = true,\\n     includeMetadata: boolean = true,\\n     outputHistory?: Map<string, unknown[]>,\\n-    executionContext?: ExecutionContext\\n+    executionContext?: ExecutionContext,\\n+    config?: CheckProviderConfig\\n   ): Record<string, unknown> {\\n     const context: Record<string, unknown> = {};\\n \\n@@ -197,7 +207,8 @@ export class LogCheckProvider extends CheckProvider {\\n     }\\n \\n     // Add workflow inputs if available\\n-    const workflowInputs = executionContext?.workflowInputs || {};\\n+    // Check config first (set by projectWorkflowToGraph), then fall back to executionContext\\n+    const workflowInputs = (config as any)?.workflowInputs || executionContext?.workflowInputs || {};\\n     logger.debug(\\n       `[LogProvider] Adding ${Object.keys(workflowInputs).length} workflow inputs to context`\\n     );\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/script-check-provider.ts\",\"additions\":37,\"deletions\":1,\"changes\":38,\"patch\":\"diff --git a/src/providers/script-check-provider.ts b/src/providers/script-check-provider.ts\\nindex 9825bb69..947dbd73 100644\\n--- a/src/providers/script-check-provider.ts\\n+++ b/src/providers/script-check-provider.ts\\n@@ -59,6 +59,16 @@ export class ScriptCheckProvider extends CheckProvider {\\n       reuseSession?: boolean;\\n     } & import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n+    // Test hook: mock output for this step (short-circuit execution)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = _sessionInfo?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        // Return mock directly as step output\\n+        return { issues: [], output: mock } as ReviewSummary & { output: unknown };\\n+      }\\n+    } catch {}\\n+\\n     const script = String(config.content || '');\\n     const memoryStore = MemoryStore.getInstance();\\n     const ctx = buildProviderTemplateContext(\\n@@ -72,10 +82,36 @@ export class ScriptCheckProvider extends CheckProvider {\\n     // Keep provider quiet by default; no step-specific debug\\n     // (historical ad-hoc logs removed to avoid hardcoding step names).\\n \\n+    // Add workflow inputs to the context\\n+    const inputs = (config as any).workflowInputs || _sessionInfo?.workflowInputs || {};\\n+    (ctx as any).inputs = inputs;\\n+\\n+    // Add environment variables to context (consistent with http-client-provider)\\n+    (ctx as any).env = process.env;\\n+\\n     // Attach synchronous memory ops consistent with memory provider\\n     const { ops, needsSave } = createSyncMemoryOps(memoryStore);\\n     (ctx as any).memory = ops as unknown as Record<string, unknown>;\\n \\n+    // Add helper functions to the context\\n+    (ctx as any).escapeXml = (str: unknown): string => {\\n+      if (str == null) return '';\\n+      return String(str)\\n+        .replace(/&/g, '&amp;')\\n+        .replace(/</g, '&lt;')\\n+        .replace(/>/g, '&gt;')\\n+        .replace(/\\\"/g, '&quot;')\\n+        .replace(/'/g, '&apos;');\\n+    };\\n+\\n+    // Add btoa/atob for base64 encoding/decoding (browser API polyfill)\\n+    (ctx as any).btoa = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'binary').toString('base64');\\n+    };\\n+    (ctx as any).atob = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'base64').toString('binary');\\n+    };\\n+\\n     // Evaluate the script in a secure sandbox (per-execution instance)\\n     const sandbox = this.createSecureSandbox();\\n     let result: unknown;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/workflow-check-provider.ts\",\"additions\":164,\"deletions\":43,\"changes\":207,\"patch\":\"diff --git a/src/providers/workflow-check-provider.ts b/src/providers/workflow-check-provider.ts\\nindex 116d4dff..681703ce 100644\\n--- a/src/providers/workflow-check-provider.ts\\n+++ b/src/providers/workflow-check-provider.ts\\n@@ -67,34 +67,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n   ): Promise<ReviewSummary> {\\n     const cfg = config as CheckProviderConfig & { workflow?: string; config?: string };\\n     const isConfigPathMode = !!cfg.config && !cfg.workflow;\\n+    const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n \\n-    // Test harness support: allow mocking workflow checks as a black box.\\n-    // If a mock is provided for this step, short-circuit nested execution and\\n-    // return a ReviewSummary with optional output field.\\n-    try {\\n-      const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n-      // test-runner passes hooks on execution context\\n-      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n-      if (mock !== undefined) {\\n-        const ms = mock as any;\\n-        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n-        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n-        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n-        const summary: ReviewSummary & { output?: unknown } = {\\n-          issues: issuesArr,\\n-          output: out,\\n-          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n-        } as any;\\n-        return summary;\\n-      }\\n-    } catch {}\\n-\\n-    // Resolve workflow definition\\n+    // Resolve workflow definition FIRST (needed for input preparation and validation)\\n     let workflow: WorkflowDefinition | undefined;\\n     let workflowId = cfg.workflow as string | undefined;\\n \\n     if (isConfigPathMode) {\\n-      const parentCwd = ((context as any)?._parentContext?.workingDirectory ||\\n+      // Use originalWorkingDirectory for config file resolution - workflow configs\\n+      // should be loaded from the original project path, not the sandbox\\n+      const parentCwd = ((context as any)?._parentContext?.originalWorkingDirectory ||\\n+        (context as any)?._parentContext?.workingDirectory ||\\n+        (context as any)?.originalWorkingDirectory ||\\n         (context as any)?.workingDirectory ||\\n         process.cwd()) as string;\\n       workflow = await this.loadWorkflowFromConfigPath(String(cfg.config), parentCwd);\\n@@ -109,9 +93,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       logger.info(`Executing workflow '${workflowId}'`);\\n     }\\n \\n-    // Prepare inputs\\n+    // Prepare inputs - do this BEFORE mock check so we can validate inputs even when mocked\\n+    // This allows tests to assert that the correct inputs were passed to workflow steps\\n     const inputs = await this.prepareInputs(workflow, config, prInfo, dependencyResults);\\n \\n+    // Capture resolved workflow inputs for testing assertions (reuse prompt capture infrastructure)\\n+    // This allows using `prompts` assertions with `provider: 'workflow'` to verify inputs\\n+    try {\\n+      // Serialize inputs to capture for assertions - specifically the context field\\n+      // which contains the rendered template with dependency outputs\\n+      const inputsCapture = Object.entries(inputs)\\n+        .map(([k, v]) => `${k}: ${typeof v === 'string' ? v : JSON.stringify(v)}`)\\n+        .join('\\\\n\\\\n');\\n+      (context as any)?.hooks?.onPromptCaptured?.({\\n+        step: String(stepName),\\n+        provider: 'workflow',\\n+        prompt: inputsCapture,\\n+      });\\n+    } catch {\\n+      // Ignore capture errors - this is only for testing\\n+    }\\n+\\n     // Validate inputs\\n     const validation = this.registry.validateInputs(workflow, inputs);\\n     if (!validation.valid) {\\n@@ -119,6 +121,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       throw new Error(`Invalid workflow inputs: ${errors}`);\\n     }\\n \\n+    // Test harness support: allow mocking workflow checks as a black box.\\n+    // If a mock is provided for this step, short-circuit nested execution and\\n+    // return a ReviewSummary with optional output field.\\n+    // NOTE: This happens AFTER input preparation/validation so tests can assert inputs are correct\\n+    try {\\n+      // test-runner passes hooks on execution context\\n+      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        const ms = mock as any;\\n+        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n+        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n+        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n+        const summary: ReviewSummary & { output?: unknown } = {\\n+          issues: issuesArr,\\n+          output: out,\\n+          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n+        } as any;\\n+        return summary;\\n+      }\\n+    } catch {}\\n+\\n     // Apply overrides to workflow steps if specified\\n     const modifiedWorkflow = this.applyOverrides(workflow, config);\\n \\n@@ -211,6 +234,80 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Extract eventContext for slack/conversation\\n+    const eventContext = config.eventContext || {};\\n+\\n+    // Debug logging for conversation context\\n+    logger.debug(`[WorkflowProvider] prepareInputs for ${workflow.id}`);\\n+    logger.debug(`[WorkflowProvider] eventContext keys: ${Object.keys(eventContext).join(', ') || 'none'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.slack: ${eventContext.slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.conversation: ${(eventContext as any).conversation ? 'present' : 'absent'}`);\\n+\\n+    // Extract slack context (if provided via eventContext.slack)\\n+    const slack = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        const slackCtx = anyCtx?.slack;\\n+        if (slackCtx && typeof slackCtx === 'object') return slackCtx;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Extract unified conversation context across transports (Slack & GitHub)\\n+    const conversation = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        if (anyCtx?.slack?.conversation) return anyCtx.slack.conversation;\\n+        if (anyCtx?.github?.conversation) return anyCtx.github.conversation;\\n+        if (anyCtx?.conversation) return anyCtx.conversation;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Debug logging for extracted context\\n+    logger.debug(`[WorkflowProvider] slack extracted: ${slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] conversation extracted: ${conversation ? 'present' : 'absent'}`);\\n+    if (conversation) {\\n+      logger.debug(`[WorkflowProvider] conversation.messages count: ${Array.isArray((conversation as any).messages) ? (conversation as any).messages.length : 0}`);\\n+    }\\n+\\n+    // Extract output history from config (passed via __outputHistory)\\n+    const outputHistory = (config as any).__outputHistory as Map<string, unknown[]> | undefined;\\n+    const outputs_history: Record<string, unknown[]> = {};\\n+    if (outputHistory) {\\n+      for (const [k, v] of outputHistory.entries()) {\\n+        outputs_history[k] = v;\\n+      }\\n+    }\\n+\\n+    // Build template context with all available data\\n+    // Extract .output from each dependency result so that outputs['step-name'].field works naturally\\n+    // (not outputs['step-name'].output.field)\\n+    const outputsMap: Record<string, unknown> = {};\\n+    logger.debug(`[WorkflowProvider] dependencyResults: ${dependencyResults ? dependencyResults.size : 'undefined'} entries`);\\n+    if (dependencyResults) {\\n+      for (const [key, result] of dependencyResults.entries()) {\\n+        // Extract the output property, or use the whole result if output is undefined\\n+        const extracted = (result as any).output ?? result;\\n+        outputsMap[key] = extracted;\\n+        // Debug: log what we extracted for each dependency\\n+        const extractedKeys = extracted && typeof extracted === 'object' ? Object.keys(extracted).join(', ') : 'not-object';\\n+        logger.debug(`[WorkflowProvider] outputs['${key}']: keys=[${extractedKeys}]`);\\n+      }\\n+    }\\n+    const templateContext = {\\n+      pr: prInfo,\\n+      outputs: outputsMap,\\n+      env: process.env,\\n+      slack,\\n+      conversation,\\n+      outputs_history,\\n+    };\\n+\\n     // Apply user-provided inputs (args)\\n     const userInputs = config.args || config.workflow_inputs; // Support both for compatibility\\n     if (userInputs) {\\n@@ -219,11 +316,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         if (typeof value === 'string') {\\n           // Check if it's a Liquid template\\n           if (value.includes('{{') || value.includes('{%')) {\\n-            inputs[key] = await this.liquid.parseAndRender(value, {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            });\\n+            inputs[key] = await this.liquid.parseAndRender(value, templateContext);\\n+            // Debug: log rendered template value for important inputs\\n+            if (key === 'text' || key === 'question' || key === 'context') {\\n+              const rendered = String(inputs[key]);\\n+              logger.info(`[WorkflowProvider] Rendered '${key}' input (${rendered.length} chars): ${rendered.substring(0, 500)}${rendered.length > 500 ? '...' : ''}`);\\n+            }\\n           } else {\\n             inputs[key] = value;\\n           }\\n@@ -234,11 +332,7 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           inputs[key] = compileAndRun(\\n             sandbox,\\n             exprValue.expression,\\n-            {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            },\\n+            templateContext,\\n             { injectLog: true, logPrefix: `workflow.input.${key}` }\\n           );\\n         } else {\\n@@ -367,6 +461,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n     try {\\n       await childMemory.initialize();\\n     } catch {}\\n+    // Enhanced debug logging for workspace propagation diagnosis\\n+    const parentWorkspace = parentContext?.workspace;\\n+    logger.info(`[WorkflowProvider] Workspace propagation for nested workflow '${workflow.id}':`);\\n+    logger.info(`[WorkflowProvider]   parentContext exists: ${!!parentContext}`);\\n+    logger.info(`[WorkflowProvider]   parentContext.workspace exists: ${!!parentWorkspace}`);\\n+    if (parentWorkspace) {\\n+      logger.info(`[WorkflowProvider]   parentWorkspace.isEnabled(): ${parentWorkspace.isEnabled?.() ?? 'N/A'}`);\\n+      const projectCount = parentWorkspace.listProjects?.()?.length ?? 'N/A';\\n+      logger.info(`[WorkflowProvider]   parentWorkspace project count: ${projectCount}`);\\n+    } else {\\n+      logger.warn(`[WorkflowProvider]   NO WORKSPACE from parent - nested checkouts won't be added to workspace!`);\\n+    }\\n \\n     const childContext = {\\n       mode: 'state-machine' as const,\\n@@ -374,7 +480,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       checks: checksMetadata,\\n       journal: childJournal,\\n       memory: childMemory,\\n+      // For nested workflows we continue to execute inside the same logical\\n+      // working directory as the parent run. When workspace isolation is\\n+      // enabled on the parent engine, its WorkspaceManager is also propagated\\n+      // so that nested checks (AI, git-checkout, etc.) see the same isolated\\n+      // workspace and project symlinks instead of falling back to the Visor\\n+      // repository root.\\n       workingDirectory: parentContext?.workingDirectory || process.cwd(),\\n+      originalWorkingDirectory:\\n+        parentContext?.originalWorkingDirectory ||\\n+        parentContext?.workingDirectory ||\\n+        process.cwd(),\\n+      workspace: parentWorkspace,\\n       // Always use a fresh session for nested workflows to isolate history\\n       sessionId: uuidv4(),\\n       event: parentContext?.event || prInfo.eventType,\\n@@ -500,6 +617,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     } catch {}\\n \\n+    // Create outputs map that directly exposes .output values (not wrapped in { output, issues })\\n+    // This makes {{ outputs['check-name'] }} work naturally without needing .output\\n+    const outputsMap = Object.fromEntries(\\n+      Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n+    );\\n+\\n     for (const output of workflow.outputs) {\\n       if (output.value_js) {\\n         // JavaScript expression\\n@@ -508,10 +631,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           output.value_js,\\n           {\\n             inputs,\\n-            steps: Object.fromEntries(\\n-              Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-            ),\\n-            outputs: flat,\\n+            outputs: outputsMap,\\n+            // Keep 'steps' as alias for backwards compatibility\\n+            steps: outputsMap,\\n             pr: prInfo,\\n           },\\n           { injectLog: true, logPrefix: `workflow.output.${output.name}` }\\n@@ -520,10 +642,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         // Liquid template\\n         outputs[output.name] = await this.liquid.parseAndRender(output.value, {\\n           inputs,\\n-          steps: Object.fromEntries(\\n-            Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-          ),\\n-          outputs: flat,\\n+          outputs: outputsMap,\\n+          // Keep 'steps' as alias for backwards compatibility\\n+          steps: outputsMap,\\n           pr: prInfo,\\n         });\\n       }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/client.ts\",\"additions\":65,\"deletions\":1,\"changes\":66,\"patch\":\"diff --git a/src/slack/client.ts b/src/slack/client.ts\\nindex 292cc358..435f8b0d 100644\\n--- a/src/slack/client.ts\\n+++ b/src/slack/client.ts\\n@@ -142,6 +142,70 @@ export class SlackClient {\\n     }\\n   }\\n \\n+  public readonly files = {\\n+    /**\\n+     * Upload a file to Slack using files.uploadV2 API\\n+     * @param options Upload options including file content, filename, channel, and thread_ts\\n+     */\\n+    uploadV2: async ({\\n+      content,\\n+      filename,\\n+      channel,\\n+      thread_ts,\\n+      title,\\n+      initial_comment,\\n+    }: {\\n+      content: Buffer;\\n+      filename: string;\\n+      channel: string;\\n+      thread_ts?: string;\\n+      title?: string;\\n+      initial_comment?: string;\\n+    }): Promise<{ ok: boolean; file?: { id: string; permalink?: string } }> => {\\n+      try {\\n+        // Step 1: Get upload URL\\n+        const getUrlResp: any = await this.api('files.getUploadURLExternal', {\\n+          filename,\\n+          length: content.length,\\n+        });\\n+        if (!getUrlResp || getUrlResp.ok !== true || !getUrlResp.upload_url) {\\n+          console.warn(`Slack files.getUploadURLExternal failed: ${getUrlResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 2: Upload file content to the URL\\n+        const uploadResp = await fetch(getUrlResp.upload_url, {\\n+          method: 'POST',\\n+          body: content,\\n+        });\\n+        if (!uploadResp.ok) {\\n+          console.warn(`Slack file upload to URL failed: ${uploadResp.status}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 3: Complete the upload and share to channel\\n+        const completeResp: any = await this.api('files.completeUploadExternal', {\\n+          files: [{ id: getUrlResp.file_id, title: title || filename }],\\n+          channel_id: channel,\\n+          thread_ts,\\n+          initial_comment,\\n+        });\\n+        if (!completeResp || completeResp.ok !== true) {\\n+          console.warn(`Slack files.completeUploadExternal failed: ${completeResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        return {\\n+          ok: true,\\n+          file: completeResp.files?.[0] || { id: getUrlResp.file_id },\\n+        };\\n+      } catch (e) {\\n+        console.warn(`Slack file upload failed: ${e instanceof Error ? e.message : String(e)}`);\\n+        return { ok: false };\\n+      }\\n+    },\\n+  };\\n+\\n   getWebClient(): any {\\n     return {\\n       conversations: {\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/markdown.ts\",\"additions\":204,\"deletions\":5,\"changes\":209,\"patch\":\"diff --git a/src/slack/markdown.ts b/src/slack/markdown.ts\\nindex cc63fb5f..9f91c25f 100644\\n--- a/src/slack/markdown.ts\\n+++ b/src/slack/markdown.ts\\n@@ -3,14 +3,191 @@\\n // without pulling in a full Markdown parser.\\n //\\n // Supported conversions:\\n+// - # Header / ## Header  ‚Üí *Header* (bold with visual separation)\\n // - **bold** / __bold__   ‚Üí *bold*\\n // - [label](url)          ‚Üí <url|label>\\n // - ![alt](url)           ‚Üí <url|alt>\\n // - *italic* (inline)     ‚Üí _italic_\\n+// - ```mermaid blocks     ‚Üí rendered to PNG and uploaded to Slack\\n //\\n // Everything else is passed through unchanged; Slack will still render many\\n // Markdown-like constructs (lists, code fences, etc.) natively.\\n \\n+import { spawn } from 'child_process';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n+import * as os from 'os';\\n+\\n+/**\\n+ * Represents an extracted mermaid diagram\\n+ */\\n+export interface MermaidDiagram {\\n+  /** The full match including ```mermaid and ``` */\\n+  fullMatch: string;\\n+  /** The mermaid code content */\\n+  code: string;\\n+  /** Start index in the original text */\\n+  startIndex: number;\\n+  /** End index in the original text */\\n+  endIndex: number;\\n+}\\n+\\n+/**\\n+ * Extract all mermaid code blocks from text\\n+ */\\n+export function extractMermaidDiagrams(text: string): MermaidDiagram[] {\\n+  const diagrams: MermaidDiagram[] = [];\\n+  // Match ```mermaid followed by newline, content, and closing ```\\n+  const regex = /```mermaid\\\\s*\\\\n([\\\\s\\\\S]*?)```/g;\\n+  let match;\\n+  while ((match = regex.exec(text)) !== null) {\\n+    diagrams.push({\\n+      fullMatch: match[0],\\n+      code: match[1].trim(),\\n+      startIndex: match.index,\\n+      endIndex: match.index + match[0].length,\\n+    });\\n+  }\\n+  return diagrams;\\n+}\\n+\\n+/**\\n+ * Render a mermaid diagram to PNG using mmdc CLI (@mermaid-js/mermaid-cli).\\n+ *\\n+ * Requirements:\\n+ * - Node.js and npx must be available in PATH\\n+ * - Network access on first run (npx downloads the package)\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ *\\n+ * On Linux, you may need to install chromium dependencies:\\n+ *   apt-get install -y chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ *\\n+ * On Docker/CI, consider using a base image with puppeteer support or\\n+ * pre-installing @mermaid-js/mermaid-cli globally.\\n+ *\\n+ * @param mermaidCode The mermaid diagram code\\n+ * @returns Buffer containing PNG data, or null if rendering failed\\n+ */\\n+export async function renderMermaidToPng(mermaidCode: string): Promise<Buffer | null> {\\n+  // Create temp files for input and output\\n+  const tmpDir = os.tmpdir();\\n+  const inputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.mmd`);\\n+  const outputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.png`);\\n+\\n+  try {\\n+    // Write mermaid code to temp file\\n+    fs.writeFileSync(inputFile, mermaidCode, 'utf-8');\\n+\\n+    // Detect system chromium for puppeteer (mermaid-cli dependency)\\n+    // Without this, puppeteer may hang trying to download its own chromium\\n+    const chromiumPaths = [\\n+      '/usr/bin/chromium',\\n+      '/usr/bin/chromium-browser',\\n+      '/usr/bin/google-chrome',\\n+      '/usr/bin/chrome',\\n+    ];\\n+    let chromiumPath: string | undefined;\\n+    for (const p of chromiumPaths) {\\n+      if (fs.existsSync(p)) {\\n+        chromiumPath = p;\\n+        break;\\n+      }\\n+    }\\n+\\n+    // Build environment with chromium path if found\\n+    const env = { ...process.env };\\n+    if (chromiumPath) {\\n+      env.PUPPETEER_EXECUTABLE_PATH = chromiumPath;\\n+    }\\n+\\n+    // Run mmdc to render PNG\\n+    const result = await new Promise<{ success: boolean; error?: string }>((resolve) => {\\n+      const proc = spawn('npx', [\\n+        '--yes',\\n+        '@mermaid-js/mermaid-cli',\\n+        '-i', inputFile,\\n+        '-o', outputFile,\\n+        '-e', 'png',\\n+        '-b', 'white',\\n+        '-w', '1200',\\n+      ], {\\n+        timeout: 60000, // 60 second timeout (first run may download packages)\\n+        stdio: ['pipe', 'pipe', 'pipe'],\\n+        env,\\n+      });\\n+\\n+      let stderr = '';\\n+      proc.stderr?.on('data', (data) => {\\n+        stderr += data.toString();\\n+      });\\n+\\n+      proc.on('close', (code) => {\\n+        if (code === 0) {\\n+          resolve({ success: true });\\n+        } else {\\n+          resolve({ success: false, error: stderr || `Exit code ${code}` });\\n+        }\\n+      });\\n+\\n+      proc.on('error', (err) => {\\n+        resolve({ success: false, error: err.message });\\n+      });\\n+    });\\n+\\n+    if (!result.success) {\\n+      console.warn(`Mermaid rendering failed: ${result.error}`);\\n+      return null;\\n+    }\\n+\\n+    // Read the output PNG\\n+    if (!fs.existsSync(outputFile)) {\\n+      console.warn('Mermaid output file not created');\\n+      return null;\\n+    }\\n+\\n+    const pngBuffer = fs.readFileSync(outputFile);\\n+    return pngBuffer;\\n+  } catch (e) {\\n+    console.warn(`Mermaid rendering error: ${e instanceof Error ? e.message : String(e)}`);\\n+    return null;\\n+  } finally {\\n+    // Cleanup temp files\\n+    try {\\n+      if (fs.existsSync(inputFile)) fs.unlinkSync(inputFile);\\n+      if (fs.existsSync(outputFile)) fs.unlinkSync(outputFile);\\n+    } catch {\\n+      // Ignore cleanup errors\\n+    }\\n+  }\\n+}\\n+\\n+/**\\n+ * Replace mermaid blocks in text with a placeholder message\\n+ * @param text Original text\\n+ * @param diagrams Extracted diagrams\\n+ * @param replacement Text to replace each diagram with (or a function that returns replacement for each index)\\n+ */\\n+export function replaceMermaidBlocks(\\n+  text: string,\\n+  diagrams: MermaidDiagram[],\\n+  replacement: string | ((index: number) => string) = '_(See diagram above)_'\\n+): string {\\n+  if (diagrams.length === 0) return text;\\n+\\n+  // Sort by start index descending to replace from end to start (preserves indices)\\n+  const sorted = [...diagrams].sort((a, b) => b.startIndex - a.startIndex);\\n+\\n+  let result = text;\\n+  sorted.forEach((diagram, sortedIndex) => {\\n+    // Calculate original index (since we sorted in reverse)\\n+    const originalIndex = diagrams.length - 1 - sortedIndex;\\n+    const rep = typeof replacement === 'function' ? replacement(originalIndex) : replacement;\\n+    result = result.slice(0, diagram.startIndex) + rep + result.slice(diagram.endIndex);\\n+  });\\n+\\n+  return result;\\n+}\\n+\\n export function markdownToSlack(text: string): string {\\n   if (!text || typeof text !== 'string') return '';\\n \\n@@ -33,7 +210,7 @@ export function markdownToSlack(text: string): string {\\n   out = out.replace(/\\\\*\\\\*([^*]+)\\\\*\\\\*/g, (_m, inner: string) => `*${inner}*`);\\n   out = out.replace(/__([^_]+)__/g, (_m, inner: string) => `*${inner}*`);\\n \\n-  // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation).\\n+  // Process lines for headers and bullet lists.\\n   // Slack's mrkdwn handles \\\"‚Ä¢\\\" bullets more naturally than raw \\\"-\\\" Markdown.\\n   const lines = out.split(/\\\\r?\\\\n/);\\n   let inCodeBlock = false;\\n@@ -46,9 +223,31 @@ export function markdownToSlack(text: string): string {\\n       continue;\\n     }\\n     if (inCodeBlock) continue;\\n-    const match = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n-    if (match) {\\n-      const [, indent, , rest] = match;\\n+\\n+    // Headers: # Header ‚Üí *Header* (Slack doesn't have native headers)\\n+    // Match 1-6 # at start of line, followed by space and text\\n+    const headerMatch = /^(#{1,6})\\\\s+(.+)$/.exec(trimmed);\\n+    if (headerMatch) {\\n+      const [, hashes, headerText] = headerMatch;\\n+      // For h1/h2, add extra emphasis with newline before (if not first line\\n+      // and previous line is not empty/header/code-fence)\\n+      const prevLine = i > 0 ? lines[i - 1].trim() : '';\\n+      const prevIsHeaderOrFence =\\n+        /^#{1,6}\\\\s+/.test(prevLine) ||\\n+        /^\\\\*[^*]+\\\\*$/.test(prevLine) ||\\n+        /^```/.test(prevLine);\\n+      if (hashes.length <= 2 && i > 0 && prevLine !== '' && !prevIsHeaderOrFence) {\\n+        lines[i] = `\\\\n*${headerText.trim()}*`;\\n+      } else {\\n+        lines[i] = `*${headerText.trim()}*`;\\n+      }\\n+      continue;\\n+    }\\n+\\n+    // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation)\\n+    const bulletMatch = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n+    if (bulletMatch) {\\n+      const [, indent, , rest] = bulletMatch;\\n       lines[i] = `${indent}‚Ä¢ ${rest}`;\\n     }\\n   }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/prompt-state.ts\",\"additions\":19,\"deletions\":2,\"changes\":21,\"patch\":\"diff --git a/src/slack/prompt-state.ts b/src/slack/prompt-state.ts\\nindex a13233b2..544777c6 100644\\n--- a/src/slack/prompt-state.ts\\n+++ b/src/slack/prompt-state.ts\\n@@ -95,7 +95,10 @@ export class PromptStateManager {\\n   setFirstMessage(channel: string, threadTs: string, text: string): void {\\n     const key = this.key(channel, threadTs);\\n     if (!text || !text.trim()) return;\\n-    if (!this.firstMessage.has(key)) {\\n+    const existing = this.firstMessage.get(key);\\n+    // Only set if: no entry exists OR the existing entry was already consumed\\n+    // This allows new messages to be captured after a resume cycle\\n+    if (!existing || existing.consumed) {\\n       this.firstMessage.set(key, { text, consumed: false });\\n     }\\n   }\\n@@ -130,6 +133,20 @@ export class PromptStateManager {\\n         removed++;\\n       }\\n     }\\n+    // Also clean up stale firstMessage entries (consumed entries older than TTL)\\n+    // Keep unconsumed entries to avoid losing user messages\\n+    for (const [key] of this.firstMessage.entries()) {\\n+      const waitingInfo = this.waiting.get(key);\\n+      // If no corresponding waiting entry exists and the firstMessage was consumed,\\n+      // the conversation is likely complete - safe to remove\\n+      if (!waitingInfo) {\\n+        const entry = this.firstMessage.get(key);\\n+        if (entry?.consumed) {\\n+          this.firstMessage.delete(key);\\n+          removed++;\\n+        }\\n+      }\\n+    }\\n     if (removed) {\\n       try {\\n         logger.info(`[prompt-state] cleanup removed ${removed} entries`);\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/socket-runner.ts\",\"additions\":56,\"deletions\":32,\"changes\":88,\"patch\":\"diff --git a/src/slack/socket-runner.ts b/src/slack/socket-runner.ts\\nindex d1681a03..5b461d22 100644\\n--- a/src/slack/socket-runner.ts\\n+++ b/src/slack/socket-runner.ts\\n@@ -7,6 +7,7 @@ import { SlackAdapter } from './adapter';\\n import { CachePrewarmer } from './cache-prewarmer';\\n import { RateLimiter, type RateLimitConfig } from './rate-limiter';\\n import type { SlackBotConfig } from '../types/bot';\\n+import { withActiveSpan } from '../telemetry/trace-helpers';\\n \\n type SlackSocketConfig = {\\n   appToken?: string; // xapp- token\\n@@ -30,6 +31,7 @@ export class SlackSocketRunner {\\n   private botUserId?: string;\\n   private processedKeys: Map<string, number> = new Map();\\n   private adapter?: SlackAdapter;\\n+  private retryCount = 0;\\n \\n   constructor(engine: StateMachineExecutionEngine, cfg: VisorConfig, opts: SlackSocketConfig) {\\n     const app = opts.appToken || process.env.SLACK_APP_TOKEN || '';\\n@@ -107,7 +109,10 @@ export class SlackSocketRunner {\\n \\n   private async connect(url: string): Promise<void> {\\n     this.ws = new WebSocket(url);\\n-    this.ws.on('open', () => logger.info('[SlackSocket] WebSocket connected'));\\n+    this.ws.on('open', () => {\\n+      this.retryCount = 0; // Reset on successful connection\\n+      logger.info('[SlackSocket] WebSocket connected');\\n+    });\\n     this.ws.on('close', (code, reason) => {\\n       logger.warn(`[SlackSocket] WebSocket closed: ${code} ${reason}`);\\n       setTimeout(() => this.restart().catch(() => {}), 1000);\\n@@ -123,7 +128,13 @@ export class SlackSocketRunner {\\n       const url = await this.openConnection();\\n       await this.connect(url);\\n     } catch (e) {\\n-      logger.error(`[SlackSocket] Restart failed: ${e instanceof Error ? e.message : e}`);\\n+      this.retryCount++;\\n+      // Exponential backoff: 2s, 4s, 8s, 16s, 32s, capped at 60s\\n+      const delay = Math.min(2000 * Math.pow(2, this.retryCount - 1), 60000);\\n+      logger.error(\\n+        `[SlackSocket] Restart failed (attempt ${this.retryCount}), retrying in ${Math.round(delay / 1000)}s: ${e instanceof Error ? e.message : e}`\\n+      );\\n+      setTimeout(() => this.restart().catch(() => {}), delay);\\n     }\\n   }\\n \\n@@ -296,36 +307,49 @@ export class SlackSocketRunner {\\n           });\\n         }\\n       } catch {}\\n-      if (path) {\\n-        try {\\n-          const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n-          const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n-          logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n-          await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n-            webhookContext: { webhookData: map, eventType: 'manual' },\\n-            debug: process.env.VISOR_DEBUG === 'true',\\n-          });\\n-          if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n-          return; // resume path handled\\n-        } catch (e) {\\n-          logger.warn(\\n-            `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n-              e instanceof Error ? e.message : String(e)\\n-            }`\\n-          );\\n-        }\\n-      }\\n+      try {\\n+        await withActiveSpan(\\n+          'visor.run',\\n+          {\\n+            'visor.run.source': 'slack',\\n+            'slack.event.type': String(type || ''),\\n+            'slack.channel': channelId,\\n+            'slack.thread_ts': threadTs,\\n+          },\\n+          async () => {\\n+            if (path) {\\n+              try {\\n+                const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n+                const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n+                logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n+                await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n+                  webhookContext: { webhookData: map, eventType: 'manual' },\\n+                  debug: process.env.VISOR_DEBUG === 'true',\\n+                });\\n+                return;\\n+              } catch (e) {\\n+                logger.warn(\\n+                  `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n+                    e instanceof Error ? e.message : String(e)\\n+                  }`\\n+                );\\n+              }\\n+            }\\n \\n-      // Cold run (no snapshot)\\n-      await runEngine.executeChecks({\\n-        checks: allChecks,\\n-        showDetails: true,\\n-        outputFormat: 'json',\\n-        config: cfgForRun,\\n-        webhookContext: { webhookData: map, eventType: 'manual' },\\n-        debug: process.env.VISOR_DEBUG === 'true',\\n-      } as any);\\n-      if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+            // Cold run (no snapshot)\\n+            await runEngine.executeChecks({\\n+              checks: allChecks,\\n+              showDetails: true,\\n+              outputFormat: 'json',\\n+              config: cfgForRun,\\n+              webhookContext: { webhookData: map, eventType: 'manual' },\\n+              debug: process.env.VISOR_DEBUG === 'true',\\n+            } as any);\\n+          }\\n+        );\\n+      } finally {\\n+        if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+      }\\n     } catch (e) {\\n       logger.error(\\n         `[SlackSocket] Engine execution failed: ${e instanceof Error ? e.message : String(e)}`\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/context/build-engine-context.ts\",\"additions\":11,\"deletions\":4,\"changes\":15,\"patch\":\"diff --git a/src/state-machine/context/build-engine-context.ts b/src/state-machine/context/build-engine-context.ts\\nindex 149c73bf..45e63b6a 100644\\n--- a/src/state-machine/context/build-engine-context.ts\\n+++ b/src/state-machine/context/build-engine-context.ts\\n@@ -57,7 +57,8 @@ export function buildEngineContextForRun(\\n       ) as EventTrigger[],\\n       group: checkConfig.group,\\n       providerType: checkConfig.type || 'ai',\\n-      dependencies: checkConfig.depends_on || [],\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: Array.isArray(checkConfig.depends_on) ? checkConfig.depends_on : checkConfig.depends_on ? [checkConfig.depends_on] : [],\\n     };\\n   }\\n \\n@@ -134,11 +135,14 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n   const originalPath = context.workingDirectory || process.cwd();\\n \\n   try {\\n+    // Check if workspace should be kept (for debugging)\\n+    const keepWorkspace = process.env.VISOR_KEEP_WORKSPACE === 'true';\\n+\\n     // Create workspace manager\\n     const workspace = WorkspaceManager.getInstance(context.sessionId, originalPath, {\\n       enabled: true,\\n-      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH,\\n-      cleanupOnExit: workspaceConfig?.cleanup_on_exit !== false,\\n+      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH || '/tmp/visor-workspaces',\\n+      cleanupOnExit: keepWorkspace ? false : workspaceConfig?.cleanup_on_exit !== false,\\n     });\\n \\n     // Initialize workspace (creates main project worktree)\\n@@ -151,6 +155,9 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n \\n     logger.info(`[Workspace] Initialized workspace: ${info.workspacePath}`);\\n     logger.debug(`[Workspace] Main project at: ${info.mainProjectPath}`);\\n+    if (keepWorkspace) {\\n+      logger.info(`[Workspace] Keeping workspace after execution (--keep-workspace)`);\\n+    }\\n \\n     return context;\\n   } catch (error) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/execution-invoker.ts\",\"additions\":84,\"deletions\":4,\"changes\":88,\"patch\":\"diff --git a/src/state-machine/dispatch/execution-invoker.ts b/src/state-machine/dispatch/execution-invoker.ts\\nindex eb7587d3..798ca8e4 100644\\n--- a/src/state-machine/dispatch/execution-invoker.ts\\n+++ b/src/state-machine/dispatch/execution-invoker.ts\\n@@ -356,10 +356,17 @@ export async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -413,6 +420,31 @@ export async function executeSingleCheck(\\n     }\\n   }\\n \\n+  // Banner-style log when a check actually starts executing. This is emitted\\n+  // after all gating (if/depends_on) has passed so it only appears for real\\n+  // runs, and gives a clear visual separator in the logs between checks.\\n+  try {\\n+    const wave = state.wave;\\n+    const level = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${level}) ‚îÅ‚îÅ‚îÅ`;\\n+\\n+    // When running in a TTY, colour the entire banner line for extra\\n+    // visibility; keep plain text for JSON/SARIF or non-TTY environments.\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   let forEachParent: string | undefined;\\n   let forEachItems: unknown[] | undefined;\\n   for (const depId of depList) {\\n@@ -563,13 +595,56 @@ export async function executeSingleCheck(\\n       files: [],\\n       commits: [],\\n     };\\n+    // Derive AI session reuse context for this check (self-mode only for now).\\n+    // When reuse_ai_session: 'self' is configured, look up the last root-scope\\n+    // journal entry for this check in the current engine session and expose its\\n+    // sessionId via ExecutionContext so ai-check-provider can call\\n+    // executeReviewWithSessionReuse() against the same ProbeAgent session.\\n+    let parentSessionId: string | undefined;\\n+    let reuseSession = false;\\n+    try {\\n+      const reuseCfg: unknown = (checkConfig as any).reuse_ai_session;\\n+      if (reuseCfg === 'self') {\\n+        const snapshotId = context.journal.beginSnapshot();\\n+        const visible = context.journal.readVisible(\\n+          context.sessionId,\\n+          snapshotId,\\n+          context.event as any\\n+        );\\n+        // Prefer the most recent root-scope result for this check\\n+        const prior = visible.filter(\\n+          e => e.checkId === checkId && (!e.scope || e.scope.length === 0)\\n+        );\\n+        if (prior.length > 0) {\\n+          const last = prior[prior.length - 1];\\n+          const sess = (last.result as any)?.sessionId;\\n+          if (typeof sess === 'string' && sess.length > 0) {\\n+            parentSessionId = sess;\\n+            reuseSession = true;\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only ‚Äì fall back to normal (non-reuse) execution on error.\\n+      parentSessionId = undefined;\\n+      reuseSession = false;\\n+    }\\n+\\n     const executionContext = {\\n       ...context.executionContext,\\n       _engineMode: context.mode,\\n       _parentContext: context,\\n       _parentState: state,\\n+      // Explicitly propagate workspace reference for nested workflows\\n+      workspace: context.workspace,\\n     };\\n \\n+    // Attach session reuse hints for providers that support them (AI, Claude Code, etc).\\n+    if (reuseSession && parentSessionId) {\\n+      (executionContext as any).parentSessionId = parentSessionId;\\n+      (executionContext as any).reuseSession = true;\\n+    }\\n+\\n     // Handle on_init lifecycle hook BEFORE main execution\\n     if (checkConfig.on_init) {\\n       try {\\n@@ -613,7 +688,12 @@ export async function executeSingleCheck(\\n \\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/foreach-processor.ts\",\"additions\":27,\"deletions\":2,\"changes\":29,\"patch\":\"diff --git a/src/state-machine/dispatch/foreach-processor.ts b/src/state-machine/dispatch/foreach-processor.ts\\nindex 336bf9df..ab630986 100644\\n--- a/src/state-machine/dispatch/foreach-processor.ts\\n+++ b/src/state-machine/dispatch/foreach-processor.ts\\n@@ -77,6 +77,26 @@ export async function executeCheckWithForEachItems(\\n   const allContents: string[] = [];\\n   const perIterationDurations: number[] = [];\\n \\n+  // Emit a banner for the forEach parent so logs clearly show when we are\\n+  // entering the aggregated execution for that step.\\n+  try {\\n+    const wave = state.wave;\\n+    const lvl = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${lvl}, forEach parent) ‚îÅ‚îÅ‚îÅ`;\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   for (let itemIndex = 0; itemIndex < forEachItems.length; itemIndex++) {\\n     const iterationStartMs = Date.now();\\n     const scope: Array<{ check: string; index: number }> = [\\n@@ -248,7 +268,12 @@ export async function executeCheckWithForEachItems(\\n \\n       const result = await withActiveSpan(\\n         `visor.check.${checkId}`,\\n-        { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+        {\\n+          'visor.check.id': checkId,\\n+          'visor.check.type': providerType,\\n+          session_id: context.sessionId,\\n+          wave: state.wave,\\n+        },\\n         async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n       );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/stats-manager.ts\",\"additions\":21,\"deletions\":1,\"changes\":22,\"patch\":\"diff --git a/src/state-machine/dispatch/stats-manager.ts b/src/state-machine/dispatch/stats-manager.ts\\nindex 8d04436e..2f317b59 100644\\n--- a/src/state-machine/dispatch/stats-manager.ts\\n+++ b/src/state-machine/dispatch/stats-manager.ts\\n@@ -44,6 +44,26 @@ export function updateStats(\\n       issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n     };\\n \\n+    // Respect explicit skip markers on the result (set by LevelDispatch).\\n+    const skippedMarker = (result as any).__skipped;\\n+    if (skippedMarker) {\\n+      stats.skipped = true;\\n+      stats.skipReason =\\n+        typeof skippedMarker === 'string'\\n+          ? (skippedMarker as any)\\n+          : (stats.skipReason as any) || 'if_condition';\\n+      // A skipped result should not contribute to run counters; reset\\n+      // them explicitly to guard against any prior values lingering\\n+      // from earlier runs of the same check.\\n+      stats.totalRuns = 0;\\n+      stats.successfulRuns = 0;\\n+      stats.failedRuns = 0;\\n+      stats.skippedRuns++;\\n+      // Do not count this as a run; keep totalRuns/failed/success unchanged.\\n+      state.stats.set(checkId, stats);\\n+      continue;\\n+    }\\n+\\n     // If this check was previously marked as skipped and now executed,\\n     // clear the skipped flag and any skip counters to ensure the run is visible.\\n     if (stats.skipped) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/runner.ts\",\"additions\":10,\"deletions\":7,\"changes\":17,\"patch\":\"diff --git a/src/state-machine/runner.ts b/src/state-machine/runner.ts\\nindex ee86b876..a13a5efd 100644\\n--- a/src/state-machine/runner.ts\\n+++ b/src/state-machine/runner.ts\\n@@ -124,14 +124,17 @@ export class StateMachineRunner {\\n    */\\n   private async executeState(state: EngineState): Promise<void> {\\n     // M4: Wrap state execution in OTEL span\\n+    const attrs: Record<string, unknown> = {\\n+      state: state,\\n+      engine_mode: this.context.mode,\\n+      wave: this.state.wave,\\n+      session_id: this.context.sessionId,\\n+    };\\n+    const waveKind = (this.state as any)?.flags?.waveKind;\\n+    if (waveKind) attrs.wave_kind = waveKind;\\n     return withActiveSpan(\\n       `engine.state.${state.toLowerCase()}`,\\n-      {\\n-        state: state,\\n-        engine_mode: this.context.mode,\\n-        wave: this.state.wave,\\n-        session_id: this.context.sessionId,\\n-      },\\n+      attrs,\\n       async () => {\\n         try {\\n           switch (state) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/level-dispatch.ts\",\"additions\":152,\"deletions\":15,\"changes\":167,\"patch\":\"diff --git a/src/state-machine/states/level-dispatch.ts b/src/state-machine/states/level-dispatch.ts\\nindex 5588b0f5..95392197 100644\\n--- a/src/state-machine/states/level-dispatch.ts\\n+++ b/src/state-machine/states/level-dispatch.ts\\n@@ -26,7 +26,7 @@ import type { CheckExecutionStats } from '../../types/execution';\\n import type { CheckProviderConfig } from '../../providers/check-provider.interface';\\n import type { CheckConfig } from '../../types/config';\\n import { handleRouting, checkLoopBudget } from './routing';\\n-import { withActiveSpan } from '../../telemetry/trace-helpers';\\n+import { withActiveSpan, setSpanAttributes, addEvent } from '../../telemetry/trace-helpers';\\n import { emitMermaidFromMarkdown } from '../../utils/mermaid-telemetry';\\n import { emitNdjsonSpanWithEvents, emitNdjsonFallback } from '../../telemetry/fallback-ndjson';\\n import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n@@ -46,6 +46,32 @@ function mapCheckNameToFocus(checkName: string): string {\\n   return focusMap[checkName] || 'all';\\n }\\n \\n+function formatScopeLabel(scope: Array<{ check: string; index: number }> | undefined): string {\\n+  if (!scope || scope.length === 0) return '';\\n+  return scope.map(item => `${item.check}:${item.index}`).join('|');\\n+}\\n+\\n+function recordOnFinishRoutingEvent(args: {\\n+  checkId: string;\\n+  action: 'run' | 'goto';\\n+  target: string;\\n+  source: 'run' | 'goto' | 'goto_js' | 'transitions';\\n+  scope?: Array<{ check: string; index: number }>;\\n+  gotoEvent?: string;\\n+}): void {\\n+  const attrs: Record<string, unknown> = {\\n+    check_id: args.checkId,\\n+    trigger: 'on_finish',\\n+    action: args.action,\\n+    target: args.target,\\n+    source: args.source,\\n+  };\\n+  const scopeLabel = formatScopeLabel(args.scope);\\n+  if (scopeLabel) attrs.scope = scopeLabel;\\n+  if (args.gotoEvent) attrs.goto_event = args.gotoEvent;\\n+  addEvent('visor.routing', attrs);\\n+}\\n+\\n /**\\n  * Build output history Map from journal for template rendering\\n  * This matches the format expected by AI providers\\n@@ -121,12 +147,13 @@ async function evaluateIfCondition(\\n         return false;\\n       }\\n     })();\\n+    // Steps with dependencies should always see outputs from all completed steps.\\n     // In forward-run waves (from on_success/on_fail goto), guards should see the\\n     // latest global outputs even if the check has no explicit dependencies.\\n     // In wave-retry (from on_finish), restrict to checks with dependencies to\\n     // avoid wrongly skipping top-level prompts like 'ask'.\\n     const useGlobalOutputs =\\n-      (useGlobalOutputsFlag && waveKind === 'forward') || (useGlobalOutputsFlag && hasDeps);\\n+      hasDeps || (useGlobalOutputsFlag && waveKind === 'forward');\\n \\n     if (useGlobalOutputs) {\\n       // Forward-run wave: allow guards to consult latest outputs from the entire\\n@@ -199,6 +226,7 @@ async function evaluateIfCondition(\\n       baseBranch: (context.prInfo as any)?.baseBranch,\\n       filesChanged: context.prInfo?.files?.map(f => f.filename),\\n       environment: envSnapshot,\\n+      workflowInputs: (context.config as any).workflow_inputs || {},\\n     };\\n \\n     const shouldRun = await evaluator.evaluateIfCondition(checkId, ifExpression, contextData);\\n@@ -238,6 +266,11 @@ export async function handleLevelDispatch(\\n   // Update current level tracking\\n   state.currentLevel = level.level;\\n   state.currentLevelChecks = new Set(level.parallel);\\n+  const levelChecksPreview = level.parallel.slice(0, 5).join(',');\\n+  setSpanAttributes({\\n+    level_size: level.parallel.length,\\n+    level_checks_preview: levelChecksPreview,\\n+  });\\n \\n   // Emit level ready event\\n   emitEvent({ type: 'LevelReady', level, wave: state.wave });\\n@@ -689,6 +722,39 @@ async function executeCheckWithForEachItems(\\n         }\\n       } catch {}\\n \\n+      // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+      // The socket-runner stores conversation data in webhookData under the endpoint key\\n+      try {\\n+        const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+        const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+        if (context.debug) {\\n+          logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+        }\\n+        if (webhookData && webhookData.size > 0) {\\n+          // Find the payload with slack_conversation\\n+          for (const payload of webhookData.values()) {\\n+            const slackConv = (payload as any)?.slack_conversation;\\n+            if (slackConv) {\\n+              // Build slack context with event and conversation\\n+              const event = (payload as any)?.event;\\n+              const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+              if (context.debug) {\\n+                logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+              }\\n+              (providerConfig as any).eventContext = {\\n+                ...(providerConfig as any).eventContext,\\n+                slack: {\\n+                  event: event || {},\\n+                  conversation: slackConv,\\n+                },\\n+                conversation: slackConv, // Also expose at top level for convenience\\n+              };\\n+              break;\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n+\\n       // Build dependency results with scope\\n       const dependencyResults = buildDependencyResultsWithScope(\\n         checkId,\\n@@ -820,15 +886,17 @@ async function executeCheckWithForEachItems(\\n       } catch {}\\n \\n       // Execute provider with telemetry\\n-      const itemResult = await withActiveSpan(\\n-        `visor.check.${checkId}`,\\n-        {\\n-          'visor.check.id': checkId,\\n-          'visor.check.type': providerType,\\n-          'visor.foreach.index': itemIndex,\\n-        },\\n-        async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n-      );\\n+    const itemResult = await withActiveSpan(\\n+      `visor.check.${checkId}`,\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        'visor.foreach.index': itemIndex,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n+      async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n+    );\\n \\n       // Enrich issues\\n       const enrichedIssues = (itemResult.issues || []).map((issue: ReviewIssue) => ({\\n@@ -942,6 +1010,7 @@ async function executeCheckWithForEachItems(\\n             const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n               previousResults: dependencyResults as any,\\n               event: context.event || 'manual',\\n+              output: output, // Pass the iteration output for guarantee evaluation\\n             } as any);\\n             if (!holds) {\\n               const issue: ReviewIssue = {\\n@@ -1311,6 +1380,13 @@ async function executeCheckWithForEachItems(\\n             // Increment loop count\\n             state.routingLoopCount++;\\n \\n+            recordOnFinishRoutingEvent({\\n+              checkId: forEachParent,\\n+              action: 'run',\\n+              target: targetCheck,\\n+              source: 'run',\\n+              scope: [],\\n+            });\\n             emitEvent({\\n               type: 'ForwardRunRequested',\\n               target: targetCheck,\\n@@ -1358,6 +1434,14 @@ async function executeCheckWithForEachItems(\\n                 return aggregatedResult; // abort further routing\\n               }\\n               state.routingLoopCount++;\\n+              recordOnFinishRoutingEvent({\\n+                checkId: forEachParent,\\n+                action: 'goto',\\n+                target: transTarget.to,\\n+                source: 'transitions',\\n+                scope: [],\\n+                gotoEvent: (transTarget as any).goto_event,\\n+              });\\n               emitEvent({\\n                 type: 'ForwardRunRequested',\\n                 target: transTarget.to,\\n@@ -1464,6 +1548,13 @@ async function executeCheckWithForEachItems(\\n           // Increment loop count\\n           state.routingLoopCount++;\\n \\n+          recordOnFinishRoutingEvent({\\n+            checkId: forEachParent,\\n+            action: 'goto',\\n+            target: gotoTarget,\\n+            source: onFinish.goto_js ? 'goto_js' : 'goto',\\n+            scope: [],\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: gotoTarget,\\n@@ -1618,10 +1709,17 @@ async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -1923,6 +2021,39 @@ async function executeSingleCheck(\\n       }\\n     } catch {}\\n \\n+    // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+    // The socket-runner stores conversation data in webhookData under the endpoint key\\n+    try {\\n+      const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+      const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+      if (context.debug) {\\n+        logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+      }\\n+      if (webhookData && webhookData.size > 0) {\\n+        // Find the payload with slack_conversation\\n+        for (const payload of webhookData.values()) {\\n+          const slackConv = (payload as any)?.slack_conversation;\\n+          if (slackConv) {\\n+            // Build slack context with event and conversation\\n+            const event = (payload as any)?.event;\\n+            const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+            if (context.debug) {\\n+              logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+            }\\n+            (providerConfig as any).eventContext = {\\n+              ...(providerConfig as any).eventContext,\\n+              slack: {\\n+                event: event || {},\\n+                conversation: slackConv,\\n+              },\\n+              conversation: slackConv, // Also expose at top level for convenience\\n+            };\\n+            break;\\n+          }\\n+        }\\n+      }\\n+    } catch {}\\n+\\n     // Build dependency results\\n     const dependencyResults = buildDependencyResults(checkId, checkConfig, context, state);\\n \\n@@ -2016,7 +2147,12 @@ async function executeSingleCheck(\\n     // Execute provider with telemetry\\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n@@ -2098,6 +2234,7 @@ async function executeSingleCheck(\\n           const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n             previousResults: dependencyResults as any,\\n             event: context.event || 'manual',\\n+            output: enrichedResult.output,\\n           } as any);\\n           if (!holds) {\\n             const issue: ReviewIssue = {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/routing.ts\",\"additions\":165,\"deletions\":1,\"changes\":166,\"patch\":\"diff --git a/src/state-machine/states/routing.ts b/src/state-machine/states/routing.ts\\nindex 5752a5bb..b1fd8654 100644\\n--- a/src/state-machine/states/routing.ts\\n+++ b/src/state-machine/states/routing.ts\\n@@ -15,6 +15,7 @@ import type { EngineContext, RunState, EngineState, EngineEvent } from '../../ty\\n import type { ReviewSummary, ReviewIssue } from '../../reviewer';\\n import type { CheckConfig, OnFailConfig, TransitionRule } from '../../types/config';\\n import { logger } from '../../logger';\\n+import { addEvent } from '../../telemetry/trace-helpers';\\n import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n import { createSecureSandbox, compileAndRun } from '../../utils/sandbox';\\n import { MemoryStore } from '../../memory-store';\\n@@ -153,6 +154,37 @@ function createMemoryHelpers() {\\n   };\\n }\\n \\n+type RoutingTrigger = 'on_success' | 'on_fail' | 'on_finish';\\n+type RoutingAction = 'run' | 'goto' | 'retry';\\n+type RoutingSource = 'run' | 'run_js' | 'goto' | 'goto_js' | 'transitions' | 'retry';\\n+\\n+function formatScopeLabel(scope: Array<{ check: string; index: number }> | undefined): string {\\n+  if (!scope || scope.length === 0) return '';\\n+  return scope.map(item => `${item.check}:${item.index}`).join('|');\\n+}\\n+\\n+function recordRoutingEvent(args: {\\n+  checkId: string;\\n+  trigger: RoutingTrigger;\\n+  action: RoutingAction;\\n+  target?: string;\\n+  source?: RoutingSource;\\n+  scope?: Array<{ check: string; index: number }>;\\n+  gotoEvent?: string;\\n+}): void {\\n+  const attrs: Record<string, unknown> = {\\n+    check_id: args.checkId,\\n+    trigger: args.trigger,\\n+    action: args.action,\\n+  };\\n+  if (args.target) attrs.target = args.target;\\n+  if (args.source) attrs.source = args.source;\\n+  const scopeLabel = formatScopeLabel(args.scope);\\n+  if (scopeLabel) attrs.scope = scopeLabel;\\n+  if (args.gotoEvent) attrs.goto_event = args.gotoEvent;\\n+  addEvent('visor.routing', attrs);\\n+}\\n+\\n /**\\n  * Handle routing state - evaluate conditions and decide next actions\\n  */\\n@@ -271,6 +303,14 @@ async function processOnFinish(\\n             { check: checkId, index: itemIndex },\\n           ];\\n \\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_finish',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -284,6 +324,14 @@ async function processOnFinish(\\n         // Increment loop count\\n         state.routingLoopCount++;\\n \\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_finish',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope: [],\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -328,6 +376,14 @@ async function processOnFinish(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_finish',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -362,6 +418,15 @@ async function processOnFinish(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_finish',\\n+        action: 'goto',\\n+        target: finishTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: finishTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: finishTransTarget.to,\\n@@ -406,6 +471,14 @@ async function processOnFinish(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_finish',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onFinish.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+    });\\n     // Enqueue forward run event\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n@@ -640,6 +713,14 @@ async function processOnSuccess(\\n             { check: checkId, index: itemIndex },\\n           ];\\n \\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_success',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -652,6 +733,14 @@ async function processOnSuccess(\\n         // Increment loop count\\n         state.routingLoopCount++;\\n \\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_success',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope,\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -695,6 +784,14 @@ async function processOnSuccess(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_success',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -728,6 +825,15 @@ async function processOnSuccess(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_success',\\n+        action: 'goto',\\n+        target: successTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: successTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: successTransTarget.to,\\n@@ -773,6 +879,15 @@ async function processOnSuccess(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_success',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onSuccess.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+      gotoEvent: onSuccess.goto_event,\\n+    });\\n     // Enqueue forward run event with optional event override\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n@@ -864,6 +979,14 @@ async function processOnFail(\\n           const itemScope: Array<{ check: string; index: number }> = [\\n             { check: checkId, index: itemIndex },\\n           ];\\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_fail',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -874,6 +997,14 @@ async function processOnFail(\\n       } else {\\n         // No forEach context: preserve current scope (if any)\\n         state.routingLoopCount++;\\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_fail',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope,\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -917,6 +1048,14 @@ async function processOnFail(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_fail',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -958,6 +1097,13 @@ async function processOnFail(\\n \\n         // Increment loop count and schedule forward run for the same check\\n         state.routingLoopCount++;\\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_fail',\\n+          action: 'retry',\\n+          source: 'retry',\\n+          scope: sc || [],\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: checkId,\\n@@ -1013,6 +1159,15 @@ async function processOnFail(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_fail',\\n+        action: 'goto',\\n+        target: failTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: failTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: failTransTarget.to,\\n@@ -1058,6 +1213,15 @@ async function processOnFail(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_fail',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onFail.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+      gotoEvent: onFail.goto_event,\\n+    });\\n     // Enqueue forward run event with optional event override\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/wave-planning.ts\",\"additions\":10,\"deletions\":3,\"changes\":13,\"patch\":\"diff --git a/src/state-machine/states/wave-planning.ts b/src/state-machine/states/wave-planning.ts\\nindex 7551d55c..7b42f043 100644\\n--- a/src/state-machine/states/wave-planning.ts\\n+++ b/src/state-machine/states/wave-planning.ts\\n@@ -351,8 +351,11 @@ export async function handleWavePlanning(\\n     for (const id of checksToRun) {\\n       // Only include dependencies that are within the same subset (often none).\\n       const cfg = context.config.checks?.[id];\\n-      const deps = (cfg?.depends_on || []).filter((d: string) => checksToRun.includes(d));\\n-      subDeps[id] = deps as string[];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = cfg?.depends_on;\\n+      const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n+      const deps = depsArray.filter((d: string) => checksToRun.includes(d));\\n+      subDeps[id] = deps;\\n     }\\n \\n     const subGraph = DependencyResolver.buildDependencyGraph(subDeps);\\n@@ -410,6 +413,10 @@ export async function handleWavePlanning(\\n     // Initialize current wave state\\n     (state as any).currentWaveCompletions = new Set<string>();\\n     (state as any).failedChecks = new Set<string>();\\n+    try {\\n+      (state as any).flags = (state as any).flags || {};\\n+      (state as any).flags.waveKind = 'initial';\\n+    } catch {}\\n   }\\n \\n   // Check if there are levels to execute\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/workflow-projection.ts\",\"additions\":3,\"deletions\":2,\"changes\":5,\"patch\":\"diff --git a/src/state-machine/workflow-projection.ts b/src/state-machine/workflow-projection.ts\\nindex b2d5eb24..2bcfd858 100644\\n--- a/src/state-machine/workflow-projection.ts\\n+++ b/src/state-machine/workflow-projection.ts\\n@@ -53,7 +53,8 @@ export function projectWorkflowToGraph(\\n       triggers: step.on || workflow.on || [],\\n       group: step.group,\\n       providerType: step.type || 'ai',\\n-      dependencies: (step.depends_on || []).map(dep => dep),\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: (Array.isArray(step.depends_on) ? step.depends_on : step.depends_on ? [step.depends_on] : []).map((dep: string) => dep),\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/telemetry/state-capture.ts\",\"additions\":10,\"deletions\":1,\"changes\":11,\"patch\":\"diff --git a/src/telemetry/state-capture.ts b/src/telemetry/state-capture.ts\\nindex f9e77c32..047c3baf 100644\\n--- a/src/telemetry/state-capture.ts\\n+++ b/src/telemetry/state-capture.ts\\n@@ -179,18 +179,27 @@ export function captureProviderCall(\\n ): void {\\n   try {\\n     span.setAttribute('visor.provider.type', providerType);\\n+    const fullCapture =\\n+      process.env.VISOR_TELEMETRY_FULL_CAPTURE === 'true' ||\\n+      process.env.VISOR_TELEMETRY_FULL_CAPTURE === '1';\\n \\n     // Request summary\\n     if (request.model) span.setAttribute('visor.provider.request.model', String(request.model));\\n     if (request.prompt) {\\n       span.setAttribute('visor.provider.request.prompt.length', request.prompt.length);\\n       span.setAttribute('visor.provider.request.prompt.preview', request.prompt.substring(0, 500));\\n+      if (fullCapture) {\\n+        span.setAttribute('visor.provider.request.prompt', safeSerialize(request.prompt));\\n+      }\\n     }\\n \\n     // Response summary\\n     if (response.content) {\\n       span.setAttribute('visor.provider.response.length', response.content.length);\\n       span.setAttribute('visor.provider.response.preview', response.content.substring(0, 500));\\n+      if (fullCapture) {\\n+        span.setAttribute('visor.provider.response.content', safeSerialize(response.content));\\n+      }\\n     }\\n     if (response.tokens) {\\n       span.setAttribute('visor.provider.response.tokens', response.tokens);\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/flow-stage.ts\",\"additions\":37,\"deletions\":4,\"changes\":41,\"patch\":\"diff --git a/src/test-runner/core/flow-stage.ts b/src/test-runner/core/flow-stage.ts\\nindex a14256c2..a587f9f1 100644\\n--- a/src/test-runner/core/flow-stage.ts\\n+++ b/src/test-runner/core/flow-stage.ts\\n@@ -352,10 +352,30 @@ export class FlowStage {\\n       } catch {}\\n       // Prefer authoritative counts from res.statistics when available, then fall back to deltas/inferred\\n       const fromResStats: Record<string, number> = {};\\n+      const resSkipped: Record<string, boolean> = {};\\n       try {\\n         for (const s of (res.statistics?.checks || []) as any[]) {\\n           const n = (s && s.checkName) || '';\\n-          if (typeof n === 'string' && n) fromResStats[n] = (s.totalRuns || 0) as number;\\n+          if (typeof n === 'string' && n) {\\n+            fromResStats[n] = (s.totalRuns || 0) as number;\\n+            resSkipped[n] = !!(s as any).skipped || !!(s as any).skipReason;\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      // Steps that have explicit expectations in this stage. We will use\\n+      // this to avoid inflating run counts (via parent-alignment heuristics)\\n+      // for checks that are not even expected in this stage.\\n+      const expectedSteps = new Set<string>();\\n+      const expectedZero = new Set<string>();\\n+      try {\\n+        const expCalls = ((stage.expect || {}).calls || []) as Array<{ step?: string }>;\\n+        for (const c of expCalls) {\\n+          if (c && typeof c.step === 'string' && c.step) expectedSteps.add(c.step);\\n+          try {\\n+            if (c && typeof c.step === 'string' && c.step && (c as any).exactly === 0)\\n+              expectedZero.add(c.step);\\n+          } catch {}\\n         }\\n       } catch {}\\n \\n@@ -410,10 +430,15 @@ export class FlowStage {\\n           const base = stageOnly ? 0 : statBase[name] || 0;\\n           const d = Math.max(0, resTotal - base);\\n           runs = d > 0 ? d : 0;\\n+          // If the engine marked this check as skipped, force runs to 0\\n+          // so parent-alignment heuristics below do not inflate it.\\n+          if (resSkipped[name]) {\\n+            runs = 0;\\n+          }\\n         } else {\\n           runs = deltaMap[name] !== undefined ? deltaMap[name] : inferred;\\n         }\\n-        if (runs === 0 && presentInResults.has(name)) runs = 1;\\n+        if (runs === 0 && presentInResults.has(name) && !resSkipped[name]) runs = 1;\\n         if (!isForEachLike && histRuns > 0) runs = histRuns;\\n         // Only use per-item history counts for non-forEach checks. For forEach parents,\\n         // use aggregated totals from res.statistics to reflect number of parent executions.\\n@@ -442,7 +467,15 @@ export class FlowStage {\\n             parentMax = Math.max(parentMax, dP);\\n           }\\n           // Apply only for non-forEach checks with no observable history in this stage\\n-          if (!isForEachLike && histRuns === 0 && histPerItemRuns === 0 && parentMax > 0) {\\n+          if (\\n+            !isForEachLike &&\\n+            histRuns === 0 &&\\n+            histPerItemRuns === 0 &&\\n+            parentMax > 0 &&\\n+            !resSkipped[name] &&\\n+            (expectedSteps.size === 0 || expectedSteps.has(name)) &&\\n+            !expectedZero.has(name)\\n+          ) {\\n             runs = Math.max(runs, parentMax);\\n           }\\n         } catch {}\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/test-execution-wrapper.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/test-runner/core/test-execution-wrapper.ts b/src/test-runner/core/test-execution-wrapper.ts\\nindex 254d0970..d0333152 100644\\n--- a/src/test-runner/core/test-execution-wrapper.ts\\n+++ b/src/test-runner/core/test-execution-wrapper.ts\\n@@ -31,6 +31,8 @@ export class TestExecutionWrapper {\\n       const prev: any = (this.engine as any).executionContext || {};\\n       const merged = {\\n         ...prev,\\n+        // Inject workflow inputs for template access via {{ inputs.* }}\\n+        workflowInputs: cfg.workflow_inputs || prev.workflowInputs || {},\\n         mode: {\\n           ...(prev.mode || {}),\\n           test: true,\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/evaluators.ts\",\"additions\":67,\"deletions\":2,\"changes\":69,\"patch\":\"diff --git a/src/test-runner/evaluators.ts b/src/test-runner/evaluators.ts\\nindex 4843163a..75e9c174 100644\\n--- a/src/test-runner/evaluators.ts\\n+++ b/src/test-runner/evaluators.ts\\n@@ -252,6 +252,69 @@ export function evaluatePrompts(\\n   }\\n }\\n \\n+/**\\n+ * Evaluate workflow_output assertions against computed workflow outputs.\\n+ * Similar to evaluateOutputs but tests workflow-level outputs (defined in outputs: section)\\n+ * rather than step outputs.\\n+ */\\n+export function evaluateWorkflowOutputs(\\n+  errors: string[],\\n+  expect: ExpectBlock,\\n+  workflowOutputs: Record<string, unknown> | undefined\\n+): void {\\n+  const expectations = (expect as any).workflow_output;\\n+  if (!Array.isArray(expectations) || expectations.length === 0) return;\\n+  if (!workflowOutputs) {\\n+    errors.push('workflow_output assertions present but no workflow outputs computed');\\n+    return;\\n+  }\\n+\\n+  for (const o of expectations) {\\n+    const path = o.path as string;\\n+    if (!path) {\\n+      errors.push('workflow_output assertion missing path');\\n+      continue;\\n+    }\\n+    const v = deepGet(workflowOutputs, path);\\n+    if (o.equals !== undefined && !deepEqual(v, o.equals)) {\\n+      errors.push(\\n+        `Workflow output ${path} expected ${JSON.stringify(o.equals)} but got ${JSON.stringify(v)}`\\n+      );\\n+    }\\n+    if (o.equalsDeep !== undefined && !deepEqual(v, o.equalsDeep)) {\\n+      errors.push(`Workflow output ${path} deepEquals failed`);\\n+    }\\n+    if (o.matches && !parseRegex(o.matches).test(String(v))) {\\n+      errors.push(`Workflow output ${path} does not match ${o.matches}`);\\n+    }\\n+    if (o.contains) {\\n+      const contents = Array.isArray(o.contains) ? o.contains : [o.contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (!strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} expected to contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.not_contains) {\\n+      const contents = Array.isArray(o.not_contains) ? o.not_contains : [o.not_contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} should not contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.contains_unordered) {\\n+      if (!Array.isArray(v)) {\\n+        errors.push(`Workflow output ${path} not an array for contains_unordered`);\\n+      } else if (!containsUnordered(v as unknown[], o.contains_unordered)) {\\n+        errors.push(`Workflow output ${path} missing elements (unordered)`);\\n+      }\\n+    }\\n+  }\\n+}\\n+\\n export function evaluateOutputs(\\n   errors: string[],\\n   expect: ExpectBlock,\\n@@ -332,7 +395,8 @@ export function evaluateCase(\\n   strict: boolean,\\n   promptsByStep: Record<string, string[]>,\\n   _results: GroupedResults,\\n-  outputHistory: Record<string, unknown[]>\\n+  outputHistory: Record<string, unknown[]>,\\n+  workflowOutputs?: Record<string, unknown>\\n ): string[] {\\n   const errors: string[] = [];\\n   const executed = buildExecutedMap(stats);\\n@@ -350,5 +414,6 @@ export function evaluateCase(\\n   evaluateNoCalls(errors, expect, executed, recorder, slackRecorder);\\n   evaluatePrompts(errors, expect, promptsByStep);\\n   evaluateOutputs(errors, expect, outputHistory);\\n+  evaluateWorkflowOutputs(errors, expect, workflowOutputs);\\n   return errors;\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/index.ts\",\"additions\":112,\"deletions\":7,\"changes\":119,\"patch\":\"diff --git a/src/test-runner/index.ts b/src/test-runner/index.ts\\nindex 29916a16..4dcc3331 100644\\n--- a/src/test-runner/index.ts\\n+++ b/src/test-runner/index.ts\\n@@ -159,6 +159,7 @@ export async function runSuites(\\n   options: {\\n     only?: string;\\n     bail?: boolean;\\n+    noMocks?: boolean;\\n     maxParallelSuites?: number;\\n     maxParallel?: number;\\n     promptMaxChars?: number;\\n@@ -225,6 +226,7 @@ export async function runSuites(\\n       const r = await runner.runCases(fp, suite as TestSuite, {\\n         only: options.only,\\n         bail: options.bail,\\n+        noMocks: options.noMocks,\\n         maxParallel: options.maxParallel,\\n         promptMaxChars: options.promptMaxChars,\\n       });\\n@@ -285,7 +287,8 @@ export class VisorTestRunner {\\n     defaultPromptCap?: number,\\n     ghRec?: { error_code?: number; timeout_ms?: number },\\n     defaultIncludeTags?: string[] | undefined,\\n-    defaultExcludeTags?: string[] | undefined\\n+    defaultExcludeTags?: string[] | undefined,\\n+    noMocks?: boolean\\n   ): {\\n     name: string;\\n     strict: boolean;\\n@@ -399,9 +402,11 @@ export class VisorTestRunner {\\n               : info.prompt;\\n           prompts[k].push(p);\\n         },\\n-        mockForStep: (step: string) => mockMgr.get(step),\\n+        // In noMocks mode, always return undefined to let real providers execute\\n+        mockForStep: (step: string) => noMocks ? undefined : mockMgr.get(step),\\n         // Ensure human-input never blocks tests: prefer case mock, then default value\\n         onHumanInput: async (req: { checkId: string; default?: string }) => {\\n+          if (noMocks) return (req.default ?? '').toString();\\n           const m = mockMgr.get(req.checkId);\\n           if (m !== undefined && m !== null) return String(m);\\n           return (req.default ?? '').toString();\\n@@ -481,6 +486,60 @@ export class VisorTestRunner {\\n     if (!checks || checks.length === 0) return;\\n     console.log(`  checks: ${checks.join(', ')}`);\\n   }\\n+  /**\\n+   * Compute workflow outputs from output definitions and step results.\\n+   * Evaluates value_js and value (Liquid) expressions to produce output values.\\n+   */\\n+  private computeWorkflowOutputs(\\n+    outputDefs: Array<{ name: string; value?: string; value_js?: string }>,\\n+    outputHistory: Record<string, unknown[]>,\\n+    results: any[]\\n+  ): Record<string, unknown> {\\n+    const computed: Record<string, unknown> = {};\\n+    // Build a steps map from outputHistory (last output for each step)\\n+    const steps: Record<string, unknown> = {};\\n+    for (const [stepId, hist] of Object.entries(outputHistory)) {\\n+      if (Array.isArray(hist) && hist.length > 0) {\\n+        steps[stepId] = hist[hist.length - 1];\\n+      }\\n+    }\\n+    // Build outputs map from history (for {{ outputs[\\\"step\\\"] }} syntax)\\n+    const outputs: Record<string, unknown> = { ...steps };\\n+\\n+    for (const outputDef of outputDefs) {\\n+      if (!outputDef.name) continue;\\n+      try {\\n+        if (outputDef.value_js) {\\n+          // Evaluate JavaScript expression\\n+          const fn = new Function('steps', 'outputs', 'results', `\\n+            try {\\n+              ${outputDef.value_js}\\n+            } catch (e) {\\n+              return undefined;\\n+            }\\n+          `);\\n+          computed[outputDef.name] = fn(steps, outputs, results);\\n+        } else if (outputDef.value) {\\n+          // Evaluate Liquid template\\n+          const { Liquid } = require('liquidjs');\\n+          const engine = new Liquid();\\n+          const rendered = engine.parseAndRenderSync(outputDef.value, { steps, outputs, results });\\n+          // Try to parse as JSON, otherwise keep as string\\n+          try {\\n+            computed[outputDef.name] = JSON.parse(rendered);\\n+          } catch {\\n+            computed[outputDef.name] = rendered.trim();\\n+          }\\n+        }\\n+      } catch (e) {\\n+        // Skip outputs that fail to compute\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.log(`  ‚ö†Ô∏è Failed to compute output '${outputDef.name}': ${e}`);\\n+        }\\n+      }\\n+    }\\n+    return computed;\\n+  }\\n   /**\\n    * Locate a tests file: explicit path > ./.visor.tests.yaml > defaults/visor.tests.yaml\\n    */\\n@@ -606,7 +665,7 @@ export class VisorTestRunner {\\n   public async runCases(\\n     testsPath: string,\\n     suite: TestSuite,\\n-    options: { only?: string; bail?: boolean; maxParallel?: number; promptMaxChars?: number }\\n+    options: { only?: string; bail?: boolean; noMocks?: boolean; maxParallel?: number; promptMaxChars?: number }\\n   ): Promise<{\\n     failures: number;\\n     results: Array<{\\n@@ -748,9 +807,14 @@ export class VisorTestRunner {\\n     const __keepAlive = setInterval(() => {}, 1000);\\n     // Header: show suite path for clarity\\n     let __suiteRel = testsPath;\\n+    const noMocksMode = options.noMocks || false;\\n     try {\\n       __suiteRel = path.relative(this.cwd, testsPath) || testsPath;\\n       console.log(`Suite: ${__suiteRel}`);\\n+      if (noMocksMode) {\\n+        console.log(this.color('üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)', '33'));\\n+        console.log(this.gray('   Step outputs will be captured and printed as suggested mocks\\\\n'));\\n+      }\\n     } catch {}\\n     const runOne = async (_case: any): Promise<{ name: string; failed: number }> => {\\n       // Case header for clarity\\n@@ -805,6 +869,11 @@ export class VisorTestRunner {\\n           cfgLocal.checks[name] = chk;\\n         }\\n       }\\n+      // Workflow testing: inject workflow_input into config for template access\\n+      const workflowInput = (_case as any).workflow_input;\\n+      if (workflowInput && typeof workflowInput === 'object') {\\n+        (cfgLocal as any).workflow_inputs = workflowInput;\\n+      }\\n       const setup = this.setupTestCase(\\n         _case,\\n         cfgLocal,\\n@@ -812,7 +881,8 @@ export class VisorTestRunner {\\n         defaultPromptCap,\\n         ghRec,\\n         defaultIncludeTags,\\n-        defaultExcludeTags\\n+        defaultExcludeTags,\\n+        noMocksMode\\n       );\\n       try {\\n         this.printSelectedChecks(setup.checksToRun);\\n@@ -826,6 +896,24 @@ export class VisorTestRunner {\\n         } catch {}\\n         const exec = await this.executeTestCase(setup, cfgLocal);\\n         const res = exec.res;\\n+\\n+        // In no-mocks mode, print captured outputs as suggested mocks\\n+        if (noMocksMode && Object.keys(exec.outHistory).length > 0) {\\n+          console.log(this.color('\\\\nüìã Suggested mocks (copy to your test case):', '36'));\\n+          console.log(this.gray('mocks:'));\\n+          for (const [stepName, outputs] of Object.entries(exec.outHistory)) {\\n+            if (!Array.isArray(outputs) || outputs.length === 0) continue;\\n+            // Use the last output for the step\\n+            const lastOutput = outputs[outputs.length - 1];\\n+            // Format as YAML with proper indentation\\n+            const yamlOutput = yaml.dump({ [stepName]: lastOutput }, { indent: 2, lineWidth: 120, noRefs: true });\\n+            // Indent the YAML output for proper nesting under 'mocks:'\\n+            const indented = yamlOutput.split('\\\\n').map(line => '  ' + line).join('\\\\n');\\n+            console.log(indented);\\n+          }\\n+          console.log('');\\n+        }\\n+\\n         if (process.env.VISOR_DEBUG === 'true') {\\n           try {\\n             const names = (res.statistics.checks || []).map(\\n@@ -836,6 +924,21 @@ export class VisorTestRunner {\\n         }\\n         // avoid printing raw history keys each case\\n         // (fallback for on_finish static targets handled inside executeTestCase)\\n+        // Workflow testing: compute workflow outputs if workflow has outputs defined\\n+        let workflowOutputs: Record<string, unknown> | undefined;\\n+        if (Array.isArray((cfgLocal as any).outputs)) {\\n+          try {\\n+            workflowOutputs = this.computeWorkflowOutputs(\\n+              (cfgLocal as any).outputs,\\n+              exec.outHistory,\\n+              res.results\\n+            );\\n+          } catch (e) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ‚ö†Ô∏è Error computing workflow outputs: ${e}`);\\n+            }\\n+          }\\n+        }\\n         const caseFailures = require('./evaluators').evaluateCase(\\n           _case.name,\\n           res.statistics,\\n@@ -845,7 +948,8 @@ export class VisorTestRunner {\\n           setup.strict,\\n           setup.prompts,\\n           res.results,\\n-          exec.outHistory\\n+          exec.outHistory,\\n+          workflowOutputs\\n         );\\n         // Warn about unmocked AI/command steps that executed\\n         try {\\n@@ -1280,7 +1384,8 @@ export class VisorTestRunner {\\n   ): void {\\n     const executed: Record<string, number> = {};\\n     for (const s of stats.checks) {\\n-      if (!s.skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n+      const skipped = (s as any).skipped === true || !!(s as any).skipReason;\\n+      if (!skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n     }\\n     const expCalls = (expect.calls || []).filter(c => c.step);\\n     const expectedSteps = new Map<\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/validator.ts\",\"additions\":48,\"deletions\":2,\"changes\":50,\"patch\":\"diff --git a/src/test-runner/validator.ts b/src/test-runner/validator.ts\\nindex dbb890c5..5ffff5ee 100644\\n--- a/src/test-runner/validator.ts\\n+++ b/src/test-runner/validator.ts\\n@@ -25,6 +25,16 @@ const schema: any = {\\n     hooks: { type: 'object' },\\n     slack: { type: 'object' },\\n     frontends: { type: 'array' },\\n+    workspace: { type: 'object' },\\n+    // Workflow definition fields (for workflow files with co-located tests)\\n+    id: { type: 'string' },\\n+    name: { type: 'string' },\\n+    description: { type: 'string' },\\n+    inputs: { type: 'array' },\\n+    outputs: { type: 'array' },\\n+    ai_mcp_servers: { type: 'object' },\\n+    ai_provider: { type: 'string' },\\n+    ai_model: { type: 'string' },\\n     tests: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -136,6 +146,8 @@ const schema: any = {\\n             oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n           },\\n         },\\n+        // Workflow testing: input values to pass to the workflow\\n+        workflow_input: { type: 'object' },\\n         expect: { $ref: '#/$defs/expectBlock' },\\n         // Flow cases\\n         flow: {\\n@@ -270,6 +282,35 @@ const schema: any = {\\n       },\\n       required: ['step', 'path'],\\n     },\\n+    // Workflow output expectation: like outputsExpectation but without step (tests workflow-level outputs)\\n+    workflowOutputExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        path: { type: 'string' },\\n+        equals: {},\\n+        equalsDeep: {},\\n+        matches: { type: 'string' },\\n+        contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        not_contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            path: { type: 'string' },\\n+            equals: {},\\n+            matches: { type: 'string' },\\n+          },\\n+          required: ['path'],\\n+        },\\n+        contains_unordered: { type: 'array' },\\n+      },\\n+      required: ['path'],\\n+    },\\n     expectBlock: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -278,6 +319,8 @@ const schema: any = {\\n         calls: { type: 'array', items: { $ref: '#/$defs/callsExpectation' } },\\n         prompts: { type: 'array', items: { $ref: '#/$defs/promptsExpectation' } },\\n         outputs: { type: 'array', items: { $ref: '#/$defs/outputsExpectation' } },\\n+        // Workflow testing: assert on workflow-level outputs (defined in workflow's outputs: section)\\n+        workflow_output: { type: 'array', items: { $ref: '#/$defs/workflowOutputExpectation' } },\\n         no_calls: {\\n           type: 'array',\\n           items: {\\n@@ -343,6 +386,7 @@ const knownKeys = new Set([\\n   'version',\\n   'extends',\\n   'tests',\\n+  'workspace',\\n   // tests\\n   'tests.defaults',\\n   'tests.fixtures',\\n@@ -363,6 +407,7 @@ const knownKeys = new Set([\\n   'env',\\n   'routing',\\n   'mocks',\\n+  'workflow_input',\\n   'expect',\\n   'flow',\\n   // expect\\n@@ -370,6 +415,7 @@ const knownKeys = new Set([\\n   'expect.calls',\\n   'expect.prompts',\\n   'expect.outputs',\\n+  'expect.workflow_output',\\n   'expect.no_calls',\\n   'expect.fail',\\n   'expect.strict_violation',\\n@@ -424,7 +470,7 @@ function formatError(e: ErrorObject): string {\\n       if (hint) msg += ` (${hint})`;\\n       // Small curated allow-list for frequent nodes to reduce guesswork\\n       if (path.endsWith('expect')) {\\n-        msg += ` (allowed: use, calls, prompts, outputs, no_calls, fail, strict_violation)`;\\n+        msg += ` (allowed: use, calls, prompts, outputs, workflow_output, no_calls, fail, strict_violation)`;\\n       } else if (path.endsWith('env')) {\\n         msg += ` (values must be strings)`;\\n       } else if (path.endsWith('tests')) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/cli.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/cli.ts b/src/types/cli.ts\\nindex a7d9ca30..f06f93d5 100644\\n--- a/src/types/cli.ts\\n+++ b/src/types/cli.ts\\n@@ -61,6 +61,8 @@ export interface CliOptions {\\n   githubV2?: boolean;\\n   /** Enable Slack Socket Mode runner */\\n   slack?: boolean;\\n+  /** Keep workspace folders after execution (for debugging) */\\n+  keepWorkspace?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/config.ts\",\"additions\":114,\"deletions\":5,\"changes\":119,\"patch\":\"diff --git a/src/types/config.ts b/src/types/config.ts\\nindex ed239959..dd363de1 100644\\n--- a/src/types/config.ts\\n+++ b/src/types/config.ts\\n@@ -122,6 +122,9 @@ export interface FailureConditionContext {\\n   /** Environment variables */\\n   env?: Record<string, string>;\\n \\n+  /** Workflow inputs - values passed to the workflow */\\n+  inputs?: Record<string, unknown>;\\n+\\n   /** Memory accessor for accessing memory store in expressions */\\n   memory?: {\\n     get: (key: string, namespace?: string) => unknown;\\n@@ -356,6 +359,8 @@ export interface AIProviderConfig {\\n   allowBash?: boolean;\\n   /** Advanced bash command execution configuration */\\n   bashConfig?: BashConfig;\\n+  /** Completion prompt for post-completion validation/review (runs after attempt_completion) */\\n+  completion_prompt?: string;\\n }\\n \\n /**\\n@@ -461,8 +466,8 @@ export interface CheckConfig {\\n   env?: EnvConfig;\\n   /** Timeout in seconds for command execution (default: 60) */\\n   timeout?: number;\\n-  /** Check IDs that this check depends on (optional) */\\n-  depends_on?: string[];\\n+  /** Check IDs that this check depends on (optional). Accepts single string or array. */\\n+  depends_on?: string | string[];\\n   /** Group name for comment separation (e.g., \\\"code-review\\\", \\\"pr-overview\\\") - optional */\\n   group?: string;\\n   /** Schema type for template rendering (e.g., \\\"code-review\\\", \\\"markdown\\\") or inline JSON schema object - optional */\\n@@ -621,6 +626,46 @@ export interface CheckConfig {\\n   overrides?: Record<string, Partial<CheckConfig>>;\\n   /** Map workflow outputs to check outputs */\\n   output_mapping?: Record<string, string>;\\n+\\n+  // Workflow aliases for backward compatibility\\n+  /** Alias for args - workflow inputs (backward compatibility) */\\n+  workflow_inputs?: Record<string, unknown>;\\n+  /** Config file path - alternative to workflow ID (loads a Visor config file as workflow) */\\n+  config?: string;\\n+  /** Alias for overrides - workflow step overrides (backward compatibility) */\\n+  workflow_overrides?: Record<string, Partial<CheckConfig>>;\\n+\\n+  /**\\n+   * Git-checkout provider specific options (optional, only used when type === 'git-checkout').\\n+   */\\n+  /** Git reference to checkout (branch, tag, commit SHA) - supports templates */\\n+  ref?: string;\\n+  /** Repository URL or owner/repo format (defaults to current repository) */\\n+  repository?: string;\\n+  /** GitHub token for private repositories (defaults to GITHUB_TOKEN env) */\\n+  token?: string;\\n+  /** Number of commits to fetch (0 for full history, default: 1) */\\n+  fetch_depth?: number;\\n+  /** Whether to fetch tags (default: false) */\\n+  fetch_tags?: boolean;\\n+  /** Checkout submodules: false, true, or 'recursive' */\\n+  submodules?: boolean | 'recursive';\\n+  /** Working directory for the checkout (defaults to temp directory) */\\n+  working_directory?: string;\\n+  /** Use git worktree for efficient parallel checkouts (default: true) */\\n+  use_worktree?: boolean;\\n+  /** Clean the working directory before checkout (default: true) */\\n+  clean?: boolean;\\n+  /** Sparse checkout paths - only checkout specific directories/files */\\n+  sparse_checkout?: string[];\\n+  /** Enable Git LFS (Large File Storage) */\\n+  lfs?: boolean;\\n+  /** Timeout in ms for cloning the bare repository (default: 300000 = 5 min) */\\n+  clone_timeout_ms?: number;\\n+  /** Clean up worktree on failure (default: true) */\\n+  cleanup_on_failure?: boolean;\\n+  /** Keep worktree after workflow completion (default: false) */\\n+  persist_worktree?: boolean;\\n }\\n \\n /**\\n@@ -1037,6 +1082,36 @@ export interface CustomToolDefinition {\\n   outputSchema?: Record<string, unknown>;\\n }\\n \\n+/**\\n+ * Workflow input definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowInput {\\n+  /** Input parameter name */\\n+  name: string;\\n+  /** JSON Schema for the input */\\n+  schema?: Record<string, unknown>;\\n+  /** Whether this input is required */\\n+  required?: boolean;\\n+  /** Default value if not provided */\\n+  default?: unknown;\\n+  /** Human-readable description */\\n+  description?: string;\\n+}\\n+\\n+/**\\n+ * Workflow output definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowOutput {\\n+  /** Output name */\\n+  name: string;\\n+  /** Human-readable description */\\n+  description?: string;\\n+  /** Value using Liquid template syntax (references step outputs) */\\n+  value?: string;\\n+  /** Value using JavaScript expression (alternative to value) */\\n+  value_js?: string;\\n+}\\n+\\n /**\\n  * Main Visor configuration\\n  */\\n@@ -1051,12 +1126,16 @@ export interface VisorConfig {\\n   tools?: Record<string, CustomToolDefinition>;\\n   /** Import workflow definitions from external files or URLs */\\n   imports?: string[];\\n+  /** Workflow inputs (for standalone reusable workflows) */\\n+  inputs?: WorkflowInput[];\\n+  /** Workflow outputs (for standalone reusable workflows) */\\n+  outputs?: WorkflowOutput[];\\n   /** Step configurations (recommended) */\\n   steps?: Record<string, CheckConfig>;\\n   /** Check configurations (legacy, use 'steps' instead) - always populated after normalization */\\n   checks?: Record<string, CheckConfig>;\\n-  /** Output configuration */\\n-  output: OutputConfig;\\n+  /** Output configuration (optional - defaults provided) */\\n+  output?: OutputConfig;\\n   /** HTTP server configuration for receiving webhooks */\\n   http_server?: HttpServerConfig;\\n   /** Memory storage configuration */\\n@@ -1092,6 +1171,36 @@ export interface VisorConfig {\\n     /** Frontend-specific configuration */\\n     config?: unknown;\\n   }>;\\n+  /** Workspace isolation configuration for sandboxed execution */\\n+  workspace?: WorkspaceConfig;\\n+  /** Slack configuration */\\n+  slack?: SlackConfig;\\n+}\\n+\\n+/**\\n+ * Workspace isolation configuration\\n+ */\\n+export interface WorkspaceConfig {\\n+  /** Enable workspace isolation (default: true when config present) */\\n+  enabled?: boolean;\\n+  /** Base path for workspaces (default: /tmp/visor-workspaces) */\\n+  base_path?: string;\\n+  /** Clean up workspace on exit (default: true) */\\n+  cleanup_on_exit?: boolean;\\n+}\\n+\\n+/**\\n+ * Slack configuration\\n+ */\\n+export interface SlackConfig {\\n+  /** Slack API version */\\n+  version?: string;\\n+  /** Mention handling: 'all', 'direct', etc. */\\n+  mentions?: string;\\n+  /** Thread handling: 'required', 'optional', etc. */\\n+  threads?: string;\\n+  /** Show raw output in Slack responses */\\n+  show_raw_output?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/git-checkout.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/git-checkout.ts b/src/types/git-checkout.ts\\nindex eb1c5ebc..4d27855d 100644\\n--- a/src/types/git-checkout.ts\\n+++ b/src/types/git-checkout.ts\\n@@ -16,6 +16,8 @@ export interface GitCheckoutConfig {\\n   fetch_depth?: number;\\n   fetch_tags?: boolean;\\n   submodules?: boolean | 'recursive';\\n+  /** Timeout (ms) for cloning the bare repository; defaults to 300000 (5 minutes) */\\n+  clone_timeout_ms?: number;\\n \\n   // Worktree configuration\\n   working_directory?: string;\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/sandbox.ts\",\"additions\":46,\"deletions\":1,\"changes\":47,\"patch\":\"diff --git a/src/utils/sandbox.ts b/src/utils/sandbox.ts\\nindex 9914a3a5..12e27c30 100644\\n--- a/src/utils/sandbox.ts\\n+++ b/src/utils/sandbox.ts\\n@@ -14,6 +14,51 @@ export interface CompileOptions {\\n   wrapFunction?: boolean;\\n }\\n \\n+export interface JsSyntaxValidationResult {\\n+  valid: boolean;\\n+  error?: string;\\n+}\\n+\\n+/**\\n+ * Validate JavaScript syntax without executing it.\\n+ * Uses the sandbox's compile method to check for syntax errors.\\n+ * Returns validation result with error message if invalid.\\n+ */\\n+export function validateJsSyntax(code: string): JsSyntaxValidationResult {\\n+  if (!code || typeof code !== 'string') {\\n+    return { valid: false, error: 'Code must be a non-empty string' };\\n+  }\\n+\\n+  const trimmed = code.trim();\\n+  if (trimmed.length === 0) {\\n+    return { valid: false, error: 'Code cannot be empty' };\\n+  }\\n+\\n+  // Create a minimal sandbox instance for syntax checking\\n+  const sandbox = createSecureSandbox();\\n+\\n+  // Wrap code similar to compileAndRun to catch the same syntax issues\\n+  const looksLikeBlock = /\\\\breturn\\\\b/.test(trimmed) || /;/.test(trimmed) || /\\\\n/.test(trimmed);\\n+  const looksLikeIife = /\\\\)\\\\s*\\\\(\\\\s*\\\\)\\\\s*;?$/.test(trimmed);\\n+  const body = looksLikeBlock\\n+    ? looksLikeIife\\n+      ? `return (\\\\n${trimmed}\\\\n);\\\\n`\\n+      : `return (() => {\\\\n${trimmed}\\\\n})();\\\\n`\\n+    : `return (\\\\n${trimmed}\\\\n);\\\\n`;\\n+\\n+  // Add log injection header (same as compileAndRun)\\n+  const header = `const __lp = \\\"[syntax-check]\\\"; const log = (...a) => { try { console.log(__lp, ...a); } catch {} };\\\\n`;\\n+  const fullCode = `${header}${body}`;\\n+\\n+  try {\\n+    sandbox.compile(fullCode);\\n+    return { valid: true };\\n+  } catch (e) {\\n+    const msg = e instanceof Error ? e.message : String(e);\\n+    return { valid: false, error: msg };\\n+  }\\n+}\\n+\\n /**\\n  * Create a hardened Sandbox with a consistent set of globals and prototype\\n  * whitelists. This is a superset of the sets previously used by individual\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/tracer-init.ts\",\"additions\":19,\"deletions\":1,\"changes\":20,\"patch\":\"diff --git a/src/utils/tracer-init.ts b/src/utils/tracer-init.ts\\nindex b42995d9..4edf1ffc 100644\\n--- a/src/utils/tracer-init.ts\\n+++ b/src/utils/tracer-init.ts\\n@@ -76,6 +76,24 @@ export async function initializeTracer(\\n \\n       const tracer = new SimpleAppTracer(telemetry, sessionId);\\n \\n+      // WORKAROUND: Add missing recordEvent method for completionPrompt feature (probe #321)\\n+      // SimpleAppTracer doesn't have recordEvent but completionPrompt requires it\\n+      if (typeof (tracer as any).recordEvent !== 'function') {\\n+        (tracer as any).recordEvent = (\\n+          name: string,\\n+          attributes?: Record<string, unknown>\\n+        ) => {\\n+          // Log completion events to telemetry for debugging\\n+          try {\\n+            if ((telemetry as any).record) {\\n+              (telemetry as any).record({ event: name, ...attributes });\\n+            }\\n+          } catch {\\n+            // Best-effort only\\n+          }\\n+        };\\n+      }\\n+\\n       console.error(`üìä Simple tracing enabled, will save to: ${traceFilePath}`);\\n \\n       // If in GitHub Actions, log the path for artifact upload\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/worktree-manager.ts\",\"additions\":142,\"deletions\":27,\"changes\":169,\"patch\":\"diff --git a/src/utils/worktree-manager.ts b/src/utils/worktree-manager.ts\\nindex f3cebf3d..2de7a9f4 100644\\n--- a/src/utils/worktree-manager.ts\\n+++ b/src/utils/worktree-manager.ts\\n@@ -6,6 +6,7 @@\\n  */\\n \\n import * as fs from 'fs';\\n+import * as fsp from 'fs/promises';\\n import * as path from 'path';\\n import * as crypto from 'crypto';\\n import { commandExecutor } from './command-executor';\\n@@ -119,7 +120,8 @@ export class WorktreeManager {\\n     repository: string,\\n     repoUrl: string,\\n     token?: string,\\n-    fetchDepth?: number\\n+    fetchDepth?: number,\\n+    cloneTimeoutMs?: number\\n   ): Promise<string> {\\n     const reposDir = this.getReposDir();\\n     const repoName = repository.replace(/\\\\//g, '-');\\n@@ -129,9 +131,21 @@ export class WorktreeManager {\\n     if (fs.existsSync(bareRepoPath)) {\\n       logger.debug(`Bare repository already exists: ${bareRepoPath}`);\\n \\n-      // Update remote refs\\n-      await this.updateBareRepo(bareRepoPath);\\n-      return bareRepoPath;\\n+      // Verify the bare repo has the correct remote URL to prevent using corrupted repos\\n+      const verifyResult = await this.verifyBareRepoRemote(bareRepoPath, repoUrl);\\n+      if (verifyResult === 'timeout') {\\n+        // Timeout during verification - use stale cache to avoid hanging on re-clone\\n+        logger.info(`Using stale bare repository (verification timed out): ${bareRepoPath}`);\\n+        return bareRepoPath;\\n+      } else if (verifyResult === false) {\\n+        logger.warn(`Bare repository at ${bareRepoPath} has incorrect remote, removing and re-cloning`);\\n+        await fsp.rm(bareRepoPath, { recursive: true, force: true });\\n+        // Fall through to clone below\\n+      } else {\\n+        // Update remote refs\\n+        await this.updateBareRepo(bareRepoPath);\\n+        return bareRepoPath;\\n+      }\\n     }\\n \\n     // Clone as bare repository\\n@@ -153,7 +167,9 @@ export class WorktreeManager {\\n     }\\n     cloneCmd += ` ${this.escapeShellArg(cloneUrl)} ${this.escapeShellArg(bareRepoPath)}`;\\n \\n-    const result = await this.executeGitCommand(cloneCmd, { timeout: 300000 }); // 5 minute timeout\\n+    const result = await this.executeGitCommand(cloneCmd, {\\n+      timeout: cloneTimeoutMs || 300000, // default 5 minutes\\n+    });\\n \\n     if (result.exitCode !== 0) {\\n       // Redact tokens from error messages\\n@@ -171,19 +187,98 @@ export class WorktreeManager {\\n   private async updateBareRepo(bareRepoPath: string): Promise<void> {\\n     logger.debug(`Updating bare repository: ${bareRepoPath}`);\\n \\n-    const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n-    const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+    try {\\n+      const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n+      const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n+        // Don't throw - we can continue with stale refs\\n+      } else {\\n+        logger.debug(`Successfully updated bare repository`);\\n+      }\\n+    } catch (error) {\\n+      // Handle timeout or other errors gracefully - we can continue with stale refs\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      logger.warn(`Failed to update bare repository (will use stale refs): ${errorMessage}`);\\n+    }\\n+  }\\n \\n-    if (result.exitCode !== 0) {\\n-      logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n-      // Don't throw - we can continue with stale refs\\n-    } else {\\n-      logger.debug(`Successfully updated bare repository`);\\n+  /**\\n+   * Verify that a bare repository has the correct remote URL.\\n+   * This prevents reusing corrupted repos that were cloned from a different repository.\\n+   * Returns: true (valid), false (invalid - should re-clone), or 'timeout' (use stale cache)\\n+   */\\n+  private async verifyBareRepoRemote(\\n+    bareRepoPath: string,\\n+    expectedUrl: string\\n+  ): Promise<boolean | 'timeout'> {\\n+    try {\\n+      const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote get-url origin`;\\n+      const result = await this.executeGitCommand(cmd, { timeout: 10000 });\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to get remote URL for ${bareRepoPath}: ${result.stderr}`);\\n+        return false;\\n+      }\\n+\\n+      const actualUrl = result.stdout.trim();\\n+\\n+      // Normalize URLs for comparison:\\n+      // - remove credentials (tokens/username)\\n+      // - remove .git suffix if present\\n+      // - trim trailing slash\\n+      // - lowercase for case‚Äëinsensitive match\\n+      const normalizeUrl = (url: string): string => {\\n+        // Convert common SSH form to https for comparison\\n+        if (url.startsWith('git@github.com:')) {\\n+          url = url.replace('git@github.com:', 'https://github.com/');\\n+        }\\n+        return url\\n+          // strip userinfo part to avoid mismatches when the cached bare\\n+          // repo was cloned with an access token but the expected URL is\\n+          // tokenless (or vice‚Äëversa)\\n+          .replace(/:\\\\/\\\\/[^@]+@/, '://')\\n+          .replace(/\\\\.git$/, '')\\n+          .replace(/\\\\/$/, '')\\n+          .toLowerCase();\\n+      };\\n+\\n+      const normalizedExpected = normalizeUrl(expectedUrl);\\n+      const normalizedActual = normalizeUrl(actualUrl);\\n+\\n+      if (normalizedExpected !== normalizedActual) {\\n+        logger.warn(\\n+          `Bare repo remote mismatch: expected ${expectedUrl}, got ${actualUrl}`\\n+        );\\n+        return false;\\n+      }\\n+\\n+      logger.debug(`Bare repo remote verified: ${actualUrl}`);\\n+      return true;\\n+    } catch (error) {\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      // If it's a timeout, return 'timeout' so caller can use stale cache instead of re-cloning\\n+      if (errorMessage.includes('timed out')) {\\n+        logger.warn(`Timeout verifying bare repo remote (will use stale cache): ${errorMessage}`);\\n+        return 'timeout';\\n+      }\\n+      logger.warn(`Error verifying bare repo remote: ${error}`);\\n+      return false;\\n     }\\n   }\\n \\n   /**\\n-   * Create a new worktree\\n+   * Create a new worktree for the given repository/ref.\\n+   *\\n+   * Important: we always create worktrees in a detached HEAD state pinned\\n+   * to a specific commit SHA rather than a named branch like \\\"main\\\". Git\\n+   * only allows a branch to be checked out in a single worktree at a time;\\n+   * using the raw commit (plus --detach) lets multiple workflows safely\\n+   * create independent worktrees for the same branch without hitting\\n+   * errors like:\\n+   *\\n+   *   fatal: 'main' is already used by worktree at '.../TykTechnologies-tyk-docs-main-XXXX'\\n    */\\n   async createWorktree(\\n     repository: string,\\n@@ -195,6 +290,7 @@ export class WorktreeManager {\\n       clean?: boolean;\\n       workflowId?: string;\\n       fetchDepth?: number;\\n+      cloneTimeoutMs?: number;\\n     } = {}\\n   ): Promise<WorktreeInfo> {\\n     // Validate ref to prevent command injection\\n@@ -205,7 +301,8 @@ export class WorktreeManager {\\n       repository,\\n       repoUrl,\\n       options.token,\\n-      options.fetchDepth\\n+      options.fetchDepth,\\n+      options.cloneTimeoutMs\\n     );\\n \\n     // Generate worktree ID and path\\n@@ -241,21 +338,24 @@ export class WorktreeManager {\\n       }\\n     }\\n \\n-    // Fetch the ref if needed\\n+    // Fetch the ref if needed, then resolve it to a concrete commit SHA.\\n+    // We use the commit (detached HEAD) instead of the branch name so we\\n+    // can have multiple worktrees for the same branch without git refusing\\n+    // with \\\"branch X is already checked out\\\".\\n     await this.fetchRef(bareRepoPath, ref);\\n+    const commit = await this.getCommitShaForRef(bareRepoPath, ref);\\n \\n-    // Create worktree\\n-    logger.info(`Creating worktree for ${repository}@${ref}`);\\n-    const createCmd = `git -C ${this.escapeShellArg(bareRepoPath)} worktree add ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(ref)}`;\\n+    // Create worktree in detached HEAD state at the resolved commit\\n+    logger.info(`Creating worktree for ${repository}@${ref} (${commit})`);\\n+    const createCmd = `git -C ${this.escapeShellArg(\\n+      bareRepoPath\\n+    )} worktree add --detach ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(commit)}`;\\n     const result = await this.executeGitCommand(createCmd, { timeout: 60000 });\\n \\n     if (result.exitCode !== 0) {\\n       throw new Error(`Failed to create worktree: ${result.stderr}`);\\n     }\\n \\n-    // Get commit SHA\\n-    const commit = await this.getCommitSha(worktreePath);\\n-\\n     // Create metadata\\n     const metadata: WorktreeMetadata = {\\n       worktree_id: worktreeId,\\n@@ -316,14 +416,17 @@ export class WorktreeManager {\\n   }\\n \\n   /**\\n-   * Get commit SHA for worktree\\n+   * Get commit SHA for a given ref inside a bare repository.\\n+   *\\n+   * This runs after fetchRef so that <ref> should resolve to either a\\n+   * local branch, tag, or remote-tracking ref.\\n    */\\n-  private async getCommitSha(worktreePath: string): Promise<string> {\\n-    const cmd = `git -C ${this.escapeShellArg(worktreePath)} rev-parse HEAD`;\\n+  private async getCommitShaForRef(bareRepoPath: string, ref: string): Promise<string> {\\n+    const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} rev-parse ${this.escapeShellArg(ref)}`;\\n     const result = await this.executeGitCommand(cmd);\\n \\n     if (result.exitCode !== 0) {\\n-      throw new Error(`Failed to get commit SHA: ${result.stderr}`);\\n+      throw new Error(`Failed to get commit SHA for ref ${ref}: ${result.stderr}`);\\n     }\\n \\n     return result.stdout.trim();\\n@@ -621,9 +724,21 @@ export class WorktreeManager {\\n     command: string,\\n     options: { timeout?: number; env?: Record<string, string> } = {}\\n   ): Promise<GitCommandResult> {\\n+    // Merge provided env with process.env and add git-specific settings\\n+    // These settings prevent git from hanging on interactive prompts while\\n+    // still allowing OS-level credential helpers to work:\\n+    // - GIT_TERMINAL_PROMPT=0: Prevents terminal credential prompts\\n+    // - GIT_SSH_COMMAND: Disables SSH password prompts (BatchMode)\\n+    const gitEnv = {\\n+      ...process.env,\\n+      ...options.env,\\n+      GIT_TERMINAL_PROMPT: '0',\\n+      GIT_SSH_COMMAND: 'ssh -o BatchMode=yes -o StrictHostKeyChecking=no',\\n+    };\\n+\\n     const result = await commandExecutor.execute(command, {\\n       timeout: options.timeout || 30000,\\n-      env: options.env || process.env,\\n+      env: gitEnv,\\n     } as any);\\n \\n     return {\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-executor.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-executor.ts b/src/workflow-executor.ts\\nindex 751e7f16..df76762f 100644\\n--- a/src/workflow-executor.ts\\n+++ b/src/workflow-executor.ts\\n@@ -219,7 +219,9 @@ export class WorkflowExecutor {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps)) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     // Use static DependencyResolver\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-registry.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-registry.ts b/src/workflow-registry.ts\\nindex 2cbe3a89..6bd855fb 100644\\n--- a/src/workflow-registry.ts\\n+++ b/src/workflow-registry.ts\\n@@ -410,7 +410,9 @@ export class WorkflowRegistry {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps || {})) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     try {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/ai-review-service.test.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/tests/unit/ai-review-service.test.ts b/tests/unit/ai-review-service.test.ts\\nindex ec2b998a..2c616c99 100644\\n--- a/tests/unit/ai-review-service.test.ts\\n+++ b/tests/unit/ai-review-service.test.ts\\n@@ -482,6 +482,37 @@ describe('AIReviewService', () => {\\n       expect(context).toContain('<additions>10</additions>');\\n       expect(context).toContain('<deletions>5</deletions>');\\n     });\\n+\\n+    it('should exclude diffs by default when in Slack mode', async () => {\\n+      // Slack mode is detected by presence of slackConversation property\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // In Slack mode, diffs should be excluded by default\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+      // Should still include file summary\\n+      expect(context).toContain('<files_summary>');\\n+    });\\n+\\n+    it('should include diffs in Slack mode when explicitly requested', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      (mockPRInfo as any).includeCodeContext = true;\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // With explicit includeCodeContext: true, diffs should be included\\n+      expect(context).toContain('<full_diff>');\\n+      expect(context).toContain('--- test.ts');\\n+    });\\n+\\n+    it('should exclude diffs in Slack mode even when includeCodeContext is undefined', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      // Deliberately not setting includeCodeContext to test default behavior\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+    });\\n   });\\n \\n   describe('Comment Filtering for Code Review', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config-extends.test.ts\",\"additions\":8,\"deletions\":8,\"changes\":16,\"patch\":\"diff --git a/tests/unit/config-extends.test.ts b/tests/unit/config-extends.test.ts\\nindex b112a869..7ee69ce6 100644\\n--- a/tests/unit/config-extends.test.ts\\n+++ b/tests/unit/config-extends.test.ts\\n@@ -564,7 +564,7 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.security).toBeDefined();\\n       expect(config.checks!.performance).toBeDefined();\\n       expect(config.checks!.custom).toBeDefined();\\n-      expect(config.output.pr_comment.format).toBe('json');\\n+      expect(config.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should extend from default configuration', async () => {\\n@@ -1002,7 +1002,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.base).toBeDefined(); // From base\\n       expect(loaded.checks!.middle).toBeDefined(); // From middle\\n       expect(loaded.checks!.top).toBeDefined(); // From top\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From top\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From top\\n     });\\n \\n     it('should handle mixed local and remote extends', async () => {\\n@@ -1079,7 +1079,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.local).toBeDefined();\\n       expect(loaded.checks!.remote).toBeDefined();\\n       expect(loaded.checks!.child).toBeDefined();\\n-      expect(loaded.output.pr_comment.format).toBe('json');\\n+      expect(loaded.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should handle extends with relative path resolution', async () => {\\n@@ -1593,9 +1593,9 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.performance).toBeDefined(); // Added in child\\n \\n       // Verify output\\n-      expect(config.output.pr_comment.format).toBe('json'); // Overridden\\n-      expect(config.output.pr_comment.group_by).toBe('file'); // Overridden\\n-      expect(config.output.pr_comment.collapse).toBe(false); // Overridden\\n+      expect(config.output!.pr_comment.format).toBe('json'); // Overridden\\n+      expect(config.output!.pr_comment.group_by).toBe('file'); // Overridden\\n+      expect(config.output!.pr_comment.collapse).toBe(false); // Overridden\\n     });\\n \\n     it('should handle configuration with disabled checks removal', async () => {\\n@@ -1814,7 +1814,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.root).toBeDefined(); // From root\\n       expect(loaded.checks!.level1).toBeDefined(); // From level1\\n       expect(loaded.checks!.level2).toBeDefined(); // From level2\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From level2\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From level2\\n     });\\n \\n     it('should handle mixed absolute and relative paths in extends array', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config.test.ts\",\"additions\":7,\"deletions\":7,\"changes\":14,\"patch\":\"diff --git a/tests/unit/config.test.ts b/tests/unit/config.test.ts\\nindex 67ce61fd..0677a3d4 100644\\n--- a/tests/unit/config.test.ts\\n+++ b/tests/unit/config.test.ts\\n@@ -48,7 +48,7 @@ output:\\n       expect(config.version).toBe('1.0');\\n       expect(config.checks).toHaveProperty('performance');\\n       expect(config.checks).toHaveProperty('security');\\n-      expect(config.output.pr_comment.format).toBe('table');\\n+      expect(config.output!.pr_comment.format).toBe('table');\\n     });\\n \\n     it('should handle missing config file gracefully', async () => {\\n@@ -264,9 +264,9 @@ checks:\\n       const config = await configManager.loadConfig('/path/to/minimal.yaml');\\n \\n       // Should have default output configuration\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.group_by).toBe('check');\\n-      expect(config.output.pr_comment.collapse).toBe(true);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.group_by).toBe('check');\\n+      expect(config.output!.pr_comment.collapse).toBe(true);\\n     });\\n   });\\n \\n@@ -483,8 +483,8 @@ output:\\n       expect(config.checks!.performance.prompt).toContain('N+1 database queries');\\n       expect(config.checks!.security.prompt).toContain('SQL injection');\\n       expect(config.checks!.architecture.prompt).toContain('SOLID principles');\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.collapse).toBe(false);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.collapse).toBe(false);\\n     });\\n   });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/foreach-empty-skip.test.ts\",\"additions\":5,\"deletions\":3,\"changes\":8,\"patch\":\"diff --git a/tests/unit/foreach-empty-skip.test.ts b/tests/unit/foreach-empty-skip.test.ts\\nindex 1ea8e1e5..2640ed14 100644\\n--- a/tests/unit/foreach-empty-skip.test.ts\\n+++ b/tests/unit/foreach-empty-skip.test.ts\\n@@ -90,11 +90,13 @@ describe('forEach Empty Array Skip', () => {\\n     );\\n     expect(processStats?.totalRuns).toBe(0);\\n \\n-    // notify-ticket should also NOT run because process-ticket returned empty\\n+    // notify-ticket SHOULD run even though process-ticket had 0 runs from empty forEach\\n+    // This is the expected behavior: downstream steps that depend on forEach children\\n+    // should still execute (with empty/null inputs) to handle the \\\"no items\\\" case\\n     const notifyStats = result.executionStatistics?.checks!.find(\\n       c => c.checkName === 'notify-ticket'\\n     );\\n-    expect(notifyStats?.totalRuns).toBe(0);\\n+    expect(notifyStats?.totalRuns).toBe(1);\\n   });\\n \\n   it('should handle empty array with transform_js', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/ai-check-provider.test.ts\",\"additions\":61,\"deletions\":1,\"changes\":62,\"patch\":\"diff --git a/tests/unit/providers/ai-check-provider.test.ts b/tests/unit/providers/ai-check-provider.test.ts\\nindex d0b94086..a858ddae 100644\\n--- a/tests/unit/providers/ai-check-provider.test.ts\\n+++ b/tests/unit/providers/ai-check-provider.test.ts\\n@@ -399,6 +399,66 @@ describe('AICheckProvider', () => {\\n         },\\n       });\\n     });\\n+\\n+    it('derives allowedFolders and path from workspace when present on parent context', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const workspace = {\\n+        isEnabled: () => true,\\n+        getWorkspaceInfo: () => ({\\n+          workspacePath: '/tmp/ws-123',\\n+          mainProjectPath: '/tmp/ws-123/main-project',\\n+        }),\\n+        listProjects: () => [\\n+          { name: 'tyk', path: '/tmp/ws-123/tyk' },\\n+          { name: 'tyk-docs', path: '/tmp/ws-123/tyk-docs' },\\n+        ],\\n+      };\\n+\\n+      const execContext: any = {\\n+        _parentContext: {\\n+          workspace,\\n+        },\\n+      };\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'code_help',\\n+        ai: {\\n+          provider: 'google',\\n+          model: 'gemini-2.5-pro',\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, execContext);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'google',\\n+        model: 'gemini-2.5-pro',\\n+        // Workspace root should be used as primary working directory for tools\\n+        path: '/tmp/ws-123',\\n+      });\\n+\\n+      // allowedFolders should contain the workspace plus all project paths, de-duped\\n+      expect(capturedConfig.allowedFolders).toEqual(\\n+        expect.arrayContaining(['/tmp/ws-123', '/tmp/ws-123/tyk', '/tmp/ws-123/tyk-docs'])\\n+      );\\n+    });\\n   });\\n \\n   describe('getSupportedConfigKeys', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/command-check-provider.test.ts\",\"additions\":105,\"deletions\":1,\"changes\":106,\"patch\":\"diff --git a/tests/unit/providers/command-check-provider.test.ts b/tests/unit/providers/command-check-provider.test.ts\\nindex 3bf11a84..0a821bbd 100644\\n--- a/tests/unit/providers/command-check-provider.test.ts\\n+++ b/tests/unit/providers/command-check-provider.test.ts\\n@@ -666,4 +666,108 @@ describe('CommandCheckProvider', () => {\\n       expect((result as any).output).toBe('content'); // CommandCheckProvider trims whitespace\\n     });\\n   });\\n+\\n+  describe('Workflow Inputs', () => {\\n+    it('should use workflowInputs from config when available', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Hello from config.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from config.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with inputs from config.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from config.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should fall back to context.workflowInputs when config.workflowInputs is not set', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Hello from context.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from context.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify the command was rendered with inputs from context.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from context.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should prefer config.workflowInputs over context.workflowInputs', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Config takes precedence',\\n+        },\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Context should be ignored',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Config takes precedence\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify config.workflowInputs takes precedence\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Config takes precedence\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should provide empty inputs when neither config nor context has workflowInputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with empty inputs (undefined renders as empty)\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+  });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/git-checkout-workspace.test.ts\",\"additions\":4,\"deletions\":5,\"changes\":9,\"patch\":\"diff --git a/tests/unit/providers/git-checkout-workspace.test.ts b/tests/unit/providers/git-checkout-workspace.test.ts\\nindex 4eb625b7..4a0fbb6e 100644\\n--- a/tests/unit/providers/git-checkout-workspace.test.ts\\n+++ b/tests/unit/providers/git-checkout-workspace.test.ts\\n@@ -129,13 +129,12 @@ describe('GitCheckoutProvider Workspace Integration', () => {\\n         const output = (result as any).output as GitCheckoutOutput;\\n         expect(output.success).toBe(true);\\n         expect(output.workspace_path).toBeDefined();\\n-        // When checkName is provided, it's used as the project name\\n-        expect(output.workspace_path).toContain('test-checkout');\\n \\n-        // Verify project was added to workspace\\n+        // Verify project was added to workspace and the name is derived\\n+        // from the repository/description, not the checkName.\\n         const projects = workspace.listProjects();\\n         expect(projects.length).toBe(1);\\n-        expect(projects[0].name).toBe('test-checkout');\\n+        expect(projects[0].name).toBe('external-repo');\\n       } finally {\\n         await workspace.cleanup();\\n         fs.rmSync(mainProjectDir, { recursive: true, force: true });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/http-client-provider.test.ts\",\"additions\":15,\"deletions\":10,\"changes\":25,\"patch\":\"diff --git a/tests/unit/providers/http-client-provider.test.ts b/tests/unit/providers/http-client-provider.test.ts\\nindex b759ad75..caa73417 100644\\n--- a/tests/unit/providers/http-client-provider.test.ts\\n+++ b/tests/unit/providers/http-client-provider.test.ts\\n@@ -120,15 +120,17 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect(result).toEqual<ReviewSummary & { data: unknown }>({\\n+      // The provider returns data in the 'output' property (consistent with other providers)\\n+      expect(result).toEqual({\\n         issues: [],\\n-        data: responseData,\\n+        output: { status: 'ok', data: { value: 123 } },\\n       });\\n     });\\n \\n     it('should handle POST request with body', async () => {\\n       mockConfig.method = 'POST';\\n-      mockConfig.body = '{\\\"request\\\": \\\"data\\\"}';\\n+      // Use a body with Liquid template to trigger parseAndRender\\n+      mockConfig.body = '{\\\"request\\\": \\\"{{ pr.title }}\\\"}';\\n \\n       const responseData = { result: 'success' };\\n       const mockResponse = {\\n@@ -143,15 +145,16 @@ describe('HttpClientProvider', () => {\\n       };\\n \\n       mockFetch.mockResolvedValue(mockResponse);\\n-      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"data\\\"}');\\n+      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"Test PR\\\"}');\\n \\n       const result = await provider.execute(mockPRInfo, mockConfig, new Map());\\n \\n+      // Verify the body template was parsed by Liquid with the correct template\\n       expect(mockLiquid.parseAndRender).toHaveBeenCalledWith(\\n-        '{\\\"request\\\": \\\"data\\\"}',\\n+        '{\\\"request\\\": \\\"{{ pr.title }}\\\"}',\\n         expect.objectContaining({\\n           pr: expect.any(Object),\\n-          outputs: {},\\n+          outputs: expect.any(Object),\\n         })\\n       );\\n \\n@@ -159,7 +162,7 @@ describe('HttpClientProvider', () => {\\n         'https://api.example.com/data',\\n         expect.objectContaining({\\n           method: 'POST',\\n-          body: '{\\\"request\\\": \\\"data\\\"}',\\n+          body: '{\\\"request\\\": \\\"Test PR\\\"}',\\n           headers: expect.objectContaining({\\n             Authorization: 'Bearer token',\\n             'Content-Type': 'application/json',\\n@@ -167,7 +170,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(responseData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { result: string } }).output.result).toEqual('success');\\n     });\\n \\n     it('should handle HTTP errors', async () => {\\n@@ -235,7 +239,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(transformedData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { transformed: string } }).output.transformed).toEqual('result');\\n     });\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/session-reuse-config.test.ts\",\"additions\":20,\"deletions\":1,\"changes\":21,\"patch\":\"diff --git a/tests/unit/session-reuse-config.test.ts b/tests/unit/session-reuse-config.test.ts\\nindex a2e2ee26..18fc42e6 100644\\n--- a/tests/unit/session-reuse-config.test.ts\\n+++ b/tests/unit/session-reuse-config.test.ts\\n@@ -152,6 +152,25 @@ describe('Session Reuse Configuration Validation', () => {\\n       }).not.toThrow();\\n     });\\n \\n+    it('should accept reuse_ai_session=\\\\\\\"self\\\\\\\" without depends_on', () => {\\n+      const config: Partial<VisorConfig> = {\\n+        version: '1.0',\\n+        checks: {\\n+          'loop-check': {\\n+            type: 'ai',\\n+            prompt: 'Chat-like loop that reuses its own session',\\n+            on: ['pr_opened'],\\n+            // Special self-reuse mode does not require depends_on\\n+            reuse_ai_session: 'self',\\n+          },\\n+        },\\n+      };\\n+\\n+      expect(() => {\\n+        (configManager as any).validateConfig(config);\\n+      }).not.toThrow();\\n+    });\\n+\\n     it('should accept reuse_ai_session=true with multiple dependencies', () => {\\n       const config: Partial<VisorConfig> = {\\n         version: '1.0',\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/slack-markdown-format.test.ts\",\"additions\":25,\"deletions\":1,\"changes\":26,\"patch\":\"diff --git a/tests/unit/slack-markdown-format.test.ts b/tests/unit/slack-markdown-format.test.ts\\nindex 5e57ccae..39d0baf3 100644\\n--- a/tests/unit/slack-markdown-format.test.ts\\n+++ b/tests/unit/slack-markdown-format.test.ts\\n@@ -39,4 +39,28 @@ describe('markdownToSlack', () => {\\n       ['‚Ä¢ parent', '  ‚Ä¢ child', '```', '- not-a-bullet inside code', '```', '‚Ä¢ after'].join('\\\\n')\\n     );\\n   });\\n+\\n+  it('converts markdown headers to bold text', () => {\\n+    const input = '# Main Title\\\\n## Subtitle\\\\n### Section';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*Main Title*\\\\n*Subtitle*\\\\n*Section*');\\n+  });\\n+\\n+  it('adds newline before h1/h2 headers when preceded by content', () => {\\n+    const input = 'Some content\\\\n## New Section\\\\nMore content';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('Some content\\\\n\\\\n*New Section*\\\\nMore content');\\n+  });\\n+\\n+  it('does not add newline before h1 if it is the first line', () => {\\n+    const input = '# First Header\\\\nContent here';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*First Header*\\\\nContent here');\\n+  });\\n+\\n+  it('ignores headers inside code blocks', () => {\\n+    const input = '```\\\\n# This is a comment\\\\n```\\\\n# Real Header';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('```\\\\n# This is a comment\\\\n```\\\\n*Real Header*');\\n+  });\\n });\\n\",\"status\":\"modified\"}],\"outputs\":{\"ask\":{\"text\":\"Can this be achieved with no modifications to the APIDefinition and SecurityPolicy CRDs?\",\"ts\":1767905289824},\"jira-context\":{\"jira_context_xml\":\"\",\"issues\":[],\"issue_count\":0,\"ts\":1767905289891},\"zendesk-context\":{\"zendesk_context_xml\":\"\",\"tickets\":\"\",\"ticket_count\":0,\"attachments\":\"\",\"ts\":1767905289909},\"log-request\":{\"issues\":[],\"logOutput\":\"‚ÑπÔ∏è **INFO**: üì• Request from user: unknown\\nChannel: unknown\\nThread: unknown\\nMessage: Can this be achieved with no modifications to the APIDefinition and SecurityPolicy CRDs?\\n\"},\"route-intent\":{\"intent\":\"code_help\",\"topic\":\"User is asking for the best way to implement a blue-green deployment strategy for an environment managed by Tyk Operator in Kubernetes using OperatorContext. They want to clone APIDefinition and SecurityPolicy CRDs from a 'blue' environment to a 'green' environment for validation before switching traffic.\",\"text\":\"{\\n  \\\"intent\\\": \\\"code_help\\\",\\n  \\\"topic\\\": \\\"User is asking for the best way to implement a blue-green deployment strategy for an environment managed by Tyk Operator in Kubernetes using OperatorContext. They want to clone APIDefinition and SecurityPolicy CRDs from a 'blue' environment to a 'green' environment for validation before switching traffic.\\\"\\n}\",\"ts\":1767904450062},\"code-help\":{\"answer\":{\"text\":\"To achieve a blue-green deployment using the Tyk Operator and `OperatorContext`, you can follow these steps:\\n\\n1.  **Set up two separate Tyk stacks:** You will need to have two distinct Tyk environments, a \\\"blue\\\" one and a \\\"green\\\" one. Each environment should have its own Tyk Dashboard, Tyk Gateway, and Tyk Sink (MDCB).\\n\\n2.  **Create two `OperatorContext` resources:** In your Kubernetes cluster, you will define two `OperatorContext` custom resources. One will be configured to connect to your \\\"blue\\\" Tyk Dashboard's API, and the other will be configured to connect to your \\\"green\\\" Tyk Dashboard's API.\\n\\n3.  **Deploy your CRDs with `contextRef`:** You will deploy your `APIDefinition` and `SecurityPolicy` CRDs to your Kubernetes cluster. The CRDs intended for the blue environment should have a `contextRef` that points to the \\\"blue\\\" `OperatorContext`. Similarly, the CRDs for the green environment should have a `contextRef` that points to the \\\"green\\\" `OperatorContext`. This will ensure that the Tyk Operator applies the configurations to the correct Tyk environment.\\n\\n4.  **Validate the green environment:** Once the CRDs are applied, you can validate your \\\"green\\\" environment by sending traffic to the \\\"green\\\" Tyk Gateway's ingress. This allows you to test the new configuration without affecting your production traffic.\\n\\n5.  **Cut over traffic:** After you have validated the \\\"green\\\" environment, you can switch traffic from the \\\"blue\\\" environment to the \\\"green\\\" environment at your load balancer or ingress controller.\\n\\nThis approach provides a clean separation between your blue and green environments, allowing you to safely validate changes before directing production traffic to them. The Tyk Operator, through the use of `OperatorContext`, is the key to managing these multiple environments from a single Kubernetes cluster.\\n\\n**References:**\\n\\n*   **Tyk Operator Documentation:** [https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/automations/operator.mdx](https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/automations/operator.mdx)\\n*   **MDCB (Tyk Sink) Documentation:** [https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-mdcb-api.mdx](https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-mdcb-api.mdx)\"},\"code_plan\":{},\"ts\":1767904567543},\"code-help-reply\":{\"text\":\"To achieve a blue-green deployment using the Tyk Operator and `OperatorContext`, you can follow these steps:\\n\\n1.  **Set up two separate Tyk stacks:** You will need to have two distinct Tyk environments, a \\\"blue\\\" one and a \\\"green\\\" one. Each environment should have its own Tyk Dashboard, Tyk Gateway, and Tyk Sink (MDCB).\\n\\n2.  **Create two `OperatorContext` resources:** In your Kubernetes cluster, you will define two `OperatorContext` custom resources. One will be configured to connect to your \\\"blue\\\" Tyk Dashboard's API, and the other will be configured to connect to your \\\"green\\\" Tyk Dashboard's API.\\n\\n3.  **Deploy your CRDs with `contextRef`:** You will deploy your `APIDefinition` and `SecurityPolicy` CRDs to your Kubernetes cluster. The CRDs intended for the blue environment should have a `contextRef` that points to the \\\"blue\\\" `OperatorContext`. Similarly, the CRDs for the green environment should have a `contextRef` that points to the \\\"green\\\" `OperatorContext`. This will ensure that the Tyk Operator applies the configurations to the correct Tyk environment.\\n\\n4.  **Validate the green environment:** Once the CRDs are applied, you can validate your \\\"green\\\" environment by sending traffic to the \\\"green\\\" Tyk Gateway's ingress. This allows you to test the new configuration without affecting your production traffic.\\n\\n5.  **Cut over traffic:** After you have validated the \\\"green\\\" environment, you can switch traffic from the \\\"blue\\\" environment to the \\\"green\\\" environment at your load balancer or ingress controller.\\n\\nThis approach provides a clean separation between your blue and green environments, allowing you to safely validate changes before directing production traffic to them. The Tyk Operator, through the use of `OperatorContext`, is the key to managing these multiple environments from a single Kubernetes cluster.\\n\\n**References:**\\n\\n*   **Tyk Operator Documentation:** [https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/automations/operator.mdx](https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/automations/operator.mdx)\\n*   **MDCB (Tyk Sink) Documentation:** [https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-mdcb-api.mdx](https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-mdcb-api.mdx)\",\"ts\":1767904567544},\"code-help-error\":{\"issues\":[]},\"customer-insights-error\":{\"issues\":[]},\"release-notes-error\":{\"issues\":[]}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.output":"{\"intent\":\"code_help\",\"topic\":\"Can a blue-green deployment be managed with Tyk Operator without modifying the APIDefinition and SecurityPolicy CRDs for each environment? The user wants to know if it's possible to apply the same set of CRDs to both a blue and a green OperatorContext without changing the `contextRef` field in the source files.\",\"text\":\"{\\n  \\\"intent\\\": \\\"code_help\\\",\\n  \\\"topic\\\": \\\"Can a blue-green deployment be managed with Tyk Operator without modifying the APIDefinition and SecurityPolicy CRDs for each environment? The user wants to know if it's possible to apply the same set of CRDs to both a blue and a green OperatorContext without changing the `contextRef` field in the source files.\\\"\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":2,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"code-help","visor.provider.type":"workflow"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"a48e6293-0f9a-4575-8007-f5b6c0d36290"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"a48e6293-0f9a-4575-8007-f5b6c0d36290"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"a48e6293-0f9a-4575-8007-f5b6c0d36290"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-tyk-docs","visor.provider.type":"git-checkout"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"a48e6293-0f9a-4575-8007-f5b6c0d36290"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"a48e6293-0f9a-4575-8007-f5b6c0d36290"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"a48e6293-0f9a-4575-8007-f5b6c0d36290"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ensure-code-plan","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"ensure-code-plan","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"outputs\":{\"checkout-tyk-docs\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-docs-main-243ebdf6\",\"ref\":\"main\",\"commit\":\"a022bb7c96ca4f48bcdac27cae3f7c4a24171ee9\",\"worktree_id\":\"TykTechnologies-tyk-docs-main-243ebdf6\",\"repository\":\"TykTechnologies/tyk-docs\",\"is_worktree\":true,\"ts\":1767905312616}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"ensure-code-plan","visor.check.output":"{\"projects\":[{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Operator interacts with the Tyk Dashboard (tyk-analytics) to manage API definitions and policies. The OperatorContext CRD, which is central to the user's question, defines the connection and authentication details for a specific Dashboard instance.\"},{\"project_id\":\"tyk\",\"reason\":\"The Tyk Gateway is the component that is ultimately configured by the Dashboard. The blue-green deployment strategy is about controlling which version of an API configuration is loaded and served by the Gateways in each environment.\"}],\"notes\":\"The user's question is about the functionality of the Tyk Operator, which acts as an automation layer on top of the Tyk stack. Since the Operator itself is not a selectable project, the plan includes the core Tyk components it operates on: the Dashboard (control plane) and the Gateway (data plane).\",\"text\":\"{\\n  \\\"projects\\\": [\\n    {\\n      \\\"project_id\\\": \\\"tyk-analytics\\\",\\n      \\\"reason\\\": \\\"The Tyk Operator interacts with the Tyk Dashboard (tyk-analytics) to manage API definitions and policies. The OperatorContext CRD, which is central to the user's question, defines the connection and authentication details for a specific Dashboard instance.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk\\\",\\n      \\\"reason\\\": \\\"The Tyk Gateway is the component that is ultimately configured by the Dashboard. The blue-green deployment strategy is about controlling which version of an API configuration is loaded and served by the Gateways in each environment.\\\"\\n    }\\n  ],\\n  \\\"notes\\\": \\\"The user's question is about the functionality of the Tyk Operator, which acts as an automation layer on top of the Tyk stack. Since the Operator itself is not a selectable project, the plan includes the core Tyk components it operates on: the Dashboard (control plane) and the Gateway (data plane).\\\"\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"a48e6293-0f9a-4575-8007-f5b6c0d36290"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"a48e6293-0f9a-4575-8007-f5b6c0d36290"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"a48e6293-0f9a-4575-8007-f5b6c0d36290"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"project-items","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"a48e6293-0f9a-4575-8007-f5b6c0d36290"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"a48e6293-0f9a-4575-8007-f5b6c0d36290"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"a48e6293-0f9a-4575-8007-f5b6c0d36290"}}]}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":0,"visor.foreach.total":2},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":1,"visor.foreach.total":2},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"a48e6293-0f9a-4575-8007-f5b6c0d36290"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"a48e6293-0f9a-4575-8007-f5b6c0d36290"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"a48e6293-0f9a-4575-8007-f5b6c0d36290"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"project-code-query","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"project-code-query","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"outputs\":{\"ensure-code-plan\":{\"projects\":[{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Operator interacts with the Tyk Dashboard (tyk-analytics) to manage API definitions and policies. The OperatorContext CRD, which is central to the user's question, defines the connection and authentication details for a specific Dashboard instance.\"},{\"project_id\":\"tyk\",\"reason\":\"The Tyk Gateway is the component that is ultimately configured by the Dashboard. The blue-green deployment strategy is about controlling which version of an API configuration is loaded and served by the Gateways in each environment.\"}],\"notes\":\"The user's question is about the functionality of the Tyk Operator, which acts as an automation layer on top of the Tyk stack. Since the Operator itself is not a selectable project, the plan includes the core Tyk components it operates on: the Dashboard (control plane) and the Gateway (data plane).\",\"text\":\"{\\n  \\\"projects\\\": [\\n    {\\n      \\\"project_id\\\": \\\"tyk-analytics\\\",\\n      \\\"reason\\\": \\\"The Tyk Operator interacts with the Tyk Dashboard (tyk-analytics) to manage API definitions and policies. The OperatorContext CRD, which is central to the user's question, defines the connection and authentication details for a specific Dashboard instance.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk\\\",\\n      \\\"reason\\\": \\\"The Tyk Gateway is the component that is ultimately configured by the Dashboard. The blue-green deployment strategy is about controlling which version of an API configuration is loaded and served by the Gateways in each environment.\\\"\\n    }\\n  ],\\n  \\\"notes\\\": \\\"The user's question is about the functionality of the Tyk Operator, which acts as an automation layer on top of the Tyk stack. Since the Operator itself is not a selectable project, the plan includes the core Tyk components it operates on: the Dashboard (control plane) and the Gateway (data plane).\\\"\\n}\",\"ts\":1767905364682},\"project-items\":[{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Operator interacts with the Tyk Dashboard (tyk-analytics) to manage API definitions and policies. The OperatorContext CRD, which is central to the user's question, defines the connection and authentication details for a specific Dashboard instance.\",\"repository\":\"TykTechnologies/tyk-analytics\",\"description\":\"Tyk Dashboard backend\"},{\"project_id\":\"tyk\",\"reason\":\"The Tyk Gateway is the component that is ultimately configured by the Dashboard. The blue-green deployment strategy is about controlling which version of an API configuration is loaded and served by the Gateways in each environment.\",\"repository\":\"TykTechnologies/tyk\",\"description\":\"Tyk Gateway\"}],\"checkout-projects\":{\"issues\":[],\"isForEach\":true,\"forEachItems\":[{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-analytics-HEAD-42c18efe\",\"ref\":\"HEAD\",\"commit\":\"b7ed8000a93e1284b28100938363a312d2d1fad1\",\"worktree_id\":\"TykTechnologies-tyk-analytics-HEAD-42c18efe\",\"repository\":\"TykTechnologies/tyk-analytics\",\"is_worktree\":true},{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-HEAD-bf1b4b9c\",\"ref\":\"HEAD\",\"commit\":\"bb67e9e7f9382a09ff152c6942d00023c463feef\",\"worktree_id\":\"TykTechnologies-tyk-HEAD-bf1b4b9c\",\"repository\":\"TykTechnologies/tyk\",\"is_worktree\":true}],\"forEachItemResults\":[{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-analytics-HEAD-42c18efe\",\"ref\":\"HEAD\",\"commit\":\"b7ed8000a93e1284b28100938363a312d2d1fad1\",\"worktree_id\":\"TykTechnologies-tyk-analytics-HEAD-42c18efe\",\"repository\":\"TykTechnologies/tyk-analytics\",\"is_worktree\":true}},{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-HEAD-bf1b4b9c\",\"ref\":\"HEAD\",\"commit\":\"bb67e9e7f9382a09ff152c6942d00023c463feef\",\"worktree_id\":\"TykTechnologies-tyk-HEAD-bf1b4b9c\",\"repository\":\"TykTechnologies/tyk\",\"is_worktree\":true}}]},\"checkout-tyk-docs\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-docs-main-243ebdf6\",\"ref\":\"main\",\"commit\":\"a022bb7c96ca4f48bcdac27cae3f7c4a24171ee9\",\"worktree_id\":\"TykTechnologies-tyk-docs-main-243ebdf6\",\"repository\":\"TykTechnologies/tyk-docs\",\"is_worktree\":true,\"ts\":1767905312616},\"project-items-raw\":[{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Operator interacts with the Tyk Dashboard (tyk-analytics) to manage API definitions and policies. The OperatorContext CRD, which is central to the user's question, defines the connection and authentication details for a specific Dashboard instance.\",\"repository\":\"TykTechnologies/tyk-analytics\",\"description\":\"Tyk Dashboard backend\"},{\"project_id\":\"tyk\",\"reason\":\"The Tyk Gateway is the component that is ultimately configured by the Dashboard. The blue-green deployment strategy is about controlling which version of an API configuration is loaded and served by the Gateways in each environment.\",\"repository\":\"TykTechnologies/tyk\",\"description\":\"Tyk Gateway\"}]},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"project-code-query","visor.check.output":"{\"answer\":{\"text\":\"No, this cannot be achieved without modifications to the `APIDefinition` and `SecurityPolicy` resource manifests that are applied to the Kubernetes cluster.\\n\\nHere's a detailed explanation:\\n\\nAs described in the initial response, the Tyk Operator distinguishes between different Tyk environments (like your blue and green stacks) using an `OperatorContext` resource. To associate an `APIDefinition` or `SecurityPolicy` with a specific environment, you must include a `contextRef` field within the specification of that resource. This field explicitly tells the Operator which Tyk Dashboard instance to configure.\\n\\nFor example, your `APIDefinition` for the blue environment would contain:\\n\\n```yaml\\nspec:\\n  # ... other api definition fields\\n  contextRef:\\n    name: \\\"blue-context\\\"\\n```\\n\\nAnd for the green environment, it would need to be changed to:\\n\\n```yaml\\nspec:\\n  # ... other api definition fields\\n  contextRef:\\n    name: \\\"green-context\\\"\\n```\\n\\nBecause this `contextRef` field is part of the resource manifest, the file applied for the blue environment is inherently different from the one for the green environment. You cannot use the exact same, unmodified file for both.\\n\\n### The Standard Approach (The \\\"Kubernetes Way\\\")\\n\\nWhile you must modify the final manifest, you can avoid manually editing your source files by using standard Kubernetes configuration management tools. This is the recommended best practice for handling environment-specific differences.\\n\\n1.  **Kustomize**: You can maintain a common \\\"base\\\" set of `APIDefinition` and `SecurityPolicy` manifests. Then, for each environment (blue and green), you create a Kustomize \\\"overlay\\\" that patches the `contextRef` field with the appropriate value. Your CI/CD pipeline would then run `kustomize build` on the correct overlay before applying the resources.\\n\\n2.  **Helm**: You can create a Helm chart for your Tyk resources. The `contextRef` would be a templated value (`{{ .Values.contextName }}`). You would then deploy the same chart to both environments but provide different `values.yaml` files or set the values via the command line (`--set contextName=blue-context`).\\n\\nBy using these tools, you can keep your core `APIDefinition` and `SecurityPolicy` configurations in a single, unmodified source, while programmatically applying the necessary changes for each target environment during deployment.\"},\"text\":\"{\\n  \\\"answer\\\": {\\n    \\\"text\\\": \\\"No, this cannot be achieved without modifications to the `APIDefinition` and `SecurityPolicy` resource manifests that are applied to the Kubernetes cluster.\\\\n\\\\nHere's a detailed explanation:\\\\n\\\\nAs described in the initial response, the Tyk Operator distinguishes between different Tyk environments (like your blue and green stacks) using an `OperatorContext` resource. To associate an `APIDefinition` or `SecurityPolicy` with a specific environment, you must include a `contextRef` field within the specification of that resource. This field explicitly tells the Operator which Tyk Dashboard instance to configure.\\\\n\\\\nFor example, your `APIDefinition` for the blue environment would contain:\\\\n\\\\n```yaml\\\\nspec:\\\\n  # ... other api definition fields\\\\n  contextRef:\\\\n    name: \\\\\\\"blue-context\\\\\\\"\\\\n```\\\\n\\\\nAnd for the green environment, it would need to be changed to:\\\\n\\\\n```yaml\\\\nspec:\\\\n  # ... other api definition fields\\\\n  contextRef:\\\\n    name: \\\\\\\"green-context\\\\\\\"\\\\n```\\\\n\\\\nBecause this `contextRef` field is part of the resource manifest, the file applied for the blue environment is inherently different from the one for the green environment. You cannot use the exact same, unmodified file for both.\\\\n\\\\n### The Standard Approach (The \\\\\\\"Kubernetes Way\\\\\\\")\\\\n\\\\nWhile you must modify the final manifest, you can avoid manually editing your source files by using standard Kubernetes configuration management tools. This is the recommended best practice for handling environment-specific differences.\\\\n\\\\n1.  **Kustomize**: You can maintain a common \\\\\\\"base\\\\\\\" set of `APIDefinition` and `SecurityPolicy` manifests. Then, for each environment (blue and green), you create a Kustomize \\\\\\\"overlay\\\\\\\" that patches the `contextRef` field with the appropriate value. Your CI/CD pipeline would then run `kustomize build` on the correct overlay before applying the resources.\\\\n\\\\n2.  **Helm**: You can create a Helm chart for your Tyk resources. The `contextRef` would be a templated value (`{{ .Values.contextName }}`). You would then deploy the same chart to both environments but provide different `values.yaml` files or set the values via the command line (`--set contextName=blue-context`).\\\\n\\\\nBy using these tools, you can keep your core `APIDefinition` and `SecurityPolicy` configurations in a single, unmodified source, while programmatically applying the necessary changes for each target environment during deployment.\\\"\\n  }\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"a48e6293-0f9a-4575-8007-f5b6c0d36290"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"a48e6293-0f9a-4575-8007-f5b6c0d36290"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"a48e6293-0f9a-4575-8007-f5b6c0d36290"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":2,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"code-help-reply","visor.provider.type":"log"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":3,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":3,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":3,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":3,"session_id":"36e2714f-1b6c-40ff-a79a-e7bb6f45932c"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"jira-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"zendesk-context","visor.provider.type":"workflow"}}
{"name":"visor.provider","attributes":{"visor.check.id":"log-request","visor.provider.type":"log"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"28f43a0c-97ec-45e1-8766-febeaf8e8b75"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"89520df8-c1df-431e-b2bf-5bd2389705e1"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"28f43a0c-97ec-45e1-8766-febeaf8e8b75"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"89520df8-c1df-431e-b2bf-5bd2389705e1"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"28f43a0c-97ec-45e1-8766-febeaf8e8b75"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"89520df8-c1df-431e-b2bf-5bd2389705e1"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-keys","visor.provider.type":"script"}}
{"name":"visor.provider","attributes":{"visor.check.id":"extract-ids","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"history\":{}},\"outputs_history\":{},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"include_comments\":true,\"max_tickets\":5,\"download_assets\":true,\"cache_dir\":\".cache/zendesk\",\"text\":\"<@U09T5KRLMPU> Can all of the APIDefinition and SEcurityPolicy CRDs be exported from an environment?\\nCan all of the APIDefinition and SEcurityPolicy CRDs be exported from an environment?\\n\"},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"28f43a0c-97ec-45e1-8766-febeaf8e8b75"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"28f43a0c-97ec-45e1-8766-febeaf8e8b75"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"28f43a0c-97ec-45e1-8766-febeaf8e8b75"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"no-issues-fallback","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"no-issues-fallback","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"extract-keys\":{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767907826353},\"fetch-issues\":{\"issues\":[]},\"history\":{\"extract-keys\":[{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767907826353}],\"fetch-issues\":[{\"issues\":[]}]}},\"outputs_history\":{\"extract-keys\":[{\"data\":[],\"count\":0,\"jql_mode\":false,\"ts\":1767907826353}],\"fetch-issues\":[{\"issues\":[]}]},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"text\":\"<@U09T5KRLMPU> Can all of the APIDefinition and SEcurityPolicy CRDs be exported from an environment?\\nCan all of the APIDefinition and SEcurityPolicy CRDs be exported from an environment?\\n\",\"jql\":\"\",\"custom_field_aliases\":{},\"include_comments\":true,\"max_issues\":3,\"ticket_prefixes\":[\"TT\",\"DX\"]},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.check","attributes":{"visor.check.id":"extract-ids","visor.check.output":"{\"data\":[],\"count\":0}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.check","attributes":{"visor.check.id":"no-issues-fallback","visor.check.output":"\"<jira_context><issue_count>0</issue_count><message>No Jira issue keys found in the provided text</message></jira_context>\""},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"28f43a0c-97ec-45e1-8766-febeaf8e8b75"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"28f43a0c-97ec-45e1-8766-febeaf8e8b75"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"28f43a0c-97ec-45e1-8766-febeaf8e8b75"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"28f43a0c-97ec-45e1-8766-febeaf8e8b75"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"28f43a0c-97ec-45e1-8766-febeaf8e8b75"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"89520df8-c1df-431e-b2bf-5bd2389705e1"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"89520df8-c1df-431e-b2bf-5bd2389705e1"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"89520df8-c1df-431e-b2bf-5bd2389705e1"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"no-tickets-fallback","visor.provider.type":"command"}}
{"name":"visor.check","attributes":{"visor.check.id":"no-tickets-fallback","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"fileCount\":0,\"outputs\":{\"extract-ids\":{\"data\":[],\"count\":0,\"ts\":1767907826375},\"fetch-tickets\":{\"issues\":[]},\"history\":{\"extract-ids\":[{\"data\":[],\"count\":0,\"ts\":1767907826375}],\"fetch-tickets\":[{\"issues\":[]}]}},\"outputs_history\":{\"extract-ids\":[{\"data\":[],\"count\":0,\"ts\":1767907826375}],\"fetch-tickets\":[{\"issues\":[]}]},\"outputs_history_stage\":{},\"outputs_raw\":{},\"inputs\":{\"include_comments\":true,\"max_tickets\":5,\"download_assets\":true,\"cache_dir\":\".cache/zendesk\",\"text\":\"<@U09T5KRLMPU> Can all of the APIDefinition and SEcurityPolicy CRDs be exported from an environment?\\nCan all of the APIDefinition and SEcurityPolicy CRDs be exported from an environment?\\n\"},\"args\":{},\"env\":{\"SHELL\":\"/usr/bin/bash\",\"STARSHIP_START_TIME\":\"1767891797135\",\"COLORTERM\":\"truecolor\",\"VISOR_TELEMETRY_SINK\":\"otlp\",\"ZELLIJ_SESSION_NAME\":\"friendly-horse\",\"EDITOR\":\"nvim\",\"PWD\":\"/home/buger/projects/visor2\",\"LOGNAME\":\"buger\",\"ZELLIJ_PANE_ID\":\"41\",\"XDG_SESSION_TYPE\":\"tty\",\"STARSHIP_CMD_STATUS\":\"0\",\"VISOR_TELEMETRY_ENABLED\":\"true\",\"MOTD_SHOWN\":\"pam\",\"LINES\":\"53\",\"HOME\":\"/home/buger\",\"LANG\":\"en_US.UTF-8\",\"COLUMNS\":\"117\",\"STARSHIP_SHELL\":\"bash\",\"WAYLAND_DISPLAY\":\"wayland-1\",\"__MISE_DIFF\":\"eAFrXpyfk9KwOC+1vGFJQWJJxgQASssINA\",\"SSH_CONNECTION\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 fe80::7a55:36ff:fe01:cdfe%enp1s0 22\",\"BAT_THEME\":\"ansi\",\"STARSHIP_SESSION_KEY\":\"5614152961699532\",\"__MISE_ORIG_PATH\":\"./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"XDG_SESSION_CLASS\":\"user\",\"TERM\":\"xterm-256color\",\"USER\":\"buger\",\"SUDO_EDITOR\":\"nvim\",\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\":\"http://localhost:4318/v1/traces\",\"__MISE_SESSION\":\"eAFrX5OTn5iSmhJfkp+fUzxhHZSXnJ+XlplePPGmin5Gfm6qflJpemqRvh5EWD83szhVH8LWK8nPzVkDYccXJJZkFE9YnJpX1rA4JbNoN7LegqL8rNTkkmL9sszi/CKjNal5ZfFliUXxGYnFGRuSjC0NU9NMDZLMDZMNTS3M1uYklqQWl8SXFqQklqQeEWCAA8aZJj5KcwGShkTR\",\"DISPLAY\":\":0\",\"STARSHIP_END_TIME\":\"1767891794667\",\"SHLVL\":\"2\",\"XDG_SESSION_ID\":\"4\",\"KUBECONFIG\":\"/home/buger/.kube/config\",\"STARSHIP_PREEXEC_READY\":\"0\",\"XDG_RUNTIME_DIR\":\"/run/user/1000\",\"PS1\":\"\\n\\\\[\\u001b[1;36m\\\\]visor2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[3;36m\\\\]feat/slack-frontend-v2\\\\[\\u001b[0m\\\\] \\\\[\\u001b[36m\\\\]Ó©± ? \\\\[\\u001b[1m\\\\]‚ùØ\\\\[\\u001b[0m\\\\] \",\"SSH_CLIENT\":\"fe80::c69:48fa:a726:c61e%enp1s0 63348 22\",\"GOOGLE_API_KEY\":\"AIzaSyBtR-_H1-HQsreADsU8tjPQ2ac3W7FnpSA\",\"DEBUGINFOD_URLS\":\"https://debuginfod.archlinux.org \",\"MISE_SHELL\":\"bash\",\"PATH\":\"/home/buger/.local/bin:./bin:/home/buger/.local/bin:/home/buger/.turso:/home/buger/.cargo/bin:/home/buger/.local/bin:./bin:/home/buger/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\",\"ZELLIJ\":\"0\",\"DBUS_SESSION_BUS_ADDRESS\":\"unix:path=/run/user/1000/bus\",\"MAIL\":\"/var/spool/mail/buger\",\"SSH_TTY\":\"/dev/pts/0\",\"STARSHIP_DURATION\":\"3320011\",\"_\":\"./dist/index.js\",\"VISOR_VERSION\":\"0.1.42\",\"PROBE_VERSION\":\"0.6.0-rc176\",\"VISOR_COMMIT_SHA\":\"unknown\",\"VISOR_COMMIT_SHORT\":\"unknown\",\"JIRA_BASE_URL\":\"https://tyktech.atlassian.net\",\"JIRA_EMAIL\":\"leo@tyk.io\",\"ZENDESK_SUBDOMAIN\":\"tyksupport\",\"ZENDESK_EMAIL\":\"martin@tyk.io\",\"ZENDESK_AUTH\":\"bWFydGluQHR5ay5pby90b2tlbjpzOFVENDNDb281c1BmMnpRUjdCQXoyUnNxTTBEMmdKT21Vc2ZqeEZF\",\"JIRA_THEME_FIELD_ID\":\"customfield_10750\",\"JIRA_AUTH\":\"bGVvQHR5ay5pbzpBVEFUVDN4RmZHRjBWWmpDUGpRa25CbDhTMnhFaXNZaThOV3Z2ZjVRLTVMdXlOUEJNNnhNOExQTFpMMVUzNE9CVmJjOTJJZ2hPTmxHczM0dUNuQU1WNHBuOVZyTlVWUHpWdEUzQ3FXRHEySzhnZ29QN3V3b0pvY1hUdDFZdkNVdkE1eW1zci1mRkZnRV9ObFU1dmhiMXVqY0M2MHdyZGlJOFRhWVNWemF0WGVJTjdXMG1USl9BUzg9Njg2MEEyRUU=\",\"VISOR_FALLBACK_TRACE_FILE\":\"/home/buger/projects/visor2/output/traces/run-2026-01-08T17-03-17-822Z.ndjson\",\"VISOR_OUTPUT_FORMAT\":\"table\",\"VISOR_DEBUG\":\"true\",\"VISOR_KEEP_WORKSPACE\":\"true\"}}"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.check","attributes":{"visor.check.id":"no-tickets-fallback","visor.check.output":"\"<zendesk_context><ticket_count>0</ticket_count><message>No Zendesk ticket IDs found in the provided text</message></zendesk_context>\""},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"89520df8-c1df-431e-b2bf-5bd2389705e1"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"89520df8-c1df-431e-b2bf-5bd2389705e1"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"89520df8-c1df-431e-b2bf-5bd2389705e1"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"89520df8-c1df-431e-b2bf-5bd2389705e1"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"89520df8-c1df-431e-b2bf-5bd2389705e1"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"89520df8-c1df-431e-b2bf-5bd2389705e1"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"89520df8-c1df-431e-b2bf-5bd2389705e1"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"89520df8-c1df-431e-b2bf-5bd2389705e1"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"89520df8-c1df-431e-b2bf-5bd2389705e1"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"89520df8-c1df-431e-b2bf-5bd2389705e1"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"89520df8-c1df-431e-b2bf-5bd2389705e1"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"route-intent","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.input.context":"{\"pr\":{\"number\":0,\"title\":\"Local Analysis: feat/slack-frontend-v2 (73 modified)\",\"author\":\"Leonid Bugaev\",\"branch\":\"feat/slack-frontend-v2\",\"base\":\"main\"},\"files\":[{\"filename\":\"CLAUDE.md\",\"additions\":14,\"deletions\":1,\"changes\":15,\"patch\":\"diff --git a/CLAUDE.md b/CLAUDE.md\\nindex 009271f9..ae68c57e 100644\\n--- a/CLAUDE.md\\n+++ b/CLAUDE.md\\n@@ -150,4 +150,17 @@ Configuration supports:\\n    - Safe JSON parsing: `try { JSON.parse(output) } catch(e) { log(\\\"Error:\\\", e) }`\\n    - Validate structure: `log(\\\"Is array?\\\", Array.isArray(outputs[\\\"check-name\\\"]));`\\n \\n+6. **Tracing with OTel/Jaeger**:\\n+   - Enable telemetry: `VISOR_TELEMETRY_ENABLED=true`, `VISOR_TELEMETRY_SINK=otlp`,\\n+     `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://localhost:4318/v1/traces`\\n+   - Root span: `visor.run` (one per CLI/Slack execution)\\n+   - State spans: `engine.state.*` with `wave`, `wave_kind`, `session_id`\\n+   - Check spans: `visor.check.<checkId>` with `visor.check.id`, `visor.check.type`,\\n+     `visor.foreach.index` (for map fanout)\\n+   - Routing decisions: `visor.routing` events attached to the active state span; fields\\n+     include `trigger`, `action`, `source`, `target`, `scope`, `goto_event` (repeats\\n+     across waves show routing loops)\\n+   - Wave visibility: `engine.state.level_dispatch` includes `level_size` and\\n+     `level_checks_preview` for the planned wave\\n+\\n See `docs/debugging.md` for comprehensive debugging guide.\\n\",\"status\":\"modified\"},{\"filename\":\"README.md\",\"additions\":7,\"deletions\":1,\"changes\":8,\"patch\":\"diff --git a/README.md b/README.md\\nindex 41131861..d92f02cc 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -534,6 +534,12 @@ steps:\\n     session_mode: append    # Shares history for full conversation\\n ```\\n \\n+You can also reuse the **same check‚Äôs** session when it loops back to itself, using:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` with `session_mode: append`\\n+\\n+See the standalone example at `examples/session-reuse-self.yaml` and the detailed guide in [docs/advanced-ai.md](docs/advanced-ai.md).\\n+\\n Learn more: [docs/advanced-ai.md](docs/advanced-ai.md)\\n \\n ## üìã Schema-Template System\\n\",\"status\":\"modified\"},{\"filename\":\"docs/advanced-ai.md\",\"additions\":61,\"deletions\":2,\"changes\":63,\"patch\":\"diff --git a/docs/advanced-ai.md b/docs/advanced-ai.md\\nindex bb8e684a..879285ed 100644\\n--- a/docs/advanced-ai.md\\n+++ b/docs/advanced-ai.md\\n@@ -1,7 +1,7 @@\\n ## üß† Advanced AI Features\\n \\n ### AI Session Reuse\\n-Use `reuse_ai_session: true` on dependent checks to continue conversation context with the AI across checks. This improves follow‚Äëups and consistency.\\n+Use `reuse_ai_session` on checks to continue conversation context with the AI across steps. This improves follow‚Äëups and consistency for follow‚Äëon analysis and chat‚Äëstyle flows.\\n \\n **Session Modes:**\\n - **`clone` (default)**: Creates a copy of the conversation history. Each check gets an independent session with the same starting context. Changes made by one check don't affect others.\\n@@ -26,6 +26,65 @@ steps:\\n     depends_on: [security-remediation]\\n     reuse_ai_session: true\\n     session_mode: append  # Share history - sees full conversation\\n+\\n+#### Reusing your own session: `reuse_ai_session: self`\\n+\\n+Sometimes the step you want to loop back into is the AI step itself (e.g. Slack assistants or multi‚Äëturn internal tools). For that case you can use:\\n+\\n+- `reuse_ai_session: \\\"self\\\"` ‚Äì the step reuses its **own** Probe session when it runs again in the same engine run.\\n+- `session_mode: append` ‚Äì makes the follow‚Äëup behave like a normal conversation turn.\\n+\\n+On the first run of the step, Visor creates a new ProbeAgent session and registers it. If routing (`on_success.goto`, `goto_js`, etc.) later jumps back to the same step within the same run, the engine:\\n+\\n+- Finds the last result for that step in the current run.\\n+- Reads the `sessionId` stored in the result.\\n+- Calls the AI provider again using `executeReviewWithSessionReuse` with that session id.\\n+\\n+Simple example (no transport wiring, just CLI/tests):\\n+\\n+```yaml\\n+version: \\\"2.0\\\"\\n+\\n+steps:\\n+  seed:\\n+    type: script\\n+    content: |\\n+      return { text: \\\"hello from seed\\\" };\\n+\\n+  convo:\\n+    type: ai\\n+    depends_on: [seed]\\n+    reuse_ai_session: self\\n+    session_mode: append\\n+    ai:\\n+      provider: mock\\n+      model: mock\\n+      disableTools: true\\n+      allowedTools: []\\n+      system_prompt: \\\"You are a tiny echo assistant.\\\"\\n+    prompt: |\\n+      Seed message: {{ outputs['seed'].text }}\\n+\\n+      Past convo outputs in this run:\\n+      {% assign hist = outputs_history['convo'] | default: empty %}\\n+      {% if hist and hist.size > 0 %}\\n+      {% for h in hist %}\\n+      - Previous reply {{ forloop.index }}.\\n+      {% endfor %}\\n+      {% else %}\\n+      - No previous replies yet.\\n+      {% endif %}\\n+    on_success:\\n+      goto_js: |\\n+        // Example: re‚Äëenter this step up to 3 times in a single run\\n+        return attempt < 3 ? 'convo' : null;\\n+```\\n+\\n+The corresponding testable example lives at:\\n+\\n+- `examples/session-reuse-self.yaml`\\n+\\n+This keeps the configuration small but shows how to wire `reuse_ai_session: self` and `session_mode: append` without touching higher‚Äëlevel workflows like `tyk-assistant`.\\n ```\\n \\n **When to use each mode:**\\n\",\"status\":\"modified\"},{\"filename\":\"docs/recipes.md\",\"additions\":149,\"deletions\":3,\"changes\":152,\"patch\":\"diff --git a/docs/recipes.md b/docs/recipes.md\\nindex 1015bc6c..1089e9a5 100644\\n--- a/docs/recipes.md\\n+++ b/docs/recipes.md\\n@@ -321,8 +321,154 @@ Tip: When you define a JSON Schema, you generally do **not** need to tell the mo\\n - Avoid noisy fallbacks like `(outputs['x']?.kind ?? '') === 'status'` when `outputs['x']?.kind === 'status'` is equivalent.\\n - These conventions apply uniformly to any provider (`ai`, `command`, `script`, `github`, `http_client`, etc).\\n \\n+### Command step best practices\\n+\\n+When using `type: command` steps:\\n+\\n+**Avoid external tool dependencies** like `jq`, `yq`, `python`, etc.:\\n+- They may not be installed in all environments (GitHub Actions, Docker, CI)\\n+- Use `transform_js` to parse and transform output instead\\n+- Keep shell commands simple: `grep`, `sed`, `awk`, `sort`, `head` are universally available\\n+\\n+```yaml\\n+# Bad - requires jq\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | jq -R -s 'split(\\\"\\\\n\\\")'\\n+  parseJson: true\\n+\\n+# Good - use transform_js for parsing\\n+extract-data:\\n+  type: command\\n+  exec: |\\n+    echo \\\"$TEXT\\\" | grep -oE '[A-Z]+-[0-9]+' | sort -u\\n+  transform_js: |\\n+    const lines = (output || '').trim().split('\\\\n').filter(Boolean);\\n+    return { data: lines, count: lines.length };\\n+```\\n+\\n+**Prefer line-separated output** over JSON from shell:\\n+- Simple to parse with `transform_js`\\n+- No need for `parseJson: true`\\n+- More robust across different shells/environments\\n+\\n+**Use transform_js for structured output**:\\n+- The sandbox provides `output` (command stdout as string)\\n+- Return an object with the fields you need\\n+- Works consistently across all environments\\n+\\n+### Testing workflows with `--no-mocks`\\n+\\n+The `--no-mocks` flag runs your test cases with real providers instead of injecting mock responses. This is essential for:\\n+\\n+1. **Debugging integration issues** - See actual API responses and errors\\n+2. **Capturing realistic mock data** - Get real output to copy into your test cases\\n+3. **Validating credentials** - Verify environment variables are set correctly\\n+4. **Developing new workflows** - Build tests incrementally with real data\\n+\\n+#### Basic usage\\n+\\n+```bash\\n+# Run all test cases with real providers\\n+visor test --config my-workflow.yaml --no-mocks\\n+\\n+# Run a specific test case with real providers\\n+visor test --config my-workflow.yaml --no-mocks --only \\\"my-test-case\\\"\\n+```\\n+\\n+#### Suggested mocks output\\n+\\n+When running with `--no-mocks`, Visor captures each step's output and prints it as YAML you can copy directly into your test case:\\n+\\n+```\\n+üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)\\n+   Step outputs will be captured and printed as suggested mocks\\n+\\n+... test execution ...\\n+\\n+üìã Suggested mocks (copy to your test case):\\n+mocks:\\n+  extract-keys:\\n+    data:\\n+      - PROJ-123\\n+      - DEV-456\\n+    count: 2\\n+  fetch-issues:\\n+    data:\\n+      - key: PROJ-123\\n+        summary: Fix authentication bug\\n+        status: In Progress\\n+```\\n+\\n+Copy the YAML under `mocks:` into your test case's `mocks:` section.\\n+\\n+#### Workflow for building tests\\n+\\n+1. **Start with a minimal test case** (no mocks):\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+   ```\\n+\\n+2. **Run with `--no-mocks`** to capture real outputs:\\n+   ```bash\\n+   visor test --config workflow.yaml --no-mocks --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+3. **Copy the suggested mocks** into your test case:\\n+   ```yaml\\n+   tests:\\n+     cases:\\n+       - name: my-new-test\\n+         event: manual\\n+         fixture: local.minimal\\n+         workflow_input:\\n+           text: \\\"Fix bug PROJ-123\\\"\\n+         mocks:\\n+           extract-keys:\\n+             data: [\\\"PROJ-123\\\"]\\n+             count: 1\\n+           # ... rest of captured mocks\\n+   ```\\n+\\n+4. **Add assertions** based on the real data:\\n+   ```yaml\\n+         expect:\\n+           workflow_output:\\n+             - path: issue_count\\n+               equals: 1\\n+   ```\\n+\\n+5. **Run normally** to verify mocks work:\\n+   ```bash\\n+   visor test --config workflow.yaml --only \\\"my-new-test\\\"\\n+   ```\\n+\\n+#### Debugging with `--no-mocks`\\n+\\n+When a test fails with mocks, use `--no-mocks` to see what's actually happening:\\n+\\n+```bash\\n+# See real API responses and errors\\n+visor test --config workflow.yaml --no-mocks --only \\\"failing-test\\\"\\n+\\n+# Common issues revealed:\\n+# - Missing or expired credentials\\n+# - API endpoint changes\\n+# - Unexpected response formats\\n+# - Network/timeout issues\\n+```\\n+\\n+The real error messages and responses help identify whether the issue is with your mocks or the actual integration.\\n+\\n ### More examples\\n \\n-- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags  \\n-- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration  \\n+- `docs/NPM_USAGE.md` ‚Äì CLI usage and flags\\n+- `GITHUB_CHECKS.md` ‚Äì Checks, outputs, and workflow integration\\n - `examples/` ‚Äì MCP, Jira, and advanced configs\\n\",\"status\":\"modified\"},{\"filename\":\"package-lock.json\",\"additions\":252,\"deletions\":9,\"changes\":261,\"patch\":\"diff --git a/package-lock.json b/package-lock.json\\nindex 5fb3ad89..e5b10d27 100644\\n--- a/package-lock.json\\n+++ b/package-lock.json\\n@@ -7,6 +7,7 @@\\n     \\\"\\\": {\\n       \\\"name\\\": \\\"@probelabs/visor\\\",\\n       \\\"version\\\": \\\"0.1.42\\\",\\n+      \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"MIT\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@actions/core\\\": \\\"^1.11.1\\\",\\n@@ -16,7 +17,7 @@\\n         \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n         \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n         \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n         \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n         \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -54,6 +55,7 @@\\n         \\\"husky\\\": \\\"^9.1.7\\\",\\n         \\\"jest\\\": \\\"^30.1.3\\\",\\n         \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+        \\\"patch-package\\\": \\\"^8.0.1\\\",\\n         \\\"prettier\\\": \\\"^3.6.2\\\",\\n         \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n         \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n@@ -5942,9 +5944,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/maid\\\": {\\n-      \\\"version\\\": \\\"0.0.21\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.21.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-H3EA2tDjWsAPgXZAwfHrNoEXnHmQFS9+JGDK/YpeiF3g3FxPgnWBgtgYPgtNuETQsEXxR1RzDgQVJL2JHWRRHw==\\\",\\n+      \\\"version\\\": \\\"0.0.22\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/maid/-/maid-0.0.22.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-FYcyyP4BlVrHQmBznXFPpMbvUqFI6wbecHUUe4S+yIeDvJ8W4bP0ndZ0cMoc2p7hekWYAG4RtlohQBr8e4TKOg==\\\",\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n         \\\"@types/dagre\\\": \\\"^0.7.53\\\",\\n@@ -5960,9 +5962,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/probe\\\": {\\n-      \\\"version\\\": \\\"0.6.0-rc169\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc169.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-/laV3+1TSjASru8gGSvGwyjQ7jalMkFuZETl35uv37J1Hb4eG95CbKGU5NihcCrLXR44q1lKJiQGHXcAav2mBA==\\\",\\n+      \\\"version\\\": \\\"0.6.0-rc176\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc176.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-w/pAvnz2/yXb4FQXZV2QazxuW02xsI/wRg36yxZoLSG0O/hIncjF4rquZe3O+emceI2Ks5zbrA7cgQEdpwT8vA==\\\",\\n       \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n@@ -5972,7 +5974,7 @@\\n         \\\"@ai-sdk/openai\\\": \\\"^2.0.10\\\",\\n         \\\"@anthropic-ai/claude-agent-sdk\\\": \\\"^0.1.46\\\",\\n         \\\"@modelcontextprotocol/sdk\\\": \\\"^1.0.0\\\",\\n-        \\\"@probelabs/maid\\\": \\\"^0.0.21\\\",\\n+        \\\"@probelabs/maid\\\": \\\"^0.0.22\\\",\\n         \\\"adm-zip\\\": \\\"^0.5.16\\\",\\n         \\\"ai\\\": \\\"^5.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -8192,6 +8194,13 @@\\n         \\\"node\\\": \\\">= 20\\\"\\n       }\\n     },\\n+    \\\"node_modules/@yarnpkg/lockfile\\\": {\\n+      \\\"version\\\": \\\"1.1.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@yarnpkg/lockfile/-/lockfile-1.1.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-GpSwvyXOcOOlV70vbnzjj4fW5xW/FdUF6nQEt1ENy7m4ZCczi1+/buVUPAqmGfqznsORNFzUMjctTIp8a9tuCQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"BSD-2-Clause\\\"\\n+    },\\n     \\\"node_modules/accepts\\\": {\\n       \\\"version\\\": \\\"1.3.8\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/accepts/-/accepts-1.3.8.tgz\\\",\\n@@ -9180,6 +9189,25 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/call-bind\\\": {\\n+      \\\"version\\\": \\\"1.0.8\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind/-/call-bind-1.0.8.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-oKlSFMcMwpUg2ednkhQ454wfWiU/ul3CkJe/PEHcTKuiX6RpbehUiFMXu13HalGZxfUwCQzZG747YXBn1im9ww==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind-apply-helpers\\\": \\\"^1.0.0\\\",\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"set-function-length\\\": \\\"^1.2.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/call-bind-apply-helpers\\\": {\\n       \\\"version\\\": \\\"1.0.2\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/call-bind-apply-helpers/-/call-bind-apply-helpers-1.0.2.tgz\\\",\\n@@ -10613,6 +10641,24 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/define-data-property\\\": {\\n+      \\\"version\\\": \\\"1.1.4\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/define-data-property/-/define-data-property-1.1.4.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-rBMvIzlpA8v6E+SJZoo++HAYqsLrkg7MSfIinMPFhmkorw7X+dOXVJQs+QT69zGkzMyfDnIMN2Wid1+NbL3T+A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/define-lazy-prop\\\": {\\n       \\\"version\\\": \\\"3.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/define-lazy-prop/-/define-lazy-prop-3.0.0.tgz\\\",\\n@@ -11794,6 +11840,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/find-yarn-workspace-root\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/find-yarn-workspace-root/-/find-yarn-workspace-root-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-1IMnbjt4KzsQfnhnzNd8wUEgXZ44IzZaZmnLYx7D5FZlaHt2gW20Cri8Q+E/t5tIj4+epTBub+2Zxu/vNILzqQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Apache-2.0\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"micromatch\\\": \\\"^4.0.2\\\"\\n+      }\\n+    },\\n     \\\"node_modules/fix-dts-default-cjs-exports\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/fix-dts-default-cjs-exports/-/fix-dts-default-cjs-exports-1.0.1.tgz\\\",\\n@@ -12267,6 +12323,19 @@\\n         \\\"node\\\": \\\">=8\\\"\\n       }\\n     },\\n+    \\\"node_modules/has-property-descriptors\\\": {\\n+      \\\"version\\\": \\\"1.0.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/has-property-descriptors/-/has-property-descriptors-1.0.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-55JNKuIW+vq4Ke1BjOTjM2YctQIvCT7GFzHwmfZPGo5wnrgkid0YQtnAleFSqumZm4az3n2BS+erby5ipJdgrg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"es-define-property\\\": \\\"^1.0.0\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/has-symbols\\\": {\\n       \\\"version\\\": \\\"1.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/has-symbols/-/has-symbols-1.1.0.tgz\\\",\\n@@ -12784,6 +12853,13 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/isarray\\\": {\\n+      \\\"version\\\": \\\"2.0.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/isarray/-/isarray-2.0.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-xHjhDr3cNBK0BzdUJSPXZntQUx/mwMS5Rw4A7lPJ90XGAO6ISP/ePDNuo0vhqOZU+UD5JoodwCAAoZQd3FeAKw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\"\\n+    },\\n     \\\"node_modules/isexe\\\": {\\n       \\\"version\\\": \\\"2.0.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz\\\",\\n@@ -13594,6 +13670,26 @@\\n       \\\"integrity\\\": \\\"sha512-NM8/P9n3XjXhIZn1lLhkFaACTOURQXjWhV4BA/RnOv8xvgqtqpAX9IO4mRQxSx1Rlo4tqzeqb0sOlruaOy3dug==\\\",\\n       \\\"license\\\": \\\"MIT\\\"\\n     },\\n+    \\\"node_modules/json-stable-stringify\\\": {\\n+      \\\"version\\\": \\\"1.3.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify/-/json-stable-stringify-1.3.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-qtYiSSFlwot9XHtF9bD9c7rwKjr+RecWT//ZnPvSmEjpV5mmPOCN4j8UjY5hbjNkOwZ/jQv3J6R1/pL7RwgMsg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"call-bind\\\": \\\"^1.0.8\\\",\\n+        \\\"call-bound\\\": \\\"^1.0.4\\\",\\n+        \\\"isarray\\\": \\\"^2.0.5\\\",\\n+        \\\"jsonify\\\": \\\"^0.0.1\\\",\\n+        \\\"object-keys\\\": \\\"^1.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/json-stable-stringify-without-jsonify\\\": {\\n       \\\"version\\\": \\\"1.0.1\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/json-stable-stringify-without-jsonify/-/json-stable-stringify-without-jsonify-1.0.1.tgz\\\",\\n@@ -13633,6 +13729,16 @@\\n         \\\"graceful-fs\\\": \\\"^4.1.6\\\"\\n       }\\n     },\\n+    \\\"node_modules/jsonify\\\": {\\n+      \\\"version\\\": \\\"0.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/jsonify/-/jsonify-0.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-2/Ki0GcmuqSrgFyelQq9M05y7PS0mEwuIzrf3f1fPqkVDVRvZrPZtVSMHxdgo8Aq0sxAOb/cr2aqqA3LeWHVPg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"Public Domain\\\",\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n+      }\\n+    },\\n     \\\"node_modules/katex\\\": {\\n       \\\"version\\\": \\\"0.16.25\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/katex/-/katex-0.16.25.tgz\\\",\\n@@ -13676,6 +13782,16 @@\\n       \\\"integrity\\\": \\\"sha512-Ls993zuzfayK269Svk9hzpeGUKob/sIgZzyHYdjQoAdQetRKpOLj+k/QQQ/6Qi0Yz65mlROrfd+Ev+1+7dz9Kw==\\\",\\n       \\\"dev\\\": true\\n     },\\n+    \\\"node_modules/klaw-sync\\\": {\\n+      \\\"version\\\": \\\"6.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/klaw-sync/-/klaw-sync-6.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-nIeuVSzdCCs6TDPTqI8w1Yre34sSq7AkZ4B3sfOBbI2CgVSB4Du4aLQijFU2+lhAFCwt9+42Hel6lQNIv6AntQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"graceful-fs\\\": \\\"^4.1.11\\\"\\n+      }\\n+    },\\n     \\\"node_modules/kolorist\\\": {\\n       \\\"version\\\": \\\"1.8.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/kolorist/-/kolorist-1.8.0.tgz\\\",\\n@@ -14785,6 +14901,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/ljharb\\\"\\n       }\\n     },\\n+    \\\"node_modules/object-keys\\\": {\\n+      \\\"version\\\": \\\"1.1.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/object-keys/-/object-keys-1.1.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NuAESUOUMrlIXOfHKzD6bpPu3tYt3xvjNdRIQ+FeT0lNb4K8WR70CaDxhuNguS2XG+GjkyMwOzsN5ZktImfhLA==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/ohash\\\": {\\n       \\\"version\\\": \\\"2.0.11\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/ohash/-/ohash-2.0.11.tgz\\\",\\n@@ -15022,6 +15148,95 @@\\n         \\\"node\\\": \\\">= 0.8\\\"\\n       }\\n     },\\n+    \\\"node_modules/patch-package\\\": {\\n+      \\\"version\\\": \\\"8.0.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/patch-package/-/patch-package-8.0.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-VsKRIA8f5uqHQ7NGhwIna6Bx6D9s/1iXlA1hthBVBEbkq+t4kXD0HHt+rJhf/Z+Ci0F/HCB2hvn0qLdLG+Qxlw==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"@yarnpkg/lockfile\\\": \\\"^1.1.0\\\",\\n+        \\\"chalk\\\": \\\"^4.1.2\\\",\\n+        \\\"ci-info\\\": \\\"^3.7.0\\\",\\n+        \\\"cross-spawn\\\": \\\"^7.0.3\\\",\\n+        \\\"find-yarn-workspace-root\\\": \\\"^2.0.0\\\",\\n+        \\\"fs-extra\\\": \\\"^10.0.0\\\",\\n+        \\\"json-stable-stringify\\\": \\\"^1.0.2\\\",\\n+        \\\"klaw-sync\\\": \\\"^6.0.0\\\",\\n+        \\\"minimist\\\": \\\"^1.2.6\\\",\\n+        \\\"open\\\": \\\"^7.4.2\\\",\\n+        \\\"semver\\\": \\\"^7.5.3\\\",\\n+        \\\"slash\\\": \\\"^2.0.0\\\",\\n+        \\\"tmp\\\": \\\"^0.2.4\\\",\\n+        \\\"yaml\\\": \\\"^2.2.2\\\"\\n+      },\\n+      \\\"bin\\\": {\\n+        \\\"patch-package\\\": \\\"index.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14\\\",\\n+        \\\"npm\\\": \\\">5\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/ci-info\\\": {\\n+      \\\"version\\\": \\\"3.9.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/ci-info/-/ci-info-3.9.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-NIxF55hv4nSqQswkAeiOi1r83xy8JldOFDTWiug55KBu9Jnblncd2U6ViHmYgHf01TPZS77NJBhBMKdWj9HQMQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"funding\\\": [\\n+        {\\n+          \\\"type\\\": \\\"github\\\",\\n+          \\\"url\\\": \\\"https://github.com/sponsors/sibiraj-s\\\"\\n+        }\\n+      ],\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/is-docker\\\": {\\n+      \\\"version\\\": \\\"2.2.1\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/is-docker/-/is-docker-2.2.1.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-F+i2BKsFrH66iaUFc0woD8sLy8getkwTwtOBjvs56Cx4CgJDeKQeqfz8wAYiSb8JOprWhHH5p77PbmYCvvUuXQ==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"bin\\\": {\\n+        \\\"is-docker\\\": \\\"cli.js\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/open\\\": {\\n+      \\\"version\\\": \\\"7.4.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/open/-/open-7.4.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-MVHddDVweXZF3awtlAS+6pgKLlm/JgxZ90+/NBurBoQctVOOB/zDdVjcyPzQ+0laDGbsWgrRkflI65sQeOgT9Q==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"is-docker\\\": \\\"^2.0.0\\\",\\n+        \\\"is-wsl\\\": \\\"^2.1.1\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=8\\\"\\n+      },\\n+      \\\"funding\\\": {\\n+        \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n+      }\\n+    },\\n+    \\\"node_modules/patch-package/node_modules/slash\\\": {\\n+      \\\"version\\\": \\\"2.0.0\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/slash/-/slash-2.0.0.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-ZYKh3Wh2z1PpEXWr0MpSBZ0V6mZHAQfYevttO11c51CaWjGTaadiKZ+wVt1PbMlDV5qhMFslpZCemhwOK7C89A==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=6\\\"\\n+      }\\n+    },\\n     \\\"node_modules/path-data-parser\\\": {\\n       \\\"version\\\": \\\"0.1.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/path-data-parser/-/path-data-parser-0.1.0.tgz\\\",\\n@@ -16473,6 +16688,24 @@\\n         \\\"node\\\": \\\">= 0.8.0\\\"\\n       }\\n     },\\n+    \\\"node_modules/set-function-length\\\": {\\n+      \\\"version\\\": \\\"1.2.2\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/set-function-length/-/set-function-length-1.2.2.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-pgRc4hJ4/sNjWCSS9AmnS40x3bNMDTknHgL5UaMBTMyJnU90EgWh1Rz+MC9eFu4BuN/UwZjKQuY/1v3rM7HMfg==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"dependencies\\\": {\\n+        \\\"define-data-property\\\": \\\"^1.1.4\\\",\\n+        \\\"es-errors\\\": \\\"^1.3.0\\\",\\n+        \\\"function-bind\\\": \\\"^1.1.2\\\",\\n+        \\\"get-intrinsic\\\": \\\"^1.2.4\\\",\\n+        \\\"gopd\\\": \\\"^1.0.1\\\",\\n+        \\\"has-property-descriptors\\\": \\\"^1.0.2\\\"\\n+      },\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">= 0.4\\\"\\n+      }\\n+    },\\n     \\\"node_modules/setprototypeof\\\": {\\n       \\\"version\\\": \\\"1.2.0\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/setprototypeof/-/setprototypeof-1.2.0.tgz\\\",\\n@@ -17384,6 +17617,16 @@\\n         \\\"url\\\": \\\"https://github.com/sponsors/sindresorhus\\\"\\n       }\\n     },\\n+    \\\"node_modules/tmp\\\": {\\n+      \\\"version\\\": \\\"0.2.5\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/tmp/-/tmp-0.2.5.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-voyz6MApa1rQGUxT3E+BK7/ROe8itEx7vD8/HEvt4xwXucvQ5G5oeEiHkmHZJuBO21RpOf+YYm9MOivj709jow==\\\",\\n+      \\\"dev\\\": true,\\n+      \\\"license\\\": \\\"MIT\\\",\\n+      \\\"engines\\\": {\\n+        \\\"node\\\": \\\">=14.14\\\"\\n+      }\\n+    },\\n     \\\"node_modules/tmpl\\\": {\\n       \\\"version\\\": \\\"1.0.5\\\",\\n       \\\"resolved\\\": \\\"https://registry.npmjs.org/tmpl/-/tmpl-1.0.5.tgz\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"package.json\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/package.json b/package.json\\nindex f9ccbb6c..302d728f 100644\\n--- a/package.json\\n+++ b/package.json\\n@@ -49,6 +49,7 @@\\n     \\\"test:with-build\\\": \\\"npm run build:cli && jest\\\",\\n     \\\"test:yaml\\\": \\\"node dist/index.js test --progress compact\\\",\\n     \\\"prepare\\\": \\\"husky\\\",\\n+    \\\"postinstall\\\": \\\"patch-package\\\",\\n     \\\"pre-commit\\\": \\\"lint-staged\\\",\\n     \\\"deploy:site\\\": \\\"cd site && npx wrangler pages deploy . --project-name=visor-site --commit-dirty=true\\\",\\n     \\\"deploy:worker\\\": \\\"npx wrangler deploy\\\",\\n@@ -96,7 +97,7 @@\\n     \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n     \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n     \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc169\\\",\\n+    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc176\\\",\\n     \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n     \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n     \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -143,6 +144,7 @@\\n     \\\"husky\\\": \\\"^9.1.7\\\",\\n     \\\"jest\\\": \\\"^30.1.3\\\",\\n     \\\"lint-staged\\\": \\\"^16.1.6\\\",\\n+    \\\"patch-package\\\": \\\"^8.0.1\\\",\\n     \\\"prettier\\\": \\\"^3.6.2\\\",\\n     \\\"reveal-md\\\": \\\"^6.1.2\\\",\\n     \\\"ts-jest\\\": \\\"^29.4.1\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"src/ai-review-service.ts\",\"additions\":101,\"deletions\":151,\"changes\":252,\"patch\":\"diff --git a/src/ai-review-service.ts b/src/ai-review-service.ts\\nindex 6b9c5b2e..f18accea 100644\\n--- a/src/ai-review-service.ts\\n+++ b/src/ai-review-service.ts\\n@@ -65,6 +65,13 @@ export interface AIReviewConfig {\\n   allowBash?: boolean;\\n   // Advanced bash command execution configuration\\n   bashConfig?: import('./types/config').BashConfig;\\n+  // Optional workspace root and allowed folders for ProbeAgent.\\n+  // When provided, these are forwarded to ProbeAgent so tools like search/query\\n+  // operate inside the isolated workspace/projects instead of the Visor repo root.\\n+  path?: string;\\n+  allowedFolders?: string[];\\n+  // Completion prompt for post-completion validation/review (runs after attempt_completion)\\n+  completionPrompt?: string;\\n }\\n \\n export interface AIDebugInfo {\\n@@ -278,7 +285,7 @@ export class AIReviewService {\\n     try {\\n       const call = this.callProbeAgent(prompt, schema, debugInfo, checkName, sessionId);\\n       const timeoutMs = Math.max(0, this.config.timeout || 0);\\n-      const { response, effectiveSchema } =\\n+      const { response, effectiveSchema, sessionId: usedSessionId } =\\n         timeoutMs > 0 ? await this.withTimeout(call, timeoutMs, 'AI review') : await call;\\n       const processingTime = Date.now() - startTime;\\n \\n@@ -290,6 +297,11 @@ export class AIReviewService {\\n \\n       const result = this.parseAIResponse(response, debugInfo, effectiveSchema);\\n \\n+      // Expose the session ID used for this call so the engine can reuse it later\\n+      try {\\n+        (result as any).sessionId = usedSessionId;\\n+      } catch {}\\n+\\n       if (debugInfo) {\\n         result.debug = debugInfo;\\n       }\\n@@ -651,17 +663,34 @@ ${prContext}${slackContextXml}\\n     const prContextInfo = prInfo as PRInfo & {\\n       isPRContext?: boolean;\\n       includeCodeContext?: boolean;\\n+      slackConversation?: unknown;\\n     };\\n     const isIssue = prContextInfo.isIssue === true;\\n \\n     // Check if we should include code context (diffs)\\n     const isPRContext = prContextInfo.isPRContext === true;\\n-    // In PR context, always include diffs. Otherwise check the flag.\\n-    const includeCodeContext = isPRContext || prContextInfo.includeCodeContext !== false;\\n+    const isSlackMode = prContextInfo.slackConversation !== undefined;\\n+\\n+    // Determine whether to include code context:\\n+    // - In explicit PR context (GitHub PR events), always include diffs\\n+    // - In Slack mode, default to NO code context unless explicitly requested\\n+    // - Otherwise, include code context unless explicitly disabled\\n+    let includeCodeContext: boolean;\\n+    if (isPRContext) {\\n+      includeCodeContext = true;\\n+    } else if (isSlackMode) {\\n+      // In Slack mode, only include code context if explicitly set to true\\n+      includeCodeContext = prContextInfo.includeCodeContext === true;\\n+    } else {\\n+      // Default: include unless explicitly disabled\\n+      includeCodeContext = prContextInfo.includeCodeContext !== false;\\n+    }\\n \\n     // Log the decision for transparency (debug level)\\n     if (isPRContext) {\\n       log('üîç Including full code diffs in AI context (PR mode)');\\n+    } else if (isSlackMode && !includeCodeContext) {\\n+      log('üí¨ Slack mode: excluding code diffs (use includeCodeContext: true to enable)');\\n     } else if (!includeCodeContext) {\\n       log('üìä Including only file summary in AI context (no diffs)');\\n     } else {\\n@@ -1537,7 +1566,15 @@ ${'='.repeat(60)}\\n     debugInfo?: AIDebugInfo,\\n     _checkName?: string,\\n     providedSessionId?: string\\n-  ): Promise<{ response: string; effectiveSchema?: string }> {\\n+  ): Promise<{ response: string; effectiveSchema?: string; sessionId: string }> {\\n+    // Derive a stable session ID for this call so the engine can reuse it later\\n+    const sessionId =\\n+      providedSessionId ||\\n+      (() => {\\n+        const timestamp = new Date().toISOString();\\n+        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n+      })();\\n+\\n     // Handle mock model/provider\\n     if (this.config.model === 'mock' || this.config.provider === 'mock') {\\n       const inJest = !!process.env.JEST_WORKER_ID;\\n@@ -1545,19 +1582,17 @@ ${'='.repeat(60)}\\n       if (!inJest) {\\n         // Fast path for CLI/integration: synthesize a mock response without invoking ProbeAgent\\n         const response = await this.generateMockResponse(prompt, _checkName, schema);\\n-        return { response, effectiveSchema: typeof schema === 'object' ? 'custom' : schema };\\n+        return {\\n+          response,\\n+          effectiveSchema: typeof schema === 'object' ? 'custom' : schema,\\n+          sessionId,\\n+        };\\n       }\\n       // In unit tests, still invoke ProbeAgent so tests can assert on options (schema) passed in\\n       // Fall through to normal flow below\\n     }\\n \\n     // Create ProbeAgent instance with proper options\\n-    const sessionId =\\n-      providedSessionId ||\\n-      (() => {\\n-        const timestamp = new Date().toISOString();\\n-        return `visor-${timestamp.replace(/[:.]/g, '-')}-${_checkName || 'unknown'}`;\\n-      })();\\n \\n     log('ü§ñ Creating ProbeAgent for AI review...');\\n     log(`üÜî Session ID: ${sessionId}`);\\n@@ -1674,6 +1709,35 @@ ${'='.repeat(60)}\\n         (options as any).bashConfig = this.config.bashConfig;\\n       }\\n \\n+      // Pass completion prompt for post-completion validation/review\\n+      if (this.config.completionPrompt !== undefined) {\\n+        (options as any).completionPrompt = this.config.completionPrompt;\\n+      }\\n+\\n+      // Propagate workspace / allowed folders to ProbeAgent so that tools\\n+      // operate inside the isolated workspace and project checkouts instead\\n+      // of the Visor repository root.\\n+      try {\\n+        const cfgAny: any = this.config as any;\\n+        const allowedFolders = cfgAny.allowedFolders as string[] | undefined;\\n+        const workspacePath =\\n+          cfgAny.workspacePath || cfgAny.path || (Array.isArray(allowedFolders) && allowedFolders[0]);\\n+        if (Array.isArray(allowedFolders) && allowedFolders.length > 0) {\\n+          (options as any).allowedFolders = allowedFolders;\\n+          if (!options.path && workspacePath) {\\n+            (options as any).path = workspacePath;\\n+          }\\n+          log(`üóÇÔ∏è ProbeAgent workspace config:`);\\n+          log(`   path (cwd): ${(options as any).path}`);\\n+          log(`   allowedFolders[0]: ${allowedFolders[0]}`);\\n+        } else if (workspacePath) {\\n+          (options as any).path = workspacePath;\\n+          log(`üóÇÔ∏è ProbeAgent path: ${workspacePath} (no allowedFolders)`);\\n+        }\\n+      } catch {\\n+        // Best-effort only; fall back to ProbeAgent defaults on error.\\n+      }\\n+\\n       // Add provider-specific options if configured\\n       if (this.config.provider) {\\n         // Map claude-code to anthropic for ProbeAgent compatibility\\n@@ -2037,7 +2101,7 @@ ${'='.repeat(60)}\\n         log(`üîß Debug: Registered AI session for potential reuse: ${sessionId}`);\\n       }\\n \\n-      return { response, effectiveSchema };\\n+      return { response, effectiveSchema, sessionId };\\n     } catch (error) {\\n       console.error('‚ùå ProbeAgent failed:', error);\\n       throw new Error(\\n@@ -2191,13 +2255,23 @@ ${'='.repeat(60)}\\n         // For other schemas (code-review, etc.), extract and parse JSON with boundary detection\\n         log('üîç Extracting JSON from AI response...');\\n \\n-        // Try direct parsing first - if AI returned pure JSON\\n+        // Sanitize response: strip BOM, zero-width chars, and other invisible characters\\n+        // that can cause JSON parsing to fail even when the text looks valid\\n+        const sanitizedResponse = response\\n+          .replace(/^\\\\uFEFF/, '') // BOM\\n+          .replace(/[\\\\u200B-\\\\u200D\\\\uFEFF\\\\u00A0]/g, '') // Zero-width chars, NBSP\\n+          .replace(/[\\\\x00-\\\\x08\\\\x0B\\\\x0C\\\\x0E-\\\\x1F\\\\x7F]/g, '') // Control chars (except \\\\t \\\\n \\\\r)\\n+          .trim();\\n+\\n+        // Try direct JSON parsing - no bracket-matching extraction\\n+        // JSON validation is offloaded to Probe agent when schema is provided\\n         try {\\n-          reviewData = JSON.parse(response.trim());\\n+          reviewData = JSON.parse(sanitizedResponse);\\n           log('‚úÖ Successfully parsed direct JSON response');\\n           if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-        } catch {\\n-          log('üîç Direct parsing failed, trying to extract JSON from response...');\\n+        } catch (parseErr) {\\n+          const errMsg = parseErr instanceof Error ? parseErr.message : String(parseErr);\\n+          log(`üîç Direct JSON parsing failed: ${errMsg}`);\\n \\n           // If the response starts with \\\"I cannot\\\" or similar, it's likely a refusal\\n           if (\\n@@ -2210,66 +2284,16 @@ ${'='.repeat(60)}\\n             };\\n           }\\n \\n-          // Try to extract JSON using improved method with proper bracket matching\\n-          const jsonString = this.extractJsonFromResponse(response);\\n-\\n-          if (jsonString) {\\n-            try {\\n-              reviewData = JSON.parse(jsonString);\\n-              log('‚úÖ Successfully parsed extracted JSON');\\n-              if (debugInfo) debugInfo.jsonParseSuccess = true;\\n-            } catch {\\n-              log('üîß Extracted JSON parsing failed, falling back to plain text handling...');\\n-\\n-              // Check if response is plain text and doesn't contain structured data\\n-              if (!response.includes('{') && !response.includes('}')) {\\n-                log('üîß Plain text response detected, creating structured fallback...');\\n-\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              } else {\\n-                // Fallback: treat the entire response as an issue\\n-                log('üîß Creating fallback response from non-JSON content...');\\n-                reviewData = {\\n-                  issues: [\\n-                    {\\n-                      file: 'AI_RESPONSE',\\n-                      line: 1,\\n-                      ruleId: 'ai/raw_response',\\n-                      message: response,\\n-                      severity: 'info',\\n-                      category: 'documentation',\\n-                    },\\n-                  ],\\n-                };\\n-              }\\n-            }\\n-          } else {\\n-            // No JSON found at all - treat as plain text response\\n-            log('üîß No JSON found in response, treating as plain text...');\\n-            reviewData = {\\n-              issues: [\\n-                {\\n-                  file: 'AI_RESPONSE',\\n-                  line: 1,\\n-                  ruleId: 'ai/raw_response',\\n-                  message: response,\\n-                  severity: 'info',\\n-                  category: 'documentation',\\n-                },\\n-              ],\\n-            };\\n-          }\\n+          // Not valid JSON - treat entire response as text output\\n+          // This allows Probe (or other AI providers) to handle JSON validation\\n+          // and avoids false positives from bracket-matching (e.g., mermaid diagrams)\\n+          log('üîß Treating response as plain text (no JSON extraction)');\\n+          const trimmed = response.trim();\\n+          return {\\n+            issues: [],\\n+            output: { text: trimmed },\\n+            debug: debugInfo,\\n+          };\\n         }\\n       }\\n \\n@@ -2457,80 +2481,6 @@ ${'='.repeat(60)}\\n     }\\n   }\\n \\n-  /**\\n-   * Extract JSON from a response that might contain surrounding text\\n-   * Uses proper bracket matching to find valid JSON objects or arrays\\n-   */\\n-  private extractJsonFromResponse(response: string): string | null {\\n-    const text = response.trim();\\n-\\n-    // Try to find JSON objects first (higher priority)\\n-    let bestJson = this.findJsonWithBracketMatching(text, '{', '}');\\n-\\n-    // If no object found, try arrays\\n-    if (!bestJson) {\\n-      bestJson = this.findJsonWithBracketMatching(text, '[', ']');\\n-    }\\n-\\n-    return bestJson;\\n-  }\\n-\\n-  /**\\n-   * Find JSON with proper bracket matching to avoid false positives\\n-   */\\n-  private findJsonWithBracketMatching(\\n-    text: string,\\n-    openChar: string,\\n-    closeChar: string\\n-  ): string | null {\\n-    const firstIndex = text.indexOf(openChar);\\n-    if (firstIndex === -1) return null;\\n-\\n-    let depth = 0;\\n-    let inString = false;\\n-    let escaping = false;\\n-\\n-    for (let i = firstIndex; i < text.length; i++) {\\n-      const char = text[i];\\n-\\n-      if (escaping) {\\n-        escaping = false;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\\\\\\' && inString) {\\n-        escaping = true;\\n-        continue;\\n-      }\\n-\\n-      if (char === '\\\"' && !escaping) {\\n-        inString = !inString;\\n-        continue;\\n-      }\\n-\\n-      if (!inString) {\\n-        if (char === openChar) {\\n-          depth++;\\n-        } else if (char === closeChar) {\\n-          depth--;\\n-          if (depth === 0) {\\n-            // Found matching closing bracket\\n-            const candidate = text.substring(firstIndex, i + 1);\\n-            try {\\n-              JSON.parse(candidate); // Validate it's actually valid JSON\\n-              return candidate;\\n-            } catch {\\n-              // This wasn't valid JSON, keep looking\\n-              break;\\n-            }\\n-          }\\n-        }\\n-      }\\n-    }\\n-\\n-    return null;\\n-  }\\n-\\n   /**\\n    * Generate mock response for testing\\n    */\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli-main.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/src/cli-main.ts b/src/cli-main.ts\\nindex 7dbef501..e671fbd8 100644\\n--- a/src/cli-main.ts\\n+++ b/src/cli-main.ts\\n@@ -158,6 +158,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n   }\\n   const only = getArg('--only');\\n   const bail = hasFlag('--bail');\\n+  const noMocks = hasFlag('--no-mocks');\\n   const listOnly = hasFlag('--list');\\n   const validateOnly = hasFlag('--validate');\\n   const progress = (getArg('--progress') as 'compact' | 'detailed' | undefined) || 'compact';\\n@@ -224,6 +225,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       const agg = await runSuites(multiFiles, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallelSuites: maxParallelSuites || Math.max(1, require('os').cpus()?.length || 2),\\n         maxParallel,\\n         promptMaxChars,\\n@@ -340,6 +342,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n       runRes = await runner.runCases(tpath, suite, {\\n         only,\\n         bail,\\n+        noMocks,\\n         maxParallel,\\n         promptMaxChars,\\n         engineMode: 'state-machine',\\n@@ -609,6 +612,9 @@ export async function main(): Promise<void> {\\n     // Set environment variables early for proper logging in all modules\\n     process.env.VISOR_OUTPUT_FORMAT = options.output;\\n     process.env.VISOR_DEBUG = options.debug ? 'true' : 'false';\\n+    if (options.keepWorkspace) {\\n+      process.env.VISOR_KEEP_WORKSPACE = 'true';\\n+    }\\n     // Configure centralized logger\\n     configureLoggerFromCli({\\n       output: options.output,\\n@@ -716,6 +722,31 @@ export async function main(): Promise<void> {\\n       const threads = slackAny.threads || 'any';\\n       const allow = Array.isArray(slackAny.channel_allowlist) ? slackAny.channel_allowlist : [];\\n       const appToken = slackAny.app_token || process.env.SLACK_APP_TOKEN;\\n+\\n+      // Initialize telemetry for Slack mode (normally done later for CLI runs).\\n+      if ((config as any)?.telemetry) {\\n+        const t = (config as any).telemetry as {\\n+          enabled?: boolean;\\n+          sink?: 'otlp' | 'file' | 'console';\\n+          file?: { dir?: string; ndjson?: boolean };\\n+          tracing?: { auto_instrumentations?: boolean; trace_report?: { enabled?: boolean } };\\n+        };\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true' || !!t?.enabled,\\n+          sink:\\n+            (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || t?.sink || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR || t?.file?.dir, ndjson: !!t?.file?.ndjson },\\n+          autoInstrument: !!t?.tracing?.auto_instrumentations,\\n+          traceReport: !!t?.tracing?.trace_report?.enabled,\\n+        });\\n+      } else {\\n+        await initTelemetry({\\n+          enabled: process.env.VISOR_TELEMETRY_ENABLED === 'true',\\n+          sink: (process.env.VISOR_TELEMETRY_SINK as 'otlp' | 'file' | 'console') || 'file',\\n+          file: { dir: process.env.VISOR_TRACE_DIR },\\n+        });\\n+      }\\n+\\n       const runner = new SlackSocketRunner(engine, config, {\\n         appToken,\\n         endpoint,\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli.ts\",\"additions\":5,\"deletions\":1,\"changes\":6,\"patch\":\"diff --git a/src/cli.ts b/src/cli.ts\\nindex 134429b2..b1c46213 100644\\n--- a/src/cli.ts\\n+++ b/src/cli.ts\\n@@ -76,6 +76,7 @@ export class CLI {\\n         parseInt(value, 10)\\n       )\\n       .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText())\\n       .exitOverride(); // Prevent automatic process.exit for better error handling\\n \\n@@ -154,6 +155,7 @@ export class CLI {\\n           parseInt(value, 10)\\n         )\\n         .option('--message <text>', 'Message for human-input checks (inline text or file path)')\\n+        .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n         .allowUnknownOption(false)\\n         .allowExcessArguments(false) // Don't allow positional arguments\\n         .addHelpText('after', this.getExamplesText())\\n@@ -224,6 +226,7 @@ export class CLI {\\n         message: options.message,\\n         githubV2: false,\\n         slack: Boolean(options.slack),\\n+        keepWorkspace: Boolean(options.keepWorkspace),\\n       };\\n     } catch (error: unknown) {\\n       // Handle commander.js exit overrides for help/version ONLY\\n@@ -348,6 +351,7 @@ export class CLI {\\n       .option('--debug-port <port>', 'Port for debug server (default: 3456)', value =>\\n         parseInt(value, 10)\\n       )\\n+      .option('--keep-workspace', 'Keep workspace folders after execution (for debugging)')\\n       .addHelpText('after', this.getExamplesText());\\n \\n     // Get the basic help and append examples manually if addHelpText doesn't work\\n\",\"status\":\"modified\"},{\"filename\":\"src/config.ts\",\"additions\":67,\"deletions\":17,\"changes\":84,\"patch\":\"diff --git a/src/config.ts b/src/config.ts\\nindex 407efa2d..02abe9df 100644\\n--- a/src/config.ts\\n+++ b/src/config.ts\\n@@ -20,6 +20,7 @@ import { ConfigLoader, ConfigLoaderOptions } from './utils/config-loader';\\n import { ConfigMerger } from './utils/config-merger';\\n import Ajv from 'ajv';\\n import addFormats from 'ajv-formats';\\n+import { validateJsSyntax } from './utils/sandbox';\\n \\n /**\\n  * Valid event triggers for checks\\n@@ -477,16 +478,29 @@ export class ConfigManager {\\n \\n     logger.info(`Registered workflow '${workflowId}' for standalone execution`);\\n \\n-    // Create a COMPLETELY NEW visor config with ONLY the test checks\\n-    // This prevents any workflow fields from leaking into the config\\n-    const visorConfig: Partial<VisorConfig> = {\\n+    // For standalone workflow testing, use the workflow's steps as checks\\n+    // This allows the test runner to execute the workflow steps directly\\n+    const workflowSteps = workflowData.steps || {};\\n+\\n+    // Create a config that includes the workflow steps as checks,\\n+    // plus the tests section and workflow metadata for output computation\\n+    const visorConfig: Partial<VisorConfig> & { tests?: any; outputs?: any; inputs?: any } = {\\n       version: '1.0',\\n-      steps: tests,\\n-      checks: tests, // Backward compatibility\\n+      steps: workflowSteps,\\n+      checks: workflowSteps,\\n+      tests: tests, // Preserve test harness config (may be empty if stripped by test runner)\\n     };\\n \\n-    logger.debug(`Standalone workflow config has ${Object.keys(tests).length} test checks`);\\n-    logger.debug(`Test check names: ${Object.keys(tests).join(', ')}`);\\n+    // Preserve workflow metadata for output computation during tests\\n+    if (workflowData.outputs) {\\n+      visorConfig.outputs = workflowData.outputs;\\n+    }\\n+    if (workflowData.inputs) {\\n+      visorConfig.inputs = workflowData.inputs;\\n+    }\\n+\\n+    logger.debug(`Standalone workflow config has ${Object.keys(workflowSteps).length} workflow steps as checks`);\\n+    logger.debug(`Workflow step names: ${Object.keys(workflowSteps).join(', ')}`);\\n     logger.debug(`Config keys after conversion: ${Object.keys(visorConfig).join(', ')}`);\\n \\n     return visorConfig;\\n@@ -982,26 +996,29 @@ export class ConfigManager {\\n \\n     // Validate reuse_ai_session configuration\\n     if (checkConfig.reuse_ai_session !== undefined) {\\n-      const isString = typeof checkConfig.reuse_ai_session === 'string';\\n-      const isBoolean = typeof checkConfig.reuse_ai_session === 'boolean';\\n+      const reuseValue = checkConfig.reuse_ai_session as unknown;\\n+      const isString = typeof reuseValue === 'string';\\n+      const isBoolean = typeof reuseValue === 'boolean';\\n+      const isSelf = reuseValue === 'self';\\n \\n       if (!isString && !isBoolean) {\\n         errors.push({\\n           field: `checks.${checkName}.reuse_ai_session`,\\n           message: `Invalid reuse_ai_session value for \\\"${checkName}\\\": must be string (check name) or boolean`,\\n-          value: checkConfig.reuse_ai_session,\\n+          value: reuseValue,\\n         });\\n-      } else if (isString) {\\n-        // When reuse_ai_session is a string, it must refer to a valid check\\n-        const targetCheckName = checkConfig.reuse_ai_session as string;\\n+      } else if (isString && !isSelf) {\\n+        // When reuse_ai_session is a string (other than the special 'self' value),\\n+        // it must refer to a valid check name\\n+        const targetCheckName = reuseValue as string;\\n         if (!config?.checks || !config.checks[targetCheckName]) {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" references non-existent check \\\"${targetCheckName}\\\" for session reuse`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n-      } else if (checkConfig.reuse_ai_session === true) {\\n+      } else if (reuseValue === true) {\\n         // When reuse_ai_session is true, depends_on must be specified and non-empty\\n         if (\\n           !checkConfig.depends_on ||\\n@@ -1011,7 +1028,7 @@ export class ConfigManager {\\n           errors.push({\\n             field: `checks.${checkName}.reuse_ai_session`,\\n             message: `Check \\\"${checkName}\\\" has reuse_ai_session=true but missing or empty depends_on. Session reuse requires dependency on another check.`,\\n-            value: checkConfig.reuse_ai_session,\\n+            value: reuseValue,\\n           });\\n         }\\n       }\\n@@ -1076,6 +1093,39 @@ export class ConfigManager {\\n         });\\n       }\\n     }\\n+\\n+    // Validate JavaScript syntax in transform_js and script content\\n+    try {\\n+      // Validate transform_js if present\\n+      const transformJs = (checkConfig as any).transform_js;\\n+      if (typeof transformJs === 'string' && transformJs.trim().length > 0) {\\n+        const result = validateJsSyntax(transformJs);\\n+        if (!result.valid) {\\n+          errors.push({\\n+            field: `checks.${checkName}.transform_js`,\\n+            message: `JavaScript syntax error in \\\"${checkName}\\\" transform_js: ${result.error}`,\\n+            value: transformJs.slice(0, 100) + (transformJs.length > 100 ? '...' : ''),\\n+          });\\n+        }\\n+      }\\n+\\n+      // Validate script content if type is 'script'\\n+      if (checkConfig.type === 'script') {\\n+        const content = (checkConfig as any).content;\\n+        if (typeof content === 'string' && content.trim().length > 0) {\\n+          const result = validateJsSyntax(content);\\n+          if (!result.valid) {\\n+            errors.push({\\n+              field: `checks.${checkName}.content`,\\n+              message: `JavaScript syntax error in \\\"${checkName}\\\" script: ${result.error}`,\\n+              value: content.slice(0, 100) + (content.length > 100 ? '...' : ''),\\n+            });\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Syntax validation is best-effort; don't fail the whole config on validation errors\\n+    }\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/failure-condition-evaluator.ts\",\"additions\":25,\"deletions\":9,\"changes\":34,\"patch\":\"diff --git a/src/failure-condition-evaluator.ts b/src/failure-condition-evaluator.ts\\nindex 3b54a015..5800df0a 100644\\n--- a/src/failure-condition-evaluator.ts\\n+++ b/src/failure-condition-evaluator.ts\\n@@ -137,6 +137,8 @@ export class FailureConditionEvaluator {\\n       previousResults?: Map<string, ReviewSummary>;\\n       authorAssociation?: string;\\n       workflowInputs?: Record<string, unknown>;\\n+      /** Current step's output for guarantee evaluation */\\n+      output?: unknown;\\n     }\\n   ): Promise<boolean> {\\n     // Build context for if evaluation\\n@@ -177,10 +179,10 @@ export class FailureConditionEvaluator {\\n       // Workflow inputs (for workflows)\\n       inputs: contextData?.workflowInputs || {},\\n \\n-      // Required output property (empty for if conditions)\\n-      output: {\\n-        issues: [],\\n-      },\\n+      // Output property: use provided output for guarantee evaluation, or empty for if conditions\\n+      output: contextData?.output !== undefined && contextData.output !== null && typeof contextData.output === 'object'\\n+        ? (contextData.output as Record<string, unknown>)\\n+        : { issues: [] },\\n       // Author association (used by permission helpers)\\n       authorAssociation: contextData?.authorAssociation,\\n \\n@@ -213,10 +215,10 @@ export class FailureConditionEvaluator {\\n               m && typeof m.get === 'function' ? m.get('all_valid', 'fact-validation') : undefined;\\n             memStr = ` mem.fact-validation.all_valid=${String(v)}`;\\n           } catch {}\\n+          // Debug if-eval output (only when VISOR_DEBUG enabled)\\n+          const outputKeys = Object.keys(context.outputs || {});\\n           console.error(\\n-            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" env.ENABLE_FACT_VALIDATION=${String(\\n-              (envMap as any).ENABLE_FACT_VALIDATION\\n-            )} event=${context.event?.event_name} result=${String(res)}${memStr}`\\n+            `[if-eval] check=${checkName} expr=\\\"${expression}\\\" result=${String(res)} outputKeys=[${outputKeys.join(',')}]`\\n           );\\n         }\\n       } catch {}\\n@@ -480,7 +482,18 @@ export class FailureConditionEvaluator {\\n \\n       // Extract context variables\\n       const output = context.output || {};\\n-      const issues = output.issues || [];\\n+      // Ensure issues is an array - it might be a JSON string from workflow outputs\\n+      let issues = output.issues || [];\\n+      if (typeof issues === 'string') {\\n+        try {\\n+          issues = JSON.parse(issues);\\n+        } catch {\\n+          issues = [];\\n+        }\\n+      }\\n+      if (!Array.isArray(issues)) {\\n+        issues = [];\\n+      }\\n \\n       // Backward compatibility: provide metadata for transition period\\n       // TODO: Remove after all configurations are updated\\n@@ -518,6 +531,7 @@ export class FailureConditionEvaluator {\\n       const event = context.event || 'manual';\\n       const env = context.env || {};\\n       const outputs = context.outputs || {};\\n+      const inputs = context.inputs || {};\\n       const debugData = context.debug || null;\\n \\n       // Get memory store and create accessor for fail_if expressions\\n@@ -555,6 +569,7 @@ export class FailureConditionEvaluator {\\n         filesCount,\\n         event,\\n         env,\\n+        inputs,\\n         // Helper functions\\n         contains,\\n         startsWith,\\n@@ -621,6 +636,7 @@ export class FailureConditionEvaluator {\\n             filesCount,\\n             event,\\n             env,\\n+            inputs,\\n             // Helpers\\n             contains,\\n             startsWith,\\n\",\"status\":\"modified\"},{\"filename\":\"src/frontends/slack-frontend.ts\",\"additions\":107,\"deletions\":6,\"changes\":113,\"patch\":\"diff --git a/src/frontends/slack-frontend.ts b/src/frontends/slack-frontend.ts\\nindex 97161a7d..41735aa2 100644\\n--- a/src/frontends/slack-frontend.ts\\n+++ b/src/frontends/slack-frontend.ts\\n@@ -1,6 +1,32 @@\\n+/**\\n+ * Slack Frontend for Visor workflows.\\n+ *\\n+ * Features:\\n+ * - Posts AI replies to Slack threads\\n+ * - Converts Markdown to Slack mrkdwn format\\n+ * - Renders mermaid diagrams to PNG and uploads as images\\n+ * - Manages üëÄ/üëç reactions for acknowledgement\\n+ * - Handles human input prompts via prompt-state\\n+ *\\n+ * Mermaid Diagram Rendering:\\n+ * - Detects ```mermaid code blocks in AI responses\\n+ * - Renders to PNG using @mermaid-js/mermaid-cli (mmdc)\\n+ * - Uploads rendered images to Slack thread\\n+ * - Replaces mermaid blocks with \\\"_(See diagram above)_\\\" placeholder\\n+ *\\n+ * Requirements for mermaid rendering:\\n+ * - Node.js and npx in PATH\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ * - On Linux: apt-get install chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ */\\n import type { Frontend, FrontendContext } from './host';\\n import { SlackClient } from '../slack/client';\\n-import { formatSlackText } from '../slack/markdown';\\n+import {\\n+  formatSlackText,\\n+  extractMermaidDiagrams,\\n+  renderMermaidToPng,\\n+  replaceMermaidBlocks,\\n+} from '../slack/markdown';\\n \\n type SlackFrontendConfig = {\\n   defaultChannel?: string;\\n@@ -284,6 +310,9 @@ export class SlackFrontend implements Frontend {\\n       const isLogChat = providerType === 'log' && checkCfg.group === 'chat';\\n       if (!isAi && !isLogChat) return;\\n \\n+      // Skip internal steps - they're intermediate processing and shouldn't post to Slack\\n+      if (checkCfg.criticality === 'internal') return;\\n+\\n       // For AI checks, only post when using simple/unstructured schemas (or none).\\n       if (isAi) {\\n         const schema = checkCfg.schema;\\n@@ -316,6 +345,13 @@ export class SlackFrontend implements Frontend {\\n         ) {\\n           text = (result as any).content.trim();\\n         }\\n+      } else if (isLogChat && typeof (result as any)?.logOutput === 'string') {\\n+        // For log-based chat checks, render the formatted log output as the\\n+        // Slack message when no structured text field is present.\\n+        const raw = (result as any).logOutput;\\n+        if (raw.trim().length > 0) {\\n+          text = raw.trim();\\n+        }\\n       } else if (isAi && showRawOutput && out !== undefined) {\\n         try {\\n           text = JSON.stringify(out, null, 2);\\n@@ -325,13 +361,78 @@ export class SlackFrontend implements Frontend {\\n       }\\n       if (!text) return;\\n \\n-      const formattedText = formatSlackText(text);\\n+      // Extract and render mermaid diagrams before posting\\n+      const diagrams = extractMermaidDiagrams(text);\\n+      let processedText = text;\\n+\\n+      if (diagrams.length > 0) {\\n+        try {\\n+          ctx.logger.info(\\n+            `[slack-frontend] found ${diagrams.length} mermaid diagram(s) to render for ${checkId}`\\n+          );\\n+        } catch {}\\n+\\n+        // Render and upload each diagram\\n+        const uploadedCount: number[] = [];\\n+        for (let i = 0; i < diagrams.length; i++) {\\n+          const diagram = diagrams[i];\\n+          try {\\n+            ctx.logger.info(`[slack-frontend] rendering mermaid diagram ${i + 1}...`);\\n+            const pngBuffer = await renderMermaidToPng(diagram.code);\\n+            if (pngBuffer) {\\n+              ctx.logger.info(\\n+                `[slack-frontend] rendered diagram ${i + 1}, size=${pngBuffer.length} bytes, uploading...`\\n+              );\\n+              const filename = `diagram-${i + 1}.png`;\\n+              const uploadResult = await slack.files.uploadV2({\\n+                content: pngBuffer,\\n+                filename,\\n+                channel,\\n+                thread_ts: threadTs,\\n+                title: `Diagram ${i + 1}`,\\n+              });\\n+              if (uploadResult.ok) {\\n+                uploadedCount.push(i);\\n+                ctx.logger.info(`[slack-frontend] uploaded mermaid diagram ${i + 1} to ${channel}`);\\n+              } else {\\n+                ctx.logger.warn(`[slack-frontend] upload failed for diagram ${i + 1}`);\\n+              }\\n+            } else {\\n+              ctx.logger.warn(\\n+                `[slack-frontend] mermaid rendering returned null for diagram ${i + 1} (mmdc failed or not installed)`\\n+              );\\n+            }\\n+          } catch (e) {\\n+            ctx.logger.warn(\\n+              `[slack-frontend] failed to render/upload mermaid diagram ${i + 1}: ${\\n+                e instanceof Error ? e.message : String(e)\\n+              }`\\n+            );\\n+          }\\n+        }\\n+\\n+        // Replace mermaid blocks with placeholder text\\n+        if (uploadedCount.length > 0) {\\n+          processedText = replaceMermaidBlocks(text, diagrams, (idx) =>\\n+            uploadedCount.includes(idx) ? '_(See diagram above)_' : '_(Diagram rendering failed)_'\\n+          );\\n+        }\\n+      }\\n+\\n+      const formattedText = formatSlackText(processedText);\\n       await slack.chat.postMessage({ channel, text: formattedText, thread_ts: threadTs });\\n+      ctx.logger.info(\\n+        `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+      );\\n+    } catch (outerErr) {\\n+      // Log errors instead of silently swallowing them\\n       try {\\n-        ctx.logger.info(\\n-          `[slack-frontend] posted AI reply for ${checkId} to ${channel} thread=${threadTs}`\\n+        ctx.logger.warn(\\n+          `[slack-frontend] maybePostDirectReply failed for ${checkId}: ${\\n+            outerErr instanceof Error ? outerErr.message : String(outerErr)\\n+          }`\\n         );\\n       } catch {}\\n-    } catch {}\\n+    }\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/generated/config-schema.ts\",\"additions\":233,\"deletions\":13,\"changes\":246,\"patch\":\"diff --git a/src/generated/config-schema.ts b/src/generated/config-schema.ts\\nindex b3caca4f..ecab390d 100644\\n--- a/src/generated/config-schema.ts\\n+++ b/src/generated/config-schema.ts\\n@@ -56,6 +56,20 @@ export const configSchema = {\\n           },\\n           description: 'Import workflow definitions from external files or URLs',\\n         },\\n+        inputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowInput',\\n+          },\\n+          description: 'Workflow inputs (for standalone reusable workflows)',\\n+        },\\n+        outputs: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/WorkflowOutput',\\n+          },\\n+          description: 'Workflow outputs (for standalone reusable workflows)',\\n+        },\\n         steps: {\\n           $ref: '#/definitions/Record%3Cstring%2CCheckConfig%3E',\\n           description: 'Step configurations (recommended)',\\n@@ -67,7 +81,7 @@ export const configSchema = {\\n         },\\n         output: {\\n           $ref: '#/definitions/OutputConfig',\\n-          description: 'Output configuration',\\n+          description: 'Output configuration (optional - defaults provided)',\\n         },\\n         http_server: {\\n           $ref: '#/definitions/HttpServerConfig',\\n@@ -139,8 +153,16 @@ export const configSchema = {\\n           },\\n           description: 'Optional integrations: event-driven frontends (e.g., ndjson-sink, github)',\\n         },\\n+        workspace: {\\n+          $ref: '#/definitions/WorkspaceConfig',\\n+          description: 'Workspace isolation configuration for sandboxed execution',\\n+        },\\n+        slack: {\\n+          $ref: '#/definitions/SlackConfig',\\n+          description: 'Slack configuration',\\n+        },\\n       },\\n-      required: ['output', 'version'],\\n+      required: ['version'],\\n       patternProperties: {\\n         '^x-': {},\\n       },\\n@@ -243,6 +265,63 @@ export const configSchema = {\\n         type: 'string',\\n       },\\n     },\\n+    WorkflowInput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Input parameter name',\\n+        },\\n+        schema: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'JSON Schema for the input',\\n+        },\\n+        required: {\\n+          type: 'boolean',\\n+          description: 'Whether this input is required',\\n+        },\\n+        default: {\\n+          description: 'Default value if not provided',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow input definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    WorkflowOutput: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Output name',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Human-readable description',\\n+        },\\n+        value: {\\n+          type: 'string',\\n+          description: 'Value using Liquid template syntax (references step outputs)',\\n+        },\\n+        value_js: {\\n+          type: 'string',\\n+          description: 'Value using JavaScript expression (alternative to value)',\\n+        },\\n+      },\\n+      required: ['name'],\\n+      additionalProperties: false,\\n+      description: 'Workflow output definition for standalone reusable workflows',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n     'Record<string,CheckConfig>': {\\n       type: 'object',\\n       additionalProperties: {\\n@@ -390,11 +469,19 @@ export const configSchema = {\\n           description: 'Timeout in seconds for command execution (default: 60)',\\n         },\\n         depends_on: {\\n-          type: 'array',\\n-          items: {\\n-            type: 'string',\\n-          },\\n-          description: 'Check IDs that this check depends on (optional)',\\n+          anyOf: [\\n+            {\\n+              type: 'string',\\n+            },\\n+            {\\n+              type: 'array',\\n+              items: {\\n+                type: 'string',\\n+              },\\n+            },\\n+          ],\\n+          description:\\n+            'Check IDs that this check depends on (optional). Accepts single string or array.',\\n         },\\n         group: {\\n           type: 'string',\\n@@ -653,13 +740,93 @@ export const configSchema = {\\n           description: 'Arguments/inputs for the workflow',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Override specific step configurations in the workflow',\\n         },\\n         output_mapping: {\\n           $ref: '#/definitions/Record%3Cstring%2Cstring%3E',\\n           description: 'Map workflow outputs to check outputs',\\n         },\\n+        workflow_inputs: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'Alias for args - workflow inputs (backward compatibility)',\\n+        },\\n+        config: {\\n+          type: 'string',\\n+          description:\\n+            'Config file path - alternative to workflow ID (loads a Visor config file as workflow)',\\n+        },\\n+        workflow_overrides: {\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n+          description: 'Alias for overrides - workflow step overrides (backward compatibility)',\\n+        },\\n+        ref: {\\n+          type: 'string',\\n+          description: 'Git reference to checkout (branch, tag, commit SHA) - supports templates',\\n+        },\\n+        repository: {\\n+          type: 'string',\\n+          description: 'Repository URL or owner/repo format (defaults to current repository)',\\n+        },\\n+        token: {\\n+          type: 'string',\\n+          description: 'GitHub token for private repositories (defaults to GITHUB_TOKEN env)',\\n+        },\\n+        fetch_depth: {\\n+          type: 'number',\\n+          description: 'Number of commits to fetch (0 for full history, default: 1)',\\n+        },\\n+        fetch_tags: {\\n+          type: 'boolean',\\n+          description: 'Whether to fetch tags (default: false)',\\n+        },\\n+        submodules: {\\n+          anyOf: [\\n+            {\\n+              type: 'boolean',\\n+            },\\n+            {\\n+              type: 'string',\\n+              const: 'recursive',\\n+            },\\n+          ],\\n+          description: \\\"Checkout submodules: false, true, or 'recursive'\\\",\\n+        },\\n+        working_directory: {\\n+          type: 'string',\\n+          description: 'Working directory for the checkout (defaults to temp directory)',\\n+        },\\n+        use_worktree: {\\n+          type: 'boolean',\\n+          description: 'Use git worktree for efficient parallel checkouts (default: true)',\\n+        },\\n+        clean: {\\n+          type: 'boolean',\\n+          description: 'Clean the working directory before checkout (default: true)',\\n+        },\\n+        sparse_checkout: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description: 'Sparse checkout paths - only checkout specific directories/files',\\n+        },\\n+        lfs: {\\n+          type: 'boolean',\\n+          description: 'Enable Git LFS (Large File Storage)',\\n+        },\\n+        clone_timeout_ms: {\\n+          type: 'number',\\n+          description: 'Timeout in ms for cloning the bare repository (default: 300000 = 5 min)',\\n+        },\\n+        cleanup_on_failure: {\\n+          type: 'boolean',\\n+          description: 'Clean up worktree on failure (default: true)',\\n+        },\\n+        persist_worktree: {\\n+          type: 'boolean',\\n+          description: 'Keep worktree after workflow completion (default: false)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'Configuration for a single check',\\n@@ -794,6 +961,11 @@ export const configSchema = {\\n           $ref: '#/definitions/BashConfig',\\n           description: 'Advanced bash command execution configuration',\\n         },\\n+        completion_prompt: {\\n+          type: 'string',\\n+          description:\\n+            'Completion prompt for post-completion validation/review (runs after attempt_completion)',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'AI provider configuration',\\n@@ -1223,7 +1395,7 @@ export const configSchema = {\\n           description: 'Custom output name (defaults to workflow name)',\\n         },\\n         overrides: {\\n-          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E%3E',\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E%3E',\\n           description: 'Step overrides',\\n         },\\n         output_mapping: {\\n@@ -1238,14 +1410,14 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n-    'Record<string,Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>>':\\n+    'Record<string,Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>>':\\n       {\\n         type: 'object',\\n         additionalProperties: {\\n-          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706%3E',\\n+          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627%3E',\\n         },\\n       },\\n-    'Partial<interface-src_types_config.ts-11138-21461-src_types_config.ts-0-36706>': {\\n+    'Partial<interface-src_types_config.ts-11359-23556-src_types_config.ts-0-40627>': {\\n       type: 'object',\\n       additionalProperties: false,\\n     },\\n@@ -1811,6 +1983,54 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n+    WorkspaceConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        enabled: {\\n+          type: 'boolean',\\n+          description: 'Enable workspace isolation (default: true when config present)',\\n+        },\\n+        base_path: {\\n+          type: 'string',\\n+          description: 'Base path for workspaces (default: /tmp/visor-workspaces)',\\n+        },\\n+        cleanup_on_exit: {\\n+          type: 'boolean',\\n+          description: 'Clean up workspace on exit (default: true)',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Workspace isolation configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    SlackConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        version: {\\n+          type: 'string',\\n+          description: 'Slack API version',\\n+        },\\n+        mentions: {\\n+          type: 'string',\\n+          description: \\\"Mention handling: 'all', 'direct', etc.\\\",\\n+        },\\n+        threads: {\\n+          type: 'string',\\n+          description: \\\"Thread handling: 'required', 'optional', etc.\\\",\\n+        },\\n+        show_raw_output: {\\n+          type: 'boolean',\\n+          description: 'Show raw output in Slack responses',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Slack configuration',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n   },\\n } as const;\\n export default configSchema;\\n\",\"status\":\"modified\"},{\"filename\":\"src/index.ts\",\"additions\":5,\"deletions\":2,\"changes\":7,\"patch\":\"diff --git a/src/index.ts b/src/index.ts\\nindex 50b89df3..9621e733 100644\\n--- a/src/index.ts\\n+++ b/src/index.ts\\n@@ -628,8 +628,11 @@ function resolveDependencies(\\n \\n     // Get dependencies for this check\\n     const checkConfig = config?.checks?.[checkId];\\n+    // Normalize depends_on to array (supports string | string[])\\n+    const rawDeps = checkConfig?.depends_on;\\n+    const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     // Expand OR groups (pipe syntax) for dependency closure discovery\\n-    const dependencies = (checkConfig?.depends_on || []).flatMap(d =>\\n+    const dependencies = depsArray.flatMap((d: string) =>\\n       typeof d === 'string' && d.includes('|')\\n         ? d\\n             .split('|')\\n\",\"status\":\"modified\"},{\"filename\":\"src/liquid-extensions.ts\",\"additions\":68,\"deletions\":1,\"changes\":69,\"patch\":\"diff --git a/src/liquid-extensions.ts b/src/liquid-extensions.ts\\nindex e9efaf68..dbf9c6ed 100644\\n--- a/src/liquid-extensions.ts\\n+++ b/src/liquid-extensions.ts\\n@@ -124,6 +124,26 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     }\\n   });\\n \\n+  // Register base64 filter for encoding strings\\n+  // Usage: {{ \\\"user:password\\\" | base64 }}\\n+  liquid.registerFilter('base64', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    return Buffer.from(str).toString('base64');\\n+  });\\n+\\n+  // Register base64_decode filter for decoding base64 strings\\n+  // Usage: {{ encoded_value | base64_decode }}\\n+  liquid.registerFilter('base64_decode', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const str = String(value);\\n+    try {\\n+      return Buffer.from(str, 'base64').toString('utf-8');\\n+    } catch {\\n+      return '[Error: Invalid base64 string]';\\n+    }\\n+  });\\n+\\n   // Sanitize a label to allowed characters only: [A-Za-z0-9:/]\\n   liquid.registerFilter('safe_label', (value: unknown) => sanitizeLabel(value));\\n \\n@@ -137,6 +157,53 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n     return s.replace(/\\\\\\\\n/g, '\\\\n').replace(/\\\\\\\\r/g, '\\\\r').replace(/\\\\\\\\t/g, '\\\\t');\\n   });\\n \\n+  // JSON escape filter - escapes a string for use inside a JSON string value\\n+  // This escapes special characters like quotes, backslashes, and control characters\\n+  // Usage: \\\"jql\\\": \\\"{{ myValue | json_escape }}\\\"\\n+  liquid.registerFilter('json_escape', (value: unknown) => {\\n+    if (value == null) return '';\\n+    const s = String(value);\\n+    // Use JSON.stringify which handles all escaping, then strip the surrounding quotes\\n+    const jsonStr = JSON.stringify(s);\\n+    // Remove the first and last character (the quotes added by JSON.stringify)\\n+    return jsonStr.slice(1, -1);\\n+  });\\n+\\n+  // Shell escape filter - wraps value in single quotes with proper escaping\\n+  // Usage: {{ value | shell_escape }}\\n+  // Example: \\\"hello'world\\\" becomes \\\"'hello'\\\\''world'\\\"\\n+  // This is POSIX-compliant and safe for arbitrary text including mermaid diagrams\\n+  liquid.registerFilter('shell_escape', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    // Replace single quotes with: end quote, escaped quote, start quote\\n+    // Then wrap the entire thing in single quotes\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Alias for shell_escape\\n+  liquid.registerFilter('escape_shell', (value: unknown) => {\\n+    if (value == null) return \\\"''\\\";\\n+    const s = String(value);\\n+    return \\\"'\\\" + s.replace(/'/g, \\\"'\\\\\\\\''\\\") + \\\"'\\\";\\n+  });\\n+\\n+  // Shell escape for double quotes (less safe but sometimes needed)\\n+  // Usage: {{ value | shell_escape_double }}\\n+  // Escapes: $, `, \\\\, \\\", and !\\n+  liquid.registerFilter('shell_escape_double', (value: unknown) => {\\n+    if (value == null) return '\\\"\\\"';\\n+    const s = String(value);\\n+    // Escape characters that have special meaning inside double quotes\\n+    const escaped = s\\n+      .replace(/\\\\\\\\/g, '\\\\\\\\\\\\\\\\')  // backslash first\\n+      .replace(/\\\\$/g, '\\\\\\\\$')   // dollar sign\\n+      .replace(/`/g, '\\\\\\\\`')    // backticks\\n+      .replace(/\\\"/g, '\\\\\\\\\\\"')    // double quotes\\n+      .replace(/!/g, '\\\\\\\\!');   // history expansion\\n+    return '\\\"' + escaped + '\\\"';\\n+  });\\n+\\n   // Register author permission filters (from main)\\n   // These filters check the author's permission level; detect local mode for tests\\n   const isLocal = detectLocalMode();\\n\",\"status\":\"modified\"},{\"filename\":\"src/logger.ts\",\"additions\":31,\"deletions\":2,\"changes\":33,\"patch\":\"diff --git a/src/logger.ts b/src/logger.ts\\nindex 6148b77c..aee7136e 100644\\n--- a/src/logger.ts\\n+++ b/src/logger.ts\\n@@ -85,7 +85,36 @@ class Logger {\\n       if (this.showTimestamps) {\\n         const ts = new Date().toISOString();\\n         const lvl = level ? level : undefined;\\n-        const prefix = lvl ? `[${ts}] [${lvl}]` : `[${ts}]`;\\n+\\n+        let tsToken = `[${ts}]`;\\n+        let lvlToken = lvl ? `[${lvl}]` : '';\\n+\\n+        // Add simple ANSI colour when running in a TTY and not emitting\\n+        // JSON/SARIF. Colours are intentionally minimal and only applied\\n+        // to the prefix markers, not the full line.\\n+        if (this.isTTY && !this.isJsonLike) {\\n+          const reset = '\\\\x1b[0m';\\n+          const dim = '\\\\x1b[2m';\\n+          const colours: Record<LogLevel, string> = {\\n+            silent: '',\\n+            error: '\\\\x1b[31m', // red\\n+            warn: '\\\\x1b[33m', // yellow\\n+            info: '\\\\x1b[36m', // cyan\\n+            verbose: '\\\\x1b[35m', // magenta\\n+            debug: '\\\\x1b[90m', // bright black / gray\\n+          };\\n+\\n+          tsToken = `${dim}${tsToken}${reset}`;\\n+\\n+          if (lvl) {\\n+            const colour = colours[lvl] || '';\\n+            if (colour) {\\n+              lvlToken = `${colour}${lvlToken}${reset}`;\\n+            }\\n+          }\\n+        }\\n+\\n+        const prefix = lvl ? `${tsToken} ${lvlToken}` : tsToken;\\n         process.stderr.write(`${prefix} ${msg}\\\\n`);\\n       } else {\\n         process.stderr.write(msg + '\\\\n');\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/ai-check-provider.ts\",\"additions\":106,\"deletions\":8,\"changes\":114,\"patch\":\"diff --git a/src/providers/ai-check-provider.ts b/src/providers/ai-check-provider.ts\\nindex 53f1a350..987f2080 100644\\n--- a/src/providers/ai-check-provider.ts\\n+++ b/src/providers/ai-check-provider.ts\\n@@ -186,7 +186,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     let promptContent: string;\\n \\n@@ -204,7 +205,8 @@ export class AICheckProvider extends CheckProvider {\\n       eventContext,\\n       dependencyResults,\\n       outputHistory,\\n-      args\\n+      args,\\n+      workflowInputs\\n     );\\n   }\\n \\n@@ -335,7 +337,8 @@ export class AICheckProvider extends CheckProvider {\\n     eventContext?: Record<string, unknown>,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     outputHistory?: Map<string, unknown[]>,\\n-    args?: Record<string, unknown>\\n+    args?: Record<string, unknown>,\\n+    workflowInputs?: Record<string, unknown>\\n   ): Promise<string> {\\n     // Build outputs_raw from -raw keys (aggregate parent values)\\n     const outputsRaw: Record<string, unknown> = {};\\n@@ -535,6 +538,8 @@ export class AICheckProvider extends CheckProvider {\\n       outputs_raw: outputsRaw,\\n       // Custom arguments from on_init 'with' directive\\n       args: args || {},\\n+      // Workflow inputs (for nested workflow steps to access parent inputs like {{ inputs.context }})\\n+      inputs: workflowInputs || {},\\n     };\\n \\n     try {\\n@@ -621,7 +626,8 @@ export class AICheckProvider extends CheckProvider {\\n         console.error(`[ai-exec] step=${String((config as any).checkName || 'unknown')}`);\\n       }\\n     } catch {}\\n-    // Extract AI configuration - only set properties that are explicitly provided\\n+    // Extract AI configuration - only set properties that are explicitly provided.\\n+    // Workspace / allowedFolders will be derived below from the execution context.\\n     const aiConfig: AIReviewConfig = {};\\n \\n     // Check-level AI configuration (ai object)\\n@@ -671,6 +677,9 @@ export class AICheckProvider extends CheckProvider {\\n       if (aiAny.bashConfig !== undefined) {\\n         aiConfig.bashConfig = aiAny.bashConfig as import('../types/config').BashConfig;\\n       }\\n+      if (aiAny.completion_prompt !== undefined) {\\n+        aiConfig.completionPrompt = aiAny.completion_prompt as string;\\n+      }\\n       if (aiAny.skip_code_context !== undefined) {\\n         // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n         (aiConfig as any).skip_code_context = aiAny.skip_code_context as boolean;\\n@@ -691,6 +700,90 @@ export class AICheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Derive workspace-aware allowedFolders for ProbeAgent when workspace\\n+    // isolation is enabled. This ensures tools like search/query operate\\n+    // inside the isolated workspace (and its project symlinks) instead of\\n+    // the Visor repository root.\\n+    // Folder names are human-readable (tyk-docs, visor2) thanks to WorkspaceManager.\\n+    try {\\n+      const ctxAny: any = sessionInfo as any;\\n+      const parentCtx = ctxAny?._parentContext;\\n+      const workspace = parentCtx?.workspace;\\n+\\n+      // Enhanced debug logging for workspace propagation diagnosis\\n+      logger.debug(`[AI Provider] Workspace detection for check '${(config as any).checkName || 'unknown'}':`);\\n+      logger.debug(`[AI Provider]   sessionInfo exists: ${!!sessionInfo}`);\\n+      logger.debug(`[AI Provider]   _parentContext exists: ${!!parentCtx}`);\\n+      logger.debug(`[AI Provider]   workspace exists: ${!!workspace}`);\\n+      if (workspace) {\\n+        logger.debug(`[AI Provider]   workspace.isEnabled exists: ${typeof workspace.isEnabled === 'function'}`);\\n+        logger.debug(`[AI Provider]   workspace.isEnabled(): ${typeof workspace.isEnabled === 'function' ? workspace.isEnabled() : 'N/A'}`);\\n+        const projectCount = typeof workspace.listProjects === 'function' ? workspace.listProjects()?.length : 'N/A';\\n+        logger.debug(`[AI Provider]   workspace.listProjects() count: ${projectCount}`);\\n+      }\\n+\\n+      if (workspace && typeof workspace.isEnabled === 'function' && workspace.isEnabled()) {\\n+        const folders: string[] = [];\\n+        let workspaceRoot: string | undefined;\\n+        let mainProjectPath: string | undefined;\\n+        try {\\n+          const info = workspace.getWorkspaceInfo?.();\\n+          if (info && typeof info.workspacePath === 'string') {\\n+            workspaceRoot = info.workspacePath;\\n+            mainProjectPath = info.mainProjectPath;\\n+            // Add workspace root\\n+            folders.push(info.workspacePath);\\n+            // Include main project path (has human-readable name like \\\"visor2\\\")\\n+            if (mainProjectPath) {\\n+              folders.push(mainProjectPath);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore workspace info errors\\n+        }\\n+        try {\\n+          const projects = workspace.listProjects?.() || [];\\n+          for (const proj of projects as any[]) {\\n+            if (proj && typeof proj.path === 'string') {\\n+              // Project paths have human-readable names (tyk-docs, not checkout-tyk-docs)\\n+              folders.push(proj.path);\\n+            }\\n+          }\\n+        } catch {\\n+          // ignore project listing errors\\n+        }\\n+        const unique = Array.from(new Set(folders.filter(p => typeof p === 'string' && p)));\\n+        if (unique.length > 0 && workspaceRoot) {\\n+          (aiConfig as any).allowedFolders = unique;\\n+          // Set path and cwd to workspace root - AI will run with cwd set to workspace\\n+          // Both are set for compatibility: path for older probe versions, cwd for rc175+\\n+          (aiConfig as any).path = workspaceRoot;\\n+          (aiConfig as any).cwd = workspaceRoot;\\n+          // Also set workspacePath for the AI to know the root\\n+          (aiConfig as any).workspacePath = workspaceRoot;\\n+          logger.debug(`[AI Provider] Workspace isolation enabled:`);\\n+          logger.debug(`[AI Provider]   workspaceRoot (cwd): ${workspaceRoot}`);\\n+          logger.debug(`[AI Provider]   allowedFolders: ${JSON.stringify(unique)}`);\\n+        }\\n+      } else if (parentCtx && typeof parentCtx.workingDirectory === 'string') {\\n+        // Fallback: when workspace is not available (or disabled), still\\n+        // constrain tools to the engine's working directory so ProbeAgent\\n+        // operates inside the same logical root as the state machine. This\\n+        // also ensures nested workflows (e.g. code-question-helper) use the\\n+        // workspace main project path once initializeWorkspace has updated\\n+        // the parent context.\\n+        if (!(aiConfig as any).allowedFolders) {\\n+          (aiConfig as any).allowedFolders = [parentCtx.workingDirectory];\\n+        }\\n+        if (!(aiConfig as any).path) {\\n+          (aiConfig as any).path = parentCtx.workingDirectory;\\n+          (aiConfig as any).cwd = parentCtx.workingDirectory;\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only; fall back to defaults on error.\\n+    }\\n+\\n     // Check-level AI model and provider (top-level properties)\\n     if (config.ai_model !== undefined) {\\n       aiConfig.model = config.ai_model as string;\\n@@ -876,7 +969,8 @@ export class AICheckProvider extends CheckProvider {\\n       ctxWithStage,\\n       _dependencyResults,\\n       (config as any).__outputHistory as Map<string, unknown[]> | undefined,\\n-      (sessionInfo as any)?.args\\n+      (sessionInfo as any)?.args,\\n+      (config as any).workflowInputs as Record<string, unknown> | undefined\\n     );\\n \\n     // Optional persona (vendor extension): ai.ai_persona or ai_persona.\\n@@ -963,6 +1057,7 @@ export class AICheckProvider extends CheckProvider {\\n       const reuseEnabled =\\n         (config as any).reuse_ai_session === true ||\\n         typeof (config as any).reuse_ai_session === 'string';\\n+      let promptUsed = finalPrompt;\\n       if (sessionInfo?.reuseSession && sessionInfo.parentSessionId && reuseEnabled) {\\n         // Safety: only reuse if the parent session actually exists\\n         try {\\n@@ -975,6 +1070,7 @@ export class AICheckProvider extends CheckProvider {\\n               );\\n             }\\n             // Fall back to new session\\n+            promptUsed = processedPrompt;\\n             const fresh = await service.executeReview(\\n               prInfo,\\n               processedPrompt,\\n@@ -999,6 +1095,7 @@ export class AICheckProvider extends CheckProvider {\\n             `üîÑ Debug: Using session reuse with parent session: ${sessionInfo.parentSessionId} (mode: ${sessionMode})`\\n           );\\n         }\\n+        promptUsed = processedPrompt;\\n         result = await service.executeReviewWithSessionReuse(\\n           prInfo,\\n           processedPrompt,\\n@@ -1011,6 +1108,7 @@ export class AICheckProvider extends CheckProvider {\\n         if (aiConfig.debug) {\\n           console.error(`üÜï Debug: Creating new AI session for check: ${config.checkName}`);\\n         }\\n+        promptUsed = finalPrompt;\\n         result = await service.executeReview(\\n           prInfo,\\n           finalPrompt,\\n@@ -1038,11 +1136,11 @@ export class AICheckProvider extends CheckProvider {\\n             span,\\n             'ai',\\n             {\\n-              prompt: processedPrompt.substring(0, 500), // Preview only\\n+              prompt: promptUsed,\\n               model: aiConfig.model,\\n             },\\n             {\\n-              content: JSON.stringify(finalResult).substring(0, 500),\\n+              content: JSON.stringify(finalResult),\\n               tokens: (result as any).usage?.totalTokens,\\n             }\\n           );\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/command-check-provider.ts\",\"additions\":50,\"deletions\":16,\"changes\":66,\"patch\":\"diff --git a/src/providers/command-check-provider.ts b/src/providers/command-check-provider.ts\\nindex e0c9d634..bf7ffd43 100644\\n--- a/src/providers/command-check-provider.ts\\n+++ b/src/providers/command-check-provider.ts\\n@@ -131,7 +131,8 @@ export class CommandCheckProvider extends CheckProvider {\\n       // New: outputs_raw exposes aggregate values (e.g., full arrays for forEach parents)\\n       outputs_raw: outputsRaw,\\n       // Workflow inputs (when executing within a workflow)\\n-      inputs: context?.workflowInputs || {},\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any).workflowInputs || context?.workflowInputs || {},\\n       // Custom arguments from on_init 'with' directive\\n       args: context?.args || {},\\n       env: this.getSafeEnvironmentVariables(),\\n@@ -178,17 +179,31 @@ export class CommandCheckProvider extends CheckProvider {\\n           mock = rawMock as Record<string, unknown>;\\n         }\\n         const m = mock as { stdout?: string; stderr?: string; exit_code?: number; exit?: number };\\n-        let out: unknown = m.stdout ?? '';\\n-        try {\\n-          if (\\n-            typeof out === 'string' &&\\n-            (out.trim().startsWith('{') || out.trim().startsWith('['))\\n-          ) {\\n-            out = JSON.parse(out);\\n-          }\\n-        } catch {}\\n-        const code =\\n-          typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0;\\n+\\n+        // Check if this looks like a command mock (has stdout/stderr/exit_code) or direct data output\\n+        const isCommandMock = m.stdout !== undefined || m.stderr !== undefined ||\\n+          m.exit_code !== undefined || m.exit !== undefined;\\n+\\n+        let out: unknown;\\n+        if (isCommandMock) {\\n+          // Traditional command mock format: { stdout: \\\"...\\\", exit_code: 0 }\\n+          out = m.stdout ?? '';\\n+          try {\\n+            if (\\n+              typeof out === 'string' &&\\n+              (out.trim().startsWith('{') || out.trim().startsWith('['))\\n+            ) {\\n+              out = JSON.parse(out);\\n+            }\\n+          } catch {}\\n+        } else {\\n+          // Direct data mock format: { data: [...], count: 1 } - use as-is\\n+          out = mock;\\n+        }\\n+\\n+        const code = isCommandMock\\n+          ? (typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0)\\n+          : 0;\\n         if (code !== 0) {\\n           return {\\n             issues: [\\n@@ -231,9 +246,8 @@ export class CommandCheckProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Get timeout from config (in seconds) or use default (60 seconds)\\n-      const timeoutSeconds = (config.timeout as number) || 60;\\n-      const timeoutMs = timeoutSeconds * 1000;\\n+      // Get timeout from config (in milliseconds) or use default (60 seconds = 60000ms)\\n+      const timeoutMs = (config.timeout as number) || 60000;\\n \\n       // Normalize only the eval payload for `node -e|--eval` invocations that may contain\\n       // literal newlines due to YAML processing (\\\"\\\\n\\\" -> newline). We re-escape newlines\\n@@ -1533,6 +1547,26 @@ ${bodyWithReturn}\\n       return null;\\n     }\\n \\n+    // Require at least one issue-specific field beyond just a message-like field.\\n+    // This prevents objects like {text: \\\"...\\\"} from being treated as issues.\\n+    const hasIssueField =\\n+      data.file !== undefined ||\\n+      data.path !== undefined ||\\n+      data.filename !== undefined ||\\n+      data.line !== undefined ||\\n+      data.startLine !== undefined ||\\n+      data.lineNumber !== undefined ||\\n+      data.severity !== undefined ||\\n+      data.level !== undefined ||\\n+      data.priority !== undefined ||\\n+      data.ruleId !== undefined ||\\n+      data.rule !== undefined ||\\n+      data.category !== undefined ||\\n+      data.type !== undefined;\\n+    if (!hasIssueField) {\\n+      return null;\\n+    }\\n+\\n     const allowedSeverities = new Set(['info', 'warning', 'error', 'critical']);\\n     const severityRaw = this.toTrimmedString(data.severity || data.level || data.priority);\\n     let severity: ReviewIssue['severity'] = 'warning';\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/git-checkout-provider.ts\",\"additions\":41,\"deletions\":6,\"changes\":47,\"patch\":\"diff --git a/src/providers/git-checkout-provider.ts b/src/providers/git-checkout-provider.ts\\nindex baac0001..6d3a2852 100644\\n--- a/src/providers/git-checkout-provider.ts\\n+++ b/src/providers/git-checkout-provider.ts\\n@@ -69,6 +69,16 @@ export class GitCheckoutProvider extends CheckProvider {\\n       return false;\\n     }\\n \\n+    if (checkoutConfig.clone_timeout_ms !== undefined) {\\n+      if (\\n+        typeof checkoutConfig.clone_timeout_ms !== 'number' ||\\n+        checkoutConfig.clone_timeout_ms <= 0\\n+      ) {\\n+        logger.error('Invalid config: clone_timeout_ms must be a positive number (milliseconds)');\\n+        return false;\\n+      }\\n+    }\\n+\\n     return true;\\n   }\\n \\n@@ -91,7 +101,13 @@ export class GitCheckoutProvider extends CheckProvider {\\n       );\\n \\n       // Resolve dynamic variables\\n-      const resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      let resolvedRef = await this.liquid.parseAndRender(checkoutConfig.ref, templateContext);\\n+      // If ref resolves to an empty string, fall back to HEAD so we rely on\\n+      // the repository's default branch. This keeps downstream git commands\\n+      // well-defined and matches existing expectations/tests.\\n+      if (!resolvedRef || resolvedRef.trim().length === 0) {\\n+        resolvedRef = 'HEAD';\\n+      }\\n       const resolvedRepository = checkoutConfig.repository\\n         ? await this.liquid.parseAndRender(checkoutConfig.repository, templateContext)\\n         : process.env.GITHUB_REPOSITORY || 'unknown/unknown';\\n@@ -118,6 +134,7 @@ export class GitCheckoutProvider extends CheckProvider {\\n           clean: checkoutConfig.clean !== false, // Default: true\\n           workflowId: (context as any)?.workflowId,\\n           fetchDepth: checkoutConfig.fetch_depth,\\n+          cloneTimeoutMs: checkoutConfig.clone_timeout_ms,\\n         }\\n       );\\n \\n@@ -134,18 +151,35 @@ export class GitCheckoutProvider extends CheckProvider {\\n \\n       // Add project to workspace if workspace isolation is enabled\\n       const workspace = (context as any)?._parentContext?.workspace;\\n+\\n+      // Enhanced debug logging for workspace project addition diagnosis\\n+      const checkName = (config as any)?.checkName || 'unknown';\\n+      logger.info(`[GitCheckout] Workspace check for '${checkName}':`);\\n+      logger.info(`[GitCheckout]   _parentContext exists: ${!!(context as any)?._parentContext}`);\\n+      logger.info(`[GitCheckout]   workspace exists: ${!!workspace}`);\\n+      logger.info(`[GitCheckout]   workspace.isEnabled(): ${workspace?.isEnabled?.() ?? 'N/A'}`);\\n+      if (workspace) {\\n+        const projectCountBefore = workspace.listProjects?.()?.length ?? 'N/A';\\n+        logger.debug(`[GitCheckout]   projects before addProject: ${projectCountBefore}`);\\n+      }\\n+\\n       if (workspace?.isEnabled()) {\\n         try {\\n+          // Don't pass checkName as description - let workspace use repo name\\n+          // This results in human-readable names like \\\"tyk-docs\\\" instead of \\\"checkout-tyk-docs\\\"\\n           const workspacePath = await workspace.addProject(\\n             resolvedRepository,\\n-            worktree.path,\\n-            checkoutConfig.checkName\\n+            worktree.path\\n           );\\n           output.workspace_path = workspacePath;\\n-          logger.debug(`Added project to workspace: ${workspacePath}`);\\n+          const projectCountAfter = workspace.listProjects?.()?.length ?? 'N/A';\\n+          logger.debug(`[GitCheckout] Added project to workspace: ${workspacePath}`);\\n+          logger.debug(`[GitCheckout]   projects after addProject: ${projectCountAfter}`);\\n         } catch (error) {\\n           logger.warn(`Failed to add project to workspace: ${error}`);\\n         }\\n+      } else {\\n+        logger.debug(`[GitCheckout] Workspace not enabled, skipping addProject`);\\n       }\\n \\n       logger.info(\\n@@ -228,7 +262,8 @@ export class GitCheckoutProvider extends CheckProvider {\\n       outputs: outputsObj,\\n       outputs_history: historyObj,\\n       env: safeEnv,\\n-      inputs: context?.workflowInputs,\\n+      // Check config first (set by projectWorkflowToGraph), then fall back to context\\n+      inputs: (config as any)?.workflowInputs || context?.workflowInputs,\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/http-client-provider.ts\",\"additions\":279,\"deletions\":35,\"changes\":314,\"patch\":\"diff --git a/src/providers/http-client-provider.ts b/src/providers/http-client-provider.ts\\nindex 4d8c41be..6db81591 100644\\n--- a/src/providers/http-client-provider.ts\\n+++ b/src/providers/http-client-provider.ts\\n@@ -4,18 +4,29 @@ import { ReviewSummary } from '../reviewer';\\n import { Liquid } from 'liquidjs';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n import { EnvironmentResolver } from '../utils/env-resolver';\\n+import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n+import { buildProviderTemplateContext } from '../utils/template-context';\\n+import Sandbox from '@nyariv/sandboxjs';\\n+import { logger } from '../logger';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n \\n /**\\n  * Check provider that fetches data from HTTP endpoints\\n  */\\n export class HttpClientProvider extends CheckProvider {\\n   private liquid: Liquid;\\n+  private sandbox?: Sandbox;\\n \\n   constructor() {\\n     super();\\n     this.liquid = createExtendedLiquid();\\n   }\\n \\n+  private createSecureSandbox(): Sandbox {\\n+    return createSecureSandbox();\\n+  }\\n+\\n   getName(): string {\\n     return 'http_client';\\n   }\\n@@ -61,50 +72,124 @@ export class HttpClientProvider extends CheckProvider {\\n     const headers = (config.headers as Record<string, string>) || {};\\n     const timeout = (config.timeout as number) || 30000;\\n     const transform = config.transform as string | undefined;\\n+    const transformJs = config.transform_js as string | undefined;\\n     const bodyTemplate = config.body as string | undefined;\\n+    const outputFileTemplate = config.output_file as string | undefined;\\n+    const skipIfExists = config.skip_if_exists !== false; // Default true for caching\\n \\n-    try {\\n-      // Prepare template context for URL and body\\n-      const templateContext = {\\n-        pr: {\\n-          number: prInfo.number,\\n-          title: prInfo.title,\\n-          body: prInfo.body,\\n-          author: prInfo.author,\\n-          base: prInfo.base,\\n-          head: prInfo.head,\\n-          totalAdditions: prInfo.totalAdditions,\\n-          totalDeletions: prInfo.totalDeletions,\\n-        },\\n-        outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-        env: process.env,\\n-      };\\n+    // Track resolved URL for error messages\\n+    let resolvedUrlForErrors = url;\\n \\n-      // Render URL with template if it contains liquid syntax\\n-      let renderedUrl = url;\\n-      if (url.includes('{{') || url.includes('{%')) {\\n-        renderedUrl = await this.liquid.parseAndRender(url, templateContext);\\n+    try {\\n+      // Use shared template context builder for consistent output extraction\\n+      const templateContext = buildProviderTemplateContext(\\n+        prInfo,\\n+        dependencyResults,\\n+        undefined, // memoryStore\\n+        undefined, // outputHistory\\n+        undefined, // stageHistoryBase\\n+        { attachMemoryReadHelpers: false }\\n+      );\\n+      // Add env to context for shell-style variable resolution\\n+      (templateContext as Record<string, unknown>).env = process.env;\\n+\\n+      // First resolve shell-style environment variables (${VAR}, $VAR, ${{ env.VAR }})\\n+      let renderedUrl = String(EnvironmentResolver.resolveValue(url));\\n+      resolvedUrlForErrors = renderedUrl; // Track for error messages\\n+\\n+      // Then render Liquid templates if present\\n+      if (renderedUrl.includes('{{') || renderedUrl.includes('{%')) {\\n+        renderedUrl = await this.liquid.parseAndRender(renderedUrl, templateContext);\\n+        resolvedUrlForErrors = renderedUrl; // Update after Liquid rendering\\n       }\\n \\n       // Prepare request body if provided\\n       let requestBody: string | undefined;\\n       if (bodyTemplate) {\\n-        const renderedBody = await this.liquid.parseAndRender(bodyTemplate, templateContext);\\n-        requestBody = renderedBody;\\n+        // First resolve shell-style environment variables\\n+        let resolvedBody = String(EnvironmentResolver.resolveValue(bodyTemplate));\\n+        // Then render Liquid templates if present\\n+        if (resolvedBody.includes('{{') || resolvedBody.includes('{%')) {\\n+          resolvedBody = await this.liquid.parseAndRender(resolvedBody, templateContext);\\n+        }\\n+        requestBody = resolvedBody;\\n       }\\n \\n-      // Resolve environment variables in headers\\n-      const resolvedHeaders = EnvironmentResolver.resolveHeaders(headers);\\n+      // Resolve environment variables and Liquid templates in headers\\n+      const resolvedHeaders: Record<string, string> = {};\\n+      for (const [key, value] of Object.entries(headers)) {\\n+        let resolvedValue = String(EnvironmentResolver.resolveValue(value));\\n+        // Render Liquid templates if present\\n+        if (resolvedValue.includes('{{') || resolvedValue.includes('{%')) {\\n+          resolvedValue = await this.liquid.parseAndRender(resolvedValue, templateContext);\\n+        }\\n+        resolvedHeaders[key] = resolvedValue;\\n+        // Debug auth header (mask most of the value for security)\\n+        if (key.toLowerCase() === 'authorization') {\\n+          const maskedValue = resolvedValue.length > 20\\n+            ? `${resolvedValue.substring(0, 15)}...${resolvedValue.substring(resolvedValue.length - 5)}`\\n+            : resolvedValue;\\n+          logger.verbose(`[http_client] ${key}: ${maskedValue}`);\\n+        }\\n+      }\\n+\\n+      // Resolve output_file path if specified\\n+      let resolvedOutputFile: string | undefined;\\n+      if (outputFileTemplate) {\\n+        let outputPath = String(EnvironmentResolver.resolveValue(outputFileTemplate));\\n+        if (outputPath.includes('{{') || outputPath.includes('{%')) {\\n+          outputPath = await this.liquid.parseAndRender(outputPath, templateContext);\\n+        }\\n+        resolvedOutputFile = outputPath.trim();\\n+\\n+        // Check if file already exists (caching)\\n+        if (skipIfExists && fs.existsSync(resolvedOutputFile)) {\\n+          const stats = fs.statSync(resolvedOutputFile);\\n+          logger.verbose(`[http_client] File cached: ${resolvedOutputFile} (${stats.size} bytes)`);\\n+          return {\\n+            issues: [],\\n+            file_path: resolvedOutputFile,\\n+            size: stats.size,\\n+            cached: true,\\n+          } as unknown as ReviewSummary;\\n+        }\\n+      }\\n \\n       // Test hook: mock HTTP response for this step\\n       const stepName = (config as any).checkName || 'unknown';\\n       const mock = context?.hooks?.mockForStep?.(String(stepName));\\n-      const data =\\n-        mock !== undefined\\n-          ? mock\\n-          : await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n \\n-      // Apply transformation if specified\\n+      // If mock is provided, return it directly as the step output (with issues: [])\\n+      if (mock !== undefined) {\\n+        const mockObj = typeof mock === 'object' && mock !== null ? mock : { data: mock };\\n+        return {\\n+          issues: [],\\n+          ...mockObj,\\n+        } as unknown as ReviewSummary & { data: unknown };\\n+      }\\n+\\n+      // Debug log the request for troubleshooting\\n+      logger.verbose(`[http_client] ${method} ${renderedUrl}`);\\n+      if (requestBody) {\\n+        logger.verbose(`[http_client] Body: ${requestBody.substring(0, 500)}${requestBody.length > 500 ? '...' : ''}`);\\n+      }\\n+\\n+      // If output_file is specified, download to file instead of returning data\\n+      if (resolvedOutputFile) {\\n+        const fileResult = await this.downloadToFile(\\n+          renderedUrl,\\n+          method,\\n+          resolvedHeaders,\\n+          requestBody,\\n+          timeout,\\n+          resolvedOutputFile\\n+        );\\n+        return fileResult;\\n+      }\\n+\\n+      const data = await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n+\\n+      // Apply Liquid transformation if specified\\n       let processedData = data;\\n       if (transform) {\\n         try {\\n@@ -136,12 +221,53 @@ export class HttpClientProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Return the fetched data as a custom field for dependent checks to access\\n+      // Apply JavaScript transformation if specified\\n+      if (transformJs) {\\n+        try {\\n+          if (!this.sandbox) {\\n+            this.sandbox = this.createSecureSandbox();\\n+          }\\n+\\n+          // Create scope for JavaScript transform (scope, not context)\\n+          const jsScope: Record<string, unknown> = {\\n+            output: data,\\n+            pr: templateContext.pr,\\n+            outputs: templateContext.outputs,\\n+            env: process.env,\\n+          };\\n+\\n+          const result = compileAndRun(this.sandbox, transformJs, jsScope, {\\n+            injectLog: true,\\n+            logPrefix: 'üîç [transform_js]',\\n+            wrapFunction: true,\\n+          });\\n+          processedData = result;\\n+          logger.verbose(`‚úì Applied JavaScript transform successfully`);\\n+        } catch (error) {\\n+          logger.error(\\n+            `‚úó Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`\\n+          );\\n+          return {\\n+            issues: [\\n+              {\\n+                file: 'http_client',\\n+                line: 0,\\n+                ruleId: 'http_client/transform_js_error',\\n+                message: `Failed to apply JavaScript transform: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              },\\n+            ],\\n+          };\\n+        }\\n+      }\\n+\\n+      // Return the fetched data in the standard `output` property for guarantee evaluation\\n+      // This is consistent with other providers (script, command, ai, etc.)\\n       return {\\n         issues: [],\\n-        // Add custom data field that will be passed through to dependent checks\\n-        data: processedData,\\n-      } as ReviewSummary & { data: unknown };\\n+        output: processedData,\\n+      } as unknown as ReviewSummary;\\n     } catch (error) {\\n       return {\\n         issues: [\\n@@ -149,7 +275,7 @@ export class HttpClientProvider extends CheckProvider {\\n             file: 'http_client',\\n             line: 0,\\n             ruleId: 'http_client/fetch_error',\\n-            message: `Failed to fetch from ${url}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            message: `Failed to fetch from ${resolvedUrlForErrors}: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n             severity: 'error',\\n             category: 'logic',\\n           },\\n@@ -198,7 +324,14 @@ export class HttpClientProvider extends CheckProvider {\\n \\n       clearTimeout(timeoutId);\\n \\n+      logger.verbose(`[http_client] Response: ${response.status} ${response.statusText}`);\\n+\\n       if (!response.ok) {\\n+        // Log response body for debugging auth failures\\n+        try {\\n+          const errorBody = await response.text();\\n+          logger.warn(`[http_client] Error body: ${errorBody.substring(0, 500)}`);\\n+        } catch {}\\n         throw new Error(`HTTP ${response.status}: ${response.statusText}`);\\n       }\\n \\n@@ -233,6 +366,112 @@ export class HttpClientProvider extends CheckProvider {\\n     }\\n   }\\n \\n+  private async downloadToFile(\\n+    url: string,\\n+    method: string,\\n+    headers: Record<string, string>,\\n+    body: string | undefined,\\n+    timeout: number,\\n+    outputFile: string\\n+  ): Promise<ReviewSummary> {\\n+    // Check if fetch is available (Node 18+)\\n+    if (typeof fetch === 'undefined') {\\n+      throw new Error('HTTP client provider requires Node.js 18+ or node-fetch package');\\n+    }\\n+\\n+    const controller = new AbortController();\\n+    const timeoutId = setTimeout(() => controller.abort(), timeout);\\n+\\n+    try {\\n+      const requestOptions: RequestInit = {\\n+        method,\\n+        headers: { ...headers },\\n+        signal: controller.signal,\\n+      };\\n+\\n+      // Add body for non-GET requests\\n+      if (method !== 'GET' && body) {\\n+        requestOptions.body = body;\\n+        if (!headers['Content-Type'] && !headers['content-type']) {\\n+          requestOptions.headers = {\\n+            ...requestOptions.headers,\\n+            'Content-Type': 'application/json',\\n+          };\\n+        }\\n+      }\\n+\\n+      const response = await fetch(url, requestOptions);\\n+      clearTimeout(timeoutId);\\n+\\n+      if (!response.ok) {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_error',\\n+              message: `Failed to download file: HTTP ${response.status}: ${response.statusText}`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      // Create parent directory if it doesn't exist\\n+      const parentDir = path.dirname(outputFile);\\n+      if (parentDir && !fs.existsSync(parentDir)) {\\n+        fs.mkdirSync(parentDir, { recursive: true });\\n+      }\\n+\\n+      // Get the response as an ArrayBuffer and write to file\\n+      const arrayBuffer = await response.arrayBuffer();\\n+      const buffer = Buffer.from(arrayBuffer);\\n+      fs.writeFileSync(outputFile, buffer);\\n+\\n+      const contentType = response.headers.get('content-type') || 'application/octet-stream';\\n+      logger.verbose(`[http_client] Downloaded: ${outputFile} (${buffer.length} bytes)`);\\n+\\n+      return {\\n+        issues: [],\\n+        file_path: outputFile,\\n+        size: buffer.length,\\n+        content_type: contentType,\\n+        cached: false,\\n+      } as unknown as ReviewSummary;\\n+    } catch (error: unknown) {\\n+      clearTimeout(timeoutId);\\n+\\n+      if (error instanceof Error && error.name === 'AbortError') {\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'http_client',\\n+              line: 0,\\n+              ruleId: 'http_client/download_timeout',\\n+              message: `Download timed out after ${timeout}ms`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n+      return {\\n+        issues: [\\n+          {\\n+            file: 'http_client',\\n+            line: 0,\\n+            ruleId: 'http_client/download_error',\\n+            message: `Failed to download file: ${error instanceof Error ? error.message : 'Unknown error'}`,\\n+            severity: 'error',\\n+            category: 'logic',\\n+          },\\n+        ],\\n+      };\\n+    }\\n+  }\\n+\\n   getSupportedConfigKeys(): string[] {\\n     return [\\n       'type',\\n@@ -241,7 +480,10 @@ export class HttpClientProvider extends CheckProvider {\\n       'headers',\\n       'body',\\n       'transform',\\n+      'transform_js',\\n       'timeout',\\n+      'output_file',\\n+      'skip_if_exists',\\n       'depends_on',\\n       'on',\\n       'if',\\n@@ -261,6 +503,8 @@ export class HttpClientProvider extends CheckProvider {\\n       'Network access to the endpoint',\\n       'Optional: Transform template for processing response data',\\n       'Optional: Body template for POST/PUT requests',\\n+      'Optional: output_file path to download response to a file',\\n+      'Optional: skip_if_exists (default: true) to enable caching for file downloads',\\n     ];\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/log-check-provider.ts\",\"additions\":19,\"deletions\":8,\"changes\":27,\"patch\":\"diff --git a/src/providers/log-check-provider.ts b/src/providers/log-check-provider.ts\\nindex 9c6332d1..b57f632c 100644\\n--- a/src/providers/log-check-provider.ts\\n+++ b/src/providers/log-check-provider.ts\\n@@ -78,7 +78,8 @@ export class LogCheckProvider extends CheckProvider {\\n       includeDependencies,\\n       includeMetadata,\\n       config.__outputHistory as Map<string, unknown[]> | undefined,\\n-      context\\n+      context,\\n+      config\\n     );\\n \\n     // Render the log message template\\n@@ -100,12 +101,20 @@ export class LogCheckProvider extends CheckProvider {\\n     else if (level === 'debug') logger.debug(logOutput);\\n     else logger.info(logOutput);\\n \\n-    // Return with the log content as custom data for dependent checks\\n-    return {\\n+    // Return with the log content as custom data for dependent checks.\\n+    // For chat-style logs (group: chat), also expose a structured\\n+    // `output.text` field so frontends like Slack can post a clean\\n+    // human-facing message without the level prefix.\\n+    const summary: ReviewSummary & { logOutput: string; output?: unknown } = {\\n       issues: [],\\n-      // Add log output as custom field\\n       logOutput,\\n-    } as ReviewSummary & { logOutput: string };\\n+    };\\n+\\n+    if ((config as any).group === 'chat') {\\n+      (summary as any).output = { text: renderedMessage };\\n+    }\\n+\\n+    return summary;\\n   }\\n \\n   private buildTemplateContext(\\n@@ -115,7 +124,8 @@ export class LogCheckProvider extends CheckProvider {\\n     _includeDependencies: boolean = true,\\n     includeMetadata: boolean = true,\\n     outputHistory?: Map<string, unknown[]>,\\n-    executionContext?: ExecutionContext\\n+    executionContext?: ExecutionContext,\\n+    config?: CheckProviderConfig\\n   ): Record<string, unknown> {\\n     const context: Record<string, unknown> = {};\\n \\n@@ -197,7 +207,8 @@ export class LogCheckProvider extends CheckProvider {\\n     }\\n \\n     // Add workflow inputs if available\\n-    const workflowInputs = executionContext?.workflowInputs || {};\\n+    // Check config first (set by projectWorkflowToGraph), then fall back to executionContext\\n+    const workflowInputs = (config as any)?.workflowInputs || executionContext?.workflowInputs || {};\\n     logger.debug(\\n       `[LogProvider] Adding ${Object.keys(workflowInputs).length} workflow inputs to context`\\n     );\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/script-check-provider.ts\",\"additions\":37,\"deletions\":1,\"changes\":38,\"patch\":\"diff --git a/src/providers/script-check-provider.ts b/src/providers/script-check-provider.ts\\nindex 9825bb69..947dbd73 100644\\n--- a/src/providers/script-check-provider.ts\\n+++ b/src/providers/script-check-provider.ts\\n@@ -59,6 +59,16 @@ export class ScriptCheckProvider extends CheckProvider {\\n       reuseSession?: boolean;\\n     } & import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n+    // Test hook: mock output for this step (short-circuit execution)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = _sessionInfo?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        // Return mock directly as step output\\n+        return { issues: [], output: mock } as ReviewSummary & { output: unknown };\\n+      }\\n+    } catch {}\\n+\\n     const script = String(config.content || '');\\n     const memoryStore = MemoryStore.getInstance();\\n     const ctx = buildProviderTemplateContext(\\n@@ -72,10 +82,36 @@ export class ScriptCheckProvider extends CheckProvider {\\n     // Keep provider quiet by default; no step-specific debug\\n     // (historical ad-hoc logs removed to avoid hardcoding step names).\\n \\n+    // Add workflow inputs to the context\\n+    const inputs = (config as any).workflowInputs || _sessionInfo?.workflowInputs || {};\\n+    (ctx as any).inputs = inputs;\\n+\\n+    // Add environment variables to context (consistent with http-client-provider)\\n+    (ctx as any).env = process.env;\\n+\\n     // Attach synchronous memory ops consistent with memory provider\\n     const { ops, needsSave } = createSyncMemoryOps(memoryStore);\\n     (ctx as any).memory = ops as unknown as Record<string, unknown>;\\n \\n+    // Add helper functions to the context\\n+    (ctx as any).escapeXml = (str: unknown): string => {\\n+      if (str == null) return '';\\n+      return String(str)\\n+        .replace(/&/g, '&amp;')\\n+        .replace(/</g, '&lt;')\\n+        .replace(/>/g, '&gt;')\\n+        .replace(/\\\"/g, '&quot;')\\n+        .replace(/'/g, '&apos;');\\n+    };\\n+\\n+    // Add btoa/atob for base64 encoding/decoding (browser API polyfill)\\n+    (ctx as any).btoa = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'binary').toString('base64');\\n+    };\\n+    (ctx as any).atob = (str: unknown): string => {\\n+      return Buffer.from(String(str), 'base64').toString('binary');\\n+    };\\n+\\n     // Evaluate the script in a secure sandbox (per-execution instance)\\n     const sandbox = this.createSecureSandbox();\\n     let result: unknown;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/workflow-check-provider.ts\",\"additions\":164,\"deletions\":43,\"changes\":207,\"patch\":\"diff --git a/src/providers/workflow-check-provider.ts b/src/providers/workflow-check-provider.ts\\nindex 116d4dff..681703ce 100644\\n--- a/src/providers/workflow-check-provider.ts\\n+++ b/src/providers/workflow-check-provider.ts\\n@@ -67,34 +67,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n   ): Promise<ReviewSummary> {\\n     const cfg = config as CheckProviderConfig & { workflow?: string; config?: string };\\n     const isConfigPathMode = !!cfg.config && !cfg.workflow;\\n+    const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n \\n-    // Test harness support: allow mocking workflow checks as a black box.\\n-    // If a mock is provided for this step, short-circuit nested execution and\\n-    // return a ReviewSummary with optional output field.\\n-    try {\\n-      const stepName = (config as any).checkName || cfg.workflow || cfg.config || 'workflow';\\n-      // test-runner passes hooks on execution context\\n-      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n-      if (mock !== undefined) {\\n-        const ms = mock as any;\\n-        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n-        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n-        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n-        const summary: ReviewSummary & { output?: unknown } = {\\n-          issues: issuesArr,\\n-          output: out,\\n-          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n-        } as any;\\n-        return summary;\\n-      }\\n-    } catch {}\\n-\\n-    // Resolve workflow definition\\n+    // Resolve workflow definition FIRST (needed for input preparation and validation)\\n     let workflow: WorkflowDefinition | undefined;\\n     let workflowId = cfg.workflow as string | undefined;\\n \\n     if (isConfigPathMode) {\\n-      const parentCwd = ((context as any)?._parentContext?.workingDirectory ||\\n+      // Use originalWorkingDirectory for config file resolution - workflow configs\\n+      // should be loaded from the original project path, not the sandbox\\n+      const parentCwd = ((context as any)?._parentContext?.originalWorkingDirectory ||\\n+        (context as any)?._parentContext?.workingDirectory ||\\n+        (context as any)?.originalWorkingDirectory ||\\n         (context as any)?.workingDirectory ||\\n         process.cwd()) as string;\\n       workflow = await this.loadWorkflowFromConfigPath(String(cfg.config), parentCwd);\\n@@ -109,9 +93,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       logger.info(`Executing workflow '${workflowId}'`);\\n     }\\n \\n-    // Prepare inputs\\n+    // Prepare inputs - do this BEFORE mock check so we can validate inputs even when mocked\\n+    // This allows tests to assert that the correct inputs were passed to workflow steps\\n     const inputs = await this.prepareInputs(workflow, config, prInfo, dependencyResults);\\n \\n+    // Capture resolved workflow inputs for testing assertions (reuse prompt capture infrastructure)\\n+    // This allows using `prompts` assertions with `provider: 'workflow'` to verify inputs\\n+    try {\\n+      // Serialize inputs to capture for assertions - specifically the context field\\n+      // which contains the rendered template with dependency outputs\\n+      const inputsCapture = Object.entries(inputs)\\n+        .map(([k, v]) => `${k}: ${typeof v === 'string' ? v : JSON.stringify(v)}`)\\n+        .join('\\\\n\\\\n');\\n+      (context as any)?.hooks?.onPromptCaptured?.({\\n+        step: String(stepName),\\n+        provider: 'workflow',\\n+        prompt: inputsCapture,\\n+      });\\n+    } catch {\\n+      // Ignore capture errors - this is only for testing\\n+    }\\n+\\n     // Validate inputs\\n     const validation = this.registry.validateInputs(workflow, inputs);\\n     if (!validation.valid) {\\n@@ -119,6 +121,27 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       throw new Error(`Invalid workflow inputs: ${errors}`);\\n     }\\n \\n+    // Test harness support: allow mocking workflow checks as a black box.\\n+    // If a mock is provided for this step, short-circuit nested execution and\\n+    // return a ReviewSummary with optional output field.\\n+    // NOTE: This happens AFTER input preparation/validation so tests can assert inputs are correct\\n+    try {\\n+      // test-runner passes hooks on execution context\\n+      const mock = (context as any)?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        const ms = mock as any;\\n+        const issuesArr = Array.isArray(ms?.issues) ? (ms.issues as any[]) : [];\\n+        // Prefer explicit output if provided; otherwise treat the mock object itself as output\\n+        const out = ms && typeof ms === 'object' && 'output' in ms ? ms.output : ms;\\n+        const summary: ReviewSummary & { output?: unknown } = {\\n+          issues: issuesArr,\\n+          output: out,\\n+          ...(typeof ms?.content === 'string' ? { content: String(ms.content) } : {}),\\n+        } as any;\\n+        return summary;\\n+      }\\n+    } catch {}\\n+\\n     // Apply overrides to workflow steps if specified\\n     const modifiedWorkflow = this.applyOverrides(workflow, config);\\n \\n@@ -211,6 +234,80 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Extract eventContext for slack/conversation\\n+    const eventContext = config.eventContext || {};\\n+\\n+    // Debug logging for conversation context\\n+    logger.debug(`[WorkflowProvider] prepareInputs for ${workflow.id}`);\\n+    logger.debug(`[WorkflowProvider] eventContext keys: ${Object.keys(eventContext).join(', ') || 'none'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.slack: ${eventContext.slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] eventContext.conversation: ${(eventContext as any).conversation ? 'present' : 'absent'}`);\\n+\\n+    // Extract slack context (if provided via eventContext.slack)\\n+    const slack = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        const slackCtx = anyCtx?.slack;\\n+        if (slackCtx && typeof slackCtx === 'object') return slackCtx;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Extract unified conversation context across transports (Slack & GitHub)\\n+    const conversation = (() => {\\n+      try {\\n+        const anyCtx = eventContext as any;\\n+        if (anyCtx?.slack?.conversation) return anyCtx.slack.conversation;\\n+        if (anyCtx?.github?.conversation) return anyCtx.github.conversation;\\n+        if (anyCtx?.conversation) return anyCtx.conversation;\\n+      } catch {\\n+        // ignore\\n+      }\\n+      return undefined;\\n+    })();\\n+\\n+    // Debug logging for extracted context\\n+    logger.debug(`[WorkflowProvider] slack extracted: ${slack ? 'present' : 'absent'}`);\\n+    logger.debug(`[WorkflowProvider] conversation extracted: ${conversation ? 'present' : 'absent'}`);\\n+    if (conversation) {\\n+      logger.debug(`[WorkflowProvider] conversation.messages count: ${Array.isArray((conversation as any).messages) ? (conversation as any).messages.length : 0}`);\\n+    }\\n+\\n+    // Extract output history from config (passed via __outputHistory)\\n+    const outputHistory = (config as any).__outputHistory as Map<string, unknown[]> | undefined;\\n+    const outputs_history: Record<string, unknown[]> = {};\\n+    if (outputHistory) {\\n+      for (const [k, v] of outputHistory.entries()) {\\n+        outputs_history[k] = v;\\n+      }\\n+    }\\n+\\n+    // Build template context with all available data\\n+    // Extract .output from each dependency result so that outputs['step-name'].field works naturally\\n+    // (not outputs['step-name'].output.field)\\n+    const outputsMap: Record<string, unknown> = {};\\n+    logger.debug(`[WorkflowProvider] dependencyResults: ${dependencyResults ? dependencyResults.size : 'undefined'} entries`);\\n+    if (dependencyResults) {\\n+      for (const [key, result] of dependencyResults.entries()) {\\n+        // Extract the output property, or use the whole result if output is undefined\\n+        const extracted = (result as any).output ?? result;\\n+        outputsMap[key] = extracted;\\n+        // Debug: log what we extracted for each dependency\\n+        const extractedKeys = extracted && typeof extracted === 'object' ? Object.keys(extracted).join(', ') : 'not-object';\\n+        logger.debug(`[WorkflowProvider] outputs['${key}']: keys=[${extractedKeys}]`);\\n+      }\\n+    }\\n+    const templateContext = {\\n+      pr: prInfo,\\n+      outputs: outputsMap,\\n+      env: process.env,\\n+      slack,\\n+      conversation,\\n+      outputs_history,\\n+    };\\n+\\n     // Apply user-provided inputs (args)\\n     const userInputs = config.args || config.workflow_inputs; // Support both for compatibility\\n     if (userInputs) {\\n@@ -219,11 +316,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         if (typeof value === 'string') {\\n           // Check if it's a Liquid template\\n           if (value.includes('{{') || value.includes('{%')) {\\n-            inputs[key] = await this.liquid.parseAndRender(value, {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            });\\n+            inputs[key] = await this.liquid.parseAndRender(value, templateContext);\\n+            // Debug: log rendered template value for important inputs\\n+            if (key === 'text' || key === 'question' || key === 'context') {\\n+              const rendered = String(inputs[key]);\\n+              logger.info(`[WorkflowProvider] Rendered '${key}' input (${rendered.length} chars): ${rendered.substring(0, 500)}${rendered.length > 500 ? '...' : ''}`);\\n+            }\\n           } else {\\n             inputs[key] = value;\\n           }\\n@@ -234,11 +332,7 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           inputs[key] = compileAndRun(\\n             sandbox,\\n             exprValue.expression,\\n-            {\\n-              pr: prInfo,\\n-              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n-              env: process.env,\\n-            },\\n+            templateContext,\\n             { injectLog: true, logPrefix: `workflow.input.${key}` }\\n           );\\n         } else {\\n@@ -367,6 +461,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n     try {\\n       await childMemory.initialize();\\n     } catch {}\\n+    // Enhanced debug logging for workspace propagation diagnosis\\n+    const parentWorkspace = parentContext?.workspace;\\n+    logger.info(`[WorkflowProvider] Workspace propagation for nested workflow '${workflow.id}':`);\\n+    logger.info(`[WorkflowProvider]   parentContext exists: ${!!parentContext}`);\\n+    logger.info(`[WorkflowProvider]   parentContext.workspace exists: ${!!parentWorkspace}`);\\n+    if (parentWorkspace) {\\n+      logger.info(`[WorkflowProvider]   parentWorkspace.isEnabled(): ${parentWorkspace.isEnabled?.() ?? 'N/A'}`);\\n+      const projectCount = parentWorkspace.listProjects?.()?.length ?? 'N/A';\\n+      logger.info(`[WorkflowProvider]   parentWorkspace project count: ${projectCount}`);\\n+    } else {\\n+      logger.warn(`[WorkflowProvider]   NO WORKSPACE from parent - nested checkouts won't be added to workspace!`);\\n+    }\\n \\n     const childContext = {\\n       mode: 'state-machine' as const,\\n@@ -374,7 +480,18 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       checks: checksMetadata,\\n       journal: childJournal,\\n       memory: childMemory,\\n+      // For nested workflows we continue to execute inside the same logical\\n+      // working directory as the parent run. When workspace isolation is\\n+      // enabled on the parent engine, its WorkspaceManager is also propagated\\n+      // so that nested checks (AI, git-checkout, etc.) see the same isolated\\n+      // workspace and project symlinks instead of falling back to the Visor\\n+      // repository root.\\n       workingDirectory: parentContext?.workingDirectory || process.cwd(),\\n+      originalWorkingDirectory:\\n+        parentContext?.originalWorkingDirectory ||\\n+        parentContext?.workingDirectory ||\\n+        process.cwd(),\\n+      workspace: parentWorkspace,\\n       // Always use a fresh session for nested workflows to isolate history\\n       sessionId: uuidv4(),\\n       event: parentContext?.event || prInfo.eventType,\\n@@ -500,6 +617,12 @@ export class WorkflowCheckProvider extends CheckProvider {\\n       }\\n     } catch {}\\n \\n+    // Create outputs map that directly exposes .output values (not wrapped in { output, issues })\\n+    // This makes {{ outputs['check-name'] }} work naturally without needing .output\\n+    const outputsMap = Object.fromEntries(\\n+      Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n+    );\\n+\\n     for (const output of workflow.outputs) {\\n       if (output.value_js) {\\n         // JavaScript expression\\n@@ -508,10 +631,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n           output.value_js,\\n           {\\n             inputs,\\n-            steps: Object.fromEntries(\\n-              Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-            ),\\n-            outputs: flat,\\n+            outputs: outputsMap,\\n+            // Keep 'steps' as alias for backwards compatibility\\n+            steps: outputsMap,\\n             pr: prInfo,\\n           },\\n           { injectLog: true, logPrefix: `workflow.output.${output.name}` }\\n@@ -520,10 +642,9 @@ export class WorkflowCheckProvider extends CheckProvider {\\n         // Liquid template\\n         outputs[output.name] = await this.liquid.parseAndRender(output.value, {\\n           inputs,\\n-          steps: Object.fromEntries(\\n-            Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n-          ),\\n-          outputs: flat,\\n+          outputs: outputsMap,\\n+          // Keep 'steps' as alias for backwards compatibility\\n+          steps: outputsMap,\\n           pr: prInfo,\\n         });\\n       }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/client.ts\",\"additions\":65,\"deletions\":1,\"changes\":66,\"patch\":\"diff --git a/src/slack/client.ts b/src/slack/client.ts\\nindex 292cc358..435f8b0d 100644\\n--- a/src/slack/client.ts\\n+++ b/src/slack/client.ts\\n@@ -142,6 +142,70 @@ export class SlackClient {\\n     }\\n   }\\n \\n+  public readonly files = {\\n+    /**\\n+     * Upload a file to Slack using files.uploadV2 API\\n+     * @param options Upload options including file content, filename, channel, and thread_ts\\n+     */\\n+    uploadV2: async ({\\n+      content,\\n+      filename,\\n+      channel,\\n+      thread_ts,\\n+      title,\\n+      initial_comment,\\n+    }: {\\n+      content: Buffer;\\n+      filename: string;\\n+      channel: string;\\n+      thread_ts?: string;\\n+      title?: string;\\n+      initial_comment?: string;\\n+    }): Promise<{ ok: boolean; file?: { id: string; permalink?: string } }> => {\\n+      try {\\n+        // Step 1: Get upload URL\\n+        const getUrlResp: any = await this.api('files.getUploadURLExternal', {\\n+          filename,\\n+          length: content.length,\\n+        });\\n+        if (!getUrlResp || getUrlResp.ok !== true || !getUrlResp.upload_url) {\\n+          console.warn(`Slack files.getUploadURLExternal failed: ${getUrlResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 2: Upload file content to the URL\\n+        const uploadResp = await fetch(getUrlResp.upload_url, {\\n+          method: 'POST',\\n+          body: content,\\n+        });\\n+        if (!uploadResp.ok) {\\n+          console.warn(`Slack file upload to URL failed: ${uploadResp.status}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        // Step 3: Complete the upload and share to channel\\n+        const completeResp: any = await this.api('files.completeUploadExternal', {\\n+          files: [{ id: getUrlResp.file_id, title: title || filename }],\\n+          channel_id: channel,\\n+          thread_ts,\\n+          initial_comment,\\n+        });\\n+        if (!completeResp || completeResp.ok !== true) {\\n+          console.warn(`Slack files.completeUploadExternal failed: ${completeResp?.error || 'unknown'}`);\\n+          return { ok: false };\\n+        }\\n+\\n+        return {\\n+          ok: true,\\n+          file: completeResp.files?.[0] || { id: getUrlResp.file_id },\\n+        };\\n+      } catch (e) {\\n+        console.warn(`Slack file upload failed: ${e instanceof Error ? e.message : String(e)}`);\\n+        return { ok: false };\\n+      }\\n+    },\\n+  };\\n+\\n   getWebClient(): any {\\n     return {\\n       conversations: {\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/markdown.ts\",\"additions\":204,\"deletions\":5,\"changes\":209,\"patch\":\"diff --git a/src/slack/markdown.ts b/src/slack/markdown.ts\\nindex cc63fb5f..9f91c25f 100644\\n--- a/src/slack/markdown.ts\\n+++ b/src/slack/markdown.ts\\n@@ -3,14 +3,191 @@\\n // without pulling in a full Markdown parser.\\n //\\n // Supported conversions:\\n+// - # Header / ## Header  ‚Üí *Header* (bold with visual separation)\\n // - **bold** / __bold__   ‚Üí *bold*\\n // - [label](url)          ‚Üí <url|label>\\n // - ![alt](url)           ‚Üí <url|alt>\\n // - *italic* (inline)     ‚Üí _italic_\\n+// - ```mermaid blocks     ‚Üí rendered to PNG and uploaded to Slack\\n //\\n // Everything else is passed through unchanged; Slack will still render many\\n // Markdown-like constructs (lists, code fences, etc.) natively.\\n \\n+import { spawn } from 'child_process';\\n+import * as fs from 'fs';\\n+import * as path from 'path';\\n+import * as os from 'os';\\n+\\n+/**\\n+ * Represents an extracted mermaid diagram\\n+ */\\n+export interface MermaidDiagram {\\n+  /** The full match including ```mermaid and ``` */\\n+  fullMatch: string;\\n+  /** The mermaid code content */\\n+  code: string;\\n+  /** Start index in the original text */\\n+  startIndex: number;\\n+  /** End index in the original text */\\n+  endIndex: number;\\n+}\\n+\\n+/**\\n+ * Extract all mermaid code blocks from text\\n+ */\\n+export function extractMermaidDiagrams(text: string): MermaidDiagram[] {\\n+  const diagrams: MermaidDiagram[] = [];\\n+  // Match ```mermaid followed by newline, content, and closing ```\\n+  const regex = /```mermaid\\\\s*\\\\n([\\\\s\\\\S]*?)```/g;\\n+  let match;\\n+  while ((match = regex.exec(text)) !== null) {\\n+    diagrams.push({\\n+      fullMatch: match[0],\\n+      code: match[1].trim(),\\n+      startIndex: match.index,\\n+      endIndex: match.index + match[0].length,\\n+    });\\n+  }\\n+  return diagrams;\\n+}\\n+\\n+/**\\n+ * Render a mermaid diagram to PNG using mmdc CLI (@mermaid-js/mermaid-cli).\\n+ *\\n+ * Requirements:\\n+ * - Node.js and npx must be available in PATH\\n+ * - Network access on first run (npx downloads the package)\\n+ * - Puppeteer/Chromium dependencies (mermaid-cli uses headless browser)\\n+ *\\n+ * On Linux, you may need to install chromium dependencies:\\n+ *   apt-get install -y chromium-browser libatk-bridge2.0-0 libgtk-3-0\\n+ *\\n+ * On Docker/CI, consider using a base image with puppeteer support or\\n+ * pre-installing @mermaid-js/mermaid-cli globally.\\n+ *\\n+ * @param mermaidCode The mermaid diagram code\\n+ * @returns Buffer containing PNG data, or null if rendering failed\\n+ */\\n+export async function renderMermaidToPng(mermaidCode: string): Promise<Buffer | null> {\\n+  // Create temp files for input and output\\n+  const tmpDir = os.tmpdir();\\n+  const inputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.mmd`);\\n+  const outputFile = path.join(tmpDir, `mermaid-${Date.now()}-${Math.random().toString(36).slice(2)}.png`);\\n+\\n+  try {\\n+    // Write mermaid code to temp file\\n+    fs.writeFileSync(inputFile, mermaidCode, 'utf-8');\\n+\\n+    // Detect system chromium for puppeteer (mermaid-cli dependency)\\n+    // Without this, puppeteer may hang trying to download its own chromium\\n+    const chromiumPaths = [\\n+      '/usr/bin/chromium',\\n+      '/usr/bin/chromium-browser',\\n+      '/usr/bin/google-chrome',\\n+      '/usr/bin/chrome',\\n+    ];\\n+    let chromiumPath: string | undefined;\\n+    for (const p of chromiumPaths) {\\n+      if (fs.existsSync(p)) {\\n+        chromiumPath = p;\\n+        break;\\n+      }\\n+    }\\n+\\n+    // Build environment with chromium path if found\\n+    const env = { ...process.env };\\n+    if (chromiumPath) {\\n+      env.PUPPETEER_EXECUTABLE_PATH = chromiumPath;\\n+    }\\n+\\n+    // Run mmdc to render PNG\\n+    const result = await new Promise<{ success: boolean; error?: string }>((resolve) => {\\n+      const proc = spawn('npx', [\\n+        '--yes',\\n+        '@mermaid-js/mermaid-cli',\\n+        '-i', inputFile,\\n+        '-o', outputFile,\\n+        '-e', 'png',\\n+        '-b', 'white',\\n+        '-w', '1200',\\n+      ], {\\n+        timeout: 60000, // 60 second timeout (first run may download packages)\\n+        stdio: ['pipe', 'pipe', 'pipe'],\\n+        env,\\n+      });\\n+\\n+      let stderr = '';\\n+      proc.stderr?.on('data', (data) => {\\n+        stderr += data.toString();\\n+      });\\n+\\n+      proc.on('close', (code) => {\\n+        if (code === 0) {\\n+          resolve({ success: true });\\n+        } else {\\n+          resolve({ success: false, error: stderr || `Exit code ${code}` });\\n+        }\\n+      });\\n+\\n+      proc.on('error', (err) => {\\n+        resolve({ success: false, error: err.message });\\n+      });\\n+    });\\n+\\n+    if (!result.success) {\\n+      console.warn(`Mermaid rendering failed: ${result.error}`);\\n+      return null;\\n+    }\\n+\\n+    // Read the output PNG\\n+    if (!fs.existsSync(outputFile)) {\\n+      console.warn('Mermaid output file not created');\\n+      return null;\\n+    }\\n+\\n+    const pngBuffer = fs.readFileSync(outputFile);\\n+    return pngBuffer;\\n+  } catch (e) {\\n+    console.warn(`Mermaid rendering error: ${e instanceof Error ? e.message : String(e)}`);\\n+    return null;\\n+  } finally {\\n+    // Cleanup temp files\\n+    try {\\n+      if (fs.existsSync(inputFile)) fs.unlinkSync(inputFile);\\n+      if (fs.existsSync(outputFile)) fs.unlinkSync(outputFile);\\n+    } catch {\\n+      // Ignore cleanup errors\\n+    }\\n+  }\\n+}\\n+\\n+/**\\n+ * Replace mermaid blocks in text with a placeholder message\\n+ * @param text Original text\\n+ * @param diagrams Extracted diagrams\\n+ * @param replacement Text to replace each diagram with (or a function that returns replacement for each index)\\n+ */\\n+export function replaceMermaidBlocks(\\n+  text: string,\\n+  diagrams: MermaidDiagram[],\\n+  replacement: string | ((index: number) => string) = '_(See diagram above)_'\\n+): string {\\n+  if (diagrams.length === 0) return text;\\n+\\n+  // Sort by start index descending to replace from end to start (preserves indices)\\n+  const sorted = [...diagrams].sort((a, b) => b.startIndex - a.startIndex);\\n+\\n+  let result = text;\\n+  sorted.forEach((diagram, sortedIndex) => {\\n+    // Calculate original index (since we sorted in reverse)\\n+    const originalIndex = diagrams.length - 1 - sortedIndex;\\n+    const rep = typeof replacement === 'function' ? replacement(originalIndex) : replacement;\\n+    result = result.slice(0, diagram.startIndex) + rep + result.slice(diagram.endIndex);\\n+  });\\n+\\n+  return result;\\n+}\\n+\\n export function markdownToSlack(text: string): string {\\n   if (!text || typeof text !== 'string') return '';\\n \\n@@ -33,7 +210,7 @@ export function markdownToSlack(text: string): string {\\n   out = out.replace(/\\\\*\\\\*([^*]+)\\\\*\\\\*/g, (_m, inner: string) => `*${inner}*`);\\n   out = out.replace(/__([^_]+)__/g, (_m, inner: string) => `*${inner}*`);\\n \\n-  // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation).\\n+  // Process lines for headers and bullet lists.\\n   // Slack's mrkdwn handles \\\"‚Ä¢\\\" bullets more naturally than raw \\\"-\\\" Markdown.\\n   const lines = out.split(/\\\\r?\\\\n/);\\n   let inCodeBlock = false;\\n@@ -46,9 +223,31 @@ export function markdownToSlack(text: string): string {\\n       continue;\\n     }\\n     if (inCodeBlock) continue;\\n-    const match = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n-    if (match) {\\n-      const [, indent, , rest] = match;\\n+\\n+    // Headers: # Header ‚Üí *Header* (Slack doesn't have native headers)\\n+    // Match 1-6 # at start of line, followed by space and text\\n+    const headerMatch = /^(#{1,6})\\\\s+(.+)$/.exec(trimmed);\\n+    if (headerMatch) {\\n+      const [, hashes, headerText] = headerMatch;\\n+      // For h1/h2, add extra emphasis with newline before (if not first line\\n+      // and previous line is not empty/header/code-fence)\\n+      const prevLine = i > 0 ? lines[i - 1].trim() : '';\\n+      const prevIsHeaderOrFence =\\n+        /^#{1,6}\\\\s+/.test(prevLine) ||\\n+        /^\\\\*[^*]+\\\\*$/.test(prevLine) ||\\n+        /^```/.test(prevLine);\\n+      if (hashes.length <= 2 && i > 0 && prevLine !== '' && !prevIsHeaderOrFence) {\\n+        lines[i] = `\\\\n*${headerText.trim()}*`;\\n+      } else {\\n+        lines[i] = `*${headerText.trim()}*`;\\n+      }\\n+      continue;\\n+    }\\n+\\n+    // Bullet lists: \\\"- item\\\" or \\\"* item\\\" ‚Üí \\\"‚Ä¢ item\\\" (preserve indentation)\\n+    const bulletMatch = /^(\\\\s*)([-*])\\\\s+(.+)$/.exec(line);\\n+    if (bulletMatch) {\\n+      const [, indent, , rest] = bulletMatch;\\n       lines[i] = `${indent}‚Ä¢ ${rest}`;\\n     }\\n   }\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/prompt-state.ts\",\"additions\":19,\"deletions\":2,\"changes\":21,\"patch\":\"diff --git a/src/slack/prompt-state.ts b/src/slack/prompt-state.ts\\nindex a13233b2..544777c6 100644\\n--- a/src/slack/prompt-state.ts\\n+++ b/src/slack/prompt-state.ts\\n@@ -95,7 +95,10 @@ export class PromptStateManager {\\n   setFirstMessage(channel: string, threadTs: string, text: string): void {\\n     const key = this.key(channel, threadTs);\\n     if (!text || !text.trim()) return;\\n-    if (!this.firstMessage.has(key)) {\\n+    const existing = this.firstMessage.get(key);\\n+    // Only set if: no entry exists OR the existing entry was already consumed\\n+    // This allows new messages to be captured after a resume cycle\\n+    if (!existing || existing.consumed) {\\n       this.firstMessage.set(key, { text, consumed: false });\\n     }\\n   }\\n@@ -130,6 +133,20 @@ export class PromptStateManager {\\n         removed++;\\n       }\\n     }\\n+    // Also clean up stale firstMessage entries (consumed entries older than TTL)\\n+    // Keep unconsumed entries to avoid losing user messages\\n+    for (const [key] of this.firstMessage.entries()) {\\n+      const waitingInfo = this.waiting.get(key);\\n+      // If no corresponding waiting entry exists and the firstMessage was consumed,\\n+      // the conversation is likely complete - safe to remove\\n+      if (!waitingInfo) {\\n+        const entry = this.firstMessage.get(key);\\n+        if (entry?.consumed) {\\n+          this.firstMessage.delete(key);\\n+          removed++;\\n+        }\\n+      }\\n+    }\\n     if (removed) {\\n       try {\\n         logger.info(`[prompt-state] cleanup removed ${removed} entries`);\\n\",\"status\":\"modified\"},{\"filename\":\"src/slack/socket-runner.ts\",\"additions\":56,\"deletions\":32,\"changes\":88,\"patch\":\"diff --git a/src/slack/socket-runner.ts b/src/slack/socket-runner.ts\\nindex d1681a03..5b461d22 100644\\n--- a/src/slack/socket-runner.ts\\n+++ b/src/slack/socket-runner.ts\\n@@ -7,6 +7,7 @@ import { SlackAdapter } from './adapter';\\n import { CachePrewarmer } from './cache-prewarmer';\\n import { RateLimiter, type RateLimitConfig } from './rate-limiter';\\n import type { SlackBotConfig } from '../types/bot';\\n+import { withActiveSpan } from '../telemetry/trace-helpers';\\n \\n type SlackSocketConfig = {\\n   appToken?: string; // xapp- token\\n@@ -30,6 +31,7 @@ export class SlackSocketRunner {\\n   private botUserId?: string;\\n   private processedKeys: Map<string, number> = new Map();\\n   private adapter?: SlackAdapter;\\n+  private retryCount = 0;\\n \\n   constructor(engine: StateMachineExecutionEngine, cfg: VisorConfig, opts: SlackSocketConfig) {\\n     const app = opts.appToken || process.env.SLACK_APP_TOKEN || '';\\n@@ -107,7 +109,10 @@ export class SlackSocketRunner {\\n \\n   private async connect(url: string): Promise<void> {\\n     this.ws = new WebSocket(url);\\n-    this.ws.on('open', () => logger.info('[SlackSocket] WebSocket connected'));\\n+    this.ws.on('open', () => {\\n+      this.retryCount = 0; // Reset on successful connection\\n+      logger.info('[SlackSocket] WebSocket connected');\\n+    });\\n     this.ws.on('close', (code, reason) => {\\n       logger.warn(`[SlackSocket] WebSocket closed: ${code} ${reason}`);\\n       setTimeout(() => this.restart().catch(() => {}), 1000);\\n@@ -123,7 +128,13 @@ export class SlackSocketRunner {\\n       const url = await this.openConnection();\\n       await this.connect(url);\\n     } catch (e) {\\n-      logger.error(`[SlackSocket] Restart failed: ${e instanceof Error ? e.message : e}`);\\n+      this.retryCount++;\\n+      // Exponential backoff: 2s, 4s, 8s, 16s, 32s, capped at 60s\\n+      const delay = Math.min(2000 * Math.pow(2, this.retryCount - 1), 60000);\\n+      logger.error(\\n+        `[SlackSocket] Restart failed (attempt ${this.retryCount}), retrying in ${Math.round(delay / 1000)}s: ${e instanceof Error ? e.message : e}`\\n+      );\\n+      setTimeout(() => this.restart().catch(() => {}), delay);\\n     }\\n   }\\n \\n@@ -296,36 +307,49 @@ export class SlackSocketRunner {\\n           });\\n         }\\n       } catch {}\\n-      if (path) {\\n-        try {\\n-          const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n-          const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n-          logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n-          await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n-            webhookContext: { webhookData: map, eventType: 'manual' },\\n-            debug: process.env.VISOR_DEBUG === 'true',\\n-          });\\n-          if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n-          return; // resume path handled\\n-        } catch (e) {\\n-          logger.warn(\\n-            `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n-              e instanceof Error ? e.message : String(e)\\n-            }`\\n-          );\\n-        }\\n-      }\\n+      try {\\n+        await withActiveSpan(\\n+          'visor.run',\\n+          {\\n+            'visor.run.source': 'slack',\\n+            'slack.event.type': String(type || ''),\\n+            'slack.channel': channelId,\\n+            'slack.thread_ts': threadTs,\\n+          },\\n+          async () => {\\n+            if (path) {\\n+              try {\\n+                const snapshot = await (runEngine as any).loadSnapshotFromFile(path);\\n+                const { resumeFromSnapshot } = await import('../state-machine-execution-engine');\\n+                logger.info(`[SlackSocket] Resuming from snapshot: ${path}`);\\n+                await resumeFromSnapshot(runEngine, snapshot, cfgForRun, {\\n+                  webhookContext: { webhookData: map, eventType: 'manual' },\\n+                  debug: process.env.VISOR_DEBUG === 'true',\\n+                });\\n+                return;\\n+              } catch (e) {\\n+                logger.warn(\\n+                  `[SlackSocket] Snapshot resume failed, falling back to cold run: ${\\n+                    e instanceof Error ? e.message : String(e)\\n+                  }`\\n+                );\\n+              }\\n+            }\\n \\n-      // Cold run (no snapshot)\\n-      await runEngine.executeChecks({\\n-        checks: allChecks,\\n-        showDetails: true,\\n-        outputFormat: 'json',\\n-        config: cfgForRun,\\n-        webhookContext: { webhookData: map, eventType: 'manual' },\\n-        debug: process.env.VISOR_DEBUG === 'true',\\n-      } as any);\\n-      if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+            // Cold run (no snapshot)\\n+            await runEngine.executeChecks({\\n+              checks: allChecks,\\n+              showDetails: true,\\n+              outputFormat: 'json',\\n+              config: cfgForRun,\\n+              webhookContext: { webhookData: map, eventType: 'manual' },\\n+              debug: process.env.VISOR_DEBUG === 'true',\\n+            } as any);\\n+          }\\n+        );\\n+      } finally {\\n+        if (this.limiter && rlReq) await this.limiter.release(rlReq);\\n+      }\\n     } catch (e) {\\n       logger.error(\\n         `[SlackSocket] Engine execution failed: ${e instanceof Error ? e.message : String(e)}`\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/context/build-engine-context.ts\",\"additions\":11,\"deletions\":4,\"changes\":15,\"patch\":\"diff --git a/src/state-machine/context/build-engine-context.ts b/src/state-machine/context/build-engine-context.ts\\nindex 149c73bf..45e63b6a 100644\\n--- a/src/state-machine/context/build-engine-context.ts\\n+++ b/src/state-machine/context/build-engine-context.ts\\n@@ -57,7 +57,8 @@ export function buildEngineContextForRun(\\n       ) as EventTrigger[],\\n       group: checkConfig.group,\\n       providerType: checkConfig.type || 'ai',\\n-      dependencies: checkConfig.depends_on || [],\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: Array.isArray(checkConfig.depends_on) ? checkConfig.depends_on : checkConfig.depends_on ? [checkConfig.depends_on] : [],\\n     };\\n   }\\n \\n@@ -134,11 +135,14 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n   const originalPath = context.workingDirectory || process.cwd();\\n \\n   try {\\n+    // Check if workspace should be kept (for debugging)\\n+    const keepWorkspace = process.env.VISOR_KEEP_WORKSPACE === 'true';\\n+\\n     // Create workspace manager\\n     const workspace = WorkspaceManager.getInstance(context.sessionId, originalPath, {\\n       enabled: true,\\n-      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH,\\n-      cleanupOnExit: workspaceConfig?.cleanup_on_exit !== false,\\n+      basePath: workspaceConfig?.base_path || process.env.VISOR_WORKSPACE_PATH || '/tmp/visor-workspaces',\\n+      cleanupOnExit: keepWorkspace ? false : workspaceConfig?.cleanup_on_exit !== false,\\n     });\\n \\n     // Initialize workspace (creates main project worktree)\\n@@ -151,6 +155,9 @@ export async function initializeWorkspace(context: EngineContext): Promise<Engin\\n \\n     logger.info(`[Workspace] Initialized workspace: ${info.workspacePath}`);\\n     logger.debug(`[Workspace] Main project at: ${info.mainProjectPath}`);\\n+    if (keepWorkspace) {\\n+      logger.info(`[Workspace] Keeping workspace after execution (--keep-workspace)`);\\n+    }\\n \\n     return context;\\n   } catch (error) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/execution-invoker.ts\",\"additions\":84,\"deletions\":4,\"changes\":88,\"patch\":\"diff --git a/src/state-machine/dispatch/execution-invoker.ts b/src/state-machine/dispatch/execution-invoker.ts\\nindex eb7587d3..798ca8e4 100644\\n--- a/src/state-machine/dispatch/execution-invoker.ts\\n+++ b/src/state-machine/dispatch/execution-invoker.ts\\n@@ -356,10 +356,17 @@ export async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -413,6 +420,31 @@ export async function executeSingleCheck(\\n     }\\n   }\\n \\n+  // Banner-style log when a check actually starts executing. This is emitted\\n+  // after all gating (if/depends_on) has passed so it only appears for real\\n+  // runs, and gives a clear visual separator in the logs between checks.\\n+  try {\\n+    const wave = state.wave;\\n+    const level = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${level}) ‚îÅ‚îÅ‚îÅ`;\\n+\\n+    // When running in a TTY, colour the entire banner line for extra\\n+    // visibility; keep plain text for JSON/SARIF or non-TTY environments.\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   let forEachParent: string | undefined;\\n   let forEachItems: unknown[] | undefined;\\n   for (const depId of depList) {\\n@@ -563,13 +595,56 @@ export async function executeSingleCheck(\\n       files: [],\\n       commits: [],\\n     };\\n+    // Derive AI session reuse context for this check (self-mode only for now).\\n+    // When reuse_ai_session: 'self' is configured, look up the last root-scope\\n+    // journal entry for this check in the current engine session and expose its\\n+    // sessionId via ExecutionContext so ai-check-provider can call\\n+    // executeReviewWithSessionReuse() against the same ProbeAgent session.\\n+    let parentSessionId: string | undefined;\\n+    let reuseSession = false;\\n+    try {\\n+      const reuseCfg: unknown = (checkConfig as any).reuse_ai_session;\\n+      if (reuseCfg === 'self') {\\n+        const snapshotId = context.journal.beginSnapshot();\\n+        const visible = context.journal.readVisible(\\n+          context.sessionId,\\n+          snapshotId,\\n+          context.event as any\\n+        );\\n+        // Prefer the most recent root-scope result for this check\\n+        const prior = visible.filter(\\n+          e => e.checkId === checkId && (!e.scope || e.scope.length === 0)\\n+        );\\n+        if (prior.length > 0) {\\n+          const last = prior[prior.length - 1];\\n+          const sess = (last.result as any)?.sessionId;\\n+          if (typeof sess === 'string' && sess.length > 0) {\\n+            parentSessionId = sess;\\n+            reuseSession = true;\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // Best-effort only ‚Äì fall back to normal (non-reuse) execution on error.\\n+      parentSessionId = undefined;\\n+      reuseSession = false;\\n+    }\\n+\\n     const executionContext = {\\n       ...context.executionContext,\\n       _engineMode: context.mode,\\n       _parentContext: context,\\n       _parentState: state,\\n+      // Explicitly propagate workspace reference for nested workflows\\n+      workspace: context.workspace,\\n     };\\n \\n+    // Attach session reuse hints for providers that support them (AI, Claude Code, etc).\\n+    if (reuseSession && parentSessionId) {\\n+      (executionContext as any).parentSessionId = parentSessionId;\\n+      (executionContext as any).reuseSession = true;\\n+    }\\n+\\n     // Handle on_init lifecycle hook BEFORE main execution\\n     if (checkConfig.on_init) {\\n       try {\\n@@ -613,7 +688,12 @@ export async function executeSingleCheck(\\n \\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/foreach-processor.ts\",\"additions\":27,\"deletions\":2,\"changes\":29,\"patch\":\"diff --git a/src/state-machine/dispatch/foreach-processor.ts b/src/state-machine/dispatch/foreach-processor.ts\\nindex 336bf9df..ab630986 100644\\n--- a/src/state-machine/dispatch/foreach-processor.ts\\n+++ b/src/state-machine/dispatch/foreach-processor.ts\\n@@ -77,6 +77,26 @@ export async function executeCheckWithForEachItems(\\n   const allContents: string[] = [];\\n   const perIterationDurations: number[] = [];\\n \\n+  // Emit a banner for the forEach parent so logs clearly show when we are\\n+  // entering the aggregated execution for that step.\\n+  try {\\n+    const wave = state.wave;\\n+    const lvl = (state as any).currentLevel ?? '?';\\n+    const banner = `‚îÅ‚îÅ‚îÅ CHECK ${checkId} (wave ${wave}, level ${lvl}, forEach parent) ‚îÅ‚îÅ‚îÅ`;\\n+    const isTTY = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+    const outputFormat = process.env.VISOR_OUTPUT_FORMAT || '';\\n+    const isJsonLike = outputFormat === 'json' || outputFormat === 'sarif';\\n+    if (isTTY && !isJsonLike) {\\n+      const cyan = '\\\\x1b[36m';\\n+      const reset = '\\\\x1b[0m';\\n+      logger.info(`${cyan}${banner}${reset}`);\\n+    } else {\\n+      logger.info(banner);\\n+    }\\n+  } catch {\\n+    // best-effort only\\n+  }\\n+\\n   for (let itemIndex = 0; itemIndex < forEachItems.length; itemIndex++) {\\n     const iterationStartMs = Date.now();\\n     const scope: Array<{ check: string; index: number }> = [\\n@@ -248,7 +268,12 @@ export async function executeCheckWithForEachItems(\\n \\n       const result = await withActiveSpan(\\n         `visor.check.${checkId}`,\\n-        { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+        {\\n+          'visor.check.id': checkId,\\n+          'visor.check.type': providerType,\\n+          session_id: context.sessionId,\\n+          wave: state.wave,\\n+        },\\n         async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n       );\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/dispatch/stats-manager.ts\",\"additions\":21,\"deletions\":1,\"changes\":22,\"patch\":\"diff --git a/src/state-machine/dispatch/stats-manager.ts b/src/state-machine/dispatch/stats-manager.ts\\nindex 8d04436e..2f317b59 100644\\n--- a/src/state-machine/dispatch/stats-manager.ts\\n+++ b/src/state-machine/dispatch/stats-manager.ts\\n@@ -44,6 +44,26 @@ export function updateStats(\\n       issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n     };\\n \\n+    // Respect explicit skip markers on the result (set by LevelDispatch).\\n+    const skippedMarker = (result as any).__skipped;\\n+    if (skippedMarker) {\\n+      stats.skipped = true;\\n+      stats.skipReason =\\n+        typeof skippedMarker === 'string'\\n+          ? (skippedMarker as any)\\n+          : (stats.skipReason as any) || 'if_condition';\\n+      // A skipped result should not contribute to run counters; reset\\n+      // them explicitly to guard against any prior values lingering\\n+      // from earlier runs of the same check.\\n+      stats.totalRuns = 0;\\n+      stats.successfulRuns = 0;\\n+      stats.failedRuns = 0;\\n+      stats.skippedRuns++;\\n+      // Do not count this as a run; keep totalRuns/failed/success unchanged.\\n+      state.stats.set(checkId, stats);\\n+      continue;\\n+    }\\n+\\n     // If this check was previously marked as skipped and now executed,\\n     // clear the skipped flag and any skip counters to ensure the run is visible.\\n     if (stats.skipped) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/runner.ts\",\"additions\":10,\"deletions\":7,\"changes\":17,\"patch\":\"diff --git a/src/state-machine/runner.ts b/src/state-machine/runner.ts\\nindex ee86b876..a13a5efd 100644\\n--- a/src/state-machine/runner.ts\\n+++ b/src/state-machine/runner.ts\\n@@ -124,14 +124,17 @@ export class StateMachineRunner {\\n    */\\n   private async executeState(state: EngineState): Promise<void> {\\n     // M4: Wrap state execution in OTEL span\\n+    const attrs: Record<string, unknown> = {\\n+      state: state,\\n+      engine_mode: this.context.mode,\\n+      wave: this.state.wave,\\n+      session_id: this.context.sessionId,\\n+    };\\n+    const waveKind = (this.state as any)?.flags?.waveKind;\\n+    if (waveKind) attrs.wave_kind = waveKind;\\n     return withActiveSpan(\\n       `engine.state.${state.toLowerCase()}`,\\n-      {\\n-        state: state,\\n-        engine_mode: this.context.mode,\\n-        wave: this.state.wave,\\n-        session_id: this.context.sessionId,\\n-      },\\n+      attrs,\\n       async () => {\\n         try {\\n           switch (state) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/level-dispatch.ts\",\"additions\":152,\"deletions\":15,\"changes\":167,\"patch\":\"diff --git a/src/state-machine/states/level-dispatch.ts b/src/state-machine/states/level-dispatch.ts\\nindex 5588b0f5..95392197 100644\\n--- a/src/state-machine/states/level-dispatch.ts\\n+++ b/src/state-machine/states/level-dispatch.ts\\n@@ -26,7 +26,7 @@ import type { CheckExecutionStats } from '../../types/execution';\\n import type { CheckProviderConfig } from '../../providers/check-provider.interface';\\n import type { CheckConfig } from '../../types/config';\\n import { handleRouting, checkLoopBudget } from './routing';\\n-import { withActiveSpan } from '../../telemetry/trace-helpers';\\n+import { withActiveSpan, setSpanAttributes, addEvent } from '../../telemetry/trace-helpers';\\n import { emitMermaidFromMarkdown } from '../../utils/mermaid-telemetry';\\n import { emitNdjsonSpanWithEvents, emitNdjsonFallback } from '../../telemetry/fallback-ndjson';\\n import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n@@ -46,6 +46,32 @@ function mapCheckNameToFocus(checkName: string): string {\\n   return focusMap[checkName] || 'all';\\n }\\n \\n+function formatScopeLabel(scope: Array<{ check: string; index: number }> | undefined): string {\\n+  if (!scope || scope.length === 0) return '';\\n+  return scope.map(item => `${item.check}:${item.index}`).join('|');\\n+}\\n+\\n+function recordOnFinishRoutingEvent(args: {\\n+  checkId: string;\\n+  action: 'run' | 'goto';\\n+  target: string;\\n+  source: 'run' | 'goto' | 'goto_js' | 'transitions';\\n+  scope?: Array<{ check: string; index: number }>;\\n+  gotoEvent?: string;\\n+}): void {\\n+  const attrs: Record<string, unknown> = {\\n+    check_id: args.checkId,\\n+    trigger: 'on_finish',\\n+    action: args.action,\\n+    target: args.target,\\n+    source: args.source,\\n+  };\\n+  const scopeLabel = formatScopeLabel(args.scope);\\n+  if (scopeLabel) attrs.scope = scopeLabel;\\n+  if (args.gotoEvent) attrs.goto_event = args.gotoEvent;\\n+  addEvent('visor.routing', attrs);\\n+}\\n+\\n /**\\n  * Build output history Map from journal for template rendering\\n  * This matches the format expected by AI providers\\n@@ -121,12 +147,13 @@ async function evaluateIfCondition(\\n         return false;\\n       }\\n     })();\\n+    // Steps with dependencies should always see outputs from all completed steps.\\n     // In forward-run waves (from on_success/on_fail goto), guards should see the\\n     // latest global outputs even if the check has no explicit dependencies.\\n     // In wave-retry (from on_finish), restrict to checks with dependencies to\\n     // avoid wrongly skipping top-level prompts like 'ask'.\\n     const useGlobalOutputs =\\n-      (useGlobalOutputsFlag && waveKind === 'forward') || (useGlobalOutputsFlag && hasDeps);\\n+      hasDeps || (useGlobalOutputsFlag && waveKind === 'forward');\\n \\n     if (useGlobalOutputs) {\\n       // Forward-run wave: allow guards to consult latest outputs from the entire\\n@@ -199,6 +226,7 @@ async function evaluateIfCondition(\\n       baseBranch: (context.prInfo as any)?.baseBranch,\\n       filesChanged: context.prInfo?.files?.map(f => f.filename),\\n       environment: envSnapshot,\\n+      workflowInputs: (context.config as any).workflow_inputs || {},\\n     };\\n \\n     const shouldRun = await evaluator.evaluateIfCondition(checkId, ifExpression, contextData);\\n@@ -238,6 +266,11 @@ export async function handleLevelDispatch(\\n   // Update current level tracking\\n   state.currentLevel = level.level;\\n   state.currentLevelChecks = new Set(level.parallel);\\n+  const levelChecksPreview = level.parallel.slice(0, 5).join(',');\\n+  setSpanAttributes({\\n+    level_size: level.parallel.length,\\n+    level_checks_preview: levelChecksPreview,\\n+  });\\n \\n   // Emit level ready event\\n   emitEvent({ type: 'LevelReady', level, wave: state.wave });\\n@@ -689,6 +722,39 @@ async function executeCheckWithForEachItems(\\n         }\\n       } catch {}\\n \\n+      // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+      // The socket-runner stores conversation data in webhookData under the endpoint key\\n+      try {\\n+        const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+        const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+        if (context.debug) {\\n+          logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+        }\\n+        if (webhookData && webhookData.size > 0) {\\n+          // Find the payload with slack_conversation\\n+          for (const payload of webhookData.values()) {\\n+            const slackConv = (payload as any)?.slack_conversation;\\n+            if (slackConv) {\\n+              // Build slack context with event and conversation\\n+              const event = (payload as any)?.event;\\n+              const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+              if (context.debug) {\\n+                logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+              }\\n+              (providerConfig as any).eventContext = {\\n+                ...(providerConfig as any).eventContext,\\n+                slack: {\\n+                  event: event || {},\\n+                  conversation: slackConv,\\n+                },\\n+                conversation: slackConv, // Also expose at top level for convenience\\n+              };\\n+              break;\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n+\\n       // Build dependency results with scope\\n       const dependencyResults = buildDependencyResultsWithScope(\\n         checkId,\\n@@ -820,15 +886,17 @@ async function executeCheckWithForEachItems(\\n       } catch {}\\n \\n       // Execute provider with telemetry\\n-      const itemResult = await withActiveSpan(\\n-        `visor.check.${checkId}`,\\n-        {\\n-          'visor.check.id': checkId,\\n-          'visor.check.type': providerType,\\n-          'visor.foreach.index': itemIndex,\\n-        },\\n-        async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n-      );\\n+    const itemResult = await withActiveSpan(\\n+      `visor.check.${checkId}`,\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        'visor.foreach.index': itemIndex,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n+      async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n+    );\\n \\n       // Enrich issues\\n       const enrichedIssues = (itemResult.issues || []).map((issue: ReviewIssue) => ({\\n@@ -942,6 +1010,7 @@ async function executeCheckWithForEachItems(\\n             const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n               previousResults: dependencyResults as any,\\n               event: context.event || 'manual',\\n+              output: output, // Pass the iteration output for guarantee evaluation\\n             } as any);\\n             if (!holds) {\\n               const issue: ReviewIssue = {\\n@@ -1311,6 +1380,13 @@ async function executeCheckWithForEachItems(\\n             // Increment loop count\\n             state.routingLoopCount++;\\n \\n+            recordOnFinishRoutingEvent({\\n+              checkId: forEachParent,\\n+              action: 'run',\\n+              target: targetCheck,\\n+              source: 'run',\\n+              scope: [],\\n+            });\\n             emitEvent({\\n               type: 'ForwardRunRequested',\\n               target: targetCheck,\\n@@ -1358,6 +1434,14 @@ async function executeCheckWithForEachItems(\\n                 return aggregatedResult; // abort further routing\\n               }\\n               state.routingLoopCount++;\\n+              recordOnFinishRoutingEvent({\\n+                checkId: forEachParent,\\n+                action: 'goto',\\n+                target: transTarget.to,\\n+                source: 'transitions',\\n+                scope: [],\\n+                gotoEvent: (transTarget as any).goto_event,\\n+              });\\n               emitEvent({\\n                 type: 'ForwardRunRequested',\\n                 target: transTarget.to,\\n@@ -1464,6 +1548,13 @@ async function executeCheckWithForEachItems(\\n           // Increment loop count\\n           state.routingLoopCount++;\\n \\n+          recordOnFinishRoutingEvent({\\n+            checkId: forEachParent,\\n+            action: 'goto',\\n+            target: gotoTarget,\\n+            source: onFinish.goto_js ? 'goto_js' : 'goto',\\n+            scope: [],\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: gotoTarget,\\n@@ -1618,10 +1709,17 @@ async function executeSingleCheck(\\n       const depCfg: any = context.config.checks?.[opt];\\n       const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n       const st = state.stats.get(opt);\\n-      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n       const skipped = !!(st && (st as any).skipped === true);\\n+      const skipReason = (st as any)?.skipReason;\\n+      // forEach_empty is not a failure - it means there was nothing to process, which is valid\\n+      // The dependent step should still run (with empty data from the forEach step)\\n+      const skippedDueToEmptyForEach = skipped && skipReason === 'forEach_empty';\\n+      // Don't treat forEach_empty as a failure even if it's in failedChecks\\n+      // (forEach_empty checks are added to failedChecks for cascading within forEach chains,\\n+      // but should not be treated as failures for non-forEach dependents)\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt)) && !skippedDueToEmptyForEach;\\n       const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n-      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      const satisfied = (!skipped || skippedDueToEmptyForEach) && ((!failedOnly && !wasMarkedFailed) || cont);\\n       if (satisfied) return true;\\n     }\\n     return false;\\n@@ -1923,6 +2021,39 @@ async function executeSingleCheck(\\n       }\\n     } catch {}\\n \\n+    // Extract Slack conversation from webhookContext (for Slack socket mode)\\n+    // The socket-runner stores conversation data in webhookData under the endpoint key\\n+    try {\\n+      const webhookCtx = (context.executionContext as any)?.webhookContext;\\n+      const webhookData = webhookCtx?.webhookData as Map<string, unknown> | undefined;\\n+      if (context.debug) {\\n+        logger.info(`[LevelDispatch] webhookContext: ${webhookCtx ? 'present' : 'absent'}, webhookData size: ${webhookData?.size || 0}`);\\n+      }\\n+      if (webhookData && webhookData.size > 0) {\\n+        // Find the payload with slack_conversation\\n+        for (const payload of webhookData.values()) {\\n+          const slackConv = (payload as any)?.slack_conversation;\\n+          if (slackConv) {\\n+            // Build slack context with event and conversation\\n+            const event = (payload as any)?.event;\\n+            const messageCount = Array.isArray(slackConv?.messages) ? slackConv.messages.length : 0;\\n+            if (context.debug) {\\n+              logger.info(`[LevelDispatch] Slack conversation extracted: ${messageCount} messages`);\\n+            }\\n+            (providerConfig as any).eventContext = {\\n+              ...(providerConfig as any).eventContext,\\n+              slack: {\\n+                event: event || {},\\n+                conversation: slackConv,\\n+              },\\n+              conversation: slackConv, // Also expose at top level for convenience\\n+            };\\n+            break;\\n+          }\\n+        }\\n+      }\\n+    } catch {}\\n+\\n     // Build dependency results\\n     const dependencyResults = buildDependencyResults(checkId, checkConfig, context, state);\\n \\n@@ -2016,7 +2147,12 @@ async function executeSingleCheck(\\n     // Execute provider with telemetry\\n     const result = await withActiveSpan(\\n       `visor.check.${checkId}`,\\n-      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      {\\n+        'visor.check.id': checkId,\\n+        'visor.check.type': providerType,\\n+        session_id: context.sessionId,\\n+        wave: state.wave,\\n+      },\\n       async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n     );\\n \\n@@ -2098,6 +2234,7 @@ async function executeSingleCheck(\\n           const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n             previousResults: dependencyResults as any,\\n             event: context.event || 'manual',\\n+            output: enrichedResult.output,\\n           } as any);\\n           if (!holds) {\\n             const issue: ReviewIssue = {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/routing.ts\",\"additions\":165,\"deletions\":1,\"changes\":166,\"patch\":\"diff --git a/src/state-machine/states/routing.ts b/src/state-machine/states/routing.ts\\nindex 5752a5bb..b1fd8654 100644\\n--- a/src/state-machine/states/routing.ts\\n+++ b/src/state-machine/states/routing.ts\\n@@ -15,6 +15,7 @@ import type { EngineContext, RunState, EngineState, EngineEvent } from '../../ty\\n import type { ReviewSummary, ReviewIssue } from '../../reviewer';\\n import type { CheckConfig, OnFailConfig, TransitionRule } from '../../types/config';\\n import { logger } from '../../logger';\\n+import { addEvent } from '../../telemetry/trace-helpers';\\n import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n import { createSecureSandbox, compileAndRun } from '../../utils/sandbox';\\n import { MemoryStore } from '../../memory-store';\\n@@ -153,6 +154,37 @@ function createMemoryHelpers() {\\n   };\\n }\\n \\n+type RoutingTrigger = 'on_success' | 'on_fail' | 'on_finish';\\n+type RoutingAction = 'run' | 'goto' | 'retry';\\n+type RoutingSource = 'run' | 'run_js' | 'goto' | 'goto_js' | 'transitions' | 'retry';\\n+\\n+function formatScopeLabel(scope: Array<{ check: string; index: number }> | undefined): string {\\n+  if (!scope || scope.length === 0) return '';\\n+  return scope.map(item => `${item.check}:${item.index}`).join('|');\\n+}\\n+\\n+function recordRoutingEvent(args: {\\n+  checkId: string;\\n+  trigger: RoutingTrigger;\\n+  action: RoutingAction;\\n+  target?: string;\\n+  source?: RoutingSource;\\n+  scope?: Array<{ check: string; index: number }>;\\n+  gotoEvent?: string;\\n+}): void {\\n+  const attrs: Record<string, unknown> = {\\n+    check_id: args.checkId,\\n+    trigger: args.trigger,\\n+    action: args.action,\\n+  };\\n+  if (args.target) attrs.target = args.target;\\n+  if (args.source) attrs.source = args.source;\\n+  const scopeLabel = formatScopeLabel(args.scope);\\n+  if (scopeLabel) attrs.scope = scopeLabel;\\n+  if (args.gotoEvent) attrs.goto_event = args.gotoEvent;\\n+  addEvent('visor.routing', attrs);\\n+}\\n+\\n /**\\n  * Handle routing state - evaluate conditions and decide next actions\\n  */\\n@@ -271,6 +303,14 @@ async function processOnFinish(\\n             { check: checkId, index: itemIndex },\\n           ];\\n \\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_finish',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -284,6 +324,14 @@ async function processOnFinish(\\n         // Increment loop count\\n         state.routingLoopCount++;\\n \\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_finish',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope: [],\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -328,6 +376,14 @@ async function processOnFinish(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_finish',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -362,6 +418,15 @@ async function processOnFinish(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_finish',\\n+        action: 'goto',\\n+        target: finishTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: finishTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: finishTransTarget.to,\\n@@ -406,6 +471,14 @@ async function processOnFinish(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_finish',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onFinish.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+    });\\n     // Enqueue forward run event\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n@@ -640,6 +713,14 @@ async function processOnSuccess(\\n             { check: checkId, index: itemIndex },\\n           ];\\n \\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_success',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -652,6 +733,14 @@ async function processOnSuccess(\\n         // Increment loop count\\n         state.routingLoopCount++;\\n \\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_success',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope,\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -695,6 +784,14 @@ async function processOnSuccess(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_success',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -728,6 +825,15 @@ async function processOnSuccess(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_success',\\n+        action: 'goto',\\n+        target: successTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: successTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: successTransTarget.to,\\n@@ -773,6 +879,15 @@ async function processOnSuccess(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_success',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onSuccess.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+      gotoEvent: onSuccess.goto_event,\\n+    });\\n     // Enqueue forward run event with optional event override\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n@@ -864,6 +979,14 @@ async function processOnFail(\\n           const itemScope: Array<{ check: string; index: number }> = [\\n             { check: checkId, index: itemIndex },\\n           ];\\n+          recordRoutingEvent({\\n+            checkId,\\n+            trigger: 'on_fail',\\n+            action: 'run',\\n+            target: targetCheck,\\n+            source: 'run',\\n+            scope: itemScope,\\n+          });\\n           emitEvent({\\n             type: 'ForwardRunRequested',\\n             target: targetCheck,\\n@@ -874,6 +997,14 @@ async function processOnFail(\\n       } else {\\n         // No forEach context: preserve current scope (if any)\\n         state.routingLoopCount++;\\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_fail',\\n+          action: 'run',\\n+          target: targetCheck,\\n+          source: 'run',\\n+          scope,\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: targetCheck,\\n@@ -917,6 +1048,14 @@ async function processOnFail(\\n       // Increment loop count\\n       state.routingLoopCount++;\\n \\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_fail',\\n+        action: 'run',\\n+        target: targetCheck,\\n+        source: 'run_js',\\n+        scope,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: targetCheck,\\n@@ -958,6 +1097,13 @@ async function processOnFail(\\n \\n         // Increment loop count and schedule forward run for the same check\\n         state.routingLoopCount++;\\n+        recordRoutingEvent({\\n+          checkId,\\n+          trigger: 'on_fail',\\n+          action: 'retry',\\n+          source: 'retry',\\n+          scope: sc || [],\\n+        });\\n         emitEvent({\\n           type: 'ForwardRunRequested',\\n           target: checkId,\\n@@ -1013,6 +1159,15 @@ async function processOnFail(\\n         return;\\n       }\\n       state.routingLoopCount++;\\n+      recordRoutingEvent({\\n+        checkId,\\n+        trigger: 'on_fail',\\n+        action: 'goto',\\n+        target: failTransTarget.to,\\n+        source: 'transitions',\\n+        scope,\\n+        gotoEvent: failTransTarget.goto_event,\\n+      });\\n       emitEvent({\\n         type: 'ForwardRunRequested',\\n         target: failTransTarget.to,\\n@@ -1058,6 +1213,15 @@ async function processOnFail(\\n     // Increment loop count\\n     state.routingLoopCount++;\\n \\n+    recordRoutingEvent({\\n+      checkId,\\n+      trigger: 'on_fail',\\n+      action: 'goto',\\n+      target: gotoTarget,\\n+      source: onFail.goto_js ? 'goto_js' : 'goto',\\n+      scope,\\n+      gotoEvent: onFail.goto_event,\\n+    });\\n     // Enqueue forward run event with optional event override\\n     emitEvent({\\n       type: 'ForwardRunRequested',\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/states/wave-planning.ts\",\"additions\":10,\"deletions\":3,\"changes\":13,\"patch\":\"diff --git a/src/state-machine/states/wave-planning.ts b/src/state-machine/states/wave-planning.ts\\nindex 7551d55c..7b42f043 100644\\n--- a/src/state-machine/states/wave-planning.ts\\n+++ b/src/state-machine/states/wave-planning.ts\\n@@ -351,8 +351,11 @@ export async function handleWavePlanning(\\n     for (const id of checksToRun) {\\n       // Only include dependencies that are within the same subset (often none).\\n       const cfg = context.config.checks?.[id];\\n-      const deps = (cfg?.depends_on || []).filter((d: string) => checksToRun.includes(d));\\n-      subDeps[id] = deps as string[];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = cfg?.depends_on;\\n+      const depsArray = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n+      const deps = depsArray.filter((d: string) => checksToRun.includes(d));\\n+      subDeps[id] = deps;\\n     }\\n \\n     const subGraph = DependencyResolver.buildDependencyGraph(subDeps);\\n@@ -410,6 +413,10 @@ export async function handleWavePlanning(\\n     // Initialize current wave state\\n     (state as any).currentWaveCompletions = new Set<string>();\\n     (state as any).failedChecks = new Set<string>();\\n+    try {\\n+      (state as any).flags = (state as any).flags || {};\\n+      (state as any).flags.waveKind = 'initial';\\n+    } catch {}\\n   }\\n \\n   // Check if there are levels to execute\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine/workflow-projection.ts\",\"additions\":3,\"deletions\":2,\"changes\":5,\"patch\":\"diff --git a/src/state-machine/workflow-projection.ts b/src/state-machine/workflow-projection.ts\\nindex b2d5eb24..2bcfd858 100644\\n--- a/src/state-machine/workflow-projection.ts\\n+++ b/src/state-machine/workflow-projection.ts\\n@@ -53,7 +53,8 @@ export function projectWorkflowToGraph(\\n       triggers: step.on || workflow.on || [],\\n       group: step.group,\\n       providerType: step.type || 'ai',\\n-      dependencies: (step.depends_on || []).map(dep => dep),\\n+      // Normalize depends_on to array (supports string | string[])\\n+      dependencies: (Array.isArray(step.depends_on) ? step.depends_on : step.depends_on ? [step.depends_on] : []).map((dep: string) => dep),\\n     };\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/telemetry/state-capture.ts\",\"additions\":10,\"deletions\":1,\"changes\":11,\"patch\":\"diff --git a/src/telemetry/state-capture.ts b/src/telemetry/state-capture.ts\\nindex f9e77c32..047c3baf 100644\\n--- a/src/telemetry/state-capture.ts\\n+++ b/src/telemetry/state-capture.ts\\n@@ -179,18 +179,27 @@ export function captureProviderCall(\\n ): void {\\n   try {\\n     span.setAttribute('visor.provider.type', providerType);\\n+    const fullCapture =\\n+      process.env.VISOR_TELEMETRY_FULL_CAPTURE === 'true' ||\\n+      process.env.VISOR_TELEMETRY_FULL_CAPTURE === '1';\\n \\n     // Request summary\\n     if (request.model) span.setAttribute('visor.provider.request.model', String(request.model));\\n     if (request.prompt) {\\n       span.setAttribute('visor.provider.request.prompt.length', request.prompt.length);\\n       span.setAttribute('visor.provider.request.prompt.preview', request.prompt.substring(0, 500));\\n+      if (fullCapture) {\\n+        span.setAttribute('visor.provider.request.prompt', safeSerialize(request.prompt));\\n+      }\\n     }\\n \\n     // Response summary\\n     if (response.content) {\\n       span.setAttribute('visor.provider.response.length', response.content.length);\\n       span.setAttribute('visor.provider.response.preview', response.content.substring(0, 500));\\n+      if (fullCapture) {\\n+        span.setAttribute('visor.provider.response.content', safeSerialize(response.content));\\n+      }\\n     }\\n     if (response.tokens) {\\n       span.setAttribute('visor.provider.response.tokens', response.tokens);\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/flow-stage.ts\",\"additions\":37,\"deletions\":4,\"changes\":41,\"patch\":\"diff --git a/src/test-runner/core/flow-stage.ts b/src/test-runner/core/flow-stage.ts\\nindex a14256c2..a587f9f1 100644\\n--- a/src/test-runner/core/flow-stage.ts\\n+++ b/src/test-runner/core/flow-stage.ts\\n@@ -352,10 +352,30 @@ export class FlowStage {\\n       } catch {}\\n       // Prefer authoritative counts from res.statistics when available, then fall back to deltas/inferred\\n       const fromResStats: Record<string, number> = {};\\n+      const resSkipped: Record<string, boolean> = {};\\n       try {\\n         for (const s of (res.statistics?.checks || []) as any[]) {\\n           const n = (s && s.checkName) || '';\\n-          if (typeof n === 'string' && n) fromResStats[n] = (s.totalRuns || 0) as number;\\n+          if (typeof n === 'string' && n) {\\n+            fromResStats[n] = (s.totalRuns || 0) as number;\\n+            resSkipped[n] = !!(s as any).skipped || !!(s as any).skipReason;\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      // Steps that have explicit expectations in this stage. We will use\\n+      // this to avoid inflating run counts (via parent-alignment heuristics)\\n+      // for checks that are not even expected in this stage.\\n+      const expectedSteps = new Set<string>();\\n+      const expectedZero = new Set<string>();\\n+      try {\\n+        const expCalls = ((stage.expect || {}).calls || []) as Array<{ step?: string }>;\\n+        for (const c of expCalls) {\\n+          if (c && typeof c.step === 'string' && c.step) expectedSteps.add(c.step);\\n+          try {\\n+            if (c && typeof c.step === 'string' && c.step && (c as any).exactly === 0)\\n+              expectedZero.add(c.step);\\n+          } catch {}\\n         }\\n       } catch {}\\n \\n@@ -410,10 +430,15 @@ export class FlowStage {\\n           const base = stageOnly ? 0 : statBase[name] || 0;\\n           const d = Math.max(0, resTotal - base);\\n           runs = d > 0 ? d : 0;\\n+          // If the engine marked this check as skipped, force runs to 0\\n+          // so parent-alignment heuristics below do not inflate it.\\n+          if (resSkipped[name]) {\\n+            runs = 0;\\n+          }\\n         } else {\\n           runs = deltaMap[name] !== undefined ? deltaMap[name] : inferred;\\n         }\\n-        if (runs === 0 && presentInResults.has(name)) runs = 1;\\n+        if (runs === 0 && presentInResults.has(name) && !resSkipped[name]) runs = 1;\\n         if (!isForEachLike && histRuns > 0) runs = histRuns;\\n         // Only use per-item history counts for non-forEach checks. For forEach parents,\\n         // use aggregated totals from res.statistics to reflect number of parent executions.\\n@@ -442,7 +467,15 @@ export class FlowStage {\\n             parentMax = Math.max(parentMax, dP);\\n           }\\n           // Apply only for non-forEach checks with no observable history in this stage\\n-          if (!isForEachLike && histRuns === 0 && histPerItemRuns === 0 && parentMax > 0) {\\n+          if (\\n+            !isForEachLike &&\\n+            histRuns === 0 &&\\n+            histPerItemRuns === 0 &&\\n+            parentMax > 0 &&\\n+            !resSkipped[name] &&\\n+            (expectedSteps.size === 0 || expectedSteps.has(name)) &&\\n+            !expectedZero.has(name)\\n+          ) {\\n             runs = Math.max(runs, parentMax);\\n           }\\n         } catch {}\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/test-execution-wrapper.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/test-runner/core/test-execution-wrapper.ts b/src/test-runner/core/test-execution-wrapper.ts\\nindex 254d0970..d0333152 100644\\n--- a/src/test-runner/core/test-execution-wrapper.ts\\n+++ b/src/test-runner/core/test-execution-wrapper.ts\\n@@ -31,6 +31,8 @@ export class TestExecutionWrapper {\\n       const prev: any = (this.engine as any).executionContext || {};\\n       const merged = {\\n         ...prev,\\n+        // Inject workflow inputs for template access via {{ inputs.* }}\\n+        workflowInputs: cfg.workflow_inputs || prev.workflowInputs || {},\\n         mode: {\\n           ...(prev.mode || {}),\\n           test: true,\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/evaluators.ts\",\"additions\":67,\"deletions\":2,\"changes\":69,\"patch\":\"diff --git a/src/test-runner/evaluators.ts b/src/test-runner/evaluators.ts\\nindex 4843163a..75e9c174 100644\\n--- a/src/test-runner/evaluators.ts\\n+++ b/src/test-runner/evaluators.ts\\n@@ -252,6 +252,69 @@ export function evaluatePrompts(\\n   }\\n }\\n \\n+/**\\n+ * Evaluate workflow_output assertions against computed workflow outputs.\\n+ * Similar to evaluateOutputs but tests workflow-level outputs (defined in outputs: section)\\n+ * rather than step outputs.\\n+ */\\n+export function evaluateWorkflowOutputs(\\n+  errors: string[],\\n+  expect: ExpectBlock,\\n+  workflowOutputs: Record<string, unknown> | undefined\\n+): void {\\n+  const expectations = (expect as any).workflow_output;\\n+  if (!Array.isArray(expectations) || expectations.length === 0) return;\\n+  if (!workflowOutputs) {\\n+    errors.push('workflow_output assertions present but no workflow outputs computed');\\n+    return;\\n+  }\\n+\\n+  for (const o of expectations) {\\n+    const path = o.path as string;\\n+    if (!path) {\\n+      errors.push('workflow_output assertion missing path');\\n+      continue;\\n+    }\\n+    const v = deepGet(workflowOutputs, path);\\n+    if (o.equals !== undefined && !deepEqual(v, o.equals)) {\\n+      errors.push(\\n+        `Workflow output ${path} expected ${JSON.stringify(o.equals)} but got ${JSON.stringify(v)}`\\n+      );\\n+    }\\n+    if (o.equalsDeep !== undefined && !deepEqual(v, o.equalsDeep)) {\\n+      errors.push(`Workflow output ${path} deepEquals failed`);\\n+    }\\n+    if (o.matches && !parseRegex(o.matches).test(String(v))) {\\n+      errors.push(`Workflow output ${path} does not match ${o.matches}`);\\n+    }\\n+    if (o.contains) {\\n+      const contents = Array.isArray(o.contains) ? o.contains : [o.contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (!strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} expected to contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.not_contains) {\\n+      const contents = Array.isArray(o.not_contains) ? o.not_contains : [o.not_contains];\\n+      const strV = String(v);\\n+      for (const c of contents) {\\n+        if (strV.includes(String(c))) {\\n+          errors.push(`Workflow output ${path} should not contain \\\"${c}\\\"`);\\n+        }\\n+      }\\n+    }\\n+    if (o.contains_unordered) {\\n+      if (!Array.isArray(v)) {\\n+        errors.push(`Workflow output ${path} not an array for contains_unordered`);\\n+      } else if (!containsUnordered(v as unknown[], o.contains_unordered)) {\\n+        errors.push(`Workflow output ${path} missing elements (unordered)`);\\n+      }\\n+    }\\n+  }\\n+}\\n+\\n export function evaluateOutputs(\\n   errors: string[],\\n   expect: ExpectBlock,\\n@@ -332,7 +395,8 @@ export function evaluateCase(\\n   strict: boolean,\\n   promptsByStep: Record<string, string[]>,\\n   _results: GroupedResults,\\n-  outputHistory: Record<string, unknown[]>\\n+  outputHistory: Record<string, unknown[]>,\\n+  workflowOutputs?: Record<string, unknown>\\n ): string[] {\\n   const errors: string[] = [];\\n   const executed = buildExecutedMap(stats);\\n@@ -350,5 +414,6 @@ export function evaluateCase(\\n   evaluateNoCalls(errors, expect, executed, recorder, slackRecorder);\\n   evaluatePrompts(errors, expect, promptsByStep);\\n   evaluateOutputs(errors, expect, outputHistory);\\n+  evaluateWorkflowOutputs(errors, expect, workflowOutputs);\\n   return errors;\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/index.ts\",\"additions\":112,\"deletions\":7,\"changes\":119,\"patch\":\"diff --git a/src/test-runner/index.ts b/src/test-runner/index.ts\\nindex 29916a16..4dcc3331 100644\\n--- a/src/test-runner/index.ts\\n+++ b/src/test-runner/index.ts\\n@@ -159,6 +159,7 @@ export async function runSuites(\\n   options: {\\n     only?: string;\\n     bail?: boolean;\\n+    noMocks?: boolean;\\n     maxParallelSuites?: number;\\n     maxParallel?: number;\\n     promptMaxChars?: number;\\n@@ -225,6 +226,7 @@ export async function runSuites(\\n       const r = await runner.runCases(fp, suite as TestSuite, {\\n         only: options.only,\\n         bail: options.bail,\\n+        noMocks: options.noMocks,\\n         maxParallel: options.maxParallel,\\n         promptMaxChars: options.promptMaxChars,\\n       });\\n@@ -285,7 +287,8 @@ export class VisorTestRunner {\\n     defaultPromptCap?: number,\\n     ghRec?: { error_code?: number; timeout_ms?: number },\\n     defaultIncludeTags?: string[] | undefined,\\n-    defaultExcludeTags?: string[] | undefined\\n+    defaultExcludeTags?: string[] | undefined,\\n+    noMocks?: boolean\\n   ): {\\n     name: string;\\n     strict: boolean;\\n@@ -399,9 +402,11 @@ export class VisorTestRunner {\\n               : info.prompt;\\n           prompts[k].push(p);\\n         },\\n-        mockForStep: (step: string) => mockMgr.get(step),\\n+        // In noMocks mode, always return undefined to let real providers execute\\n+        mockForStep: (step: string) => noMocks ? undefined : mockMgr.get(step),\\n         // Ensure human-input never blocks tests: prefer case mock, then default value\\n         onHumanInput: async (req: { checkId: string; default?: string }) => {\\n+          if (noMocks) return (req.default ?? '').toString();\\n           const m = mockMgr.get(req.checkId);\\n           if (m !== undefined && m !== null) return String(m);\\n           return (req.default ?? '').toString();\\n@@ -481,6 +486,60 @@ export class VisorTestRunner {\\n     if (!checks || checks.length === 0) return;\\n     console.log(`  checks: ${checks.join(', ')}`);\\n   }\\n+  /**\\n+   * Compute workflow outputs from output definitions and step results.\\n+   * Evaluates value_js and value (Liquid) expressions to produce output values.\\n+   */\\n+  private computeWorkflowOutputs(\\n+    outputDefs: Array<{ name: string; value?: string; value_js?: string }>,\\n+    outputHistory: Record<string, unknown[]>,\\n+    results: any[]\\n+  ): Record<string, unknown> {\\n+    const computed: Record<string, unknown> = {};\\n+    // Build a steps map from outputHistory (last output for each step)\\n+    const steps: Record<string, unknown> = {};\\n+    for (const [stepId, hist] of Object.entries(outputHistory)) {\\n+      if (Array.isArray(hist) && hist.length > 0) {\\n+        steps[stepId] = hist[hist.length - 1];\\n+      }\\n+    }\\n+    // Build outputs map from history (for {{ outputs[\\\"step\\\"] }} syntax)\\n+    const outputs: Record<string, unknown> = { ...steps };\\n+\\n+    for (const outputDef of outputDefs) {\\n+      if (!outputDef.name) continue;\\n+      try {\\n+        if (outputDef.value_js) {\\n+          // Evaluate JavaScript expression\\n+          const fn = new Function('steps', 'outputs', 'results', `\\n+            try {\\n+              ${outputDef.value_js}\\n+            } catch (e) {\\n+              return undefined;\\n+            }\\n+          `);\\n+          computed[outputDef.name] = fn(steps, outputs, results);\\n+        } else if (outputDef.value) {\\n+          // Evaluate Liquid template\\n+          const { Liquid } = require('liquidjs');\\n+          const engine = new Liquid();\\n+          const rendered = engine.parseAndRenderSync(outputDef.value, { steps, outputs, results });\\n+          // Try to parse as JSON, otherwise keep as string\\n+          try {\\n+            computed[outputDef.name] = JSON.parse(rendered);\\n+          } catch {\\n+            computed[outputDef.name] = rendered.trim();\\n+          }\\n+        }\\n+      } catch (e) {\\n+        // Skip outputs that fail to compute\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.log(`  ‚ö†Ô∏è Failed to compute output '${outputDef.name}': ${e}`);\\n+        }\\n+      }\\n+    }\\n+    return computed;\\n+  }\\n   /**\\n    * Locate a tests file: explicit path > ./.visor.tests.yaml > defaults/visor.tests.yaml\\n    */\\n@@ -606,7 +665,7 @@ export class VisorTestRunner {\\n   public async runCases(\\n     testsPath: string,\\n     suite: TestSuite,\\n-    options: { only?: string; bail?: boolean; maxParallel?: number; promptMaxChars?: number }\\n+    options: { only?: string; bail?: boolean; noMocks?: boolean; maxParallel?: number; promptMaxChars?: number }\\n   ): Promise<{\\n     failures: number;\\n     results: Array<{\\n@@ -748,9 +807,14 @@ export class VisorTestRunner {\\n     const __keepAlive = setInterval(() => {}, 1000);\\n     // Header: show suite path for clarity\\n     let __suiteRel = testsPath;\\n+    const noMocksMode = options.noMocks || false;\\n     try {\\n       __suiteRel = path.relative(this.cwd, testsPath) || testsPath;\\n       console.log(`Suite: ${__suiteRel}`);\\n+      if (noMocksMode) {\\n+        console.log(this.color('üî¥ NO-MOCKS MODE: Running with real providers (no mock injection)', '33'));\\n+        console.log(this.gray('   Step outputs will be captured and printed as suggested mocks\\\\n'));\\n+      }\\n     } catch {}\\n     const runOne = async (_case: any): Promise<{ name: string; failed: number }> => {\\n       // Case header for clarity\\n@@ -805,6 +869,11 @@ export class VisorTestRunner {\\n           cfgLocal.checks[name] = chk;\\n         }\\n       }\\n+      // Workflow testing: inject workflow_input into config for template access\\n+      const workflowInput = (_case as any).workflow_input;\\n+      if (workflowInput && typeof workflowInput === 'object') {\\n+        (cfgLocal as any).workflow_inputs = workflowInput;\\n+      }\\n       const setup = this.setupTestCase(\\n         _case,\\n         cfgLocal,\\n@@ -812,7 +881,8 @@ export class VisorTestRunner {\\n         defaultPromptCap,\\n         ghRec,\\n         defaultIncludeTags,\\n-        defaultExcludeTags\\n+        defaultExcludeTags,\\n+        noMocksMode\\n       );\\n       try {\\n         this.printSelectedChecks(setup.checksToRun);\\n@@ -826,6 +896,24 @@ export class VisorTestRunner {\\n         } catch {}\\n         const exec = await this.executeTestCase(setup, cfgLocal);\\n         const res = exec.res;\\n+\\n+        // In no-mocks mode, print captured outputs as suggested mocks\\n+        if (noMocksMode && Object.keys(exec.outHistory).length > 0) {\\n+          console.log(this.color('\\\\nüìã Suggested mocks (copy to your test case):', '36'));\\n+          console.log(this.gray('mocks:'));\\n+          for (const [stepName, outputs] of Object.entries(exec.outHistory)) {\\n+            if (!Array.isArray(outputs) || outputs.length === 0) continue;\\n+            // Use the last output for the step\\n+            const lastOutput = outputs[outputs.length - 1];\\n+            // Format as YAML with proper indentation\\n+            const yamlOutput = yaml.dump({ [stepName]: lastOutput }, { indent: 2, lineWidth: 120, noRefs: true });\\n+            // Indent the YAML output for proper nesting under 'mocks:'\\n+            const indented = yamlOutput.split('\\\\n').map(line => '  ' + line).join('\\\\n');\\n+            console.log(indented);\\n+          }\\n+          console.log('');\\n+        }\\n+\\n         if (process.env.VISOR_DEBUG === 'true') {\\n           try {\\n             const names = (res.statistics.checks || []).map(\\n@@ -836,6 +924,21 @@ export class VisorTestRunner {\\n         }\\n         // avoid printing raw history keys each case\\n         // (fallback for on_finish static targets handled inside executeTestCase)\\n+        // Workflow testing: compute workflow outputs if workflow has outputs defined\\n+        let workflowOutputs: Record<string, unknown> | undefined;\\n+        if (Array.isArray((cfgLocal as any).outputs)) {\\n+          try {\\n+            workflowOutputs = this.computeWorkflowOutputs(\\n+              (cfgLocal as any).outputs,\\n+              exec.outHistory,\\n+              res.results\\n+            );\\n+          } catch (e) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ‚ö†Ô∏è Error computing workflow outputs: ${e}`);\\n+            }\\n+          }\\n+        }\\n         const caseFailures = require('./evaluators').evaluateCase(\\n           _case.name,\\n           res.statistics,\\n@@ -845,7 +948,8 @@ export class VisorTestRunner {\\n           setup.strict,\\n           setup.prompts,\\n           res.results,\\n-          exec.outHistory\\n+          exec.outHistory,\\n+          workflowOutputs\\n         );\\n         // Warn about unmocked AI/command steps that executed\\n         try {\\n@@ -1280,7 +1384,8 @@ export class VisorTestRunner {\\n   ): void {\\n     const executed: Record<string, number> = {};\\n     for (const s of stats.checks) {\\n-      if (!s.skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n+      const skipped = (s as any).skipped === true || !!(s as any).skipReason;\\n+      if (!skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n     }\\n     const expCalls = (expect.calls || []).filter(c => c.step);\\n     const expectedSteps = new Map<\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/validator.ts\",\"additions\":48,\"deletions\":2,\"changes\":50,\"patch\":\"diff --git a/src/test-runner/validator.ts b/src/test-runner/validator.ts\\nindex dbb890c5..5ffff5ee 100644\\n--- a/src/test-runner/validator.ts\\n+++ b/src/test-runner/validator.ts\\n@@ -25,6 +25,16 @@ const schema: any = {\\n     hooks: { type: 'object' },\\n     slack: { type: 'object' },\\n     frontends: { type: 'array' },\\n+    workspace: { type: 'object' },\\n+    // Workflow definition fields (for workflow files with co-located tests)\\n+    id: { type: 'string' },\\n+    name: { type: 'string' },\\n+    description: { type: 'string' },\\n+    inputs: { type: 'array' },\\n+    outputs: { type: 'array' },\\n+    ai_mcp_servers: { type: 'object' },\\n+    ai_provider: { type: 'string' },\\n+    ai_model: { type: 'string' },\\n     tests: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -136,6 +146,8 @@ const schema: any = {\\n             oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n           },\\n         },\\n+        // Workflow testing: input values to pass to the workflow\\n+        workflow_input: { type: 'object' },\\n         expect: { $ref: '#/$defs/expectBlock' },\\n         // Flow cases\\n         flow: {\\n@@ -270,6 +282,35 @@ const schema: any = {\\n       },\\n       required: ['step', 'path'],\\n     },\\n+    // Workflow output expectation: like outputsExpectation but without step (tests workflow-level outputs)\\n+    workflowOutputExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        path: { type: 'string' },\\n+        equals: {},\\n+        equalsDeep: {},\\n+        matches: { type: 'string' },\\n+        contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        not_contains: {\\n+          oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+        },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            path: { type: 'string' },\\n+            equals: {},\\n+            matches: { type: 'string' },\\n+          },\\n+          required: ['path'],\\n+        },\\n+        contains_unordered: { type: 'array' },\\n+      },\\n+      required: ['path'],\\n+    },\\n     expectBlock: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -278,6 +319,8 @@ const schema: any = {\\n         calls: { type: 'array', items: { $ref: '#/$defs/callsExpectation' } },\\n         prompts: { type: 'array', items: { $ref: '#/$defs/promptsExpectation' } },\\n         outputs: { type: 'array', items: { $ref: '#/$defs/outputsExpectation' } },\\n+        // Workflow testing: assert on workflow-level outputs (defined in workflow's outputs: section)\\n+        workflow_output: { type: 'array', items: { $ref: '#/$defs/workflowOutputExpectation' } },\\n         no_calls: {\\n           type: 'array',\\n           items: {\\n@@ -343,6 +386,7 @@ const knownKeys = new Set([\\n   'version',\\n   'extends',\\n   'tests',\\n+  'workspace',\\n   // tests\\n   'tests.defaults',\\n   'tests.fixtures',\\n@@ -363,6 +407,7 @@ const knownKeys = new Set([\\n   'env',\\n   'routing',\\n   'mocks',\\n+  'workflow_input',\\n   'expect',\\n   'flow',\\n   // expect\\n@@ -370,6 +415,7 @@ const knownKeys = new Set([\\n   'expect.calls',\\n   'expect.prompts',\\n   'expect.outputs',\\n+  'expect.workflow_output',\\n   'expect.no_calls',\\n   'expect.fail',\\n   'expect.strict_violation',\\n@@ -424,7 +470,7 @@ function formatError(e: ErrorObject): string {\\n       if (hint) msg += ` (${hint})`;\\n       // Small curated allow-list for frequent nodes to reduce guesswork\\n       if (path.endsWith('expect')) {\\n-        msg += ` (allowed: use, calls, prompts, outputs, no_calls, fail, strict_violation)`;\\n+        msg += ` (allowed: use, calls, prompts, outputs, workflow_output, no_calls, fail, strict_violation)`;\\n       } else if (path.endsWith('env')) {\\n         msg += ` (values must be strings)`;\\n       } else if (path.endsWith('tests')) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/cli.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/cli.ts b/src/types/cli.ts\\nindex a7d9ca30..f06f93d5 100644\\n--- a/src/types/cli.ts\\n+++ b/src/types/cli.ts\\n@@ -61,6 +61,8 @@ export interface CliOptions {\\n   githubV2?: boolean;\\n   /** Enable Slack Socket Mode runner */\\n   slack?: boolean;\\n+  /** Keep workspace folders after execution (for debugging) */\\n+  keepWorkspace?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/config.ts\",\"additions\":114,\"deletions\":5,\"changes\":119,\"patch\":\"diff --git a/src/types/config.ts b/src/types/config.ts\\nindex ed239959..dd363de1 100644\\n--- a/src/types/config.ts\\n+++ b/src/types/config.ts\\n@@ -122,6 +122,9 @@ export interface FailureConditionContext {\\n   /** Environment variables */\\n   env?: Record<string, string>;\\n \\n+  /** Workflow inputs - values passed to the workflow */\\n+  inputs?: Record<string, unknown>;\\n+\\n   /** Memory accessor for accessing memory store in expressions */\\n   memory?: {\\n     get: (key: string, namespace?: string) => unknown;\\n@@ -356,6 +359,8 @@ export interface AIProviderConfig {\\n   allowBash?: boolean;\\n   /** Advanced bash command execution configuration */\\n   bashConfig?: BashConfig;\\n+  /** Completion prompt for post-completion validation/review (runs after attempt_completion) */\\n+  completion_prompt?: string;\\n }\\n \\n /**\\n@@ -461,8 +466,8 @@ export interface CheckConfig {\\n   env?: EnvConfig;\\n   /** Timeout in seconds for command execution (default: 60) */\\n   timeout?: number;\\n-  /** Check IDs that this check depends on (optional) */\\n-  depends_on?: string[];\\n+  /** Check IDs that this check depends on (optional). Accepts single string or array. */\\n+  depends_on?: string | string[];\\n   /** Group name for comment separation (e.g., \\\"code-review\\\", \\\"pr-overview\\\") - optional */\\n   group?: string;\\n   /** Schema type for template rendering (e.g., \\\"code-review\\\", \\\"markdown\\\") or inline JSON schema object - optional */\\n@@ -621,6 +626,46 @@ export interface CheckConfig {\\n   overrides?: Record<string, Partial<CheckConfig>>;\\n   /** Map workflow outputs to check outputs */\\n   output_mapping?: Record<string, string>;\\n+\\n+  // Workflow aliases for backward compatibility\\n+  /** Alias for args - workflow inputs (backward compatibility) */\\n+  workflow_inputs?: Record<string, unknown>;\\n+  /** Config file path - alternative to workflow ID (loads a Visor config file as workflow) */\\n+  config?: string;\\n+  /** Alias for overrides - workflow step overrides (backward compatibility) */\\n+  workflow_overrides?: Record<string, Partial<CheckConfig>>;\\n+\\n+  /**\\n+   * Git-checkout provider specific options (optional, only used when type === 'git-checkout').\\n+   */\\n+  /** Git reference to checkout (branch, tag, commit SHA) - supports templates */\\n+  ref?: string;\\n+  /** Repository URL or owner/repo format (defaults to current repository) */\\n+  repository?: string;\\n+  /** GitHub token for private repositories (defaults to GITHUB_TOKEN env) */\\n+  token?: string;\\n+  /** Number of commits to fetch (0 for full history, default: 1) */\\n+  fetch_depth?: number;\\n+  /** Whether to fetch tags (default: false) */\\n+  fetch_tags?: boolean;\\n+  /** Checkout submodules: false, true, or 'recursive' */\\n+  submodules?: boolean | 'recursive';\\n+  /** Working directory for the checkout (defaults to temp directory) */\\n+  working_directory?: string;\\n+  /** Use git worktree for efficient parallel checkouts (default: true) */\\n+  use_worktree?: boolean;\\n+  /** Clean the working directory before checkout (default: true) */\\n+  clean?: boolean;\\n+  /** Sparse checkout paths - only checkout specific directories/files */\\n+  sparse_checkout?: string[];\\n+  /** Enable Git LFS (Large File Storage) */\\n+  lfs?: boolean;\\n+  /** Timeout in ms for cloning the bare repository (default: 300000 = 5 min) */\\n+  clone_timeout_ms?: number;\\n+  /** Clean up worktree on failure (default: true) */\\n+  cleanup_on_failure?: boolean;\\n+  /** Keep worktree after workflow completion (default: false) */\\n+  persist_worktree?: boolean;\\n }\\n \\n /**\\n@@ -1037,6 +1082,36 @@ export interface CustomToolDefinition {\\n   outputSchema?: Record<string, unknown>;\\n }\\n \\n+/**\\n+ * Workflow input definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowInput {\\n+  /** Input parameter name */\\n+  name: string;\\n+  /** JSON Schema for the input */\\n+  schema?: Record<string, unknown>;\\n+  /** Whether this input is required */\\n+  required?: boolean;\\n+  /** Default value if not provided */\\n+  default?: unknown;\\n+  /** Human-readable description */\\n+  description?: string;\\n+}\\n+\\n+/**\\n+ * Workflow output definition for standalone reusable workflows\\n+ */\\n+export interface WorkflowOutput {\\n+  /** Output name */\\n+  name: string;\\n+  /** Human-readable description */\\n+  description?: string;\\n+  /** Value using Liquid template syntax (references step outputs) */\\n+  value?: string;\\n+  /** Value using JavaScript expression (alternative to value) */\\n+  value_js?: string;\\n+}\\n+\\n /**\\n  * Main Visor configuration\\n  */\\n@@ -1051,12 +1126,16 @@ export interface VisorConfig {\\n   tools?: Record<string, CustomToolDefinition>;\\n   /** Import workflow definitions from external files or URLs */\\n   imports?: string[];\\n+  /** Workflow inputs (for standalone reusable workflows) */\\n+  inputs?: WorkflowInput[];\\n+  /** Workflow outputs (for standalone reusable workflows) */\\n+  outputs?: WorkflowOutput[];\\n   /** Step configurations (recommended) */\\n   steps?: Record<string, CheckConfig>;\\n   /** Check configurations (legacy, use 'steps' instead) - always populated after normalization */\\n   checks?: Record<string, CheckConfig>;\\n-  /** Output configuration */\\n-  output: OutputConfig;\\n+  /** Output configuration (optional - defaults provided) */\\n+  output?: OutputConfig;\\n   /** HTTP server configuration for receiving webhooks */\\n   http_server?: HttpServerConfig;\\n   /** Memory storage configuration */\\n@@ -1092,6 +1171,36 @@ export interface VisorConfig {\\n     /** Frontend-specific configuration */\\n     config?: unknown;\\n   }>;\\n+  /** Workspace isolation configuration for sandboxed execution */\\n+  workspace?: WorkspaceConfig;\\n+  /** Slack configuration */\\n+  slack?: SlackConfig;\\n+}\\n+\\n+/**\\n+ * Workspace isolation configuration\\n+ */\\n+export interface WorkspaceConfig {\\n+  /** Enable workspace isolation (default: true when config present) */\\n+  enabled?: boolean;\\n+  /** Base path for workspaces (default: /tmp/visor-workspaces) */\\n+  base_path?: string;\\n+  /** Clean up workspace on exit (default: true) */\\n+  cleanup_on_exit?: boolean;\\n+}\\n+\\n+/**\\n+ * Slack configuration\\n+ */\\n+export interface SlackConfig {\\n+  /** Slack API version */\\n+  version?: string;\\n+  /** Mention handling: 'all', 'direct', etc. */\\n+  mentions?: string;\\n+  /** Thread handling: 'required', 'optional', etc. */\\n+  threads?: string;\\n+  /** Show raw output in Slack responses */\\n+  show_raw_output?: boolean;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/git-checkout.ts\",\"additions\":3,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/src/types/git-checkout.ts b/src/types/git-checkout.ts\\nindex eb1c5ebc..4d27855d 100644\\n--- a/src/types/git-checkout.ts\\n+++ b/src/types/git-checkout.ts\\n@@ -16,6 +16,8 @@ export interface GitCheckoutConfig {\\n   fetch_depth?: number;\\n   fetch_tags?: boolean;\\n   submodules?: boolean | 'recursive';\\n+  /** Timeout (ms) for cloning the bare repository; defaults to 300000 (5 minutes) */\\n+  clone_timeout_ms?: number;\\n \\n   // Worktree configuration\\n   working_directory?: string;\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/sandbox.ts\",\"additions\":46,\"deletions\":1,\"changes\":47,\"patch\":\"diff --git a/src/utils/sandbox.ts b/src/utils/sandbox.ts\\nindex 9914a3a5..12e27c30 100644\\n--- a/src/utils/sandbox.ts\\n+++ b/src/utils/sandbox.ts\\n@@ -14,6 +14,51 @@ export interface CompileOptions {\\n   wrapFunction?: boolean;\\n }\\n \\n+export interface JsSyntaxValidationResult {\\n+  valid: boolean;\\n+  error?: string;\\n+}\\n+\\n+/**\\n+ * Validate JavaScript syntax without executing it.\\n+ * Uses the sandbox's compile method to check for syntax errors.\\n+ * Returns validation result with error message if invalid.\\n+ */\\n+export function validateJsSyntax(code: string): JsSyntaxValidationResult {\\n+  if (!code || typeof code !== 'string') {\\n+    return { valid: false, error: 'Code must be a non-empty string' };\\n+  }\\n+\\n+  const trimmed = code.trim();\\n+  if (trimmed.length === 0) {\\n+    return { valid: false, error: 'Code cannot be empty' };\\n+  }\\n+\\n+  // Create a minimal sandbox instance for syntax checking\\n+  const sandbox = createSecureSandbox();\\n+\\n+  // Wrap code similar to compileAndRun to catch the same syntax issues\\n+  const looksLikeBlock = /\\\\breturn\\\\b/.test(trimmed) || /;/.test(trimmed) || /\\\\n/.test(trimmed);\\n+  const looksLikeIife = /\\\\)\\\\s*\\\\(\\\\s*\\\\)\\\\s*;?$/.test(trimmed);\\n+  const body = looksLikeBlock\\n+    ? looksLikeIife\\n+      ? `return (\\\\n${trimmed}\\\\n);\\\\n`\\n+      : `return (() => {\\\\n${trimmed}\\\\n})();\\\\n`\\n+    : `return (\\\\n${trimmed}\\\\n);\\\\n`;\\n+\\n+  // Add log injection header (same as compileAndRun)\\n+  const header = `const __lp = \\\"[syntax-check]\\\"; const log = (...a) => { try { console.log(__lp, ...a); } catch {} };\\\\n`;\\n+  const fullCode = `${header}${body}`;\\n+\\n+  try {\\n+    sandbox.compile(fullCode);\\n+    return { valid: true };\\n+  } catch (e) {\\n+    const msg = e instanceof Error ? e.message : String(e);\\n+    return { valid: false, error: msg };\\n+  }\\n+}\\n+\\n /**\\n  * Create a hardened Sandbox with a consistent set of globals and prototype\\n  * whitelists. This is a superset of the sets previously used by individual\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/tracer-init.ts\",\"additions\":19,\"deletions\":1,\"changes\":20,\"patch\":\"diff --git a/src/utils/tracer-init.ts b/src/utils/tracer-init.ts\\nindex b42995d9..4edf1ffc 100644\\n--- a/src/utils/tracer-init.ts\\n+++ b/src/utils/tracer-init.ts\\n@@ -76,6 +76,24 @@ export async function initializeTracer(\\n \\n       const tracer = new SimpleAppTracer(telemetry, sessionId);\\n \\n+      // WORKAROUND: Add missing recordEvent method for completionPrompt feature (probe #321)\\n+      // SimpleAppTracer doesn't have recordEvent but completionPrompt requires it\\n+      if (typeof (tracer as any).recordEvent !== 'function') {\\n+        (tracer as any).recordEvent = (\\n+          name: string,\\n+          attributes?: Record<string, unknown>\\n+        ) => {\\n+          // Log completion events to telemetry for debugging\\n+          try {\\n+            if ((telemetry as any).record) {\\n+              (telemetry as any).record({ event: name, ...attributes });\\n+            }\\n+          } catch {\\n+            // Best-effort only\\n+          }\\n+        };\\n+      }\\n+\\n       console.error(`üìä Simple tracing enabled, will save to: ${traceFilePath}`);\\n \\n       // If in GitHub Actions, log the path for artifact upload\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/worktree-manager.ts\",\"additions\":142,\"deletions\":27,\"changes\":169,\"patch\":\"diff --git a/src/utils/worktree-manager.ts b/src/utils/worktree-manager.ts\\nindex f3cebf3d..2de7a9f4 100644\\n--- a/src/utils/worktree-manager.ts\\n+++ b/src/utils/worktree-manager.ts\\n@@ -6,6 +6,7 @@\\n  */\\n \\n import * as fs from 'fs';\\n+import * as fsp from 'fs/promises';\\n import * as path from 'path';\\n import * as crypto from 'crypto';\\n import { commandExecutor } from './command-executor';\\n@@ -119,7 +120,8 @@ export class WorktreeManager {\\n     repository: string,\\n     repoUrl: string,\\n     token?: string,\\n-    fetchDepth?: number\\n+    fetchDepth?: number,\\n+    cloneTimeoutMs?: number\\n   ): Promise<string> {\\n     const reposDir = this.getReposDir();\\n     const repoName = repository.replace(/\\\\//g, '-');\\n@@ -129,9 +131,21 @@ export class WorktreeManager {\\n     if (fs.existsSync(bareRepoPath)) {\\n       logger.debug(`Bare repository already exists: ${bareRepoPath}`);\\n \\n-      // Update remote refs\\n-      await this.updateBareRepo(bareRepoPath);\\n-      return bareRepoPath;\\n+      // Verify the bare repo has the correct remote URL to prevent using corrupted repos\\n+      const verifyResult = await this.verifyBareRepoRemote(bareRepoPath, repoUrl);\\n+      if (verifyResult === 'timeout') {\\n+        // Timeout during verification - use stale cache to avoid hanging on re-clone\\n+        logger.info(`Using stale bare repository (verification timed out): ${bareRepoPath}`);\\n+        return bareRepoPath;\\n+      } else if (verifyResult === false) {\\n+        logger.warn(`Bare repository at ${bareRepoPath} has incorrect remote, removing and re-cloning`);\\n+        await fsp.rm(bareRepoPath, { recursive: true, force: true });\\n+        // Fall through to clone below\\n+      } else {\\n+        // Update remote refs\\n+        await this.updateBareRepo(bareRepoPath);\\n+        return bareRepoPath;\\n+      }\\n     }\\n \\n     // Clone as bare repository\\n@@ -153,7 +167,9 @@ export class WorktreeManager {\\n     }\\n     cloneCmd += ` ${this.escapeShellArg(cloneUrl)} ${this.escapeShellArg(bareRepoPath)}`;\\n \\n-    const result = await this.executeGitCommand(cloneCmd, { timeout: 300000 }); // 5 minute timeout\\n+    const result = await this.executeGitCommand(cloneCmd, {\\n+      timeout: cloneTimeoutMs || 300000, // default 5 minutes\\n+    });\\n \\n     if (result.exitCode !== 0) {\\n       // Redact tokens from error messages\\n@@ -171,19 +187,98 @@ export class WorktreeManager {\\n   private async updateBareRepo(bareRepoPath: string): Promise<void> {\\n     logger.debug(`Updating bare repository: ${bareRepoPath}`);\\n \\n-    const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n-    const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+    try {\\n+      const updateCmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote update --prune`;\\n+      const result = await this.executeGitCommand(updateCmd, { timeout: 60000 }); // 1 minute timeout\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n+        // Don't throw - we can continue with stale refs\\n+      } else {\\n+        logger.debug(`Successfully updated bare repository`);\\n+      }\\n+    } catch (error) {\\n+      // Handle timeout or other errors gracefully - we can continue with stale refs\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      logger.warn(`Failed to update bare repository (will use stale refs): ${errorMessage}`);\\n+    }\\n+  }\\n \\n-    if (result.exitCode !== 0) {\\n-      logger.warn(`Failed to update bare repository: ${result.stderr}`);\\n-      // Don't throw - we can continue with stale refs\\n-    } else {\\n-      logger.debug(`Successfully updated bare repository`);\\n+  /**\\n+   * Verify that a bare repository has the correct remote URL.\\n+   * This prevents reusing corrupted repos that were cloned from a different repository.\\n+   * Returns: true (valid), false (invalid - should re-clone), or 'timeout' (use stale cache)\\n+   */\\n+  private async verifyBareRepoRemote(\\n+    bareRepoPath: string,\\n+    expectedUrl: string\\n+  ): Promise<boolean | 'timeout'> {\\n+    try {\\n+      const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} remote get-url origin`;\\n+      const result = await this.executeGitCommand(cmd, { timeout: 10000 });\\n+\\n+      if (result.exitCode !== 0) {\\n+        logger.warn(`Failed to get remote URL for ${bareRepoPath}: ${result.stderr}`);\\n+        return false;\\n+      }\\n+\\n+      const actualUrl = result.stdout.trim();\\n+\\n+      // Normalize URLs for comparison:\\n+      // - remove credentials (tokens/username)\\n+      // - remove .git suffix if present\\n+      // - trim trailing slash\\n+      // - lowercase for case‚Äëinsensitive match\\n+      const normalizeUrl = (url: string): string => {\\n+        // Convert common SSH form to https for comparison\\n+        if (url.startsWith('git@github.com:')) {\\n+          url = url.replace('git@github.com:', 'https://github.com/');\\n+        }\\n+        return url\\n+          // strip userinfo part to avoid mismatches when the cached bare\\n+          // repo was cloned with an access token but the expected URL is\\n+          // tokenless (or vice‚Äëversa)\\n+          .replace(/:\\\\/\\\\/[^@]+@/, '://')\\n+          .replace(/\\\\.git$/, '')\\n+          .replace(/\\\\/$/, '')\\n+          .toLowerCase();\\n+      };\\n+\\n+      const normalizedExpected = normalizeUrl(expectedUrl);\\n+      const normalizedActual = normalizeUrl(actualUrl);\\n+\\n+      if (normalizedExpected !== normalizedActual) {\\n+        logger.warn(\\n+          `Bare repo remote mismatch: expected ${expectedUrl}, got ${actualUrl}`\\n+        );\\n+        return false;\\n+      }\\n+\\n+      logger.debug(`Bare repo remote verified: ${actualUrl}`);\\n+      return true;\\n+    } catch (error) {\\n+      const errorMessage = error instanceof Error ? error.message : String(error);\\n+      // If it's a timeout, return 'timeout' so caller can use stale cache instead of re-cloning\\n+      if (errorMessage.includes('timed out')) {\\n+        logger.warn(`Timeout verifying bare repo remote (will use stale cache): ${errorMessage}`);\\n+        return 'timeout';\\n+      }\\n+      logger.warn(`Error verifying bare repo remote: ${error}`);\\n+      return false;\\n     }\\n   }\\n \\n   /**\\n-   * Create a new worktree\\n+   * Create a new worktree for the given repository/ref.\\n+   *\\n+   * Important: we always create worktrees in a detached HEAD state pinned\\n+   * to a specific commit SHA rather than a named branch like \\\"main\\\". Git\\n+   * only allows a branch to be checked out in a single worktree at a time;\\n+   * using the raw commit (plus --detach) lets multiple workflows safely\\n+   * create independent worktrees for the same branch without hitting\\n+   * errors like:\\n+   *\\n+   *   fatal: 'main' is already used by worktree at '.../TykTechnologies-tyk-docs-main-XXXX'\\n    */\\n   async createWorktree(\\n     repository: string,\\n@@ -195,6 +290,7 @@ export class WorktreeManager {\\n       clean?: boolean;\\n       workflowId?: string;\\n       fetchDepth?: number;\\n+      cloneTimeoutMs?: number;\\n     } = {}\\n   ): Promise<WorktreeInfo> {\\n     // Validate ref to prevent command injection\\n@@ -205,7 +301,8 @@ export class WorktreeManager {\\n       repository,\\n       repoUrl,\\n       options.token,\\n-      options.fetchDepth\\n+      options.fetchDepth,\\n+      options.cloneTimeoutMs\\n     );\\n \\n     // Generate worktree ID and path\\n@@ -241,21 +338,24 @@ export class WorktreeManager {\\n       }\\n     }\\n \\n-    // Fetch the ref if needed\\n+    // Fetch the ref if needed, then resolve it to a concrete commit SHA.\\n+    // We use the commit (detached HEAD) instead of the branch name so we\\n+    // can have multiple worktrees for the same branch without git refusing\\n+    // with \\\"branch X is already checked out\\\".\\n     await this.fetchRef(bareRepoPath, ref);\\n+    const commit = await this.getCommitShaForRef(bareRepoPath, ref);\\n \\n-    // Create worktree\\n-    logger.info(`Creating worktree for ${repository}@${ref}`);\\n-    const createCmd = `git -C ${this.escapeShellArg(bareRepoPath)} worktree add ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(ref)}`;\\n+    // Create worktree in detached HEAD state at the resolved commit\\n+    logger.info(`Creating worktree for ${repository}@${ref} (${commit})`);\\n+    const createCmd = `git -C ${this.escapeShellArg(\\n+      bareRepoPath\\n+    )} worktree add --detach ${this.escapeShellArg(worktreePath)} ${this.escapeShellArg(commit)}`;\\n     const result = await this.executeGitCommand(createCmd, { timeout: 60000 });\\n \\n     if (result.exitCode !== 0) {\\n       throw new Error(`Failed to create worktree: ${result.stderr}`);\\n     }\\n \\n-    // Get commit SHA\\n-    const commit = await this.getCommitSha(worktreePath);\\n-\\n     // Create metadata\\n     const metadata: WorktreeMetadata = {\\n       worktree_id: worktreeId,\\n@@ -316,14 +416,17 @@ export class WorktreeManager {\\n   }\\n \\n   /**\\n-   * Get commit SHA for worktree\\n+   * Get commit SHA for a given ref inside a bare repository.\\n+   *\\n+   * This runs after fetchRef so that <ref> should resolve to either a\\n+   * local branch, tag, or remote-tracking ref.\\n    */\\n-  private async getCommitSha(worktreePath: string): Promise<string> {\\n-    const cmd = `git -C ${this.escapeShellArg(worktreePath)} rev-parse HEAD`;\\n+  private async getCommitShaForRef(bareRepoPath: string, ref: string): Promise<string> {\\n+    const cmd = `git -C ${this.escapeShellArg(bareRepoPath)} rev-parse ${this.escapeShellArg(ref)}`;\\n     const result = await this.executeGitCommand(cmd);\\n \\n     if (result.exitCode !== 0) {\\n-      throw new Error(`Failed to get commit SHA: ${result.stderr}`);\\n+      throw new Error(`Failed to get commit SHA for ref ${ref}: ${result.stderr}`);\\n     }\\n \\n     return result.stdout.trim();\\n@@ -621,9 +724,21 @@ export class WorktreeManager {\\n     command: string,\\n     options: { timeout?: number; env?: Record<string, string> } = {}\\n   ): Promise<GitCommandResult> {\\n+    // Merge provided env with process.env and add git-specific settings\\n+    // These settings prevent git from hanging on interactive prompts while\\n+    // still allowing OS-level credential helpers to work:\\n+    // - GIT_TERMINAL_PROMPT=0: Prevents terminal credential prompts\\n+    // - GIT_SSH_COMMAND: Disables SSH password prompts (BatchMode)\\n+    const gitEnv = {\\n+      ...process.env,\\n+      ...options.env,\\n+      GIT_TERMINAL_PROMPT: '0',\\n+      GIT_SSH_COMMAND: 'ssh -o BatchMode=yes -o StrictHostKeyChecking=no',\\n+    };\\n+\\n     const result = await commandExecutor.execute(command, {\\n       timeout: options.timeout || 30000,\\n-      env: options.env || process.env,\\n+      env: gitEnv,\\n     } as any);\\n \\n     return {\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-executor.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-executor.ts b/src/workflow-executor.ts\\nindex 751e7f16..df76762f 100644\\n--- a/src/workflow-executor.ts\\n+++ b/src/workflow-executor.ts\\n@@ -219,7 +219,9 @@ export class WorkflowExecutor {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps)) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     // Use static DependencyResolver\\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-registry.ts\",\"additions\":4,\"deletions\":2,\"changes\":6,\"patch\":\"diff --git a/src/workflow-registry.ts b/src/workflow-registry.ts\\nindex 2cbe3a89..6bd855fb 100644\\n--- a/src/workflow-registry.ts\\n+++ b/src/workflow-registry.ts\\n@@ -410,7 +410,9 @@ export class WorkflowRegistry {\\n     // Build dependency map\\n     const dependencies: Record<string, string[]> = {};\\n     for (const [stepId, step] of Object.entries(workflow.steps || {})) {\\n-      dependencies[stepId] = step.depends_on || [];\\n+      // Normalize depends_on to array (supports string | string[])\\n+      const rawDeps = step.depends_on;\\n+      dependencies[stepId] = Array.isArray(rawDeps) ? rawDeps : rawDeps ? [rawDeps] : [];\\n     }\\n \\n     try {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/ai-review-service.test.ts\",\"additions\":32,\"deletions\":1,\"changes\":33,\"patch\":\"diff --git a/tests/unit/ai-review-service.test.ts b/tests/unit/ai-review-service.test.ts\\nindex ec2b998a..2c616c99 100644\\n--- a/tests/unit/ai-review-service.test.ts\\n+++ b/tests/unit/ai-review-service.test.ts\\n@@ -482,6 +482,37 @@ describe('AIReviewService', () => {\\n       expect(context).toContain('<additions>10</additions>');\\n       expect(context).toContain('<deletions>5</deletions>');\\n     });\\n+\\n+    it('should exclude diffs by default when in Slack mode', async () => {\\n+      // Slack mode is detected by presence of slackConversation property\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // In Slack mode, diffs should be excluded by default\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+      // Should still include file summary\\n+      expect(context).toContain('<files_summary>');\\n+    });\\n+\\n+    it('should include diffs in Slack mode when explicitly requested', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      (mockPRInfo as any).includeCodeContext = true;\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      // With explicit includeCodeContext: true, diffs should be included\\n+      expect(context).toContain('<full_diff>');\\n+      expect(context).toContain('--- test.ts');\\n+    });\\n+\\n+    it('should exclude diffs in Slack mode even when includeCodeContext is undefined', async () => {\\n+      (mockPRInfo as any).slackConversation = [{ user: 'U123', text: 'hello' }];\\n+      // Deliberately not setting includeCodeContext to test default behavior\\n+      const context = await (service as any).formatPRContext(mockPRInfo);\\n+\\n+      expect(context).not.toContain('<full_diff>');\\n+      expect(context).toContain('Code diffs excluded to reduce token usage');\\n+    });\\n   });\\n \\n   describe('Comment Filtering for Code Review', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config-extends.test.ts\",\"additions\":8,\"deletions\":8,\"changes\":16,\"patch\":\"diff --git a/tests/unit/config-extends.test.ts b/tests/unit/config-extends.test.ts\\nindex b112a869..7ee69ce6 100644\\n--- a/tests/unit/config-extends.test.ts\\n+++ b/tests/unit/config-extends.test.ts\\n@@ -564,7 +564,7 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.security).toBeDefined();\\n       expect(config.checks!.performance).toBeDefined();\\n       expect(config.checks!.custom).toBeDefined();\\n-      expect(config.output.pr_comment.format).toBe('json');\\n+      expect(config.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should extend from default configuration', async () => {\\n@@ -1002,7 +1002,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.base).toBeDefined(); // From base\\n       expect(loaded.checks!.middle).toBeDefined(); // From middle\\n       expect(loaded.checks!.top).toBeDefined(); // From top\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From top\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From top\\n     });\\n \\n     it('should handle mixed local and remote extends', async () => {\\n@@ -1079,7 +1079,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.local).toBeDefined();\\n       expect(loaded.checks!.remote).toBeDefined();\\n       expect(loaded.checks!.child).toBeDefined();\\n-      expect(loaded.output.pr_comment.format).toBe('json');\\n+      expect(loaded.output!.pr_comment.format).toBe('json');\\n     });\\n \\n     it('should handle extends with relative path resolution', async () => {\\n@@ -1593,9 +1593,9 @@ describe('Config Extends Functionality', () => {\\n       expect(config.checks!.performance).toBeDefined(); // Added in child\\n \\n       // Verify output\\n-      expect(config.output.pr_comment.format).toBe('json'); // Overridden\\n-      expect(config.output.pr_comment.group_by).toBe('file'); // Overridden\\n-      expect(config.output.pr_comment.collapse).toBe(false); // Overridden\\n+      expect(config.output!.pr_comment.format).toBe('json'); // Overridden\\n+      expect(config.output!.pr_comment.group_by).toBe('file'); // Overridden\\n+      expect(config.output!.pr_comment.collapse).toBe(false); // Overridden\\n     });\\n \\n     it('should handle configuration with disabled checks removal', async () => {\\n@@ -1814,7 +1814,7 @@ describe('Config Extends Functionality', () => {\\n       expect(loaded.checks!.root).toBeDefined(); // From root\\n       expect(loaded.checks!.level1).toBeDefined(); // From level1\\n       expect(loaded.checks!.level2).toBeDefined(); // From level2\\n-      expect(loaded.output.pr_comment.format).toBe('json'); // From level2\\n+      expect(loaded.output!.pr_comment.format).toBe('json'); // From level2\\n     });\\n \\n     it('should handle mixed absolute and relative paths in extends array', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config.test.ts\",\"additions\":7,\"deletions\":7,\"changes\":14,\"patch\":\"diff --git a/tests/unit/config.test.ts b/tests/unit/config.test.ts\\nindex 67ce61fd..0677a3d4 100644\\n--- a/tests/unit/config.test.ts\\n+++ b/tests/unit/config.test.ts\\n@@ -48,7 +48,7 @@ output:\\n       expect(config.version).toBe('1.0');\\n       expect(config.checks).toHaveProperty('performance');\\n       expect(config.checks).toHaveProperty('security');\\n-      expect(config.output.pr_comment.format).toBe('table');\\n+      expect(config.output!.pr_comment.format).toBe('table');\\n     });\\n \\n     it('should handle missing config file gracefully', async () => {\\n@@ -264,9 +264,9 @@ checks:\\n       const config = await configManager.loadConfig('/path/to/minimal.yaml');\\n \\n       // Should have default output configuration\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.group_by).toBe('check');\\n-      expect(config.output.pr_comment.collapse).toBe(true);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.group_by).toBe('check');\\n+      expect(config.output!.pr_comment.collapse).toBe(true);\\n     });\\n   });\\n \\n@@ -483,8 +483,8 @@ output:\\n       expect(config.checks!.performance.prompt).toContain('N+1 database queries');\\n       expect(config.checks!.security.prompt).toContain('SQL injection');\\n       expect(config.checks!.architecture.prompt).toContain('SOLID principles');\\n-      expect(config.output.pr_comment.format).toBe('markdown');\\n-      expect(config.output.pr_comment.collapse).toBe(false);\\n+      expect(config.output!.pr_comment.format).toBe('markdown');\\n+      expect(config.output!.pr_comment.collapse).toBe(false);\\n     });\\n   });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/foreach-empty-skip.test.ts\",\"additions\":5,\"deletions\":3,\"changes\":8,\"patch\":\"diff --git a/tests/unit/foreach-empty-skip.test.ts b/tests/unit/foreach-empty-skip.test.ts\\nindex 1ea8e1e5..2640ed14 100644\\n--- a/tests/unit/foreach-empty-skip.test.ts\\n+++ b/tests/unit/foreach-empty-skip.test.ts\\n@@ -90,11 +90,13 @@ describe('forEach Empty Array Skip', () => {\\n     );\\n     expect(processStats?.totalRuns).toBe(0);\\n \\n-    // notify-ticket should also NOT run because process-ticket returned empty\\n+    // notify-ticket SHOULD run even though process-ticket had 0 runs from empty forEach\\n+    // This is the expected behavior: downstream steps that depend on forEach children\\n+    // should still execute (with empty/null inputs) to handle the \\\"no items\\\" case\\n     const notifyStats = result.executionStatistics?.checks!.find(\\n       c => c.checkName === 'notify-ticket'\\n     );\\n-    expect(notifyStats?.totalRuns).toBe(0);\\n+    expect(notifyStats?.totalRuns).toBe(1);\\n   });\\n \\n   it('should handle empty array with transform_js', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/ai-check-provider.test.ts\",\"additions\":61,\"deletions\":1,\"changes\":62,\"patch\":\"diff --git a/tests/unit/providers/ai-check-provider.test.ts b/tests/unit/providers/ai-check-provider.test.ts\\nindex d0b94086..a858ddae 100644\\n--- a/tests/unit/providers/ai-check-provider.test.ts\\n+++ b/tests/unit/providers/ai-check-provider.test.ts\\n@@ -399,6 +399,66 @@ describe('AICheckProvider', () => {\\n         },\\n       });\\n     });\\n+\\n+    it('derives allowedFolders and path from workspace when present on parent context', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const workspace = {\\n+        isEnabled: () => true,\\n+        getWorkspaceInfo: () => ({\\n+          workspacePath: '/tmp/ws-123',\\n+          mainProjectPath: '/tmp/ws-123/main-project',\\n+        }),\\n+        listProjects: () => [\\n+          { name: 'tyk', path: '/tmp/ws-123/tyk' },\\n+          { name: 'tyk-docs', path: '/tmp/ws-123/tyk-docs' },\\n+        ],\\n+      };\\n+\\n+      const execContext: any = {\\n+        _parentContext: {\\n+          workspace,\\n+        },\\n+      };\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'code_help',\\n+        ai: {\\n+          provider: 'google',\\n+          model: 'gemini-2.5-pro',\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, execContext);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'google',\\n+        model: 'gemini-2.5-pro',\\n+        // Workspace root should be used as primary working directory for tools\\n+        path: '/tmp/ws-123',\\n+      });\\n+\\n+      // allowedFolders should contain the workspace plus all project paths, de-duped\\n+      expect(capturedConfig.allowedFolders).toEqual(\\n+        expect.arrayContaining(['/tmp/ws-123', '/tmp/ws-123/tyk', '/tmp/ws-123/tyk-docs'])\\n+      );\\n+    });\\n   });\\n \\n   describe('getSupportedConfigKeys', () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/command-check-provider.test.ts\",\"additions\":105,\"deletions\":1,\"changes\":106,\"patch\":\"diff --git a/tests/unit/providers/command-check-provider.test.ts b/tests/unit/providers/command-check-provider.test.ts\\nindex 3bf11a84..0a821bbd 100644\\n--- a/tests/unit/providers/command-check-provider.test.ts\\n+++ b/tests/unit/providers/command-check-provider.test.ts\\n@@ -666,4 +666,108 @@ describe('CommandCheckProvider', () => {\\n       expect((result as any).output).toBe('content'); // CommandCheckProvider trims whitespace\\n     });\\n   });\\n+\\n+  describe('Workflow Inputs', () => {\\n+    it('should use workflowInputs from config when available', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Hello from config.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from config.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with inputs from config.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from config.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should fall back to context.workflowInputs when config.workflowInputs is not set', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Hello from context.workflowInputs',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Hello from context.workflowInputs\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify the command was rendered with inputs from context.workflowInputs\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Hello from context.workflowInputs\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should prefer config.workflowInputs over context.workflowInputs', async () => {\\n+      const config: CheckProviderConfig & { workflowInputs: Record<string, string> } = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+        workflowInputs: {\\n+          text: 'Config takes precedence',\\n+        },\\n+      };\\n+\\n+      const context = {\\n+        workflowInputs: {\\n+          text: 'Context should be ignored',\\n+        },\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=Config takes precedence\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config, undefined, context as any);\\n+\\n+      // Verify config.workflowInputs takes precedence\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=Config takes precedence\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+\\n+    it('should provide empty inputs when neither config nor context has workflowInputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'command',\\n+        exec: 'echo \\\"TEXT={{ inputs.text }}\\\"',\\n+      };\\n+\\n+      mockExecute.mockResolvedValue({\\n+        stdout: 'TEXT=\\\\n',\\n+        stderr: '',\\n+        exitCode: 0,\\n+      });\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      // Verify the command was rendered with empty inputs (undefined renders as empty)\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"TEXT=\\\"',\\n+        expect.any(Object)\\n+      );\\n+    });\\n+  });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/git-checkout-workspace.test.ts\",\"additions\":4,\"deletions\":5,\"changes\":9,\"patch\":\"diff --git a/tests/unit/providers/git-checkout-workspace.test.ts b/tests/unit/providers/git-checkout-workspace.test.ts\\nindex 4eb625b7..4a0fbb6e 100644\\n--- a/tests/unit/providers/git-checkout-workspace.test.ts\\n+++ b/tests/unit/providers/git-checkout-workspace.test.ts\\n@@ -129,13 +129,12 @@ describe('GitCheckoutProvider Workspace Integration', () => {\\n         const output = (result as any).output as GitCheckoutOutput;\\n         expect(output.success).toBe(true);\\n         expect(output.workspace_path).toBeDefined();\\n-        // When checkName is provided, it's used as the project name\\n-        expect(output.workspace_path).toContain('test-checkout');\\n \\n-        // Verify project was added to workspace\\n+        // Verify project was added to workspace and the name is derived\\n+        // from the repository/description, not the checkName.\\n         const projects = workspace.listProjects();\\n         expect(projects.length).toBe(1);\\n-        expect(projects[0].name).toBe('test-checkout');\\n+        expect(projects[0].name).toBe('external-repo');\\n       } finally {\\n         await workspace.cleanup();\\n         fs.rmSync(mainProjectDir, { recursive: true, force: true });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/http-client-provider.test.ts\",\"additions\":15,\"deletions\":10,\"changes\":25,\"patch\":\"diff --git a/tests/unit/providers/http-client-provider.test.ts b/tests/unit/providers/http-client-provider.test.ts\\nindex b759ad75..caa73417 100644\\n--- a/tests/unit/providers/http-client-provider.test.ts\\n+++ b/tests/unit/providers/http-client-provider.test.ts\\n@@ -120,15 +120,17 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect(result).toEqual<ReviewSummary & { data: unknown }>({\\n+      // The provider returns data in the 'output' property (consistent with other providers)\\n+      expect(result).toEqual({\\n         issues: [],\\n-        data: responseData,\\n+        output: { status: 'ok', data: { value: 123 } },\\n       });\\n     });\\n \\n     it('should handle POST request with body', async () => {\\n       mockConfig.method = 'POST';\\n-      mockConfig.body = '{\\\"request\\\": \\\"data\\\"}';\\n+      // Use a body with Liquid template to trigger parseAndRender\\n+      mockConfig.body = '{\\\"request\\\": \\\"{{ pr.title }}\\\"}';\\n \\n       const responseData = { result: 'success' };\\n       const mockResponse = {\\n@@ -143,15 +145,16 @@ describe('HttpClientProvider', () => {\\n       };\\n \\n       mockFetch.mockResolvedValue(mockResponse);\\n-      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"data\\\"}');\\n+      mockLiquid.parseAndRender.mockResolvedValue('{\\\"request\\\": \\\"Test PR\\\"}');\\n \\n       const result = await provider.execute(mockPRInfo, mockConfig, new Map());\\n \\n+      // Verify the body template was parsed by Liquid with the correct template\\n       expect(mockLiquid.parseAndRender).toHaveBeenCalledWith(\\n-        '{\\\"request\\\": \\\"data\\\"}',\\n+        '{\\\"request\\\": \\\"{{ pr.title }}\\\"}',\\n         expect.objectContaining({\\n           pr: expect.any(Object),\\n-          outputs: {},\\n+          outputs: expect.any(Object),\\n         })\\n       );\\n \\n@@ -159,7 +162,7 @@ describe('HttpClientProvider', () => {\\n         'https://api.example.com/data',\\n         expect.objectContaining({\\n           method: 'POST',\\n-          body: '{\\\"request\\\": \\\"data\\\"}',\\n+          body: '{\\\"request\\\": \\\"Test PR\\\"}',\\n           headers: expect.objectContaining({\\n             Authorization: 'Bearer token',\\n             'Content-Type': 'application/json',\\n@@ -167,7 +170,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(responseData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { result: string } }).output.result).toEqual('success');\\n     });\\n \\n     it('should handle HTTP errors', async () => {\\n@@ -235,7 +239,8 @@ describe('HttpClientProvider', () => {\\n         })\\n       );\\n \\n-      expect((result as ReviewSummary & { data: unknown }).data).toEqual(transformedData);\\n+      // The provider returns data in the 'output' property\\n+      expect((result as ReviewSummary & { output: { transformed: string } }).output.transformed).toEqual('result');\\n     });\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/session-reuse-config.test.ts\",\"additions\":20,\"deletions\":1,\"changes\":21,\"patch\":\"diff --git a/tests/unit/session-reuse-config.test.ts b/tests/unit/session-reuse-config.test.ts\\nindex a2e2ee26..18fc42e6 100644\\n--- a/tests/unit/session-reuse-config.test.ts\\n+++ b/tests/unit/session-reuse-config.test.ts\\n@@ -152,6 +152,25 @@ describe('Session Reuse Configuration Validation', () => {\\n       }).not.toThrow();\\n     });\\n \\n+    it('should accept reuse_ai_session=\\\\\\\"self\\\\\\\" without depends_on', () => {\\n+      const config: Partial<VisorConfig> = {\\n+        version: '1.0',\\n+        checks: {\\n+          'loop-check': {\\n+            type: 'ai',\\n+            prompt: 'Chat-like loop that reuses its own session',\\n+            on: ['pr_opened'],\\n+            // Special self-reuse mode does not require depends_on\\n+            reuse_ai_session: 'self',\\n+          },\\n+        },\\n+      };\\n+\\n+      expect(() => {\\n+        (configManager as any).validateConfig(config);\\n+      }).not.toThrow();\\n+    });\\n+\\n     it('should accept reuse_ai_session=true with multiple dependencies', () => {\\n       const config: Partial<VisorConfig> = {\\n         version: '1.0',\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/slack-markdown-format.test.ts\",\"additions\":25,\"deletions\":1,\"changes\":26,\"patch\":\"diff --git a/tests/unit/slack-markdown-format.test.ts b/tests/unit/slack-markdown-format.test.ts\\nindex 5e57ccae..39d0baf3 100644\\n--- a/tests/unit/slack-markdown-format.test.ts\\n+++ b/tests/unit/slack-markdown-format.test.ts\\n@@ -39,4 +39,28 @@ describe('markdownToSlack', () => {\\n       ['‚Ä¢ parent', '  ‚Ä¢ child', '```', '- not-a-bullet inside code', '```', '‚Ä¢ after'].join('\\\\n')\\n     );\\n   });\\n+\\n+  it('converts markdown headers to bold text', () => {\\n+    const input = '# Main Title\\\\n## Subtitle\\\\n### Section';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*Main Title*\\\\n*Subtitle*\\\\n*Section*');\\n+  });\\n+\\n+  it('adds newline before h1/h2 headers when preceded by content', () => {\\n+    const input = 'Some content\\\\n## New Section\\\\nMore content';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('Some content\\\\n\\\\n*New Section*\\\\nMore content');\\n+  });\\n+\\n+  it('does not add newline before h1 if it is the first line', () => {\\n+    const input = '# First Header\\\\nContent here';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('*First Header*\\\\nContent here');\\n+  });\\n+\\n+  it('ignores headers inside code blocks', () => {\\n+    const input = '```\\\\n# This is a comment\\\\n```\\\\n# Real Header';\\n+    const out = markdownToSlack(input);\\n+    expect(out).toBe('```\\\\n# This is a comment\\\\n```\\\\n*Real Header*');\\n+  });\\n });\\n\",\"status\":\"modified\"}],\"outputs\":{\"ask\":{\"text\":\"Can all of the APIDefinition and SEcurityPolicy CRDs be exported from an environment?\",\"ts\":1767907826317},\"jira-context\":{\"jira_context_xml\":\"\",\"issues\":[],\"issue_count\":0,\"ts\":1767907826373},\"zendesk-context\":{\"zendesk_context_xml\":\"\",\"tickets\":\"\",\"ticket_count\":0,\"attachments\":\"\",\"ts\":1767907826386},\"log-request\":{\"issues\":[],\"logOutput\":\"‚ÑπÔ∏è **INFO**: üì• Request from user: unknown\\nChannel: unknown\\nThread: unknown\\nMessage: Can all of the APIDefinition and SEcurityPolicy CRDs be exported from an environment?\\n\"},\"code-help-error\":{\"issues\":[]},\"customer-insights-error\":{\"issues\":[]},\"release-notes-error\":{\"issues\":[]}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"route-intent","visor.check.output":"{\"intent\":\"code_help\",\"topic\":\"User is asking if all APIDefinition and SecurityPolicy CRDs can be exported from an environment.\",\"text\":\"{\\n  \\\"intent\\\": \\\"code_help\\\",\\n  \\\"topic\\\": \\\"User is asking if all APIDefinition and SecurityPolicy CRDs can be exported from an environment.\\\"\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":2,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"code-help","visor.provider.type":"workflow"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"ad13b579-01ee-4f98-9c43-742589b0c2c5"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"ad13b579-01ee-4f98-9c43-742589b0c2c5"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"ad13b579-01ee-4f98-9c43-742589b0c2c5"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-tyk-docs","visor.provider.type":"git-checkout"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ad13b579-01ee-4f98-9c43-742589b0c2c5"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ad13b579-01ee-4f98-9c43-742589b0c2c5"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"ad13b579-01ee-4f98-9c43-742589b0c2c5"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ensure-code-plan","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"ensure-code-plan","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"outputs\":{\"checkout-tyk-docs\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-docs-main-fd7d7640\",\"ref\":\"main\",\"commit\":\"a022bb7c96ca4f48bcdac27cae3f7c4a24171ee9\",\"worktree_id\":\"TykTechnologies-tyk-docs-main-fd7d7640\",\"repository\":\"TykTechnologies/tyk-docs\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/b909c433-44f6-49ca-a8e6-b11088cb05cb/tyk-docs\",\"ts\":1767907833942}},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"ensure-code-plan","visor.check.output":"{\"projects\":[{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Dashboard is the control plane that stores and manages API Definitions and Security Policies. Any export functionality, whether via its API or a tool like tyk-sync, would source the data from the Dashboard.\"},{\"project_id\":\"tyk\",\"reason\":\"The Tyk Gateway defines the schema and behavior for API Definitions and Security Policies. Understanding the structure and content of these exported resources requires knowledge of the Gateway's configuration.\"}],\"notes\":\"The user is asking about exporting Kubernetes CRDs (APIDefinition, SecurityPolicy). These are managed by the Tyk Operator, which acts as a client to the Tyk Dashboard API. Therefore, the core components involved are the Dashboard (for managing the resources) and the Gateway (for defining what goes into those resources).\",\"text\":\"{\\n  \\\"projects\\\": [\\n    {\\n      \\\"project_id\\\": \\\"tyk-analytics\\\",\\n      \\\"reason\\\": \\\"The Tyk Dashboard is the control plane that stores and manages API Definitions and Security Policies. Any export functionality, whether via its API or a tool like tyk-sync, would source the data from the Dashboard.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk\\\",\\n      \\\"reason\\\": \\\"The Tyk Gateway defines the schema and behavior for API Definitions and Security Policies. Understanding the structure and content of these exported resources requires knowledge of the Gateway's configuration.\\\"\\n    }\\n  ],\\n  \\\"notes\\\": \\\"The user is asking about exporting Kubernetes CRDs (APIDefinition, SecurityPolicy). These are managed by the Tyk Operator, which acts as a client to the Tyk Dashboard API. Therefore, the core components involved are the Dashboard (for managing the resources) and the Gateway (for defining what goes into those resources).\\\"\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ad13b579-01ee-4f98-9c43-742589b0c2c5"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ad13b579-01ee-4f98-9c43-742589b0c2c5"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"ad13b579-01ee-4f98-9c43-742589b0c2c5"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"project-items","visor.provider.type":"script"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ad13b579-01ee-4f98-9c43-742589b0c2c5"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ad13b579-01ee-4f98-9c43-742589b0c2c5"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"ad13b579-01ee-4f98-9c43-742589b0c2c5"}}]}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":0,"visor.foreach.total":2},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.foreach.item","attributes":{"visor.check.id":"checkout-projects","visor.foreach.index":1,"visor.foreach.total":2},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"checkout-projects","visor.provider.type":"git-checkout"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ad13b579-01ee-4f98-9c43-742589b0c2c5"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ad13b579-01ee-4f98-9c43-742589b0c2c5"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"ad13b579-01ee-4f98-9c43-742589b0c2c5"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"project-code-query","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"project-code-query","visor.check.input.context":"{\"pr\":{\"number\":1,\"title\":\"State Machine Execution\",\"author\":\"system\"},\"files\":[],\"outputs\":{\"ensure-code-plan\":{\"projects\":[{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Dashboard is the control plane that stores and manages API Definitions and Security Policies. Any export functionality, whether via its API or a tool like tyk-sync, would source the data from the Dashboard.\"},{\"project_id\":\"tyk\",\"reason\":\"The Tyk Gateway defines the schema and behavior for API Definitions and Security Policies. Understanding the structure and content of these exported resources requires knowledge of the Gateway's configuration.\"}],\"notes\":\"The user is asking about exporting Kubernetes CRDs (APIDefinition, SecurityPolicy). These are managed by the Tyk Operator, which acts as a client to the Tyk Dashboard API. Therefore, the core components involved are the Dashboard (for managing the resources) and the Gateway (for defining what goes into those resources).\",\"text\":\"{\\n  \\\"projects\\\": [\\n    {\\n      \\\"project_id\\\": \\\"tyk-analytics\\\",\\n      \\\"reason\\\": \\\"The Tyk Dashboard is the control plane that stores and manages API Definitions and Security Policies. Any export functionality, whether via its API or a tool like tyk-sync, would source the data from the Dashboard.\\\"\\n    },\\n    {\\n      \\\"project_id\\\": \\\"tyk\\\",\\n      \\\"reason\\\": \\\"The Tyk Gateway defines the schema and behavior for API Definitions and Security Policies. Understanding the structure and content of these exported resources requires knowledge of the Gateway's configuration.\\\"\\n    }\\n  ],\\n  \\\"notes\\\": \\\"The user is asking about exporting Kubernetes CRDs (APIDefinition, SecurityPolicy). These are managed by the Tyk Operator, which acts as a client to the Tyk Dashboard API. Therefore, the core components involved are the Dashboard (for managing the resources) and the Gateway (for defining what goes into those resources).\\\"\\n}\",\"ts\":1767907868275},\"project-items\":[{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Dashboard is the control plane that stores and manages API Definitions and Security Policies. Any export functionality, whether via its API or a tool like tyk-sync, would source the data from the Dashboard.\",\"repository\":\"TykTechnologies/tyk-analytics\",\"description\":\"Tyk Dashboard backend\"},{\"project_id\":\"tyk\",\"reason\":\"The Tyk Gateway defines the schema and behavior for API Definitions and Security Policies. Understanding the structure and content of these exported resources requires knowledge of the Gateway's configuration.\",\"repository\":\"TykTechnologies/tyk\",\"description\":\"Tyk Gateway\"}],\"checkout-projects\":{\"issues\":[],\"isForEach\":true,\"forEachItems\":[{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-analytics-HEAD-0edabb11\",\"ref\":\"HEAD\",\"commit\":\"b7ed8000a93e1284b28100938363a312d2d1fad1\",\"worktree_id\":\"TykTechnologies-tyk-analytics-HEAD-0edabb11\",\"repository\":\"TykTechnologies/tyk-analytics\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/b909c433-44f6-49ca-a8e6-b11088cb05cb/tyk-analytics\"},{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-HEAD-7080d83f\",\"ref\":\"HEAD\",\"commit\":\"bb67e9e7f9382a09ff152c6942d00023c463feef\",\"worktree_id\":\"TykTechnologies-tyk-HEAD-7080d83f\",\"repository\":\"TykTechnologies/tyk\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/b909c433-44f6-49ca-a8e6-b11088cb05cb/tyk\"}],\"forEachItemResults\":[{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-analytics-HEAD-0edabb11\",\"ref\":\"HEAD\",\"commit\":\"b7ed8000a93e1284b28100938363a312d2d1fad1\",\"worktree_id\":\"TykTechnologies-tyk-analytics-HEAD-0edabb11\",\"repository\":\"TykTechnologies/tyk-analytics\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/b909c433-44f6-49ca-a8e6-b11088cb05cb/tyk-analytics\"}},{\"issues\":[],\"output\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-HEAD-7080d83f\",\"ref\":\"HEAD\",\"commit\":\"bb67e9e7f9382a09ff152c6942d00023c463feef\",\"worktree_id\":\"TykTechnologies-tyk-HEAD-7080d83f\",\"repository\":\"TykTechnologies/tyk\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/b909c433-44f6-49ca-a8e6-b11088cb05cb/tyk\"}}]},\"checkout-tyk-docs\":{\"success\":true,\"path\":\"/home/buger/projects/visor2/.visor/worktrees/worktrees/TykTechnologies-tyk-docs-main-fd7d7640\",\"ref\":\"main\",\"commit\":\"a022bb7c96ca4f48bcdac27cae3f7c4a24171ee9\",\"worktree_id\":\"TykTechnologies-tyk-docs-main-fd7d7640\",\"repository\":\"TykTechnologies/tyk-docs\",\"is_worktree\":true,\"workspace_path\":\"/tmp/visor-workspaces/b909c433-44f6-49ca-a8e6-b11088cb05cb/tyk-docs\",\"ts\":1767907833942},\"project-items-raw\":[{\"project_id\":\"tyk-analytics\",\"reason\":\"The Tyk Dashboard is the control plane that stores and manages API Definitions and Security Policies. Any export functionality, whether via its API or a tool like tyk-sync, would source the data from the Dashboard.\",\"repository\":\"TykTechnologies/tyk-analytics\",\"description\":\"Tyk Dashboard backend\"},{\"project_id\":\"tyk\",\"reason\":\"The Tyk Gateway defines the schema and behavior for API Definitions and Security Policies. Understanding the structure and content of these exported resources requires knowledge of the Gateway's configuration.\",\"repository\":\"TykTechnologies/tyk\",\"description\":\"Tyk Gateway\"}]},\"args\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"project-code-query","visor.check.output":"{\"answer\":{\"text\":\"Yes, `APIDefinition` and `SecurityPolicy` Custom Resource Definitions (CRDs) can be exported from a Kubernetes environment where the Tyk Operator is installed. Here‚Äôs how it works and what tools you can use:\\n\\n### Using `kubectl`\\n\\nSince `APIDefinition` and `SecurityPolicy` are standard Kubernetes CRDs managed by the Tyk Operator, you can use `kubectl` to export them directly from your cluster. This is the most straightforward method for backing up or migrating your configurations.\\n\\nTo export all `APIDefinition` resources in YAML format, you can run:\\n\\n```bash\\nkubectl get apidefinitions.tyk.tyk.io -o yaml > apidefinitions.yaml\\n```\\n\\nSimilarly, to export all `SecurityPolicy` resources:\\n\\n```bash\\nkubectl get securitypolicies.tyk.tyk.io -o yaml > securitypolicies.yaml\\n```\\n\\nYou can also export individual resources by specifying their names:\\n\\n```bash\\nkubectl get apidefinitions.tyk.tyk.io <api-name> -o yaml\\n```\\n\\nThese exported YAML files contain the full resource definitions and can be applied to another Kubernetes cluster (with the Tyk Operator installed) to recreate the same APIs and policies.\\n\\n### Using `tyk-sync`\\n\\nFor more advanced use cases, such as GitOps workflows or synchronizing configurations between different Tyk environments (e.g., from a non-Kubernetes Tyk Dashboard to a Kubernetes cluster), you can use `tyk-sync`.\\n\\n`tyk-sync` is a command-line tool that can dump (export) API definitions and policies from a Tyk Dashboard and format them as `APIDefinition` and `SecurityPolicy` CRDs.\\n\\nA typical workflow would be:\\n\\n1.  **Configure `tyk-sync`** to connect to your Tyk Dashboard's API.\\n2.  **Run the `dump` command** to export the resources. You can specify the output format to be CRDs.\\n\\nFor example, to dump APIs as `APIDefinition` CRDs:\\n\\n```bash\\ntyk-sync dump --apis --k8s -o /path/to/output\\n```\\n\\nAnd for policies as `SecurityPolicy` CRDs:\\n\\n```bash\\ntyk-sync dump --policies --k8s -o /path/to/output\\n```\\n\\nThis will generate YAML files for each API and policy in the specified output directory, which can then be committed to a Git repository and applied to your Kubernetes cluster using tools like Argo CD or Flux.\\n\\n### Summary\\n\\n*   **For direct export from a Kubernetes cluster**, `kubectl` is the simplest and most direct method.\\n*   **For exporting from a Tyk Dashboard (Kubernetes or not) into CRD format for a GitOps workflow**, `tyk-sync` is the recommended tool.\\n\\nBoth methods allow you to export all of your `APIDefinition` and `SecurityPolicy` resources, providing a reliable way to back up, migrate, and manage your Tyk configurations as code.\\n\\n### References\\n\\n*   [Tyk Operator Documentation](https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/automations/operator.mdx)\\n*   [Create and Secure an API with Tyk Operator](https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-stack/tyk-operator/create-an-api.mdx)\\n*   [Tyk Sync Documentation](https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/automations/sync.mdx)\"},\"text\":\"{\\n  \\\"answer\\\": {\\n    \\\"text\\\": \\\"Yes, `APIDefinition` and `SecurityPolicy` Custom Resource Definitions (CRDs) can be exported from a Kubernetes environment where the Tyk Operator is installed. Here‚Äôs how it works and what tools you can use:\\\\n\\\\n### Using `kubectl`\\\\n\\\\nSince `APIDefinition` and `SecurityPolicy` are standard Kubernetes CRDs managed by the Tyk Operator, you can use `kubectl` to export them directly from your cluster. This is the most straightforward method for backing up or migrating your configurations.\\\\n\\\\nTo export all `APIDefinition` resources in YAML format, you can run:\\\\n\\\\n```bash\\\\nkubectl get apidefinitions.tyk.tyk.io -o yaml > apidefinitions.yaml\\\\n```\\\\n\\\\nSimilarly, to export all `SecurityPolicy` resources:\\\\n\\\\n```bash\\\\nkubectl get securitypolicies.tyk.tyk.io -o yaml > securitypolicies.yaml\\\\n```\\\\n\\\\nYou can also export individual resources by specifying their names:\\\\n\\\\n```bash\\\\nkubectl get apidefinitions.tyk.tyk.io <api-name> -o yaml\\\\n```\\\\n\\\\nThese exported YAML files contain the full resource definitions and can be applied to another Kubernetes cluster (with the Tyk Operator installed) to recreate the same APIs and policies.\\\\n\\\\n### Using `tyk-sync`\\\\n\\\\nFor more advanced use cases, such as GitOps workflows or synchronizing configurations between different Tyk environments (e.g., from a non-Kubernetes Tyk Dashboard to a Kubernetes cluster), you can use `tyk-sync`.\\\\n\\\\n`tyk-sync` is a command-line tool that can dump (export) API definitions and policies from a Tyk Dashboard and format them as `APIDefinition` and `SecurityPolicy` CRDs.\\\\n\\\\nA typical workflow would be:\\\\n\\\\n1.  **Configure `tyk-sync`** to connect to your Tyk Dashboard's API.\\\\n2.  **Run the `dump` command** to export the resources. You can specify the output format to be CRDs.\\\\n\\\\nFor example, to dump APIs as `APIDefinition` CRDs:\\\\n\\\\n```bash\\\\ntyk-sync dump --apis --k8s -o /path/to/output\\\\n```\\\\n\\\\nAnd for policies as `SecurityPolicy` CRDs:\\\\n\\\\n```bash\\\\ntyk-sync dump --policies --k8s -o /path/to/output\\\\n```\\\\n\\\\nThis will generate YAML files for each API and policy in the specified output directory, which can then be committed to a Git repository and applied to your Kubernetes cluster using tools like Argo CD or Flux.\\\\n\\\\n### Summary\\\\n\\\\n*   **For direct export from a Kubernetes cluster**, `kubectl` is the simplest and most direct method.\\\\n*   **For exporting from a Tyk Dashboard (Kubernetes or not) into CRD format for a GitOps workflow**, `tyk-sync` is the recommended tool.\\\\n\\\\nBoth methods allow you to export all of your `APIDefinition` and `SecurityPolicy` resources, providing a reliable way to back up, migrate, and manage your Tyk configurations as code.\\\\n\\\\n### References\\\\n\\\\n*   [Tyk Operator Documentation](https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/automations/operator.mdx)\\\\n*   [Create and Secure an API with Tyk Operator](https://github.com/TykTechnologies/tyk-docs/blob/main/tyk-stack/tyk-operator/create-an-api.mdx)\\\\n*   [Tyk Sync Documentation](https://github.com/TykTechnologies/tyk-docs/blob/main/api-management/automations/sync.mdx)\\\"\\n  }\\n}\"}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ad13b579-01ee-4f98-9c43-742589b0c2c5"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"ad13b579-01ee-4f98-9c43-742589b0c2c5"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"ad13b579-01ee-4f98-9c43-742589b0c2c5"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":2,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"code-help-reply","visor.provider.type":"log"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":2,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":3,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"ask","visor.provider.type":"human-input"}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":3,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"WavePlanning","engine_mode":"state-machine","wave":3,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":3,"session_id":"b909c433-44f6-49ca-a8e6-b11088cb05cb"}}]}

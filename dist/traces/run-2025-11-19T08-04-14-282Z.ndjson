{"name":"visor.run","attributes":{"started":true}}
{"name":"visor.run","attributes":{"started":true}}
{"name":"visor.run","attributes":{"started":true}}
{"name":"visor.run","attributes":{"started":true}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"2d22fe8f-df8b-47c8-811d-518fa9bf491d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"2d22fe8f-df8b-47c8-811d-518fa9bf491d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"2d22fe8f-df8b-47c8-811d-518fa9bf491d"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"overview","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"overview","visor.check.input.context":"{\"pr\":{\"number\":0,\"title\":\"Local Analysis: engine-state-machine (23 modified)\",\"author\":\"Leonid Bugaev\",\"branch\":\"engine-state-machine\",\"base\":\"main\"},\"files\":[{\"filename\":\".github/workflows/ci.yml\",\"additions\":1,\"deletions\":1,\"changes\":45,\"patch\":\"diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml\\nindex 6b5f7bf0..41a1d75d 100644\\n--- a/.github/workflows/ci.yml\\n+++ b/.github/workflows/ci.yml\\n@@ -58,6 +58,11 @@ jobs:\\n           ./dist/index.js --cli --version\\n           ./dist/index.js --cli --help\\n \\n+      - name: Run scenario tests\\n+        run: npm test -- tests/scenarios/\\n+        env:\\n+          CI: true\\n+\\n       - name: Test package installation\\n         run: |\\n           npm pack\\n@@ -89,41 +94,5 @@ jobs:\\n           comment-on-pr: 'false'\\n           add-reactions: 'false'\\n \\n-  test-scenarios:\\n-    runs-on: ubuntu-latest\\n-    steps:\\n-      - uses: actions/checkout@v4\\n-\\n-      - name: Setup Node.js\\n-        uses: actions/setup-node@v4\\n-        with:\\n-          node-version: '20'\\n-          # cache: 'npm' # Disabled due to rollup optional dependency bug\\n-\\n-      - name: Install dependencies\\n-        run: |\\n-          # Workaround for npm bug with rollup optional dependencies\\n-          # https://github.com/npm/cli/issues/4828\\n-          rm -rf node_modules package-lock.json\\n-          npm install\\n-          # Explicitly install rollup platform-specific binary\\n-          npm install --no-save @rollup/rollup-linux-x64-gnu\\n-\\n-      - name: Build (first to ensure dist/ exists)\\n-        run: npm run build --ignore-scripts\\n-\\n-      - name: Run act scenario tests\\n-        run: npm test -- tests/scenarios/\\n-\\n-      - name: Test PR review functionality (mock)\\n-        uses: ./\\n-        with:\\n-          github-token: ${{ secrets.GITHUB_TOKEN }}\\n-          auto-review: 'false'\\n-          ai-model: 'mock'\\n-          ai-provider: 'mock'\\n-          config-path: '.visor.test.yaml'\\n-          comment-on-pr: 'false'\\n-          add-reactions: 'false'\\n-        env:\\n-          GITHUB_EVENT_NAME: 'repository'\\n+  # Consolidated scenario tests into the main 'test' job to avoid duplication.\\n+  # If you need to split later, prefer a matrix or a reusable workflow.\\n\",\"status\":\"modified\"},{\"filename\":\".github/workflows/sdk-smoke.yml\",\"additions\":1,\"deletions\":1,\"changes\":36,\"patch\":\"diff --git a/.github/workflows/sdk-smoke.yml b/.github/workflows/sdk-smoke.yml\\nindex 2e7fe0a2..970b817d 100644\\n--- a/.github/workflows/sdk-smoke.yml\\n+++ b/.github/workflows/sdk-smoke.yml\\n@@ -17,9 +17,38 @@ jobs:\\n         uses: actions/setup-node@v4\\n         with:\\n           node-version: '20'\\n-\\n-      - name: Install deps\\n-        run: npm ci\\n+          cache: 'npm'\\n+          cache-dependency-path: package-lock.json\\n+\\n+      - name: Configure npm (retries, timeouts)\\n+        run: |\\n+          npm config set fetch-retries 5\\n+          npm config set fetch-retry-maxtimeout 120000\\n+          npm config set fetch-retry-mintimeout 20000\\n+          npm config set fetch-timeout 120000\\n+\\n+      - name: Install deps (retry up to 3x)\\n+        env:\\n+          PUPPETEER_SKIP_DOWNLOAD: '1'\\n+          PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: '1'\\n+          npm_config_audit: 'false'\\n+          npm_config_fund: 'false'\\n+        shell: bash\\n+        run: |\\n+          set -euo pipefail\\n+          attempts=0\\n+          until [ $attempts -ge 3 ]; do\\n+            if npm ci --no-audit --no-fund; then\\n+              break\\n+            fi\\n+            attempts=$((attempts+1))\\n+            echo \\\"npm ci failed (attempt $attempts); retrying in $((attempts*10))s...\\\" >&2\\n+            sleep $((attempts*10))\\n+          done\\n+          if [ $attempts -ge 3 ]; then\\n+            echo \\\"npm ci failed after 3 attempts\\\" >&2\\n+            exit 1\\n+          fi\\n \\n       - name: Build (CLI + SDK)\\n         run: npm run build\\n@@ -29,4 +58,3 @@ jobs:\\n \\n       - name: Run CJS example\\n         run: node examples/sdk-cjs.cjs\\n-\\n\",\"status\":\"modified\"},{\"filename\":\".gitignore\",\"additions\":1,\"deletions\":0,\"changes\":5,\"patch\":\"diff --git a/.gitignore b/.gitignore\\nindex 4b4dbcbd..9c3f9a77 100644\\n--- a/.gitignore\\n+++ b/.gitignore\\n@@ -5,6 +5,10 @@ npm-debug.log*\\n yarn-debug.log*\\n yarn-error.log*\\n \\n+.tmp*\\n+.*out\\n+.*json\\n+\\n # Build outputs\\n dist/\\n # Do not keep legacy ncc bundle\\n@@ -142,3 +146,4 @@ dist/output/**/*.ndjson\\n # Local schema temp and scratch\\n scripts/.schema-tmp/\\n tmp/\\n+.visor-agent-files.json\\n\",\"status\":\"added\"},{\"filename\":\"MANUAL_TESTING.md\",\"additions\":6,\"deletions\":0,\"changes\":194,\"patch\":\"diff --git a/MANUAL_TESTING.md b/MANUAL_TESTING.md\\nnew file mode 100644\\nindex 00000000..3f2eb30a\\n--- /dev/null\\n+++ b/MANUAL_TESTING.md\\n@@ -0,0 +1,194 @@\\n+# Manual Testing for Bash Configuration\\n+\\n+This document explains how to manually validate the bash configuration feature.\\n+\\n+## Quick Start\\n+\\n+```bash\\n+# Set your API key\\n+export ANTHROPIC_API_KEY=\\\"your-key-here\\\"\\n+\\n+# Run the manual tests\\n+npm run test:manual:bash\\n+```\\n+\\n+## What Gets Tested\\n+\\n+The manual tests validate:\\n+\\n+1. **Basic Bash Execution** - `allowBash: true` enables bash commands\\n+2. **Custom Configuration** - `bashConfig` options are passed correctly\\n+3. **Working Directory** - Custom `workingDirectory` is respected\\n+4. **Default Behavior** - Bash is disabled when not configured\\n+\\n+## Expected Output\\n+\\n+When tests pass, you'll see:\\n+\\n+```\\n+‚è≠Ô∏è  Skipping manual tests. Set RUN_MANUAL_TESTS=true to run.\\n+\\n+Bash Configuration Manual Tests\\n+  With API Key\\n+    üìù Testing allowBash: true\\n+    ‚úÖ allowBash test completed\\n+    üìä Result: { ... }\\n+    ‚úì should execute bash commands when allowBash is true (5000ms)\\n+\\n+    üìù Testing allowBash with bashConfig\\n+    ‚úÖ bashConfig test completed\\n+    üìä Result: { ... }\\n+    ‚úì should pass bashConfig options to ProbeAgent (5000ms)\\n+\\n+    üìù Testing bashConfig.workingDirectory\\n+    ‚úÖ workingDirectory test completed\\n+    üìä Result: { ... }\\n+    ‚úì should respect custom working directory (5000ms)\\n+\\n+    üìù Testing without allowBash (default behavior)\\n+    ‚úÖ No bash test completed\\n+    üìä Result: { ... }\\n+    ‚úì should work without bash when allowBash is not set (5000ms)\\n+\\n+  Configuration Validation\\n+    ‚úì should accept allowBash boolean\\n+    ‚úì should accept bashConfig object\\n+    ‚úì should accept both allowBash and bashConfig\\n+```\\n+\\n+## Alternative: Test with Real Configuration\\n+\\n+You can also test with a real Visor configuration file:\\n+\\n+### 1. Create a test configuration\\n+\\n+```yaml\\n+# test-bash-config.yaml\\n+version: \\\"1.0\\\"\\n+\\n+ai_provider: anthropic\\n+ai_model: claude-3-5-sonnet-20241022\\n+\\n+steps:\\n+  bash-test:\\n+    type: ai\\n+    prompt: |\\n+      Please run these bash commands:\\n+      1. echo \\\"Hello from Visor bash config test\\\"\\n+      2. pwd\\n+      3. ls -la\\n+\\n+      Summarize what you found.\\n+    ai:\\n+      allowBash: true\\n+      bashConfig:\\n+        timeout: 10000\\n+    on: [\\\"manual\\\"]\\n+```\\n+\\n+### 2. Run with Visor CLI\\n+\\n+```bash\\n+# Build first\\n+npm run build\\n+\\n+# Run the check\\n+export ANTHROPIC_API_KEY=\\\"your-key-here\\\"\\n+./dist/cli-main.js --config test-bash-config.yaml --check bash-test --event manual\\n+```\\n+\\n+### 3. Expected Output\\n+\\n+You should see the AI agent:\\n+- Successfully execute bash commands\\n+- Return results from `echo`, `pwd`, `ls`\\n+- Provide a summary of findings\\n+\\n+## Testing Different Configurations\\n+\\n+### Test Custom Allow List\\n+\\n+```yaml\\n+ai:\\n+  allowBash: true\\n+  bashConfig:\\n+    allow: ['git status', 'git log --oneline -5']\\n+```\\n+\\n+### Test Working Directory\\n+\\n+```yaml\\n+ai:\\n+  allowBash: true\\n+  bashConfig:\\n+    workingDirectory: '/tmp'\\n+```\\n+\\n+### Test Timeout\\n+\\n+```yaml\\n+ai:\\n+  allowBash: true\\n+  bashConfig:\\n+    timeout: 5000  # 5 seconds\\n+```\\n+\\n+## Troubleshooting\\n+\\n+### Tests are skipped\\n+\\n+Make sure you set `RUN_MANUAL_TESTS=true`:\\n+\\n+```bash\\n+RUN_MANUAL_TESTS=true npm run test:manual:bash\\n+```\\n+\\n+### API key not found\\n+\\n+Set your API key before running:\\n+\\n+```bash\\n+export ANTHROPIC_API_KEY=\\\"your-key-here\\\"\\n+```\\n+\\n+### Bash commands not working\\n+\\n+1. Verify ProbeAgent version supports bash config (>= 0.6.0-rc164)\\n+2. Check that `allowBash: true` is set\\n+3. Verify the command is in the default allow list or your custom `allow` list\\n+4. Check ProbeAgent logs with `debug: true`\\n+\\n+## Cost Considerations\\n+\\n+‚ö†Ô∏è **Warning**: These tests make real API calls to Anthropic and will incur costs. Each test run costs approximately $0.01-0.05 depending on the model and response length.\\n+\\n+To minimize costs:\\n+- Run tests only when needed\\n+- Use a cheaper model for testing (claude-3-haiku)\\n+- Keep prompts concise\\n+\\n+## Debugging\\n+\\n+Enable debug mode to see ProbeAgent interactions:\\n+\\n+```yaml\\n+ai:\\n+  provider: anthropic\\n+  debug: true  # Shows all tool calls and responses\\n+  allowBash: true\\n+```\\n+\\n+This will show:\\n+- Bash commands being executed\\n+- Command outputs\\n+- Tool call traces\\n+- Token usage\\n+\\n+## Next Steps\\n+\\n+After manual validation:\\n+1. Review test results\\n+2. Check ProbeAgent logs\\n+3. Verify bash commands executed correctly\\n+4. Test with your specific use cases\\n+5. Document any edge cases or issues\\n\",\"status\":\"added\"},{\"filename\":\"README.md\",\"additions\":1,\"deletions\":0,\"changes\":1,\"patch\":\"diff --git a/README.md b/README.md\\nindex 978f3e59..41131861 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -124,6 +124,7 @@ Additional guides:\\n - forEach behavior and dependent propagation (including outputs_raw and history precedence): [docs/foreach-dependency-propagation.md](docs/foreach-dependency-propagation.md)\\n - Failure routing and `on_finish` (with outputs_raw in routing JS): [docs/failure-routing.md](docs/failure-routing.md)\\n - timeouts and provider units: [docs/timeouts.md](docs/timeouts.md)\\n+- execution limits (run caps for safety): [docs/limits.md](docs/limits.md)\\n - output formatting limits and truncation controls: [docs/output-formatting.md](docs/output-formatting.md)\\n - live execution visualizer and control API: [docs/debug-visualizer.md](docs/debug-visualizer.md)\\n \\n\",\"status\":\"added\"},{\"filename\":\"defaults/agent-builder.yaml\",\"additions\":20,\"deletions\":0,\"changes\":714,\"patch\":\"diff --git a/defaults/agent-builder.yaml b/defaults/agent-builder.yaml\\nnew file mode 100644\\nindex 00000000..f3ccd29e\\n--- /dev/null\\n+++ b/defaults/agent-builder.yaml\\n@@ -0,0 +1,714 @@\\n+version: \\\"1.0\\\"\\n+\\n+# Agent Build (cwd-first): generate or improve a Visor agent config purely via YAML.\\n+# - Writes to current working directory by default (VISOR_AGENT_OUT_DIR, defaults \\\".\\\").\\n+# - To improve an existing config in-place, run the CLI with: visor build <path/to/agent.yaml>\\n+#   The YAML detects mode by reading the file; missing/empty => new, else improve.\\n+\\n+steps:\\n+  brief:\\n+    type: human-input\\n+    group: agent-build\\n+    on: [manual]\\n+    # no dependencies; can run standalone in manual prompt-only mode\\n+    prompt: |\\n+      Provide a concise brief for the agent to build.\\n+      Include: purpose, events, essential steps, and minimal success criteria.\\n+    placeholder: \\\"e.g., Agent that labels PRs by change size and runs jest\\\"\\n+    multiline: false\\n+    allow_empty: true\\n+    default: \\\"Create a minimal agent that can scaffold other agents (config + tests).\\\"\\n+    on_success:\\n+      goto: draft\\n+      goto_event: schedule\\n+\\n+  start:\\n+    type: log\\n+    group: agent-build\\n+    on: [issue_opened]\\n+    message: \\\"start\\\"\\n+    level: debug\\n+    # no goto; detect-mode runs on event filtering\\n+\\n+  detect-mode:\\n+    type: command\\n+    group: agent-build\\n+    on: [manual, issue_opened, schedule]\\n+    if: \\\"Boolean(env.VISOR_AGENT_PATH || env.VISOR_AGENT_OUT_DIR || env.VISOR_AGENT_MODE == 'improve')\\\"\\n+    exec: echo \\\"{}\\\"\\n+    transform: |\\n+      {% assign mode = 'new' %}\\n+      {% assign exists = false %}\\n+      {% assign empty = true %}\\n+      {% if env.VISOR_AGENT_MODE == 'improve' %}\\n+        {% assign mode = 'improve' %}\\n+      {% else %}\\n+        {% capture cfg %}{% readfile env.VISOR_AGENT_PATH %}{% endcapture %}\\n+        {% assign has_error = cfg contains 'Error reading file' or cfg contains 'Invalid file path' %}\\n+        {% assign trimmed = cfg | strip %}\\n+        {% assign empty = trimmed == '' %}\\n+        {% assign exists = has_error == false %}\\n+        {% if exists and empty == false %}{% assign mode = 'improve' %}{% endif %}\\n+      {% endif %}\\n+      {\\\"path\\\":\\\"{{ env.VISOR_AGENT_PATH }}\\\",\\\"exists\\\": {{ exists }}, \\\"empty\\\": {{ empty }}, \\\"mode\\\": \\\"{{ mode }}\\\"}\\n+    # no on_success routing; downstream steps run based on event + if guards\\n+\\n+  load-existing:\\n+    type: command\\n+    group: agent-build\\n+    depends_on: [detect-mode]\\n+    on: [issue_opened, schedule]\\n+    if: \\\"env.VISOR_AGENT_MODE == 'improve' || (outputs['detect-mode'] && outputs['detect-mode'].mode == 'improve')\\\"\\n+    exec: |\\n+      node <<'NODE'\\n+      const fs=require('fs'); const path=require('path');\\n+      const cfgPath = path.resolve(process.env.VISOR_AGENT_PATH);\\n+      if (!fs.existsSync(cfgPath)) { console.log(JSON.stringify({ ok:false, error:'not_found', path:cfgPath })); process.exit(0); }\\n+      const dir = path.dirname(cfgPath);\\n+      const slug = path.basename(cfgPath).replace(/\\\\.ya?ml$/i,'');\\n+      const testsPath = process.env.VISOR_AGENT_TESTS_PATH ? path.resolve(process.env.VISOR_AGENT_TESTS_PATH) : path.join(dir, slug + '.tests.yaml');\\n+      let cfgText = ''; let testsText = '';\\n+      try { cfgText = fs.readFileSync(cfgPath,'utf8'); } catch {}\\n+      try { testsText = fs.readFileSync(testsPath,'utf8'); } catch {}\\n+      const files = [];\\n+      if (cfgText) files.push({ path: cfgPath, content: cfgText });\\n+      if (testsText) files.push({ path: testsPath, content: testsText });\\n+      console.log(JSON.stringify({ ok:true, slug, config_path: cfgPath, tests_path: testsPath, files }));\\n+      NODE\\n+    output_format: json\\n+    on_success:\\n+      goto: write\\n+      goto_event: schedule\\n+\\n+  draft:\\n+    type: ai\\n+    group: agent-build\\n+    on: [issue_opened, schedule]\\n+    if: \\\"String((env.VISOR_AGENT_MODE || 'new')).toLowerCase() !== 'improve'\\\"\\n+    ai_prompt_type: engineer\\n+    ai:\\n+      provider: mock\\n+      skip_code_context: true\\n+    schema:\\n+      type: object\\n+      properties:\\n+        slug: { type: string }\\n+        summary: { type: string }\\n+        files:\\n+          type: array\\n+          items:\\n+            type: object\\n+            properties: { path: { type: string }, content: { type: string } }\\n+            required: [path, content]\\n+      required: [slug, files]\\n+    prompt: |\\n+      Generate a new Visor agent with YAML config and YAML tests only. Keep it simple and tailored to this repository.\\n+\\n+      Brief:\\n+      {{ outputs['brief'] | default: 'Create a minimal agent that can scaffold other agents (config + tests).' }}\\n+\\n+      Return JSON fields: slug, summary, files[] (path+content). Only content matters; paths will be normalized to the target path.\\n+      Include two files: <slug>.yaml and <slug>.tests.yaml.\\n+    on_success:\\n+      goto: write\\n+      goto_event: schedule\\n+\\n+  write:\\n+    type: command\\n+    group: agent-build\\n+    # Triggered only via goto from draft/refine/load-existing to avoid early evaluation\\n+    on: [issue_opened, schedule]\\n+    if: \\\"((((outputs||{})['refine']||{}).files) || (((outputs||{})['draft']||{}).files) || (((outputs||{})['load-existing']||{}).files)) && (env.VISOR_AGENT_PATH || env.VISOR_AGENT_OUT_DIR)\\\"\\n+    exec: |\\n+      {% assign set = outputs['refine'].files | default: outputs['draft'].files | default: outputs['load-existing'].files %}\\n+      cat > .visor-agent-files.json <<'JSON'\\n+      {{ set | to_json | default: '[]' }}\\n+      JSON\\n+      node <<'NODE'\\n+      const fs = require('fs');\\n+      const path = require('path');\\n+      const yaml = require('js-yaml');\\n+      // Read AI/refine results\\n+      let files = [];\\n+      try { files = JSON.parse(fs.readFileSync('.visor-agent-files.json','utf8')) || []; } catch {}\\n+      // Destination path (single co-located file)\\n+      const targetPath = process.env.VISOR_AGENT_PATH ? path.resolve(process.env.VISOR_AGENT_PATH) : path.resolve(String(process.env.VISOR_AGENT_OUT_DIR||'.'), 'agent.yaml');\\n+      const ensureObj = (v) => (v && typeof v === 'object' ? v : {});\\n+      const loadYaml = (t) => { try { return ensureObj(yaml.load(String(t||''))); } catch { return {}; } };\\n+      let configDoc = {};\\n+      let testsDoc = {};\\n+      for (const f of files) {\\n+        const doc = loadYaml(f.content);\\n+        if (doc && typeof doc === 'object') {\\n+          const p = String((f && f.path) || '');\\n+          const looksLikeTests = /\\\\.tests\\\\.ya?ml$/i.test(p) || ('cases' in doc) || ('defaults' in doc);\\n+          if (doc.tests && typeof doc.tests === 'object') {\\n+            testsDoc = doc;\\n+          } else if (looksLikeTests && Object.keys(testsDoc).length === 0) {\\n+            // Treat as tests source even without top-level 'tests'\\n+            testsDoc = doc;\\n+          } else if (!configDoc || Object.keys(configDoc).length === 0) {\\n+            configDoc = doc;\\n+          }\\n+        }\\n+      }\\n+      const finalDoc = {};\\n+      const cfg = ensureObj(configDoc);\\n+      // Copy all non-tests top-level keys from config\\n+      for (const k of Object.keys(cfg)) { if (k !== 'tests' && k !== 'extends') finalDoc[k] = cfg[k]; }\\n+      if (!finalDoc.version) finalDoc.version = '1.0';\\n+      // If no steps provided in config, create a minimal hello step as a sane fallback\\n+      if (!finalDoc.steps || typeof finalDoc.steps !== 'object' || Object.keys(finalDoc.steps).length === 0) {\\n+        finalDoc.steps = {\\n+          hello: { type: 'log', message: 'hello from agent-builder', level: 'info', on: ['issue_opened'] }\\n+        };\\n+      }\\n+      // Merge tests:\\n+      // - If testsDoc.tests is an object ‚Üí use it (valid path)\\n+      // - Else if cfg.tests is an object ‚Üí use it\\n+      // - Else if a tests file exists but is malformed ‚Üí preserve AS-IS (to let tests-validate fail)\\n+      // - Else (no tests provided at all) ‚Üí create a minimal valid tests block\\n+      let testsBlock = undefined;\\n+      if (testsDoc && typeof testsDoc.tests === 'object') {\\n+        testsBlock = testsDoc.tests;\\n+      } else if (cfg.tests && typeof cfg.tests === 'object') {\\n+        testsBlock = cfg.tests;\\n+      } else if (testsDoc && Object.keys(testsDoc).length > 0) {\\n+        // Preserve malformed shape to trigger validator failure in the first pass\\n+        // Examples: testsDoc = { version, extends, cases: [...] } (no top-level tests)\\n+        testsBlock = testsDoc as any;\\n+      } else {\\n+        testsBlock = {\\n+          defaults: { strict: true, ai_provider: 'mock' },\\n+          cases: [ { name: 'smoke', event: 'issue_opened', fixture: 'gh.issue_open.minimal', expect: {} } ],\\n+        };\\n+      }\\n+      finalDoc.tests = testsBlock;\\n+      // Write one file only\\n+      fs.mkdirSync(path.dirname(targetPath), { recursive: true });\\n+      fs.writeFileSync(targetPath, yaml.dump(finalDoc), 'utf8');\\n+      console.log(JSON.stringify({ written: [targetPath], slug: (cfg.slug || 'agent'), config_path: targetPath, tests_path: targetPath, mode: '{{ outputs['detect-mode'].mode | default: 'new' }}' }));\\n+      NODE\\n+    output_format: json\\n+\\n+  config-lint:\\n+    type: command\\n+    group: agent-quality\\n+    depends_on: [write]\\n+    on: [issue_opened, schedule]\\n+    if: \\\"((outputs||{})['write'])\\\"\\n+    exec: |\\n+      node dist/index.js validate --config {{ outputs['write'].config_path }}\\n+    output_format: text\\n+    # rely on provider exit code / issues, avoid double-routing via fail_if\\n+    fail_if: \\\"false\\\"\\n+    on_fail:\\n+      goto: refine\\n+      goto_event: schedule\\n+\\n+  tests-validate:\\n+    type: command\\n+    group: agent-quality\\n+    depends_on: [write]\\n+    on: [issue_opened, schedule]\\n+    if: \\\"((outputs||{})['write'])\\\"\\n+    exec: |\\n+      node <<'NODE'\\n+      const fs=require('fs'); const path=require('path'); const yaml=require('js-yaml');\\n+      const p = path.resolve('{{ outputs['write'].tests_path }}');\\n+      let text=''; try { text = fs.readFileSync(p,'utf8'); } catch { console.log('read_error'); process.exit(2); }\\n+      let doc; try { doc = yaml.load(text) || {}; } catch { console.log('parse_error'); process.exit(3); }\\n+      if (typeof doc !== 'object' || !doc) { console.log('doc_type_error'); process.exit(4); }\\n+      const t = doc.tests;\\n+      if (!t || typeof t !== 'object') { console.log('tests_missing_or_not_object'); process.exit(5); }\\n+      const allowed = new Set(['defaults','fixtures','cases']);\\n+      for (const k of Object.keys(t)) if (!allowed.has(k)) { console.log('tests_has_additional_properties'); process.exit(6); }\\n+      if (!Array.isArray(t.cases)) { console.log('cases_missing'); process.exit(7); }\\n+      console.log('valid');\\n+      NODE\\n+    output_format: text\\n+    fail_if: \\\"output && output.trim() !== 'valid'\\\"\\n+    on_fail:\\n+      goto: refine\\n+      goto_event: schedule\\n+\\n+  verify-tests:\\n+    type: command\\n+    group: agent-quality\\n+    depends_on: [write, config-lint, tests-validate]\\n+    on: [issue_opened, schedule]\\n+    if: \\\"((outputs||{})['write']) && !(String(((outputs||{})['tests-validate']||'')).toLowerCase().includes('failed'))\\\"\\n+    exec: |\\n+      node dist/index.js test --config {{ outputs['write'].tests_path }} --json - --bail\\n+    output_format: json\\n+    fail_if: \\\"output && Number(output.failures||0) > 0\\\"\\n+    on_fail:\\n+      goto: refine\\n+      goto_event: schedule\\n+    env:\\n+      VISOR_TEST_SUMMARY_SILENT: \\\"true\\\"\\n+    # no on_success routing; dependents will be scheduled by the initial goto to 'write'\\n+\\n+  code-review:\\n+    type: ai\\n+    group: agent-quality\\n+    depends_on: [verify-tests]\\n+    on: [issue_opened, schedule]\\n+    if: \\\"((outputs||{})['write'])\\\"\\n+    ai_prompt_type: engineer\\n+    ai:\\n+      provider: mock\\n+      skip_code_context: true\\n+    schema:\\n+      type: object\\n+      properties:\\n+        ok: { type: 'boolean' }\\n+        feedback: { type: 'string' }\\n+        concerns: { type: 'array', items: { type: 'string' } }\\n+      required: [ok]\\n+    prompt: |\\n+      Validate the generated agent against the brief and its tests. Ensure it:\\n+      - Directly addresses the brief without over-complication.\\n+      - Includes tests that cover success and failure cases.\\n+\\n+      Brief:\\n+      {{ outputs['brief'] | default: 'Create a minimal agent that can scaffold other agents (config + tests).' }}\\n+\\n+      Tests validate (text):\\n+      {{ outputs['tests-validate'] }}\\n+\\n+      Tests run (JSON):\\n+      {{ outputs['verify-tests'] | json }}\\n+\\n+      Respond with JSON: { ok: boolean, feedback: string, concerns?: string[] }.\\n+    fail_if: \\\"output && output.ok === false\\\"\\n+    on_fail:\\n+      goto: refine\\n+    # no on_success routing; dependents will be scheduled by the initial goto to 'write'\\n+\\n+  refine:\\n+    type: ai\\n+    group: agent-build\\n+    reuse_ai_session: draft\\n+    session_mode: append\\n+    on: [schedule]\\n+    ai_prompt_type: engineer\\n+    schema:\\n+      type: object\\n+      properties:\\n+        files:\\n+          type: array\\n+          items:\\n+            type: object\\n+            properties: { path: { type: string }, content: { type: string } }\\n+            required: [path, content]\\n+      required: [files]\\n+    prompt: |\\n+      Refine the files so that lint and tests pass. Keep changes focused and minimal.\\n+\\n+      Brief:\\n+      {{ outputs['brief'] }}\\n+\\n+      Config validate (text):\\n+      {{ outputs['config-lint'] }}\\n+\\n+      Tests validate (text):\\n+      {{ outputs['tests-validate'] }}\\n+\\n+      Tests run (JSON):\\n+      {{ outputs['verify-tests'] | json }}\\n+    on_success:\\n+      goto: write\\n+\\n+  cleanup:\\n+    type: command\\n+    group: agent-quality\\n+    depends_on: [code-review]\\n+    on: [issue_opened, schedule]\\n+    if: \\\"((outputs||{})['write'])\\\"\\n+    exec: |\\n+      node <<'NODE'\\n+      const fs=require('fs');\\n+      const written = (process.env.WRITTEN && process.env.WRITTEN.split('\\\\n').filter(Boolean)) || [];\\n+      const force = String(process.env.VISOR_AGENT_CLEANUP_FORCE||'').toLowerCase()==='true';\\n+      const removed=[];\\n+      for (const f of written){ const isTmp = /(^|\\\\/)tmp\\\\//.test(f); if (isTmp || force) { try{ if(fs.existsSync(f)){ fs.unlinkSync(f); removed.push(f);} }catch{} } }\\n+      try{ if(fs.existsSync('.visor-agent-files.json')) fs.unlinkSync('.visor-agent-files.json'); }catch{}\\n+      console.log(JSON.stringify({ removed }));\\n+      NODE\\n+    output_format: json\\n+    # finish depends_on cleanup; it will run automatically in the same forward-run\\n+\\n+  finish:\\n+    type: log\\n+    group: agent-build\\n+    on: [issue_opened, schedule]\\n+    depends_on: [cleanup]\\n+    message: \\\"‚úÖ Agent build finished: files written, linted, tested, and reviewed.\\\"\\n+    level: info\\n+    on_success:\\n+      goto_js: |\\n+        const enabled = String(env.VISOR_CHAT_MODE||'').toLowerCase()==='true';\\n+        if (!enabled) return null;\\n+        const max = Number(env.VISOR_CHAT_MAX_LOOPS||1);\\n+        const count = (outputs_history['brief']||[]).length|0;\\n+        return count < max ? 'detect-mode' : null;\\n+      goto_event: manual\\n+\\n+tests:\\n+  defaults:\\n+    strict: true\\n+    ai_provider: mock\\n+    fail_on_unexpected_calls: true\\n+\\n+  cases:\\n+    - name: new-write-to-cwd\\n+      description: Generates to output/ (cwd) and cleans up forcibly.\\n+      event: issue_opened\\n+      fixture: gh.issue_open.minimal\\n+      mocks:\\n+        draft:\\n+          slug: helloagent\\n+          files:\\n+            - path: helloagent.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                output:\\n+                  pr_comment:\\n+                    format: markdown\\n+                    group_by: check\\n+                    collapse: true\\n+                steps:\\n+                  hello:\\n+                    type: log\\n+                    message: \\\"hi\\\"\\n+                    level: info\\n+                    on: [issue_opened]\\n+            - path: helloagent.tests.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                extends: \\\"helloagent.yaml\\\"\\n+                tests:\\n+                  defaults: { strict: true, ai_provider: mock }\\n+                  cases:\\n+                    - name: ok\\n+                      event: issue_opened\\n+                      fixture: gh.issue_open.minimal\\n+                      expect:\\n+                        calls:\\n+                          - step: hello\\n+                            exactly: 1\\n+        verify-tests:\\n+          failures: 0\\n+          results:\\n+            - name: ok\\n+              passed: true\\n+        code-review:\\n+          ok: true\\n+          feedback: \\\"LGTM\\\"\\n+        cleanup:\\n+          removed: []\\n+      expect:\\n+        calls:\\n+          - step: start\\n+            exactly: 1\\n+          - step: detect-mode\\n+            exactly: 1\\n+          - step: draft\\n+            exactly: 1\\n+          - step: write\\n+            exactly: 1\\n+          - step: config-lint\\n+            exactly: 1\\n+          - step: tests-validate\\n+            exactly: 1\\n+          - step: verify-tests\\n+            exactly: 1\\n+          - step: code-review\\n+            exactly: 1\\n+          - step: cleanup\\n+            exactly: 1\\n+          - step: finish\\n+            exactly: 1\\n+        prompts:\\n+          - step: draft\\n+            index: last\\n+            not_contains:\\n+              - \\\"<pull_request>\\\"\\n+              - \\\"<files_summary>\\\"\\n+      env:\\n+        VISOR_AGENT_OUT_DIR: \\\"output\\\"\\n+        VISOR_AGENT_CLEANUP_FORCE: \\\"true\\\"\\n+\\n+    - name: improve-existing\\n+      description: Improves an existing config in-place using load-existing + refine.\\n+      event: issue_opened\\n+      fixture: gh.issue_open.minimal\\n+      mocks:\\n+        detect-mode:\\n+          path: \\\"tmp/improv.yaml\\\"\\n+          exists: true\\n+          empty: false\\n+          mode: \\\"improve\\\"\\n+        load-existing:\\n+          ok: true\\n+          slug: improv\\n+          config_path: tmp/improv.yaml\\n+          tests_path: tmp/improv.tests.yaml\\n+          files:\\n+            - path: tmp/improv.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                output:\\n+                  pr_comment:\\n+                    format: markdown\\n+                    group_by: check\\n+                    collapse: true\\n+                steps:\\n+                  hello:\\n+                    type: log\\n+                    message: \\\"hello\\\"\\n+                    level: info\\n+                    on: [issue_opened]\\n+            - path: tmp/improv.tests.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                extends: \\\"improv.yaml\\\"\\n+                tests:\\n+                  defaults: { strict: true, ai_provider: mock }\\n+                  cases:\\n+                    - name: failing\\n+                      event: issue_opened\\n+                      fixture: gh.issue_open.minimal\\n+                      expect:\\n+                        calls:\\n+                          - step: hello\\n+                            exactly: 2\\n+        refine:\\n+          files:\\n+            - path: tmp/improv.tests.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                extends: \\\"improv.yaml\\\"\\n+                tests:\\n+                  defaults: { strict: true, ai_provider: mock }\\n+                  cases:\\n+                    - name: fixed\\n+                      event: issue_opened\\n+                      fixture: gh.issue_open.minimal\\n+                      expect:\\n+                        calls:\\n+                          - step: hello\\n+                            exactly: 1\\n+        verify-tests:\\n+          failures: 0\\n+          results:\\n+            - name: fixed\\n+              passed: true\\n+        code-review:\\n+          ok: true\\n+          feedback: \\\"Refined existing\\\"\\n+      expect:\\n+        calls:\\n+          - step: start\\n+            exactly: 1\\n+          - step: detect-mode\\n+            exactly: 1\\n+          - step: load-existing\\n+            exactly: 1\\n+          - step: write\\n+            exactly: 1\\n+          - step: config-lint\\n+            exactly: 1\\n+          - step: tests-validate\\n+            exactly: 1\\n+          - step: verify-tests\\n+            exactly: 1\\n+          - step: code-review\\n+            exactly: 1\\n+          - step: cleanup\\n+            exactly: 1\\n+          - step: finish\\n+            exactly: 1\\n+      env:\\n+        VISOR_AGENT_MODE: \\\"improve\\\"\\n+        VISOR_AGENT_OUT_DIR: \\\"tmp\\\"\\n+        VISOR_AGENT_PATH: \\\"tmp/improv.yaml\\\"\\n+        VISOR_AGENT_TESTS_PATH: \\\"tmp/improv.tests.yaml\\\"\\n+\\n+    - name: loop-on-tests-validate\\n+      description: tests-validate fails first, refine fixes, loop passes.\\n+      event: issue_opened\\n+      fixture: gh.issue_open.minimal\\n+      mocks:\\n+        draft:\\n+          slug: tval\\n+          files:\\n+            - path: tval.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                output:\\n+                  pr_comment:\\n+                    format: markdown\\n+                    group_by: check\\n+                    collapse: true\\n+                steps:\\n+                  hello:\\n+                    type: log\\n+                    message: \\\"hi\\\"\\n+                    level: info\\n+                    on: [issue_opened]\\n+            - path: tval.tests.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                extends: \\\"tval.yaml\\\"\\n+                # wrong shape on purpose\\n+                cases:\\n+                  - name: wrong-shape\\n+        refine:\\n+          files:\\n+            - path: tval.tests.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                extends: \\\"tval.yaml\\\"\\n+                tests:\\n+                  defaults: { strict: true, ai_provider: mock }\\n+                  cases:\\n+                    - name: ok\\n+                      event: issue_opened\\n+                      fixture: gh.issue_open.minimal\\n+                      expect: {}\\n+        verify-tests:\\n+          failures: 0\\n+          results:\\n+            - name: ok\\n+              passed: true\\n+        code-review:\\n+          ok: true\\n+          feedback: \\\"OK\\\"\\n+      expect:\\n+        calls:\\n+          - step: start\\n+            exactly: 1\\n+          - step: detect-mode\\n+            exactly: 1\\n+          - step: draft\\n+            exactly: 1\\n+          - step: refine\\n+            exactly: 1\\n+          - step: write\\n+            exactly: 2\\n+          - step: config-lint\\n+            exactly: 2\\n+          - step: tests-validate\\n+            exactly: 2\\n+          - step: verify-tests\\n+            exactly: 1\\n+          - step: code-review\\n+            exactly: 1\\n+          - step: cleanup\\n+            exactly: 1\\n+          - step: finish\\n+            exactly: 1\\n+      env:\\n+        VISOR_AGENT_OUT_DIR: \\\"tmp\\\"\\n+\\n+    - name: chat-loop-manual\\n+      description: Chat loop returns to brief once, then stops.\\n+      event: manual\\n+      fixture: local.minimal\\n+      mocks:\\n+        detect-mode:\\n+          path: \\\"tmp/nonexistent.yaml\\\"\\n+          exists: false\\n+          empty: true\\n+          mode: \\\"new\\\"\\n+        brief[]:\\n+          - \\\"first brief\\\"\\n+          - \\\"second brief\\\"\\n+        draft:\\n+          slug: chatg\\n+          files:\\n+            - path: chatg.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                output:\\n+                  pr_comment:\\n+                    format: markdown\\n+                    group_by: check\\n+                    collapse: true\\n+                steps:\\n+                  hello:\\n+                    type: log\\n+                    message: \\\"hi\\\"\\n+                    level: info\\n+                    on: [manual, issue_opened]\\n+            - path: chatg.tests.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                extends: \\\"chatg.yaml\\\"\\n+                tests:\\n+                  defaults: { strict: true, ai_provider: mock }\\n+                  cases:\\n+                    - name: ok\\n+                      event: manual\\n+                      fixture: local.minimal\\n+                      expect:\\n+                        calls:\\n+                          - step: hello\\n+                            exactly: 1\\n+        verify-tests:\\n+          failures: 0\\n+          results:\\n+            - name: ok\\n+              passed: true\\n+        code-review:\\n+          ok: true\\n+          feedback: \\\"LGTM\\\"\\n+      expect:\\n+        calls:\\n+          - step: brief\\n+            at_least: 1\\n+          - step: detect-mode\\n+            at_least: 1\\n+          - step: draft\\n+            at_least: 1\\n+          - step: write\\n+            at_least: 1\\n+          - step: config-lint\\n+            at_least: 1\\n+          - step: tests-validate\\n+            at_least: 1\\n+          - step: verify-tests\\n+            at_least: 1\\n+          - step: code-review\\n+            at_least: 1\\n+          - step: cleanup\\n+            at_least: 1\\n+          - step: finish\\n+            at_least: 1\\n+      env:\\n+        VISOR_AGENT_OUT_DIR: \\\"tmp\\\"\\n+        VISOR_AGENT_PATH: \\\"tmp/nonexistent.yaml\\\"\\n+        VISOR_CHAT_MODE: \\\"true\\\"\\n+        VISOR_CHAT_MAX_LOOPS: \\\"1\\\"\\n+        VISOR_TEST_MODE: \\\"true\\\"\\n+\\n+    - name: prompt-never-includes-context-even-when-allowed\\n+      description: Even if tests allow code context, draft disables it at step level.\\n+      event: manual\\n+      fixture: local.minimal\\n+      expect:\\n+        calls:\\n+          - step: brief\\n+            exactly: 1\\n+          - step: draft\\n+            exactly: 1\\n+        prompts:\\n+          - step: draft\\n+            index: last\\n+            not_contains:\\n+              - \\\"<pull_request>\\\"\\n+              - \\\"<files_summary>\\\"\\n+      env:\\n+        VISOR_TEST_ALLOW_CODE_CONTEXT: \\\"true\\\"\\n\",\"status\":\"added\"},{\"filename\":\"defaults/code-review.yaml\",\"additions\":8,\"deletions\":0,\"changes\":275,\"patch\":\"diff --git a/defaults/code-review.yaml b/defaults/code-review.yaml\\nnew file mode 100644\\nindex 00000000..4d2811d8\\n--- /dev/null\\n+++ b/defaults/code-review.yaml\\n@@ -0,0 +1,275 @@\\n+version: \\\"1.0\\\"\\n+\\n+# Extracted code review steps for reuse and composition\\n+outputs:\\n+  - name: issues\\n+    description: Aggregated issues from review steps\\n+    value_js: |\\n+      const values = Object.values(outputs || {});\\n+      const all = [];\\n+      for (const v of values) {\\n+        const arr = (v && Array.isArray(v.issues)) ? v.issues : [];\\n+        if (arr.length) all.push(...arr);\\n+      }\\n+      return all;\\n+\\n+  - name: hasErrors\\n+    description: True if any critical or error issues exist\\n+    value_js: |\\n+      for (const v of Object.values(outputs || {})) {\\n+        const arr = (v && Array.isArray(v.issues)) ? v.issues : [];\\n+        if (arr.some(i => i && (i.severity === 'critical' || i.severity === 'error'))) return true;\\n+      }\\n+      return false;\\n+\\n+\\n+steps:\\n+  # PR overview with intelligent analysis - runs first to establish context\\n+  overview:\\n+    type: ai\\n+    group: overview\\n+    on: [pr_opened, pr_updated]\\n+    prompt: |\\n+        PR Title: {{ pr.title }}\\n+\\n+        You are generating PR overview, to help owners of the repository to understand what this PR is above, and help reviewer to point to the right parts of the code. First you should provide detailed but concise description, mentioning all the changes.\\n+\\n+        ## Files Changed Analysis\\n+        After you need to summarize insights from `<files_summary>`: changed files, additions/deletions, notable patterns.\\n+\\n+        Next ensure you cover all below:\\n+\\n+        ## Architecture & Impact Assessment\\n+          - What this PR accomplishes\\n+          - Key technical changes introduced\\n+          - Affected system components\\n+          - Include one or more mermaid diagrams when useful to visualize component relationships or flow.\\n+          \\n+        ## Scope Discovery & Context Expansion\\n+        - From the `<files_summary>` and code diffs, infer the broader scope of impact across modules, services, and boundaries.\\n+        - If your environment supports code search/extract tools, use them to peek at immediately-related files (tests, configs, entrypoints) for better context. If tools are not available, infer and list what you would search next.\\n+\\n+        You may also be asked to assign labels to PR; if so use this:\\n+        - `tags.review-effort`: integer 1‚Äì5 estimating review effort (1=trivial, 5=very high).\\n+        - `tags.label`: one of [bug, chore, documentation, enhancement, feature]. Choose the best fit.\\n+\\n+        Important:\\n+        - Propose `tags.review-effort` and `tags.label` only for the initial PR open event.\\n+        - Do not change or re-suggest labels on PR update events; the repository applies labels only on `pr_opened`.\\n+\\n+        Be concise, specific, and actionable. Avoid praise or celebration.\\n+    schema: overview\\n+\\n+  # Security analysis - Critical for all projects\\n+  security:\\n+    type: ai\\n+    group: review\\n+    on: [pr_opened, pr_updated]\\n+    depends_on: [overview]\\n+    prompt: |\\n+        Based on our overview discussion, please perform a comprehensive security analysis of the code changes.\\n+\\n+        Analyze the files listed in the `<files_summary>` section and focus on the code changes shown in the diff sections.\\n+\\n+        ## Security Analysis Areas\\n+\\n+        **Input Validation & Injection:**\\n+        - SQL injection in database queries\\n+        - XSS vulnerabilities in user input handling\\n+        - Command injection in system calls\\n+        - Path traversal in file operations\\n+\\n+        **Authentication & Authorization:**\\n+        - Weak authentication mechanisms\\n+        - Session management flaws\\n+        - Access control bypasses\\n+        - Privilege escalation opportunities\\n+\\n+        **Data Protection:**\\n+        - Sensitive data exposure in logs/errors\\n+        - Unencrypted data storage\\n+        - API key or credential leaks\\n+        - Privacy regulation compliance\\n+\\n+        **Infrastructure Security:**\\n+        - Insecure configurations\\n+        - Missing security headers\\n+        - Vulnerable dependencies\\n+        - Resource exhaustion vulnerabilities\\n+\\n+        Provide specific findings with clear explanations and actionable remediation steps.\\n+\\n+        ## Severity Guidelines\\n+        Use the following severity levels appropriately:\\n+        - **critical**: Security vulnerabilities that could lead to immediate compromise (RCE, SQL injection, authentication bypass, exposed secrets)\\n+        - **error**: Security issues that must be fixed before production (XSS, path traversal, weak crypto, missing auth checks)\\n+        - **warning**: Security concerns that should be addressed (verbose errors, missing rate limiting, insecure defaults)\\n+        - **info**: Security best practices and hardening suggestions (defense in depth, additional validation)\\n+    schema: code-review\\n+\\n+  # Architecture analysis - Ensures sound design and avoids over-engineering\\n+  architecture:\\n+    type: ai\\n+    group: review\\n+    on: [pr_opened, pr_updated]\\n+    depends_on: [overview]\\n+    prompt: |\\n+        Building on our overview analysis, evaluate the architectural decisions and design patterns.\\n+\\n+        Focus on the files listed in `<files_summary>` and analyze the code changes shown in the `<full_diff>` or `<commit_diff>` sections.\\n+\\n+        ## Architecture Analysis Areas\\n+        **Design Simplicity & Pragmatism:**\\n+        - Over-engineering: Is the solution more complex than necessary?\\n+        - YAGNI violations: Are we building features not currently needed?\\n+        - Can this be implemented more simply using existing patterns?\\n+        - Are there existing functions/modules that could be reused instead?\\n+        - Is the abstraction level appropriate for the problem size?\\n+\\n+        **Special Cases & Edge Cases:**\\n+        - Does the code introduce special case handling that could be generalized?\\n+        - Are there hard-coded values or logic for specific scenarios?\\n+        - Can special cases be eliminated through better design?\\n+        - Are edge cases handled through general patterns rather than one-offs?\\n+\\n+        **Consistency & Reusability:**\\n+        - Does this follow existing architectural patterns in the codebase?\\n+        - Are there similar problems already solved differently elsewhere?\\n+        - Could this functionality be generalized to handle more cases?\\n+        - Does it duplicate existing functionality that could be extended?\\n+\\n+        **Design Patterns & Structure:**\\n+        - Are design patterns used appropriately (not forced)?\\n+        - Is the separation of concerns clear and logical?\\n+        - Are dependencies and coupling minimized?\\n+        - Is the code organized in a way that makes sense?\\n+\\n+        **Scalability & Extensibility:**\\n+        - Will this design scale with future requirements?\\n+        - Is it easy to extend without major refactoring?\\n+        - Are boundaries and interfaces well-defined?\\n+\\n+        Flag issues where simpler solutions exist, special cases are introduced unnecessarily, or existing functionality could be reused.\\n+\\n+        ## Severity Guidelines\\n+        Use the following severity levels appropriately:\\n+        - **critical**: Architectural decisions that will cause system failures or major technical debt (circular dependencies, tight coupling causing deadlocks)\\n+        - **error**: Significant architectural problems (over-engineering, unnecessary special cases, violation of core patterns, missed reuse opportunities)\\n+        - **warning**: Architectural concerns that should be addressed (minor over-abstraction, could be simplified, inconsistent patterns)\\n+        - **info**: Architectural suggestions and alternative approaches (simpler patterns available, potential for future reuse)\\n+    schema: code-review\\n+\\n+  # Performance analysis - Important for all applications\\n+  performance:\\n+    type: ai\\n+    group: review\\n+    on: [pr_opened, pr_updated]\\n+    depends_on: [overview]\\n+    prompt: |\\n+        Building on our overview analysis, now review the code changes for performance issues.\\n+\\n+        Focus on the files listed in `<files_summary>` and analyze the code changes shown in the `<full_diff>` or `<commit_diff>` sections.\\n+\\n+        ## Performance Analysis Areas\\n+        **Algorithm & Data Structure Efficiency:**\\n+        - Time complexity analysis (O(n), O(n¬≤), etc.)\\n+        - Space complexity and memory usage\\n+        - Inefficient loops and nested operations\\n+        - Suboptimal data structure choices\\n+\\n+        **Database Performance:**\\n+        - N+1 query problems\\n+        - Missing database indexes\\n+        - Inefficient JOIN operations\\n+        - Large result set retrievals\\n+\\n+        **Resource Management:**\\n+        - Memory leaks and excessive allocations\\n+        - File handle management\\n+        - Connection pooling issues\\n+        - Resource cleanup patterns\\n+\\n+        **Async & Concurrency:**\\n+        - Blocking operations in async contexts\\n+        - Race conditions and deadlocks\\n+        - Inefficient parallel processing\\n+\\n+        Building on our overview analysis, identify performance issues and provide optimization recommendations.\\n+\\n+        ## Severity Guidelines\\n+        Use the following severity levels appropriately:\\n+        - **critical**: Performance issues causing system failure or severe degradation (infinite loops, memory leaks causing OOM)\\n+        - **error**: Significant performance problems affecting user experience (O(n¬≤) in critical path, N+1 queries, blocking I/O)\\n+        - **warning**: Performance concerns that should be optimized (inefficient algorithms, missing indexes, unnecessary operations)\\n+        - **info**: Performance best practices and optimization opportunities (caching suggestions, async improvements)\\n+    schema: code-review\\n+\\n+  # Code quality and maintainability\\n+  quality:\\n+    type: ai\\n+    group: review\\n+    on: [pr_opened, pr_updated]\\n+    depends_on: [overview]\\n+    prompt: |\\n+        Building on our overview discussion, evaluate the code quality and maintainability.\\n+\\n+        Review the code changes shown in the `<full_diff>` or `<commit_diff>` sections, considering the files listed in `<files_summary>`.\\n+\\n+        ## Quality Assessment Areas\\n+        **Code Structure & Design:**\\n+        - SOLID principles adherence\\n+        - Design pattern appropriateness\\n+        - Separation of concerns\\n+        - Code organization and clarity\\n+\\n+        **Error Handling & Reliability:**\\n+        - Exception handling completeness\\n+        - Error propagation patterns\\n+        - Input validation thoroughness\\n+        - Edge case coverage\\n+\\n+        **Testing & Test Coverage:**\\n+        - Missing tests for critical functionality\\n+        - Test coverage gaps\\n+        - Test quality and effectiveness\\n+        - Edge cases and error scenarios coverage\\n+\\n+        **Test Quality - Critical Checks:**\\n+        - **Magic Numbers in Tests**: Are tests using hard-coded values just to make assertions pass?\\n+          - Look for arbitrary numbers without clear meaning (e.g., `expect(result).toBe(42)` where 42 has no semantic meaning)\\n+          - Tests should use meaningful constants or derive expected values from the input\\n+          - Tests that \\\"cut corners\\\" by adjusting expected values to match output are fragile\\n+          - Flag tests where the expected value seems arbitrary or reverse-engineered from implementation\\n+        - **Test Integrity**: Do tests actually validate correct behavior or just check current behavior?\\n+          - Are tests meaningful and would catch real bugs?\\n+          - Do tests explain why a value should be expected (through variable names, comments, or clear logic)?\\n+          - Are test assertions based on requirements rather than implementation details?\\n+        - **Negative Testing Coverage**: Are failure cases and error conditions properly tested?\\n+          - Are there tests for invalid inputs, boundary conditions, and error states?\\n+          - Do tests verify error messages and error handling paths?\\n+          - Are edge cases like null, empty, undefined, zero, negative values tested?\\n+          - Do tests cover \\\"unhappy path\\\" scenarios (authentication failures, network errors, invalid data)?\\n+          - Are exception handling and error recovery mechanisms validated?\\n+          - Flag missing negative tests for critical functionality\\n+\\n+        **Maintainability:**\\n+        - Code testability issues\\n+        - Dependencies and coupling problems\\n+        - Technical debt introduction\\n+        - Code duplication (DRY violations)\\n+\\n+        **Language-Specific Best Practices:**\\n+        - Idiomatic code usage\\n+        - Framework/library best practices\\n+        - Type safety (if applicable)\\n+\\n+        Focus on actionable improvements that enhance code maintainability based on the overview analysis.\\n+        Pay special attention to test quality - tests that use magic numbers or cut corners undermine reliability.\\n+\\n+        ## Severity Guidelines\\n+        Use the following severity levels appropriately:\\n+        - **critical**: Code quality issues that will cause bugs or failures (logic errors, race conditions, null pointer issues)\\n+        - **error**: Quality problems that significantly impact maintainability (no error handling, high complexity, severe coupling, tests with magic numbers)\\n+        - **warning**: Quality concerns that should be addressed (missing tests, code duplication, poor naming, unclear test expectations)\\n+        - **info**: Best practices and improvement suggestions (refactoring opportunities, documentation improvements, test clarity)\\n+    schema: code-review\\n\",\"status\":\"added\"},{\"filename\":\"defaults/task-refinement.yaml\",\"additions\":18,\"deletions\":0,\"changes\":621,\"patch\":\"diff --git a/defaults/task-refinement.yaml b/defaults/task-refinement.yaml\\nnew file mode 100644\\nindex 00000000..a87d402b\\n--- /dev/null\\n+++ b/defaults/task-refinement.yaml\\n@@ -0,0 +1,621 @@\\n+version: \\\"1.0\\\"\\n+\\n+# Simple agent: task-refinement\\n+# - Collects user input, refines it with AI (skip code context), and loops until refined.\\n+# - Returns final refined text via the `finish` step output.\\n+\\n+steps:\\n+  ask:\\n+    type: human-input\\n+    group: task-refinement\\n+    if: \\\"!(outputs && outputs['ask'])\\\"\\n+    # No explicit event trigger; run in CLI by default but guard via if\\n+    prompt: |\\n+      {% assign last_refine = outputs_history.refine | last %}\\n+      {% if last_refine and last_refine.refined == false %}\\n+      {{ last_refine.text }}\\n+      {% else %}\\n+      Provide the task you want to accomplish. Be specific about constraints\\n+      (inputs, outputs, environment, success criteria).\\n+      {% endif %}\\n+    multiline: false\\n+    allow_empty: false\\n+    # No on_success.goto required ‚Äî refine depends_on ask\\n+\\n+  refine:\\n+    type: ai\\n+    group: task-refinement\\n+    # Run only after 'ask' ‚Äî dependency drives ordering\\n+    depends_on: [ask]\\n+    # Execute once per run; forward-run loops do not need to re-run refine\\n+    if: \\\"!(outputs && outputs['refine'])\\\"\\n+    ai:\\n+      skip_code_context: true\\n+      disableTools: true\\n+      system_prompt: |\\n+        You are a helpful, precise task refinement assistant (role: requirements-analyst).\\n+        Your goal is to get to an agreed, testable task definition and clear acceptance criteria.\\n+        - Refine the user's task into an unambiguous, executable description with minimal assumptions.\\n+        - Define how correctness will be validated (objective success criteria and measurables).\\n+        - If information is missing, set refined=false and ask_user=true and put a single, specific\\n+          clarification question in the \\\"text\\\" field (one question at a time).\\n+        - If everything is sufficient, set refined=true and put the final refined wording in \\\"text\\\".\\n+        - Be succinct and concrete. Prefer measurable outcomes over vague phrasing.\\n+        - You can't mark plan as refined until explicit user confirmation/agreement is implied.\\n+    # Schema ensures the agent either finalizes or asks to clarify\\n+    schema:\\n+      type: object\\n+      additionalProperties: false\\n+      properties:\\n+        refined: { type: boolean, description: \\\"true if the task is fully specified and accepted\\\" }\\n+        text: { type: string, description: \\\"final refined task or question to user\\\" }\\n+      required: [refined, text]\\n+    prompt: |\\n+      <history>\\n+        {% assign umsgs = outputs_history.ask | default: [] %}\\n+        {% assign amsgs = outputs_history.refine | default: [] %}\\n+        {% assign merged = umsgs | concat: amsgs | sort: 'ts' %}\\n+        {% for m in merged %}\\n+          {% if m.refined != nil %}\\n+            <assistant>{{ m.text }}</assistant>\\n+          {% else %}\\n+            <user>{{ m.text }}</user>\\n+          {% endif %}\\n+        {% endfor %}\\n+      </history>\\n+\\n+      <input>\\n+        {{ outputs['ask'].text }}\\n+      </input>\\n+\\n+    # Loop control using fail_if + on_fail (no goto_js)\\n+    fail_if: \\\"output['refined'] !== true\\\"\\n+    on_fail:\\n+      goto: ask\\n+\\n+  plan-commands:\\n+    type: ai\\n+    group: task-refinement\\n+    depends_on: [refine]\\n+    reuse_ai_session: refine\\n+    session_mode: append\\n+    ai:\\n+      # Allow tools so the model can inspect the repo context if needed\\n+      disableTools: false\\n+      skip_code_context: true\\n+      system_prompt: |\\n+        You are a Task Validation Planner.\\n+        Produce a deterministic, minimal list of shell commands that, when executed in order,\\n+        validate that the refined task is complete for THIS repository/local env.\\n+        Constraints:\\n+        - Commands must be safe and non-destructive. Do not delete files or push to remotes.\\n+        - Prefer read-only checks and standard project commands (build, lint, test) when present.\\n+        - Each item is a single shell line; you may use && or || inside a line if necessary.\\n+        - Favor idempotent commands. Avoid interactive prompts.\\n+        - Detect package manager/tooling pragmatically (npm/pnpm/yarn/bun, eslint/biome, jest/vitest, etc.).\\n+        Return JSON with an array of strings under \\\"commands\\\" and optional \\\"notes\\\".\\n+    schema:\\n+      type: object\\n+      additionalProperties: false\\n+      properties:\\n+        commands:\\n+          type: array\\n+          minItems: 1\\n+          items: { type: string, minLength: 1 }\\n+        notes: { type: string }\\n+      required: [commands]\\n+    prompt: |\\n+      Refined task:\\n+      {{ outputs['refine'].text }}\\n+\\n+      If the repository has build/lint/test, include them. Otherwise propose basic checks that still\\n+      demonstrate completion (e.g., typecheck, compile, format verification, smoke run).\\n+\\n+      {% assign prev_conf_hist = outputs_history['confirm-interpret'] | default: [] %}\\n+      {% assign prev_run_hist = outputs_history['run-commands'] | default: [] %}\\n+      {% assign last_failed = nil %}\\n+      {% for r in prev_run_hist %}\\n+        {% if r.failed and r.failed > 0 %}\\n+          {% assign last_failed = r %}\\n+        {% endif %}\\n+      {% endfor %}\\n+      {% assign last_conf = prev_conf_hist | last %}\\n+      {% assign prev_run_count = prev_run_hist | size %}\\n+      {% assign last_run = prev_run_hist | last %}\\n+      {% if last_failed or prev_run_count > 0 %}\\n+      Previous attempt failed. Here are the details to learn from:\\n+      - Previous commands: {{ last_conf.commands | to_json }}\\n+      - Run results: {{ last_failed | default: last_run | to_json }}\\n+      Please revise the commands to address failures. Keep the list minimal and deterministic.\\n+      {% endif %}\\n+\\n+      Output strictly JSON per schema. No prose around it.\\n+    # No on_success routing needed; dependents naturally follow in the DAG\\n+\\n+  ask-confirm:\\n+    type: human-input\\n+    group: task-refinement\\n+    depends_on: [plan-commands]\\n+    prompt: |\\n+      Here is the proposed validation command list (to run sequentially):\\n+      {% for c in outputs['plan-commands'].commands %}\\n+      {{ forloop.index }}. {{ c }}\\n+      {% endfor %}\\n+\\n+      Confirm running these? Reply \\\"yes\\\" to accept, or provide edits:\\n+      - JSON array of commands, e.g. [\\\"npm ci\\\", \\\"npm test\\\"], or\\n+      - Plain text with one command per line.\\n+    placeholder: \\\"yes | or paste modified list...\\\"\\n+    multiline: true\\n+    allow_empty: true\\n+    default: \\\"yes\\\"\\n+\\n+  confirm-interpret:\\n+    type: ai\\n+    group: task-refinement\\n+    depends_on: [plan-commands, ask-confirm]\\n+    ai:\\n+      skip_code_context: true\\n+      disableTools: true\\n+      system_prompt: |\\n+        You are a confirmation interpreter. Given the planned command list and the user's reply,\\n+        decide whether to proceed to execution with a normalized list of shell commands, or\\n+        return for replanning.\\n+        - If the user says yes/approve, set proceed=true and provide commands as-is.\\n+        - If the user supplied edits (JSON array or one per line), parse, trim, dedupe, and set proceed=true with commands.\\n+        - If the reply indicates high-level changes (not runnable commands), set proceed=false and include a short reason.\\n+        Output strictly JSON per schema.\\n+    schema:\\n+      type: object\\n+      additionalProperties: false\\n+      properties:\\n+        proceed: { type: boolean }\\n+        commands:\\n+          type: array\\n+          items: { type: string, minLength: 1 }\\n+        reason: { type: string }\\n+      required: [proceed]\\n+    prompt: |\\n+      Planned commands:\\n+      {{ outputs['plan-commands'].commands | to_json }}\\n+\\n+      User reply:\\n+      {{ outputs['ask-confirm'].text }}\\n+\\n+      Return JSON per schema. If proceed=true, commands must be a non-empty array of shell lines.\\n+    fail_if: \\\"output && output.proceed !== true\\\"\\n+    on_fail:\\n+      goto: plan-commands\\n+\\n+  run-commands:\\n+    type: command\\n+    criticality: internal\\n+    group: task-refinement\\n+    depends_on: [confirm-interpret]\\n+    assume:\\n+      - \\\"outputs['confirm-interpret']?.proceed === true\\\"\\n+      - \\\"(outputs['confirm-interpret']?.commands?.length ?? 0) > 0\\\"\\n+    exec: |\\n+      node <<'NODE'\\n+      const { spawn } = require('child_process');\\n+      const cmds = {{ outputs['confirm-interpret'] | to_json }}.commands || [];\\n+      const results = [];\\n+      const runOne = (cmd) => new Promise((resolve) => {\\n+        const child = spawn('bash', ['-lc', cmd], { stdio: ['ignore', 'pipe', 'pipe'] });\\n+        let out = '', err = '';\\n+        const started = Date.now();\\n+        child.stdout.on('data', d => (out += d.toString()));\\n+        child.stderr.on('data', d => (err += d.toString()));\\n+        child.on('close', code => {\\n+          results.push({ cmd, code, stdout: out, stderr: err, durationMs: Date.now() - started });\\n+          resolve();\\n+        });\\n+      });\\n+      (async () => {\\n+        for (const c of cmds) { await runOne(c); }\\n+        const failed = results.filter(r => Number(r.code||0) !== 0).length;\\n+        process.stdout.write(JSON.stringify({ failed, results }));\\n+      })().catch(e => { process.stdout.write(JSON.stringify({ failed: 1, error: String(e) })); process.exit(0); });\\n+      NODE\\n+    output_format: json\\n+    schema:\\n+      type: object\\n+      additionalProperties: true\\n+      properties:\\n+        failed: { type: number }\\n+        results:\\n+          type: array\\n+          items:\\n+            type: object\\n+            additionalProperties: true\\n+            properties:\\n+              cmd: { type: string }\\n+              code: {}\\n+              stdout: { type: string }\\n+              stderr: { type: string }\\n+              durationMs: { type: number }\\n+            required: [cmd, code]\\n+      required: [failed]\\n+    fail_if: \\\"output && Number(output.failed||0) > 0\\\"\\n+    on_fail:\\n+      goto: plan-commands\\n+\\n+  finish:\\n+    type: log\\n+    group: task-refinement\\n+    depends_on: [run-commands]\\n+    if: \\\"(outputs && outputs['run-commands'] && Number((outputs['run-commands'].failed||0)) === 0) && !(outputs && outputs['finish'])\\\"\\n+    message: |\\n+      ‚úÖ Refined Task:\\n+      {{ outputs['refine'].text }}\\n+\\n+      ‚úÖ Validation commands (final):\\n+      {% for c in outputs['confirm-interpret'].commands %}\\n+      - {{ c }}\\n+      {% endfor %}\\n+    level: info\\n+    include_pr_context: false\\n+    include_dependencies: false\\n+    include_metadata: false\\n+\\n+tests:\\n+  defaults:\\n+    strict: true\\n+    ai_provider: mock\\n+  cases:\\n+    - name: one-pass-refinement\\n+      description: Single turn; AI is happy and returns refined text.\\n+      event: manual\\n+      fixture: local.minimal\\n+      mocks:\\n+        ask: \\\"Build a small Node CLI that prints \\\\\\\"hello\\\\\\\"\\\"\\n+        refine:\\n+          refined: true\\n+          text: \\\"Create a Node.js CLI (using Node >=18) that prints 'hello' when run; include usage example and exit code 0.\\\"\\n+        plan-commands:\\n+          commands: [\\\"echo hello-build\\\", \\\"echo hello-lint\\\", \\\"echo hello-test\\\"]\\n+        ask-confirm: \\\"yes\\\"\\n+        confirm-interpret:\\n+          proceed: true\\n+          commands: [\\\"echo hello-build\\\",\\\"echo hello-lint\\\",\\\"echo hello-test\\\"]\\n+        run-commands:\\n+          stdout: '{\\\"failed\\\":0,\\\"results\\\":[{\\\"cmd\\\":\\\"echo hello-build\\\",\\\"code\\\":0},{\\\"cmd\\\":\\\"echo hello-lint\\\",\\\"code\\\":0},{\\\"cmd\\\":\\\"echo hello-test\\\",\\\"code\\\":0}]}'\\n+      expect:\\n+        calls:\\n+          - step: plan-commands\\n+            exactly: 1\\n+          - step: ask-confirm\\n+            exactly: 1\\n+          - step: confirm-interpret\\n+            exactly: 1\\n+          - step: run-commands\\n+            exactly: 1\\n+          - step: ask\\n+            exactly: 1\\n+          - step: refine\\n+            exactly: 1\\n+          - step: finish\\n+            exactly: 1\\n+        outputs:\\n+          - step: run-commands\\n+            path: failed\\n+            equals: 0\\n+\\n+    - name: multi-turn-refinement-loop\\n+      description: Two clarifying turns followed by a final refinement; manual-only chat.\\n+      event: manual\\n+      fixture: local.minimal\\n+      mocks:\\n+        ask[]:\\n+          - \\\"Create a CI job\\\"\\n+          - \\\"GitHub Actions; run on push to main\\\"\\n+          - \\\"Use Node 18 and npm ci + npm test\\\"\\n+        refine[]:\\n+          - { refined: false, ask_user: true, text: \\\"Which CI platform and trigger conditions?\\\" }\\n+          - { refined: false, ask_user: true, text: \\\"What Node version and commands should run?\\\" }\\n+          - { refined: true, text: \\\"Set up GitHub Actions workflow: on push to main, use Node 18.x, cache npm, run npm ci && npm test.\\\" }\\n+        plan-commands:\\n+          commands: [\\\"echo build\\\", \\\"echo lint\\\", \\\"echo test\\\"]\\n+        ask-confirm: \\\"yes\\\"\\n+        confirm-interpret:\\n+          proceed: true\\n+          commands: [\\\"echo build\\\",\\\"echo lint\\\",\\\"echo test\\\"]\\n+        run-commands:\\n+          stdout: '{\\\"failed\\\":0,\\\"results\\\":[{\\\"cmd\\\":\\\"echo build\\\",\\\"code\\\":0},{\\\"cmd\\\":\\\"echo lint\\\",\\\"code\\\":0},{\\\"cmd\\\":\\\"echo test\\\",\\\"code\\\":0}]}'\\n+      expect:\\n+        calls:\\n+          - step: ask\\n+            exactly: 3\\n+          - step: refine\\n+            exactly: 3\\n+          - step: plan-commands\\n+            exactly: 1\\n+          - step: ask-confirm\\n+            exactly: 1\\n+          - step: confirm-interpret\\n+            exactly: 1\\n+          - step: run-commands\\n+            exactly: 1\\n+          - step: finish\\n+            exactly: 1\\n+        prompts:\\n+          # Ask prompt should surface the last refine clarification\\n+          - step: ask\\n+            index: 1\\n+            contains:\\n+              - \\\"Which CI platform and trigger conditions?\\\"\\n+          - step: ask\\n+            index: last\\n+            contains:\\n+              - \\\"What Node version and commands should run?\\\"\\n+          - step: refine\\n+            index: last\\n+            contains:\\n+              - \\\"Which CI platform and trigger conditions?\\\"\\n+              - \\\"Use Node 18 and npm ci + npm test\\\"\\n+              - \\\"What Node version and commands should run?\\\"\\n+          # Keep prompt assertions resilient to minor formatting changes by using 'contains'\\n+          # instead of a single large regex.\\n+        outputs:\\n+          # Ensure the final successful refinement carries a timestamp and expected text\\n+          - step: refine\\n+            where: { path: refined, equals: true }\\n+            path: ts\\n+            matches: \\\"^\\\\\\\\d{10,}$\\\"\\n+          - step: refine\\n+            where: { path: refined, equals: true }\\n+            path: text\\n+            matches: \\\"(?is).*GitHub Actions.*Node 18.*npm ci.*npm test.*\\\"\\n+          - step: run-commands\\n+            path: failed\\n+            equals: 0\\n+\\n+    - name: plan-and-run-success\\n+      description: Plans commands, user confirms, runs successfully, finishes.\\n+      event: manual\\n+      fixture: local.minimal\\n+      mocks:\\n+        ask: \\\"Add CI to run build, lint, and tests\\\"\\n+        refine:\\n+          refined: true\\n+          text: \\\"Ensure project builds, lints, and tests pass via CI\\\"\\n+        plan-commands:\\n+          commands: [\\\"echo build\\\", \\\"echo lint\\\", \\\"echo test\\\"]\\n+        ask-confirm: \\\"yes\\\"\\n+        confirm-interpret:\\n+          proceed: true\\n+          commands: [\\\"echo build\\\",\\\"echo lint\\\",\\\"echo test\\\"]\\n+        run-commands:\\n+          stdout: '{\\\"failed\\\":0,\\\"results\\\":[{\\\"cmd\\\":\\\"echo build\\\",\\\"code\\\":0},{\\\"cmd\\\":\\\"echo lint\\\",\\\"code\\\":0},{\\\"cmd\\\":\\\"echo test\\\",\\\"code\\\":0}]}'\\n+      expect:\\n+        calls:\\n+          - step: ask\\n+            exactly: 1\\n+          - step: refine\\n+            exactly: 1\\n+          - step: plan-commands\\n+            exactly: 1\\n+          - step: ask-confirm\\n+            exactly: 1\\n+          - step: confirm-interpret\\n+            exactly: 1\\n+          - step: run-commands\\n+            exactly: 1\\n+          - step: finish\\n+            exactly: 1\\n+        outputs:\\n+          - step: run-commands\\n+            path: failed\\n+            equals: 0\\n+\\n+    - name: plan-run-fail-then-refine\\n+      description: First run fails, planner adjusts, second run passes, finish.\\n+      event: manual\\n+      fixture: local.minimal\\n+      mocks:\\n+        ask: \\\"Verify app passes tests\\\"\\n+        refine:\\n+          refined: true\\n+          text: \\\"Run tests to verify app correctness\\\"\\n+        plan-commands[]:\\n+          - { commands: [\\\"node -e \\\\\\\"process.exit(1)\\\\\\\"\\\"] }\\n+          - { commands: [\\\"node -e \\\\\\\"process.exit(0)\\\\\\\"\\\"] }\\n+        ask-confirm[]:\\n+          - \\\"yes\\\"\\n+          - \\\"yes\\\"\\n+        confirm-interpret:\\n+          proceed: true\\n+          commands: [\\\"node -e \\\\\\\"process.exit(1)\\\\\\\"\\\"]\\n+        run-commands[]:\\n+          - { stdout: '{\\\"failed\\\":1,\\\"results\\\":[{\\\"cmd\\\":\\\"node -e \\\\\\\\\\\"process.exit(1)\\\\\\\\\\\"\\\",\\\"code\\\":1,\\\"stderr\\\":\\\"fail\\\"}]}' , exit_code: 1 }\\n+          - { stdout: '{\\\"failed\\\":0,\\\"results\\\":[{\\\"cmd\\\":\\\"node -e \\\\\\\\\\\"process.exit(0)\\\\\\\\\\\"\\\",\\\"code\\\":0}]}' }\\n+      expect:\\n+        calls:\\n+          - step: ask\\n+            exactly: 2\\n+          - step: refine\\n+            exactly: 2\\n+          - step: plan-commands\\n+            exactly: 2\\n+          - step: ask-confirm\\n+            exactly: 2\\n+          - step: confirm-interpret\\n+            exactly: 2\\n+          - step: run-commands\\n+            exactly: 2\\n+          - step: finish\\n+            exactly: 1\\n+        prompts:\\n+          - step: plan-commands\\n+            index: last\\n+            contains:\\n+              - \\\"Previous attempt failed\\\"\\n+              - \\\"Run results\\\"\\n+\\n+    - name: confirm-amend-json\\n+      description: User pastes JSON array of edited commands; interpreter parses and proceeds.\\n+      event: manual\\n+      fixture: local.minimal\\n+      mocks:\\n+        ask: \\\"Make sure project installs deps and runs tests\\\"\\n+        refine:\\n+          refined: true\\n+          text: \\\"Install dependencies and run tests\\\"\\n+        plan-commands:\\n+          commands: [\\\"npm test\\\"]\\n+        ask-confirm: '[\\\"npm ci\\\",\\\"npm test\\\"]'\\n+        confirm-interpret:\\n+          proceed: true\\n+          commands: [\\\"npm ci\\\",\\\"npm test\\\"]\\n+        run-commands:\\n+          stdout: '{\\\"failed\\\":0,\\\"results\\\":[{\\\"cmd\\\":\\\"npm ci\\\",\\\"code\\\":0},{\\\"cmd\\\":\\\"npm test\\\",\\\"code\\\":0}]}'\\n+      expect:\\n+        calls:\\n+          - step: ask\\n+            exactly: 1\\n+          - step: refine\\n+            exactly: 1\\n+          - step: plan-commands\\n+            exactly: 1\\n+          - step: ask-confirm\\n+            exactly: 1\\n+          - step: confirm-interpret\\n+            exactly: 1\\n+          - step: run-commands\\n+            exactly: 1\\n+          - step: finish\\n+            exactly: 1\\n+        outputs:\\n+          - step: confirm-interpret\\n+            path: proceed\\n+            equals: true\\n+          - step: confirm-interpret\\n+            path: commands.length\\n+            equals: 2\\n+\\n+    - name: confirm-amend-lines\\n+      description: User pastes multi-line commands with duplicates; interpreter dedupes and proceeds.\\n+      event: manual\\n+      fixture: local.minimal\\n+      mocks:\\n+        ask: \\\"Run lint and tests\\\"\\n+        refine:\\n+          refined: true\\n+          text: \\\"Run lint and tests\\\"\\n+        plan-commands:\\n+          commands: [\\\"npm run lint\\\",\\\"npm test\\\"]\\n+        ask-confirm: |\\n+          npm test\\\\nnpm test\\\\n  npm run lint  \\n+        confirm-interpret:\\n+          proceed: true\\n+          commands: [\\\"npm test\\\",\\\"npm run lint\\\"]\\n+        run-commands:\\n+          stdout: '{\\\"failed\\\":0,\\\"results\\\":[{\\\"cmd\\\":\\\"npm test\\\",\\\"code\\\":0},{\\\"cmd\\\":\\\"npm run lint\\\",\\\"code\\\":0}]}'\\n+      expect:\\n+        calls:\\n+          - step: ask\\n+            exactly: 1\\n+          - step: refine\\n+            exactly: 1\\n+          - step: plan-commands\\n+            exactly: 1\\n+          - step: ask-confirm\\n+            exactly: 1\\n+          - step: confirm-interpret\\n+            exactly: 1\\n+          - step: run-commands\\n+            exactly: 1\\n+          - step: finish\\n+            exactly: 1\\n+        outputs:\\n+          - step: confirm-interpret\\n+            path: proceed\\n+            equals: true\\n+          - step: confirm-interpret\\n+            path: commands.length\\n+            equals: 2\\n+\\n+    - name: user-declines-replan-then-accept\\n+      description: User declines; interpreter routes to replan; user then accepts; run succeeds.\\n+      event: manual\\n+      fixture: local.minimal\\n+      mocks:\\n+        ask: \\\"Set up lint and test checks\\\"\\n+        refine:\\n+          refined: true\\n+          text: \\\"Ensure lint and tests run\\\"\\n+        plan-commands[]:\\n+          - { commands: [\\\"npm test\\\"] }\\n+          - { commands: [\\\"npm run lint\\\",\\\"npm test\\\"] }\\n+        ask-confirm[]:\\n+          - \\\"No, add lint too\\\"\\n+          - \\\"yes\\\"\\n+        confirm-interpret[]:\\n+          - { proceed: false, reason: \\\"needs lint\\\" }\\n+          - { proceed: true, commands: [\\\"npm run lint\\\",\\\"npm test\\\"] }\\n+        run-commands:\\n+          stdout: '{\\\"failed\\\":0,\\\"results\\\":[{\\\"cmd\\\":\\\"npm run lint\\\",\\\"code\\\":0},{\\\"cmd\\\":\\\"npm test\\\",\\\"code\\\":0}]}'\\n+      expect:\\n+        calls:\\n+          - step: ask\\n+            at_least: 1\\n+          - step: refine\\n+            at_least: 1\\n+          - step: plan-commands\\n+            exactly: 2\\n+          - step: ask-confirm\\n+            exactly: 2\\n+          - step: confirm-interpret\\n+            exactly: 2\\n+          - step: run-commands\\n+            exactly: 1\\n+          - step: finish\\n+            exactly: 1\\n+        outputs:\\n+          - step: confirm-interpret\\n+            index: last\\n+            path: proceed\\n+            equals: true\\n+\\n+    - name: run-fail-then-success-loop\\n+      description: First run fails; planner sees history; second run succeeds; prompt shows failure context.\\n+      event: manual\\n+      strict: false\\n+      fixture: local.minimal\\n+      mocks:\\n+        ask: \\\"Verify app tests pass\\\"\\n+        refine:\\n+          refined: true\\n+          text: \\\"Run tests to verify app correctness\\\"\\n+        plan-commands[]:\\n+          - { commands: [\\\"node -e \\\\\\\"process.exit(1)\\\\\\\"\\\"] }\\n+          - { commands: [\\\"echo ok\\\"] }\\n+        ask-confirm[]:\\n+          - \\\"yes\\\"\\n+          - \\\"yes\\\"\\n+        confirm-interpret[]:\\n+          - { proceed: true, commands: [\\\"node -e \\\\\\\"process.exit(1)\\\\\\\"\\\"] }\\n+          - { proceed: true, commands: [\\\"echo ok\\\"] }\\n+        run-commands[]:\\n+          - { stdout: '{\\\"failed\\\":1,\\\"results\\\":[{\\\"cmd\\\":\\\"node -e \\\\\\\\\\\"process.exit(1)\\\\\\\\\\\"\\\",\\\"code\\\":1,\\\"stderr\\\":\\\"fail\\\"}]}' , exit_code: 1 }\\n+          - { stdout: '{\\\"failed\\\":0,\\\"results\\\":[{\\\"cmd\\\":\\\"echo ok\\\",\\\"code\\\":0}]}' }\\n+      expect:\\n+        calls:\\n+          - step: ask\\n+            at_least: 1\\n+          - step: refine\\n+            at_least: 1\\n+          - step: plan-commands\\n+            exactly: 2\\n+          - step: ask-confirm\\n+            exactly: 2\\n+          - step: confirm-interpret\\n+            exactly: 2\\n+          - step: run-commands\\n+            exactly: 2\\n+        prompts:\\n+          - step: plan-commands\\n+            index: last\\n+            contains:\\n+              - \\\"Previous attempt failed\\\"\\n+              - \\\"Run results\\\"\\n\",\"status\":\"added\"},{\"filename\":\"defaults/visor.tests.yaml\",\"additions\":1,\"deletions\":1,\"changes\":69,\"patch\":\"diff --git a/defaults/visor.tests.yaml b/defaults/visor.tests.yaml\\nindex 3242f87e..b88d1d61 100644\\n--- a/defaults/visor.tests.yaml\\n+++ b/defaults/visor.tests.yaml\\n@@ -60,7 +60,32 @@ tests:\\n           - step: overview\\n             contains:\\n               - \\\"feat: add user search\\\"\\n-              - \\\"diff --git a/src/search.ts\\\"\\n+\\n+    - name: references-example-link-issue\\n+      description: Ensure issue-assistant prompt renders example with HEAD fallback and includes References section.\\n+      event: issue_opened\\n+      fixture: gh.issue_open.minimal\\n+      ai_include_code_context: true\\n+      strict: false\\n+      expect:\\n+        prompts:\\n+          - step: issue-assistant\\n+            contains:\\n+              - \\\"References:\\\"\\n+              - \\\"https://github.com/owner/repo/blob/HEAD/path/to/file.ext#LSTART-LEND\\\"\\n+\\n+    - name: references-example-link-comment\\n+      description: Ensure comment-assistant prompt renders example with HEAD fallback and includes References section.\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_help\\n+      ai_include_code_context: true\\n+      strict: false\\n+      expect:\\n+        prompts:\\n+          - step: comment-assistant\\n+            contains:\\n+              - \\\"References:\\\"\\n+              - \\\"https://github.com/owner/repo/blob/HEAD/path/to/file.ext#LSTART-LEND\\\"\\n \\n     - name: issue-triage\\n       skip: true\\n@@ -105,7 +130,7 @@ tests:\\n     - name: issue-triage-labels\\n       description: |\\n         Minimal issue triage path to validate label application from issue-assistant output.\\n-        This specifically exercises the apply-issue-labels value_js to catch sandbox errors.\\n+        Provider derives labels from dependency outputs and normalizes them.\\n       event: issue_opened\\n       fixture: gh.issue_open.minimal\\n       mocks:\\n@@ -294,10 +319,13 @@ tests:\\n             Invalid fact path: after assistant reply, extract-facts finds one claim and\\n             validate-fact returns is_valid=false; the runner detects problems from\\n             validate-fact history and reruns the assistant once with correction context.\\n+            Due to goto forward-running dependents, extract-facts and validate-fact also run again.\\n           event: issue_comment\\n           fixture: gh.issue_comment.visor_help\\n           env:\\n             ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          routing:\\n+            max_loops: 1\\n           mocks:\\n             comment-assistant:\\n               text: \\\"We rely on defaults/visor.yaml line 11 for max_parallelism=4.\\\"\\n@@ -314,11 +342,13 @@ tests:\\n           expect:\\n             calls:\\n               - step: comment-assistant\\n-                at_least: 2\\n+                exactly: 2\\n               - step: extract-facts\\n-                exactly: 1\\n+                exactly: 2\\n               - step: validate-fact\\n-                at_least: 1\\n+                exactly: 2\\n+              - step: aggregate\\n+                exactly: 1\\n             outputs:\\n               - step: validate-fact\\n                 where: { path: fact_id, equals: f1 }\\n@@ -328,10 +358,13 @@ tests:\\n         - name: facts-two-items (comment)\\n           description: |\\n             Two facts extracted; only the invalid fact should appear in the correction pass.\\n+            Due to goto forward-running dependents, extract-facts and validate-fact run again on retry.\\n           event: issue_comment\\n           fixture: gh.issue_comment.visor_help\\n           env:\\n             ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          routing:\\n+            max_loops: 1\\n           mocks:\\n             comment-assistant:\\n               text: \\\"We rely on defaults/visor.yaml for concurrency defaults.\\\"\\n@@ -345,11 +378,13 @@ tests:\\n           expect:\\n             calls:\\n               - step: comment-assistant\\n-                at_least: 2\\n+                exactly: 2\\n               - step: extract-facts\\n-                exactly: 1\\n-              - step: validate-fact\\n                 exactly: 2\\n+              - step: validate-fact\\n+                exactly: 4\\n+              - step: aggregate\\n+                exactly: 1\\n             outputs:\\n               - step: validate-fact\\n                 where: { path: fact_id, equals: f1 }\\n@@ -401,17 +436,23 @@ tests:\\n             calls:\\n               - step: issue-assistant\\n                 at_least: 1\\n+              - step: apply-issue-labels\\n+                exactly: 1\\n               - step: extract-facts\\n                 exactly: 1\\n               - step: validate-fact\\n                 at_least: 1\\n \\n         - name: facts-invalid (issue)\\n-          description: Invalid claim triggers correction by rerunning issue-assistant.\\n+          description: |\\n+            Invalid claim triggers correction by rerunning issue-assistant.\\n+            Due to goto forward-running dependents, extract-facts and validate-fact also run again.\\n           event: issue_opened\\n           fixture: gh.issue_open.minimal\\n           env:\\n             ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          routing:\\n+            max_loops: 1\\n           mocks:\\n             issue-assistant:\\n               text: \\\"Claim: max_parallelism defaults to 4\\\"\\n@@ -428,11 +469,15 @@ tests:\\n           expect:\\n             calls:\\n               - step: issue-assistant\\n-                at_least: 2\\n+                exactly: 2\\n+              - step: apply-issue-labels\\n+                exactly: 2\\n               - step: extract-facts\\n-                exactly: 1\\n+                exactly: 2\\n               - step: validate-fact\\n-                at_least: 1\\n+                exactly: 2\\n+              - step: aggregate\\n+                exactly: 1\\n             outputs:\\n               - step: validate-fact\\n                 where: { path: fact_id, equals: f1 }\\n\",\"status\":\"modified\"},{\"filename\":\"defaults/visor.yaml\",\"additions\":5,\"deletions\":10,\"changes\":522,\"patch\":\"diff --git a/defaults/visor.yaml b/defaults/visor.yaml\\nindex 50b2ef71..d041eb65 100644\\n--- a/defaults/visor.yaml\\n+++ b/defaults/visor.yaml\\n@@ -1,4 +1,5 @@\\n version: \\\"1.0\\\"\\n+include: \\\"./code-review.yaml\\\"\\n \\n # Default Visor configuration - provides comprehensive code analysis out-of-the-box\\n # Uses mock provider for CI compatibility when no AI API keys are configured\\n@@ -19,7 +20,7 @@ steps:\\n   release-notes:\\n     type: ai\\n     group: release\\n-    schema: plain\\n+    on: [manual]\\n     prompt: |\\n       Generate professional release notes for version {{ env.TAG_NAME }} of this project.\\n \\n@@ -56,279 +57,30 @@ steps:\\n \\n       Keep descriptions concise and user-friendly. Focus on what changed from a user perspective, not implementation details.\\n       Use present tense and action-oriented language. Group similar changes together.\\n-    on: [manual]\\n-\\n-  # PR overview with intelligent analysis - runs first to establish context\\n-  overview:\\n-    type: ai\\n-    group: overview\\n-    schema: overview\\n-    prompt: |\\n-        You are generating PR overview, to help owners of the repository to understand what this PR is above, and help reviewer to point to the right parts of the code. First you should provide detailed but concise description, mentioning all the changes.\\n-\\n-        ## Files Changed Analysis\\n-        After you need to summarize insights from `<files_summary>`: changed files, additions/deletions, notable patterns.\\n-\\n-        Next ensure you cover all below:\\n-\\n-        ## Architecture & Impact Assessment\\n-          - What this PR accomplishes\\n-          - Key technical changes introduced\\n-          - Affected system components\\n-          - Include one or more mermaid diagrams when useful to visualize component relationships or flow.\\n-          \\n-        ## Scope Discovery & Context Expansion\\n-        - From the `<files_summary>` and code diffs, infer the broader scope of impact across modules, services, and boundaries.\\n-        - If your environment supports code search/extract tools, use them to peek at immediately-related files (tests, configs, entrypoints) for better context. If tools are not available, infer and list what you would search next.\\n-\\n-        You may also be asked to assign labels to PR; if so use this:\\n-        - `tags.review-effort`: integer 1‚Äì5 estimating review effort (1=trivial, 5=very high).\\n-        - `tags.label`: one of [bug, chore, documentation, enhancement, feature]. Choose the best fit.\\n-\\n-        Important:\\n-        - Propose `tags.review-effort` and `tags.label` only for the initial PR open event.\\n-        - Do not change or re-suggest labels on PR update events; the repository applies labels only on `pr_opened`.\\n-\\n-        Be concise, specific, and actionable. Avoid praise or celebration.\\n-    on: [pr_opened, pr_updated]\\n-\\n-  # Security analysis - Critical for all projects\\n-  security:\\n-    type: ai\\n-    group: review\\n-    schema: code-review\\n-    prompt: |\\n-        Based on our overview discussion, please perform a comprehensive security analysis of the code changes.\\n-\\n-        Analyze the files listed in the `<files_summary>` section and focus on the code changes shown in the diff sections.\\n-\\n-        ## Security Analysis Areas\\n-\\n-        **Input Validation & Injection:**\\n-        - SQL injection in database queries\\n-        - XSS vulnerabilities in user input handling\\n-        - Command injection in system calls\\n-        - Path traversal in file operations\\n-\\n-        **Authentication & Authorization:**\\n-        - Weak authentication mechanisms\\n-        - Session management flaws\\n-        - Access control bypasses\\n-        - Privilege escalation opportunities\\n-\\n-        **Data Protection:**\\n-        - Sensitive data exposure in logs/errors\\n-        - Unencrypted data storage\\n-        - API key or credential leaks\\n-        - Privacy regulation compliance\\n-\\n-        **Infrastructure Security:**\\n-        - Insecure configurations\\n-        - Missing security headers\\n-        - Vulnerable dependencies\\n-        - Resource exhaustion vulnerabilities\\n-\\n-        Provide specific findings with clear explanations and actionable remediation steps.\\n-\\n-        ## Severity Guidelines\\n-        Use the following severity levels appropriately:\\n-        - **critical**: Security vulnerabilities that could lead to immediate compromise (RCE, SQL injection, authentication bypass, exposed secrets)\\n-        - **error**: Security issues that must be fixed before production (XSS, path traversal, weak crypto, missing auth checks)\\n-        - **warning**: Security concerns that should be addressed (verbose errors, missing rate limiting, insecure defaults)\\n-        - **info**: Security best practices and hardening suggestions (defense in depth, additional validation)\\n-    depends_on: [overview]\\n-    on: [pr_opened, pr_updated]\\n-\\n-  # Architecture analysis - Ensures sound design and avoids over-engineering\\n-  architecture:\\n-    type: ai\\n-    group: review\\n-    schema: code-review\\n-    prompt: |\\n-        Building on our overview analysis, evaluate the architectural decisions and design patterns.\\n-\\n-        Focus on the files listed in `<files_summary>` and analyze the code changes shown in the `<full_diff>` or `<commit_diff>` sections.\\n-\\n-        ## Architecture Analysis Areas\\n-        **Design Simplicity & Pragmatism:**\\n-        - Over-engineering: Is the solution more complex than necessary?\\n-        - YAGNI violations: Are we building features not currently needed?\\n-        - Can this be implemented more simply using existing patterns?\\n-        - Are there existing functions/modules that could be reused instead?\\n-        - Is the abstraction level appropriate for the problem size?\\n-\\n-        **Special Cases & Edge Cases:**\\n-        - Does the code introduce special case handling that could be generalized?\\n-        - Are there hard-coded values or logic for specific scenarios?\\n-        - Can special cases be eliminated through better design?\\n-        - Are edge cases handled through general patterns rather than one-offs?\\n-\\n-        **Consistency & Reusability:**\\n-        - Does this follow existing architectural patterns in the codebase?\\n-        - Are there similar problems already solved differently elsewhere?\\n-        - Could this functionality be generalized to handle more cases?\\n-        - Does it duplicate existing functionality that could be extended?\\n-\\n-        **Design Patterns & Structure:**\\n-        - Are design patterns used appropriately (not forced)?\\n-        - Is the separation of concerns clear and logical?\\n-        - Are dependencies and coupling minimized?\\n-        - Is the code organized in a way that makes sense?\\n-\\n-        **Scalability & Extensibility:**\\n-        - Will this design scale with future requirements?\\n-        - Is it easy to extend without major refactoring?\\n-        - Are boundaries and interfaces well-defined?\\n-\\n-        Flag issues where simpler solutions exist, special cases are introduced unnecessarily, or existing functionality could be reused.\\n-\\n-        ## Severity Guidelines\\n-        Use the following severity levels appropriately:\\n-        - **critical**: Architectural decisions that will cause system failures or major technical debt (circular dependencies, tight coupling causing deadlocks)\\n-        - **error**: Significant architectural problems (over-engineering, unnecessary special cases, violation of core patterns, missed reuse opportunities)\\n-        - **warning**: Architectural concerns that should be addressed (minor over-abstraction, could be simplified, inconsistent patterns)\\n-        - **info**: Architectural suggestions and alternative approaches (simpler patterns available, potential for future reuse)\\n-    depends_on: [overview]\\n-    on: [pr_opened, pr_updated]\\n-\\n-  # Performance analysis - Important for all applications\\n-  performance:\\n-    type: ai\\n-    group: review\\n-    schema: code-review\\n-    prompt: |\\n-        Building on our overview analysis, now review the code changes for performance issues.\\n-\\n-        Focus on the files listed in `<files_summary>` and analyze the code changes shown in the `<full_diff>` or `<commit_diff>` sections.\\n-\\n-        ## Performance Analysis Areas\\n-        **Algorithm & Data Structure Efficiency:**\\n-        - Time complexity analysis (O(n), O(n¬≤), etc.)\\n-        - Space complexity and memory usage\\n-        - Inefficient loops and nested operations\\n-        - Suboptimal data structure choices\\n-\\n-        **Database Performance:**\\n-        - N+1 query problems\\n-        - Missing database indexes\\n-        - Inefficient JOIN operations\\n-        - Large result set retrievals\\n-\\n-        **Resource Management:**\\n-        - Memory leaks and excessive allocations\\n-        - File handle management\\n-        - Connection pooling issues\\n-        - Resource cleanup patterns\\n-\\n-        **Async & Concurrency:**\\n-        - Blocking operations in async contexts\\n-        - Race conditions and deadlocks\\n-        - Inefficient parallel processing\\n-\\n-        Building on our overview analysis, identify performance issues and provide optimization recommendations.\\n-\\n-        ## Severity Guidelines\\n-        Use the following severity levels appropriately:\\n-        - **critical**: Performance issues causing system failure or severe degradation (infinite loops, memory leaks causing OOM)\\n-        - **error**: Significant performance problems affecting user experience (O(n¬≤) in critical path, N+1 queries, blocking I/O)\\n-        - **warning**: Performance concerns that should be optimized (inefficient algorithms, missing indexes, unnecessary operations)\\n-        - **info**: Performance best practices and optimization opportunities (caching suggestions, async improvements)\\n-    depends_on: [overview]\\n-    on: [pr_opened, pr_updated]\\n+    schema: plain\\n \\n-\\n-  # Code quality and maintainability\\n-  quality:\\n-    type: ai\\n-    group: review\\n-    schema: code-review\\n-    prompt: |\\n-        Building on our overview discussion, evaluate the code quality and maintainability.\\n-\\n-        Review the code changes shown in the `<full_diff>` or `<commit_diff>` sections, considering the files listed in `<files_summary>`.\\n-\\n-        ## Quality Assessment Areas\\n-        **Code Structure & Design:**\\n-        - SOLID principles adherence\\n-        - Design pattern appropriateness\\n-        - Separation of concerns\\n-        - Code organization and clarity\\n-\\n-        **Error Handling & Reliability:**\\n-        - Exception handling completeness\\n-        - Error propagation patterns\\n-        - Input validation thoroughness\\n-        - Edge case coverage\\n-\\n-        **Testing & Test Coverage:**\\n-        - Missing tests for critical functionality\\n-        - Test coverage gaps\\n-        - Test quality and effectiveness\\n-        - Edge cases and error scenarios coverage\\n-\\n-        **Test Quality - Critical Checks:**\\n-        - **Magic Numbers in Tests**: Are tests using hard-coded values just to make assertions pass?\\n-          - Look for arbitrary numbers without clear meaning (e.g., `expect(result).toBe(42)` where 42 has no semantic meaning)\\n-          - Tests should use meaningful constants or derive expected values from the input\\n-          - Tests that \\\"cut corners\\\" by adjusting expected values to match output are fragile\\n-          - Flag tests where the expected value seems arbitrary or reverse-engineered from implementation\\n-        - **Test Integrity**: Do tests actually validate correct behavior or just check current behavior?\\n-          - Are tests meaningful and would catch real bugs?\\n-          - Do tests explain why a value should be expected (through variable names, comments, or clear logic)?\\n-          - Are test assertions based on requirements rather than implementation details?\\n-        - **Negative Testing Coverage**: Are failure cases and error conditions properly tested?\\n-          - Are there tests for invalid inputs, boundary conditions, and error states?\\n-          - Do tests verify error messages and error handling paths?\\n-          - Are edge cases like null, empty, undefined, zero, negative values tested?\\n-          - Do tests cover \\\"unhappy path\\\" scenarios (authentication failures, network errors, invalid data)?\\n-          - Are exception handling and error recovery mechanisms validated?\\n-          - Flag missing negative tests for critical functionality\\n-\\n-        **Maintainability:**\\n-        - Code testability issues\\n-        - Dependencies and coupling problems\\n-        - Technical debt introduction\\n-        - Code duplication (DRY violations)\\n-\\n-        **Language-Specific Best Practices:**\\n-        - Idiomatic code usage\\n-        - Framework/library best practices\\n-        - Type safety (if applicable)\\n-\\n-        Focus on actionable improvements that enhance code maintainability based on the overview analysis.\\n-        Pay special attention to test quality - tests that use magic numbers or cut corners undermine reliability.\\n-\\n-        ## Severity Guidelines\\n-        Use the following severity levels appropriately:\\n-        - **critical**: Code quality issues that will cause bugs or failures (logic errors, race conditions, null pointer issues)\\n-        - **error**: Quality problems that significantly impact maintainability (no error handling, high complexity, severe coupling, tests with magic numbers)\\n-        - **warning**: Quality concerns that should be addressed (missing tests, code duplication, poor naming, unclear test expectations)\\n-        - **info**: Best practices and improvement suggestions (refactoring opportunities, documentation improvements, test clarity)\\n-    depends_on: [overview]\\n-    on: [pr_opened, pr_updated]\\n+  # (overview, security, architecture, performance, quality) were extracted to defaults/code-review.yaml\\n \\n   # Apply labels based on overview tags ‚Äî runs only on PR open (GitHub environments only)\\n   apply-overview-labels:\\n     type: github\\n+    criticality: external\\n     tags: [github]\\n-    depends_on: [overview]\\n     on: [pr_opened]\\n+    depends_on: [overview]\\n+    assume:\\n+      - \\\"outputs['overview']?.tags?.label\\\"\\n+      - \\\"outputs['overview']?.tags?.['review-effort'] != null\\\"\\n     op: labels.add\\n     values:\\n       - \\\"{{ outputs.overview.tags.label | default: '' | safe_label }}\\\"\\n       - \\\"{{ outputs.overview.tags['review-effort'] | default: '' | prepend: 'review/effort:' | safe_label }}\\\"\\n-    value_js: |\\n-      // Normalize, trim, and drop empties (ES5-safe)\\n-      return values\\n-        .map(v => (v == null ? '' : String(v)))\\n-        .map(s => s.trim())\\n-        .filter(s => s.length > 0);\\n \\n   # Issue Assistant (issues only) ‚Äî triage-quality prompt from main branch, structured output\\n   issue-assistant:\\n     type: ai\\n     group: dynamic  # New issue triage posts a standalone comment\\n-    schema: issue-assistant\\n+    on: [issue_opened]\\n     prompt: |\\n         You are an intelligent GitHub issue assistant for the {{ event.repository.fullName }} repository. Your role is to provide professional, knowledgeable assistance when a NEW issue is opened.\\n \\n@@ -366,20 +118,11 @@ steps:\\n         - `intent`: must be \\\"issue_triage\\\" for this flow.\\n         - `labels` (optional): array of labels that would help organization for this new issue.\\n \\n-        Add a short ‚ÄúReferences‚Äù section at the end of the markdown `text` that lists any repository files/lines you relied on for the answer. Use this exact machine‚Äëparsable format:\\n+        Add a short ‚ÄúReferences‚Äù section at the end of the markdown `text` as a simple, clickable markdown list (no fenced code blocks). Keep it minimal. If you didn‚Äôt consult code, write `References: none`.\\n \\n+        Example:\\n         References:\\n-        ```refs\\n-        path/to/file.ext[:start[-end]|#SymbolName] - very short note\\n-        ```\\n-\\n-        Rules for References:\\n-        - Only include files/lines/symbols you actually used. Keep it minimal and relevant.\\n-        - If you didn‚Äôt consult code, write:\\n-          References:\\n-          ```refs\\n-          none\\n-          ```\\n+        - [path/to/file.ext:START-END](https://github.com/{{ event.repository.fullName }}/blob/{{ event.pull_request.head.sha | default: 'HEAD' }}/path/to/file.ext#LSTART-LEND) ‚Äì very short note\\n \\n         Use this triage rubric (adopted from our main prompt):\\n         1) Categorize the issue - choose from: bug, chore, documentation, enhancement, feature, question, wontfix, invalid, duplicate\\n@@ -401,17 +144,19 @@ steps:\\n         - NEVER make promises about timelines, release dates, or team commitments\\n         - NEVER say things like \\\"we'll pick this up\\\", \\\"will be included in upcoming release\\\", or \\\"we will post updates\\\"\\n         - Focus on technical analysis and helpful information rather than commitments\\n-    on: [issue_opened]\\n \\n   # Comment Assistant (comments only) ‚Äî intent detection and reply\\n   comment-assistant:\\n     type: ai\\n     group: dynamic\\n-    schema: issue-assistant\\n+    on: [issue_comment]\\n     command: \\\"visor\\\"\\n     prompt: |\\n         You are the GitHub comment assistant for {{ event.repository.fullName }}. Respond to user comments on issues or PR discussion threads.\\n \\n+        Latest comment (verbatim):\\n+        {{ event.comment.body | default: \\\"\\\" }}\\n+\\n         {%- liquid\\n           # Correction context from the last validation wave (filtered)\\n           assign issues = outputs_history[\\\"validate-fact\\\"].last | where_exp: 'i', 'i && (i.is_valid != true || i.confidence != \\\"high\\\")'\\n@@ -445,22 +190,11 @@ steps:\\n         - `intent`: choose one: \\\"comment_reply\\\" (normal reply) or \\\"comment_retrigger\\\" (pick this ONLY when the user explicitly asks to re-run checks OR explicitly asks to disable some checks).\\n         - `labels`: omit for comments (do not include).\\n \\n-        {% raw %}\\n-        Add a short ‚ÄúReferences‚Äù section at the end of the markdown `text` that lists any repository files/lines you relied on for the answer. Use this exact machine‚Äëparsable format:\\n+        Add a short ‚ÄúReferences‚Äù section at the end of the markdown `text` as a clickable markdown list (no fenced blocks). If none used, write `References: none`.\\n \\n+        Example:\\n         References:\\n-        ```refs\\n-        path/to/file.ext[:start[-end]] - very short note\\n-        ```\\n-\\n-        Rules for References:\\n-        - Only include files/lines you actually used. Keep it minimal and relevant.\\n-        - If you didn‚Äôt consult code, write:\\n-          References:\\n-          ```refs\\n-          none\\n-          ```\\n-        {% endraw %}\\n+        - [path/to/file.ext:START-END](https://github.com/{{ event.repository.fullName }}/blob/{{ event.pull_request.head.sha | default: 'HEAD' }}/path/to/file.ext#LSTART-LEND) ‚Äì very short note\\n \\n         Rules:\\n         - Never suggest rerun/disable unless asked explicitly.\\n@@ -471,40 +205,33 @@ steps:\\n         - Be honest when you don't know something or can't find the answer in the available context\\n         - If the question requires information not available in the PR/issue context, clearly state what's missing\\n         - Provide partial answers when possible, and indicate what additional information would help give a complete response\\n-    on: [issue_comment]\\n+    schema: issue-assistant\\n     on_success:\\n-      goto_js: |\\n-        const intent = output?.intent;\\n-        const isComment = event?.name === 'issue_comment';\\n-        const allowed = typeof hasMinPermission === 'function' ? hasMinPermission('MEMBER') : true;\\n-        return isComment && allowed && intent === 'comment_retrigger' ? 'overview' : null;\\n-      goto_event: pr_updated\\n+      transitions:\\n+        - when: \\\"event.name === 'issue_comment' && output?.intent === 'comment_retrigger'\\\"\\n+          to: overview\\n+          goto_event: pr_updated\\n \\n   # Apply labels to new issues based on assistant output (GitHub-only)\\n   apply-issue-labels:\\n     type: github\\n+    criticality: external\\n     tags: [github]\\n-    depends_on: [issue-assistant]\\n     on: [issue_opened]\\n+    depends_on: [issue-assistant]\\n+    assume:\\n+      - \\\"(outputs['issue-assistant']?.labels?.length ?? 0) > 0\\\"\\n     op: labels.add\\n-    value_js: |\\n-      try {\\n-        var ai = outputs && outputs['issue-assistant'] ? outputs['issue-assistant'] : {};\\n-        var labels = Array.isArray(ai && ai.labels) ? ai.labels : [];\\n-        // Sanitize labels: keep [A-Za-z0-9:/\\\\- ] (alphanumerics, colon, slash, hyphen, and space), collapse repeated '/'\\n-        return labels\\n-          .map(function(v) { return String(v == null ? '' : v); })\\n-          .map(function(s) { return s.replace(/[^A-Za-z0-9:\\\\/\\\\- ]/g, '').replace(/\\\\/{2,}/g, '/').trim(); })\\n-          .filter(Boolean);\\n-      } catch (e) {\\n-        // Avoid shadowing/resolution quirks with identifier name \\\"error\\\" in sandboxed environments\\n-        log('Error processing issue labels:', e);\\n-        return [];\\n-      }\\n+    # Explicitly derive labels from issue-assistant output with guardrails\\n+    # - Use Liquid to serialize labels array to JSON; provider will expand\\n+    # - Assumptions ensure the dependency exists and produced at least one label\\n+    values:\\n+      - \\\"{{ outputs['issue-assistant'].labels | default: [] | json }}\\\"\\n \\n   # External origin labelling for PRs and Issues\\n   external-label:\\n     type: github\\n+    criticality: external\\n     tags: [github]\\n     on: [pr_opened, issue_opened]\\n     if: \\\"!isMember() && !isContributor()\\\"\\n@@ -528,44 +255,16 @@ steps:\\n   extract-facts:\\n     type: ai\\n     group: fact-validation\\n-    schema:\\n-      type: array\\n-      items:\\n-        type: object\\n-        properties:\\n-          id:\\n-            type: string\\n-            description: Unique identifier for the fact (e.g., fact-1, fact-2)\\n-          category:\\n-            type: string\\n-            description: Type of claim (Configuration, Feature, Documentation, API, etc.)\\n-          claim:\\n-            type: string\\n-            description: The exact factual statement being made\\n-          verifiable:\\n-            type: boolean\\n-            description: Whether this claim can be verified against the codebase\\n-          refs:\\n-            type: array\\n-            description: Optional list of code references (parsed from assistant References block)\\n-            items:\\n-              type: object\\n-              properties:\\n-                path:\\n-                  type: string\\n-                lines:\\n-                  type: string\\n-                  description: Line or range, e.g., \\\"120-145\\\" or \\\"88\\\"\\n-              required: [path]\\n-        required: [id, category, claim, verifiable]\\n-    # Declarative OR dependency: run after either assistant in the same stage\\n-    depends_on: [\\\"issue-assistant|comment-assistant\\\"]\\n     on: [issue_opened, issue_comment]\\n+    depends_on: [\\\"issue-assistant|comment-assistant\\\"]\\n     # Only when validation is enabled (assistants schedule validate-fact; engine runs this dependency inline)\\n     if: \\\"env.ENABLE_FACT_VALIDATION === 'true'\\\"\\n+    # Ensure we have an assistant output to analyze for this event type\\n+    assume:\\n+      - \\\"outputs['issue-assistant'] || outputs['comment-assistant']\\\"\\n     ai:\\n       skip_code_context: true\\n-      disable_tools: true\\n+      disableTools: true\\n     prompt: |\\n       Your task is to EXTRACT factual claims from the assistant's response below.\\n \\n@@ -624,49 +323,55 @@ steps:\\n       Return ONLY the JSON array of fact objects. Do not investigate or verify anything.\\n       Each item must be: { id, category, claim, verifiable, refs? } where refs items may contain `lines` or `symbol`.\\n \\n+    schema:\\n+      type: array\\n+      items:\\n+        type: object\\n+        properties:\\n+          id:\\n+            type: string\\n+            description: Unique identifier for the fact (e.g., fact-1, fact-2)\\n+          category:\\n+            type: string\\n+            description: Type of claim (Configuration, Feature, Documentation, API, etc.)\\n+          claim:\\n+            type: string\\n+            description: The exact factual statement being made\\n+          verifiable:\\n+            type: boolean\\n+            description: Whether this claim can be verified against the codebase\\n+          refs:\\n+            type: array\\n+            description: Optional list of code references (parsed from assistant References block)\\n+            items:\\n+              type: object\\n+              properties:\\n+                path:\\n+                  type: string\\n+                lines:\\n+                  type: string\\n+                  description: Line or range, e.g., \\\"120-145\\\" or \\\"88\\\"\\n+              required: [path]\\n+        required: [id, category, claim, verifiable]\\n+\\n     forEach: true\\n \\n-    # After facts are validated, run the aggregator; it decides about corrections in its on_success\\n+    # After one validation wave completes, route back to the appropriate assistant\\n+    # using declarative transitions so the engine forward-runs dependents\\n+    # (assistant ‚Üí extract-facts ‚Üí validate-fact).\\n     on_finish:\\n-      run_js: |\\n-        // ES2023 style: use at() and concise arrows; inspect only the last wave\\n-        const facts = (outputs_history['extract-facts'] || []).at(-1) || [];\\n-        const ids = facts.map(f => String(f.id || '')).filter(Boolean);\\n-        const vf = outputs_history['validate-fact'] || [];\\n-        const lastItems = vf.filter(v => ids.includes(String((v && v.fact_id) || '')));\\n-        const hasProblems = lastItems.some(v => v.is_valid !== true || v.confidence !== 'high');\\n-        if (!hasProblems) return [];\\n-        return (event && event.name) === 'issue_opened' ? ['issue-assistant'] : ['comment-assistant'];\\n+      transitions:\\n+        - when: \\\"any(outputs_history['validate-fact'], v => v && (v.is_valid === false || v.valid === false)) && event.name === 'issue_opened'\\\"\\n+          to: issue-assistant\\n+        - when: \\\"any(outputs_history['validate-fact'], v => v && (v.is_valid === false || v.valid === false)) && event.name === 'issue_comment'\\\"\\n+          to: comment-assistant\\n \\n   # Validate each extracted fact\\n   validate-fact:\\n     type: ai\\n     group: fact-validation\\n-    schema:\\n-      type: object\\n-      properties:\\n-        fact_id:\\n-          type: string\\n-          description: ID of the fact being validated\\n-        claim:\\n-          type: string\\n-          description: The original claim being validated\\n-        is_valid:\\n-          type: boolean\\n-          description: Whether the claim is accurate\\n-        confidence:\\n-          type: string\\n-          enum: [high, medium, low]\\n-          description: Confidence level in the validation\\n-        evidence:\\n-          type: string\\n-          description: Evidence found in the codebase supporting the validation\\n-        correction:\\n-          type: string\\n-          description: If invalid, the correct information (optional)\\n-      required: [fact_id, claim, is_valid, confidence, evidence]\\n-    depends_on: [extract-facts]\\n     on: [issue_opened, issue_comment]\\n+    depends_on: [extract-facts]\\n     if: \\\"env.ENABLE_FACT_VALIDATION === 'true'\\\"\\n     ai:\\n       timeout: 180000 # 3 minutes hard cap per validation\\n@@ -703,10 +408,75 @@ steps:\\n       - Confidence level (high/medium/low) in your validation\\n       - If the claim is incorrect, provide the accurate information as a correction\\n \\n+    schema:\\n+      type: object\\n+      properties:\\n+        fact_id:\\n+          type: string\\n+          description: ID of the fact being validated\\n+        claim:\\n+          type: string\\n+          description: The original claim being validated\\n+        is_valid:\\n+          type: boolean\\n+          description: Whether the claim is accurate\\n+        confidence:\\n+          type: string\\n+          enum: [high, medium, low]\\n+          description: Confidence level in the validation\\n+        evidence:\\n+          type: string\\n+          description: Evidence found in the codebase supporting the validation\\n+        correction:\\n+          type: string\\n+          description: If invalid, the correct information (optional)\\n+      required: [fact_id, claim, is_valid, confidence, evidence]\\n+\\n+  # Aggregate validation results and expose boolean all_valid\\n+  aggregate:\\n+    type: script\\n+    group: fact-validation\\n+    on: [issue_opened, issue_comment]\\n+    depends_on: [validate-fact]\\n+    if: \\\"env.ENABLE_FACT_VALIDATION === 'true'\\\"\\n+    assume:\\n+      # Run only when we have per-item validations and it's the first attempt\\n+      - \\\"(outputs['validate-fact']?.forEachItems?.length ?? 0) > 0\\\"\\n+      - \\\"(memory.get('attempt', 'fact-validation') ?? 0) === 0\\\"\\n+    content: |\\n+      const vf = (outputs.history['validate-fact']||[]).filter(v => v && typeof v === 'object');\\n+      const ex = (outputs.history['extract-facts']||[]);\\n+      let lastSize = 0; for (let i = ex.length - 1; i >= 0 && lastSize === 0; i--) { if (Array.isArray(ex[i])) { lastSize = ex[i].length; } }\\n+      const recent = lastSize > 0 ? vf.slice(-lastSize) : vf;\\n+      const allValid = recent.length > 0 && recent.every(i => i && (i.is_valid === true || i.valid === true));\\n+      memory.set('all_valid', allValid, 'fact-validation');\\n+      return { all_valid: allValid };\\n+    schema:\\n+      type: object\\n+      properties:\\n+        all_valid:\\n+          type: boolean\\n+      required:\\n+        - all_valid\\n+      additionalProperties: false\\n+    guarantee: \\\"output && typeof output.all_valid === 'boolean'\\\"\\n+    namespace: fact-validation\\n+\\n+  # Post only when all facts are valid\\n+  post-verified:\\n+    type: log\\n+    group: fact-validation\\n+    on: [issue_opened, issue_comment]\\n+    depends_on: [extract-facts]\\n+    if: \\\"env.ENABLE_FACT_VALIDATION === 'true' && memory.get('all_valid', 'fact-validation') === true\\\"\\n+    message: \\\"‚úÖ Posted verified response\\\"\\n+    level: info\\n+\\n   # Retrigger noop removed ‚Äî comment-assistant schedules overview directly\\n \\n output:\\n   pr_comment:\\n     format: markdown\\n-    group_by: check\\n+    # Grouping is determined solely by each check's `group` field.\\n+    # The renderer ignores any global group_by; keep comments compact.\\n     collapse: true\\n\",\"status\":\"modified\"},{\"filename\":\"docs/NPM_USAGE.md\",\"additions\":2,\"deletions\":1,\"changes\":77,\"patch\":\"diff --git a/docs/NPM_USAGE.md b/docs/NPM_USAGE.md\\nindex fd4dd47b..411b0dae 100644\\n--- a/docs/NPM_USAGE.md\\n+++ b/docs/NPM_USAGE.md\\n@@ -8,6 +8,32 @@ Run Visor directly using npx:\\n npx -y @probelabs/visor@latest --help\\n ```\\n \\n+### Safety & Criticality (Quick Note)\\n+\\n+Visor follows a criticality‚Äëfirst model:\\n+\\n+- Declare criticality on steps (`criticality: external|internal|policy|info`).\\n+- Pair critical steps with contracts:\\n+  - `assume:` preconditions (skip if unmet; use a guard step if you need a hard fail)\\n+  - `guarantee:` postconditions (violation adds issues and routes `on_fail`)\\n+- Prefer declarative `transitions` over `goto_js` for routing.\\n+\\n+Example (block‚Äëstyle YAML):\\n+```yaml\\n+checks:\\n+  post-comment:\\n+    type: github\\n+    criticality: external\\n+    on:\\n+      - pr_opened\\n+    op: comment.create\\n+    assume:\\n+      - \\\"isMember()\\\"\\n+    guarantee:\\n+      - \\\"output && typeof output.id === 'number'\\\"\\n+    continue_on_failure: false\\n+```\\n+\\n ## Global Installation\\n \\n Install globally for frequent use:\\n@@ -100,13 +126,62 @@ project:\\n steps:\\n   - type: ai\\n     prompt: security\\n-    \\n   - type: ai\\n     prompt: performance\\n     \\n output:\\n   format: table\\n   verbose: false\\n+\\n+## Structured outputs and schemas (unified `schema`)\\n+\\n+Use a single `schema` field:\\n+\\n+- Strings (e.g., `code-review` or `markdown`) select the renderer/template and do not imply validation.\\n+- Objects (JSON Schema) validate the produced `output` after execution for any provider (ai, command, script, http).\\n+\\n+Examples:\\n+```yaml\\n+checks:\\n+  summary:\\n+    type: ai\\n+    schema: code-review\\n+    prompt: |\\n+      Summarize the PR...\\n+\\n+  summarize-json:\\n+    type: ai\\n+    schema:\\n+      type: object\\n+      properties:\\n+        ok: { type: boolean }\\n+        items: { type: array, items: { type: string } }\\n+      required: [ok, items]\\n+    prompt: |\\n+      Return JSON with ok and items...\\n+\\n+  parse:\\n+    type: command\\n+    exec: node scripts/parse.js\\n+    schema:\\n+      type: object\\n+      properties:\\n+        count: { type: integer }\\n+      required: [count]\\n+\\n+  aggregate:\\n+    type: script\\n+    content: |\\n+      return { all_valid: true };\\n+    schema:\\n+      type: object\\n+      properties:\\n+        all_valid: { type: boolean }\\n+      required: [all_valid]\\n+      additionalProperties: false\\n+```\\n+\\n+Note: `output_schema` is deprecated and kept for backward compatibility. Prefer `schema` as a JSON object for validation.\\n ```\\n \\n ## Environment Variables\\n\",\"status\":\"modified\"},{\"filename\":\"docs/ai-configuration.md\",\"additions\":6,\"deletions\":1,\"changes\":224,\"patch\":\"diff --git a/docs/ai-configuration.md b/docs/ai-configuration.md\\nindex 1a0e18d4..81902ff3 100644\\n--- a/docs/ai-configuration.md\\n+++ b/docs/ai-configuration.md\\n@@ -114,6 +114,47 @@ steps:\\n       provider: openai\\n       model: gpt-4-turbo\\n     prompt: \\\"Review code style and best practices\\\"\\n+\\n+#### Prompt Controls (Probe promptType, customPrompt, and persona)\\n+\\n+Visor exposes Probe‚Äôs prompt controls to adjust the agent‚Äôs behavior for a given step. Use underscore names only.\\n+\\n+Accepted keys\\n+- Under `ai:`\\n+  - `prompt_type`: string ‚Äî Probe persona/family, e.g., `engineer`, `code-review`, `architect`.\\n+  - `custom_prompt`: string ‚Äî Baseline/system prompt prepended by the SDK.\\n+- At the check level (aliases if you prefer not to nest):\\n+  - `ai_prompt_type`: string\\n+  - `ai_custom_prompt`: string\\n+  - `ai_persona`: string ‚Äî optional hint we prepend as a first line: `Persona: <value>`.\\n+\\n+Examples\\n+\\n+```yaml\\n+steps:\\n+  engineer-review:\\n+    type: ai\\n+    ai:\\n+      provider: anthropic\\n+      model: claude-3-5-sonnet-latest\\n+      prompt_type: engineer\\n+      custom_prompt: |\\n+        You are a specialist in analyzing security vulnerabilities.\\n+        Focus on injection, authn/z, crypto, and data exposure.\\n+    schema: code-review\\n+    prompt: |\\n+      Review the following changes.\\n+\\n+  quick-architect-check:\\n+    type: ai\\n+    ai_prompt_type: architect     # check-level alias\\n+    ai_custom_prompt: \\\"Favor modular boundaries and low coupling.\\\"\\n+    prompt: \\\"Assess high-level design risks in the diff\\\"\\n+```\\n+\\n+Notes\\n+- If `prompt_type` is omitted and a `schema` is provided, Visor defaults to `code-review`.\\n+- `ai_persona` is a lightweight hint added as a first line; prefer `prompt_type` when integrating with Probe personas.\\n ```\\n \\n #### AWS Bedrock Specific Configuration\\n@@ -196,6 +237,79 @@ steps:\\n \\n **Security Note:** Edit tools respect existing `allowedFolders` configuration and perform exact string matching to prevent unintended modifications. Always review changes before merging.\\n \\n+#### Tool Filtering (`allowedTools`, `disableTools`)\\n+\\n+Control which tools the AI agent can access during execution. This feature supports three filtering modes for fine-grained control over agent capabilities.\\n+\\n+**Filtering Modes:**\\n+\\n+1. **Allow All Tools (default)**: No filtering applied, agent has access to all available tools\\n+2. **Whitelist Mode**: Specify exact tools the agent can use (e.g., `['Read', 'Grep']`)\\n+3. **Exclusion Mode**: Block specific tools using `!` prefix (e.g., `['!Edit', '!Write']`)\\n+4. **Raw AI Mode**: Disable all tools for pure conversational interactions\\n+\\n+```yaml\\n+steps:\\n+  # Whitelist specific tools only\\n+  restricted-analysis:\\n+    type: ai\\n+    prompt: \\\"Analyze the codebase structure\\\"\\n+    ai:\\n+      provider: anthropic\\n+      allowedTools: ['Read', 'Grep', 'Glob']  # Only these tools allowed\\n+\\n+  # Exclude specific tools\\n+  safe-review:\\n+    type: ai\\n+    prompt: \\\"Review code without making changes\\\"\\n+    ai:\\n+      provider: google\\n+      allowedTools: ['!Edit', '!Write', '!Delete']  # Block modification tools\\n+\\n+  # Raw AI mode - no tools\\n+  conversational:\\n+    type: ai\\n+    prompt: \\\"Explain the architecture\\\"\\n+    ai:\\n+      provider: openai\\n+      disableTools: true  # Pure conversation, no tool access\\n+\\n+  # Alternative raw AI mode\\n+  conversational-alt:\\n+    type: ai\\n+    prompt: \\\"Explain the architecture\\\"\\n+    ai:\\n+      provider: anthropic\\n+      allowedTools: []  # Empty array also disables all tools\\n+```\\n+\\n+**MCP Tool Filtering:**\\n+\\n+Filter external Model Context Protocol tools using the `mcp__` prefix pattern:\\n+\\n+```yaml\\n+steps:\\n+  mcp-filtered:\\n+    type: ai\\n+    prompt: \\\"Search the codebase\\\"\\n+    ai:\\n+      provider: anthropic\\n+      allowedTools: ['mcp__code-search__*']  # Allow all code-search MCP tools\\n+      mcpServers:\\n+        code-search:\\n+          command: \\\"npx\\\"\\n+          args: [\\\"-y\\\", \\\"@modelcontextprotocol/server-code-search\\\"]\\n+```\\n+\\n+**When to use tool filtering:**\\n+- Restrict agent capabilities for security-sensitive tasks\\n+- Prevent unintended file modifications\\n+- Create specialized agents with limited toolsets\\n+- Testing and debugging specific tool interactions\\n+- Compliance requirements that limit agent autonomy\\n+\\n+**Security Note:** Tool filtering is enforced at runtime through system message filtering. Always combine with other security measures like `allowedFolders` for defense in depth.\\n+\\n #### Task Delegation (`enableDelegate`)\\n \\n Enable the delegate tool to allow AI agents to break down complex tasks and distribute them to specialized subagents for parallel processing. This feature is available when using Probe as the AI provider (Google Gemini, Anthropic Claude, OpenAI GPT, AWS Bedrock).\\n@@ -237,10 +351,118 @@ steps:\\n \\n **Note:** Task delegation increases execution time and token usage, but can provide more thorough analysis for complex tasks.\\n \\n+#### Bash Command Execution (`allowBash` / `bashConfig`)\\n+\\n+Enable secure bash command execution for AI agents to run read-only commands and analyze system state. This feature is disabled by default for security and requires explicit opt-in.\\n+\\n+**Simple Configuration:**\\n+\\n+Use `allowBash: true` for basic bash command execution with default safe commands:\\n+\\n+```yaml\\n+steps:\\n+  # Simple: Enable bash with default safe commands\\n+  git-status-analysis:\\n+    type: ai\\n+    prompt: \\\"Analyze the project structure and git status\\\"\\n+    ai:\\n+      provider: anthropic\\n+      model: claude-3-opus\\n+      allowBash: true  # Simple one-line enable\\n+```\\n+\\n+**Advanced Configuration:**\\n+\\n+Use `bashConfig` for fine-grained control over bash command execution:\\n+\\n+```yaml\\n+steps:\\n+  # Advanced: Custom allow/deny lists\\n+  custom-bash-config:\\n+    type: ai\\n+    prompt: \\\"Run custom analysis commands\\\"\\n+    ai:\\n+      provider: google\\n+      allowBash: true  # Enable bash execution\\n+      bashConfig:\\n+        allow: ['npm test', 'npm run lint']  # Additional allowed commands\\n+        deny: ['npm install']  # Additional blocked commands\\n+        timeout: 30000  # 30 second timeout per command\\n+        workingDirectory: './src'  # Default working directory\\n+\\n+  # Advanced: Disable default filters (expert mode)\\n+  advanced-bash:\\n+    type: ai\\n+    prompt: \\\"Run advanced system commands\\\"\\n+    ai:\\n+      provider: anthropic\\n+      allowBash: true\\n+      bashConfig:\\n+        noDefaultAllow: true  # Disable default safe command list\\n+        noDefaultDeny: false  # Keep default dangerous command blocklist\\n+        allow: ['specific-command-1', 'specific-command-2']\\n+```\\n+\\n+**Configuration Options:**\\n+\\n+- **`allowBash`** (boolean): Simple toggle to enable bash command execution. Default: `false`\\n+- **`allow`** (string[]): Additional permitted command patterns (e.g., `['ls', 'git status']`)\\n+- **`deny`** (string[]): Additional blocked command patterns (e.g., `['rm -rf', 'sudo']`)\\n+- **`noDefaultAllow`** (boolean): Disable default safe command list (~235 commands). Default: `false`\\n+- **`noDefaultDeny`** (boolean): Disable default dangerous command blocklist (~191 patterns). Default: `false`\\n+- **`timeout`** (number): Execution timeout in milliseconds. Default: varies by ProbeAgent\\n+- **`workingDirectory`** (string): Base directory for command execution\\n+\\n+**Default Security:**\\n+\\n+ProbeAgent includes comprehensive security by default:\\n+- **Safe Commands** (~235): Read-only operations like `ls`, `cat`, `git status`, `npm list`, `grep`\\n+- **Blocked Commands** (~191): Dangerous operations like `rm -rf`, `sudo`, `npm install`, `curl`, system modifications\\n+\\n+**When to enable bash commands:**\\n+- System state analysis (git status, file listings, environment info)\\n+- Running read-only diagnostic commands\\n+- Executing test suites or linters\\n+- Analyzing build outputs or logs\\n+\\n+**When to keep bash disabled (default):**\\n+- Security-sensitive environments\\n+- Untrusted AI prompts or inputs\\n+- Code review without system access needs\\n+- Compliance requirements that prohibit command execution\\n+\\n+**Security Best Practices:**\\n+1. Always use the default allow/deny lists unless you have specific requirements\\n+2. Set reasonable timeouts to prevent long-running commands\\n+3. Use `workingDirectory` to restrict command execution scope\\n+4. Audit command patterns in your allow list regularly\\n+5. Test configuration in a safe environment first\\n+6. Review AI-generated commands before enabling in production\\n+\\n+**Example: Git Status Analysis**\\n+\\n+```yaml\\n+steps:\\n+  git-status-review:\\n+    type: ai\\n+    prompt: |\\n+      Analyze the current git status and provide insights:\\n+      - Check for uncommitted changes\\n+      - Review branch state\\n+      - Identify any potential issues\\n+    ai:\\n+      provider: anthropic\\n+      allowBash: true  # Simple enable\\n+      bashConfig:\\n+        allow: ['git log --oneline']  # Add custom git command\\n+        workingDirectory: '.'\\n+```\\n+\\n+**Security Note:** Bash command execution respects existing security boundaries and permissions. Commands run with the same privileges as the Visor process. Always review and test bash configurations before deploying to production environments.\\n+\\n ### Fallback Behavior\\n \\n If no key is configured, Visor falls back to fast, heuristic checks (simple patterns, basic style/perf). For best results, set a provider.\\n \\n ### MCP (Tools) Support\\n See docs/mcp.md for adding MCP servers (Probe, Jira, Filesystem, etc.).\\n-\\n\",\"status\":\"modified\"},{\"filename\":\"docs/command-provider.md\",\"additions\":1,\"deletions\":1,\"changes\":2,\"patch\":\"diff --git a/docs/command-provider.md b/docs/command-provider.md\\nindex a99ab51f..0240ca53 100644\\n--- a/docs/command-provider.md\\n+++ b/docs/command-provider.md\\n@@ -538,7 +538,7 @@ steps:\\n 1. **Use JSON output** when possible for better integration\\n 2. **Set appropriate groups** to organize related checks\\n 3. **Use tags** for filtering check execution\\n-4. **Handle errors gracefully** - consider using `|| true` for non-critical commands\\n+4. **Handle errors gracefully** - consider using `|| true` for info-mode commands\\n 5. **Keep commands simple** - complex logic should be in scripts\\n 6. **Use dependencies** to chain related commands\\n 7. **Set timeouts** for long-running commands if needed\\n\",\"status\":\"modified\"},{\"filename\":\"docs/custom-tools.md\",\"additions\":12,\"deletions\":0,\"changes\":424,\"patch\":\"diff --git a/docs/custom-tools.md b/docs/custom-tools.md\\nnew file mode 100644\\nindex 00000000..e0dd1159\\n--- /dev/null\\n+++ b/docs/custom-tools.md\\n@@ -0,0 +1,424 @@\\n+# Custom Tools in YAML Configuration\\n+\\n+## Overview\\n+\\n+Custom tools allow you to define reusable command-line tools directly in your YAML configuration. These tools can then be used in MCP (Model Context Protocol) blocks throughout your configuration, making it easy to integrate any command-line tool or script into your workflow.\\n+\\n+## Features\\n+\\n+- **Define tools in YAML**: No need to create separate scripts or programs\\n+- **Input validation**: Define JSON Schema for tool parameters\\n+- **Template support**: Use Liquid templates for dynamic command generation\\n+- **Transform outputs**: Process tool output with Liquid templates or JavaScript\\n+- **Reusable**: Define once, use multiple times across your configuration\\n+- **Importable**: Share tools across projects using the `extends` mechanism\\n+- **Type-safe**: Full TypeScript support with input/output schemas\\n+- **MCP-compatible**: Tools follow the Model Context Protocol specification\\n+\\n+## Basic Tool Definition\\n+\\n+```yaml\\n+tools:\\n+  my-tool:\\n+    name: my-tool\\n+    description: Description of what the tool does\\n+    exec: 'echo \\\"Hello World\\\"'\\n+```\\n+\\n+## Complete Tool Schema\\n+\\n+```yaml\\n+tools:\\n+  tool-name:\\n+    # MCP-compatible fields (these map directly to MCP tool interface)\\n+    name: tool-name                    # Required: Tool identifier (MCP: name)\\n+    description: Tool description       # Recommended: Human-readable description (MCP: description)\\n+\\n+    # Input schema (JSON Schema format) - MCP: inputSchema\\n+    # This follows the JSON Schema specification and is used for:\\n+    # 1. Validating tool inputs before execution\\n+    # 2. Providing type information to AI models\\n+    # 3. Auto-generating documentation\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        param1:\\n+          type: string\\n+          description: Parameter description  # Describe each parameter for AI models\\n+        param2:\\n+          type: number\\n+          description: Optional parameter\\n+      required: [param1]                     # List required parameters\\n+      additionalProperties: false            # Strict mode: reject unknown parameters\\n+\\n+    # Custom tool execution fields\\n+    exec: 'command {{ args.param1 }}'  # Required: Command to execute (supports Liquid)\\n+    stdin: '{{ args.param2 }}'         # Optional: Data to pipe to stdin (supports Liquid)\\n+\\n+    cwd: /path/to/directory            # Optional: Working directory\\n+    env:                                # Optional: Environment variables\\n+      MY_VAR: value\\n+\\n+    timeout: 30000                      # Optional: Timeout in milliseconds (default: 30000)\\n+    parseJson: true                     # Optional: Parse output as JSON\\n+\\n+    # Transform output with Liquid template\\n+    transform: '{ \\\"result\\\": {{ output | json }} }'\\n+\\n+    # OR transform with JavaScript\\n+    transform_js: |\\n+      return {\\n+        processed: output.trim().toUpperCase()\\n+      };\\n+\\n+    # Output schema for validation (optional) - MCP: outputSchema\\n+    # Not currently enforced but useful for documentation\\n+    outputSchema:\\n+      type: object\\n+      properties:\\n+        result:\\n+          type: string\\n+          description: The processed result\\n+```\\n+\\n+## MCP Compatibility\\n+\\n+Custom tools are designed to be fully compatible with the Model Context Protocol (MCP) specification. When you define a custom tool, it automatically becomes available as an MCP tool with the following mapping:\\n+\\n+| Custom Tool Field | MCP Tool Field | Purpose |\\n+|------------------|----------------|---------|\\n+| `name` | `name` | Unique identifier for the tool |\\n+| `description` | `description` | Human-readable description for AI models and documentation |\\n+| `inputSchema` | `inputSchema` | JSON Schema defining expected parameters |\\n+| `outputSchema` | `outputSchema` | JSON Schema for output validation (informational) |\\n+\\n+### Why MCP Compatibility Matters\\n+\\n+1. **AI Model Integration**: Tools with proper descriptions and schemas can be automatically understood and used by AI models\\n+2. **Type Safety**: Input schemas provide runtime validation and type checking\\n+3. **Documentation**: Schemas serve as self-documenting interfaces\\n+4. **Interoperability**: Tools can potentially be used with other MCP-compatible systems\\n+\\n+### Best Practices for MCP Compatibility\\n+\\n+1. **Always provide descriptions**: Help AI models understand what your tool does\\n+   ```yaml\\n+   tools:\\n+     analyze-code:\\n+       name: analyze-code\\n+       description: \\\"Analyzes source code for complexity metrics and potential issues\\\"\\n+   ```\\n+\\n+2. **Use detailed input schemas**: Include descriptions for each parameter\\n+   ```yaml\\n+   inputSchema:\\n+     type: object\\n+     properties:\\n+       file:\\n+         type: string\\n+         description: \\\"Path to the source code file to analyze\\\"\\n+       metrics:\\n+         type: array\\n+         description: \\\"List of metrics to calculate\\\"\\n+         items:\\n+           type: string\\n+           enum: [\\\"complexity\\\", \\\"lines\\\", \\\"dependencies\\\"]\\n+     required: [\\\"file\\\"]\\n+   ```\\n+\\n+3. **Consider output schemas**: While not enforced, they document expected outputs\\n+   ```yaml\\n+   outputSchema:\\n+     type: object\\n+     properties:\\n+       complexity:\\n+         type: number\\n+         description: \\\"Cyclomatic complexity score\\\"\\n+       issues:\\n+         type: array\\n+         description: \\\"List of detected issues\\\"\\n+   ```\\n+\\n+## Using Custom Tools\\n+\\n+Once defined, custom tools can be used in any MCP block by setting `transport: custom`:\\n+\\n+```yaml\\n+steps:\\n+  my-check:\\n+    type: mcp\\n+    transport: custom              # Use custom transport\\n+    method: my-tool                 # Tool name\\n+    methodArgs:                     # Tool arguments\\n+      param1: \\\"value1\\\"\\n+      param2: 42\\n+```\\n+\\n+## Template Context\\n+\\n+Tools have access to a rich template context through Liquid templates:\\n+\\n+### In `exec` and `stdin`:\\n+- `{{ args }}` - The arguments passed to the tool\\n+- `{{ pr }}` - Pull request information (number, title, author, etc.)\\n+- `{{ files }}` - List of files in the PR\\n+- `{{ outputs }}` - Outputs from previous checks\\n+- `{{ env }}` - Environment variables\\n+\\n+### In `transform` and `transform_js`:\\n+- All of the above, plus:\\n+- `{{ output }}` - The raw command output\\n+- `{{ stdout }}` - Standard output\\n+- `{{ stderr }}` - Standard error\\n+- `{{ exitCode }}` - Command exit code\\n+\\n+## Examples\\n+\\n+### 1. Simple Grep Tool\\n+\\n+```yaml\\n+tools:\\n+  grep-todos:\\n+    name: grep-todos\\n+    description: Find TODO comments in code\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        pattern:\\n+          type: string\\n+        files:\\n+          type: array\\n+          items:\\n+            type: string\\n+    exec: 'grep -n \\\"{{ args.pattern }}\\\" {{ args.files | join: \\\" \\\" }}'\\n+```\\n+\\n+### 2. JSON Processing Tool\\n+\\n+```yaml\\n+tools:\\n+  analyze-package:\\n+    name: analyze-package\\n+    description: Analyze package.json dependencies\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        file:\\n+          type: string\\n+    exec: 'cat {{ args.file }}'\\n+    parseJson: true\\n+    transform_js: |\\n+      const deps = Object.keys(output.dependencies || {});\\n+      const devDeps = Object.keys(output.devDependencies || {});\\n+      return {\\n+        totalDeps: deps.length + devDeps.length,\\n+        prodDeps: deps.length,\\n+        devDeps: devDeps.length\\n+      };\\n+```\\n+\\n+### 3. Multi-Step Tool with Error Handling\\n+\\n+```yaml\\n+tools:\\n+  build-and-test:\\n+    name: build-and-test\\n+    description: Build project and run tests\\n+    exec: |\\n+      npm run build && npm test\\n+    timeout: 300000  # 5 minutes\\n+    transform_js: |\\n+      if (exitCode !== 0) {\\n+        return {\\n+          success: false,\\n+          error: stderr || 'Build or tests failed'\\n+        };\\n+      }\\n+      return {\\n+        success: true,\\n+        output: output\\n+      };\\n+```\\n+\\n+### 4. Tool with Dynamic Command Generation\\n+\\n+```yaml\\n+tools:\\n+  flexible-linter:\\n+    name: flexible-linter\\n+    description: Run appropriate linter based on file type\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        file:\\n+          type: string\\n+    exec: |\\n+      {% assign ext = args.file | split: \\\".\\\" | last %}\\n+      {% case ext %}\\n+        {% when \\\"js\\\", \\\"ts\\\" %}\\n+          eslint {{ args.file }}\\n+        {% when \\\"py\\\" %}\\n+          pylint {{ args.file }}\\n+        {% when \\\"go\\\" %}\\n+          golint {{ args.file }}\\n+        {% else %}\\n+          echo \\\"No linter for .{{ ext }} files\\\"\\n+      {% endcase %}\\n+```\\n+\\n+## Tool Libraries and Extends\\n+\\n+### Creating a Tool Library\\n+\\n+Create a file with just tool definitions:\\n+\\n+```yaml\\n+# tools-library.yaml\\n+version: \\\"1.0\\\"\\n+\\n+tools:\\n+  tool1:\\n+    name: tool1\\n+    exec: 'command1'\\n+\\n+  tool2:\\n+    name: tool2\\n+    exec: 'command2'\\n+```\\n+\\n+### Importing Tools\\n+\\n+Use the `extends` mechanism to import tools:\\n+\\n+```yaml\\n+version: \\\"1.0\\\"\\n+extends: ./tools-library.yaml\\n+\\n+# Additional tools can be defined here\\n+tools:\\n+  local-tool:\\n+    name: local-tool\\n+    exec: 'local-command'\\n+\\n+# Use both imported and local tools\\n+steps:\\n+  check1:\\n+    type: mcp\\n+    transport: custom\\n+    method: tool1  # From tools-library.yaml\\n+\\n+  check2:\\n+    type: mcp\\n+    transport: custom\\n+    method: local-tool  # Defined locally\\n+```\\n+\\n+### Multiple Extends\\n+\\n+You can import from multiple sources:\\n+\\n+```yaml\\n+extends:\\n+  - ./base-tools.yaml\\n+  - ./security-tools.yaml\\n+  - https://example.com/shared-tools.yaml\\n+```\\n+\\n+Tools are merged with later sources overriding earlier ones.\\n+\\n+## Integration with Other Features\\n+\\n+### Using with forEach\\n+\\n+```yaml\\n+steps:\\n+  lint-all-files:\\n+    type: mcp\\n+    transport: custom\\n+    method: my-linter\\n+    forEach: \\\"{{ files }}\\\"\\n+    methodArgs:\\n+      file: \\\"{{ item.filename }}\\\"\\n+```\\n+\\n+### Conditional Execution\\n+\\n+```yaml\\n+steps:\\n+  optional-check:\\n+    type: mcp\\n+    transport: custom\\n+    method: my-tool\\n+    if: \\\"files.some(f => f.filename.endsWith('.js'))\\\"\\n+    methodArgs:\\n+      target: \\\"src/\\\"\\n+```\\n+\\n+### Chaining with on_success/on_failure\\n+\\n+```yaml\\n+steps:\\n+  main-check:\\n+    type: mcp\\n+    transport: custom\\n+    method: build-tool\\n+    on_success:\\n+      - type: mcp\\n+        transport: custom\\n+        method: test-tool\\n+    on_failure:\\n+      - type: mcp\\n+        transport: custom\\n+        method: cleanup-tool\\n+```\\n+\\n+## Best Practices\\n+\\n+1. **Use Input Schemas**: Always define `inputSchema` to validate tool inputs\\n+2. **Handle Errors**: Use `transform_js` to check exit codes and handle errors\\n+3. **Set Timeouts**: Configure appropriate timeouts for long-running commands\\n+4. **Parse JSON**: Use `parseJson: true` for tools that output JSON\\n+5. **Document Tools**: Provide clear descriptions for each tool\\n+6. **Create Libraries**: Group related tools in separate YAML files\\n+7. **Version Control**: Store tool libraries in version control for sharing\\n+8. **Test Tools**: Test tools independently before using in complex workflows\\n+\\n+## Security Considerations\\n+\\n+- Tools execute with the same permissions as the Visor process\\n+- Be cautious with user input in tool commands\\n+- Use input validation to prevent command injection\\n+- Avoid exposing sensitive data in tool outputs\\n+- Consider using environment variables for secrets\\n+\\n+## Troubleshooting\\n+\\n+### Tool Not Found\\n+\\n+If you get \\\"Tool not found\\\" errors:\\n+1. Ensure the tool is defined in the `tools` section\\n+2. Check that the tool name matches exactly\\n+3. Verify extends paths are correct\\n+\\n+### Command Failures\\n+\\n+For command execution issues:\\n+1. Test the command manually first\\n+2. Check working directory (`cwd`) settings\\n+3. Verify required binaries are installed\\n+4. Check timeout settings for long operations\\n+\\n+### Template Errors\\n+\\n+For Liquid template problems:\\n+1. Validate template syntax\\n+2. Check that variables exist in context\\n+3. Use filters correctly (e.g., `| json`, `| join`)\\n+\\n+### Transform Errors\\n+\\n+For JavaScript transform issues:\\n+1. Ensure valid JavaScript syntax\\n+2. Always return a value\\n+3. Handle undefined/null cases\\n+4. Use try-catch for error handling\\n\\\\ No newline at end of file\\n\",\"status\":\"added\"},{\"filename\":\"docs/default-output-schema.md\",\"additions\":1,\"deletions\":0,\"changes\":28,\"patch\":\"diff --git a/docs/default-output-schema.md b/docs/default-output-schema.md\\nnew file mode 100644\\nindex 00000000..c8754d5e\\n--- /dev/null\\n+++ b/docs/default-output-schema.md\\n@@ -0,0 +1,28 @@\\n+# Default Output Schema and Timestamps\\n+\\n+Visor normalizes check outputs to make prompts and history handling predictable. There are exactly two modes:\\n+\\n+1) Checks with a schema (e.g., `schema: {...}` on the check)\\n+- The provider's output shape is respected.\\n+- If the final output is an Object (map), Visor injects a `ts` field (milliseconds since epoch) if it is missing.\\n+- If the final output is a primitive or an array, it is passed through unchanged (no wrapping, no `ts`).\\n+\\n+2) Checks without a schema\\n+- Visor uses a default schema: `{ text: string, ts: number }`.\\n+- If the provider returned a primitive (string/number/boolean), it is wrapped into `{ text, ts }`.\\n+- If it returned an Object, Visor injects `ts` if it is missing.\\n+- If it returned an array, it is passed through unchanged.\\n+\\n+This normalization happens after the provider returns and before outputs are recorded into `outputs` and `outputs_history`.\\n+\\n+Why this exists\\n+- Prompts and templates can reliably access `.text` and `.ts` for no‚Äëschema checks (e.g., human‚Äëinput), and can still trust custom shapes for schema‚Äôd checks.\\n+- `ts` allows you to sort/merge histories across steps without bespoke engines or roles.\\n+\\n+Practical tips\\n+- Human input defaults to `{ text, ts }`. In Liquid, read `outputs_history.ask[i].text` safely with a fallback: `{% if u.text %}{{ u.text }}{% else %}{{ u }}{% endif %}` for legacy mocks.\\n+- For schema‚Äôd AI checks, add `ts` to your schema if you want it persisted by validators; otherwise Visor will add it at runtime (not validated).\\n+- Arrays are passed through untouched; if you need timestamps per item, include them in your own schema.\\n+\\n+Related\\n+- See `docs/human-input-provider.md` for default output shape of human input.\\n\",\"status\":\"added\"},{\"filename\":\"docs/dependencies.md\",\"additions\":1,\"deletions\":1,\"changes\":56,\"patch\":\"diff --git a/docs/dependencies.md b/docs/dependencies.md\\nindex c63995c4..19c44225 100644\\n--- a/docs/dependencies.md\\n+++ b/docs/dependencies.md\\n@@ -16,8 +16,13 @@ steps:\\n     group: code-review\\n     schema: code-review\\n     prompt: \\\"Comprehensive security analysis...\\\"\\n-    tags: [\\\"security\\\", \\\"critical\\\", \\\"comprehensive\\\"]\\n-    on: [pr_opened, pr_updated]\\n+    tags:\\n+      - security\\n+      - critical\\n+      - comprehensive\\n+    on:\\n+      - pr_opened\\n+      - pr_updated\\n     # No dependencies - runs first\\n \\n   performance:\\n@@ -25,8 +30,14 @@ steps:\\n     group: code-review\\n     schema: code-review\\n     prompt: \\\"Performance analysis...\\\"\\n-    tags: [\\\"performance\\\", \\\"fast\\\", \\\"local\\\", \\\"remote\\\"]\\n-    on: [pr_opened, pr_updated]\\n+    tags:\\n+      - performance\\n+      - fast\\n+      - local\\n+      - remote\\n+    on:\\n+      - pr_opened\\n+      - pr_updated\\n     # No dependencies - runs parallel with security\\n \\n   style:\\n@@ -34,17 +45,26 @@ steps:\\n     group: code-review\\n     schema: code-review\\n     prompt: \\\"Style analysis based on security findings...\\\"\\n-    tags: [\\\"style\\\", \\\"fast\\\", \\\"local\\\"]\\n-    on: [pr_opened]\\n-    depends_on: [security]  # Waits for security to complete\\n+    tags:\\n+      - style\\n+      - fast\\n+      - local\\n+    on:\\n+      - pr_opened\\n+    depends_on:\\n+      - security  # Waits for security to complete\\n \\n   architecture:\\n     type: ai\\n     group: code-review\\n     schema: code-review\\n     prompt: \\\"Architecture analysis building on previous checks...\\\"\\n-    on: [pr_opened, pr_updated]\\n-    depends_on: [security, performance]\\n+    on:\\n+      - pr_opened\\n+      - pr_updated\\n+    depends_on:\\n+      - security\\n+      - performance\\n ```\\n \\n ### Execution Flow\\n@@ -78,6 +98,24 @@ steps:\\n \\n Sometimes a check can proceed when any one of several upstream steps has completed successfully. Visor supports this with pipe‚Äëseparated tokens inside `depends_on`.\\n \\n+## Criticality and Gating\\n+\\n+`continue_on_failure` controls whether dependents may run after a failure ‚Äî it is a gating knob, not the definition of criticality. Classify steps by criticality (external | internal | policy | info) and derive defaults:\\n+\\n+- Critical: `continue_on_failure: false`, require `assume`/`guarantee`, tighter loop budgets, retries only for transient faults.\\n+- Non‚Äëcritical: may allow `continue_on_failure: true` to keep non‚Äëcritical branches moving.\\n+\\n+Example ‚Äî non‚Äëcritical branch that can proceed after a soft failure:\\n+```yaml\\n+steps:\\n+  summarize:\\n+    type: ai\\n+    tags:\\n+      - info\\n+    continue_on_failure: true\\n+    fail_if: \\\"(output.errors || []).length > 0\\\"\\n+```\\n+\\n ```yaml\\n checks:\\n   parse-issue:   { type: noop }\\n\",\"status\":\"modified\"},{\"filename\":\"docs/engine-state-machine-plan.md\",\"additions\":10,\"deletions\":0,\"changes\":333,\"patch\":\"diff --git a/docs/engine-state-machine-plan.md b/docs/engine-state-machine-plan.md\\nnew file mode 100644\\nindex 00000000..7bcf0f33\\n--- /dev/null\\n+++ b/docs/engine-state-machine-plan.md\\n@@ -0,0 +1,333 @@\\n+# Engine State Machine (v2) Research\\n+\\n+This note captures the current execution flow of the Visor engine, the surfaces where a feature flag can live, and an initial proposal for a bespoke state-machine-based Runner v2. The user-provided research artifact referenced in the request is not available inside this workspace yet, so there are a few open questions that we should fill in once we can read it.\\n+\\n+> Safety Model & Criticality (Summary)\\n+>\\n+> The engine follows a NASA‚Äëstyle safety model. Every check participates in a safety policy that depends on its criticality:\\n+>\\n+> - criticality (field on each check): external | internal | policy | info\\n+>   - external: mutates outside world (e.g., GitHub ops, HTTP methods ‚â† GET/HEAD)\\n+>   - internal: alters routing/fan‚Äëout (forEach parents, on_* with goto/run, memory used by guards)\\n+>   - policy: enforces permissions/policy (strong fail_if/guarantee)\\n+>   - info: read‚Äëonly compute\\n+>\\n+> Defaults derived from criticality:\\n+> - Critical (external/control‚Äëplane/policy): require `assume` (preconditions) and `guarantee` (postconditions); `continue_on_failure: false`; retries only for transient faults; tighter loop budgets; suppress downstream side‚Äëeffects when contracts fail.\\n+> - Non‚Äëcritical: contracts recommended; may allow `continue_on_failure: true`; standard budgets and retry bounds.\\n+>\\n+> Expressions apply at distinct phases:\\n+> - `if` (plan-time scheduling) ‚Üí `assume` (pre‚Äëexec) ‚Üí provider ‚Üí `guarantee`/`fail_if` (post‚Äëexec) ‚Üí transitions/goto.\\n+>\\n+> See also: docs/guides/fault-management-and-contracts.md for the full checklist and examples.\\n+\\n+## 1. Current Engine Map\\n+\\n+### 1.1 Entry points & global state\\n+- `CheckExecutionEngine` wires together the git analyzer, provider registry, failure evaluator, snapshot journal, memory store, telemetry, and GitHub clients (`src/check-execution-engine.ts:228-332`).\\n+- Per-run state is stored in multiple mutable maps/sets such as `forwardRunGuards`, `executionStats`, `outputHistory`, `runCounters`, and `journal`; `resetPerRunState` must be called before each grouped execution to avoid leakage (`src/check-execution-engine.ts:258-306`).\\n+- `executeChecks` is the legacy CLI-style entry point that still routes through the PR reviewer after a repo scan; it loads memory, tags, GitHub checks, and eventually calls `executeReviewChecks` (`src/check-execution-engine.ts:3645-3815`).\\n+\\n+### 1.2 Config-driven execution\\n+- Modern callers (CLI, GitHub Action, SDK, test runner) invoke `executeGroupedChecks` which handles tag/event filtering, debug visualization and GitHub context capture (`src/check-execution-engine.ts:3964-4250`).\\n+- `executeGroupedChecks` chooses between three paths:\\n+  1. `executeDependencyAwareChecks` when every requested check has config (`src/check-execution-engine.ts:3976-3986`).\\n+  2. `executeSingleGroupedCheck` when exactly one configured check remains (`src/check-execution-engine.ts:4291-4470`).\\n+  3. Provider fallbacks (`src/check-execution-engine.ts:3990-4079`) or reviewer fallback for legacy ‚Äúfocus‚Äù runs.\\n+- GitHub Action mode adds grouped comment posting and per-check CheckRun updates via `GitHubCheckService`.\\n+\\n+### 1.3 Dependency-aware runner\\n+- `executeDependencyAwareChecks` orchestrates ‚Äúwaves‚Äù of checks:\\n+  1. Expand the requested set with transitive dependencies, event/tag filters, and session reuse metadata (`src/check-execution-engine.ts:5132-5320`).\\n+  2. Build/validate the dependency graph using `DependencyResolver` (`src/check-execution-engine.ts:5321-5368`, `src/dependency-resolver.ts:1-150`).\\n+  3. Maintain `wave` counters that reschedule plan execution whenever routing (`on_fail`, `on_finish`, `goto`) requests a forward run. A wave resets dedupe state and replays the DAG (`src/check-execution-engine.ts:5384-5440`).\\n+  4. For each topological level, spawn tasks up to `maxParallelism`, honoring session reuse barriers, fail-fast, and debug pauses (`src/check-execution-engine.ts:5442-6188`).\\n+  5. After each pass, inspect flags like `onFailForwardRunSeen`/`onFinishForwardRunSeen` and rebuild the graph if goto changed the event or dependency set (`src/check-execution-engine.ts:6824-7072`).\\n+- Supporting helpers:\\n+  - `executeWithLimitedParallelism` implements the generic async task pool (`src/check-execution-engine.ts:3861-3926`).\\n+  - `shouldFailFast` inspects issue severities to stop the pool early (`src/check-execution-engine.ts:8942-8958`).\\n+\\n+### 1.4 Check lifecycle & routing\\n+- `runNamedCheck` is the central dispatcher used by dependency levels, routing hooks, and inline executions. It evaluates `if` guards, enforces per-scope run caps, calls `executeCheckInline`, evaluates `fail_if`, records stats/history, and schedules routing targets (`src/check-execution-engine.ts:1516-2050`).\\n+- `executeCheckInline` ensures dependencies are materialized (recursively running missing ancestors), handles forEach fan-out, commits results into the execution journal, and records output history (`src/check-execution-engine.ts:1047-1513`).\\n+- `scheduleForwardRun` is the goto implementation that replans subsets of the DAG, filtering by event triggers and respecting per-run dedupe guards (`src/check-execution-engine.ts:412-520`).\\n+- `on_finish` orchestration is split into `runOnFinishChildren`, `decideRouting`, and `computeAllValid` helpers under `src/engine/on-finish/*`. The main engine wires them up after all forEach items and dependents settle, reusing snapshot projections for context (`src/check-execution-engine.ts:2200-2350`).\\n+- `shouldRunCheck` (and the `FailureConditionEvaluator`) power `if`, `fail_if`, and manual gating for forEach dependents (`src/check-execution-engine.ts:4801-4870`).\\n+\\n+### 1.5 Supporting services\\n+- `MemoryStore` and helper functions provide scriptable scratch storage for checks and on_finish contexts (see injections in `composeOnFinishContext`, `src/check-execution-engine.ts:900-1010` and `src/engine/on-finish/utils.ts:1-160`).\\n+- `ExecutionJournal`/`ContextView` capture per-scope results for snapshot-based dependency evaluation; journals are reset each grouped run (`src/check-execution-engine.ts:170-210` and `src/snapshot-store.ts`).\\n+- Telemetry emitters (`emitNdjsonSpanWithEvents`, OTEL spans, `addEvent`) are sprinkled through `runNamedCheck`, forEach loops, and failure evaluators.\\n+- GitHub Check integration wires into `executeChecks` and `executeGroupedChecks` to create/update CheckRuns and annotate output (`src/check-execution-engine.ts:3670-3780`, `src/github-check-service.ts`).\\n+\\n+### 1.6 Observed pain points\\n+- Engine state is scattered across mutable maps/sets, which makes it hard to reason about transitions (e.g., `forwardRunGuards`, `postOnFinishGuards`, `gotoSuppressedChecks`).\\n+- Waves are implicit booleans rather than explicit states; nested goto/forward-run flows toggle flags that outer loops poll (`src/check-execution-engine.ts:5384-5455`, `src/check-execution-engine.ts:6824-6880`).\\n+- Routing logic (on_success/on_fail/on_finish) is interwoven with execution; suppression guards (one-shot tags, goto suppression when re-running for dependents) make control flow difficult to extend.\\n+- Debug visualizer and telemetry rely on ad-hoc hooks inside loops, increasing the risk of regressions when adjusting flow control.\\n+\\n+### 1.7 Capability coverage checklist\\n+| Capability / nuance | Current implementation | State machine accommodation |\\n+| --- | --- | --- |\\n+| Per-check `if` / `fail_if` gating | `shouldRunCheck`, `evaluateFailureConditions` (`src/check-execution-engine.ts:1516-2050`, `8975-9057`) | Model as routing sub-states: every `CheckCompleted` event flows through a deterministic `Routing` state that evaluates conditions before enqueuing follow-up events. |\\n+| forEach fan-out & `on_finish` loops | `executeCheckInline` + on_finish helpers (`src/check-execution-engine.ts:1047-1513`, `2200-2350`; `src/engine/on-finish/*`) | Represent forEach items as scoped dispatch records; let `WavePlanning` schedule per-item events and treat `on_finish` as a specialized routing transition capable of emitting `WaveRetry`. |\\n+| `goto` / `goto_event` / forward-run dedupe | `scheduleForwardRun` + guard sets (`src/check-execution-engine.ts:412-520`) | Replace guard sets with queue-level deduping: before enqueuing `ForwardRunRequested`, consult a hashed tuple `(target, event, scope)` to avoid re-scheduling. |\\n+| Session reuse & provider contexts | Session metadata assembled in `executeDependencyAwareChecks`; enforced via `runNamedCheck` sessionInfo | Store session provider relationships inside `EngineContext.checks` metadata so the dispatcher can force sequential execution whenever checks share a session. |\\n+| Memory store & outputs history | `MemoryStore`, `ExecutionJournal`, scoped snapshots threaded through the legacy engine | Keep `memory`/`journal` in `EngineContext`; have state transitions commit snapshots, ensuring history is still authoritative for `on_finish`, forEach gating, and goto history queries. |\\n+| Debug visualizer pause/resume | `pauseGate`/`DebugVisualizerServer` gating inside `executeGroupedChecks` (`src/cli-main.ts:820-880`) | Have `LevelDispatch` consult a pause controller before spawning tasks and mirror all `EngineEvent`s to the debug server for visualization. |\\n+| GitHub CheckRuns & PR comments | `initializeGitHubChecks`, `completeGitHubChecks*`, `reviewer.postReviewComment` | Treat these as side effects of `Init` and `Completed` states so both engines emit identical updates; attach `engine_mode` metadata to CheckRuns for observability. |\\n+| Human input provider prompts | CLI message plumbing + provider execution context | ExecutionContext plumbing is untouched; state machine just continues passing it into providers. |\\n+| Nested workflows | Workflow provider executing its own DAG (`src/providers/workflow-check-provider.ts`) | Covered in ¬ß3.6 ‚Äî the state machine will compile workflows into child graphs and run them under the same scheduler. |\\n+| Telemetry spans / metrics | `emitNdjsonSpanWithEvents`, OTEL instrumentation sprinkled through current engine | Emit spans when entering/exiting each state (`state_from`, `state_to`, `engine_mode` attrs) so trace density stays similar but more structured. |\\n+\\n+## 2. Feature-flag surfaces\\n+\\n+### 2.1 CLI\\n+- Commander definition lives in `src/cli.ts`; new flags are added to both `setupProgram` and `parseArgs` builders (`src/cli.ts:19-141`).\\n+- Parsed options are consumed in `src/cli-main.ts`: after config discovery the engine is instantiated and `executeGroupedChecks` is called (`src/cli-main.ts:784-910`).\\n+- `CliOptions` type in `src/types/cli.ts` must include any new flag (`src/types/cli.ts:17-74`).\\n+\\n+### 2.2 GitHub Action\\n+- `src/index.ts` creates a `CheckExecutionEngine` around line 716 and always runs grouped execution (`src/index.ts:680-820`).\\n+- Inputs are read via `@actions/core.getInput`; introducing an input such as `state-machine` or honoring `VISOR_STATE_MACHINE` would happen alongside other inputs near the top of `run()` (`src/index.ts:40-140`).\\n+\\n+### 2.3 SDK & tooling\\n+- The published SDK (`src/sdk.ts:1-120`) exposes `runChecks` which instantiates the engine directly and calls `executeChecks`.\\n+- Dev/test scripts (`scripts/dev-run.ts`, `scripts/simulate-gh-run.ts`) and sample SDK programs also instantiate the engine manually (see `rg \\\"new CheckExecutionEngine\\\"`).\\n+- The YAML test runner (`src/test-runner/index.ts:100-220` and `:780+`) needs a flip to point at the new engine when the flag is turned on so regression suites can exercise both paths.\\n+\\n+### 2.4 Proposed gating strategy\\n+- Introduce an `EngineMode = 'legacy' | 'state-machine'` option accepted by `CheckExecutionEngine` (constructor or `execute*` methods). Default stays `'legacy'`.\\n+- CLI: add `--state-machine` (and env `VISOR_STATE_MACHINE=1`) that sets `engineMode` before instantiating the engine. The flag needs to propagate to **all** CLI commands (`visor`, `visor test`, `visor validate`, etc.), so `handleTestCommand` in `src/cli-main.ts` should accept/pass it through to the YAML test runner, ensuring regression suites exercise both engines.\\n+- GitHub Action: follow the CLI behavior if either a new input (`state-machine: true`) or `VISOR_STATE_MACHINE` env var is present; bubble the mode into the Action comment/telemetry payload so we can detect it in logs.\\n+- SDK/test runner/scripts: accept an optional `engineMode` option so programmatic callers can participate in canaries without CLI flags.\\n+- Telemetry/debug: add the mode to spans (e.g., `visor.run.engine_mode`) to keep traces filterable.\\n+- Initial implementation can keep mode selection localized to a helper such as `createExecutionEngine({ mode })` so that non-flagged call sites continue using the current class.\\n+\\n+## 3. State machine proposal\\n+\\n+### 3.1 Goals\\n+1. Make control flow explicit (well-defined states, transitions, and events).\\n+2. Preserve existing semantics (dependency gating, forEach fan-out, routing) while simplifying mental load.\\n+3. Unlock incremental features (pause/resume, deterministic tracing, easier retries) without relying on xstate or third-party interpreters.\\n+\\n+### 3.2 Candidate states & transitions\\n+| State | Responsibility | Exits |\\n+| --- | --- | --- |\\n+| `Init` | Capture CLI/action options, hydrate config, construct helpers, reset journals. | `PlanReady` once config + inputs validated. |\\n+| `PlanReady` | Build dependency graph, expand checks, compute per-step metadata (tags, sessions, fan-out). | `WavePlanning` (success) / `Error`. |\\n+| `WavePlanning` | Inspect outstanding events (forward runs, goto, manual triggers) and queue the next wave‚Äôs DAG snapshot. | `LevelDispatch` when a wave is ready, `Completed` when no work remains. |\\n+| `LevelDispatch` | Pop the next topological level, spawn tasks up to `maxParallelism`, hand them to `CheckRunning`. | `CheckRunning` for each task, `WavePlanning` once the level finishes. |\\n+| `CheckRunning` | Run provider + check logic, emit stats, assemble `CheckResult`. | `Routing` with success/failure payload, or `Error`. |\\n+| `Routing` | Evaluate `fail_if`, `on_success`, `on_fail`, `on_finish` triggers; enqueue new events (`ForwardRun`, `WaveRetry`, `GotoEvent`). | `WavePlanning` when new work scheduled, `Completed` if this was the final sink. |\\n+| `Completed` | Finalize stats, flush telemetry/CheckRuns, surface grouped results. | terminal |\\n+| `Error` | Capture fatal issues and unwind (shielding CLI/action from partial states). | terminal |\\n+\\n+Events flowing between states include: `PlanBuilt`, `WaveRequested`, `LevelDepleted`, `CheckComplete`, `CheckErrored`, `ForwardRunRequested`, `OnFinishLoop`, and `Shutdown`.\\n+\\n+### 3.3 Runtime data model\\n+- `EngineContext` struct holding: immutable config snapshot, dependency graph, check metadata (tags, triggers, session provider, fan-out mode), `ExecutionJournal`, output/memory stores, telemetry sinks, and persistence hooks.\\n+- `RunState` struct capturing: current engine mode, pending events queue, active wave number, active levels, outstanding tasks, global flags (e.g., `failFastTriggered`), GitHub check bookkeeping, and debug server hooks. The struct is designed to be serializable so we can persist/resume executions.\\n+- `DispatchRecord` capturing per-check data (scope path, provider id, start ts, attempts, forEach item index) to tie stats/telemetry to state transitions.\\n+- Event queue implementation (simple array or deque) so routing can push `ForwardRun`/`GotoEvent` events instead of toggling booleans. The queue doubles as the source for a structured event log, enabling time-travel debugging.\\n+\\n+TypeScript sketch:\\n+\\n+```ts\\n+type EngineMode = 'legacy' | 'state-machine';\\n+\\n+interface EngineContext {\\n+  mode: EngineMode;\\n+  config: VisorConfig;\\n+  dependencyGraph: DependencyGraph;\\n+  checks: Record<string, {\\n+    tags: string[];\\n+    triggers: EventTrigger[];\\n+    group?: string;\\n+    sessionProvider?: string;\\n+    fanout?: 'map' | 'reduce';\\n+    providerType: string;\\n+  }>;\\n+  journal: ExecutionJournal;\\n+  memory: MemoryStore;\\n+  telemetry: TelemetrySink;\\n+  gitHubChecks?: GitHubCheckService;\\n+  persistence?: {\\n+    saveState: (state: SerializedRunState) => Promise<void>;\\n+    loadState?: () => Promise<SerializedRunState | null>;\\n+  };\\n+}\\n+\\n+interface RunState {\\n+  wave: number;\\n+  levelQueue: ExecutionGroup[];\\n+  eventQueue: EngineEvent[];\\n+  activeDispatches: Map<string, DispatchRecord>;\\n+  flags: {\\n+    failFastTriggered: boolean;\\n+    forwardRunRequested: boolean;\\n+  };\\n+  stats: Map<string, CheckExecutionStats>;\\n+  historyLog: EngineEvent[]; // append-only log for time-travel debugging\\n+}\\n+\\n+type EngineEvent =\\n+  | { type: 'ForwardRunRequested'; target: string; gotoEvent?: EventTrigger; scope?: ScopePath }\\n+  | { type: 'WaveRetry'; reason: 'on_fail' | 'on_finish' | 'external' }\\n+  | { type: 'CheckScheduled'; checkId: string; scope: ScopePath }\\n+  | { type: 'CheckCompleted'; checkId: string; scope: ScopePath; result: ReviewSummary }\\n+  | { type: 'CheckErrored'; checkId: string; scope: ScopePath; error: SerializedError }\\n+  | { type: 'StateTransition'; from: EngineStateId; to: EngineStateId }\\n+  | { type: 'Shutdown'; error?: SerializedError };\\n+\\n+interface DispatchRecord {\\n+  id: string;\\n+  scope: ScopePath;\\n+  provider: string;\\n+  startMs: number;\\n+  attempts: number;\\n+  foreachIndex?: number;\\n+  sessionInfo?: { parent?: string; reuse?: boolean };\\n+}\\n+\\n+type SerializedRunState = {\\n+  wave: number;\\n+  levelQueue: ExecutionGroup[];\\n+  eventQueue: EngineEvent[];\\n+  flags: RunState['flags'];\\n+  stats: CheckExecutionStats[];\\n+  historyLog: EngineEvent[];\\n+};\\n+```\\n+\\n+Persistence/time-travel strategy (debugger-only for now):\\n+- When the debug visualizer is enabled, after every state transition the engine appends to `historyLog`, streams the event over the WebSocket, mirrors it to `output/debug-events/<session>.jsonl`, and (optionally) flushes the minimal `SerializedRunState` via `persistence.saveState`. By default we persist under `tmp/visor-state/<session>.json` (override via config/env if needed). Outside of debugger mode we skip persistence/log mirroring to avoid overhead.\\n+- During a debug resume, the engine loads the last serialized state, reconstructs in-memory maps, and continues dequeuing events, ensuring retries and routing decisions survive restarts within the debugging session.\\n+\\n+### 3.4 Migration strategy\\n+1. **Scaffolding:** introduce `EngineMode` flag plus a skeleton `StateMachineExecutionEngine` that simply proxies to the legacy runner; wire the flag through CLI/Action/SDK/tests.\\n+2. **State-core:** implement new state machine that supports dependency execution without routing (no goto/on_finish yet) and hide it behind the flag for targeted tests.\\n+3. **Routing parity:** port `scheduleForwardRun`, `runNamedCheck`, and `on_finish` semantics into state transitions; reuse existing helper functions where practical to avoid regressions.\\n+4. **Observability:** add structured tracing for state transitions so we can debug the new engine with the debug visualizer and OTEL spans.\\n+5. **Canary & cleanup:** run regression suites in both modes, flip CI to exercise the state machine on dedicated jobs, and deprecate legacy-only code once parity is proven.\\n+\\n+### 3.5 Open questions / follow-ups\\n+- Need access to the user‚Äôs research doc (currently outside the workspace) to reconcile requirements or additional tasks that were listed there.\\n+- Confirm how the debug visualizer wants to tap into the new state transitions (current server polls spans in `executeGroupedChecks`). We likely need a small event bus that mirrors `EngineEvent`s to the WebSocket server and OTEL spans (e.g., `visorevent.state_transition` with `{ from, to, checkId }` attributes).\\n+- Decide whether GitHub Action inputs should expose a first-class `state-machine` boolean or rely on env vars.\\n+- Determine whether we want to version the engine externally (e.g., `engine: v2` in config) once the flag stabilizes, or keep CLI-only toggles.\\n+\\n+Once the missing research document is accessible we should merge those findings into this plan, update the open questions list, and refine the migration steps accordingly.\\n+\\n+### 3.6 Nested workflows and reusable DAGs\\n+Visor already supports nested workflows via `type: 'workflow'` checks and the `WorkflowRegistry`/`WorkflowExecutor` (`src/workflow-registry.ts`, `src/providers/workflow-check-provider.ts`). Today those executions run entirely inside the provider, which means:\\n+\\n+- The outer engine treats the workflow check as a single node even though the workflow definition contains its own dependency graph, inputs, overrides, and potentially recursive workflow references.\\n+- Nested workflow iterations cannot benefit from core-engine features like pause/resume, telemetry, or goto semantics unless the workflow provider re-implements them.\\n+\\n+For the state-machine engine we should:\\n+\\n+1. Expose a ‚Äúsubgraph‚Äù capability so a workflow definition can be compiled into an internal DAG and scheduled as a child `EngineContext`. That keeps a single state abstraction whether we are running top-level checks or workflow steps.\\n+2. Carry parent scope into the child `RunState` so results from workflow steps register under meaningful journal scopes (`workflow:stepA@item1`).\\n+3. Allow workflows to emit their own `ForwardRunRequested`/`WaveRetry` events that bubble up to the parent queue. This prevents nested workflows from deadlocking when they need to re-run ancestor steps.\\n+4. Document limits (depth, fan-out) so that arbitrarily nested workflows do not starve the scheduler. We can enforce a `maxWorkflowDepth` (default 3) in `RunState.flags`.\\n+\\n+Implementation strategy:\\n+- Start by projecting a workflow definition into the same `DependencyGraph` structure the main engine uses (the registry already validates steps and dependencies).\\n+- When the workflow provider is invoked in state-machine mode, hand the projected graph to the engine instead of running it privately; the provider becomes a thin adapter that returns the child engine‚Äôs aggregated `ReviewSummary`.\\n+- For compatibility, keep the current ‚Äúself-contained workflow execution‚Äù path in legacy mode until all workflows are verified under the state machine.\\n+\\n+## 4. Rollout & testing milestones\\n+\\n+| Milestone | Description | Test strategy |\\n+| --- | --- | --- |\\n+| **M0 ‚Äì Flag plumbing & proxy** ‚úÖ DONE | Add `EngineMode`, CLI flag/env plumbing (including `visor test`), and a proxy state-machine runner that simply delegates to the legacy engine so tooling can toggle the mode. | Update Jest/YAML harnesses to accept `engineMode`. CI continues running legacy-only while we ensure the flag wires through all commands. |\\n+| **M1 ‚Äì Core state machine (no routing)** ‚úÖ DONE | Implement Init ‚Üí Plan ‚Üí Wave ‚Üí Level ‚Üí Check ‚Üí Completed transitions covering dependency expansion, fail-fast, stats, GitHub checks, debug pause, but still delegate routing (`goto`, `on_finish`) to legacy helpers. | Run the entire suite twice (legacy + state-machine) via a CI matrix; YAML tests remain unchanged‚Äîthey're just invoked under both modes. Add targeted unit tests for queue/dispatch logic. |\\n+| **M2 ‚Äì Routing & forEach parity** ‚úÖ DONE | Port `scheduleForwardRun`, `on_fail`, `on_success`, `on_finish`, and full forEach fan-out into the state machine; ensure flags/guards map to structured events. | Keep dual-mode CI. Add focused e2e tests covering routing loops, fail_if gating, and forEach retries, plus assertions on emitted `EngineEvent`s. |\\n+| **M3 ‚Äì Nested workflows** ‚úÖ DONE | Allow the workflow provider to hand child DAGs to the state machine, enforce depth/fan-out limits, propagate journal scopes. | Re-run existing workflow YAML suites under both modes; add a couple of dedicated unit tests that assert depth enforcement and parent/child event propagation. |\\n+| **M4 ‚Äì Observability & default flip** ‚úÖ DONE | Stream `EngineEvent`s to the debug visualizer, enrich OTEL spans/check runs with `engine_mode`, remove legacy guard maps, and make the state machine the default once confidence is high. | Continue running a reduced legacy suite in CI until full deprecation; monitor telemetry dashboards for regressions before removing legacy mode entirely. |\\n+\\n+### Test philosophy\\n+- YAML-based regression suites **must not change**; they encode behavior, not engine internals. We simply re-run them with `--state-machine` (e.g., `node scripts/run-visor-tests.js --state-machine`) during rollout to prove parity.\\n+- Jest/unit/integration tests remain authoritative; we only add a handful of state-machine-specific cases (event queue ordering, wave retry limits) instead of duplicating every scenario.\\n+- CI should eventually run in a matrix (`ENGINE_MODE=legacy` vs `state-machine`) so every PR exercises both engines until we flip the default. This is easier than maintaining a completely separate test suite.\\n+\\n+## 5. Toward structured, NASA-style guarantees\\n+\\n+One of the driving reasons for this rewrite is to reach NASA-like rigor: strong separation of concerns, declarative control flow, and statically checkable contracts. The state machine gives us the runtime substrate; we‚Äôll complement it with two configuration-level enhancements.\\n+\\n+### 5.1 Declarative transitions instead of ad-hoc `goto`\\n+We will keep `goto` / `goto_js` fully functional for backwards compatibility‚Äînothing is removed‚Äîbut we‚Äôll introduce a structured transition DSL that offers better static guarantees. Example:\\n+\\n+```yaml\\n+on_finish:\\n+  transitions:\\n+    - when: \\\"wave('validate-fact').invalid_count > 0 && event.name == 'issue_opened'\\\"\\n+      to: issue-assistant\\n+    - when: \\\"wave('validate-fact').invalid_count > 0 && event.name == 'issue_comment'\\\"\\n+      to: comment-assistant\\n+    - when: \\\"wave('validate-fact').invalid_count == 0\\\"\\n+      to: null\\n+```\\n+\\n+Plan:\\n+1. Extend the config schema with optional `transitions[]` entries (fields: `when`, `to`, optional metadata). During the `Routing` state, the engine evaluates `when` expressions in priority order and enqueues the resulting transition.\\n+2. Build a static validator that ensures each `to` refers to an existing check (or `null`), expressions only use approved helpers (`wave`, `event`, `outputs`, `memory`, etc.), and that transitions either cover all cases or explicitly fall back to `null`.\\n+3. When both `goto`/`goto_js` and `transitions` are present, the state machine honors `transitions` first (still executing the others as a fallback) and logs a warning so we can gradually migrate built-in configs away from dynamic `goto_js` without breaking existing flows.\\n+\\n+### 5.2 Assume/guarantee contracts\\n+To support design-by-contract we‚Äôll let checks declare assumptions about their inputs and guarantees about their outputs:\\n+\\n+```yaml\\n+extract-facts:\\n+  guarantee:\\n+    - \\\"Array.isArray(output) && output.every(f => f.id && f.claim)\\\"\\n+\\n+validate-fact:\\n+  assume:\\n+    - \\\"typeof extract-facts.item.id === 'string'\\\"\\n+  guarantee:\\n+    - \\\"typeof output.fact_id === 'string' && output.fact_id === extract-facts.item.id\\\"\\n+```\\n+\\n+Execution steps:\\n+1. Extend `CheckConfig` with optional `assume[]` and `guarantee[]` arrays. Before executing a provider, the state machine evaluates `assume` expressions using dependency outputs (and forEach item context); failures short-circuit execution with a structured issue referencing the violated assumption. After execution, it evaluates `guarantee` expressions against the check‚Äôs output and records fatal issues if they fail.\\n+2. Add compile-time validation: parse each expression and ensure it only references known symbols. For example, `assume` can read `dependencyName.output`, `dependencyName.item`, or `memory` but cannot mutate state; `guarantee` can read outputs but not future steps.\\n+3. Emit telemetry (`engine.contract.assume_failed`, `engine.contract.guarantee_failed`) so CI and runtime monitoring can flag contract regressions.\\n+\\n+By lifting control flow into `transitions` and correctness rules into `assume`/`guarantee`, we make configurations statically analyzable, reduce reliance on imperative `goto_js`, and move closer to NASA-inspired static validation goals while still honoring legacy constructs for advanced scenarios.\\n+## 5. Routing and Loops (spec to impl status)\\n+\\n+- on_success/on_fail evaluate `run`, `run_js`, and `goto` (with optional `goto_event`).\\n+- on_finish for forEach parents is processed after children complete; loop budget enforced.\\n+\\n+### 5.1 Declarative transitions (implemented)\\n+\\n+In addition to `goto`/`goto_js`, checks can use declarative transitions on `on_success`, `on_fail`, and `on_finish`:\\n+\\n+```\\n+on_finish:\\n+  transitions:\\n+    - when: \\\"any(outputs_history['validate-fact'], v => v.is_valid === false) && event.name === 'issue_opened'\\\"\\n+      to: issue-assistant\\n+    - when: \\\"any(outputs_history['validate-fact'], v => v.is_valid === false) && event.name === 'issue_comment'\\\"\\n+      to: comment-assistant\\n+```\\n+\\n+- Rules evaluate in order; first true wins. Use `to: null` to explicitly do nothing.\\n+- Backward compatible: if `transitions` is omitted or none match, the engine falls back to `goto_js/goto`.\\n+- Helpers available: `outputs`, `outputs_history`, `output`, `event`, `memory`, plus `any/all/none/count`.\\n+\\n+### 5.2 Assume/Guarantee contracts (implemented)\\n+\\n+Per-check contracts:\\n+\\n+```\\n+assume:\\n+  - \\\"env.NODE_ENV === 'ci'\\\"                 # preconditions ‚Äì if any is false, skip with skipReason=assume\\n+guarantee:\\n+  - \\\"Array.isArray(output.items)\\\"           # postconditions ‚Äì violations add error issues (contract/guarantee_failed)\\n+```\\n+\\n+- `assume` is evaluated pre-execution; skipped checks are recorded and visible in stats/history.\\n+- `guarantee` is evaluated post-execution; violations are non-fatal by default (routing unaffected) but produce issues.\\n\",\"status\":\"added\"},{\"filename\":\"docs/failure-conditions-schema.md\",\"additions\":1,\"deletions\":1,\"changes\":32,\"patch\":\"diff --git a/docs/failure-conditions-schema.md b/docs/failure-conditions-schema.md\\nindex 54436221..91c07636 100644\\n--- a/docs/failure-conditions-schema.md\\n+++ b/docs/failure-conditions-schema.md\\n@@ -46,7 +46,9 @@ steps:\\n     prompt: \\\"Analyze for security vulnerabilities...\\\"\\n     group: review\\n     schema: code-review\\n-    on: [pr_opened, pr_updated]\\n+    on:\\n+      - pr_opened\\n+      - pr_updated\\n \\n     # Check-specific failure conditions override global ones\\n     failure_conditions:\\n@@ -58,7 +60,9 @@ steps:\\n     prompt: \\\"Analyze performance implications...\\\"\\n     group: review\\n     schema: code-review\\n-    on: [pr_opened, pr_updated]\\n+    on:\\n+      - pr_opened\\n+      - pr_updated\\n \\n     # Inherits global failure conditions unless overridden\\n     failure_conditions:\\n@@ -112,7 +116,9 @@ steps:\\n   security:\\n     type: ai\\n     prompt: \\\"Security analysis...\\\"\\n-    on: [pr_opened, pr_updated]\\n+    on:\\n+      - pr_opened\\n+      - pr_updated\\n \\n # Enhanced format with failure conditions\\n version: \\\"1.0\\\"\\n@@ -123,7 +129,23 @@ steps:\\n   security:\\n     type: ai\\n     prompt: \\\"Security analysis...\\\"\\n-    on: [pr_opened, pr_updated]\\n+    on:\\n+      - pr_opened\\n+      - pr_updated\\n+\\n+## Interaction with Criticality\\n+\\n+Failure conditions (`fail_if`) and design‚Äëby‚Äëcontract (`assume`, `guarantee`) work together with criticality:\\n+\\n+- Critical steps (external/control‚Äëplane/policy):\\n+  - Require meaningful `assume` and `guarantee`.\\n+  - `continue_on_failure: false` by default; dependents skip when this step fails.\\n+  - Retries only for transient provider faults; no auto‚Äëretry for logical failures (`fail_if`/`guarantee`).\\n+- Non‚Äëcritical steps:\\n+  - Contracts recommended; may allow `continue_on_failure: true`.\\n+  - Same retry bounds; tolerant gating.\\n+\\n+See docs/guides/fault-management-and-contracts.md for the full policy checklist and examples.\\n     failure_conditions:\\n       security_specific: \\\"metadata.errorIssues >= 1\\\"\\n ```\\n@@ -148,4 +170,4 @@ steps:\\n ```\\n \\n ### Step 3: Test and Refine\\n-Use debug mode to test conditions and refine expressions based on actual results.\\n\\\\ No newline at end of file\\n+Use debug mode to test conditions and refine expressions based on actual results.\\n\",\"status\":\"modified\"},{\"filename\":\"docs/failure-routing.md\",\"additions\":1,\"deletions\":1,\"changes\":2,\"patch\":\"diff --git a/docs/failure-routing.md b/docs/failure-routing.md\\nindex f0750863..fc3ce91d 100644\\n--- a/docs/failure-routing.md\\n+++ b/docs/failure-routing.md\\n@@ -127,7 +127,7 @@ Per-step actions:\\n - Retry: re-run the same step up to `retry.max`; backoff adds fixed or exponential delay with deterministic jitter.\\n - Run: on failure (or success), run listed steps first; if successful, the failed step is re-attempted once (failure path).\\n - Goto (ancestor-only): jump back to a previously executed dependency, then continue forward. On success, Visor re-runs the current step once after the jump.\\n-- Loop safety: `routing.max_loops` counts all routing transitions (runs, gotos, retries). Exceeding it aborts the current scope with a clear error.\\n+- Loop safety: `routing.max_loops` counts all routing transitions (runs, gotos, retries). Exceeding it aborts the current scope with a clear error. For a hard cap on repeated executions of the same step, see [Execution Limits](./limits.md).\\n - forEach: each item is isolated with its own loop/attempt counters; `*_js` receives `{ foreach: { index, total, parent } }`.\\n \\n ### Fan‚Äëout vs. Reduce (Phase 5)\\n\",\"status\":\"modified\"},{\"filename\":\"docs/goto-forward-run-plan.md\",\"additions\":4,\"deletions\":0,\"changes\":113,\"patch\":\"diff --git a/docs/goto-forward-run-plan.md b/docs/goto-forward-run-plan.md\\nnew file mode 100644\\nindex 00000000..ae637426\\n--- /dev/null\\n+++ b/docs/goto-forward-run-plan.md\\n@@ -0,0 +1,113 @@\\n+# Visor Engine Plan: Use `goto` for Looping on Failures\\n+\\n+This document captures the plan to simplify looping by using `goto` in `on_fail` and letting the engine re‚Äërun the dependent chain deterministically.\\n+\\n+## Background\\n+\\n+Today, builder YAML uses `on_fail.run: [agent-refine, agent-write, config-lint, tests-validate, agent-verify-tests, ‚Ä¶]` to bounce back through the pipeline. This is verbose and couples control‚Äëflow to YAML.\\n+\\n+Engine behavior:\\n+- `on_success.goto` performs a forward‚Äërun: it executes the `goto` target and all its dependents in topological order.\\n+- `on_fail.goto` is currently limited to ancestor targets and does not forward‚Äërun dependents.\\n+\\n+## Goal\\n+\\n+Allow clean, minimal YAML that uses only `goto` for looping:\\n+- Validators: `on_fail: goto: agent-refine` (or directly `goto: agent-write`).\\n+- Refine: `on_success: goto: agent-write`.\\n+\\n+The engine should handle re‚Äërunning the necessary chain; YAML should not list the entire sequence.\\n+\\n+## Proposed Engine Changes\\n+\\n+1) Unify `goto` semantics across origins\\n+- Make `goto` perform the same forward‚Äërun whether invoked from `on_success`, `on_fail`, or `on_finish`.\\n+- Factor shared code into a helper (e.g., `scheduleForwardRun(target, opts)`), currently implemented only inside the `on_success.goto` branch.\\n+\\n+2) Relax ancestor‚Äëonly restriction for `on_fail.goto`\\n+- Allow `goto` to any step in the DAG, not only ancestors.\\n+- Keep safety guards (below) to prevent runaway loops.\\n+\\n+3) Optional: Add anchors\\n+- Introduce `anchor: true` (or `loop_anchor: true`) on steps like `agent-write`.\\n+- If a validator has `on_fail` without explicit `goto`, engine can jump to the nearest anchor.\\n+\\n+4) Loop safety and predictability\\n+- Keep `routing.max_loops` budget (already implemented).\\n+- Respect `one_shot` tag: skip re‚Äërunning steps with `tags: [one_shot]` that already executed in this run.\\n+- Maintain per‚Äërun statistics to avoid duplicate scheduling within a wave.\\n+\\n+5) Forward‚Äërun details\\n+- From the target (e.g., `agent-write`), compute the dependent subgraph (topological order) honoring `depends_on` and event filters.\\n+- Preserve current event by default; honor `goto_event` only when explicitly provided.\\n+\\n+## YAML Patterns After the Change\\n+\\n+Pattern A (pure goto):\\n+- Validators (`config-lint`, `tests-validate`, `agent-verify-tests`):\\n+  ```yaml\\n+  on_fail:\\n+    goto: agent-refine\\n+  ```\\n+- Refine:\\n+  ```yaml\\n+  on_success:\\n+    goto: agent-write\\n+  ```\\n+\\n+Pattern B (single hop to anchor):\\n+- Validators:\\n+  ```yaml\\n+  on_fail:\\n+    goto: agent-write\\n+  ```\\n+\\n+## Migration Plan\\n+\\n+1) Engine implementation\\n+- Unify forward‚Äërun for `goto` in `on_fail` and `on_finish`.\\n+- Remove ancestor‚Äëonly restriction or gate it behind a feature flag (e.g., `VISOR_GOTO_GLOBAL=true`).\\n+- Extract forward‚Äërun into a shared helper.\\n+\\n+2) Builder YAML simplification\\n+- Replace `on_fail.run: [ ‚Ä¶full list‚Ä¶ ]` with `on_fail: goto: agent-refine`.\\n+- Keep `agent-refine on_success: goto: agent-write`.\\n+\\n+3) Tests (exact counts only)\\n+- Single‚Äëinvocation multi‚Äërefine (3 cycles):\\n+  - `refine = 3`, `write/lint/validate = 4`, `verify-tests = 3`, `code-review/cleanup/finish = 1`.\\n+- Flow tests (staged multi‚Äërefine) remain for readability.\\n+- Edge cases: loop budget exceeded, `one_shot` steps, event override.\\n+\\n+## Code Pointers\\n+\\n+File: `src/check-execution-engine.ts`\\n+- `executeWithRouting` ‚Äî handles `on_fail.run/goto` and `on_success.run/goto`:\\n+  - Unify forward‚Äërun behavior for `goto` across origins.\\n+  - Current forward‚Äërun logic lives in the `on_success.goto` branch (search for comments near topological ordering and `forwardSet`).\\n+- `runNamedCheck` ‚Äî respects `if` conditions and records stats; ensure forward‚Äërun uses consistent overlays/results.\\n+- Guards: `routing.max_loops`, `oncePerRun`/`one_shot` behavior, execution statistics.\\n+\\n+Suggested refactor:\\n+- Introduce `scheduleForwardRun(target, scope, opts)` used by all `goto` sites.\\n+- Extract subgraph building + topo sort into a helper for reuse.\\n+\\n+## Telemetry / Debug\\n+- Add concise debug logs for `goto` forward‚Äërun across all origins: target, number of dependents, topological order.\\n+- Keep existing OTEL and NDJSON traces.\\n+\\n+## Rollout\\n+- Implemented without a feature flag. Old configs are not broken: `goto` to\\n+  ancestors preserves the previous ‚Äúre-run ancestor only‚Äù behavior. New behavior\\n+  simply allows `goto` to any step and forward‚Äëruns its dependents when routing\\n+  to a non‚Äëancestor.\\n+- Builder YAML switched to pure `goto` in validators.\\n+\\n+## Open Questions\\n+- Should `goto` always forward‚Äërun, or only when target is an anchor? (Leaning: always forward‚Äërun for clarity.)\\n+- Should we auto‚Äëselect an anchor when `goto` target is omitted? (Future convenience.)\\n+\\n+## Acceptance Criteria\\n+- Single‚Äërun multi‚Äërefine test passes with exact counts.\\n+- Flow multi‚Äërefine tests pass.\\n+- No regression in existing suites; loop budget respected.\\n\",\"status\":\"added\"},{\"filename\":\"docs/guides/criticality-modes.md\",\"additions\":10,\"deletions\":0,\"changes\":332,\"patch\":\"diff --git a/docs/guides/criticality-modes.md b/docs/guides/criticality-modes.md\\nnew file mode 100644\\nindex 00000000..94dd25a4\\n--- /dev/null\\n+++ b/docs/guides/criticality-modes.md\\n@@ -0,0 +1,332 @@\\n+# Criticality Modes ‚Äî External, Control‚ÄëPlane, Policy, Non‚ÄëCritical\\n+\\n+This document explains what each criticality mode means, how to declare it, the engine defaults it enables, and how core constructs (if, assume, guarantee, fail_if, transitions, retries, loop budgets) behave per mode. All examples use block‚Äëstyle YAML.\\n+\\n+> Assume vs. Guarantee ‚Äî Do‚Äôs and Don‚Äôts\\n+>\\n+> Do\\n+> - Use `assume` for pre‚Äëexecution prerequisites (env/memory/upstream), not this step‚Äôs `output`.\\n+> - Use `guarantee` for assertions about this step‚Äôs produced `output` (shape, fan‚Äëout size, control signals).\\n+> - Use `fail_if` for policy/threshold decisions.\\n+>\\n+> Don‚Äôt\\n+> - Don‚Äôt reference this step‚Äôs `output` in `assume`.\\n+> - Don‚Äôt mix policy thresholds into `guarantee`‚Äîuse `fail_if`.\\n+> - Don‚Äôt rely on time/random/network in expressions.\\n+\\n+## Why criticality?\\n+\\n+Criticality classifies a step by the operational risk it carries. The engine uses it to pick safe defaults for contracts, dependency gating, retries, and loop budgets. `continue_on_failure` only controls gating; it does not define criticality.\\n+\\n+Declare criticality on each check:\\n+```yaml\\n+checks:\\n+  some-step:\\n+    type: command\\n+    criticality: internal   # external | internal | policy | info\\n+```\\n+\\n+If the field is omitted, the engine may infer a default (mutating ‚Üí external; forEach parent or on_* goto/run ‚Üí control‚Äëplane; policy gates ‚Üí policy; else non‚Äëcritical). You can override any default on a per‚Äëcheck basis.\\n+\\n+### Mode selection ‚Äî quick checklist\\n+- Does this step mutate external state? ‚Üí external\\n+- Does it steer execution (fan‚Äëout, transitions/goto, sets flags for other guards)? ‚Üí internal\\n+- Does it enforce permissions/policy/compliance and gate external steps? ‚Üí policy\\n+- Otherwise, is it read‚Äëonly/pure and low‚Äërisk? ‚Üí info\\n+\\n+If in doubt, start with info and promote to policy/internal/external when you add gating, routing, or side‚Äëeffects.\\n+\\n+---\\n+\\n+## External\\n+\\n+Mutates systems outside the engine (GitHub ops, HTTP methods ‚â† GET/HEAD, file writes, ticket creation).\\n+\\n+Defaults\\n+- Contracts required: declare `assume` (preconditions) and `guarantee` (postconditions).\\n+- Gating: `continue_on_failure: false` by default; dependents skip when this step fails.\\n+- Retries: transient faults only, bounded (max 2‚Äì3 with backoff); no auto‚Äëretry for logical (policy/contract) violations, including `contract/schema_validation_failed`.\\n+- Loop budget: standard (10) unless the step also routes.\\n+- Side‚Äëeffects: suppress/postpone mutating actions when `guarantee`/`fail_if` fail; require remediation/approval.\\n+\\n+Recommended contracts\\n+- assume examples: authenticated/authorized; dry‚Äërun disabled when posting; rate‚Äëlimit budget present.\\n+- guarantee examples: created resource IDs present; idempotency markers written; invariants about payload size/format.\\n+\\n+Example ‚Äî safely posting a PR comment\\n+```yaml\\n+checks:\\n+  post-comment:\\n+    type: github\\n+    criticality: external\\n+    on:\\n+      - pr_opened\\n+    op: comment.create\\n+    assume:\\n+      - \\\"isMember()\\\"\\n+      - \\\"env.DRY_RUN !== 'true'\\\"\\n+    guarantee:\\n+      - \\\"output && typeof output.id === 'number'\\\"\\n+    continue_on_failure: false\\n+    on_fail:\\n+      retry: { max: 2, backoff: { mode: exponential, delay_ms: 1200 } }\\n+```\\n+\\n+### When to pick EXTERNAL (real‚Äëlife)\\n+- GitHub comment/label/edit operations (comment.create, labels.add/remove).\\n+- HTTP webhooks that mutate (POST/PUT/PATCH/DELETE) ‚Äî Slack messages, PagerDuty incidents, Notion/Linear/Jira ticket creation.\\n+- File system writes (artifact publishing, changelog generation into repo) or any step that makes persistent changes.\\n+- Git operations that change state (push, tag, merge) ‚Äî if ever enabled, treat as external by default.\\n+\\n+---\\n+\\n+## Control‚ÄëPlane\\n+\\n+Steers execution (decides what runs next and how often). Examples: forEach parents, steps with on_* transitions/goto/run, memory/flags used by conditions.\\n+\\n+Defaults\\n+- Contracts required (route integrity): meaningful `assume` and `guarantee`.\\n+- Gating: `continue_on_failure: false` by default.\\n+- Retries: transient faults only (provider crashes); no auto‚Äëretry for logical violations (including `contract/schema_validation_failed`).\\n+- Loop budgets: tighter per‚Äëscope (recommended 8) to avoid oscillations.\\n+\\n+Recommended contracts\\n+- assume: pre‚Äëexecution prerequisites independent of this step's own output (e.g., env/memory flags, upstream readiness).\\n+- guarantee: postconditions about this step's produced control signals and shape/size caps (e.g., arrays, max fan‚Äëout, valid transition targets).\\n+\\n+Example ‚Äî fan‚Äëout producer with loopback transitions\\n+```yaml\\n+routing:\\n+  max_loops: 8\\n+\\n+checks:\\n+  extract-items:\\n+    type: command\\n+    criticality: internal\\n+    exec: \\\"node -e \\\\\\\"console.log('[\\\\\\\\\\\"a\\\\\\\\\\\",\\\\\\\\\\\"b\\\\\\\\\\\",\\\\\\\\\\\"c\\\\\\\\\\\"]')\\\\\\\"\\\"\\n+    forEach: true\\n+    guarantee:\\n+      - \\\"Array.isArray(output)\\\"\\n+      - \\\"output.every(x => typeof x === 'string')\\\"\\n+      - \\\"output.length <= 100\\\"           # size cap belongs in guarantee (post‚Äëexec)\\n+    on_finish:\\n+      transitions:\\n+        - when: \\\"any(outputs_history['validate'], v => v && v.ok === false)\\\"\\n+          to: remediate\\n+\\n+  validate:\\n+    type: command\\n+    depends_on:\\n+      - extract-items\\n+    fanout: map\\n+    exec: node scripts/validate.js\\n+\\n+  remediate:\\n+    type: command\\n+    exec: node scripts/fix.js\\n+```\\n+\\n+### When to pick CONTROL‚ÄëPLANE (real‚Äëlife)\\n+- A forEach parent that fans out work to child steps (e.g., facts, files, services, directories, modules).\\n+- An aggregator that computes a run decision (`all_valid`, `needs_retry`, `next_targets`) and routes via transitions.\\n+- A small `memory`/`log`/`script` step that sets flags used by `if/assume/guarantee` on other checks (e.g., `needs_retry=true`).\\n+- Workflow orchestration steps whose purpose is routing/looping rather than producing user-facing output.\\n+\\n+---\\n+\\n+## Policy\\n+\\n+Enforces permissions, compliance, or organizational policy (e.g., reviewer must be MEMBER, commit message format, license checks). Often gates external actions even if it doesn‚Äôt mutate itself.\\n+\\n+Defaults\\n+- Contracts required; strict handling of logical violations.\\n+- Gating: `continue_on_failure: false` by default.\\n+- Retries: do not auto‚Äëretry logical failures; only retry transient provider errors.\\n+\\n+Recommended contracts\\n+- assume: environment/org state required for policy checks.\\n+- guarantee: policy pass/fail booleans, lists of violations, etc.\\n+\\n+Example ‚Äî permission gate that blocks labels/comments on failure\\n+```yaml\\n+checks:\\n+  permission-check:\\n+    type: command\\n+    criticality: policy\\n+    exec: node scripts/check-permissions.js    # -> { allowed: boolean }\\n+    guarantee:\\n+      - \\\"typeof output.allowed === 'boolean'\\\"\\n+\\n+  post-label:\\n+    type: github\\n+    depends_on:\\n+      - permission-check\\n+    criticality: external\\n+    op: labels.add\\n+    values:\\n+      - \\\"reviewed\\\"\\n+    if: \\\"outputs['permission-check'].allowed === true\\\"   # only proceed when policy passes\\n+```\\n+\\n+### When to pick POLICY (real‚Äëlife)\\n+- Permission & role checks that gate external actions (only MEMBERS may post/label).\\n+- Compliance or guardrail checks (branch protection, commit message format, DCO/CLA verification) that block mutating steps.\\n+- Change‚Äëmanagement windows (e.g., ‚Äúno posts on weekends‚Äù), environment gates (PROD vs. STAGING), or organization‚Äëwide safety toggles.\\n+\\n+---\\n+\\n+## Non‚ÄëCritical\\n+\\n+Pure/read‚Äëonly compute where failures don‚Äôt risk unsafe behavior.\\n+\\n+Defaults\\n+- Contracts recommended (not mandatory).\\n+- Gating: `continue_on_failure: true` allowed if safe.\\n+- Retries: bounded; can be slightly looser.\\n+\\n+Example ‚Äî summary step that won‚Äôt block the pipeline\\n+```yaml\\n+checks:\\n+  summarize:\\n+    type: ai\\n+    criticality: info\\n+    on:\\n+      - pr_opened\\n+      - pr_updated\\n+    continue_on_failure: true\\n+    fail_if: \\\"(output.errors || []).length > 0\\\"\\n+```\\n+\\n+### When to pick NON‚ÄëCRITICAL (real‚Äëlife)\\n+- Read‚Äëonly analysis and summaries (lint, style, performance hints, PR summary) where failure should not block critical tasks.\\n+- Exploratory AI steps that help humans but don‚Äôt gate or mutate (draft review comments in dry‚Äërun; heuristics, suggestions).\\n+- Any leaf computation whose outputs aren‚Äôt consumed by control‚Äëplane or external steps.\\n+\\n+---\\n+\\n+## Construct Behavior by Mode\\n+\\n+- if (plan‚Äëtime): false/error ‚Üí skip in all modes. In critical branches, dependents should be gated so mutating steps do not run.\\n+- assume (pre‚Äëexec): false/error ‚Üí skip before provider call. In critical modes this should block downstream mutators; use a guard step if you need a hard failure.\\n+- guarantee (post‚Äëexec): violation ‚Üí failure, add `contract/guarantee_failed`, route `on_fail`. In critical modes, block mutating side‚Äëeffects until remediated.\\n+- fail_if (post‚Äëexec): true ‚Üí failure, route `on_fail`. Do not auto‚Äëretry logical failures in critical modes.\\n+- transitions/goto: prefer declarative transitions; enforce loop budgets (default 10; recommended 8 for control‚Äëplane fan‚Äëouts).\\n+\\n+Numeric defaults (recommended)\\n+- Retries: max 3 (non‚Äëcritical), max 2‚Äì3 (critical), exponential backoff with jitter.\\n+- Loop budget: 10 (default), 8 for control‚Äëplane branches.\\n+\\n+---\\n+\\n+## Patterns & Guardrails\\n+\\n+- Guard step for hard‚Äëfail on unmet preconditions\\n+```yaml\\n+checks:\\n+  prechecks:\\n+    type: command\\n+    exec: node scripts/check-tools.js   # exit 1 when tools missing\\n+    fail_if: \\\"output.exitCode !== 0\\\"\\n+  analyze:\\n+    type: command\\n+    depends_on:\\n+      - prechecks\\n+    exec: node scripts/analyze.js\\n+```\\n+\\n+- Side‚Äëeffect suppression\\n+  - Ensure mutating steps depend on the critical gate/contract step so failures/violations block posting.\\n+\\n+- Determinism\\n+  - Keep routing/contract expressions pure (no time/random/network), with short evaluation timeouts.\\n+\\n+---\\n+\\n+## See also\\n+- docs/guides/fault-management-and-contracts.md ‚Äî full safety checklist, behavior matrix, and examples\\n+- docs/engine-state-machine-plan.md ‚Äî engine phases, routing, and loop budgets\\n+\\n+## Comprehensive Example ‚Äî End‚Äëto‚ÄëEnd Flow Using All Primitives\\n+\\n+```yaml\\n+version: \\\"1.0\\\"\\n+\\n+routing:\\n+  max_loops: 8\\n+\\n+checks:\\n+  extract-facts:\\n+    type: command\\n+    criticality: internal\\n+    on:\\n+      - issue_opened\\n+      - issue_comment\\n+    exec: \\\"node -e \\\\\\\"console.log('[{\\\"\\\"id\\\"\\\":1,\\\"\\\"claim\\\"\\\":\\\"\\\"A\\\"\\\"},{\\\"\\\"id\\\"\\\":2,\\\"\\\"claim\\\"\\\":\\\"\\\"B\\\"\\\"}]')\\\\\\\"\\\"\\n+    forEach: true\\n+    guarantee:\\n+      - \\\"Array.isArray(output)\\\"\\n+      - \\\"output.every(x => typeof x.id === 'number' && typeof x.claim === 'string')\\\"\\n+      - \\\"output.length <= 50\\\"\\n+    on_finish:\\n+      transitions:\\n+        - when: \\\"any(outputs_history['validate-fact'], v => v && v.is_valid === false) && event.name === 'issue_opened'\\\"\\n+          to: issue-assistant\\n+        - when: \\\"any(outputs_history['validate-fact'], v => v && v.is_valid === false) && event.name === 'issue_comment'\\\"\\n+          to: comment-assistant\\n+\\n+  validate-fact:\\n+    type: command\\n+    depends_on:\\n+      - extract-facts\\n+    fanout: map\\n+    exec: node scripts/validate-fact.js\\n+    fail_if: \\\"output && output.is_valid === false\\\"\\n+    on_fail:\\n+      retry: { max: 1, backoff: { mode: exponential, delay_ms: 1000 } }\\n+\\n+  aggregate:\\n+    type: command\\n+    criticality: internal\\n+    depends_on:\\n+      - validate-fact\\n+    exec: node scripts/aggregate-validity.js   # -> { all_valid: boolean }\\n+    guarantee:\\n+      - \\\"output && typeof output.all_valid === 'boolean'\\\"\\n+    on_success:\\n+      transitions:\\n+        - when: \\\"output.all_valid === true\\\"\\n+          to: permission-check\\n+\\n+  permission-check:\\n+    type: command\\n+    criticality: policy\\n+    exec: node scripts/check-permissions.js    # -> { allowed: boolean }\\n+    guarantee:\\n+      - \\\"typeof output.allowed === 'boolean'\\\"\\n+\\n+  post-comment:\\n+    type: github\\n+    criticality: external\\n+    depends_on:\\n+      - permission-check\\n+    on:\\n+      - issue_opened\\n+    if: \\\"outputs['permission-check'] && outputs['permission-check'].allowed === true\\\"\\n+    assume:\\n+      - \\\"outputs['permission-check'] && outputs['permission-check'].allowed === true\\\"\\n+      - \\\"env.DRY_RUN !== 'true'\\\"\\n+    op: comment.create\\n+    guarantee:\\n+      - \\\"output && typeof output.id === 'number'\\\"\\n+    continue_on_failure: false\\n+\\n+  summarize:\\n+    type: ai\\n+    criticality: info\\n+    on:\\n+      - issue_opened\\n+    continue_on_failure: true\\n+    fail_if: \\\"(output.errors || []).length > 0\\\"\\n+```\\n+\\n+This scenario demonstrates all primitives across modes: control‚Äëplane fan‚Äëout + transitions, policy gating, external action with contracts, and a non‚Äëcritical leaf that may fail softly.\\n\",\"status\":\"added\"},{\"filename\":\"docs/guides/fault-management-and-contracts.md\",\"additions\":21,\"deletions\":0,\"changes\":738,\"patch\":\"diff --git a/docs/guides/fault-management-and-contracts.md b/docs/guides/fault-management-and-contracts.md\\nnew file mode 100644\\nindex 00000000..f2376f3f\\n--- /dev/null\\n+++ b/docs/guides/fault-management-and-contracts.md\\n@@ -0,0 +1,738 @@\\n+# Visor Engine: Fault Management, Contracts, and Transitions (NASA‚Äëstyle)\\n+\\n+This guide consolidates the expected behavior for conditional gating, design‚Äëby‚Äëcontract, routing, and retries in the state‚Äëmachine engine. It follows safety‚Äëcritical software principles (detect ‚Üí isolate ‚Üí recover ‚Üí report) while remaining practical for CI/PR automation.\\n+\\n+> Assume vs. Guarantee ‚Äî Do‚Äôs and Don‚Äôts\\n+>\\n+> Do\\n+> - Use `assume` for pre‚Äëexecution prerequisites that do not depend on this step‚Äôs output (env, memory, upstream results).\\n+> - Keep expressions pure (no time/random/network); short and deterministic.\\n+> - Use `guarantee` to assert properties of this step‚Äôs produced output (shape, size caps, idempotency markers, control signals).\\n+> - For critical steps, pair both: `assume` (preflight) + `guarantee` (post‚Äëexec safety lock).\\n+>\\n+> Don‚Äôt\\n+> - Don‚Äôt reference `output` of the same step in `assume` (it runs before execution).\\n+> - Don‚Äôt put policy thresholds into `guarantee`‚Äîuse `fail_if` for policy/quality gates.\\n+> - Don‚Äôt rely on side‚Äëeffects or external clocks in expressions.\\n+\\n+## Core Principles\\n+- Deterministic evaluations: expressions are pure (no side‚Äëeffects, time/network), evaluated in a sandbox.\\n+- Fail‚Äësecure defaults: evaluation errors pick the safest behavior (skip or fail closed), and are logged.\\n+- Bounded retries: never unbounded loops; per‚Äëscope caps and loop budgets.\\n+- Isolation: failures do not cascade unless explicitly permitted.\\n+- Auditability: every decision is journaled with cause, scope, and timestamps; JSON snapshots are exportable.\\n+\\n+## Criticality Model (What It Is, How To Declare It, What It Does)\\n+\\n+Criticality classifies a step by the operational risk it carries. The engine uses it to pick safe defaults for contracts, gating, retries, loop budgets, and side‚Äëeffects. `continue_on_failure` only controls dependency gating; it does not define criticality.\\n+\\n+Declare criticality on each check:\\n+```yaml\\n+checks:\\n+  post-comment:\\n+    type: github\\n+    criticality: external        # external | internal | policy | info\\n+```\\n+\\n+Meanings:\\n+- external\\n+  - Step mutates an external system (GitHub ops, HTTP methods ‚â† GET/HEAD, file writes).\\n+  - Defaults: contracts required; `continue_on_failure: false`; retries only for transient faults; tighter loop budgets; suppress downstream mutating actions when contracts or `fail_if` fail; idempotency or compensation expected.\\n+- internal\\n+  - Step drives routing or fan‚Äëout (forEach parents; on_* with goto/run; memory used by guards).\\n+  - Defaults: contracts required for route integrity; `continue_on_failure: false`; tighter loop budgets (recommended 8); retries only transient; treat loops/recirculation conservatively.\\n+- policy\\n+  - Step enforces permissions/compliance gates (permission checks, org policy, human‚Äëin‚Äëthe‚Äëloop approvals).\\n+  - Defaults: contracts required; `continue_on_failure: false`; logical violations are failures (no auto‚Äëretry), downstream mutating actions blocked until remediated.\\n+- info\\n+  - Read‚Äëonly or low‚Äërisk compute.\\n+  - Defaults: contracts recommended (not required); may allow `continue_on_failure: true`; standard loop budgets and retry bounds.\\n+\\n+Precedence & inference:\\n+- Explicit `criticality` on a check takes precedence over tags or heuristics.\\n+- If `criticality` is omitted, the engine may infer:\\n+  - mutating providers ‚Üí external; forEach parents or on_* goto/run ‚Üí control‚Äëplane; strong policy gates ‚Üí policy; otherwise non‚Äëcritical.\\n+\\n+Overriding defaults:\\n+- You can override any default (e.g., set `continue_on_failure: true` or adjust budgets) per check. Criticality sets sensible baselines; it does not lock you in.\\n+\\n+## Behavior Matrix by Construct and Criticality\\n+\\n+Below is the exact behavior for each construct depending on criticality. ‚ÄúSkip‚Äù means provider is not executed and it does not count as a run.\\n+\\n+- if (plan‚Äëtime)\\n+  - Non‚Äëcritical: false/error ‚Üí skip; dependents may run if OR‚Äëdeps satisfy or they are unrelated.\\n+  - Critical: same skip; because `continue_on_failure: false` is default, downstream mutators must depend on this step and will skip.\\n+\\n+- assume (pre‚Äëexec)\\n+  - Non‚Äëcritical: false/error ‚Üí skip (skipReason=assume); no retry.\\n+  - Critical: false/error ‚Üí skip and block downstream side‚Äëeffects via dependency gating. If you need an explicit failure (not a skip), add a guard step (see Example C below) or (optional) `assume_mode: 'fail'` when available.\\n+\\n+- guarantee (post‚Äëexec)\\n+  - Non‚Äëcritical: violation ‚Üí add `contract/guarantee_failed` issue; mark failure; route `on_fail`; no auto‚Äëretry unless remediation exists.\\n+  - Critical: violation ‚Üí mark failure; suppress downstream mutating actions (dependents should depend_on this step). Route `on_fail` to remediation; retries only for transient exec faults, not logical ones.\\n+\\n+- fail_if (post‚Äëexec)\\n+  - Non‚Äëcritical: true ‚Üí failure; bounded retry only for transient faults; otherwise remediation.\\n+  - Critical: true ‚Üí failure; do not auto‚Äëretry logical failures; block side‚Äëeffects; route remediation with tight caps.\\n+\\n+- transitions / goto\\n+  - Both: prefer declarative `transitions` first; respect per‚Äëscope loop budgets (default 10; critical recommended 8). Exceeding budget adds `routing/loop_budget_exceeded` and halts routing in that scope.\\n+\\n+Numeric defaults (recommended)\\n+- Retries: max 3 (non‚Äëcritical), max 2 (critical), exponential backoff with jitter (e.g., 1s, 2s, 4s ¬±10%).\\n+- Loop budgets: 10 (non‚Äëcritical), 8 (critical/control‚Äëplane branches).\\n+\\n+## Constructs and Expected Behavior\\n+\\n+### Quick reference: differences at a glance\\n+\\n+- Purpose\\n+  - `if`: Scheduling gate ‚Äî decides whether the step should be scheduled in this run.\\n+  - `assume`: Preconditions ‚Äî must hold immediately before executing the provider.\\n+  - `guarantee`: Postconditions ‚Äî must hold for the result the provider produced.\\n+  - `fail_if`: Failure detector ‚Äî declares which results count as failures.\\n+\\n+- When it runs\\n+  - `if`: before scheduling (earliest).\\n+  - `assume`: after scheduling, right before calling the provider.\\n+  - `guarantee`: immediately after provider returns.\\n+  - `fail_if`: immediately after provider returns (can co‚Äëexist with `guarantee`).\\n+\\n+- Inputs visible to the expression\\n+  - `if`: event, env, filesChanged meta, previous check outputs (current wave), memory (read‚Äëonly helpers).\\n+  - `assume`: same as `if`, plus fully resolved dependency results for this scope.\\n+  - `guarantee`/`fail_if`: same as `assume`, plus the step‚Äôs own output/result.\\n+\\n+- Effect on execution\\n+  - `if` false (or error): step is skipped and never scheduled.\\n+  - `assume` false (or error): step is skipped right before execution; provider is not called.\\n+  - `guarantee` violation: step has executed; violation adds issues; routes `on_fail`.\\n+  - `fail_if` true: step has executed; marks failure; routes `on_fail`.\\n+\\n+- Stats/journal\\n+  - `if`/`assume` skip: recorded as a skip; does not count as a run; journal contains an empty result entry.\\n+  - `guarantee`/`fail_if`: counted run; issues recorded; journal contains the full result.\\n+\\n+- Routing & dependents\\n+  - Skips (`if`/`assume`) propagate gating to dependents unless OR‚Äëdeps satisfy or `continue_on_failure` applies on an alternate path.\\n+  - Failures (`guarantee`/`fail_if`) route via `on_fail` with bounded retries/remediation.\\n+\\n+When to choose which\\n+- Use `if` when you can decide at plan time whether a step should even be considered (tags, events, coarse repo conditions).\\n+- Use `assume` when prerequisites depend on dynamic dependencies or environment right before execution (e.g., tools bootstrapped by a `prepare` step).\\n+- Use `guarantee` when the provider must produce outputs that satisfy invariants (shape, counts, idempotency confirmations).\\n+- Use `fail_if` when policy/thresholds on the produced results define failure (test counts, lints, security finding thresholds).\\n+\\n+### 1) `if` (pre‚Äërun gate)\\n+- Purpose: schedule a step only when conditions are met (event, env, prior outputs).\\n+- Behavior:\\n+  - `if` true ‚Üí run.\\n+  - `if` false or evaluation error ‚Üí skip with reason `if_condition` (does not count as a run); dependents skip unless alternate OR‚Äëdeps satisfy.\\n+- Example:\\n+```yaml\\n+checks:\\n+  lint:\\n+    type: command\\n+    on:\\n+      - pr_opened\\n+      - pr_updated\\n+    if: \\\"filesCount > 0 && env.CI === 'true'\\\"\\n+    exec: npx eslint .\\n+```\\n+\\n+### 2) `assume` (preconditions, design‚Äëby‚Äëcontract)\\n+- Purpose: non‚Äënegotiable prerequisites before a step executes.\\n+- Behavior:\\n+- Any `assume` expression false ‚Üí skip with reason `assume`. In critical branches, this blocks dependent mutating steps via dependency gating.\\n+- No automatic retry unless a defined remediation can satisfy the precondition.\\n+\\n+Important: `assume` runs before the provider; do not reference this step‚Äôs own `output` inside `assume`. Use dependency results (e.g., `outputs['dep']`) or environment/memory. Assertions about this step‚Äôs produced data belong in `guarantee`.\\n+- Example with remediation:\\n+```yaml\\n+checks:\\n+  prepare-env:\\n+    type: command\\n+    exec: node scripts/bootstrap.js\\n+  analyze:\\n+    type: command\\n+    depends_on:\\n+      - prepare-env\\n+    assume:\\n+      - \\\"env.TOOLING_READY === 'true'\\\"\\n+      - \\\"Array.isArray(outputs_history['prepare-env']) ? true : true\\\"\\n+    exec: node scripts/analyze.js\\n+```\\n+\\n+### 3) `guarantee` (postconditions, design‚Äëby‚Äëcontract)\\n+- Purpose: invariants that must hold after a step completes.\\n+- Behavior:\\n+  - Violations add issues with ruleId `contract/guarantee_failed`, mark failure, and route via `on_fail`.\\n+  - In critical branches, violation blocks downstream mutating actions (dependents should be gated on this step) and is not auto‚Äëretried as a logical failure.\\n+- Example:\\n+```yaml\\n+checks:\\n+  summarize:\\n+    type: command\\n+    exec: \\\"node -e \\\\\\\"console.log('{\\\\\\\\\\\"items\\\\\\\\\\\":[1,2,3]}')\\\\\\\"\\\"\\n+    guarantee:\\n+      - \\\"output && Array.isArray(output.items)\\\"\\n+      - \\\"output.items.length > 0\\\"\\n+    on_fail:\\n+      run:\\n+        - recompute\\n+  recompute:\\n+    type: command\\n+    exec: node scripts/recompute.js\\n+```\\n+\\n+### 4) `fail_if` (post‚Äërun failure detector)\\n+- Purpose: codifies ‚Äúthis result means failure.‚Äù\\n+- Behavior:\\n+  - If true ‚Üí mark step failed, append `<check>_fail_if` issue, and route `on_fail`.\\n+  - Evaluation errors ‚Üí log, treat as not triggered (prefer separate system issue).\\n+- Example with bounded retry/backoff:\\n+```yaml\\n+checks:\\n+  tests:\\n+    type: command\\n+    exec: npm test -- --runInBand\\n+    fail_if: \\\"output.summary.failed > 0\\\"\\n+    on_fail:\\n+      retry: { max: 2, backoff: { mode: exponential, delay_ms: 1000 } }\\n+      run:\\n+        - collect-logs\\n+  collect-logs:\\n+    type: command\\n+    exec: node scripts/collect-logs.js\\n+```\\n+\\n+## Declarative Transitions (on_success / on_fail / on_finish)\\n+Use transitions for clear, testable routing without inline JS logic. If none match, the engine falls back to `goto_js/goto`.\\n+\\n+Helpers available inside `when`: `outputs`, `outputs_history`, `output`, `event`, `memory`, plus `any/all/none/count`.\\n+\\n+### Example ‚Äî Fact Validation Loopback\\n+```yaml\\n+checks:\\n+  extract-facts:\\n+    type: ai\\n+    forEach: true\\n+    on_finish:\\n+      transitions:\\n+        - when: \\\"any(outputs_history['validate-fact'], v => v && v.is_valid === false) && event.name === 'issue_opened'\\\"\\n+          to: issue-assistant\\n+        - when: \\\"any(outputs_history['validate-fact'], v => v && v.is_valid === false) && event.name === 'issue_comment'\\\"\\n+          to: comment-assistant\\n+\\n+  validate-fact:\\n+    type: ai\\n+    depends_on:\\n+      - extract-facts\\n+\\n+  issue-assistant:\\n+    type: ai\\n+    on_success:\\n+      transitions:\\n+        - when: \\\"event.name === 'issue_comment' && output?.intent === 'comment_retrigger'\\\"\\n+          to: overview\\n+          goto_event: pr_updated\\n+```\\n+\\n+## Critical vs Non‚ÄëCritical Steps\\n+\\n+A step is critical when it meets any of:\\n+- External side effects (mutating: GitHub ops, HTTP methods ‚â† GET/HEAD, file writes).\\n+- Control‚Äëplane impact (forEach parents, on_* that drive goto/run, memory used by conditions).\\n+- Safety/policy gates (permission checks, strong `fail_if`/`guarantee`).\\n+- Irreversible/noisy effects (user‚Äëvisible posts, ticket creation).\\n+\\n+Pragmatic marking today:\\n+- Use `tags: [critical]` (and optionally `internal`, `external`).\\n+- Heuristics: treat mutating providers as critical by default.\\n+\\n+Policy matrix (default)\\n+- Non‚Äëcritical: `assume` skip (no retry); `guarantee` ‚Üí issues + on_fail; `fail_if` ‚Üí failure; retries only for transient faults.\\n+- Critical: `assume` violation blocks dependents; `guarantee` violations prevent downstream side‚Äëeffects; `fail_if` retried only if transient; tighter loop budgets.\\n+\\n+## Criticality vs. `continue_on_failure`\\n+\\n+`continue_on_failure` is a dependency‚Äëgating knob: it decides whether dependents may run after this step fails. It does not fully define criticality. A NASA‚Äëstyle notion of criticality also governs contracts, retries, loop budgets, side‚Äëeffect controls, and escalation paths.\\n+\\n+Recommended practice:\\n+\\n+- Use `continue_on_failure` to control gating per edge, but classify steps explicitly as critical or not.\\n+- Express criticality today via tags, and (optionally) promote to a dedicated field later.\\n+\\n+### Expressing criticality (current config)\\n+\\n+- Using tags (immediately usable):\\n+```yaml\\n+checks:\\n+  post-comment:\\n+    type: github\\n+    tags:\\n+      - critical\\n+      - external\\n+    on:\\n+      - pr_opened\\n+    op: comment.create\\n+    assume:\\n+      - \\\"env.ALLOW_POST === 'true'\\\"\\n+    guarantee:\\n+      - \\\"typeof output.id === 'number'\\\"\\n+    continue_on_failure: false\\n+    on_fail:\\n+      retry: { max: 2, backoff: { mode: exponential, delay_ms: 1500 } }\\n+```\\n+\\n+- Using a proposed field (future‚Äëproof, clearer intent):\\n+```yaml\\n+checks:\\n+  label:\\n+    type: github\\n+    criticality: external   # or: internal | policy | info\\n+    on:\\n+      - pr_opened\\n+    op: labels.add\\n+    values:\\n+      - \\\"reviewed\\\"\\n+    assume: \\\"isMember()\\\"\\n+    guarantee: \\\"Array.isArray(output.added) && output.added.includes('reviewed')\\\"\\n+```\\n+\\n+Engine policy derived from criticality (summary):\\n+\\n+- Critical (external/control‚Äëplane/policy):\\n+  - require meaningful `assume` and `guarantee`.\\n+  - `continue_on_failure: false` by default.\\n+  - retries only for transient faults, with tight caps and backoff.\\n+  - lower routing loop budgets for branches this step drives.\\n+  - suppress downstream mutating side‚Äëeffects when guarantees fail.\\n+- Non‚Äëcritical:\\n+  - `assume`/`guarantee` recommended but not mandatory.\\n+  - may set `continue_on_failure: true` to keep non‚Äëcritical branches running.\\n+\\n+### Concrete examples\\n+\\n+1) Non‚Äëcritical compute that may fail without stopping the pipeline:\\n+```yaml\\n+checks:\\n+  summarize:\\n+    type: ai\\n+    tags:\\n+      - info\\n+    on:\\n+      - pr_opened\\n+      - pr_updated\\n+    continue_on_failure: true\\n+    fail_if: \\\"(output.errors || []).length > 0\\\"\\n+```\\n+\\n+2) External (critical) ‚Äî posting a PR comment with strict contracts and bounded retries:\\n+```yaml\\n+checks:\\n+  post-comment:\\n+    type: github\\n+    tags:\\n+      - critical\\n+      - external\\n+    on:\\n+      - pr_opened\\n+    op: comment.create\\n+    assume:\\n+      - \\\"isMember()\\\"\\n+      - \\\"env.DRY_RUN !== 'true'\\\"\\n+    guarantee:\\n+      - \\\"output && typeof output.id === 'number'\\\"\\n+    continue_on_failure: false\\n+    on_fail:\\n+      retry: { max: 2, backoff: { mode: exponential, delay_ms: 1200 } }\\n+```\\n+\\n+3) Control‚Äëplane (critical) ‚Äî forEach parent that drives routing with a tighter loop budget:\\n+```yaml\\n+routing:\\n+  max_loops: 8   # lower than default for safety on control‚Äëplane flows\\n+\\n+checks:\\n+  extract-items:\\n+    type: command\\n+    tags:\\n+      - critical\\n+      - internal\\n+    exec: \\\"node -e \\\\\\\"console.log('[\\\\\\\\\\\"a\\\\\\\\\\\",\\\\\\\\\\\"b\\\\\\\\\\\",\\\\\\\\\\\"c\\\\\\\\\\\"]')\\\\\\\"\\\"\\n+    forEach: true\\n+    on_finish:\\n+      transitions:\\n+        - when: \\\"any(outputs_history['validate'], x => x && x.ok === false)\\\"\\n+          to: remediate\\n+\\n+  validate:\\n+    type: command\\n+    depends_on:\\n+      - extract-items\\n+    fanout: map\\n+    exec: node scripts/validate.js\\n+\\n+  remediate:\\n+    type: command\\n+    exec: node scripts/fix.js\\n+```\\n+\\n+## Retries, Loop Budgets, and ForEach\\n+- Retries: bounded (e.g., max 3), exponential backoff with jitter; per‚Äëscope attempt counters stored in memory.\\n+- Routing loop budget: `routing.max_loops` (default 10) per scope; exceeding emits `routing/loop_budget_exceeded` and halts routing for that scope.\\n+- ForEach fan‚Äëout:\\n+  - Per‚Äëitem retries are independent; partial success allowed; failed items are isolated.\\n+  - Aggregates reflect per‚Äëitem outcomes; reduce/map fan‚Äëout is controlled via `fanout: 'reduce' | 'map'` on dependents.\\n+\\n+### Example ‚Äî ForEach With Per‚ÄëItem Retries\\n+```yaml\\n+checks:\\n+  list:\\n+    type: command\\n+    exec: \\\"node -e \\\\\\\"console.log('[\\\\\\\\\\\"a\\\\\\\\\\\",\\\\\\\\\\\"b\\\\\\\\\\\"]')\\\\\\\"\\\"\\n+    forEach: true\\n+\\n+  process:\\n+    type: command\\n+    depends_on: [list]\\n+    fanout: map\\n+    exec: node scripts/process-item.js\\n+    fail_if: \\\"output.__failed === true\\\"\\n+    on_fail:\\n+      retry: { max: 1, backoff: { mode: fixed, delay_ms: 500 } }\\n+```\\n+\\n+## Observability and Snapshots\\n+- Every decision is committed to the journal (check id, scope, event, output, issues, timing).\\n+- Export last run snapshot to JSON for post‚Äëmortem or replay scaffolding:\\n+```ts\\n+const engine = new StateMachineExecutionEngine();\\n+const result = await engine.executeChecks({ checks: ['build'], config });\\n+await engine.saveSnapshotToFile('run-snapshot.json');\\n+// const snap = await engine.loadSnapshotFromFile('run-snapshot.json');\\n+```\\n+\\n+## Safety Defaults Recap\\n+- `if`: error ‚Üí skip (if_condition).\\n+- `assume`: violation ‚Üí skip (or block dependents if critical).\\n+- `guarantee`: violation ‚Üí add `contract/guarantee_failed` issue; route on_fail.\\n+- `fail_if`: true ‚Üí failure; retries only for transient classifications.\\n+- Loop budgets and retry caps prevent unbounded execution.\\n+\\n+---\\n+\\n+For additional examples, see:\\n+- defaults/visor.yaml (fact validation transitions)\\n+- tests/unit/routing-transitions-and-contracts.test.ts (transitions, assume/guarantee)\\n+- docs/engine-state-machine-plan.md (state machine overview)\\n+\\n+## End-to-End Policy (Do-It-Right Checklist)\\n+\\n+This section summarizes the full, NASA‚Äëstyle approach we recommend. Items marked (optional) are enhancements you can phase in.\\n+\\n+### Config / Schema\\n+- Criticality (proposed field; tags remain a fallback)\\n+  - `criticality: external | internal | policy | info`\\n+  - or minimal boolean `critical: true|false` if you prefer simplicity.\\n+- Contracts (implemented)\\n+  - `assume:` preconditions (list of expressions)\\n+  - `guarantee:` postconditions (list of expressions)\\n+  - (optional) `assume_mode: 'skip' | 'fail'` ‚Äî if set to `fail`, unmet assume marks failure and routes `on_fail`.\\n+- Transitions (implemented)\\n+  - `on_success|on_fail|on_finish.transitions: [{ when, to, goto_event? }]` with `goto_js` fallback.\\n+- Retries\\n+  - (proposed) `retry_on: ['transient'] | ['transient','logical']` (default: transient only).\\n+- Safety profiles (optional)\\n+  - `safety: strict | standard` (global defaults for budgets/retries on critical branches).\\n+\\n+### Engine Policy (derived from criticality)\\n+- External / Control‚Äëplane / Policy (critical)\\n+  - Require meaningful `assume` and `guarantee`.\\n+  - Default `continue_on_failure: false`.\\n+  - Retries: bounded (max 2‚Äì3), transient faults only; no auto‚Äëretry for logical violations.\\n+  - Lower per‚Äëscope loop budget (e.g., 8 instead of 10).\\n+  - Suppress downstream mutating actions if guarantees/fail_if violate; remediate or escalate.\\n+- Non‚Äëcritical\\n+  - Contracts recommended but not required.\\n+  - `continue_on_failure: true` allowed where safe.\\n+  - Default loop budget (10), normal retry bounds.\\n+\\n+### Runtime Semantics\\n+- Evaluation order\\n+  1) `if` (plan‚Äëtime scheduling) ‚Üí 2) `assume` (pre‚Äëexec) ‚Üí 3) provider ‚Üí 4) `guarantee` + `fail_if` (post‚Äëexec) ‚Üí 5) transitions/goto.\\n+- Determinism & safety\\n+  - Expressions run in a secure sandbox; no I/O/time randomness; short timeouts.\\n+- ForEach isolation\\n+  - `fanout: map` executes per‚Äëitem; failures isolate; reduce aggregates once.\\n+  - (optional) per‚Äëitem concurrency with default 1.\\n+\\n+### Side‚ÄëEffect Control\\n+- Detect mutating providers (GitHub ops except read‚Äëonly, HTTP methods ‚â† GET/HEAD, file writes).\\n+- For critical steps: require idempotency or compensating actions; block side‚Äëeffects when contracts fail.\\n+\\n+### Observability / Telemetry\\n+- Journal each decision (check, scope, expression, inputs, result, timestamps).\\n+- Emit structured fault events: `fault.detected`, `fault.isolated`, `fault.recovery.*`.\\n+- Metrics: retries, fault counts by class, loop budget hits.\\n+\\n+### Persistence / Resume (debug‚Äëfirst)\\n+- Export last run as JSON (implemented): `saveSnapshotToFile()`.\\n+- (future) Debug‚Äëonly resume that reconstructs state from snapshot.\\n+\\n+### Validation / Guardrails\\n+- Warn if a critical step lacks `assume` or `guarantee`.\\n+- Warn if mutating provider lacks criticality classification.\\n+- Warn if `transitions` exist with tight loops disabled in `strict` safety profile.\\n+- CLI `--safe-mode` to disable mutating providers for dry‚Äëruns.\\n+\\n+### Verification (Tests & Acceptance Criteria)\\n+- Unit\\n+  - `assume` skip vs guard‚Äëstep hard‚Äëfail.\\n+  - `guarantee` violations add issues; no extra provider calls.\\n+  - Transitions precedence over `goto_js`; loop budget enforcement.\\n+- Integration\\n+  - Critical external step blocks downstream side‚Äëeffects on contract failure.\\n+  - Control‚Äëplane forEach parent with tight budget; verifies no loops past limit.\\n+  - Retry policy honors transient vs logical classification.\\n+- YAML e2e\\n+  - Updated defaults remain green; include a strict safety profile scenario.\\n+\\n+### Acceptance Criteria (done when)\\n+- All tests (unit/integration/YAML) green with critical/non‚Äëcritical mixes.\\n+- Docs updated (this guide + engine plan); examples use block‚Äëstyle YAML.\\n+- Logger outputs timestamps; debug is gated.\\n+- No dist/ committed; config validators warn on unsafe critical steps.\\n+\\n+## Additional Examples\\n+\\n+### Critical External Step\\n+```yaml\\n+checks:\\n+  post-comment:\\n+    type: github\\n+    criticality: external\\n+    on:\\n+      - pr_opened\\n+    op: comment.create\\n+    assume:\\n+      - \\\"isMember()\\\"\\n+      - \\\"env.DRY_RUN !== 'true'\\\"\\n+    guarantee:\\n+      - \\\"output && typeof output.id === 'number'\\\"\\n+    continue_on_failure: false\\n+    on_fail:\\n+      retry: { max: 2, backoff: { mode: exponential, delay_ms: 1200 } }\\n+```\\n+\\n+### Control‚ÄëPlane ForEach With Transitions\\n+```yaml\\n+routing:\\n+  max_loops: 8\\n+\\n+checks:\\n+  extract-items:\\n+    type: command\\n+    criticality: internal\\n+    exec: \\\"node -e \\\\\\\"console.log('[\\\\\\\\\\\"a\\\\\\\\\\\",\\\\\\\\\\\"b\\\\\\\\\\\"]')\\\\\\\"\\\"\\n+    forEach: true\\n+    on_finish:\\n+      transitions:\\n+        - when: \\\"any(outputs_history['validate'], v => v && v.ok === false)\\\"\\n+          to: remediate\\n+\\n+  validate:\\n+    type: command\\n+    depends_on:\\n+      - extract-items\\n+    fanout: map\\n+    exec: node scripts/validate.js\\n+\\n+  remediate:\\n+    type: command\\n+    exec: node scripts/fix.js\\n+```\\n+\\n+## Side‚Äëby‚Äëside examples: the same intent with different constructs\\n+\\n+### Example A ‚Äî Skip entirely when the repo has no changes\\n+Using `if` (best: planning‚Äëtime decision):\\n+```yaml\\n+checks:\\n+  summarize:\\n+    type: ai\\n+    on:\\n+      - pr_opened\\n+      - pr_updated\\n+    if: \\\"filesCount > 0\\\"\\n+    exec: node scripts/summarize.js\\n+```\\n+\\n+Using `assume` (works but later in the lifecycle):\\n+```yaml\\n+checks:\\n+  summarize:\\n+    type: ai\\n+    on:\\n+      - pr_opened\\n+      - pr_updated\\n+    assume:\\n+      - \\\"filesCount > 0\\\"\\n+    exec: node scripts/summarize.js\\n+```\\n+Both skip the step; `if` prunes earlier, `assume` skips right before calling the provider.\\n+\\n+### Example B ‚Äî Ensure outputs obey invariants\\n+Using `guarantee` (contract):\\n+```yaml\\n+checks:\\n+  collect:\\n+    type: command\\n+    exec: \\\"node collect.js\\\"    # produces { items: [...] }\\n+    guarantee:\\n+      - \\\"output && Array.isArray(output.items)\\\"\\n+      - \\\"output.items.length > 0\\\"\\n+    on_fail:\\n+      run:\\n+        - recompute\\n+```\\n+\\n+Using `fail_if` (policy):\\n+```yaml\\n+checks:\\n+  collect:\\n+    type: command\\n+    exec: \\\"node collect.js\\\"\\n+    fail_if: \\\"!(output && Array.isArray(output.items) && output.items.length > 0)\\\"\\n+    on_fail:\\n+      run:\\n+        - recompute\\n+```\\n+Both mark the run as failed and route `on_fail`; use `guarantee` for design‚Äëby‚Äëcontract semantics, `fail_if` for policy rules.\\n+\\n+### Example C ‚Äî ‚ÄúHard‚Äëfail‚Äù on unmet preconditions (guard step pattern)\\n+If you need an explicit failure instead of a skip for an unmet `assume`, use a guard:\\n+```yaml\\n+checks:\\n+  prechecks:\\n+    type: command\\n+    exec: node scripts/check-tools.js   # exit 1 when tools missing\\n+    fail_if: \\\"output.exitCode !== 0\\\"\\n+  analyze:\\n+    type: command\\n+    depends_on:\\n+      - prechecks\\n+    exec: node scripts/analyze.js\\n+```\\n+\\n+## Comprehensive Example ‚Äî All Primitives Working Together\\n+\\n+This end‚Äëto‚Äëend example shows `criticality`, `if`, `assume`, `guarantee`, `fail_if`, and declarative `transitions` in one flow. It includes fan‚Äëout (control‚Äëplane), a policy gate, and an external step with contracts.\\n+\\n+```yaml\\n+version: \\\"1.0\\\"\\n+\\n+routing:\\n+  # Tighter budget recommended for control‚Äëplane loops\\n+  max_loops: 8\\n+\\n+checks:\\n+  # 1) Control‚Äëplane fan‚Äëout producer\\n+  extract-facts:\\n+    type: command\\n+    criticality: internal\\n+    on:\\n+      - issue_opened\\n+      - issue_comment\\n+    exec: \\\"node -e \\\\\\\"console.log('[{\\\"\\\"id\\\"\\\":1,\\\"\\\"claim\\\"\\\":\\\"\\\"A\\\"\\\"},{\\\"\\\"id\\\"\\\":2,\\\"\\\"claim\\\"\\\":\\\"\\\"B\\\"\\\"}]')\\\\\\\"\\\"\\n+    forEach: true\\n+    # Postconditions: enforce shape and cap fan‚Äëout size\\n+    guarantee:\\n+      - \\\"Array.isArray(output)\\\"\\n+      - \\\"output.every(x => typeof x.id === 'number' && typeof x.claim === 'string')\\\"\\n+      - \\\"output.length <= 50\\\"\\n+    # Route back for remediation when any validation failed\\n+    on_finish:\\n+      transitions:\\n+        - when: \\\"any(outputs_history['validate-fact'], v => v && v.is_valid === false) && event.name === 'issue_opened'\\\"\\n+          to: issue-assistant\\n+        - when: \\\"any(outputs_history['validate-fact'], v => v && v.is_valid === false) && event.name === 'issue_comment'\\\"\\n+          to: comment-assistant\\n+\\n+  # 2) Map fan‚Äëout validator\\n+  validate-fact:\\n+    type: command\\n+    depends_on:\\n+      - extract-facts\\n+    fanout: map\\n+    exec: node scripts/validate-fact.js      # -> { is_valid: boolean, errors?: number }\\n+    # declare policy failure\\n+    fail_if: \\\"output && output.is_valid === false\\\"\\n+    on_fail:\\n+      # Only retry transient provider errors (e.g., script crashed), not logical invalids\\n+      retry: { max: 1, backoff: { mode: exponential, delay_ms: 1000 } }\\n+\\n+  # 3) Control‚Äëplane aggregator that computes overall validity\\n+  aggregate:\\n+    type: command\\n+    criticality: internal\\n+    depends_on:\\n+      - validate-fact\\n+    exec: node scripts/aggregate-validity.js # -> { all_valid: boolean }\\n+    guarantee:\\n+      - \\\"output && typeof output.all_valid === 'boolean'\\\"\\n+    on_success:\\n+      transitions:\\n+        - when: \\\"output.all_valid === true\\\"\\n+          to: permission-check\\n+\\n+  # 4) Policy gate (no external side‚Äëeffect but gates external actions)\\n+  permission-check:\\n+    type: command\\n+    criticality: policy\\n+    exec: node scripts/check-permissions.js  # -> { allowed: boolean }\\n+    guarantee:\\n+      - \\\"typeof output.allowed === 'boolean'\\\"\\n+\\n+  # 5) External action ‚Äî only runs when policy passes (belt and suspenders)\\n+  post-comment:\\n+    type: github\\n+    criticality: external\\n+    depends_on:\\n+      - permission-check\\n+    on:\\n+      - issue_opened\\n+    # Coarse plan‚Äëtime gate (cheap & early)\\n+    if: \\\"outputs['permission-check'] && outputs['permission-check'].allowed === true\\\"\\n+    # Final execution preflight to avoid side‚Äëeffects if context shifted\\n+    assume:\\n+      - \\\"outputs['permission-check'] && outputs['permission-check'].allowed === true\\\"\\n+      - \\\"env.DRY_RUN !== 'true'\\\"\\n+    op: comment.create\\n+    guarantee:\\n+      - \\\"output && typeof output.id === 'number'\\\"\\n+    continue_on_failure: false\\n+\\n+  # 6) Non‚Äëcritical compute ‚Äî allowed to fail softly\\n+  summarize:\\n+    type: ai\\n+    criticality: info\\n+    on:\\n+      - issue_opened\\n+    continue_on_failure: true\\n+    fail_if: \\\"(output.errors || []).length > 0\\\"\\n+```\\n+\\n+Highlights\\n+- control‚Äëplane steps (extract-facts, aggregate) carry `assume`/`guarantee` and drive transitions under a tight loop budget.\\n+- validate-fact uses `fail_if` for policy failure and a bounded retry only for transient provider errors.\\n+- permission-check (policy) gates external actions without itself mutating external systems.\\n+- post-comment (external) uses both `if` (early prune) and `assume` (preflight) plus a `guarantee` after posting.\\n+- summarize shows a non‚Äëcritical step with soft failure handling via `continue_on_failure: true`.\\n+```\\n+- JSON Schema validation (unified `schema`)\\n+  - `schema: <string>` selects layout/renderer (no validation).\\n+  - `schema: <object>` is a JSON Schema; the engine validates `output` for any provider (ai/command/script/http). Violations create `contract/schema_validation_failed` and follow criticality rules.\\n+  - `output_schema` is deprecated; keep it only for backward compatibility.\\n\",\"status\":\"added\"},{\"filename\":\"docs/guides/workflow-style-guide.md\",\"additions\":7,\"deletions\":0,\"changes\":224,\"patch\":\"diff --git a/docs/guides/workflow-style-guide.md b/docs/guides/workflow-style-guide.md\\nnew file mode 100644\\nindex 00000000..9d097192\\n--- /dev/null\\n+++ b/docs/guides/workflow-style-guide.md\\n@@ -0,0 +1,224 @@\\n+# Visor Workflow Style Guide\\n+\\n+This guide captures pragmatic conventions for writing clear, safe, and maintainable Visor workflows (visor.yaml and friends). It complements the feature guides by focusing on readability, intent, and day‚Äë2 operability.\\n+\\n+## Why This Matters\\n+\\n+- Readability reduces on‚Äëcall cognitive load and speeds reviews.\\n+- A consistent structure makes behaviors obvious (what, when, how, and why).\\n+- Guardrails prevent accidental side effects and clarify intent.\\n+\\n+## Key Principles\\n+\\n+- One step, one responsibility. Prefer small, composable steps over ‚Äúkitchen‚Äësink‚Äù checks.\\n+- Declare intent before mechanics. Readers should see what a step is and when it runs before how it runs.\\n+- Guard and contract every important step. Preconditions (‚Äúassume‚Äù or ‚Äúif‚Äù) up front, postconditions (‚Äúschema‚Äù or ‚Äúguarantee‚Äù) after execution.\\n+- Avoid hidden control flow. Prefer declarative transitions and explicit dependencies over imperative logic.\\n+- Be idempotent, especially for external effects. Plan for retries and partial failures.\\n+\\n+## Recommended Key Order (Per Step)\\n+\\n+Use this top‚Äëdown order for every step. Omit sections that don‚Äôt apply.\\n+\\n+1) Identity & Intent\\n+- `type`\\n+- `criticality` (external | internal | policy | info)\\n+- `group`\\n+- `tags`\\n+- `description`\\n+\\n+2) Triggers & Dependencies\\n+- `on`\\n+- `depends_on`\\n+- `fanout` / `forEach` / `reduce`\\n+\\n+3) Preconditions (Guards)\\n+- `assume`\\n+- `if`\\n+\\n+4) Provider Configuration (Only fields for the given type)\\n+- ai: `prompt`, `ai.{model,provider,tools,‚Ä¶}`\\n+- command: `exec`, `args`, `cwd`, `shell`\\n+- script: `content` or `file`\\n+- github: `op`, `values`\\n+- http/http_client/http_input: `url`, `method`, `body`, `headers`\\n+- log/memory/workflow/noop: minimal fields\\n+\\n+5) Contracts (Post‚ÄëExec)\\n+- `schema` (renderer name or JSON Schema)\\n+- `guarantee`\\n+\\n+6) Failure Policies\\n+- `fail_if`\\n+- `continue_on_failure`\\n+\\n+7) Routing & Transitions\\n+- `on_success`\\n+- `on_fail`\\n+- `on_finish`\\n+\\n+8) Runtime Controls\\n+- `timeout`, `retries/backoff`, `env`\\n+- `namespace`, `reuse_ai_session`, `session_mode`\\n+\\n+9) Output Formatting\\n+- `template: { content | file }`\\n+- `message`, `level` (for `log`)\\n+\\n+## Criticality & Contracts (Default Safety)\\n+\\n+- `external`: side effects outside the repo/CI boundary (e.g., GitHub ops, webhooks).\\n+  - Require a precondition: `assume` (preferred) or `if`.\\n+  - If output‚Äëproducing, also require a post‚Äëexec contract: `schema` or `guarantee`.\\n+  - Logical failures (schema/guarantee/fail_if) are not auto‚Äëretried.\\n+\\n+- `internal`: orchestration/state within CI/repo (formerly ‚Äúcontrol‚Äëplane‚Äù).\\n+  - Same enforcement as `external` (precondition + contract for output steps).\\n+  - No auto‚Äëretry for logical failures.\\n+\\n+- `policy`: evaluative checks (security/perf/quality/docs). Optional guards/contracts.\\n+\\n+- `info`: purely informational; never gates dependents. Good for exploratory or advisory steps.\\n+\\n+Notes\\n+- Global `fail_if` is non‚Äëgating by design; it marks the run status but must not block dependents.\\n+- Check‚Äëlevel `fail_if` is gating (treated as fatal for routing).\\n+\\n+## Declarative Flow > Imperative Glue\\n+\\n+- Prefer `transitions` under `on_success` / `on_fail` / `on_finish` over imperative `goto_js`.\\n+- Keep transition expressions short, pure, and readable; use optional chaining and nullish coalescing for safety.\\n+\\n+Example\\n+\\n+```yaml\\n+on_finish:\\n+  transitions:\\n+    - when: \\\"any(outputs_history['validate-fact'], v => v?.is_valid === false) && event.name === 'issue_opened'\\\"\\n+      to: issue-assistant\\n+    - when: \\\"any(outputs_history['validate-fact'], v => v?.is_valid === false) && event.name === 'issue_comment'\\\"\\n+      to: comment-assistant\\n+    - when: \\\"all(outputs_history['validate-fact'], v => v?.is_valid === true)\\\"\\n+      to: null\\n+```\\n+\\n+## forEach (Fan‚ÄëOut) Patterns\\n+\\n+- Use `forEach: true` on the parent that produces an array; children with `fanout: map` run per item; with `fanout: reduce` run once (aggregate).\\n+- Empty arrays should skip dependents with a visible message and not increment stats.\\n+- Aggregate parents should route (on_success/on_fail) before committing; dependents read per‚Äëscope outputs.\\n+\\n+Minimal Map + Aggregate\\n+\\n+```yaml\\n+extract-facts:\\n+  type: ai\\n+  on: [issue_opened]\\n+  forEach: true\\n+  prompt: |\\n+    Return JSON array of facts: [{ id, claim, verifiable }]\\n+  schema:\\n+    type: array\\n+    items:\\n+      type: object\\n+      required: [id, claim, verifiable]\\n+\\n+validate-fact:\\n+  type: ai\\n+  on: [issue_opened]\\n+  depends_on: [extract-facts]\\n+  fanout: map\\n+  prompt: \\\"Validate: {{ outputs['extract-facts'].claim }}\\\"\\n+\\n+aggregate:\\n+  type: script\\n+  on: [issue_opened]\\n+  depends_on: [validate-fact]\\n+  content: |\\n+    const all = (outputs.history['validate-fact']||[]).filter(Boolean);\\n+    return { all_valid: all.every(v => v?.is_valid === true) };\\n+  schema:\\n+    type: object\\n+    required: [all_valid]\\n+```\\n+\\n+## GitHub Ops (External)\\n+\\n+- Normalize values at the provider; still use `assume` to guard empties.\\n+- Keep idempotency: label adds/sets should tolerate duplicates and ordering.\\n+\\n+Example\\n+\\n+```yaml\\n+apply-issue-labels:\\n+  type: github\\n+  criticality: external\\n+  on: [issue_opened]\\n+  depends_on: [issue-assistant]\\n+  assume:\\n+    - \\\"(outputs['issue-assistant']?.labels?.length ?? 0) > 0\\\"\\n+  op: labels.add\\n+  values:\\n+    - \\\"{{ outputs['issue-assistant'].labels | default: [] | json }}\\\"\\n+```\\n+\\n+## Memory & Idempotency\\n+\\n+- Use `namespace` to avoid collisions.\\n+- Treat memory reads in `assume`/`if` as guards only; avoid side effects in expressions.\\n+- For external calls, design retry‚Äësafe operations (check‚Äëbefore‚Äëwrite, idempotency keys).\\n+\\n+## YAML Style\\n+\\n+- Prefer block arrays/lists over inline `[]` unless trivially short.\\n+- Quote JS expressions in `assume`/`if` using double quotes.\\n+- Use `|` for multiline `prompt`/`content`; avoid trailing whitespace.\\n+- Keep keys in the recommended order across all steps.\\n+\\n+## Do‚Äôs and Don‚Äôts\\n+\\n+Do\\n+- Declare `criticality` and follow the guard/contract rules for `external`/`internal`.\\n+- Keep expressions short and defensive: `outputs?.x?.length ?? 0`.\\n+- Add `schema` whenever output shape matters (AI/script/command/http).\\n+\\n+Don‚Äôt\\n+- Hide control flow in templates or long `*_js` snippets.\\n+- Mix unrelated responsibilities in a single step.\\n+- Depend on outputs you didn‚Äôt guard (always use `assume`).\\n+\\n+## Quick Checklist (Per Step)\\n+\\n+- Identity: `type`, `criticality`, `group` set?\\n+- When: `on` clear and minimal?\\n+- Inputs: `depends_on` accurate? `assume` present for risky reads?\\n+- How: provider config minimal and readable?\\n+- Contracts: `schema` or `guarantee` (required for external/internal outputs)?\\n+- Policies: `fail_if` only for step‚Äëspecific gating?\\n+- Flow: transitions (`on_success`/`on_fail`/`on_finish`) instead of imperative glue?\\n+- Controls: timeouts and env only when necessary?\\n+\\n+## Complete Example (Well‚ÄëStructured External Labeling)\\n+\\n+```yaml\\n+apply-overview-labels:\\n+  type: github\\n+  criticality: external\\n+  tags: [github]\\n+  on: [pr_opened]\\n+  depends_on: [overview]\\n+  assume:\\n+    - \\\"outputs['overview']?.tags?.label\\\"\\n+    - \\\"outputs['overview']?.tags?.['review-effort'] != null\\\"\\n+  op: labels.add\\n+  values:\\n+    - \\\"{{ outputs.overview.tags.label | default: '' | safe_label }}\\\"\\n+    - \\\"{{ outputs.overview.tags['review-effort'] | default: '' | prepend: 'review/effort:' | safe_label }}\\\"\\n+```\\n+\\n+## References\\n+\\n+- Fault Management & Contracts\\n+- Criticality Modes\\n+- Dependencies & Routing\\n+\\n\",\"status\":\"added\"},{\"filename\":\"docs/human-input-provider.md\",\"additions\":1,\"deletions\":1,\"changes\":9,\"patch\":\"diff --git a/docs/human-input-provider.md b/docs/human-input-provider.md\\nindex 8c2a3dbf..28ae06f9 100644\\n--- a/docs/human-input-provider.md\\n+++ b/docs/human-input-provider.md\\n@@ -46,7 +46,7 @@ checks:\\n \\n ### Using Input in Dependent Checks\\n \\n-The user's input is available to dependent checks via the `outputs` variable:\\n+The user's input is available to dependent checks via the `outputs` variable. By default (no schema), human-input returns an object `{ text: string, ts: number }` where `ts` is the timestamp (milliseconds since epoch):\\n \\n ```yaml\\n checks:\\n@@ -58,8 +58,11 @@ checks:\\n     type: command\\n     depends_on: [get-version]\\n     exec: |\\n-      git tag v{{ outputs['get-version'] }}\\n-      git push origin v{{ outputs['get-version'] }}\\n+      # Prefer .text to read the string payload\\n+      git tag v{{ outputs['get-version'].text | default: outputs['get-version'] }}\\n+      git push origin v{{ outputs['get-version'].text | default: outputs['get-version'] }}\\n+\\n+> See also: [Default Output Schema](./default-output-schema.md)\\n ```\\n \\n ## Input Methods\\n\",\"status\":\"modified\"},{\"filename\":\"docs/limits.md\",\"additions\":2,\"deletions\":0,\"changes\":64,\"patch\":\"diff --git a/docs/limits.md b/docs/limits.md\\nnew file mode 100644\\nindex 00000000..9d96263e\\n--- /dev/null\\n+++ b/docs/limits.md\\n@@ -0,0 +1,64 @@\\n+## üö¶ Execution Limits (Run Caps)\\n+\\n+This feature protects workflows from accidental infinite loops by capping how many times a step may execute in a single engine run. It complements (but is different from) routing loop budgets.\\n+\\n+### Why this exists\\n+\\n+- Complex `on_fail`/`on_success` routing can create feedback loops when a remediation step immediately routes back to its source.\\n+- The cap provides a hard stop with a clear error if a step keeps re-running without converging.\\n+\\n+### Configuration\\n+\\n+Global (default is 50 if omitted):\\n+\\n+```yaml\\n+version: \\\"1.0\\\"\\n+\\n+limits:\\n+  max_runs_per_check: 50  # Applies to every step unless overridden\\n+```\\n+\\n+Per-step override:\\n+\\n+```yaml\\n+steps:\\n+  refine:\\n+    type: ai\\n+    max_runs: 10  # Hard cap for this step within one engine run\\n+```\\n+\\n+Disable cap for a specific step (not recommended unless you know it converges quickly):\\n+\\n+```yaml\\n+steps:\\n+  extract:\\n+    type: command\\n+    max_runs: 0   # or any negative value\\n+```\\n+\\n+### Behavior\\n+\\n+- The engine counts executions per step. For `forEach` children, the counter is tracked per item scope (each item has its own budget).\\n+- When the cap is exceeded, the step fails immediately with a single error issue:\\n+  - `ruleId`: `<step-id>/limits/max_runs_exceeded`\\n+  - `severity`: `error`\\n+  - `message` includes the scope and attempt number\\n+- Dependents are gated as with any error unless the dependency declares `continue_on_failure: true`.\\n+\\n+### How this differs from `routing.max_loops`\\n+\\n+- `routing.max_loops` caps routing transitions (e.g., goto/retry waves) per scope.\\n+- `limits.max_runs_per_check` caps actual step executions per step (also per scope for `forEach`).\\n+- Both guard rails can be used together: set a modest routing budget (e.g., 5‚Äì10) and leave the execution cap at the default (50) or tailor per step.\\n+\\n+### Recommendations\\n+\\n+- Keep `routing.max_loops` small for fast feedback (5‚Äì10).\\n+- Use per-step `max_runs` on chat-like loops or known retryers if you need tighter control.\\n+- Prefer fixing the loop logic (conditions/routing) over raising the caps.\\n+\\n+### Troubleshooting\\n+\\n+- If you hit `.../limits/max_runs_exceeded` immediately, check if a step is routed back without changing state.\\n+- For `forEach` flows, confirm whether the error is tied to a specific item scope; fix that item‚Äôs remediation path.\\n+\\n\",\"status\":\"added\"},{\"filename\":\"docs/liquid-templates.md\",\"additions\":1,\"deletions\":0,\"changes\":2,\"patch\":\"diff --git a/docs/liquid-templates.md b/docs/liquid-templates.md\\nindex d36d3d8f..6f152d22 100644\\n--- a/docs/liquid-templates.md\\n+++ b/docs/liquid-templates.md\\n@@ -198,6 +198,8 @@ echo '{{ pr | json }}' | jq .\\n {{ files | map: \\\"filename\\\" }}   # Array of filenames\\n ```\\n \\n+<!-- Removed merge_sort_by example: filter no longer provided -->\\n+\\n ## Examples\\n \\n ### Debugging Outputs\\n\",\"status\":\"added\"},{\"filename\":\"docs/loop-routing-refactor.md\",\"additions\":3,\"deletions\":0,\"changes\":89,\"patch\":\"diff --git a/docs/loop-routing-refactor.md b/docs/loop-routing-refactor.md\\nnew file mode 100644\\nindex 00000000..b1e0f55e\\n--- /dev/null\\n+++ b/docs/loop-routing-refactor.md\\n@@ -0,0 +1,89 @@\\n+# Manual Loop Routing Refactor ‚Äî Plan and Status\\n+\\n+This document captures the plan, rationale, completed work, and next steps to support manual‚Äëonly chat loops in Visor without special tags or goto_js.\\n+\\n+## Background\\n+\\n+The previous behavior de‚Äëduplicated a step when re‚Äërouted in the same event, which stalled manual chat loops (ask ‚Üí refine ‚Üí ask ‚Ä¶). We also experimented with a `repeatable` tag to bypass the guard, but it added concept complexity.\\n+\\n+## Goals\\n+\\n+- Manual‚Äëonly loop: ask ‚Üí refine ‚Üí ask ‚Ä¶ until refined=true, then finish.\\n+- No special tags, no goto_js, no schedule event hops.\\n+- Use fail_if + on_fail/on_success only.\\n+- Keep default suites green and avoid regressions.\\n+\\n+## Changes (Completed)\\n+\\n+1) Engine parity for inline runs\\n+- Inline `fail_if` evaluation and post‚Äë`fail_if` routing: honor `on_fail.goto` for inline runs.\\n+\\n+2) Routed re‚Äëruns (no special tags)\\n+- For `origin='on_fail'`, forward‚Äërun allows re‚Äërunning the same step within the same grouped run; loop safety relies on `routing.max_loops`.\\n+\\n+3) Failure‚Äëaware forward runs\\n+- Skip static `on_success.goto` chains when the target produced fatal issues (including `fail_if`).\\n+- For `origin='on_fail'`, schedule only direct dependents of the failed target; skip dependents when any direct dep has fatal issues.\\n+\\n+4) One‚Äëshot opt‚Äëin\\n+- `tags: [one_shot]` prevents a terminal step (e.g., `finish`) from running more than once per grouped run.\\n+\\n+5) Test‚Äëvisible history\\n+- `executeChecks` now attaches `reviewSummary.history` with a safe snapshot of per‚Äëstep outputs history for deterministic testing (no I/O).\\n+\\n+6) Task‚Äërefinement agent (manual‚Äëonly)\\n+- `defaults/task-refinement.yaml` uses `ask` ‚Üí `refine` loop with `fail_if` and `on_fail/on_success` only; no `repeatable`, no `goto_js`, no `schedule`.\\n+- Embedded tests: one‚Äëpass and multi‚Äëturn pass locally.\\n+\\n+## Removed\\n+\\n+- `repeatable` / `x-repeatable` mechanics: no longer needed.\\n+\\n+## Tests\\n+\\n+- YAML suites (green):\\n+  - `defaults/task-refinement.yaml` (both cases)\\n+  - `defaults/visor.tests.yaml` (10/10)\\n+\\n+- Jest integration (added):\\n+  - `tests/integration/on-fail-no-cascade.test.ts`: verifies failure‚Äëaware forward runs do not cascade into success chains.\\n+\\n+- Jest integration (deferred):\\n+  - A deterministic loop test using `reviewSummary.history` to assert multiple turns. Will add a tiny test driver to stabilize execution context and tag filtering.\\n+\\n+## How to Validate Locally\\n+\\n+```bash\\n+# Build CLI\\n+npm run build:cli\\n+\\n+# Task-refinement YAML\\n+VISOR_DEBUG=true node dist/index.js test --config defaults/task-refinement.yaml --max-parallel 1\\n+\\n+# Default suite\\n+node dist/index.js test --config defaults/visor.tests.yaml --max-parallel 2 --json tmp/visor.json\\n+\\n+# Focused Jest tests (engine behavior)\\n+npm test -- on-fail-no-cascade.test.ts\\n+```\\n+\\n+Acceptance criteria:\\n+- Both YAML suites pass.\\n+- No `repeatable`/`x-repeatable` in the codebase.\\n+- `defaults/task-refinement.yaml` contains no `goto_js` and no `schedule` hops.\\n+\\n+## Next Steps (Planned)\\n+\\n+1) Deterministic Jest loop test\\n+- Add a small engine test driver util (internal only) that seeds event=`manual` and disables tag filtering; assert loop counts via `reviewSummary.history`.\\n+\\n+2) Documentation\\n+- Add a short ‚ÄúManual Loops‚Äù page covering `fail_if`+`on_fail/on_success`, loop budgets, and `one_shot` for terminal steps.\\n+\\n+3) CI gates\\n+- Add a CI job to run: default YAML, task‚Äërefinement YAML, and the focused Jest tests.\\n+\\n+## Risk & Rollback\\n+\\n+- Risk: forward‚Äërun changes could over/under schedule dependents; mitigated by direct‚Äëdependent + fatal‚Äëskip guards.\\n+- Rollback: revert to pre‚Äërefactor `scheduleForwardRun` and inline `fail_if` handling while keeping `reviewSummary.history` attachment (benign).\\n\",\"status\":\"added\"},{\"filename\":\"docs/roadmap/criticality-implementation-tasks.md\",\"additions\":3,\"deletions\":0,\"changes\":92,\"patch\":\"diff --git a/docs/roadmap/criticality-implementation-tasks.md b/docs/roadmap/criticality-implementation-tasks.md\\nnew file mode 100644\\nindex 00000000..3f5ed0c0\\n--- /dev/null\\n+++ b/docs/roadmap/criticality-implementation-tasks.md\\n@@ -0,0 +1,92 @@\\n+# Criticality & Contracts ‚Äî Implementation Tasks (Do‚ÄëIt‚ÄëRight)\\n+\\n+This file lists the remaining engineering tasks to fully implement the criticality model, contracts, and transitions as documented. Items are grouped and check‚Äëlistable. ‚Äú(optional)‚Äù items can be phased in later.\\n+\\n+## 1) Schema & Types\\n+- [ ] Add `criticality` field to `CheckConfig` (`external | internal | policy | info`).\\n+- [ ] (optional) Add `assume_mode: 'skip' | 'fail'` to control unmet preconditions handling.\\n+- [ ] (optional) Add `retry_on: ['transient'] | ['transient','logical']` to narrow retry classes.\\n+- [ ] Update JSON schema generator and `src/generated/config-schema.ts`.\\n+- [ ] Update TypeScript types (`src/types/config.ts`, SDK exports).\\n+\\n+## 2) Config Validation & Linting\\n+- [ ] Validator: warn when `criticality` omitted on mutating providers (external inference) or forEach parents (control‚Äëplane inference).\\n+- [ ] Validator: warn when critical steps lack `assume`/`guarantee`.\\n+- [ ] Validator: warn when `assume` references this step‚Äôs own `output`.\\n+- [ ] Validator: warn when `guarantee` contains policy thresholds better modeled as `fail_if`.\\n+- [ ] Validator: ensure `transitions[].to` targets exist (or null) and expressions compile.\\n+\\n+## 3) Engine Policy Mapping\\n+- [ ] Derive defaults from `criticality` at load time (but allow per‚Äëcheck overrides):\\n+      - external/control‚Äëplane/policy: `continue_on_failure=false`, retries transient‚Äëonly (max 2‚Äì3), loop budgets tighter (e.g., 8), contracts required.\\n+      - non‚Äëcritical: contracts optional, `continue_on_failure` may be true, default loop budget 10, retries standard.\\n+- [ ] Enforce ‚Äúno auto‚Äëretry for logical failures‚Äù in critical modes (fail_if/guarantee violations).\\n+- [ ] (optional) Per‚Äëcriticality loop budget override (e.g., control‚Äëplane default 8).\\n+\\n+## 4) Runtime Semantics (clarity & safety)\\n+- [ ] Ensure `assume` is evaluated pre‚Äëexec (no access to this step‚Äôs `output`), with clear error messaging.\\n+- [ ] Ensure `guarantee` is evaluated post‚Äëexec, with issues emitted as `contract/guarantee_failed`.\\n+- [ ] Keep expressions sandboxed, pure, and short‚Äëtimed; log evaluation errors as fail‚Äësecure decisions.\\n+- [ ] Transitions precedence over `goto_js` when both present; loop budget enforcement per scope.\\n+- [ ] (optional) forEach per‚Äëitem concurrency with default 1; cap via config.\\n+\\n+## 5) Side‚ÄëEffect Classification\\n+- [ ] Provider capability flags: identify mutating actions (GitHub ops except read‚Äëonly; HTTP methods ‚â† GET/HEAD; file writes).\\n+- [ ] For critical external steps: provide idempotency and/or compensation hooks (sagas) (optional roadmap).\\n+- [ ] Suppress downstream mutating steps when contracts/fail_if fail in critical branches (via dependency gating).\\n+\\n+## 6) CLI & Safety Switches\\n+- [ ] `--safe-mode` flag to disable mutating providers (dry‚Äërun all externals) for verification.\\n+- [ ] (optional) `--safety-profile strict|standard` to adjust loop budgets and retry caps globally.\\n+\\n+## 7) Telemetry & Observability\\n+- [ ] Emit structured fault events: `fault.detected`, `fault.isolated`, `fault.recovery.{attempted,failed,succeeded}`.\\n+- [ ] Metrics: retries attempted, logical vs transient failure counts, loop budget hits, contract violations by check.\\n+- [ ] Journal: ensure all contract/transition decisions and expressions are captured with scope/timestamps.\\n+\\n+## 8) Persistence\\n+- [ ] Keep JSON snapshot export (done) and add (optional) debug‚Äëresume path gated by a debug flag.\\n+\\n+## 9) Defaults & Examples\\n+- [ ] Update `defaults/visor.yaml` (and any bundled defaults) to declare `criticality` for relevant checks and prefer `transitions` over `goto_js` where applicable.\\n+- [ ] Update `defaults/task-refinement.yaml` and `defaults/agent-builder.yaml` to use `criticality` and transitions where appropriate; ensure no `assume` refers to own output.\\n+- [ ] Convert inline YAML arrays in defaults to block‚Äëstyle lists for consistency.\\n+- [ ] Add an annotated example block with all primitives (if, assume, guarantee, fail_if, transitions) and modes (reference the guides) in the defaults or examples folder.\\n+- [ ] Verify defaults run green via dist CLI:\\n+      - `npm run build:cli`\\n+      - `node dist/index.js test defaults/visor.tests.yaml --progress compact`\\n+      - Any other default suites (`task-refinement`, `agent-builder`) if present.\\n+\\n+## 10) Tests\\n+- [ ] Unit (engine/native):\\n+      - `assume` skip vs guard‚Äëstep hard‚Äëfail (no provider call on skip).\\n+      - `guarantee` violation adds `contract/guarantee_failed` and does not double‚Äëexecute provider.\\n+      - Transitions precedence over `goto_js`; undefined transition falls back to `goto`.\\n+      - Loop budget enforcement per scope (error surfaced; routing halts in that scope).\\n+      - Criticality policy mapping (external/control‚Äëplane/policy/non‚Äëcritical) sets defaults (gating, retries, budgets).\\n+- [ ] Integration:\\n+      - Critical external step blocks downstream mutating side‚Äëeffects on contract/fail_if failure (dependents gated).\\n+      - Control‚Äëplane forEach parent respects tighter loop budget; no oscillation beyond cap.\\n+      - Retry classifier: transient provider errors retried; logical (fail_if/guarantee) not auto‚Äëretried in critical modes.\\n+      - Non‚Äëcritical step with `continue_on_failure: true` does not block pipeline.\\n+- [ ] YAML e2e / Defaults:\\n+      - `defaults/visor.yaml` flow passes using transitions.\\n+      - `defaults/task-refinement.yaml` and `defaults/agent-builder.yaml` pass with `criticality` declared.\\n+      - Add a strict safety profile scenario (e.g., `safety: strict`) and ensure it passes.\\n+- [ ] CI Gates:\\n+      - Add a job to build CLI and run default YAML suites with `--progress compact`.\\n+      - Run unit/integration on PR; block merges on regressions.\\n+\\n+## 11) Docs (remaining polish)\\n+- [ ] README or landing page: link to Criticality Modes and Fault Management guides.\\n+- [ ] Ensure quick‚Äëstarts show `criticality` in at least one example (SDK & CLI done).\\n+- [ ] Sweep older docs for inline arrays (`[a, b]`) and convert to block lists.\\n+- [ ] Add ‚Äúassume vs guarantee ‚Äî do‚Äôs and don‚Äôts‚Äù callout in any doc that introduces contracts (done for two guides).\\n+\\n+## Acceptance Criteria\\n+- [ ] All tests pass (unit/integration/YAML) with representative critical/non‚Äëcritical mixes.\\n+- [ ] Config validator warns on unsafe/missing contracts and mis‚Äëdeclared criticality.\\n+- [ ] Engine enforces defaults per `criticality` while allowing explicit overrides.\\n+- [ ] Logs have timestamps; debug gated; decisions visible in journal and metrics.\\n+- [ ] No dist/ artifacts in commits.\\n+- [ ] Updated defaults (`defaults/visor.yaml`, `defaults/task-refinement.yaml`, `defaults/agent-builder.yaml`) run green via dist CLI in CI.\\n\",\"status\":\"added\"},{\"filename\":\"docs/sdk.md\",\"additions\":2,\"deletions\":0,\"changes\":44,\"patch\":\"diff --git a/docs/sdk.md b/docs/sdk.md\\nindex 40bd1104..7c5b6c92 100644\\n--- a/docs/sdk.md\\n+++ b/docs/sdk.md\\n@@ -149,6 +149,50 @@ try {\\n \\n Refer to `src/types/config.ts` for `VisorConfig`, `Issue`, and related types.\\n \\n+## Safety & Criticality (Quick Note)\\n+\\n+When building configs programmatically, model safety explicitly:\\n+\\n+- Declare criticality on steps with `criticality: external|internal|policy|info`.\\n+- Add contracts to critical steps:\\n+  - `assume:` preconditions checked before execution\\n+  - `guarantee:` postconditions checked after execution\\n+- Use declarative `transitions` for routing rather than `goto_js`.\\n+\\n+Example config (JS object):\\n+```ts\\n+const cfg = await loadConfig({\\n+  version: '1.0',\\n+  checks: {\\n+    'post-comment': {\\n+      type: 'github',\\n+      criticality: 'external',\\n+      on: ['pr_opened'],\\n+      op: 'comment.create',\\n+      assume: [\\\"isMember()\\\"],\\n+      guarantee: [\\\"output && typeof output.id === 'number'\\\"],\\n+      continue_on_failure: false,\\n+    },\\n+    // Structured outputs with unified `schema` (object) for validation\\n+    'summarize-json': {\\n+      type: 'ai',\\n+      schema: {\\n+        type: 'object',\\n+        properties: { ok: { type: 'boolean' }, items: { type: 'array', items: { type: 'string' } } },\\n+        required: ['ok', 'items']\\n+      },\\n+      prompt: 'Return JSON with ok and items...'\\n+    },\\n+    // Command/script can also use JSON Schema via `schema`\\n+    'aggregate': {\\n+      type: 'script',\\n+      content: 'return { all_valid: true };',\\n+      schema: { type: 'object', properties: { all_valid: { type: 'boolean' } }, required: ['all_valid'], additionalProperties: false }\\n+    }\\n+  },\\n+});\\n+```\\n+\\n ## Notes\\n \\n - SDK adds no new sandboxing or providers; all safety lives in the core engine.\\n\",\"status\":\"added\"},{\"filename\":\"docs/workflows.md\",\"additions\":16,\"deletions\":0,\"changes\":569,\"patch\":\"diff --git a/docs/workflows.md b/docs/workflows.md\\nnew file mode 100644\\nindex 00000000..97999883\\n--- /dev/null\\n+++ b/docs/workflows.md\\n@@ -0,0 +1,569 @@\\n+# Reusable Workflows\\n+\\n+Visor supports defining reusable workflows that can be used as building blocks in your CI/CD pipeline. Workflows allow you to create modular, parameterized sequences of checks that can be shared across projects and teams.\\n+\\n+## Table of Contents\\n+\\n+- [Overview](#overview)\\n+- [Workflow Structure](#workflow-structure)\\n+- [Input Parameters](#input-parameters)\\n+- [Output Parameters](#output-parameters)\\n+- [Using Workflows](#using-workflows)\\n+- [Advanced Features](#advanced-features)\\n+- [Examples](#examples)\\n+- [Best Practices](#best-practices)\\n+\\n+## Overview\\n+\\n+Workflows are reusable components that:\\n+- Accept input parameters (args) with JSON Schema validation\\n+- Define a sequence of steps at the root level (just like regular visor configs)\\n+- Produce output values that can be consumed by other checks\\n+- Must be defined in separate files and imported\\n+- Support all existing check types as steps\\n+\\n+## Workflow Structure\\n+\\n+Each workflow is defined in its own file with the following structure:\\n+\\n+```yaml\\n+# workflow-name.yaml\\n+id: workflow-name          # Unique identifier\\n+name: Workflow Display Name # Human-readable name\\n+description: What this workflow does\\n+version: \\\"1.0.0\\\"           # Semantic versioning\\n+\\n+# Input parameters\\n+inputs:\\n+  - name: param_name\\n+    description: Parameter description\\n+    schema:\\n+      type: string\\n+      enum: [\\\"option1\\\", \\\"option2\\\"]\\n+    default: \\\"option1\\\"\\n+    required: false\\n+\\n+# Output parameters\\n+outputs:\\n+  - name: result\\n+    description: Computation result\\n+    value_js: steps.analyze.output.score\\n+\\n+# Steps at root level - just like regular visor configs\\n+steps:\\n+  analyze:\\n+    type: ai\\n+    prompt: Analyze code with {{ inputs.param_name }}\\n+    focus: security\\n+```\\n+\\n+## Importing Workflows\\n+\\n+Import workflow files in your main configuration:\\n+\\n+```yaml\\n+# visor.yaml\\n+version: \\\"1.0\\\"\\n+\\n+# Import workflow definitions\\n+imports:\\n+  - ./workflows/security-scan.yaml\\n+  - ./workflows/code-quality.yaml\\n+  - https://example.com/workflows/shared.yaml\\n+\\n+# Use imported workflows in your steps\\n+steps:\\n+  security_check:\\n+    type: workflow\\n+    workflow: security-scan\\n+    args:\\n+      severity_threshold: high\\n+```\\n+\\n+## Input Parameters\\n+\\n+Workflows accept input parameters with JSON Schema validation:\\n+\\n+```yaml\\n+inputs:\\n+  - name: language\\n+    description: Programming language to analyze\\n+    schema:\\n+      type: string\\n+      enum: [\\\"javascript\\\", \\\"typescript\\\", \\\"python\\\", \\\"go\\\"]\\n+    required: true\\n+\\n+  - name: strict_mode\\n+    description: Enable strict checking\\n+    schema:\\n+      type: boolean\\n+    default: false\\n+    required: false\\n+\\n+  - name: patterns\\n+    description: Custom patterns to check\\n+    schema:\\n+      type: array\\n+      items:\\n+        type: string\\n+      minItems: 1\\n+```\\n+\\n+### Supported Schema Types\\n+\\n+- `string` - Text values with optional patterns, enums, length constraints\\n+- `number` - Numeric values with min/max constraints\\n+- `boolean` - True/false values\\n+- `array` - Lists with item schemas\\n+- `object` - Structured data with property schemas\\n+\\n+## Output Parameters\\n+\\n+Workflows produce outputs that can be consumed by other checks:\\n+\\n+```yaml\\n+outputs:\\n+  - name: total_issues\\n+    description: Total number of issues found\\n+    value_js: |\\n+      steps.scan1.output.issues.length +\\n+      steps.scan2.output.issues.length\\n+\\n+  - name: summary\\n+    description: Human-readable summary\\n+    value: |\\n+      Found {{ outputs.total_issues }} issues:\\n+      - Critical: {{ steps.scan1.output.critical_count }}\\n+      - Warning: {{ steps.scan2.output.warning_count }}\\n+```\\n+\\n+### Output Computation Methods\\n+\\n+1. **JavaScript expressions** (`value_js`): Compute outputs using JavaScript\\n+2. **Liquid templates** (`value`): Format outputs using Liquid templating\\n+\\n+## Workflow Steps\\n+\\n+Steps in a workflow support all standard check features:\\n+\\n+```yaml\\n+steps:\\n+  validate_input:\\n+    type: script\\n+    content: |\\n+      if (!inputs.api_key) {\\n+        throw new Error(\\\"API key is required\\\");\\n+      }\\n+      return { valid: true };\\n+\\n+  fetch_data:\\n+    type: http_client\\n+    url: https://api.example.com/data\\n+    headers:\\n+      Authorization: \\\"Bearer {{ inputs.api_key }}\\\"\\n+    depends_on: [validate_input]\\n+\\n+  analyze_data:\\n+    type: ai\\n+    prompt: |\\n+      Analyze the following data:\\n+      {{ steps.fetch_data.output | json }}\\n+\\n+      Apply threshold: {{ inputs.threshold }}\\n+    depends_on: [fetch_data]\\n+\\n+  store_results:\\n+    type: memory\\n+    operation: set\\n+    key: analysis_results\\n+    value: \\\"{{ steps.analyze_data.output }}\\\"\\n+    depends_on: [analyze_data]\\n+```\\n+\\n+### Step Input Mappings\\n+\\n+Map workflow inputs to step parameters:\\n+\\n+```yaml\\n+steps:\\n+  my_step:\\n+    type: command\\n+    exec: echo \\\"Processing...\\\"\\n+    inputs:\\n+      # Direct parameter reference\\n+      param1:\\n+        source: param\\n+        value: input_name\\n+\\n+      # Step output reference\\n+      param2:\\n+        source: step\\n+        stepId: previous_step\\n+        outputParam: result\\n+\\n+      # Constant value\\n+      param3:\\n+        source: constant\\n+        value: \\\"fixed value\\\"\\n+\\n+      # JavaScript expression\\n+      param4:\\n+        source: expression\\n+        expression: inputs.value * 2\\n+```\\n+\\n+## Using Workflows\\n+\\n+### Basic Usage\\n+\\n+Use a workflow as a check with the `workflow` type:\\n+\\n+```yaml\\n+steps:\\n+  security_check:\\n+    type: workflow\\n+    workflow: security-scan  # Workflow ID from imported file\\n+    args:\\n+      severity_threshold: high\\n+      scan_dependencies: true\\n+    on: [pr_opened, pr_updated]\\n+```\\n+\\n+### With Output Mapping\\n+\\n+Map workflow outputs to check outputs:\\n+\\n+```yaml\\n+steps:\\n+  quality_analysis:\\n+    type: workflow\\n+    workflow: code-quality\\n+    args:\\n+      language: typescript\\n+    output_mapping:\\n+      final_score: quality_score  # Map workflow output to check output\\n+      issues_list: recommendations\\n+```\\n+\\n+### With Step Overrides\\n+\\n+Override specific steps in the workflow:\\n+\\n+```yaml\\n+steps:\\n+  custom_scan:\\n+    type: workflow\\n+    workflow: security-scan\\n+    args:\\n+      severity_threshold: low\\n+    overrides:\\n+      secrets:  # Override the 'secrets' step\\n+        prompt: \\\"Custom prompt for secret scanning\\\"\\n+        timeout: 120\\n+      sql_injection:  # Override the 'sql_injection' step\\n+        ai_model: claude-3-opus-20240229\\n+```\\n+\\n+## Advanced Features\\n+\\n+### Conditional Steps\\n+\\n+Use conditions in workflow steps:\\n+\\n+```yaml\\n+steps:\\n+  optional_check:\\n+    type: ai\\n+    prompt: Run expensive check\\n+    if: inputs.enable_expensive_checks === true\\n+```\\n+\\n+### Dynamic Routing\\n+\\n+Use workflow outputs for dynamic behavior:\\n+\\n+```yaml\\n+steps:\\n+  decision_point:\\n+    type: script\\n+    content: |\\n+      if (outputs.severity_check.critical_count > 0) {\\n+        return { next_action: \\\"block\\\" };\\n+      }\\n+      return { next_action: \\\"proceed\\\" };\\n+\\n+  follow_up:\\n+    type: workflow\\n+    workflow: \\\"{{ steps.decision_point.output.next_action }}-workflow\\\"\\n+    depends_on: [decision_point]\\n+```\\n+\\n+### Workflow Composition\\n+\\n+Workflows can use other workflows:\\n+\\n+```yaml\\n+workflows:\\n+  comprehensive-check:\\n+    steps:\\n+      security:\\n+        type: workflow\\n+        workflow: security-scan\\n+        workflow_inputs:\\n+          severity_threshold: \\\"{{ inputs.security_level }}\\\"\\n+\\n+      quality:\\n+        type: workflow\\n+        workflow: code-quality\\n+        workflow_inputs:\\n+          language: \\\"{{ inputs.language }}\\\"\\n+\\n+      aggregate:\\n+        type: script\\n+        content: |\\n+          return {\\n+            passed: steps.security.output.passed && steps.quality.output.passed,\\n+            score: (steps.security.output.score + steps.quality.output.score) / 2\\n+          };\\n+        depends_on: [security, quality]\\n+```\\n+\\n+## Examples\\n+\\n+### Security Scan Workflow\\n+\\n+```yaml\\n+id: security-scan\\n+name: Security Scanner\\n+inputs:\\n+  - name: scan_level\\n+    schema:\\n+      type: string\\n+      enum: [basic, standard, comprehensive]\\n+    default: standard\\n+\\n+outputs:\\n+  - name: vulnerabilities\\n+    value_js: |\\n+      [...(steps.secrets.output.issues || []),\\n+       ...(steps.injection.output.issues || [])]\\n+\\n+  - name: passed\\n+    value_js: outputs.vulnerabilities.length === 0\\n+\\n+steps:\\n+  secrets:\\n+    type: ai\\n+    prompt: Scan for hardcoded secrets and API keys\\n+\\n+  injection:\\n+    type: ai\\n+    prompt: Check for injection vulnerabilities\\n+    depends_on: [secrets]\\n+```\\n+\\n+### Multi-Language Support Workflow\\n+\\n+```yaml\\n+id: language-check\\n+name: Multi-Language Analyzer\\n+\\n+inputs:\\n+  - name: languages\\n+    schema:\\n+      type: array\\n+      items:\\n+        type: string\\n+\\n+steps:\\n+  detect_languages:\\n+    type: script\\n+    content: |\\n+      const detected = [];\\n+      if (filesChanged.some(f => f.endsWith('.js'))) detected.push('javascript');\\n+      if (filesChanged.some(f => f.endsWith('.py'))) detected.push('python');\\n+      return { languages: detected };\\n+\\n+  analyze_each:\\n+    type: ai\\n+    forEach: true\\n+    prompt: Analyze {{ item }} code for best practices\\n+    depends_on: [detect_languages]\\n+\\n+  summarize:\\n+    type: script\\n+    content: |\\n+      const results = outputs.analyze_each;\\n+      return {\\n+        total_issues: results.reduce((sum, r) => sum + r.issues.length, 0),\\n+        by_language: results.map((r, i) => ({\\n+          language: steps.detect_languages.output.languages[i],\\n+          issues: r.issues.length\\n+        }))\\n+      };\\n+    depends_on: [analyze_each]\\n+```\\n+\\n+## Best Practices\\n+\\n+### 1. Design for Reusability\\n+\\n+- Use meaningful parameter names\\n+- Provide sensible defaults\\n+- Document all inputs and outputs\\n+- Keep workflows focused on a single concern\\n+\\n+### 2. Validate Inputs\\n+\\n+```yaml\\n+inputs:\\n+  - name: url\\n+    schema:\\n+      type: string\\n+      format: uri\\n+      pattern: \\\"^https://\\\"\\n+    description: HTTPS URL only\\n+```\\n+\\n+### 3. Handle Errors Gracefully\\n+\\n+```yaml\\n+steps:\\n+  safe_operation:\\n+    type: script\\n+    content: |\\n+      try {\\n+        return processData(inputs.data);\\n+      } catch (error) {\\n+        return {\\n+          success: false,\\n+          error: error.message,\\n+          fallback: inputs.default_value\\n+        };\\n+      }\\n+```\\n+\\n+### 4. Version Your Workflows\\n+\\n+```yaml\\n+version: \\\"2.0.0\\\"  # Semantic versioning\\n+# Breaking changes from 1.x:\\n+# - Renamed 'threshold' input to 'quality_threshold'\\n+# - Added required 'language' input\\n+```\\n+\\n+### 5. Provide Examples\\n+\\n+```yaml\\n+examples:\\n+  - name: Basic usage\\n+    description: Run with default settings\\n+    inputs:\\n+      severity: medium\\n+\\n+  - name: Strict mode\\n+    description: Maximum security scanning\\n+    inputs:\\n+      severity: critical\\n+      deep_scan: true\\n+```\\n+\\n+### 6. Test Your Workflows\\n+\\n+Create test configurations to validate workflows:\\n+\\n+```yaml\\n+# test-workflow.yaml\\n+steps:\\n+  test_workflow:\\n+    type: workflow\\n+    workflow: my-workflow\\n+    workflow_inputs:\\n+      test_param: \\\"test_value\\\"\\n+\\n+  validate_output:\\n+    type: script\\n+    content: |\\n+      const output = outputs.test_workflow;\\n+      assert(output.result !== undefined, \\\"Result is required\\\");\\n+      assert(output.score >= 0 && output.score <= 100, \\\"Score out of range\\\");\\n+    depends_on: [test_workflow]\\n+```\\n+\\n+## Workflow Schema Reference\\n+\\n+Complete workflow schema:\\n+\\n+```typescript\\n+interface WorkflowDefinition {\\n+  id: string;                    // Unique identifier\\n+  name: string;                   // Display name\\n+  description?: string;           // Description\\n+  version?: string;              // Semantic version\\n+  tags?: string[];               // Categorization tags\\n+  category?: string;             // Category (security, quality, etc.)\\n+\\n+  inputs?: WorkflowInputParam[]; // Input parameters\\n+  outputs?: WorkflowOutputParam[]; // Output parameters\\n+  steps: Record<string, WorkflowStep>; // Workflow steps\\n+\\n+  on?: EventTrigger[];          // Events that can trigger this workflow\\n+  defaults?: Partial<CheckConfig>; // Default config for steps\\n+  reusable?: boolean;            // Can be used as component\\n+\\n+  author?: {                    // Author information\\n+    name?: string;\\n+    email?: string;\\n+    url?: string;\\n+  };\\n+\\n+  license?: string;              // License information\\n+  examples?: WorkflowExample[];  // Usage examples\\n+}\\n+```\\n+\\n+## Integration with CI/CD\\n+\\n+Workflows integrate seamlessly with GitHub Actions:\\n+\\n+```yaml\\n+name: PR Review\\n+on: [pull_request]\\n+\\n+jobs:\\n+  visor:\\n+    runs-on: ubuntu-latest\\n+    steps:\\n+      - uses: actions/checkout@v3\\n+\\n+      - name: Run Visor with Workflows\\n+        uses: your-org/visor-action@v1\\n+        with:\\n+          config: .visor.yaml\\n+          workflow_imports: |\\n+            ./workflows/*.yaml\\n+            https://workflows.example.com/shared/*.yaml\\n+```\\n+\\n+## Troubleshooting\\n+\\n+### Common Issues\\n+\\n+1. **Workflow not found**: Ensure the workflow is registered via `workflows` or `workflow_imports`\\n+2. **Input validation failed**: Check that inputs match the defined schema\\n+3. **Circular dependencies**: Ensure workflow steps don't have circular `depends_on`\\n+4. **Output computation error**: Verify JavaScript expressions and Liquid templates are valid\\n+\\n+### Debug Mode\\n+\\n+Enable debug output to troubleshoot workflows:\\n+\\n+```bash\\n+visor --debug --config visor.yaml\\n+```\\n+\\n+This will show:\\n+- Workflow registration details\\n+- Input validation results\\n+- Step execution order\\n+- Output computation values\\n\\\\ No newline at end of file\\n\",\"status\":\"added\"},{\"filename\":\"eslint.config.js\",\"additions\":1,\"deletions\":1,\"changes\":19,\"patch\":\"diff --git a/eslint.config.js b/eslint.config.js\\nindex a6829e6f..fb839364 100644\\n--- a/eslint.config.js\\n+++ b/eslint.config.js\\n@@ -8,6 +8,10 @@ module.exports = [\\n         sourceType: 'module',\\n       },\\n     },\\n+    linterOptions: {\\n+      // Do not warn about legacy disable comments as we migrate types\\n+      reportUnusedDisableDirectives: false,\\n+    },\\n     plugins: {\\n       '@typescript-eslint': require('@typescript-eslint/eslint-plugin'),\\n     },\\n@@ -16,10 +20,21 @@ module.exports = [\\n       'no-console': 'off',\\n       'prefer-const': 'error',\\n       'no-var': 'error',\\n+      // Prefer our extended Liquid engine everywhere\\n+      'no-restricted-imports': [\\n+        'warn',\\n+        {\\n+          name: 'liquidjs',\\n+          message: 'Use createExtendedLiquid() from src/liquid-extensions instead of raw Liquid.',\\n+        },\\n+      ],\\n       \\n       // TypeScript rules\\n-      '@typescript-eslint/no-unused-vars': ['error', { argsIgnorePattern: '^_' }],\\n-      '@typescript-eslint/no-explicit-any': 'warn',\\n+      '@typescript-eslint/no-unused-vars': ['error', { argsIgnorePattern: '^_', caughtErrorsIgnorePattern: '^_' }],\\n+      // Older engine files intentionally use narrow 'any' in a few places.\\n+      // Treat as disabled to keep CI and pre-commit green; we can re-enable\\n+      // per-file with explicit types in a follow-up.\\n+      '@typescript-eslint/no-explicit-any': 'off',\\n     },\\n   },\\n   {\\n\",\"status\":\"modified\"},{\"filename\":\"examples/ai-with-bash.yaml\",\"additions\":4,\"deletions\":0,\"changes\":126,\"patch\":\"diff --git a/examples/ai-with-bash.yaml b/examples/ai-with-bash.yaml\\nnew file mode 100644\\nindex 00000000..4ac971b2\\n--- /dev/null\\n+++ b/examples/ai-with-bash.yaml\\n@@ -0,0 +1,126 @@\\n+# Example Visor configuration demonstrating bash command execution in AI checks\\n+version: \\\"1.0\\\"\\n+\\n+# Global AI provider configuration\\n+ai_provider: anthropic\\n+ai_model: claude-3-sonnet\\n+\\n+steps:\\n+  # Example 1: Simple - Enable bash with default safe commands\\n+  git-status-analysis:\\n+    type: ai\\n+    prompt: |\\n+      Analyze the current git repository status:\\n+      - Check for uncommitted changes\\n+      - Review the current branch\\n+      - List recent commits\\n+      - Identify any potential issues\\n+    ai:\\n+      provider: anthropic\\n+      model: claude-3-opus\\n+      allowBash: true  # Simple one-line enable\\n+    on: [\\\"pr_opened\\\", \\\"pr_updated\\\"]\\n+    tags: [\\\"git\\\", \\\"analysis\\\"]\\n+\\n+  # Example 2: Advanced - Custom allow/deny lists for npm commands\\n+  npm-audit-check:\\n+    type: ai\\n+    prompt: |\\n+      Run npm audit and analyze the security vulnerabilities:\\n+      - Check for high/critical vulnerabilities\\n+      - Review outdated dependencies\\n+      - Suggest remediation steps\\n+    ai:\\n+      provider: google\\n+      model: gemini-2.0-flash-exp\\n+      allowBash: true\\n+      bashConfig:\\n+        allow:\\n+          - 'npm audit --json'\\n+          - 'npm outdated --json'\\n+          - 'npm list --depth=0'\\n+        timeout: 60000  # 60 second timeout\\n+    on: [\\\"pr_opened\\\"]\\n+    tags: [\\\"security\\\", \\\"npm\\\"]\\n+\\n+  # Example 3: Advanced - Test execution with custom config\\n+  test-runner-analysis:\\n+    type: ai\\n+    prompt: |\\n+      Run the test suite and analyze the results:\\n+      - Execute all tests\\n+      - Identify failing tests\\n+      - Review code coverage\\n+      - Suggest improvements\\n+    ai:\\n+      provider: anthropic\\n+      allowBash: true\\n+      bashConfig:\\n+        allow:\\n+          - 'npm test'\\n+          - 'npm run test:coverage'\\n+        deny:\\n+          - 'npm install'  # Explicitly block installation\\n+        timeout: 300000  # 5 minute timeout for tests\\n+        workingDirectory: '.'\\n+    on: [\\\"pr_opened\\\", \\\"pr_updated\\\"]\\n+    tags: [\\\"tests\\\", \\\"coverage\\\"]\\n+\\n+  # Example 4: Advanced - Build and lint with timeouts\\n+  build-lint-check:\\n+    type: ai\\n+    prompt: |\\n+      Run build and lint checks:\\n+      - Execute the build process\\n+      - Run ESLint\\n+      - Check TypeScript compilation\\n+      - Review any errors or warnings\\n+    ai:\\n+      provider: openai\\n+      model: gpt-4\\n+      allowBash: true\\n+      bashConfig:\\n+        allow:\\n+          - 'npm run build'\\n+          - 'npm run lint'\\n+          - 'tsc --noEmit'\\n+        timeout: 180000  # 3 minute timeout\\n+    on: [\\\"pr_opened\\\"]\\n+    tags: [\\\"build\\\", \\\"lint\\\"]\\n+\\n+  # Example 5: Expert - Custom commands only (no defaults)\\n+  custom-commands-only:\\n+    type: ai\\n+    prompt: \\\"Run custom analysis commands with strict control\\\"\\n+    ai:\\n+      provider: anthropic\\n+      allowBash: true\\n+      bashConfig:\\n+        noDefaultAllow: true  # Disable default safe commands\\n+        noDefaultDeny: false  # Keep dangerous command blocklist\\n+        allow:\\n+          - 'custom-tool analyze'\\n+          - 'custom-tool report'\\n+        timeout: 30000\\n+    on: [\\\"manual\\\"]\\n+    tags: [\\\"custom\\\", \\\"advanced\\\"]\\n+\\n+  # Example 6: Simple - File system analysis with defaults\\n+  filesystem-analysis:\\n+    type: ai\\n+    prompt: |\\n+      Analyze the project file structure:\\n+      - List all source files\\n+      - Check file sizes\\n+      - Review directory structure\\n+      - Identify any organizational issues\\n+    ai:\\n+      provider: anthropic\\n+      allowBash: true  # Uses default safe commands (ls, find, etc.)\\n+    on: [\\\"pr_opened\\\"]\\n+    tags: [\\\"filesystem\\\", \\\"structure\\\"]\\n+\\n+output:\\n+  pr_comment:\\n+    enabled: true\\n+    group_by: check\\n\",\"status\":\"added\"},{\"filename\":\"examples/custom-tools-example.yaml\",\"additions\":8,\"deletions\":0,\"changes\":281,\"patch\":\"diff --git a/examples/custom-tools-example.yaml b/examples/custom-tools-example.yaml\\nnew file mode 100644\\nindex 00000000..1fcfdaef\\n--- /dev/null\\n+++ b/examples/custom-tools-example.yaml\\n@@ -0,0 +1,281 @@\\n+version: \\\"1.0\\\"\\n+\\n+# Custom Tool Definitions\\n+# These tools can be used in any MCP block with transport: custom\\n+tools:\\n+  # Simple grep tool for finding patterns in files\\n+  grep-pattern:\\n+    name: grep-pattern\\n+    description: Search for patterns in files using grep\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        pattern:\\n+          type: string\\n+          description: Regular expression pattern to search for\\n+        files:\\n+          type: array\\n+          items:\\n+            type: string\\n+          description: List of files to search in\\n+      required: [pattern]\\n+    exec: 'grep -n \\\"{{ args.pattern }}\\\" {{ args.files | join: \\\" \\\" }}'\\n+    transform_js: |\\n+      // Parse grep output into structured format\\n+      const lines = output.trim().split('\\\\n').filter(l => l);\\n+      return lines.map(line => {\\n+        const match = line.match(/^([^:]+):(\\\\d+):(.*)$/);\\n+        if (!match) return null;\\n+        return {\\n+          file: match[1],\\n+          line: parseInt(match[2]),\\n+          content: match[3].trim()\\n+        };\\n+      }).filter(Boolean);\\n+\\n+  # Tool to count lines of code\\n+  count-lines:\\n+    name: count-lines\\n+    description: Count lines of code in files\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        files:\\n+          type: array\\n+          items:\\n+            type: string\\n+          description: Files to count lines in\\n+        exclude_blank:\\n+          type: boolean\\n+          description: Exclude blank lines from count\\n+    exec: |\\n+      {% if args.exclude_blank %}\\n+        cat {{ args.files | join: \\\" \\\" }} | grep -v \\\"^$\\\" | wc -l\\n+      {% else %}\\n+        cat {{ args.files | join: \\\" \\\" }} | wc -l\\n+      {% endif %}\\n+    transform_js: 'return parseInt(output.trim());'\\n+\\n+  # Tool to check if a file contains sensitive data patterns\\n+  check-secrets:\\n+    name: check-secrets\\n+    description: Check for potential secrets in files\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        file:\\n+          type: string\\n+          description: File to check\\n+      required: [file]\\n+    exec: |\\n+      grep -E \\\"(api[_-]?key|secret|token|password|pwd|auth|credential)\\\" -i \\\"{{ args.file }}\\\" || echo \\\"No secrets found\\\"\\n+    transform_js: |\\n+      if (output.includes(\\\"No secrets found\\\")) {\\n+        return { safe: true, issues: [] };\\n+      }\\n+      const lines = output.trim().split('\\\\n');\\n+      return {\\n+        safe: false,\\n+        issues: lines.map(line => ({\\n+          type: 'potential_secret',\\n+          content: line.substring(0, 100) // Truncate for safety\\n+        }))\\n+      };\\n+\\n+  # Tool to analyze code complexity\\n+  analyze-complexity:\\n+    name: analyze-complexity\\n+    description: Analyze code complexity metrics\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        file:\\n+          type: string\\n+          description: Source code file to analyze\\n+        language:\\n+          type: string\\n+          description: Programming language (js, py, go, etc.)\\n+      required: [file]\\n+    exec: |\\n+      echo \\\"Analyzing {{ args.file }}\\\"\\n+      # Count functions/methods\\n+      {% if args.language == \\\"js\\\" or args.language == \\\"ts\\\" %}\\n+        grep -c \\\"function\\\\|=>\\\" \\\"{{ args.file }}\\\" || echo \\\"0\\\"\\n+      {% elsif args.language == \\\"py\\\" %}\\n+        grep -c \\\"def \\\" \\\"{{ args.file }}\\\" || echo \\\"0\\\"\\n+      {% elsif args.language == \\\"go\\\" %}\\n+        grep -c \\\"func \\\" \\\"{{ args.file }}\\\" || echo \\\"0\\\"\\n+      {% else %}\\n+        echo \\\"0\\\"\\n+      {% endif %}\\n+    transform_js: |\\n+      const functionCount = parseInt(output.trim().split('\\\\n').pop() || '0');\\n+      return {\\n+        file: args.file,\\n+        language: args.language || 'unknown',\\n+        metrics: {\\n+          functionCount: functionCount,\\n+          complexity: functionCount > 10 ? 'high' : functionCount > 5 ? 'medium' : 'low'\\n+        }\\n+      };\\n+\\n+  # Tool to validate JSON/YAML files\\n+  validate-config:\\n+    name: validate-config\\n+    description: Validate configuration file syntax\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        file:\\n+          type: string\\n+          description: Configuration file to validate\\n+        type:\\n+          type: string\\n+          enum: [json, yaml]\\n+          description: File type to validate\\n+      required: [file, type]\\n+    exec: |\\n+      {% if args.type == \\\"json\\\" %}\\n+        python3 -m json.tool \\\"{{ args.file }}\\\" > /dev/null 2>&1 && echo \\\"Valid JSON\\\" || echo \\\"Invalid JSON\\\"\\n+      {% else %}\\n+        python3 -c \\\"import yaml; yaml.safe_load(open('{{ args.file }}'))\\\" 2>&1 && echo \\\"Valid YAML\\\" || echo \\\"Invalid YAML\\\"\\n+      {% endif %}\\n+    parseJson: false\\n+    transform_js: |\\n+      const isValid = output.includes(\\\"Valid\\\");\\n+      return {\\n+        file: args.file,\\n+        type: args.type,\\n+        valid: isValid,\\n+        message: output.trim()\\n+      };\\n+\\n+  # Tool to generate file statistics\\n+  file-stats:\\n+    name: file-stats\\n+    description: Generate statistics about files\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        pattern:\\n+          type: string\\n+          description: Glob pattern for files to analyze\\n+    exec: |\\n+      find . -name \\\"{{ args.pattern }}\\\" -type f | xargs wc -l | tail -1\\n+    transform_js: |\\n+      const parts = output.trim().split(/\\\\s+/);\\n+      return {\\n+        totalLines: parseInt(parts[0] || '0'),\\n+        fileCount: Math.max(0, parts.length - 2) // Subtract total line\\n+      };\\n+\\n+# Example usage of custom tools in checks\\n+steps:\\n+  # Use custom tool to find TODO comments\\n+  find-todos:\\n+    type: mcp\\n+    transport: custom\\n+    method: grep-pattern\\n+    methodArgs:\\n+      pattern: \\\"TODO|FIXME|HACK\\\"\\n+      files: [\\\"*.js\\\", \\\"*.ts\\\", \\\"*.py\\\"]\\n+    transform_js: |\\n+      // Convert grep results to issues\\n+      output.map(match => ({\\n+        file: match.file,\\n+        line: match.line,\\n+        message: `Found comment: ${match.content}`,\\n+        severity: match.content.includes('FIXME') ? 'warning' : 'info',\\n+        category: 'documentation',\\n+        ruleId: 'todo-comment'\\n+      }))\\n+\\n+  # Check for secrets using custom tool\\n+  security-check:\\n+    type: mcp\\n+    transport: custom\\n+    method: check-secrets\\n+    forEach: \\\"{{ files | map: 'filename' }}\\\"\\n+    methodArgs:\\n+      file: \\\"{{ item }}\\\"\\n+    transform_js: |\\n+      if (!output.safe) {\\n+        return output.issues.map(issue => ({\\n+          file: args.file,\\n+          line: 0,\\n+          message: `Potential secret detected: ${issue.type}`,\\n+          severity: 'critical',\\n+          category: 'security',\\n+          ruleId: 'potential-secret'\\n+        }));\\n+      }\\n+      return [];\\n+\\n+  # Analyze code complexity\\n+  complexity-analysis:\\n+    type: mcp\\n+    transport: custom\\n+    method: analyze-complexity\\n+    forEach: \\\"{{ files | where: 'filename', 'endsWith', '.js' }}\\\"\\n+    methodArgs:\\n+      file: \\\"{{ item.filename }}\\\"\\n+      language: \\\"js\\\"\\n+    transform_js: |\\n+      const complexity = output.metrics.complexity;\\n+      if (complexity === 'high') {\\n+        return [{\\n+          file: output.file,\\n+          line: 0,\\n+          message: `High complexity detected: ${output.metrics.functionCount} functions`,\\n+          severity: 'warning',\\n+          category: 'performance',\\n+          ruleId: 'high-complexity'\\n+        }];\\n+      }\\n+      return [];\\n+\\n+  # Validate all JSON config files\\n+  validate-json-configs:\\n+    type: mcp\\n+    transport: custom\\n+    method: validate-config\\n+    forEach: \\\"{{ files | where: 'filename', 'endsWith', '.json' }}\\\"\\n+    methodArgs:\\n+      file: \\\"{{ item.filename }}\\\"\\n+      type: \\\"json\\\"\\n+    fail_if: \\\"!output.valid\\\"\\n+    transform_js: |\\n+      if (!output.valid) {\\n+        return [{\\n+          file: output.file,\\n+          line: 0,\\n+          message: output.message,\\n+          severity: 'error',\\n+          category: 'style',\\n+          ruleId: 'invalid-json'\\n+        }];\\n+      }\\n+      return [];\\n+\\n+  # Get repository statistics\\n+  repo-stats:\\n+    type: mcp\\n+    transport: custom\\n+    method: file-stats\\n+    methodArgs:\\n+      pattern: \\\"*.{js,ts,py,go}\\\"\\n+    transform_js: |\\n+      // Create informational message about repo size\\n+      [{\\n+        file: 'repository',\\n+        line: 0,\\n+        message: `Repository contains ${output.fileCount} source files with ${output.totalLines} total lines`,\\n+        severity: 'info',\\n+        category: 'documentation',\\n+        ruleId: 'repo-stats'\\n+      }]\\n+\\n+# Output configuration\\n+output:\\n+  format: table\\n+  groupBy: severity\\n\\\\ No newline at end of file\\n\",\"status\":\"added\"},{\"filename\":\"examples/project-with-tools.yaml\",\"additions\":5,\"deletions\":0,\"changes\":174,\"patch\":\"diff --git a/examples/project-with-tools.yaml b/examples/project-with-tools.yaml\\nnew file mode 100644\\nindex 00000000..30dede5b\\n--- /dev/null\\n+++ b/examples/project-with-tools.yaml\\n@@ -0,0 +1,174 @@\\n+version: \\\"1.0\\\"\\n+\\n+# This configuration extends the tools library to reuse tool definitions\\n+extends: ./tools-library.yaml\\n+\\n+# Additional project-specific tools can be defined here\\n+tools:\\n+  # Project-specific tool for checking API endpoints\\n+  check-api-endpoints:\\n+    name: check-api-endpoints\\n+    description: Verify all API endpoints are documented\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        source_dir:\\n+          type: string\\n+          description: Directory containing API source code\\n+        docs_file:\\n+          type: string\\n+          description: API documentation file\\n+    exec: |\\n+      echo \\\"Checking endpoints in {{ args.source_dir }} against {{ args.docs_file }}\\\"\\n+      grep -r \\\"router\\\\.\\\\(get\\\\|post\\\\|put\\\\|delete\\\\|patch\\\\)\\\" {{ args.source_dir }} |\\n+      sed 's/.*router\\\\.\\\\([a-z]*\\\\).*\\\"\\\\([^\\\"]*\\\\)\\\".*/\\\\1 \\\\2/' |\\n+      sort -u\\n+    transform_js: |\\n+      const endpoints = output.trim().split('\\\\n').filter(l => l);\\n+      return endpoints.map(ep => {\\n+        const [method, path] = ep.split(' ');\\n+        return { method: method.toUpperCase(), path };\\n+      });\\n+\\n+# Use both imported and local tools in checks\\n+steps:\\n+  # Use imported git tool\\n+  check-git-status:\\n+    type: mcp\\n+    transport: custom\\n+    method: git-status\\n+    fail_if: \\\"output.length > 10\\\"\\n+    transform_js: |\\n+      if (output.length > 10) {\\n+        return [{\\n+          file: 'repository',\\n+          line: 0,\\n+          message: `Too many uncommitted changes: ${output.length} files`,\\n+          severity: 'warning',\\n+          category: 'style',\\n+          ruleId: 'uncommitted-changes'\\n+        }];\\n+      }\\n+      return [];\\n+\\n+  # Use imported git diff stats\\n+  analyze-pr-size:\\n+    type: mcp\\n+    transport: custom\\n+    method: git-diff-stats\\n+    methodArgs:\\n+      base: \\\"{{ pr.base | default: 'main' }}\\\"\\n+    fail_if: \\\"output.filesChanged > 50 || output.insertions > 1000\\\"\\n+    transform_js: |\\n+      const issues = [];\\n+      if (output.filesChanged > 50) {\\n+        issues.push({\\n+          file: 'pull-request',\\n+          line: 0,\\n+          message: `Large PR: ${output.filesChanged} files changed. Consider breaking into smaller PRs.`,\\n+          severity: 'warning',\\n+          category: 'style',\\n+          ruleId: 'large-pr'\\n+        });\\n+      }\\n+      if (output.insertions > 1000) {\\n+        issues.push({\\n+          file: 'pull-request',\\n+          line: 0,\\n+          message: `Too many lines added: ${output.insertions}. This may be difficult to review.`,\\n+          severity: 'warning',\\n+          category: 'style',\\n+          ruleId: 'too-many-lines'\\n+        });\\n+      }\\n+      return issues;\\n+\\n+  # Use imported npm audit tool\\n+  security-audit:\\n+    type: mcp\\n+    transport: custom\\n+    method: npm-audit\\n+    if: \\\"files.some(f => f.filename === 'package.json')\\\"\\n+    fail_if: \\\"output.some(i => i.severity === 'error')\\\"\\n+\\n+  # Use imported Docker linting tool\\n+  lint-dockerfiles:\\n+    type: mcp\\n+    transport: custom\\n+    method: docker-lint\\n+    forEach: \\\"{{ files | where: 'filename', 'match', 'Dockerfile' }}\\\"\\n+    methodArgs:\\n+      file: \\\"{{ item.filename }}\\\"\\n+\\n+  # Use imported ESLint tool\\n+  lint-javascript:\\n+    type: mcp\\n+    transport: custom\\n+    method: eslint-check\\n+    if: \\\"files.some(f => f.filename.endsWith('.js') || f.filename.endsWith('.ts'))\\\"\\n+    methodArgs:\\n+      files: \\\"{{ files | where: 'filename', 'match', '\\\\\\\\.(js|ts)$' | map: 'filename' }}\\\"\\n+\\n+  # Use local project-specific tool\\n+  verify-api-docs:\\n+    type: mcp\\n+    transport: custom\\n+    method: check-api-endpoints\\n+    methodArgs:\\n+      source_dir: \\\"./src/routes\\\"\\n+      docs_file: \\\"./docs/api.md\\\"\\n+    transform_js: |\\n+      // Check if all endpoints are documented\\n+      const documented = ['GET /users', 'POST /users', 'GET /posts']; // Would parse from docs_file\\n+      const undocumented = output.filter(ep =>\\n+        !documented.includes(`${ep.method} ${ep.path}`)\\n+      );\\n+\\n+      return undocumented.map(ep => ({\\n+        file: 'docs/api.md',\\n+        line: 0,\\n+        message: `Undocumented endpoint: ${ep.method} ${ep.path}`,\\n+        severity: 'warning',\\n+        category: 'documentation',\\n+        ruleId: 'missing-api-docs'\\n+      }));\\n+\\n+  # Chain multiple tools together\\n+  comprehensive-check:\\n+    type: mcp\\n+    transport: custom\\n+    method: run-tests\\n+    methodArgs:\\n+      command: \\\"npm test\\\"\\n+      format: \\\"jest\\\"\\n+    on_success:\\n+      - type: mcp\\n+        transport: custom\\n+        method: check-outdated\\n+        methodArgs:\\n+          manager: \\\"npm\\\"\\n+        transform_js: |\\n+          // Only warn about major version updates\\n+          return output\\n+            .filter(pkg => pkg.current.split('.')[0] !== pkg.latest.split('.')[0])\\n+            .map(pkg => ({\\n+              file: 'package.json',\\n+              line: 0,\\n+              message: `Major update available for ${pkg.package}: ${pkg.current} ‚Üí ${pkg.latest}`,\\n+              severity: 'info',\\n+              category: 'documentation',\\n+              ruleId: 'major-update-available'\\n+            }));\\n+\\n+# You can also import tools from remote URLs\\n+# extends: https://example.com/shared-tools.yaml\\n+\\n+# Or import multiple tool libraries\\n+# extends:\\n+#   - ./tools-library.yaml\\n+#   - ./security-tools.yaml\\n+#   - https://example.com/quality-tools.yaml\\n+\\n+output:\\n+  format: markdown\\n+  groupBy: category\\n\\\\ No newline at end of file\\n\",\"status\":\"added\"},{\"filename\":\"examples/tools-library.yaml\",\"additions\":8,\"deletions\":0,\"changes\":281,\"patch\":\"diff --git a/examples/tools-library.yaml b/examples/tools-library.yaml\\nnew file mode 100644\\nindex 00000000..b21e4433\\n--- /dev/null\\n+++ b/examples/tools-library.yaml\\n@@ -0,0 +1,281 @@\\n+version: \\\"1.0\\\"\\n+\\n+# Reusable Tool Library\\n+# This file contains only tool definitions that can be imported by other configs\\n+\\n+tools:\\n+  # Git tools\\n+  git-status:\\n+    name: git-status\\n+    description: Get git repository status\\n+    exec: 'git status --porcelain'\\n+    transform_js: |\\n+      const lines = output.trim().split('\\\\n').filter(l => l);\\n+      return lines.map(line => {\\n+        const [status, ...pathParts] = line.trim().split(/\\\\s+/);\\n+        return {\\n+          status: status,\\n+          file: pathParts.join(' ')\\n+        };\\n+      });\\n+\\n+  git-diff-stats:\\n+    name: git-diff-stats\\n+    description: Get statistics about changes\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        base:\\n+          type: string\\n+          description: Base branch to compare against\\n+    exec: 'git diff --stat {{ args.base }}..HEAD'\\n+    transform_js: |\\n+      const lines = output.trim().split('\\\\n');\\n+      const summary = lines[lines.length - 1];\\n+      const match = summary.match(/(\\\\d+) files? changed(?:, (\\\\d+) insertions?)?(?:, (\\\\d+) deletions?)?/);\\n+      return {\\n+        filesChanged: parseInt(match?.[1] || '0'),\\n+        insertions: parseInt(match?.[2] || '0'),\\n+        deletions: parseInt(match?.[3] || '0')\\n+      };\\n+\\n+  git-log-recent:\\n+    name: git-log-recent\\n+    description: Get recent commit messages\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        count:\\n+          type: number\\n+          description: Number of commits to retrieve\\n+    exec: 'git log --oneline -n {{ args.count | default: 5 }}'\\n+    transform_js: |\\n+      const lines = output.trim().split('\\\\n').filter(l => l);\\n+      return lines.map(line => {\\n+        const [hash, ...messageParts] = line.split(/\\\\s+/);\\n+        return {\\n+          hash: hash,\\n+          message: messageParts.join(' ')\\n+        };\\n+      });\\n+\\n+  # Docker tools\\n+  docker-lint:\\n+    name: docker-lint\\n+    description: Lint Dockerfile for best practices\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        file:\\n+          type: string\\n+          description: Dockerfile to lint\\n+      required: [file]\\n+    exec: 'hadolint {{ args.file }} --format json || echo \\\"[]\\\"'\\n+    parseJson: true\\n+    transform_js: |\\n+      // Convert hadolint output to issues\\n+      return output.map(issue => ({\\n+        file: args.file,\\n+        line: issue.line || 0,\\n+        message: issue.message,\\n+        severity: issue.level === 'error' ? 'error' : issue.level === 'warning' ? 'warning' : 'info',\\n+        category: 'style',\\n+        ruleId: issue.code || 'docker-lint'\\n+      }));\\n+\\n+  docker-scan:\\n+    name: docker-scan\\n+    description: Scan Docker image for vulnerabilities\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        image:\\n+          type: string\\n+          description: Docker image to scan\\n+      required: [image]\\n+    exec: 'trivy image --format json --quiet {{ args.image }} || echo \\\"{}\\\"'\\n+    parseJson: true\\n+    timeout: 60000\\n+    transform_js: |\\n+      const vulnerabilities = [];\\n+      if (output.Results) {\\n+        for (const result of output.Results) {\\n+          if (result.Vulnerabilities) {\\n+            for (const vuln of result.Vulnerabilities) {\\n+              vulnerabilities.push({\\n+                file: result.Target || 'docker-image',\\n+                line: 0,\\n+                message: `${vuln.VulnerabilityID}: ${vuln.Title || vuln.Description}`,\\n+                severity: vuln.Severity?.toLowerCase() || 'info',\\n+                category: 'security',\\n+                ruleId: vuln.VulnerabilityID\\n+              });\\n+            }\\n+          }\\n+        }\\n+      }\\n+      return vulnerabilities;\\n+\\n+  # Package management tools\\n+  npm-audit:\\n+    name: npm-audit\\n+    description: Run npm security audit\\n+    exec: 'npm audit --json || echo \\\"{}\\\"'\\n+    parseJson: true\\n+    transform_js: |\\n+      const issues = [];\\n+      if (output.vulnerabilities) {\\n+        for (const [pkg, vuln] of Object.entries(output.vulnerabilities)) {\\n+          issues.push({\\n+            file: 'package.json',\\n+            line: 0,\\n+            message: `${pkg}: ${vuln.severity} severity vulnerability`,\\n+            severity: vuln.severity === 'high' || vuln.severity === 'critical' ? 'error' : 'warning',\\n+            category: 'security',\\n+            ruleId: `npm-${vuln.severity}`\\n+          });\\n+        }\\n+      }\\n+      return issues;\\n+\\n+  check-outdated:\\n+    name: check-outdated\\n+    description: Check for outdated dependencies\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        manager:\\n+          type: string\\n+          enum: [npm, pip, go]\\n+          description: Package manager to use\\n+    exec: |\\n+      {% if args.manager == \\\"npm\\\" %}\\n+        npm outdated --json || echo \\\"{}\\\"\\n+      {% elsif args.manager == \\\"pip\\\" %}\\n+        pip list --outdated --format json || echo \\\"[]\\\"\\n+      {% elsif args.manager == \\\"go\\\" %}\\n+        go list -u -m -json all || echo \\\"{}\\\"\\n+      {% else %}\\n+        echo \\\"{}\\\"\\n+      {% endif %}\\n+    parseJson: true\\n+    transform_js: |\\n+      const outdated = [];\\n+      if (args.manager === 'npm' && typeof output === 'object') {\\n+        for (const [pkg, info] of Object.entries(output)) {\\n+          if (info.wanted !== info.current) {\\n+            outdated.push({\\n+              package: pkg,\\n+              current: info.current,\\n+              wanted: info.wanted,\\n+              latest: info.latest\\n+            });\\n+          }\\n+        }\\n+      }\\n+      // Add handlers for pip and go...\\n+      return outdated;\\n+\\n+  # Testing tools\\n+  run-tests:\\n+    name: run-tests\\n+    description: Run test suite and parse results\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        command:\\n+          type: string\\n+          description: Test command to run\\n+        format:\\n+          type: string\\n+          enum: [jest, pytest, go]\\n+          description: Test output format\\n+      required: [command]\\n+    exec: '{{ args.command }} 2>&1'\\n+    timeout: 300000\\n+    transform_js: |\\n+      // Parse test output based on format\\n+      const lines = output.split('\\\\n');\\n+      let passed = 0, failed = 0, skipped = 0;\\n+\\n+      if (args.format === 'jest') {\\n+        const summary = lines.find(l => l.includes('Tests:'));\\n+        if (summary) {\\n+          const match = summary.match(/(\\\\d+) passed/);\\n+          if (match) passed = parseInt(match[1]);\\n+          const failMatch = summary.match(/(\\\\d+) failed/);\\n+          if (failMatch) failed = parseInt(failMatch[1]);\\n+        }\\n+      }\\n+\\n+      return {\\n+        passed: passed,\\n+        failed: failed,\\n+        skipped: skipped,\\n+        success: failed === 0\\n+      };\\n+\\n+  # Code quality tools\\n+  eslint-check:\\n+    name: eslint-check\\n+    description: Run ESLint on JavaScript/TypeScript files\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        files:\\n+          type: array\\n+          items:\\n+            type: string\\n+          description: Files to lint\\n+    exec: 'npx eslint --format json {{ args.files | join: \\\" \\\" }} || echo \\\"[]\\\"'\\n+    parseJson: true\\n+    transform_js: |\\n+      const issues = [];\\n+      for (const file of output) {\\n+        for (const message of file.messages || []) {\\n+          issues.push({\\n+            file: file.filePath,\\n+            line: message.line || 0,\\n+            endLine: message.endLine,\\n+            column: message.column,\\n+            endColumn: message.endColumn,\\n+            message: message.message,\\n+            severity: message.severity === 2 ? 'error' : 'warning',\\n+            category: 'style',\\n+            ruleId: message.ruleId || 'eslint'\\n+          });\\n+        }\\n+      }\\n+      return issues;\\n+\\n+  prettier-check:\\n+    name: prettier-check\\n+    description: Check code formatting with Prettier\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        files:\\n+          type: array\\n+          items:\\n+            type: string\\n+          description: Files to check\\n+    exec: 'npx prettier --check {{ args.files | join: \\\" \\\" }} 2>&1'\\n+    transform_js: |\\n+      const unformatted = [];\\n+      const lines = output.split('\\\\n');\\n+      for (const line of lines) {\\n+        if (line.includes('[warn]') && line.includes('Code style issues found')) {\\n+          const match = line.match(/in (.+?)$/);\\n+          if (match) {\\n+            unformatted.push({\\n+              file: match[1],\\n+              line: 0,\\n+              message: 'File needs formatting',\\n+              severity: 'warning',\\n+              category: 'style',\\n+              ruleId: 'prettier'\\n+            });\\n+          }\\n+        }\\n+      }\\n+      return unformatted;\\n\\\\ No newline at end of file\\n\",\"status\":\"added\"},{\"filename\":\"examples/workflows/calculator-workflow.yaml\",\"additions\":5,\"deletions\":0,\"changes\":163,\"patch\":\"diff --git a/examples/workflows/calculator-workflow.yaml b/examples/workflows/calculator-workflow.yaml\\nnew file mode 100644\\nindex 00000000..05aab90e\\n--- /dev/null\\n+++ b/examples/workflows/calculator-workflow.yaml\\n@@ -0,0 +1,163 @@\\n+# Standalone executable workflow with inputs, outputs, and self-tests\\n+# Can be run directly: visor --config defaults/calculator-workflow.yaml\\n+# Or imported and used as a step in another config\\n+\\n+id: calculator\\n+name: Simple Calculator\\n+description: A simple calculator workflow that performs arithmetic operations\\n+version: \\\"1.0.0\\\"\\n+\\n+inputs:\\n+  - name: operation\\n+    description: The operation to perform (add, subtract, multiply, divide)\\n+    schema:\\n+      type: string\\n+      enum: [add, subtract, multiply, divide]\\n+    default: \\\"add\\\"\\n+\\n+  - name: a\\n+    description: First operand\\n+    schema:\\n+      type: number\\n+    default: 10\\n+\\n+  - name: b\\n+    description: Second operand\\n+    schema:\\n+      type: number\\n+    default: 5\\n+\\n+outputs:\\n+  - name: result\\n+    description: The calculated result\\n+    value_js: |\\n+      const calc = steps.calculate.output;\\n+      return calc ? calc.result : null;\\n+\\n+  - name: operation_performed\\n+    description: The operation that was performed\\n+    value_js: |\\n+      return inputs.operation;\\n+\\n+  - name: summary\\n+    description: Human readable summary\\n+    value_js: |\\n+      const calc = steps.calculate.output;\\n+      if (!calc) return \\\"Calculation failed\\\";\\n+      return `${inputs.a} ${inputs.operation} ${inputs.b} = ${calc.result}`;\\n+\\n+# Workflow steps - these define the workflow logic\\n+steps:\\n+  validate_inputs:\\n+    type: command\\n+    exec: |\\n+      echo '{\\\"valid\\\": true, \\\"a\\\": {{ inputs.a }}, \\\"b\\\": {{ inputs.b }}, \\\"operation\\\": \\\"{{ inputs.operation }}\\\"}'\\n+\\n+  calculate:\\n+    type: command\\n+    depends_on: [validate_inputs]\\n+    exec: |\\n+      #!/bin/bash\\n+      operation=\\\"{{ inputs.operation }}\\\"\\n+      a={{ inputs.a }}\\n+      b={{ inputs.b }}\\n+\\n+      case $operation in\\n+        add)\\n+          result=$(echo \\\"$a + $b\\\" | bc -l)\\n+          ;;\\n+        subtract)\\n+          result=$(echo \\\"$a - $b\\\" | bc -l)\\n+          ;;\\n+        multiply)\\n+          result=$(echo \\\"$a * $b\\\" | bc -l)\\n+          ;;\\n+        divide)\\n+          if [ \\\"$b\\\" = \\\"0\\\" ]; then\\n+            echo '{\\\"error\\\": \\\"Division by zero\\\"}'\\n+            exit 1\\n+          fi\\n+          result=$(echo \\\"scale=2; $a / $b\\\" | bc -l)\\n+          ;;\\n+        *)\\n+          echo '{\\\"error\\\": \\\"Unknown operation\\\"}'\\n+          exit 1\\n+          ;;\\n+      esac\\n+\\n+      echo \\\"{\\\\\\\"result\\\\\\\": $result, \\\\\\\"operation\\\\\\\": \\\\\\\"$operation\\\\\\\"}\\\"\\n+\\n+  log_result:\\n+    type: log\\n+    depends_on: [calculate]\\n+    message: |\\n+      Calculator Result:\\n+      Operation: {{ inputs.operation }}\\n+      {{ inputs.a }} {{ inputs.operation }} {{ inputs.b }} = {{ steps.calculate.output.result }}\\n+\\n+# Tests - these are ONLY executed when running this file standalone\\n+# They are NOT imported when this workflow is used as a step in another config\\n+tests:\\n+  # Test 1: Default inputs (10 + 5 = 15)\\n+  test_default_addition:\\n+    type: workflow\\n+    workflow: calculator\\n+    # Uses default inputs (operation=add, a=10, b=5)\\n+\\n+  # Test 2: Subtraction\\n+  test_subtraction:\\n+    type: workflow\\n+    workflow: calculator\\n+    args:\\n+      operation: subtract\\n+      a: 20\\n+      b: 8\\n+    # Expected: 20 - 8 = 12\\n+\\n+  # Test 3: Multiplication\\n+  test_multiplication:\\n+    type: workflow\\n+    workflow: calculator\\n+    args:\\n+      operation: multiply\\n+      a: 6\\n+      b: 7\\n+    # Expected: 6 * 7 = 42\\n+\\n+  # Test 4: Division\\n+  test_division:\\n+    type: workflow\\n+    workflow: calculator\\n+    args:\\n+      operation: divide\\n+      a: 100\\n+      b: 4\\n+    # Expected: 100 / 4 = 25\\n+\\n+  # Validate all tests passed\\n+  validate_tests:\\n+    type: command\\n+    depends_on: [test_default_addition, test_subtraction, test_multiplication, test_division]\\n+    exec: |\\n+      #!/bin/bash\\n+      # Get results from each test\\n+      addition=\\\"{{ outputs.test_default_addition.result }}\\\"\\n+      subtraction=\\\"{{ outputs.test_subtraction.result }}\\\"\\n+      multiplication=\\\"{{ outputs.test_multiplication.result }}\\\"\\n+      division=\\\"{{ outputs.test_division.result }}\\\"\\n+\\n+      echo \\\"Test Results:\\\"\\n+      echo \\\"  Addition (10 + 5): $addition (expected 15)\\\"\\n+      echo \\\"  Subtraction (20 - 8): $subtraction (expected 12)\\\"\\n+      echo \\\"  Multiplication (6 * 7): $multiplication (expected 42)\\\"\\n+      echo \\\"  Division (100 / 4): $division (expected 25)\\\"\\n+\\n+      # Simple validation\\n+      if [ \\\"$addition\\\" = \\\"15\\\" ] && [ \\\"$subtraction\\\" = \\\"12\\\" ] && [ \\\"$multiplication\\\" = \\\"42\\\" ] && [ \\\"$division\\\" = \\\"25\\\" ]; then\\n+        echo '{\\\"all_tests_passed\\\": true}'\\n+        exit 0\\n+      else\\n+        echo '{\\\"all_tests_passed\\\": false, \\\"error\\\": \\\"Some tests failed\\\"}'\\n+        exit 1\\n+      fi\\n+    fail_if: output.all_tests_passed === false\\n\",\"status\":\"added\"},{\"filename\":\"examples/workflows/code-quality.yaml\",\"additions\":7,\"deletions\":0,\"changes\":222,\"patch\":\"diff --git a/examples/workflows/code-quality.yaml b/examples/workflows/code-quality.yaml\\nnew file mode 100644\\nindex 00000000..2202f252\\n--- /dev/null\\n+++ b/examples/workflows/code-quality.yaml\\n@@ -0,0 +1,222 @@\\n+# Example reusable workflow for code quality checks\\n+id: code-quality\\n+name: Code Quality Workflow\\n+description: Comprehensive code quality checks including linting, complexity, and best practices\\n+version: \\\"1.0.0\\\"\\n+tags: [\\\"quality\\\", \\\"reusable\\\", \\\"linting\\\"]\\n+category: quality\\n+\\n+# Input parameters\\n+inputs:\\n+  - name: language\\n+    description: Programming language to analyze\\n+    schema:\\n+      type: string\\n+      enum: [\\\"javascript\\\", \\\"typescript\\\", \\\"python\\\", \\\"go\\\", \\\"java\\\"]\\n+    required: true\\n+\\n+  - name: complexity_threshold\\n+    description: Maximum allowed cyclomatic complexity\\n+    schema:\\n+      type: number\\n+      minimum: 1\\n+      maximum: 20\\n+    default: 10\\n+\\n+  - name: enable_formatting\\n+    description: Check code formatting\\n+    schema:\\n+      type: boolean\\n+    default: true\\n+\\n+  - name: custom_rules\\n+    description: Custom linting rules to apply\\n+    schema:\\n+      type: object\\n+      additionalProperties: true\\n+    required: false\\n+\\n+# Output parameters\\n+outputs:\\n+  - name: quality_score\\n+    description: Overall code quality score (0-100)\\n+    value_js: |\\n+      const lintScore = steps.linting?.output?.score || 0;\\n+      const complexityScore = steps.complexity?.output?.score || 0;\\n+      const formattingScore = steps.formatting?.output?.score || 100;\\n+      const weights = { lint: 0.4, complexity: 0.4, formatting: 0.2 };\\n+      return Math.round(\\n+        lintScore * weights.lint +\\n+        complexityScore * weights.complexity +\\n+        formattingScore * weights.formatting\\n+      );\\n+\\n+  - name: recommendations\\n+    description: List of recommendations for improvement\\n+    value_js: |\\n+      const recs = [];\\n+      if (steps.linting?.output?.issues?.length > 0) {\\n+        recs.push(`Fix ${steps.linting.output.issues.length} linting issues`);\\n+      }\\n+      if (steps.complexity?.output?.high_complexity_functions?.length > 0) {\\n+        recs.push(`Refactor ${steps.complexity.output.high_complexity_functions.length} complex functions`);\\n+      }\\n+      if (steps.formatting?.output?.issues?.length > 0) {\\n+        recs.push(`Fix ${steps.formatting.output.issues.length} formatting issues`);\\n+      }\\n+      return recs;\\n+\\n+  - name: detailed_report\\n+    description: Detailed quality report\\n+    value: |\\n+      Code Quality Report for {{ inputs.language }}:\\n+      ================================================\\n+      Overall Score: {{ outputs.quality_score }}/100\\n+\\n+      Linting Results:\\n+      - Issues found: {{ steps.linting.output.issues | size }}\\n+      - Score: {{ steps.linting.output.score }}/100\\n+\\n+      Complexity Analysis:\\n+      - High complexity functions: {{ steps.complexity.output.high_complexity_functions | size }}\\n+      - Average complexity: {{ steps.complexity.output.average_complexity }}\\n+\\n+      {% if inputs.enable_formatting %}\\n+      Formatting Check:\\n+      - Issues found: {{ steps.formatting.output.issues | size }}\\n+      - Score: {{ steps.formatting.output.score }}/100\\n+      {% endif %}\\n+\\n+      Recommendations:\\n+      {% for rec in outputs.recommendations %}\\n+      - {{ rec }}\\n+      {% endfor %}\\n+\\n+# Workflow steps\\n+steps:\\n+  linting:\\n+    type: ai\\n+    prompt: |\\n+      Perform linting analysis for {{ inputs.language }} code.\\n+      Check for:\\n+      - Syntax errors\\n+      - Code style violations\\n+      - Best practices violations\\n+      - Unused variables and imports\\n+      - Type errors (if applicable)\\n+\\n+      {% if inputs.custom_rules %}\\n+      Additional custom rules to check:\\n+      {{ inputs.custom_rules | json }}\\n+      {% endif %}\\n+\\n+      Provide a score from 0-100 based on the issues found.\\n+    focus: style\\n+    schema: code-review\\n+    inputs:\\n+      language:\\n+        source: param\\n+        value: language\\n+      custom_rules:\\n+        source: param\\n+        value: custom_rules\\n+\\n+  complexity:\\n+    type: ai\\n+    prompt: |\\n+      Analyze code complexity for {{ inputs.language }}.\\n+\\n+      Check for:\\n+      - Cyclomatic complexity (threshold: {{ inputs.complexity_threshold }})\\n+      - Cognitive complexity\\n+      - Nested conditionals\\n+      - Long functions/methods\\n+      - Too many parameters\\n+\\n+      Report:\\n+      - List of functions exceeding complexity threshold\\n+      - Average complexity across the codebase\\n+      - Provide a score from 0-100\\n+    focus: architecture\\n+    schema: code-review\\n+    depends_on: [linting]\\n+    inputs:\\n+      language:\\n+        source: param\\n+        value: language\\n+      complexity_threshold:\\n+        source: param\\n+        value: complexity_threshold\\n+\\n+  formatting:\\n+    type: command\\n+    exec: |\\n+      case \\\"{{ inputs.language }}\\\" in\\n+        javascript|typescript)\\n+          npx prettier --check \\\"**/*.{js,ts,jsx,tsx}\\\" --list-different || true\\n+          ;;\\n+        python)\\n+          python -m black --check . --diff || true\\n+          ;;\\n+        go)\\n+          gofmt -l . || true\\n+          ;;\\n+        java)\\n+          echo \\\"Java formatting check not implemented\\\"\\n+          ;;\\n+      esac\\n+    if: inputs.enable_formatting === true\\n+    depends_on: [complexity]\\n+    inputs:\\n+      language:\\n+        source: param\\n+        value: language\\n+      enable_formatting:\\n+        source: param\\n+        value: enable_formatting\\n+\\n+  best_practices:\\n+    type: ai\\n+    prompt: |\\n+      Review {{ inputs.language }} code for best practices:\\n+\\n+      - Design patterns usage\\n+      - SOLID principles adherence\\n+      - Error handling patterns\\n+      - Documentation quality\\n+      - Test coverage indicators\\n+      - Performance considerations\\n+\\n+      Provide actionable recommendations.\\n+    focus: architecture\\n+    depends_on: [formatting]\\n+    inputs:\\n+      language:\\n+        source: param\\n+        value: language\\n+\\n+  aggregate_results:\\n+    type: noop\\n+    depends_on: [best_practices]\\n+    # This step exists to ensure all previous steps complete\\n+    # The actual aggregation happens in the output parameters\\n+\\n+# Usage examples (documentation only):\\n+#\\n+# 1. TypeScript with strict settings:\\n+#    language: typescript\\n+#    complexity_threshold: 8\\n+#    enable_formatting: true\\n+#    custom_rules:\\n+#      no-any: error\\n+#      explicit-function-return-type: warning\\n+#\\n+# 2. Python basic check:\\n+#    language: python\\n+#    complexity_threshold: 15\\n+#    enable_formatting: false\\n+#\\n+# 3. Go with formatting:\\n+#    language: go\\n+#    complexity_threshold: 10\\n+#    enable_formatting: true\\n\\\\ No newline at end of file\\n\",\"status\":\"added\"},{\"filename\":\"examples/workflows/quick-pr-check.yaml\",\"additions\":3,\"deletions\":0,\"changes\":90,\"patch\":\"diff --git a/examples/workflows/quick-pr-check.yaml b/examples/workflows/quick-pr-check.yaml\\nnew file mode 100644\\nindex 00000000..3248f14f\\n--- /dev/null\\n+++ b/examples/workflows/quick-pr-check.yaml\\n@@ -0,0 +1,90 @@\\n+# Quick PR validation workflow\\n+# Performs basic checks for PR validation\\n+\\n+id: quick-pr-check\\n+name: Quick PR Check\\n+description: Fast workflow for initial PR validation\\n+version: \\\"1.0.0\\\"\\n+category: validation\\n+\\n+# Input parameters\\n+inputs:\\n+  - name: pr_type\\n+    description: Type of PR (feature, bugfix, hotfix)\\n+    schema:\\n+      type: string\\n+      enum: [\\\"feature\\\", \\\"bugfix\\\", \\\"hotfix\\\"]\\n+    required: true\\n+\\n+  - name: run_tests\\n+    description: Whether to run tests\\n+    schema:\\n+      type: boolean\\n+    default: true\\n+\\n+  - name: run_lint\\n+    description: Whether to run linting\\n+    schema:\\n+      type: boolean\\n+    default: true\\n+\\n+# Output parameters\\n+outputs:\\n+  - name: approval_status\\n+    description: Whether the PR passes quick checks\\n+    value_js: |\\n+      const testsPassed = !inputs.run_tests || steps.run_tests?.output?.success || false;\\n+      const lintPassed = !inputs.run_lint || steps.lint_check?.output?.passed || false;\\n+      return testsPassed && lintPassed ? \\\"approved\\\" : \\\"needs_work\\\";\\n+\\n+  - name: summary\\n+    description: Summary of the quick check\\n+    value: |\\n+      Quick Check Results for {{ inputs.pr_type }} PR:\\n+      {% if inputs.run_tests %}\\n+      - Tests: {{ steps.run_tests.output.success ? 'Passed' : 'Failed' }}\\n+      {% endif %}\\n+      {% if inputs.run_lint %}\\n+      - Linting: {{ steps.lint_check.output.passed ? 'Passed' : 'Failed' }}\\n+      {% endif %}\\n+      Overall Status: {{ outputs.approval_status }}\\n+\\n+# Workflow steps at root level\\n+steps:\\n+  run_tests:\\n+    type: command\\n+    exec: npm test\\n+    timeout: 300\\n+    if: inputs.run_tests === true\\n+\\n+  lint_check:\\n+    type: command\\n+    exec: npm run lint\\n+    depends_on: [run_tests]\\n+    if: inputs.run_lint === true\\n+\\n+  pr_summary:\\n+    type: log\\n+    message: |\\n+      Quick Check Results for {{ inputs.pr_type }} PR:\\n+      {% if inputs.run_tests %}\\n+      - Tests: {{ steps.run_tests.output.success ? 'Passed' : 'Failed' }}\\n+      {% endif %}\\n+      {% if inputs.run_lint %}\\n+      - Linting: {{ steps.lint_check.output.passed ? 'Passed' : 'Failed' }}\\n+      {% endif %}\\n+      Status: {{ outputs.approval_status }}\\n+    depends_on: [lint_check]\\n+    level: info\\n+\\n+# Usage examples (documentation only):\\n+#\\n+# 1. Full validation:\\n+#    pr_type: feature\\n+#    run_tests: true\\n+#    run_lint: true\\n+#\\n+# 2. Quick lint only:\\n+#    pr_type: hotfix\\n+#    run_tests: false\\n+#    run_lint: true\\n\\\\ No newline at end of file\\n\",\"status\":\"added\"},{\"filename\":\"examples/workflows/workflow-composition-example.yaml\",\"additions\":4,\"deletions\":0,\"changes\":130,\"patch\":\"diff --git a/examples/workflows/workflow-composition-example.yaml b/examples/workflows/workflow-composition-example.yaml\\nnew file mode 100644\\nindex 00000000..6e73b125\\n--- /dev/null\\n+++ b/examples/workflows/workflow-composition-example.yaml\\n@@ -0,0 +1,130 @@\\n+# Example demonstrating workflow composition and state isolation\\n+# This config imports the calculator workflow and uses it as a step\\n+# Demonstrates that:\\n+# 1. Workflow tests are NOT auto-executed when imported\\n+# 2. Workflow steps run in isolated context\\n+# 3. State does not leak between workflow and parent config\\n+\\n+version: \\\"1.0\\\"\\n+\\n+# Import the calculator workflow\\n+imports:\\n+  - ./calculator-workflow.yaml\\n+\\n+# Main checks/steps in this config\\n+checks:\\n+  # Step 1: Set some state in parent context\\n+  parent_state_setup:\\n+    type: memory\\n+    operation: set\\n+    key: parent_value\\n+    value: \\\"I am from parent context\\\"\\n+\\n+  # Step 2: Use the calculator workflow as an isolated step\\n+  use_calculator:\\n+    type: workflow\\n+    workflow: calculator\\n+    depends_on: [parent_state_setup]\\n+    args:\\n+      operation: multiply\\n+      a: 7\\n+      b: 6\\n+\\n+  # Step 3: Verify calculator result is accessible\\n+  verify_calculator_result:\\n+    type: command\\n+    depends_on: [use_calculator]\\n+    exec: |\\n+      #!/bin/bash\\n+      result=\\\"{{ outputs.use_calculator.result }}\\\"\\n+      summary=\\\"{{ outputs.use_calculator.summary }}\\\"\\n+\\n+      echo \\\"Calculator returned: $result\\\"\\n+      echo \\\"Summary: $summary\\\"\\n+\\n+      if [ \\\"$result\\\" = \\\"42\\\" ]; then\\n+        echo '{\\\"validation\\\": \\\"passed\\\", \\\"result\\\": '$result'}'\\n+        exit 0\\n+      else\\n+        echo '{\\\"validation\\\": \\\"failed\\\", \\\"expected\\\": 42, \\\"got\\\": '$result'}'\\n+        exit 1\\n+      fi\\n+\\n+  # Step 4: Verify parent state is still intact (not affected by workflow)\\n+  verify_parent_state:\\n+    type: memory\\n+    operation: get\\n+    key: parent_value\\n+    depends_on: [use_calculator]\\n+\\n+  # Step 5: Test state isolation - workflow should NOT have access to parent memory\\n+  test_state_isolation:\\n+    type: command\\n+    depends_on: [verify_parent_state]\\n+    exec: |\\n+      #!/bin/bash\\n+      parent_value=\\\"{{ outputs.verify_parent_state }}\\\"\\n+\\n+      echo \\\"Parent state verification:\\\"\\n+      echo \\\"  parent_value = $parent_value\\\"\\n+\\n+      if [ \\\"$parent_value\\\" = \\\"I am from parent context\\\" ]; then\\n+        echo '{\\\"isolation_test\\\": \\\"passed\\\", \\\"message\\\": \\\"Parent state intact\\\"}'\\n+        exit 0\\n+      else\\n+        echo '{\\\"isolation_test\\\": \\\"failed\\\", \\\"message\\\": \\\"Parent state was modified\\\"}'\\n+        exit 1\\n+      fi\\n+\\n+  # Step 6: Use calculator again with different inputs (reusability test)\\n+  use_calculator_again:\\n+    type: workflow\\n+    workflow: calculator\\n+    depends_on: [test_state_isolation]\\n+    args:\\n+      operation: add\\n+      a: 100\\n+      b: 23\\n+\\n+  # Step 7: Verify both calculator invocations produced correct results\\n+  verify_multiple_invocations:\\n+    type: command\\n+    depends_on: [use_calculator_again]\\n+    exec: |\\n+      #!/bin/bash\\n+      first_result=\\\"{{ outputs.use_calculator.result }}\\\"\\n+      second_result=\\\"{{ outputs.use_calculator_again.result }}\\\"\\n+\\n+      echo \\\"Multiple invocation test:\\\"\\n+      echo \\\"  First call (7 * 6): $first_result (expected 42)\\\"\\n+      echo \\\"  Second call (100 + 23): $second_result (expected 123)\\\"\\n+\\n+      if [ \\\"$first_result\\\" = \\\"42\\\" ] && [ \\\"$second_result\\\" = \\\"123\\\" ]; then\\n+        echo '{\\\"multiple_invocations\\\": \\\"passed\\\"}'\\n+        exit 0\\n+      else\\n+        echo '{\\\"multiple_invocations\\\": \\\"failed\\\"}'\\n+        exit 1\\n+      fi\\n+\\n+  # Final summary\\n+  summary:\\n+    type: log\\n+    depends_on: [verify_multiple_invocations]\\n+    message: |\\n+      ========================================\\n+      Workflow Composition Test Summary\\n+      ========================================\\n+\\n+      ‚úì Calculator workflow imported successfully\\n+      ‚úì Workflow tests were NOT auto-executed\\n+      ‚úì Calculator step executed in isolation\\n+      ‚úì Results: {{ outputs.use_calculator.summary }}\\n+      ‚úì Parent state remained intact\\n+      ‚úì Multiple workflow invocations work correctly\\n+\\n+      First calculation: {{ outputs.use_calculator.result }}\\n+      Second calculation: {{ outputs.use_calculator_again.result }}\\n+\\n+      All state isolation tests PASSED!\\n+      ========================================\\n\",\"status\":\"added\"},{\"filename\":\"override.yaml\",\"additions\":2,\"deletions\":0,\"changes\":52,\"patch\":\"diff --git a/override.yaml b/override.yaml\\nnew file mode 100644\\nindex 00000000..270e82de\\n--- /dev/null\\n+++ b/override.yaml\\n@@ -0,0 +1,52 @@\\n+version: \\\"1.0\\\"\\n+\\n+# Bring in the full default Visor workflow (which itself includes code-review.yaml)\\n+include:\\n+  - ./defaults/visor.yaml\\n+\\n+# Override exactly one of the imported code-review steps using appendPrompt\\n+steps:\\n+  security:\\n+    appendPrompt: |\\n+      Additionally, search for any hard-coded credentials (API keys, tokens,\\n+      passwords) or secrets in diffs and configuration files. If found, mark\\n+      them as critical and recommend using a secret manager or environment\\n+      variables instead.\\n+  # Add a new lightweight code-review step to demonstrate extending the suite\\n+  readability:\\n+    type: ai\\n+    group: review\\n+    on: [pr_opened, pr_updated]\\n+    depends_on: [overview]\\n+    prompt: |\\n+      Perform a lightweight readability review of the proposed changes.\\n+      Focus on:\\n+      - Clear, intention-revealing naming\\n+      - Helpful comments (avoid redundant ones)\\n+      - Function length and cohesion\\n+      - Early returns to reduce nesting\\n+      - Eliminate dead code and commented-out blocks\\n+    schema: code-review\\n+\\n+tests:\\n+  defaults:\\n+    strict: false\\n+    ai_provider: mock\\n+  cases:\\n+    - name: override-appendPrompt-security\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview body\\\"\\n+          tags: { label: feature, review-effort: 2 }\\n+        security: { issues: [] }\\n+      expect:\\n+        calls:\\n+          - step: security\\n+            exactly: 1\\n+        prompts:\\n+          - step: security\\n+            contains:\\n+              - \\\"hard-coded credentials\\\"\\n+              - \\\"secret manager\\\"\\n\",\"status\":\"added\"},{\"filename\":\"package-lock.json\",\"additions\":1,\"deletions\":1,\"changes\":8,\"patch\":\"diff --git a/package-lock.json b/package-lock.json\\nindex bd02b99e..67e7b4ca 100644\\n--- a/package-lock.json\\n+++ b/package-lock.json\\n@@ -16,7 +16,7 @@\\n         \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n         \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n         \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc161\\\",\\n+        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc164\\\",\\n         \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n         \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -5946,9 +5946,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/probe\\\": {\\n-      \\\"version\\\": \\\"0.6.0-rc161\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc161.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-zXHNoLSfWTg+3kH67rAjzgGMD2W7azdBhJW+4JH7wKkpyaGa3cEY+u5ngxPTcWqhR15HHhYEwoY6gNiI/x3tiQ==\\\",\\n+      \\\"version\\\": \\\"0.6.0-rc164\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc164.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-/SBrlYCx6sliKgKV3ywUzFgVH9fWc2O94sm3StNpDFU3FK9cXvuRWaS9t9oO2cGv85/1pCBKW1ptlCTDWZu15w==\\\",\\n       \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n\",\"status\":\"modified\"},{\"filename\":\"package.json\",\"additions\":1,\"deletions\":1,\"changes\":7,\"patch\":\"diff --git a/package.json b/package.json\\nindex f1716c52..18db3e46 100644\\n--- a/package.json\\n+++ b/package.json\\n@@ -31,20 +31,21 @@\\n     \\\"registry\\\": \\\"https://registry.npmjs.org/\\\"\\n   },\\n   \\\"scripts\\\": {\\n-    \\\"build:cli\\\": \\\"ncc build src/index.ts -o dist && cp -r defaults dist/ && cp -r output dist/ && cp -r src/debug-visualizer/ui dist/debug-visualizer/ && node scripts/inject-version.js && echo '#!/usr/bin/env node' | cat - dist/index.js > temp && mv temp dist/index.js && chmod +x dist/index.js\\\",\\n+    \\\"build:cli\\\": \\\"ncc build src/index.ts -o dist && cp -r defaults dist/ && cp -r output dist/ && cp -r docs dist/ && cp -r examples dist/ && cp -r src/debug-visualizer/ui dist/debug-visualizer/ && node scripts/inject-version.js && echo '#!/usr/bin/env node' | cat - dist/index.js > temp && mv temp dist/index.js && chmod +x dist/index.js\\\",\\n     \\\"build:sdk\\\": \\\"tsup src/sdk.ts --dts --sourcemap --format esm,cjs --out-dir dist/sdk\\\",\\n     \\\"build\\\": \\\"npm run build:cli && npm run build:sdk\\\",\\n     \\\"test\\\": \\\"jest && npm run test:yaml\\\",\\n     \\\"prepublishOnly\\\": \\\"npm run build\\\",\\n     \\\"test:watch\\\": \\\"jest --watch\\\",\\n     \\\"test:coverage\\\": \\\"jest --coverage\\\",\\n+    \\\"test:manual:bash\\\": \\\"RUN_MANUAL_TESTS=true jest tests/manual/bash-config-manual.test.ts\\\",\\n     \\\"lint\\\": \\\"eslint src tests --ext .ts\\\",\\n     \\\"lint:fix\\\": \\\"eslint src tests --ext .ts --fix\\\",\\n     \\\"format\\\": \\\"prettier --write src tests\\\",\\n     \\\"format:check\\\": \\\"prettier --check src tests\\\",\\n     \\\"clean\\\": \\\"\\\",\\n     \\\"prebuild\\\": \\\"npm run clean && node scripts/generate-config-schema.js\\\",\\n-    \\\"pretest\\\": \\\"node scripts/generate-config-schema.js\\\",\\n+    \\\"pretest\\\": \\\"node scripts/generate-config-schema.js && npm run build:cli\\\",\\n     \\\"test:with-build\\\": \\\"npm run build:cli && jest\\\",\\n     \\\"test:yaml\\\": \\\"node scripts/run-visor-tests.js\\\",\\n     \\\"prepare\\\": \\\"husky\\\",\\n@@ -95,7 +96,7 @@\\n     \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n     \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n     \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc161\\\",\\n+    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc164\\\",\\n     \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n     \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n     \\\"ajv\\\": \\\"^8.17.1\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"scripts/run-visor-tests.js\",\"additions\":1,\"deletions\":1,\"changes\":66,\"patch\":\"diff --git a/scripts/run-visor-tests.js b/scripts/run-visor-tests.js\\nindex c6724abf..e4e27d93 100755\\n--- a/scripts/run-visor-tests.js\\n+++ b/scripts/run-visor-tests.js\\n@@ -14,22 +14,26 @@ function main() {\\n   const repoRoot = path.resolve(__dirname, '..');\\n   const distCli = path.join(repoRoot, 'dist', 'index.js');\\n   const srcCli = path.join(repoRoot, 'src', 'index.ts');\\n-  // Prefer new non-dot tests filename; test runner still discovers legacy name\\n-  const testsPath = process.env.VISOR_TESTS_PATH || path.join(repoRoot, 'defaults', 'visor.tests.yaml');\\n+  // Prefer new non-dot tests filename; allow multiple suites\\n+  const primarySuite = process.env.VISOR_TESTS_PATH || path.join(repoRoot, 'defaults', 'visor.tests.yaml');\\n+  const refinementSuite = path.join(repoRoot, 'defaults', 'task-refinement.yaml');\\n+  const refinerSuite = path.join(repoRoot, 'defaults', 'code-refiner.yaml');\\n+  // Local override tests that validate include/extends + appendPrompt behavior\\n+  const overrideSuite = path.join(repoRoot, 'tests', 'override.tests.yaml');\\n   const isCI = process.env.CI === 'true' || process.env.GITHUB_ACTIONS === 'true';\\n \\n   let nodeArgs = [];\\n-  let argv = [];\\n+  const baseArgs = [];\\n   if (!isCI) {\\n     // Prefer TypeScript source in local/dev for correctness\\n     try {\\n       require.resolve('ts-node/register');\\n       nodeArgs = ['-r', 'ts-node/register'];\\n-      argv = [srcCli, 'test', '--config', testsPath, '--progress', 'compact'];\\n+      baseArgs.push(srcCli);\\n     } catch (_) {\\n       // Fallback to dist if ts-node is not installed\\n       if (fs.existsSync(distCli)) {\\n-        argv = [distCli, 'test', '--config', testsPath, '--progress', 'compact'];\\n+        baseArgs.push(distCli);\\n       } else {\\n         console.error('Neither ts-node nor dist/index.js found. Run `npm run build:cli` first.');\\n         process.exit(2);\\n@@ -39,12 +43,12 @@ function main() {\\n     // In CI we always use the freshly built dist\\n     // Fall back to ts-node\\n     if (fs.existsSync(distCli)) {\\n-      argv = [distCli, 'test', '--config', testsPath, '--progress', 'compact'];\\n+      baseArgs.push(distCli);\\n     } else {\\n       try {\\n         require.resolve('ts-node/register');\\n         nodeArgs = ['-r', 'ts-node/register'];\\n-        argv = [srcCli, 'test', '--config', testsPath, '--progress', 'compact'];\\n+        baseArgs.push(srcCli);\\n       } catch (e) {\\n         console.error('Build artifacts missing and ts-node not available.');\\n         process.exit(2);\\n@@ -52,29 +56,41 @@ function main() {\\n     }\\n   }\\n \\n-  if (isCI) {\\n-    const outDir = path.join(repoRoot, 'output');\\n-    try { fs.mkdirSync(outDir, { recursive: true }); } catch {}\\n-    argv.push('--json', path.join(outDir, 'visor-tests.json'));\\n-    argv.push('--report', `junit:${path.join(outDir, 'visor-tests.xml')}`);\\n-    argv.push('--summary', `md:${path.join(outDir, 'visor-tests.md')}`);\\n-  }\\n-\\n   // Ensure VISOR_DEBUG is not noisy in CI\\n   const env = { ...process.env };\\n   if (isCI && env.VISOR_DEBUG === 'true') delete env.VISOR_DEBUG;\\n \\n-  const res = spawnSync(process.execPath, [...nodeArgs, ...argv], {\\n-    stdio: 'inherit',\\n-    env,\\n-    cwd: repoRoot,\\n-  });\\n-  if (typeof res.status === 'number') process.exit(res.status);\\n-  if (res.error) {\\n-    console.error(res.error);\\n-    process.exit(1);\\n+  const suites = [primarySuite];\\n+  if (fs.existsSync(refinementSuite)) suites.push(refinementSuite);\\n+  if (fs.existsSync(refinerSuite)) suites.push(refinerSuite);\\n+  if (fs.existsSync(overrideSuite)) suites.push(overrideSuite);\\n+\\n+  let exitCode = 0;\\n+  const outDir = path.join(repoRoot, 'output');\\n+  if (isCI) { try { fs.mkdirSync(outDir, { recursive: true }); } catch {}\\n+  }\\n+\\n+  for (const suite of suites) {\\n+    const label = path.basename(suite).replace(/\\\\.[^.]+$/, '');\\n+    const args = [...baseArgs, 'test', '--config', suite, '--progress', 'compact'];\\n+    if (isCI) {\\n+      args.push('--json', path.join(outDir, `${label}.json`));\\n+      args.push('--report', `junit:${path.join(outDir, `${label}.xml`)}`);\\n+      args.push('--summary', `md:${path.join(outDir, `${label}.md`)}`);\\n+    }\\n+    const res = spawnSync(process.execPath, [...nodeArgs, ...args], {\\n+      stdio: 'inherit',\\n+      env,\\n+      cwd: repoRoot,\\n+    });\\n+    if (typeof res.status === 'number' && res.status !== 0) exitCode = res.status;\\n+    if (res.error) {\\n+      console.error(res.error);\\n+      exitCode = 1;\\n+    }\\n   }\\n-  process.exit(0);\\n+\\n+  process.exit(exitCode);\\n }\\n \\n main();\\n\",\"status\":\"modified\"},{\"filename\":\"src/ai-review-service.ts\",\"additions\":1,\"deletions\":1,\"changes\":60,\"patch\":\"diff --git a/src/ai-review-service.ts b/src/ai-review-service.ts\\nindex 7dc1c458..7ba846ec 100644\\n--- a/src/ai-review-service.ts\\n+++ b/src/ai-review-service.ts\\n@@ -30,6 +30,7 @@ interface TracedProbeAgentOptions extends ProbeAgentOptions {\\n   tracer?: unknown; // SimpleTelemetry tracer\\n   _telemetryConfig?: unknown; // SimpleTelemetry config\\n   _traceFilePath?: string;\\n+  customPrompt?: string;\\n }\\n \\n export interface AIReviewConfig {\\n@@ -43,12 +44,26 @@ export interface AIReviewConfig {\\n   mcpServers?: Record<string, import('./types/config').McpServerConfig>;\\n   // Enable delegate tool for task distribution to subagents\\n   enableDelegate?: boolean;\\n+  // ProbeAgent persona/prompt family (e.g., 'engineer', 'code-review', 'architect')\\n+  promptType?: string;\\n+  // System prompt to prepend (baseline/preamble). Replaces legacy customPrompt\\n+  systemPrompt?: string;\\n+  // Backward-compat: legacy key still accepted internally\\n+  customPrompt?: string;\\n   // Retry configuration for AI provider calls\\n   retry?: import('./types/config').AIRetryConfig;\\n   // Fallback configuration for provider failures\\n   fallback?: import('./types/config').AIFallbackConfig;\\n   // Enable Edit and Create tools for file modification\\n   allowEdit?: boolean;\\n+  // Filter allowed tools - supports whitelist, exclusion (!prefix), or raw AI mode (empty array)\\n+  allowedTools?: string[];\\n+  // Disable all tools for raw AI mode (alternative to allowedTools: [])\\n+  disableTools?: boolean;\\n+  // Enable bash command execution (shorthand for bashConfig.enabled)\\n+  allowBash?: boolean;\\n+  // Advanced bash command execution configuration\\n+  bashConfig?: import('./types/config').BashConfig;\\n }\\n \\n export interface AIDebugInfo {\\n@@ -123,6 +138,16 @@ export class AIReviewService {\\n \\n     this.sessionRegistry = SessionRegistry.getInstance();\\n \\n+    // If debug was not explicitly provided, honor standard env flags so tests/CLI\\n+    // can enable provider-level debug without modifying per-check configs.\\n+    if (typeof this.config.debug === 'undefined') {\\n+      try {\\n+        if (process.env.VISOR_PROVIDER_DEBUG === 'true' || process.env.VISOR_DEBUG === 'true') {\\n+          this.config.debug = true;\\n+        }\\n+      } catch {}\\n+    }\\n+\\n     // Respect explicit provider if set (e.g., 'mock' during tests) ‚Äî do not override from env\\n     const providerExplicit =\\n       typeof this.config.provider === 'string' && this.config.provider.length > 0;\\n@@ -178,7 +203,10 @@ export class AIReviewService {\\n     const timestamp = new Date().toISOString();\\n \\n     // Build prompt from custom instructions\\n-    const prompt = await this.buildCustomPrompt(prInfo, customPrompt, schema);\\n+    // Respect provider-level skip_code_context by skipping PR context wrapper when requested\\n+    const prompt = await this.buildCustomPrompt(prInfo, customPrompt, schema, {\\n+      skipPRContext: (this.config as any)?.skip_code_context === true,\\n+    });\\n \\n     log(`Executing AI review with ${this.config.provider} provider...`);\\n     log(`üîß Debug: Raw schema parameter: ${JSON.stringify(schema)} (type: ${typeof schema})`);\\n@@ -1361,11 +1389,22 @@ ${'='.repeat(60)}\\n         // No need to set apiKey as it uses AWS SDK authentication\\n         // ProbeAgent will check for AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, etc.\\n       }\\n+      const explicitPromptType = (process.env.VISOR_PROMPT_TYPE || '').trim();\\n       const options: TracedProbeAgentOptions = {\\n         sessionId: sessionId,\\n-        promptType: schema ? ('code-review-template' as 'code-review') : undefined,\\n+        // Prefer config promptType, then env override, else fallback to code-review when schema is set\\n+        promptType:\\n+          this.config.promptType && this.config.promptType.trim()\\n+            ? (this.config.promptType.trim() as any)\\n+            : explicitPromptType\\n+              ? (explicitPromptType as any)\\n+              : schema === 'code-review'\\n+                ? ('code-review-template' as any)\\n+                : undefined,\\n         allowEdit: false, // We don't want the agent to modify files\\n         debug: this.config.debug || false,\\n+        // Map systemPrompt to Probe customPrompt until SDK exposes a first-class field\\n+        customPrompt: this.config.systemPrompt || this.config.customPrompt,\\n       };\\n \\n       // Enable tracing in debug mode for better diagnostics\\n@@ -1407,6 +1446,23 @@ ${'='.repeat(60)}\\n         (options as any).allowEdit = this.config.allowEdit;\\n       }\\n \\n+      // Pass tool filtering options to ProbeAgent\\n+      if (this.config.allowedTools !== undefined) {\\n+        (options as any).allowedTools = this.config.allowedTools;\\n+      }\\n+      if (this.config.disableTools !== undefined) {\\n+        (options as any).disableTools = this.config.disableTools;\\n+      }\\n+\\n+      // Pass bash command execution configuration to ProbeAgent\\n+      // Pass allowBash and bashConfig separately (following allowEdit pattern)\\n+      if (this.config.allowBash !== undefined) {\\n+        (options as any).allowBash = this.config.allowBash;\\n+      }\\n+      if (this.config.bashConfig !== undefined) {\\n+        (options as any).bashConfig = this.config.bashConfig;\\n+      }\\n+\\n       // Add provider-specific options if configured\\n       if (this.config.provider) {\\n         // Map claude-code to anthropic for ProbeAgent compatibility\\n\",\"status\":\"modified\"},{\"filename\":\"src/check-execution-engine.ts\",\"additions\":1,\"deletions\":214,\"changes\":7729,\"patch\":\"diff --git a/src/check-execution-engine.ts b/src/check-execution-engine.ts\\nindex 49927451..5744d5fc 100644\\n--- a/src/check-execution-engine.ts\\n+++ b/src/check-execution-engine.ts\\n@@ -1,111 +1,36 @@\\n-import {\\n-  PRReviewer,\\n-  ReviewSummary,\\n-  ReviewOptions,\\n-  GroupedCheckResults,\\n-  CheckResult,\\n-  ReviewIssue,\\n-} from './reviewer';\\n-import { GitRepositoryAnalyzer, GitRepositoryInfo } from './git-repository-analyzer';\\n-import { AnalysisResult } from './output-formatters';\\n-import { PRInfo } from './pr-analyzer';\\n-import { PRAnalyzer } from './pr-analyzer';\\n-import { CheckProviderRegistry } from './providers/check-provider-registry';\\n-import { CheckProviderConfig } from './providers/check-provider.interface';\\n-import { DependencyResolver, DependencyGraph } from './dependency-resolver';\\n-import { FailureConditionEvaluator } from './failure-condition-evaluator';\\n-import { FailureConditionResult, CheckConfig } from './types/config';\\n-import { GitHubCheckService, CheckRunOptions } from './github-check-service';\\n-import { IssueFilter } from './issue-filter';\\n-import { logger } from './logger';\\n-import Sandbox from '@nyariv/sandboxjs';\\n-import { ExecutionJournal, ScopePath, ContextView } from './snapshot-store';\\n-import { createSecureSandbox, compileAndRun } from './utils/sandbox';\\n-import {\\n-  projectOutputs as ofProject,\\n-  decideRouting as ofDecide,\\n-  computeAllValid as ofAllValid,\\n-  runOnFinishChildren as ofRunChildren,\\n-} from './engine/on-finish/orchestrator';\\n-import { composeOnFinishContext as ofComposeCtx } from './engine/on-finish/utils';\\n-import { VisorConfig, OnFailConfig, OnSuccessConfig, OnFinishConfig } from './types/config';\\n-import {\\n-  createPermissionHelpers,\\n-  detectLocalMode,\\n-  resolveAssociationFromEvent,\\n-} from './utils/author-permissions';\\n-import { MemoryStore } from './memory-store';\\n-import { emitNdjsonSpanWithEvents, emitNdjsonFallback } from './telemetry/fallback-ndjson';\\n-import { addEvent, withActiveSpan } from './telemetry/trace-helpers';\\n-import { addFailIfTriggered } from './telemetry/metrics';\\n-\\n-type ExtendedReviewSummary = ReviewSummary & {\\n-  output?: unknown;\\n-  content?: string;\\n-  isForEach?: boolean;\\n-  forEachItems?: unknown[];\\n-  // Preserve per-item results for forEach-dependent checks so children can gate per item\\n-  forEachItemResults?: ReviewSummary[];\\n-  // Per-item fatal mask: true means this item is fatal/should gate descendants\\n-  forEachFatalMask?: boolean[];\\n-};\\n-\\n /**\\n- * Statistics for a single check execution\\n+ * Legacy compatibility layer\\n+ *\\n+ * This file re-exports StateMachineExecutionEngine as CheckExecutionEngine\\n+ * to maintain backward compatibility with existing test files.\\n+ *\\n+ * @deprecated Use StateMachineExecutionEngine directly from state-machine-execution-engine.ts\\n+ *\\n+ * Migration path:\\n+ * - Old: import { CheckExecutionEngine } from './check-execution-engine'\\n+ * - New: import { StateMachineExecutionEngine } from './state-machine-execution-engine'\\n+ *\\n+ * This compatibility layer will be removed in a future version once all tests\\n+ * have been migrated to use the new state machine engine directly.\\n  */\\n-export interface CheckExecutionStats {\\n-  checkName: string;\\n-  totalRuns: number; // How many times the check executed (1 or forEach iterations)\\n-  successfulRuns: number;\\n-  failedRuns: number;\\n-  skipped: boolean;\\n-  skipReason?: 'if_condition' | 'fail_fast' | 'dependency_failed';\\n-  skipCondition?: string; // The actual if condition text\\n-  totalDuration: number; // Total duration in milliseconds\\n-  // Provider/self time (excludes time spent running routed children/descendants)\\n-  providerDurationMs?: number;\\n-  perIterationDuration?: number[]; // Duration for each iteration (if forEach)\\n-  issuesFound: number;\\n-  issuesBySeverity: {\\n-    critical: number;\\n-    error: number;\\n-    warning: number;\\n-    info: number;\\n-  };\\n-  outputsProduced?: number; // Number of outputs for forEach checks\\n-  errorMessage?: string; // Error message if failed\\n-  forEachPreview?: string[]; // Preview of forEach items processed (first few)\\n-}\\n \\n-/**\\n- * Overall execution statistics for all checks\\n- */\\n-export interface ExecutionStatistics {\\n-  totalChecksConfigured: number;\\n-  totalExecutions: number; // Sum of all runs including forEach iterations\\n-  successfulExecutions: number;\\n-  failedExecutions: number;\\n-  skippedChecks: number;\\n-  totalDuration: number;\\n-  checks: CheckExecutionStats[];\\n-}\\n+// Re-export the state machine engine as CheckExecutionEngine\\n+export { StateMachineExecutionEngine as CheckExecutionEngine } from './state-machine-execution-engine';\\n \\n-/**\\n- * Result of executing checks, including both the grouped results and execution statistics\\n- */\\n-export interface ExecutionResult {\\n-  results: GroupedCheckResults;\\n-  statistics: ExecutionStatistics;\\n-}\\n+// Re-export types from types/execution\\n+export type {\\n+  CheckExecutionOptions,\\n+  ExecutionResult,\\n+  CheckExecutionStats,\\n+  ExecutionStatistics,\\n+} from './types/execution';\\n \\n /**\\n- * Filter environment variables to only include safe ones for sandbox evaluation\\n+ * Mock Octokit interface for testing\\n+ *\\n+ * @deprecated This type is preserved for backward compatibility but should not be used.\\n+ * Use createMockOctokit() from test utilities instead.\\n  */\\n-function getSafeEnvironmentVariables(): Record<string, string> {\\n-  const { buildSandboxEnv } = require('./utils/env-exposure');\\n-  return buildSandboxEnv(process.env);\\n-}\\n-\\n export interface MockOctokit {\\n   rest: {\\n     pulls: {\\n@@ -133,7605 +58,3 @@ export interface MockOctokit {\\n   };\\n   auth: () => Promise<{ token: string }>;\\n }\\n-\\n-export interface CheckExecutionOptions {\\n-  checks: string[];\\n-  workingDirectory?: string;\\n-  showDetails?: boolean;\\n-  timeout?: number;\\n-  maxParallelism?: number; // Maximum number of checks to run in parallel (default: 3)\\n-  failFast?: boolean; // Stop execution when any check fails (default: false)\\n-  outputFormat?: string;\\n-  config?: import('./types/config').VisorConfig;\\n-  debug?: boolean; // Enable debug mode to collect AI execution details\\n-  // Tag filter for selective check execution\\n-  tagFilter?: import('./types/config').TagFilter;\\n-  // Webhook context for passing webhook data to http_input providers\\n-  webhookContext?: {\\n-    webhookData: Map<string, unknown>;\\n-  };\\n-  // GitHub Check integration options\\n-  githubChecks?: {\\n-    enabled: boolean;\\n-    octokit?: import('@octokit/rest').Octokit;\\n-    owner?: string;\\n-    repo?: string;\\n-    headSha?: string;\\n-    prNumber?: number;\\n-  };\\n-}\\n-\\n-export class CheckExecutionEngine {\\n-  private gitAnalyzer: GitRepositoryAnalyzer;\\n-  private mockOctokit: MockOctokit;\\n-  private reviewer: PRReviewer;\\n-  private providerRegistry: CheckProviderRegistry;\\n-  private failureEvaluator: FailureConditionEvaluator;\\n-  private githubCheckService?: GitHubCheckService;\\n-  private checkRunMap?: Map<string, { id: number; url: string }>;\\n-  private githubContext?: { owner: string; repo: string };\\n-  private workingDirectory: string;\\n-  private config?: import('./types/config').VisorConfig;\\n-  private webhookContext?: { webhookData: Map<string, unknown> };\\n-  private routingSandbox?: Sandbox;\\n-  private executionStats: Map<string, CheckExecutionStats> = new Map();\\n-  // Track history of all outputs for each check (useful for loops and goto)\\n-  private outputHistory: Map<string, unknown[]> = new Map();\\n-  // Track on_finish loop counts per forEach parent during a single execution run\\n-  private onFinishLoopCounts: Map<string, number> = new Map();\\n-  // Track how many times a forEach parent check has produced an array during this run (\\\"waves\\\")\\n-  private forEachWaveCounts: Map<string, number> = new Map();\\n-  // One-shot guards for post on_finish scheduling to avoid duplicate replies when\\n-  // multiple signals (aggregator, memory, history) agree. Keyed by session + parent check.\\n-  private postOnFinishGuards: Set<string> = new Set();\\n-  // Snapshot+Scope journal (Phase 0: commit only, no behavior changes yet)\\n-  private journal: ExecutionJournal = new ExecutionJournal();\\n-  private sessionId: string = `sess-${Date.now().toString(36)}-${Math.random()\\n-    .toString(36)\\n-    .slice(2, 8)}`;\\n-  // Dedup forward-run targets within a single grouped run (stage/event).\\n-  // Keyed by `${event}:${target}`.\\n-  private forwardRunGuards: Set<string> = new Set();\\n-  // Track per-grouped-run scheduling of specific steps we want to allow only once.\\n-  // Currently used to ensure 'validate-fact' is scheduled at most once per stage.\\n-  private oncePerRunScheduleGuards: Set<string> = new Set();\\n-  // Event override to simulate alternate event (used during routing goto)\\n-  private routingEventOverride?: import('./types/config').EventTrigger;\\n-  // Execution context for providers (CLI message, hooks, etc.)\\n-  private executionContext?: import('./providers/check-provider.interface').ExecutionContext;\\n-  // Cached GitHub context for context elevation when running in Actions\\n-  private actionContext?: {\\n-    owner: string;\\n-    repo: string;\\n-    octokit?: import('@octokit/rest').Octokit;\\n-  };\\n-\\n-  constructor(workingDirectory?: string, octokit?: import('@octokit/rest').Octokit) {\\n-    this.workingDirectory = workingDirectory || process.cwd();\\n-    this.gitAnalyzer = new GitRepositoryAnalyzer(this.workingDirectory);\\n-    this.providerRegistry = CheckProviderRegistry.getInstance();\\n-    this.failureEvaluator = new FailureConditionEvaluator();\\n-\\n-    // If authenticated octokit is provided, cache it for provider use\\n-    if (octokit) {\\n-      const repoEnv = process.env.GITHUB_REPOSITORY || '';\\n-      const [owner, repo] = repoEnv.split('/') as [string, string];\\n-      if (owner && repo) {\\n-        this.actionContext = { owner, repo, octokit };\\n-      }\\n-    }\\n-\\n-    // Create a mock Octokit instance for local analysis\\n-    // This allows us to reuse the existing PRReviewer logic without network calls\\n-    this.mockOctokit = this.createMockOctokit();\\n-    // Prefer the provided authenticated/recording Octokit (from test runner or Actions)\\n-    // so that comment create/update operations are visible to recorders and assertions.\\n-    const reviewerOctokit =\\n-      (octokit as unknown as import('@octokit/rest').Octokit) ||\\n-      (this.mockOctokit as unknown as import('@octokit/rest').Octokit);\\n-    this.reviewer = new PRReviewer(reviewerOctokit);\\n-  }\\n-\\n-  private sessionUUID(): string {\\n-    return this.sessionId;\\n-  }\\n-\\n-  /**\\n-   * Reset per-run guard and statistics state. Callers that orchestrate grouped\\n-   * executions (e.g., the YAML test runner) can invoke this to ensure clean\\n-   * stage-local accounting without introducing test-specific branches in the\\n-   * core engine.\\n-   */\\n-  public resetPerRunState(): void {\\n-    try {\\n-      this.forwardRunGuards.clear();\\n-    } catch {}\\n-    try {\\n-      this.oncePerRunScheduleGuards.clear();\\n-    } catch {}\\n-    try {\\n-      this.onFinishLoopCounts.clear();\\n-      this.forEachWaveCounts.clear();\\n-    } catch {}\\n-    try {\\n-      this['executionStats'].clear();\\n-    } catch {}\\n-  }\\n-\\n-  private commitJournal(\\n-    checkId: string,\\n-    result: ExtendedReviewSummary,\\n-    event?: import('./types/config').EventTrigger,\\n-    scopeOverride?: ScopePath\\n-  ): void {\\n-    try {\\n-      const scope: ScopePath = scopeOverride || [];\\n-      this.journal.commitEntry({\\n-        sessionId: this.sessionUUID(),\\n-        scope,\\n-        checkId,\\n-        event,\\n-        result,\\n-      });\\n-    } catch {\\n-      // best effort; never throw\\n-    }\\n-  }\\n-\\n-  /** Build dependencyResults from a snapshot of all committed results, optionally overlaying provided results. */\\n-  private buildSnapshotDependencyResults(\\n-    scope: ScopePath,\\n-    overlay: Map<string, ReviewSummary> | undefined,\\n-    event: import('./types/config').EventTrigger | undefined\\n-  ): Map<string, ReviewSummary> {\\n-    const snap = this.journal.beginSnapshot();\\n-    const view = new ContextView(this.journal, this.sessionUUID(), snap, scope, event);\\n-    const visible = new Map<string, ReviewSummary>();\\n-    try {\\n-      const entries = this.journal.readVisible(this.sessionUUID(), snap, event);\\n-      const ids = Array.from(new Set(entries.map(e => e.checkId)));\\n-      for (const id of ids) {\\n-        const v = view.get(id);\\n-        if (v) visible.set(id, v);\\n-        const raw = view.getRaw(id);\\n-        if (raw) visible.set(`${id}-raw`, raw);\\n-      }\\n-      // Overlay any provided results (e.g., per-item context) on top.\\n-      // Root-cause hardening: ignore non-string keys and log once.\\n-      if (overlay) {\\n-        for (const [k, v] of overlay.entries()) {\\n-          if (typeof k === 'string' && k) {\\n-            visible.set(k, v);\\n-          } else {\\n-            try {\\n-              require('./logger').logger.warn(\\n-                `sanitize: dropping non-string overlay key type=${typeof k}`\\n-              );\\n-            } catch {}\\n-          }\\n-        }\\n-      }\\n-    } catch {}\\n-    return visible;\\n-  }\\n-\\n-  /** Drop any non-string keys from a results-like map (root-cause guard). */\\n-  private sanitizeResultMapKeys(\\n-    m: Map<unknown, ReviewSummary> | undefined\\n-  ): Map<string, ReviewSummary> {\\n-    const out = new Map<string, ReviewSummary>();\\n-    if (!m) return out;\\n-    for (const [k, v] of m.entries()) {\\n-      if (typeof k === 'string' && k) out.set(k, v);\\n-      else {\\n-        try {\\n-          require('./logger').logger.warn(\\n-            `sanitize: dropping non-string results key type=${typeof k}`\\n-          );\\n-        } catch {}\\n-      }\\n-    }\\n-    return out;\\n-  }\\n-\\n-  /**\\n-   * Enrich event context with authenticated octokit instance\\n-   * @param eventContext - The event context to enrich\\n-   * @returns Enriched event context with octokit if available\\n-   */\\n-  private enrichEventContext(eventContext?: Record<string, unknown>): Record<string, unknown> {\\n-    const baseContext = eventContext || {};\\n-    const injected = this.actionContext?.octokit || (baseContext as any).octokit;\\n-    if (injected) {\\n-      return { ...baseContext, octokit: injected };\\n-    }\\n-    return baseContext;\\n-  }\\n-\\n-  /**\\n-   * Set execution context for providers (CLI message, hooks, etc.)\\n-   * This allows passing state without using static properties\\n-   */\\n-  setExecutionContext(\\n-    context: import('./providers/check-provider.interface').ExecutionContext\\n-  ): void {\\n-    this.executionContext = context;\\n-  }\\n-\\n-  /**\\n-   * Lazily create a secure sandbox for routing JS (goto_js, run_js)\\n-   */\\n-  private getRoutingSandbox(): Sandbox {\\n-    if (this.routingSandbox) return this.routingSandbox;\\n-    this.routingSandbox = createSecureSandbox();\\n-    return this.routingSandbox;\\n-  }\\n-\\n-  private redact(str: unknown, limit = 200): string {\\n-    try {\\n-      const s = typeof str === 'string' ? str : JSON.stringify(str);\\n-      return s.length > limit ? s.slice(0, limit) + '‚Ä¶' : s;\\n-    } catch {\\n-      return String(str).slice(0, limit);\\n-    }\\n-  }\\n-\\n-  private async sleep(ms: number): Promise<void> {\\n-    return new Promise(resolve => setTimeout(resolve, ms));\\n-  }\\n-\\n-  private deterministicJitter(baseMs: number, seedStr: string): number {\\n-    let h = 2166136261;\\n-    for (let i = 0; i < seedStr.length; i++) h = (h ^ seedStr.charCodeAt(i)) * 16777619;\\n-    const frac = ((h >>> 0) % 1000) / 1000; // 0..1\\n-    return Math.floor(baseMs * 0.15 * frac); // up to 15% jitter\\n-  }\\n-\\n-  // === on_finish helpers (extracted to reduce handleOnFinishHooks complexity) ===\\n-  private composeOnFinishContext(\\n-    checkName: string,\\n-    checkConfig: import('./types/config').CheckConfig,\\n-    outputsForContext: Record<string, unknown>,\\n-    outputsHistoryForContext: Record<string, unknown[]>,\\n-    forEachStats: any,\\n-    prInfo: PRInfo\\n-  ): {\\n-    step: { id: string; tags: string[]; group?: string };\\n-    attempt: number;\\n-    loop: number;\\n-    outputs: Record<string, unknown>;\\n-    outputs_history: Record<string, unknown[]>;\\n-    outputs_raw: Record<string, unknown>;\\n-    forEach: any;\\n-    memory: {\\n-      get: (key: string, ns?: string) => unknown;\\n-      has: (key: string, ns?: string) => boolean;\\n-      list: (ns?: string) => string[];\\n-      getAll: (ns?: string) => Record<string, unknown>;\\n-      set: (key: string, value: unknown, ns?: string) => void;\\n-      increment: (key: string, amount: number, ns?: string) => number;\\n-    };\\n-    pr: { number: number; title: string; author: string; branch: string; base: string };\\n-    files: PRInfo['files'];\\n-    env: Record<string, string>;\\n-    event: { name: string };\\n-  } {\\n-    const memoryStore = MemoryStore.getInstance(this.config?.memory);\\n-    const memoryHelpers = {\\n-      get: (key: string, ns?: string) => memoryStore.get(key, ns),\\n-      has: (key: string, ns?: string) => memoryStore.has(key, ns),\\n-      list: (ns?: string) => memoryStore.list(ns),\\n-      getAll: (ns?: string) => {\\n-        const keys = memoryStore.list(ns);\\n-        const result: Record<string, unknown> = {};\\n-        for (const key of keys) result[key] = memoryStore.get(key, ns);\\n-        return result;\\n-      },\\n-      set: (key: string, value: unknown, ns?: string) => {\\n-        const nsName = ns || memoryStore.getDefaultNamespace();\\n-        if (!memoryStore['data'].has(nsName)) memoryStore['data'].set(nsName, new Map());\\n-        memoryStore['data'].get(nsName)!.set(key, value);\\n-      },\\n-      increment: (key: string, amount: number, ns?: string) => {\\n-        const current = memoryStore.get(key, ns);\\n-        const numCurrent = typeof current === 'number' ? current : 0;\\n-        const newValue = numCurrent + amount;\\n-        const nsName = ns || memoryStore.getDefaultNamespace();\\n-        if (!memoryStore['data'].has(nsName)) memoryStore['data'].set(nsName, new Map());\\n-        memoryStore['data'].get(nsName)!.set(key, newValue);\\n-        return newValue;\\n-      },\\n-    };\\n-    const outputsRawForContext: Record<string, unknown> = {};\\n-    for (const [name, val] of Object.entries(outputsForContext)) {\\n-      if (name === 'history') continue;\\n-      outputsRawForContext[name] = val;\\n-    }\\n-    const outputsMergedForContext: Record<string, unknown> = {\\n-      ...outputsForContext,\\n-      history: outputsHistoryForContext,\\n-    };\\n-    return {\\n-      step: { id: checkName, tags: checkConfig.tags || [], group: checkConfig.group },\\n-      attempt: 1,\\n-      loop: 0,\\n-      outputs: outputsMergedForContext,\\n-      outputs_history: outputsHistoryForContext,\\n-      outputs_raw: outputsRawForContext,\\n-      forEach: forEachStats,\\n-      memory: memoryHelpers,\\n-      pr: {\\n-        number: prInfo.number,\\n-        title: prInfo.title,\\n-        author: prInfo.author,\\n-        branch: prInfo.head,\\n-        base: prInfo.base,\\n-      },\\n-      files: prInfo.files,\\n-      env: getSafeEnvironmentVariables(),\\n-      event: { name: prInfo.eventType || 'manual' },\\n-    };\\n-  }\\n-\\n-  private evaluateOnFinishGoto(\\n-    checkName: string,\\n-    onFinish: NonNullable<import('./types/config').CheckConfig['on_finish']>,\\n-    onFinishContext: any,\\n-    debug: boolean,\\n-    log: (msg: string) => void\\n-  ): string | null {\\n-    let gotoTarget: string | null = null;\\n-    if (onFinish.goto_js) {\\n-      logger.info(`‚ñ∂ on_finish.goto_js: evaluating for \\\"${checkName}\\\"`);\\n-      try {\\n-        const sandbox = this.getRoutingSandbox();\\n-        const scope = onFinishContext;\\n-        const code = `\\n-          const step = scope.step; const attempt = scope.attempt; const loop = scope.loop; const outputs = scope.outputs; const outputs_history = scope.outputs_history; const outputs_raw = scope.outputs_raw; const forEach = scope.forEach; const memory = scope.memory; const pr = scope.pr; const files = scope.files; const env = scope.env; const event = scope.event; const log = (...a)=> console.log('üîç Debug:',...a);\\n-          const __fn = () => {\\\\n${onFinish.goto_js}\\\\n};\\n-          const __res = __fn();\\n-          return (typeof __res === 'string' && __res) ? __res : null;\\n-        `;\\n-        const exec = sandbox.compile(code);\\n-        const result = exec({ scope }).run();\\n-        gotoTarget = typeof result === 'string' && result ? result : null;\\n-        if (debug) log(`üîß Debug: on_finish.goto_js evaluated ‚Üí ${this.redact(gotoTarget)}`);\\n-        logger.info(\\n-          `‚úì on_finish.goto_js: evaluated to '${gotoTarget || 'null'}' for \\\"${checkName}\\\"`\\n-        );\\n-      } catch (error) {\\n-        const errorMsg = error instanceof Error ? error.message : String(error);\\n-        logger.warn(`‚ö†Ô∏è on_finish.goto_js: evaluation failed for \\\"${checkName}\\\": ${errorMsg}`);\\n-        if (error instanceof Error && error.stack) logger.debug(`Stack trace: ${error.stack}`);\\n-        if (onFinish.goto) {\\n-          logger.info(`  ‚ö† Falling back to static goto: '${onFinish.goto}'`);\\n-          gotoTarget = onFinish.goto;\\n-        }\\n-      }\\n-    } else if (onFinish.goto) {\\n-      gotoTarget = onFinish.goto;\\n-      logger.info(`‚ñ∂ on_finish.goto: routing to '${gotoTarget}' for \\\"${checkName}\\\"`);\\n-    }\\n-    return gotoTarget;\\n-  }\\n-\\n-  private computeBackoffDelay(\\n-    attempt: number,\\n-    mode: 'fixed' | 'exponential',\\n-    baseMs: number,\\n-    seed: string\\n-  ): number {\\n-    const jitter = this.deterministicJitter(baseMs, seed);\\n-    if (mode === 'exponential') {\\n-      return baseMs * Math.pow(2, Math.max(0, attempt - 1)) + jitter;\\n-    }\\n-    return baseMs + jitter;\\n-  }\\n-\\n-  /**\\n-   * Execute a single named check inline (used by routing logic and on_finish)\\n-   * This is extracted from executeWithRouting to be reusable\\n-   */\\n-  private async executeCheckInline(\\n-    checkId: string,\\n-    event: import('./types/config').EventTrigger,\\n-    context: {\\n-      config: VisorConfig;\\n-      dependencyGraph: DependencyGraph;\\n-      prInfo: PRInfo;\\n-      resultsMap: Map<string, ReviewSummary>;\\n-      dependencyResults: Map<string, ReviewSummary>;\\n-      sessionInfo?: { parentSessionId?: string; reuseSession?: boolean };\\n-      debug: boolean;\\n-      eventOverride?: import('./types/config').EventTrigger;\\n-      scope?: ScopePath;\\n-      origin?: 'on_finish' | 'on_success' | 'on_fail' | 'foreach' | 'initial' | 'inline';\\n-    }\\n-  ): Promise<ReviewSummary> {\\n-    const {\\n-      config,\\n-      prInfo,\\n-      resultsMap,\\n-      dependencyResults,\\n-      sessionInfo,\\n-      debug,\\n-      eventOverride,\\n-      scope,\\n-    } = context;\\n-    const log = (msg: string) => (config?.output?.pr_comment ? console.error : console.log)(msg);\\n-    const origin = (context as any).origin || 'inline';\\n-\\n-    // Find the check configuration\\n-    const checkConfig = config?.checks?.[checkId];\\n-    if (!checkConfig) {\\n-      throw new Error(`on_finish referenced unknown check '${checkId}'`);\\n-    }\\n-\\n-    // Helper to get all dependencies recursively from config\\n-    const getAllDepsFromConfig = (name: string): string[] => {\\n-      const visited = new Set<string>();\\n-      const acc: string[] = [];\\n-      const dfs = (n: string) => {\\n-        if (visited.has(n)) return;\\n-        visited.add(n);\\n-        const cfg = config?.checks?.[n];\\n-        const deps = cfg?.depends_on || [];\\n-        for (const d of deps) {\\n-          acc.push(d);\\n-          dfs(d);\\n-        }\\n-      };\\n-      dfs(name);\\n-      return Array.from(new Set(acc));\\n-    };\\n-\\n-    // Ensure all dependencies of target are available; execute missing ones in topological order\\n-    const allTargetDeps = getAllDepsFromConfig(checkId);\\n-    if (allTargetDeps.length > 0) {\\n-      // Build subgraph mapping for ordered execution\\n-      const subSet = new Set<string>([...allTargetDeps]);\\n-      const subDeps: Record<string, string[]> = {};\\n-      for (const id of subSet) {\\n-        const cfg = config?.checks?.[id];\\n-        subDeps[id] = (cfg?.depends_on || []).filter(d => subSet.has(d));\\n-      }\\n-      const subGraph = DependencyResolver.buildDependencyGraph(subDeps);\\n-      for (const group of subGraph.executionOrder) {\\n-        for (const depId of group.parallel) {\\n-          // Skip if already have results\\n-          if (resultsMap?.has(depId) || dependencyResults.has(depId)) continue;\\n-          // Execute dependency inline (recursively ensures its deps are also present)\\n-          await this.executeCheckInline(depId, event, context);\\n-        }\\n-      }\\n-    }\\n-\\n-    // No legacy adapters; use configuration as-is\\n-    const adaptedConfig: any = { ...checkConfig };\\n-    const providerType = adaptedConfig.type || 'ai';\\n-    const provider = this.providerRegistry.getProviderOrThrow(providerType);\\n-    this.setProviderWebhookContext(provider);\\n-\\n-    // Build provider configuration\\n-    const provCfg: CheckProviderConfig = {\\n-      type: providerType,\\n-      prompt: adaptedConfig.prompt,\\n-      exec: adaptedConfig.exec,\\n-      focus: adaptedConfig.focus || this.mapCheckNameToFocus(checkId),\\n-      schema: adaptedConfig.schema,\\n-      group: adaptedConfig.group,\\n-      checkName: checkId,\\n-      eventContext: this.enrichEventContext(prInfo.eventContext),\\n-      transform: adaptedConfig.transform,\\n-      transform_js: adaptedConfig.transform_js,\\n-      env: adaptedConfig.env,\\n-      forEach: adaptedConfig.forEach,\\n-      // Pass output history for loop/goto scenarios\\n-      __outputHistory: this.outputHistory,\\n-      // Include provider-specific keys (e.g., op/values for github)\\n-      ...adaptedConfig,\\n-      ai: {\\n-        ...(adaptedConfig.ai || {}),\\n-        timeout: adaptedConfig.ai?.timeout || 600000,\\n-        debug: !!debug,\\n-      },\\n-    };\\n-\\n-    // Build dependency results for this check using snapshot-based visibility (overlay per-scope results)\\n-    const depResults = this.buildSnapshotDependencyResults(\\n-      scope || [],\\n-      dependencyResults,\\n-      eventOverride || prInfo.eventType\\n-    );\\n-\\n-    // Debug: log key dependent outputs for visibility\\n-    if (debug) {\\n-      try {\\n-        const depPreview: Record<string, unknown> = {};\\n-        for (const [k, v] of depResults.entries()) {\\n-          const out = (v as any)?.output;\\n-          if (out !== undefined) depPreview[k] = out;\\n-        }\\n-        log(`üîß Debug: inline exec '${checkId}' deps output: ${JSON.stringify(depPreview)}`);\\n-      } catch {}\\n-    }\\n-\\n-    if (debug) {\\n-      const execStr = (provCfg as any).exec;\\n-      if (execStr) log(`üîß Debug: inline exec '${checkId}' command: ${execStr}`);\\n-    }\\n-\\n-    // If event override provided, clone prInfo with overridden eventType\\n-    let prInfoForInline = prInfo;\\n-    const prevEventOverride = this.routingEventOverride;\\n-    if (eventOverride) {\\n-      // Try to elevate to PR context when routing to PR events from issue threads\\n-      const elevated = await this.elevateContextToPullRequest(\\n-        { ...(prInfo as any), eventType: eventOverride } as PRInfo,\\n-        eventOverride,\\n-        log,\\n-        debug\\n-      );\\n-      if (elevated) {\\n-        prInfoForInline = elevated;\\n-      } else {\\n-        prInfoForInline = { ...(prInfo as any), eventType: eventOverride } as PRInfo;\\n-      }\\n-      this.routingEventOverride = eventOverride;\\n-      const msg = `‚Ü™ goto_event: inline '${checkId}' with event=${eventOverride}${\\n-        elevated ? ' (elevated to PR context)' : ''\\n-      }`;\\n-      if (debug) log(`üîß Debug: ${msg}`);\\n-      try {\\n-        require('./logger').logger.info(msg);\\n-      } catch {}\\n-    }\\n-\\n-    // Execute the check\\n-    let result: ReviewSummary;\\n-    try {\\n-      const __provStart = Date.now();\\n-      const inlineContext: import('./providers/check-provider.interface').ExecutionContext = {\\n-        ...sessionInfo,\\n-        ...this.executionContext,\\n-      } as any;\\n-      // dependency printout removed\\n-      result = await withActiveSpan(\\n-        `visor.check.${checkId}`,\\n-        { 'visor.check.id': checkId, 'visor.check.type': provCfg.type || 'ai' },\\n-        async () => provider.execute(prInfoForInline, provCfg, depResults, inlineContext)\\n-      );\\n-      this.recordProviderDuration(checkId, Date.now() - __provStart);\\n-    } catch (error) {\\n-      // Restore previous override before rethrowing\\n-      this.routingEventOverride = prevEventOverride;\\n-      throw error;\\n-    } finally {\\n-      // Always restore previous override\\n-      this.routingEventOverride = prevEventOverride;\\n-    }\\n-\\n-    // Enrich issues with metadata\\n-    const enrichedIssues = (result.issues || []).map(issue => ({\\n-      ...issue,\\n-      checkName: checkId,\\n-      ruleId: `${checkId}/${issue.ruleId}`,\\n-      group: checkConfig.group,\\n-      schema: typeof checkConfig.schema === 'object' ? 'custom' : checkConfig.schema,\\n-      template: checkConfig.template,\\n-      timestamp: Date.now(),\\n-    }));\\n-    let enriched = { ...result, issues: enrichedIssues } as ReviewSummary;\\n-\\n-    // Track output history for loop/goto scenarios\\n-    const enrichedWithOutput = enriched as ReviewSummary & { output?: unknown };\\n-    if (enrichedWithOutput.output !== undefined) {\\n-      this.trackOutputHistory(checkId, enrichedWithOutput.output);\\n-    }\\n-\\n-    // Handle forEach iteration for this check if it returned an array\\n-    if (checkConfig.forEach && Array.isArray(enrichedWithOutput.output)) {\\n-      const forEachItems = enrichedWithOutput.output;\\n-      // Always log forEach detection (not just in debug mode) for visibility\\n-      const wave = (this.forEachWaveCounts.get(checkId) || 0) + 1;\\n-      this.forEachWaveCounts.set(checkId, wave);\\n-      log(\\n-        `üîÑ forEach check '${checkId}' returned ${forEachItems.length} items - starting iteration (wave #${wave}, origin=${origin})`\\n-      );\\n-      if (debug) {\\n-        log(\\n-          `üîß Debug: forEach item preview: ${JSON.stringify(forEachItems[0] || {}).substring(0, 200)}`\\n-        );\\n-      }\\n-\\n-      // Store the array output with forEach metadata\\n-      const forEachResult = {\\n-        ...enriched,\\n-        forEachItems,\\n-        forEachItemResults: forEachItems.map(item => ({\\n-          issues: [],\\n-          output: item,\\n-        })),\\n-      };\\n-      enriched = forEachResult as ReviewSummary;\\n-\\n-      // Make the parent result visible to dependency resolution BEFORE scheduling dependents\\n-      // so that recursive dependency checks do not re-execute this forEach parent in the same wave.\\n-      try {\\n-        resultsMap?.set(checkId, enriched);\\n-      } catch {}\\n-\\n-      // Phase 4: commit aggregate parent result early (root scope) so outputs_raw is visible\\n-      this.commitJournal(\\n-        checkId,\\n-        enriched as ExtendedReviewSummary,\\n-        prInfoForInline.eventType || prInfo.eventType,\\n-        []\\n-      );\\n-\\n-      // Wave guard: if waves exceed routing.max_loops, stop scheduling dependents to prevent runaway loops\\n-      const maxLoops = config?.routing?.max_loops ?? 10;\\n-      if (wave > maxLoops) {\\n-        try {\\n-          logger.warn(\\n-            `‚õî forEach wave guard: '${checkId}' exceeded max_loops=${maxLoops} (wave #${wave}); skipping dependents and routing`\\n-          );\\n-        } catch {}\\n-        // Store and return aggregated result\\n-        resultsMap?.set(checkId, enriched);\\n-        return enriched;\\n-      }\\n-\\n-      // Find checks that depend on this forEach check\\n-      const dependentChecks = Object.keys(config?.checks || {}).filter(name => {\\n-        const cfg = config?.checks?.[name];\\n-        return cfg?.depends_on?.includes(checkId);\\n-      });\\n-\\n-      // Always log dependents for visibility\\n-      try {\\n-        if (dependentChecks.length > 0) {\\n-          log(\\n-            `üîÑ forEach check '${checkId}' has ${dependentChecks.length} dependents: ${dependentChecks.join(', ')}`\\n-          );\\n-        } else {\\n-          log(`‚ö†Ô∏è  forEach check '${checkId}' has NO dependents - nothing to iterate`);\\n-        }\\n-      } catch {}\\n-\\n-      // Execute each dependent check once per forEach item (scope-based; no per-item map cloning)\\n-      for (const depCheckName of dependentChecks) {\\n-        const depCheckConfig = config?.checks?.[depCheckName];\\n-        if (!depCheckConfig) continue;\\n-\\n-        // Always (re)run dependents during inline reruns (on_finish.goto to parent).\\n-        // We intentionally do not short-circuit on existing results here so stats/history\\n-        // reflect multiple waves.\\n-        // Skip if no items to iterate over\\n-        if (forEachItems.length === 0) {\\n-          if (debug) {\\n-            log(`üîß Debug: Skipping forEach dependent '${depCheckName}' - no items to iterate`);\\n-          }\\n-          // Store empty result\\n-          resultsMap?.set(depCheckName, { issues: [] });\\n-          continue;\\n-        }\\n-\\n-        // Always log iteration start\\n-        try {\\n-          const wave = this.forEachWaveCounts.get(checkId) || 1;\\n-          log(\\n-            `üîÑ Executing forEach dependent '${depCheckName}' for ${forEachItems.length} items (wave #${wave})`\\n-          );\\n-        } catch {}\\n-\\n-        const depResults: ReviewSummary[] = [];\\n-\\n-        // Execute once per forEach item\\n-        for (let itemIndex = 0; itemIndex < forEachItems.length; itemIndex++) {\\n-          const item = forEachItems[itemIndex];\\n-          const wave = this.forEachWaveCounts.get(checkId) || 1;\\n-          log(\\n-            `  üîÑ Iteration ${itemIndex + 1}/${forEachItems.length} f|| '${depCheckName}' (wave #${wave})`\\n-          );\\n-\\n-          // Phase 4: Commit per-item entry for parent in journal under item scope\\n-          const itemScope: ScopePath = [{ check: checkId, index: itemIndex }];\\n-          try {\\n-            this.commitJournal(\\n-              checkId,\\n-              { issues: [], output: item } as ExtendedReviewSummary,\\n-              prInfoForInline.eventType || prInfo.eventType,\\n-              itemScope\\n-            );\\n-          } catch (error) {\\n-            const msg = error instanceof Error ? error.message : String(error);\\n-            logger.error(`Failed to commit per-item journal for ${checkId}: ${msg}`);\\n-            // Non-fatal: journal is best-effort; continue without retry.\\n-          }\\n-\\n-          try {\\n-            // Build provider + config for dependent and execute with full routing semantics\\n-            const depProviderType = depCheckConfig.type || 'ai';\\n-            const depProvider = this.providerRegistry.getProviderOrThrow(depProviderType);\\n-            this.setProviderWebhookContext(depProvider);\\n-\\n-            // Build dependency results from snapshot at item scope (no cloning)\\n-            const snapshotDeps = this.buildSnapshotDependencyResults(\\n-              itemScope,\\n-              undefined,\\n-              prInfoForInline.eventType || prInfo.eventType\\n-            );\\n-\\n-            // Use unified helper to ensure stats and history are tracked for each item run\\n-            const res = await this.runNamedCheck(depCheckName, itemScope, {\\n-              origin: 'foreach',\\n-              config: config!,\\n-              dependencyGraph: context.dependencyGraph,\\n-              prInfo,\\n-              resultsMap: resultsMap || new Map(),\\n-              debug: !!debug,\\n-              eventOverride: prInfoForInline.eventType || prInfo.eventType,\\n-              overlay: snapshotDeps,\\n-            });\\n-            depResults.push(res);\\n-          } catch (error) {\\n-            // Store error result for this iteration\\n-            const errorMsg = error instanceof Error ? error.message : String(error);\\n-            const errorIssue: ReviewIssue = {\\n-              file: '',\\n-              line: 0,\\n-              ruleId: `${depCheckName}/forEach/iteration_error`,\\n-              message: `forEach iteration ${itemIndex + 1} failed: ${errorMsg}`,\\n-              severity: 'error',\\n-              category: 'logic',\\n-            };\\n-            depResults.push({\\n-              issues: [errorIssue],\\n-            });\\n-          }\\n-        }\\n-\\n-        // Aggregate results from all iterations\\n-        const aggregatedResult: ReviewSummary = {\\n-          issues: depResults.flatMap(r => r.issues || []),\\n-        };\\n-\\n-        // Store in results map\\n-        resultsMap?.set(depCheckName, aggregatedResult);\\n-\\n-        if (debug) {\\n-          log(\\n-            `üîß Debug: Completed forEach dependent '${depCheckName}' with ${depResults.length} iterations`\\n-          );\\n-        }\\n-      }\\n-    }\\n-\\n-    // Store result in results map\\n-    resultsMap?.set(checkId, enriched);\\n-    // Commit to journal with provided scope (or root). Avoid double-commit if we already committed aggregate above.\\n-    const isForEachAggregate = checkConfig.forEach && Array.isArray(enrichedWithOutput.output);\\n-    if (!isForEachAggregate) {\\n-      this.commitJournal(\\n-        checkId,\\n-        enriched as ExtendedReviewSummary,\\n-        prInfoForInline.eventType || prInfo.eventType,\\n-        scope || []\\n-      );\\n-    }\\n-\\n-    if (debug) log(`üîß Debug: inline executed '${checkId}', issues: ${enrichedIssues.length}`);\\n-\\n-    return enriched;\\n-  }\\n-\\n-  /**\\n-   * Phase 3: Unified scheduling helper\\n-   * Runs a named check in the current session/scope and records results.\\n-   * Used by on_success/on_fail/on_finish routing and internal inline execution.\\n-   */\\n-  private async runNamedCheck(\\n-    target: string,\\n-    scope: ScopePath,\\n-    opts: {\\n-      config: VisorConfig;\\n-      dependencyGraph: DependencyGraph;\\n-      prInfo: PRInfo;\\n-      resultsMap: Map<string, ReviewSummary>;\\n-      debug: boolean;\\n-      sessionInfo?: { parentSessionId?: string; reuseSession?: boolean };\\n-      eventOverride?: import('./types/config').EventTrigger;\\n-      overlay?: Map<string, ReviewSummary>;\\n-      origin?: 'on_finish' | 'on_success' | 'on_fail' | 'foreach' | 'initial' | 'inline';\\n-    }\\n-  ): Promise<ReviewSummary> {\\n-    const {\\n-      config,\\n-      dependencyGraph,\\n-      prInfo,\\n-      resultsMap,\\n-      debug,\\n-      sessionInfo,\\n-      eventOverride,\\n-      overlay,\\n-    } = opts;\\n-    try {\\n-      if (debug && opts.origin === 'on_finish') {\\n-        console.error(`[runNamedCheck] origin=on_finish step=${target}`);\\n-      }\\n-    } catch {}\\n-\\n-    // Evaluate 'if' condition for checks executed via routing (run/goto).\\n-    try {\\n-      const tcfg = opts.config.checks?.[target] as import('./types/config').CheckConfig | undefined;\\n-      if (tcfg && tcfg.if) {\\n-        const gate = await this.shouldRunCheck(\\n-          target,\\n-          tcfg.if,\\n-          opts.prInfo,\\n-          opts.resultsMap || new Map<string, ReviewSummary>(),\\n-          !!debug,\\n-          opts.eventOverride,\\n-          /* failSecure */ true\\n-        );\\n-        if (!gate.shouldRun) {\\n-          // Record a skipped marker compatible with summary rendering\\n-          const skipped: ReviewSummary = {\\n-            issues: [\\n-              {\\n-                file: '',\\n-                line: 0,\\n-                ruleId: `${target}/__skipped`,\\n-                message: `Skipped by if condition: ${tcfg.if}`,\\n-                severity: 'info',\\n-                category: 'logic',\\n-              },\\n-            ],\\n-          } as ReviewSummary;\\n-          try {\\n-            this.recordSkip(target, 'if_condition', tcfg.if);\\n-            logger.info(`‚è≠  Skipped (if: ${this.truncate(tcfg.if, 40)})`);\\n-          } catch (error) {\\n-            const msg = error instanceof Error ? error.message : String(error);\\n-            logger.error(`Failed to record skip for ${target}: ${msg}`);\\n-          }\\n-          // Commit a minimal journal entry to make downstream visibility consistent\\n-          this.commitJournal(\\n-            target,\\n-            skipped as any,\\n-            opts.eventOverride || opts.prInfo.eventType,\\n-            scope || []\\n-          );\\n-          opts.resultsMap?.set(target, skipped);\\n-          return skipped;\\n-        }\\n-      }\\n-    } catch (error) {\\n-      const msg = error instanceof Error ? error.message : String(error);\\n-      logger.error(`Failed to evaluate if condition for ${target}: ${msg}`);\\n-      // Fail secure: if condition evaluation fails, skip execution\\n-      const skipped: ReviewSummary = {\\n-        issues: [\\n-          {\\n-            file: '',\\n-            line: 0,\\n-            ruleId: `${target}/__skipped`,\\n-            message: `Skipped due to condition evaluation error`,\\n-            severity: 'info',\\n-            category: 'logic',\\n-          },\\n-        ],\\n-      } as ReviewSummary;\\n-      try {\\n-        const cond =\\n-          (opts.config.checks?.[target] as import('./types/config').CheckConfig | undefined)?.if ||\\n-          '';\\n-        this.recordSkip(target, 'if_condition', cond);\\n-      } catch {}\\n-      this.commitJournal(\\n-        target,\\n-        skipped as any,\\n-        opts.eventOverride || opts.prInfo.eventType,\\n-        scope || []\\n-      );\\n-      opts.resultsMap?.set(target, skipped);\\n-      return skipped;\\n-    }\\n-\\n-    // Build context overlay from current results; prefer snapshot visibility for scope (Phase 4)\\n-    const depOverlay = overlay ? new Map(overlay) : new Map(resultsMap);\\n-    const depOverlaySanitized = this.sanitizeResultMapKeys(depOverlay);\\n-    // For event overrides, avoid leaking cross-event results via overlay; rely on snapshot-only view\\n-    const overlayForExec =\\n-      eventOverride && eventOverride !== (prInfo.eventType || 'manual')\\n-        ? new Map<string, ReviewSummary>()\\n-        : depOverlaySanitized;\\n-    if (!this.executionStats.has(target)) this.initializeCheckStats(target);\\n-    const startTs = this.recordIterationStart(target);\\n-    try {\\n-      const res = await this.executeCheckInline(\\n-        target,\\n-        eventOverride || prInfo.eventType || 'manual',\\n-        {\\n-          config,\\n-          dependencyGraph,\\n-          prInfo,\\n-          resultsMap,\\n-          // Use snapshot-only deps when eventOverride is set\\n-          dependencyResults: overlayForExec,\\n-          sessionInfo,\\n-          debug,\\n-          eventOverride,\\n-          scope,\\n-          origin: opts.origin || 'inline',\\n-        }\\n-      );\\n-      const issues = (res.issues || []).map(i => ({ ...i }));\\n-      const success = !this.hasFatal(issues);\\n-      const out: unknown = (res as { output?: unknown }).output;\\n-      const isForEachParent =\\n-        (res as any)?.isForEach === true ||\\n-        Array.isArray((res as any)?.forEachItems) ||\\n-        Array.isArray(out);\\n-      this.recordIterationComplete(\\n-        target,\\n-        startTs,\\n-        success,\\n-        issues,\\n-        isForEachParent ? undefined : out\\n-      );\\n-      // Output history is already tracked inside executeCheckInline when a check\\n-      // produces an output. Avoid tracking again here to prevent double-counting\\n-      // (particularly for forward-run goto chains within a single stage).\\n-      return res;\\n-    } catch (e) {\\n-      this.recordIterationComplete(target, startTs, false, [], undefined);\\n-      throw e;\\n-    }\\n-  }\\n-\\n-  /**\\n-   * Handle on_finish hooks for forEach checks after ALL dependents complete\\n-   */\\n-  private async handleOnFinishHooks(\\n-    config: VisorConfig,\\n-    dependencyGraph: DependencyGraph,\\n-    results: Map<string, ReviewSummary>,\\n-    prInfo: PRInfo,\\n-    debug: boolean\\n-  ): Promise<void> {\\n-    const log = (msg: string) => (config?.output?.pr_comment ? console.error : console.log)(msg);\\n-    try {\\n-      if (debug) console.error('[on_finish] handler invoked');\\n-    } catch {}\\n-\\n-    const forEachChecksWithOnFinish = this.collectForEachParentsWithOnFinish(config);\\n-\\n-    try {\\n-      logger.info(\\n-        `üß≠ on_finish: discovered ${forEachChecksWithOnFinish.length} forEach parent(s) with hooks`\\n-      );\\n-    } catch {}\\n-    if (forEachChecksWithOnFinish.length === 0) {\\n-      return; // No on_finish hooks to process\\n-    }\\n-\\n-    // Fast path: if none of the forEach parents executed in this run, skip on_finish entirely.\\n-    // This avoids unnecessary work (and log noise) in cases where these parents belong to a different event.\\n-    try {\\n-      const anyParentRan = forEachChecksWithOnFinish.some(({ checkName }) =>\\n-        results.has(checkName)\\n-      );\\n-      if (!anyParentRan) {\\n-        if (debug) log('üß≠ on_finish: no forEach parent executed in this run ‚Äî skip');\\n-        return;\\n-      }\\n-    } catch {}\\n-\\n-    if (debug) {\\n-      log(`üéØ Processing on_finish hooks for ${forEachChecksWithOnFinish.length} forEach check(s)`);\\n-    }\\n-\\n-    // Process each forEach check's on_finish hook\\n-    for (const { checkName, checkConfig, onFinish } of forEachChecksWithOnFinish) {\\n-      try {\\n-        const forEachResult = results.get(checkName) as ExtendedReviewSummary | undefined;\\n-        if (!forEachResult) {\\n-          try {\\n-            logger.info(`‚è≠ on_finish: no result found for \\\"${checkName}\\\" ‚Äî skip`);\\n-          } catch {}\\n-          continue;\\n-        }\\n-\\n-        // Skip if the forEach check returned empty array\\n-        const forEachItems = forEachResult.forEachItems || [];\\n-        if (forEachItems.length === 0) {\\n-          try {\\n-            logger.info(`‚è≠ on_finish: \\\"${checkName}\\\" produced 0 items ‚Äî skip`);\\n-          } catch {}\\n-          continue;\\n-        }\\n-\\n-        // Get all dependents of this forEach check\\n-        const node = dependencyGraph.nodes.get(checkName);\\n-        const dependents = node?.dependents || [];\\n-\\n-        try {\\n-          logger.info(`üîç on_finish: \\\"${checkName}\\\" ‚Üí ${dependents.length} dependent(s)`);\\n-        } catch {}\\n-\\n-        // Ensure all dependents have completed before processing on_finish.\\n-        // If any are missing, try to execute them now in the on_finish phase so aggregation\\n-        // has up-to-date data (particularly important for forEach + validators).\\n-        for (const depId of dependents) {\\n-          if (results.has(depId)) continue;\\n-          try {\\n-            if (debug)\\n-              log(\\n-                `üîß on_finish: executing missing dependent '${depId}' before processing '${checkName}'`\\n-              );\\n-            const depRes = await this.runNamedCheck(depId, [], {\\n-              origin: 'on_finish',\\n-              config,\\n-              dependencyGraph,\\n-              prInfo,\\n-              resultsMap: results,\\n-              sessionInfo: (this.executionContext as any) || undefined,\\n-              debug,\\n-              overlay: new Map(results),\\n-            });\\n-            try {\\n-              results.set(depId, depRes as ReviewSummary);\\n-            } catch {}\\n-          } catch (e) {\\n-            // If a dependent cannot run, continue; downstream hooks may still choose to skip\\n-            try {\\n-              const msg = e instanceof Error ? e.message : String(e);\\n-              logger.warn(`‚ö†Ô∏è on_finish: failed to execute dependent '${depId}': ${msg}`);\\n-            } catch {}\\n-          }\\n-        }\\n-\\n-        logger.info(`‚ñ∂ on_finish: processing for \\\"${checkName}\\\"`);\\n-\\n-        // Build context projection (pure)\\n-        const { outputsForContext, outputsHistoryForContext } = ofProject(\\n-          results,\\n-          this.getOutputHistorySnapshot()\\n-        );\\n-\\n-        // Create forEach stats\\n-        const forEachStats = {\\n-          total: forEachItems.length,\\n-          successful: forEachResult.forEachItemResults\\n-            ? forEachResult.forEachItemResults.filter(\\n-                r => r && (!r.issues || r.issues.length === 0)\\n-              ).length\\n-            : forEachItems.length,\\n-          failed: forEachResult.forEachItemResults\\n-            ? forEachResult.forEachItemResults.filter(r => r && r.issues && r.issues.length > 0)\\n-                .length\\n-            : 0,\\n-          items: forEachItems,\\n-        };\\n-\\n-        // Get memory store for context (used for diagnostics below)\\n-        const memoryStore = MemoryStore.getInstance(this.config?.memory);\\n-\\n-        // Build context for on_finish evaluation (extracted helper)\\n-        const onFinishContext = ofComposeCtx(\\n-          this.config?.memory,\\n-          checkName,\\n-          checkConfig,\\n-          outputsForContext,\\n-          outputsHistoryForContext,\\n-          forEachStats,\\n-          prInfo\\n-        );\\n-\\n-        // Diagnostics: log attempt, dependents, items, and current budget usage\\n-        try {\\n-          const ns = 'fact-validation';\\n-          const attemptNow = Number(memoryStore.get('fact_validation_attempt', ns) || 0);\\n-          const usedBudget = this.onFinishLoopCounts.get(checkName) || 0;\\n-          const maxBudget = config?.routing?.max_loops ?? 10;\\n-          logger.info(\\n-            `üß≠ on_finish: check=\\\"${checkName}\\\" items=${forEachItems.length} dependents=${dependents.length} attempt=${attemptNow} budget=${usedBudget}/${maxBudget}`\\n-          );\\n-          const vfHist = (outputsHistoryForContext['validate-fact'] as unknown[]) || [];\\n-          if (vfHist.length) {\\n-            logger.debug(`üß≠ on_finish: outputs.history['validate-fact'] length=${vfHist.length}`);\\n-          }\\n-        } catch {}\\n-\\n-        let lastRunOutput: unknown = undefined;\\n-\\n-        // Execute on_finish.run (static) first, then evaluate run_js with updated context\\n-        {\\n-          const maxLoops = config?.routing?.max_loops ?? 10;\\n-          let loopCount = 0;\\n-          const runList = Array.from(new Set([...(onFinish.run || [])].filter(Boolean)));\\n-          if (runList.length > 0)\\n-            logger.info(`‚ñ∂ on_finish.run: executing [${runList.join(', ')}] for \\\"${checkName}\\\"`);\\n-          const runCheck = async (id: string): Promise<ReviewSummary> => {\\n-            if (++loopCount > maxLoops)\\n-              throw new Error(\\n-                `Routing loop budget exceeded (max_loops=${maxLoops}) during on_finish run`\\n-              );\\n-            const childCfgFull = (config?.checks || {})[id] as\\n-              | import('./types/config').CheckConfig\\n-              | undefined;\\n-            if (!childCfgFull) throw new Error(`Unknown check in on_finish.run: ${id}`);\\n-            const childProvider = this.providerRegistry.getProviderOrThrow(\\n-              childCfgFull.type || 'ai'\\n-            );\\n-            this.setProviderWebhookContext(childProvider);\\n-            const depOverlayForChild = new Map(results);\\n-            const resChild = await this.runNamedCheck(id, [], {\\n-              origin: 'on_finish',\\n-              config: config!,\\n-              dependencyGraph,\\n-              prInfo,\\n-              resultsMap: results,\\n-              debug,\\n-              sessionInfo: (this.executionContext as any) || undefined,\\n-              overlay: depOverlayForChild,\\n-            });\\n-            try {\\n-              results.set(id, resChild as ReviewSummary);\\n-            } catch {}\\n-            return resChild as ReviewSummary;\\n-          };\\n-          try {\\n-            const o = await ofRunChildren(\\n-              runList,\\n-              runCheck,\\n-              config!,\\n-              onFinishContext,\\n-              debug || false,\\n-              log\\n-            );\\n-            lastRunOutput = o.lastRunOutput;\\n-            if (runList.length > 0) logger.info(`‚úì on_finish.run: completed for \\\"${checkName}\\\"`);\\n-          } catch (error) {\\n-            const errorMsg = error instanceof Error ? error.message : String(error);\\n-            logger.error(`‚úó on_finish.run: failed for \\\"${checkName}\\\": ${errorMsg}`);\\n-            if (error instanceof Error && error.stack) logger.debug(`Stack trace: ${error.stack}`);\\n-            throw error;\\n-          }\\n-\\n-          // Now evaluate \\n\\n... [TRUNCATED: Diff too large (296.8KB), showing first 50KB] ...\",\"status\":\"modified\"},{\"filename\":\"src/cli-main.ts\",\"additions\":5,\"deletions\":1,\"changes\":203,\"patch\":\"diff --git a/src/cli-main.ts b/src/cli-main.ts\\nindex 0e53300b..534d0945 100644\\n--- a/src/cli-main.ts\\n+++ b/src/cli-main.ts\\n@@ -5,7 +5,7 @@ import 'dotenv/config';\\n \\n import { CLI } from './cli';\\n import { ConfigManager } from './config';\\n-import { CheckExecutionEngine } from './check-execution-engine';\\n+import { StateMachineExecutionEngine } from './state-machine-execution-engine';\\n import { OutputFormatters, AnalysisResult } from './output-formatters';\\n import { CheckResult, GroupedCheckResults } from './reviewer';\\n import { PRInfo } from './pr-analyzer';\\n@@ -120,7 +120,17 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n   };\\n   const hasFlag = (name: string): boolean => argv.includes(name);\\n \\n-  const testsPath = getArg('--config');\\n+  // Support both --config flag and positional argument for tests path\\n+  let testsPath = getArg('--config');\\n+  if (!testsPath) {\\n+    // Look for first positional argument (non-flag) after the command\\n+    // argv is [node, script, 'test', ...rest]\\n+    const rest = argv.slice(3); // Skip node, script, and 'test' command\\n+    const positional = rest.find(arg => !arg.startsWith('--') && !arg.startsWith('-'));\\n+    if (positional) {\\n+      testsPath = positional;\\n+    }\\n+  }\\n   const only = getArg('--only');\\n   const bail = hasFlag('--bail');\\n   const listOnly = hasFlag('--list');\\n@@ -161,7 +171,13 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n     const runner = new (VisorTestRunner as any)();\\n     const tpath = runner.resolveTestsPath(testsPath);\\n     const suite = runner.loadSuite(tpath);\\n-    const runRes = await runner.runCases(tpath, suite, { only, bail, maxParallel, promptMaxChars });\\n+    const runRes = await runner.runCases(tpath, suite, {\\n+      only,\\n+      bail,\\n+      maxParallel,\\n+      promptMaxChars,\\n+      engineMode: 'state-machine',\\n+    });\\n     const failures = runRes.failures;\\n     // Fallback: If for any reason the runner didn't print its own summary\\n     // (e.g., natural early exit in some environments), print a concise one here.\\n@@ -289,10 +305,41 @@ export async function main(): Promise<void> {\\n   let debugServer: DebugVisualizerServer | null = null;\\n \\n   try {\\n+    // Preflight: detect obviously stale dist relative to src and warn early.\\n+    // This avoids confusing behavior when engine routing changed but dist wasn't rebuilt.\\n+    (function warnIfStaleDist() {\\n+      try {\\n+        const projectRoot = process.cwd();\\n+        const distIndex = path.join(projectRoot, 'dist', 'index.js');\\n+        const srcDir = path.join(projectRoot, 'src');\\n+        const statDist = fs.existsSync(distIndex) ? fs.statSync(distIndex) : null;\\n+        const srcNewestMtime = (function walk(dir: string): number {\\n+          let newest = 0;\\n+          if (!fs.existsSync(dir)) return 0;\\n+          for (const entry of fs.readdirSync(dir)) {\\n+            if (entry === 'debug-visualizer' || entry === 'sdk') continue;\\n+            const full = path.join(dir, entry);\\n+            const st = fs.statSync(full);\\n+            if (st.isDirectory()) newest = Math.max(newest, walk(full));\\n+            else if (/\\\\.tsx?$/.test(entry)) newest = Math.max(newest, st.mtimeMs);\\n+          }\\n+          return newest;\\n+        })(srcDir);\\n+        if (statDist && srcNewestMtime && srcNewestMtime > statDist.mtimeMs + 1) {\\n+          // Print once, concise but explicit.\\n+          console.error(\\n+            '‚ö†  Detected stale build: src/* is newer than dist/index.js. Run \\\"npm run build:cli\\\".'\\n+          );\\n+        }\\n+      } catch {\\n+        /* ignore preflight errors */\\n+      }\\n+    })();\\n+\\n     // IMPORTANT: detect subcommands before constructing CLI/commander to avoid\\n     // any argument parsing side-effects (e.g., extra positional args like 'test').\\n     // Also filter out the --cli flag if it exists (used to force CLI mode in GH Actions)\\n-    const filteredArgv = process.argv.filter(arg => arg !== '--cli');\\n+    let filteredArgv = process.argv.filter(arg => arg !== '--cli');\\n \\n     // EARLY: ensure trace dir and fallback NDJSON file exist BEFORE any early exits\\n     try {\\n@@ -328,6 +375,51 @@ export async function main(): Promise<void> {\\n       await handleTestCommand(filteredArgv);\\n       return;\\n     }\\n+    // Check for code-review subcommands: run the built-in code-review suite\\n+    // Aliases: core-review | code-review | review\\n+    if (\\n+      filteredArgv.length > 2 &&\\n+      ['core-review', 'code-review', 'review'].includes(filteredArgv[2])\\n+    ) {\\n+      const base = filteredArgv.slice(0, 2);\\n+      const rest = filteredArgv.slice(3); // preserve flags like --output, --debug, etc.\\n+      // Prefer packaged default under dist/; fall back to local defaults/ for dev\\n+      const packaged = path.resolve(__dirname, 'defaults', 'code-review.yaml');\\n+      const localDev = path.resolve(process.cwd(), 'defaults', 'code-review.yaml');\\n+      const chosen = fs.existsSync(packaged) ? packaged : localDev;\\n+      if (!fs.existsSync(chosen)) {\\n+        console.error(\\n+          '‚ùå Could not locate built-in code-review config. Expected at dist/defaults/code-review.yaml (packaged) or ./defaults/code-review.yaml (dev).'\\n+        );\\n+        process.exit(1);\\n+      }\\n+      // Let event auto-detection pick pr_updated for code-review schemas unless user overrides with --event\\n+      filteredArgv = [...base, '--config', chosen, ...rest];\\n+    }\\n+    // Check for build subcommand: run the official agent-builder config\\n+    if (filteredArgv.length > 2 && filteredArgv[2] === 'build') {\\n+      // Transform into a standard run with our official builder config (agent-builder.yaml).\\n+      // Require a positional target: `build <path/to/agent.yaml>`\\n+      const base = filteredArgv.slice(0, 2);\\n+      let rest = filteredArgv.slice(3); // preserve flags like --message\\n+      const preferred = path.resolve(process.cwd(), 'defaults', 'agent-builder.yaml');\\n+      const fallback = path.resolve(process.cwd(), 'defaults', 'agent-build.yaml');\\n+      const chosen = fs.existsSync(preferred) ? preferred : fallback;\\n+\\n+      if (rest.length === 0 || String(rest[0]).startsWith('-')) {\\n+        console.error('Usage: visor build <path/to/agent.yaml> [--message \\\"brief\\\" ...]');\\n+        process.exitCode = 1;\\n+        return;\\n+      }\\n+\\n+      const targetPath = path.resolve(process.cwd(), String(rest[0]));\\n+      process.env.VISOR_AGENT_PATH = targetPath; // builder decides mode via Liquid readfile\\n+      rest = rest.slice(1);\\n+\\n+      // Do not force code context globally; respect per-step ai.skip_code_context.\\n+      // Builder YAML controls whether to include repo context.\\n+      filteredArgv = [...base, '--config', chosen, '--event', 'manual', ...rest];\\n+    }\\n     // Construct CLI and ConfigManager only after subcommand handling\\n     const cli = new CLI();\\n     const configManager = new ConfigManager();\\n@@ -452,12 +544,12 @@ export async function main(): Promise<void> {\\n \\n     // Start debug server if requested (AFTER config is loaded)\\n     if (options.debugServer) {\\n-      const port = options.debugPort || 3456;\\n+      const requestedPort = options.debugPort || 3456;\\n \\n-      console.log(`üîç Starting debug visualizer on port ${port}...`);\\n+      console.log(`üîç Starting debug visualizer on port ${requestedPort}...`);\\n \\n       debugServer = new DebugVisualizerServer();\\n-      await debugServer.start(port);\\n+      await debugServer.start(requestedPort);\\n \\n       // Set config on server BEFORE opening browser\\n       debugServer.setConfig(config);\\n@@ -472,12 +564,13 @@ export async function main(): Promise<void> {\\n         quiet: true, // Suppress console output when debug server is active\\n       });\\n \\n-      console.log(`‚úÖ Debug visualizer running at http://localhost:${port}`);\\n+      const boundPort = debugServer.getPort();\\n+      console.log(`‚úÖ Debug visualizer running at http://localhost:${boundPort}`);\\n \\n       // Open browser unless VISOR_NOBROWSER is set (useful for CI/tests)\\n       if (process.env.VISOR_NOBROWSER !== 'true') {\\n         console.log(`   Opening browser...`);\\n-        await open(`http://localhost:${port}`);\\n+        await open(`http://localhost:${boundPort}`);\\n       }\\n \\n       console.log(`‚è∏Ô∏è  Waiting for you to click \\\"Start Execution\\\" in the browser...`);\\n@@ -557,6 +650,29 @@ export async function main(): Promise<void> {\\n     // Determine checks to run and validate check types early\\n     let checksToRun = options.checks.length > 0 ? options.checks : Object.keys(config.checks || {});\\n \\n+    // Generic: remove checks that are meant to be scheduled via routing (on_*.run)\\n+    // from the initial root set unless the user explicitly requested them.\\n+    if (options.checks.length === 0) {\\n+      const routingRunTargets = new Set<string>();\\n+      for (const [, cfg] of Object.entries(config.checks || {})) {\\n+        const onFinish: any = (cfg as any).on_finish || {};\\n+        const onSuccess: any = (cfg as any).on_success || {};\\n+        const onFail: any = (cfg as any).on_fail || {};\\n+        const collect = (arr?: string[]) => {\\n+          if (Array.isArray(arr))\\n+            for (const t of arr) if (typeof t === 'string' && t) routingRunTargets.add(t);\\n+        };\\n+        collect(onFinish.run);\\n+        collect(onSuccess.run);\\n+        collect(onFail.run);\\n+      }\\n+      const before = checksToRun.length;\\n+      checksToRun = checksToRun.filter(chk => !routingRunTargets.has(chk));\\n+      if (before !== checksToRun.length) {\\n+        logger.verbose(`Pruned ${before - checksToRun.length} routing-run target(s) from roots`);\\n+      }\\n+    }\\n+\\n     // Validate that all requested checks exist in the configuration\\n     const availableChecks = Object.keys(config.checks || {});\\n     const invalidChecks = checksToRun.filter(check => !availableChecks.includes(check));\\n@@ -570,10 +686,20 @@ export async function main(): Promise<void> {\\n     const addDependencies = (checkName: string) => {\\n       const checkConfig = config.checks?.[checkName];\\n       if (checkConfig?.depends_on) {\\n-        for (const dep of checkConfig.depends_on) {\\n-          if (!checksWithDependencies.has(dep)) {\\n-            checksWithDependencies.add(dep);\\n-            addDependencies(dep); // Recursively add dependencies of dependencies\\n+        for (const raw of checkConfig.depends_on) {\\n+          const parts =\\n+            typeof raw === 'string' && raw.includes('|')\\n+              ? raw\\n+                  .split('|')\\n+                  .map(s => s.trim())\\n+                  .filter(Boolean)\\n+              : [String(raw)];\\n+          for (const dep of parts) {\\n+            if (!availableChecks.includes(dep)) continue; // ignore OR tokens that are not real checks\\n+            if (!checksWithDependencies.has(dep)) {\\n+              checksWithDependencies.add(dep);\\n+              addDependencies(dep); // Recursively add dependencies of dependencies\\n+            }\\n           }\\n         }\\n       }\\n@@ -587,6 +713,42 @@ export async function main(): Promise<void> {\\n     // Update checksToRun to include dependencies\\n     checksToRun = Array.from(checksWithDependencies);\\n \\n+    // Prune internal dependencies from the root set so we only start from DAG sinks.\\n+    // This prevents re-running dependency steps (e.g., human-input collectors) as\\n+    // independent roots across waves. The engine will expand dependencies anyway.\\n+    const getAllDeps = (name: string, seen = new Set<string>()): Set<string> => {\\n+      if (seen.has(name)) return new Set();\\n+      seen.add(name);\\n+      const out = new Set<string>();\\n+      const cfg = config.checks?.[name];\\n+      const depTokens: any[] = cfg?.depends_on\\n+        ? Array.isArray(cfg.depends_on)\\n+          ? cfg.depends_on\\n+          : [cfg.depends_on]\\n+        : [];\\n+      const expand = (tok: any): string[] =>\\n+        typeof tok === 'string' && tok.includes('|')\\n+          ? tok\\n+              .split('|')\\n+              .map(s => s.trim())\\n+              .filter(Boolean)\\n+          : tok != null\\n+            ? [String(tok)]\\n+            : [];\\n+      for (const raw of depTokens.flatMap(expand)) {\\n+        if (!availableChecks.includes(raw)) continue;\\n+        out.add(raw);\\n+        for (const d of getAllDeps(raw, seen)) out.add(d);\\n+      }\\n+      return out;\\n+    };\\n+\\n+    const rootsPruned = checksToRun.filter(chk => {\\n+      // Keep chk only if no other selected root depends on it (directly or transitively)\\n+      return !checksToRun.some(other => other !== chk && getAllDeps(other).has(chk));\\n+    });\\n+    if (rootsPruned.length > 0) checksToRun = rootsPruned;\\n+\\n     // Use stderr for status messages when outputting formatted results to stdout\\n     // Suppress all status messages when outputting JSON to avoid breaking parsers\\n     const logFn = (msg: string) => logger.info(msg);\\n@@ -678,8 +840,8 @@ export async function main(): Promise<void> {\\n     logger.step(`Executing ${checksToRun.length} check(s)`);\\n     logger.verbose(`Checks: ${checksToRun.join(', ')}`);\\n \\n-    // Create CheckExecutionEngine for running checks\\n-    const engine = new CheckExecutionEngine();\\n+    // Create StateMachineExecutionEngine for running checks\\n+    const engine = new StateMachineExecutionEngine(undefined, undefined, debugServer || undefined);\\n \\n     // Set execution context on engine\\n     engine.setExecutionContext(executionContext);\\n@@ -921,8 +1083,17 @@ export async function main(): Promise<void> {\\n       { total: 0, critical: 0, error: 0, warning: 0, info: 0 }\\n     );\\n \\n+    // Build execution summary for display\\n+    // For user-facing readability, count distinct checks that produced a result\\n+    // (ignores forEach per-item iterations and multi-wave re-executions).\\n+    // Keep detailed per-run counts in executionStatistics for programmatic use.\\n+    const distinctExecuted = new Set(allResults.map((r: any) => r.checkName).filter(Boolean)).size;\\n+    const executionSummary = executionStatistics\\n+      ? `Checks: ${executionStatistics.totalChecksConfigured} configured ‚Üí ${distinctExecuted} executions`\\n+      : `Completed ${distinctExecuted} check(s)`;\\n+\\n     logger.success(\\n-      `Completed ${executedCheckNames.length} check(s): ${counts.total} issues (${counts.critical} critical, ${counts.error} error, ${counts.warning} warning)`\\n+      `${executionSummary}: ${counts.total} issues (${counts.critical} critical, ${counts.error} error, ${counts.warning} warning)`\\n     );\\n     logger.verbose(`Checks executed: ${executedCheckNames.join(', ')}`);\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/config.ts\",\"additions\":6,\"deletions\":1,\"changes\":228,\"patch\":\"diff --git a/src/config.ts b/src/config.ts\\nindex e9c9f84e..60aa4d79 100644\\n--- a/src/config.ts\\n+++ b/src/config.ts\\n@@ -45,6 +45,7 @@ export class ConfigManager {\\n     'claude-code',\\n     'mcp',\\n     'command',\\n+    'script',\\n     'http',\\n     'http_input',\\n     'http_client',\\n@@ -53,6 +54,7 @@ export class ConfigManager {\\n     'log',\\n     'github',\\n     'human-input',\\n+    'workflow',\\n   ];\\n   private validEventTriggers: EventTrigger[] = [...VALID_EVENT_TRIGGERS];\\n   private validOutputFormats: ConfigOutputFormat[] = ['table', 'json', 'markdown', 'sarif'];\\n@@ -98,8 +100,9 @@ export class ConfigManager {\\n         throw new Error('Configuration file must contain a valid YAML object');\\n       }\\n \\n-      // Handle extends directive if present\\n-      if (parsedConfig.extends) {\\n+      // Handle extends/include directive (include is an alias for extends)\\n+      const extendsValue = parsedConfig.extends || (parsedConfig as any).include;\\n+      if (extendsValue) {\\n         const loaderOptions: ConfigLoaderOptions = {\\n           baseDir: path.dirname(resolvedPath),\\n           allowRemote: this.isRemoteExtendsAllowed(),\\n@@ -110,12 +113,12 @@ export class ConfigManager {\\n         const loader = new ConfigLoader(loaderOptions);\\n         const merger = new ConfigMerger();\\n \\n-        // Process extends\\n-        const extends_ = Array.isArray(parsedConfig.extends)\\n-          ? parsedConfig.extends\\n-          : [parsedConfig.extends];\\n+        // Process extends/include\\n+        const extends_ = Array.isArray(extendsValue) ? extendsValue : [extendsValue];\\n+\\n+        // Remove extends and include fields from config\\n         // eslint-disable-next-line @typescript-eslint/no-unused-vars\\n-        const { extends: _extendsField, ...configWithoutExtends } = parsedConfig;\\n+        const { extends: _, include: __, ...configWithoutExtends } = parsedConfig as any;\\n \\n         // Load and merge all parent configurations\\n         let mergedConfig: Partial<VisorConfig> = {};\\n@@ -132,9 +135,18 @@ export class ConfigManager {\\n         parsedConfig = merger.removeDisabledChecks(parsedConfig);\\n       }\\n \\n+      // Check if this is a workflow definition file (has 'id' field indicating it's a workflow)\\n+      // Do this BEFORE normalizing to avoid copying workflow steps to checks\\n+      if ((parsedConfig as any).id && typeof (parsedConfig as any).id === 'string') {\\n+        parsedConfig = await this.convertWorkflowToConfig(parsedConfig, path.dirname(resolvedPath));\\n+      }\\n+\\n       // Normalize 'checks' and 'steps' - support both keys for backward compatibility\\n       parsedConfig = this.normalizeStepsAndChecks(parsedConfig);\\n \\n+      // Load workflows if defined\\n+      await this.loadWorkflows(parsedConfig, path.dirname(resolvedPath));\\n+\\n       if (validate) {\\n         this.validateConfig(parsedConfig);\\n       }\\n@@ -298,17 +310,43 @@ export class ConfigManager {\\n       if (bundledConfigPath) {\\n         // Always log to stderr to avoid contaminating formatted output\\n         console.error(`üì¶ Loading bundled default configuration from ${bundledConfigPath}`);\\n-        const configContent = fs.readFileSync(bundledConfigPath, 'utf8');\\n-        let parsedConfig = yaml.load(configContent) as Partial<VisorConfig>;\\n-\\n-        if (!parsedConfig || typeof parsedConfig !== 'object') {\\n-          return null;\\n-        }\\n+        // Synchronous loader for bundled defaults with shallow extends support\\n+        const readAndParse = (p: string): Partial<VisorConfig> => {\\n+          const raw = fs.readFileSync(p, 'utf8');\\n+          const obj = yaml.load(raw) as Partial<VisorConfig>;\\n+          if (!obj || typeof obj !== 'object') return {};\\n+          // Alias: support 'include' as 'extends' in packaged defaults\\n+          if ((obj as any).include && !(obj as any).extends) {\\n+            const inc = (obj as any).include;\\n+            (obj as any).extends = Array.isArray(inc) ? inc : [inc];\\n+            delete (obj as any).include;\\n+          }\\n+          return obj;\\n+        };\\n+        const baseDir = path.dirname(bundledConfigPath);\\n+        const merger = new (require('./utils/config-merger').ConfigMerger)();\\n+\\n+        const loadWithExtendsSync = (p: string): Partial<VisorConfig> => {\\n+          const current = readAndParse(p);\\n+          const extVal: any = (current as any).extends || (current as any).include;\\n+          // Strip extends/include to prevent re-processing\\n+          if ((current as any).extends !== undefined) delete (current as any).extends;\\n+          if ((current as any).include !== undefined) delete (current as any).include;\\n+          if (!extVal) return current;\\n+          const list = Array.isArray(extVal) ? extVal : [extVal];\\n+          let acc: Partial<VisorConfig> = {};\\n+          for (const src of list) {\\n+            const rel = typeof src === 'string' ? src : String(src);\\n+            const abs = path.isAbsolute(rel) ? rel : path.resolve(baseDir, rel);\\n+            const parentCfg = loadWithExtendsSync(abs);\\n+            acc = merger.merge(acc, parentCfg);\\n+          }\\n+          return merger.merge(acc, current);\\n+        };\\n \\n-        // Normalize 'checks' and 'steps' for backward compatibility\\n+        let parsedConfig = loadWithExtendsSync(bundledConfigPath);\\n+        // Normalize and validate as usual\\n         parsedConfig = this.normalizeStepsAndChecks(parsedConfig);\\n-\\n-        // Validate and merge with defaults\\n         this.validateConfig(parsedConfig);\\n         return this.mergeWithDefaults(parsedConfig) as VisorConfig;\\n       }\\n@@ -349,15 +387,87 @@ export class ConfigManager {\\n     return null;\\n   }\\n \\n+  /**\\n+   * Convert a workflow definition file to a visor config\\n+   * When a workflow YAML is run standalone, register the workflow and use its tests as checks\\n+   */\\n+  private async convertWorkflowToConfig(\\n+    workflowData: any,\\n+    _basePath: string\\n+  ): Promise<Partial<VisorConfig>> {\\n+    const { WorkflowRegistry } = await import('./workflow-registry');\\n+    const registry = WorkflowRegistry.getInstance();\\n+\\n+    // Register the workflow\\n+    const workflowId = workflowData.id;\\n+    logger.info(`Detected standalone workflow file: ${workflowId}`);\\n+\\n+    // Extract tests before modifying workflowData\\n+    const tests = workflowData.tests || {};\\n+\\n+    // Create a clean workflow definition (without tests)\\n+    const workflowDefinition = { ...workflowData };\\n+    delete workflowDefinition.tests;\\n+\\n+    // Register the workflow itself\\n+    const result = registry.register(workflowDefinition, 'standalone', { override: true });\\n+    if (!result.valid && result.errors) {\\n+      const errors = result.errors.map(e => `  ${e.path}: ${e.message}`).join('\\\\n');\\n+      throw new Error(`Failed to register workflow '${workflowId}':\\\\n${errors}`);\\n+    }\\n+\\n+    logger.info(`Registered workflow '${workflowId}' for standalone execution`);\\n+\\n+    // Create a COMPLETELY NEW visor config with ONLY the test checks\\n+    // This prevents any workflow fields from leaking into the config\\n+    const visorConfig: Partial<VisorConfig> = {\\n+      version: '1.0',\\n+      steps: tests,\\n+      checks: tests, // Backward compatibility\\n+    };\\n+\\n+    logger.debug(`Standalone workflow config has ${Object.keys(tests).length} test checks`);\\n+    logger.debug(`Test check names: ${Object.keys(tests).join(', ')}`);\\n+    logger.debug(`Config keys after conversion: ${Object.keys(visorConfig).join(', ')}`);\\n+\\n+    return visorConfig;\\n+  }\\n+\\n+  /**\\n+   * Load and register workflows from configuration\\n+   */\\n+  private async loadWorkflows(config: Partial<VisorConfig>, basePath: string): Promise<void> {\\n+    // Only import workflows from external files\\n+    if (!config.imports || config.imports.length === 0) {\\n+      return;\\n+    }\\n+\\n+    const { WorkflowRegistry } = await import('./workflow-registry');\\n+    const registry = WorkflowRegistry.getInstance();\\n+\\n+    // Import workflow files\\n+    for (const source of config.imports) {\\n+      const results = await registry.import(source, { basePath, validate: true });\\n+      for (const result of results) {\\n+        if (!result.valid && result.errors) {\\n+          const errors = result.errors.map(e => `  ${e.path}: ${e.message}`).join('\\\\n');\\n+          throw new Error(`Failed to import workflow from '${source}':\\\\n${errors}`);\\n+        }\\n+      }\\n+      logger.info(`Imported workflows from: ${source}`);\\n+    }\\n+  }\\n+\\n   /**\\n    * Normalize 'checks' and 'steps' keys for backward compatibility\\n    * Ensures both keys are present and contain the same data\\n    */\\n   private normalizeStepsAndChecks(config: Partial<VisorConfig>): Partial<VisorConfig> {\\n-    // If both are present, 'steps' takes precedence\\n+    // If both are present, merge with 'steps' taking precedence on key conflicts\\n     if (config.steps && config.checks) {\\n-      // Use steps as the source of truth\\n-      config.checks = config.steps;\\n+      const merged = { ...(config.checks as Record<string, any>), ...(config.steps as any) };\\n+      config.checks = merged;\\n+      config.steps = merged;\\n     } else if (config.steps && !config.checks) {\\n       // Copy steps to checks for internal compatibility\\n       config.checks = config.steps;\\n@@ -468,7 +578,7 @@ export class ConfigManager {\\n           checkConfig.type = 'ai';\\n         }\\n         // 'on' field is optional - if not specified, check can run on any event\\n-        this.validateCheckConfig(checkName, checkConfig, errors, config);\\n+        this.validateCheckConfig(checkName, checkConfig, errors, config, warnings);\\n \\n         // Unknown/typo keys at the check level are produced by Ajv.\\n \\n@@ -616,7 +726,8 @@ export class ConfigManager {\\n     checkName: string,\\n     checkConfig: CheckConfig,\\n     errors: ConfigValidationError[],\\n-    config?: Partial<VisorConfig>\\n+    config?: Partial<VisorConfig>,\\n+    _warnings?: ConfigValidationError[]\\n   ): void {\\n     // Default to 'ai' if no type specified\\n     if (!checkConfig.type) {\\n@@ -643,6 +754,66 @@ export class ConfigManager {\\n       });\\n     }\\n \\n+    // Require specifying criticality for external-effect providers\\n+    try {\\n+      const externalTypes = new Set(['github', 'http', 'http_client', 'http_input', 'workflow']);\\n+      if (externalTypes.has(checkConfig.type as string) && !checkConfig.criticality) {\\n+        errors.push({\\n+          field: `checks.${checkName}.criticality`,\\n+          message: `Missing required criticality for step \\\"${checkName}\\\" (type: ${checkConfig.type}). Set criticality: 'external' or 'internal' to enable safe defaults for side-effecting steps.`,\\n+        });\\n+      }\\n+    } catch {\\n+      // best-effort hint only\\n+    }\\n+\\n+    // Criticality-driven contract requirements (lint-time)\\n+    // For higher-safety modes, require a pre-run guard (assume or if) and,\\n+    // for output-producing providers, a post-run contract (schema or guarantee).\\n+    try {\\n+      const crit = (checkConfig.criticality || 'policy') as string;\\n+      const isCritical = crit === 'external' || crit === 'internal';\\n+\\n+      if (isCritical) {\\n+        // 1) Pre-run guard: either assume (string or non-empty array) or if\\n+        const hasAssume =\\n+          typeof (checkConfig as any).assume === 'string' ||\\n+          (Array.isArray((checkConfig as any).assume) && (checkConfig as any).assume.length > 0);\\n+        const hasIf =\\n+          typeof (checkConfig as any).if === 'string' && (checkConfig as any).if.trim().length > 0;\\n+        if (!hasAssume && !hasIf) {\\n+          errors.push({\\n+            field: `checks.${checkName}.assume`,\\n+            message: `Critical step \\\"${checkName}\\\" (criticality: ${crit}) requires a precondition: set 'assume:' (preferred) or 'if:' to guard execution.`,\\n+          });\\n+        }\\n+\\n+        // 2) Post-run contract for output-producing providers\\n+        const outputProviders = new Set([\\n+          'ai',\\n+          'script',\\n+          'command',\\n+          'http',\\n+          'http_client',\\n+          'http_input',\\n+        ]);\\n+        if (outputProviders.has(checkConfig.type as string)) {\\n+          const hasSchema = typeof (checkConfig as any).schema !== 'undefined';\\n+          const hasGuarantee =\\n+            typeof (checkConfig as any).guarantee === 'string' &&\\n+            (checkConfig as any).guarantee.trim().length > 0;\\n+          if (!hasSchema && !hasGuarantee) {\\n+            errors.push({\\n+              field: `checks.${checkName}.schema/guarantee`,\\n+              message: `Critical step \\\"${checkName}\\\" (type: ${checkConfig.type}) requires an output contract: provide 'schema:' (renderer name or JSON Schema) or 'guarantee:' expression.`,\\n+            });\\n+          }\\n+        }\\n+      }\\n+    } catch {\\n+      // lint-only; never throw from here\\n+    }\\n+\\n     // Command checks require exec field\\n     if (checkConfig.type === 'command' && !checkConfig.exec) {\\n       errors.push({\\n@@ -678,6 +849,21 @@ export class ConfigManager {\\n       });\\n     }\\n \\n+    // Prefer single-field schema: if both schema (object) and output_schema are present, warn\\n+    try {\\n+      const hasObjSchema =\\n+        (checkConfig as any)?.schema && typeof (checkConfig as any).schema === 'object';\\n+      const hasOutputSchema =\\n+        (checkConfig as any)?.output_schema &&\\n+        typeof (checkConfig as any).output_schema === 'object';\\n+      if (hasObjSchema && hasOutputSchema) {\\n+        (_warnings || errors).push({\\n+          field: `checks.${checkName}.schema`,\\n+          message: `Both 'schema' (object) and 'output_schema' are set; 'schema' will be used for validation. 'output_schema' is deprecated.`,\\n+        });\\n+      }\\n+    } catch {}\\n+\\n     // HTTP client checks require url field\\n     if (checkConfig.type === 'http_client' && !checkConfig.url) {\\n       errors.push({\\n\",\"status\":\"modified\"},{\"filename\":\"src/cron-scheduler.ts\",\"additions\":1,\"deletions\":1,\"changes\":8,\"patch\":\"diff --git a/src/cron-scheduler.ts b/src/cron-scheduler.ts\\nindex c24bb0e5..bb38b412 100644\\n--- a/src/cron-scheduler.ts\\n+++ b/src/cron-scheduler.ts\\n@@ -1,6 +1,6 @@\\n import * as cron from 'node-cron';\\n import { VisorConfig, CheckConfig } from './types/config';\\n-import { CheckExecutionEngine } from './check-execution-engine';\\n+import { StateMachineExecutionEngine } from './state-machine-execution-engine';\\n \\n export interface ScheduledCheck {\\n   checkName: string;\\n@@ -14,11 +14,11 @@ export interface ScheduledCheck {\\n  */\\n export class CronScheduler {\\n   private scheduledChecks: Map<string, ScheduledCheck> = new Map();\\n-  private executionEngine: CheckExecutionEngine;\\n+  private executionEngine: StateMachineExecutionEngine;\\n   private config: VisorConfig;\\n   private isRunning = false;\\n \\n-  constructor(config: VisorConfig, executionEngine: CheckExecutionEngine) {\\n+  constructor(config: VisorConfig, executionEngine: StateMachineExecutionEngine) {\\n     this.config = config;\\n     this.executionEngine = executionEngine;\\n   }\\n@@ -228,7 +228,7 @@ export class CronScheduler {\\n  */\\n export function createCronScheduler(\\n   config: VisorConfig,\\n-  executionEngine: CheckExecutionEngine\\n+  executionEngine: StateMachineExecutionEngine\\n ): CronScheduler {\\n   const scheduler = new CronScheduler(config, executionEngine);\\n   scheduler.initialize();\\n\",\"status\":\"modified\"},{\"filename\":\"src/debug-visualizer/ws-server.ts\",\"additions\":1,\"deletions\":1,\"changes\":65,\"patch\":\"diff --git a/src/debug-visualizer/ws-server.ts b/src/debug-visualizer/ws-server.ts\\nindex 454dfb7a..3e2940e8 100644\\n--- a/src/debug-visualizer/ws-server.ts\\n+++ b/src/debug-visualizer/ws-server.ts\\n@@ -50,25 +50,58 @@ export class DebugVisualizerServer {\\n    * Start the HTTP server\\n    */\\n   async start(port: number = 3456): Promise<void> {\\n-    this.port = port;\\n-\\n-    // Create HTTP server to serve UI and API endpoints\\n-    this.httpServer = http.createServer((req, res) => {\\n-      this.handleHttpRequest(req, res);\\n-    });\\n-\\n-    // Start HTTP server\\n-    await new Promise<void>((resolve, reject) => {\\n-      this.httpServer!.listen(port, () => {\\n-        this.isRunning = true;\\n-        console.log(`[debug-server] Debug Visualizer running at http://localhost:${port}`);\\n-        resolve();\\n+    // Try the requested port first; if it's busy, fall back to an ephemeral port.\\n+    const maxAttempts = 10;\\n+    let attempt = 0;\\n+    let lastError: unknown = null;\\n+\\n+    while (attempt < maxAttempts) {\\n+      const tryPort = attempt === 0 ? port : 0; // 0 ‚Üí random free port\\n+      this.port = tryPort || this.port;\\n+\\n+      // Create a fresh HTTP server for each attempt\\n+      this.httpServer = http.createServer((req, res) => {\\n+        this.handleHttpRequest(req, res);\\n       });\\n \\n-      this.httpServer!.on('error', error => {\\n-        reject(error);\\n+      const success = await new Promise<boolean>(resolve => {\\n+        const onListening = () => {\\n+          try {\\n+            const addr = this.httpServer!.address();\\n+            if (addr && typeof addr !== 'string') this.port = addr.port;\\n+          } catch {}\\n+          this.isRunning = true;\\n+          console.log(`[debug-server] Debug Visualizer running at http://localhost:${this.port}`);\\n+          resolve(true);\\n+        };\\n+        const onError = (error: any) => {\\n+          lastError = error;\\n+          // Retry only on EADDRINUSE; otherwise stop trying\\n+          if (error && (error.code === 'EADDRINUSE' || error.code === 'EACCES')) {\\n+            console.warn(\\n+              `‚ùå Error: ${error.code} on port ${tryPort}. Retrying with a free port...`\\n+            );\\n+            try {\\n+              this.httpServer?.removeListener('listening', onListening);\\n+              this.httpServer?.removeListener('error', onError);\\n+              this.httpServer?.close();\\n+            } catch {}\\n+            resolve(false);\\n+            return;\\n+          }\\n+          resolve(false);\\n+        };\\n+        this.httpServer!.once('listening', onListening);\\n+        this.httpServer!.once('error', onError);\\n+        this.httpServer!.listen(tryPort);\\n       });\\n-    });\\n+\\n+      if (success) return;\\n+      attempt++;\\n+    }\\n+\\n+    // All attempts failed\\n+    throw lastError instanceof Error ? lastError : new Error('Failed to start debug server');\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/engine/on-finish/orchestrator.ts\",\"additions\":1,\"deletions\":1,\"changes\":44,\"patch\":\"diff --git a/src/engine/on-finish/orchestrator.ts b/src/engine/on-finish/orchestrator.ts\\nindex 38952049..6397e506 100644\\n--- a/src/engine/on-finish/orchestrator.ts\\n+++ b/src/engine/on-finish/orchestrator.ts\\n@@ -70,7 +70,49 @@ export function decideRouting(\\n     prInfo\\n   );\\n   const onFinish = checkConfig.on_finish!;\\n-  const gotoTarget = evaluateOnFinishGoto(onFinish, ctx, debug, log);\\n+  let gotoTarget = evaluateOnFinishGoto(onFinish, ctx, debug, log);\\n+  // Gentle, config-informed fallback: If goto_js returned null but the\\n+  // configuration declares a finite retry budget (via a literal\\n+  // `const maxWaves = 1 + <N>` style), and last wave is not all-valid,\\n+  // suggest routing back to the parent exactly once per remaining budget.\\n+  if (!gotoTarget) {\\n+    try {\\n+      const js = String(onFinish.goto_js || '');\\n+      // Extract N from \\\"const maxWaves = 1 + N\\\" or \\\"maxWaves=1+N\\\" (common pattern in our configs/tests)\\n+      let n = NaN;\\n+      {\\n+        const m = js.match(/maxWaves\\\\s*=\\\\s*1\\\\s*\\\\+\\\\s*(\\\\d+)/);\\n+        if (m) n = Number(m[1]);\\n+      }\\n+      if (!Number.isFinite(n)) {\\n+        // Generic fallback: find any literal \\\"1 + <number>\\\"; take the last occurrence\\n+        const all = Array.from(js.matchAll(/1\\\\s*\\\\+\\\\s*(\\\\d+)/g));\\n+        if (all.length > 0) {\\n+          const last = all[all.length - 1];\\n+          const num = Number(last[1]);\\n+          if (Number.isFinite(num)) n = num;\\n+        }\\n+      }\\n+      const items = (ctx.forEach && (ctx.forEach as any).last_wave_size) || 0;\\n+      const vf = Array.isArray((ctx.outputs as any).history?.['validate-fact'])\\n+        ? ((ctx.outputs as any).history['validate-fact'] as unknown[]).filter(\\n+            (x: unknown) => !Array.isArray(x)\\n+          )\\n+        : [];\\n+      const waves = items > 0 ? Math.floor(vf.length / items) : 0;\\n+      const last = items > 0 ? vf.slice(-items) : [];\\n+      const allOk =\\n+        last.length === items &&\\n+        last.every((v: any) => v && (v.is_valid === true || v.valid === true));\\n+      if (!gotoTarget && !allOk && Number.isFinite(n) && n > 0 && waves < 1 + n) {\\n+        gotoTarget = checkName;\\n+        if (debug)\\n+          log(\\n+            `üîß Debug: decideRouting fallback ‚Üí '${checkName}' (waves=${waves} < maxWaves=${1 + n})`\\n+          );\\n+      }\\n+    } catch {}\\n+  }\\n   return { gotoTarget };\\n }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/engine/on-finish/utils.ts\",\"additions\":5,\"deletions\":2,\"changes\":221,\"patch\":\"diff --git a/src/engine/on-finish/utils.ts b/src/engine/on-finish/utils.ts\\nindex 074c166c..d1d1a628 100644\\n--- a/src/engine/on-finish/utils.ts\\n+++ b/src/engine/on-finish/utils.ts\\n@@ -1,9 +1,9 @@\\n-import { MemoryStore } from '../../memory-store';\\n import { createSecureSandbox } from '../../utils/sandbox';\\n import type { PRInfo } from '../../pr-analyzer';\\n import type { ReviewSummary } from '../../reviewer';\\n import type { VisorConfig, CheckConfig, OnFinishConfig } from '../../types/config';\\n import { buildSandboxEnv } from '../../utils/env-exposure';\\n+import { MemoryStore } from '../../memory-store';\\n \\n export function buildProjectionFrom(\\n   results: Map<string, ReviewSummary>,\\n@@ -24,48 +24,76 @@ export function buildProjectionFrom(\\n   return { outputsForContext, outputsHistoryForContext };\\n }\\n \\n+export interface OnFinishContext {\\n+  step: { id: string; tags: string[]; group?: string };\\n+  attempt: number;\\n+  loop: number;\\n+  outputs: Record<string, unknown>;\\n+  outputs_history: Record<string, unknown[]>;\\n+  outputs_raw: Record<string, unknown>;\\n+  forEach: unknown;\\n+  memory: {\\n+    get: (key: string, ns?: string) => unknown;\\n+    has: (key: string, ns?: string) => boolean;\\n+    getAll: (ns?: string) => Record<string, unknown>;\\n+    set: (key: string, value: unknown, ns?: string) => void;\\n+    clear: (ns?: string) => void;\\n+    increment: (key: string, amount?: number, ns?: string) => number;\\n+  };\\n+  pr: { number: number; title?: string; author?: string; branch?: string; base?: string };\\n+  files?: unknown;\\n+  env: Record<string, string | undefined>;\\n+  event: { name: string };\\n+}\\n+\\n export function composeOnFinishContext(\\n-  memoryConfig: VisorConfig['memory'] | undefined,\\n+  _memoryConfig: VisorConfig['memory'] | undefined,\\n   checkName: string,\\n   checkConfig: CheckConfig,\\n   outputsForContext: Record<string, unknown>,\\n   outputsHistoryForContext: Record<string, unknown[]>,\\n   forEachStats: any,\\n   prInfo: PRInfo\\n-) {\\n-  const memoryStore = MemoryStore.getInstance(memoryConfig);\\n-  const memory = {\\n+): OnFinishContext {\\n+  // No MemoryStore in on_finish context ‚Äî outputs and outputs_history are sufficient\\n+  const outputs_raw: Record<string, unknown> = {};\\n+  for (const [name, val] of Object.entries(outputsForContext))\\n+    if (name !== 'history') outputs_raw[name] = val;\\n+  const outputsMerged = { ...outputsForContext, history: outputsHistoryForContext } as Record<\\n+    string,\\n+    unknown\\n+  >;\\n+  // Memory helpers backed by MemoryStore, but exposed synchronously for\\n+  // sandboxed goto_js/on_success.run_js compatibility.\\n+  const memoryStore = MemoryStore.getInstance();\\n+  const memoryHelpers = {\\n     get: (key: string, ns?: string) => memoryStore.get(key, ns),\\n     has: (key: string, ns?: string) => memoryStore.has(key, ns),\\n-    list: (ns?: string) => memoryStore.list(ns),\\n-    getAll: (ns?: string) => {\\n-      const keys = memoryStore.list(ns);\\n-      const result: Record<string, unknown> = {};\\n-      for (const key of keys) result[key] = memoryStore.get(key, ns);\\n-      return result;\\n-    },\\n+    getAll: (ns?: string) => memoryStore.getAll(ns),\\n     set: (key: string, value: unknown, ns?: string) => {\\n       const nsName = ns || memoryStore.getDefaultNamespace();\\n-      if (!memoryStore['data'].has(nsName)) memoryStore['data'].set(nsName, new Map());\\n-      memoryStore['data'].get(nsName)!.set(key, value);\\n+      const data: Map<string, Map<string, unknown>> = (memoryStore as any)['data'];\\n+      if (!data.has(nsName)) data.set(nsName, new Map());\\n+      data.get(nsName)!.set(key, value);\\n     },\\n-    increment: (key: string, amount: number, ns?: string) => {\\n-      const current = memoryStore.get(key, ns);\\n+    clear: (ns?: string) => {\\n+      const data: Map<string, Map<string, unknown>> = (memoryStore as any)['data'];\\n+      if (ns) data.delete(ns);\\n+      else data.clear();\\n+    },\\n+    increment: (key: string, amount = 1, ns?: string) => {\\n+      const nsName = ns || memoryStore.getDefaultNamespace();\\n+      const data: Map<string, Map<string, unknown>> = (memoryStore as any)['data'];\\n+      if (!data.has(nsName)) data.set(nsName, new Map());\\n+      const nsMap = data.get(nsName)!;\\n+      const current = nsMap.get(key);\\n       const numCurrent = typeof current === 'number' ? current : 0;\\n       const newValue = numCurrent + amount;\\n-      const nsName = ns || memoryStore.getDefaultNamespace();\\n-      if (!memoryStore['data'].has(nsName)) memoryStore['data'].set(nsName, new Map());\\n-      memoryStore['data'].get(nsName)!.set(key, newValue);\\n+      nsMap.set(key, newValue);\\n       return newValue;\\n     },\\n   };\\n-  const outputs_raw: Record<string, unknown> = {};\\n-  for (const [name, val] of Object.entries(outputsForContext))\\n-    if (name !== 'history') outputs_raw[name] = val;\\n-  const outputsMerged = { ...outputsForContext, history: outputsHistoryForContext } as Record<\\n-    string,\\n-    unknown\\n-  >;\\n+\\n   return {\\n     step: { id: checkName, tags: checkConfig.tags || [], group: checkConfig.group },\\n     attempt: 1,\\n@@ -74,7 +102,7 @@ export function composeOnFinishContext(\\n     outputs_history: outputsHistoryForContext,\\n     outputs_raw,\\n     forEach: forEachStats,\\n-    memory,\\n+    memory: memoryHelpers,\\n     pr: {\\n       number: prInfo.number,\\n       title: prInfo.title,\\n@@ -102,14 +130,43 @@ export function evaluateOnFinishGoto(\\n       const code = `\\n         const step = scope.step; const attempt = scope.attempt; const loop = scope.loop; const outputs = scope.outputs; const outputs_history = scope.outputs_history; const outputs_raw = scope.outputs_raw; const forEach = scope.forEach; const memory = scope.memory; const pr = scope.pr; const files = scope.files; const env = scope.env; const event = scope.event; const log = (...a)=> console.log('üîç Debug:',...a);\\n         const __fn = () => {\\\\n${onFinish.goto_js}\\\\n};\\n-        const __res = __fn();\\n-        return (typeof __res === 'string' && __res) ? __res : null;\\n+        return __fn();\\n       `;\\n-      const exec = sandbox.compile(code);\\n-      const result = exec({ scope }).run();\\n+      // Use shared compileAndRun helper for consistent behavior\\n+      const { compileAndRun } = require('../../utils/sandbox');\\n+      const result = compileAndRun(\\n+        sandbox,\\n+        code,\\n+        { scope },\\n+        { injectLog: false, wrapFunction: false }\\n+      );\\n+      try {\\n+        if (debug) {\\n+          const hist =\\n+            (onFinishContext &&\\n+              onFinishContext.outputs &&\\n+              (onFinishContext.outputs as any).history) ||\\n+            {};\\n+          const vf = Array.isArray(hist['validate-fact'])\\n+            ? hist['validate-fact'].filter((x: any) => !Array.isArray(x))\\n+            : [];\\n+          const items =\\n+            (onFinishContext &&\\n+              onFinishContext.forEach &&\\n+              (onFinishContext.forEach as any).last_wave_size) ||\\n+            0;\\n+          log(`üîß Debug: goto_js result=${String(result)} items=${items} vf_count=${vf.length}`);\\n+        }\\n+      } catch {}\\n       gotoTarget = typeof result === 'string' && result ? result : null;\\n       if (debug) log(`üîß Debug: on_finish.goto_js evaluated ‚Üí ${String(gotoTarget)}`);\\n-    } catch {\\n+    } catch (e) {\\n+      try {\\n+        // Surface evaluation problems in debug logs to aid diagnosis\\n+        const msg = e instanceof Error ? e.message : String(e);\\n+\\n+        console.error(`‚úó on_finish.goto_js: evaluation error: ${msg}`);\\n+      } catch {}\\n       // Fall back to static goto\\n       if (onFinish.goto) gotoTarget = onFinish.goto;\\n     }\\n@@ -123,10 +180,98 @@ export function recomputeAllValidFromHistory(\\n   history: Record<string, unknown[]>,\\n   forEachItemsCount: number\\n ): boolean | undefined {\\n-  const vfNow = (history['validate-fact'] || []) as unknown[];\\n-  if (!Array.isArray(vfNow) || forEachItemsCount <= 0 || vfNow.length < forEachItemsCount)\\n-    return undefined;\\n-  const lastWave = vfNow.slice(-forEachItemsCount);\\n-  const ok = lastWave.every((v: any) => v && (v.is_valid === true || v.valid === true));\\n-  return ok;\\n+  const vfArrRaw = Array.isArray(history['validate-fact'])\\n+    ? (history['validate-fact'] as unknown[])\\n+    : [];\\n+  if (forEachItemsCount <= 0) return undefined;\\n+\\n+  // Consider only non-array entries (per-item results)\\n+  const vfArr = vfArrRaw.filter(v => !Array.isArray(v)) as any[];\\n+  if (vfArr.length < forEachItemsCount) return false;\\n+\\n+  // 1) Prefer strict last-wave grouping when loop_idx metadata is present.\\n+  const withLoop = vfArr.filter(\\n+    v => v && typeof v === 'object' && Number.isFinite((v as any).loop_idx)\\n+  ) as Array<{ loop_idx: number } & Record<string, unknown>>;\\n+  if (withLoop.length >= forEachItemsCount) {\\n+    const maxLoop = Math.max(...withLoop.map(v => Number(v.loop_idx)));\\n+    const sameWave = withLoop.filter(v => Number(v.loop_idx) === maxLoop);\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        console.error(\\n+          `[ofAllValid] loop_idx=${maxLoop} sameWave=${sameWave.length} items=${forEachItemsCount}`\\n+        );\\n+      }\\n+    } catch {}\\n+    if (sameWave.length >= forEachItemsCount) {\\n+      // If we have ids, take the last N distinct by id; otherwise, take last N\\n+      const take = (() => {\\n+        const withIds = sameWave.filter(\\n+          o => typeof (o as any).fact_id === 'string' || typeof (o as any).id === 'string'\\n+        );\\n+        if (withIds.length >= forEachItemsCount) {\\n+          const recent: any[] = [];\\n+          const seen = new Set<string>();\\n+          for (let i = sameWave.length - 1; i >= 0 && recent.length < forEachItemsCount; i--) {\\n+            const o: any = sameWave[i];\\n+            const key = (o.fact_id || o.id) as string | undefined;\\n+            if (!key || seen.has(key)) continue;\\n+            seen.add(key);\\n+            recent.push(o);\\n+          }\\n+          if (recent.length === forEachItemsCount) return recent;\\n+        }\\n+        return sameWave.slice(-forEachItemsCount);\\n+      })();\\n+      const ok = take.every(o => o && ((o as any).is_valid === true || (o as any).valid === true));\\n+      try {\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          const vals = take.map(o => (o as any).is_valid ?? (o as any).valid);\\n+          console.error(`[ofAllValid] loop verdicts=${JSON.stringify(vals)} ok=${ok}`);\\n+        }\\n+      } catch {}\\n+      return ok;\\n+    }\\n+  }\\n+\\n+  // 2) Fall back to last N distinct-by-id across the whole history\\n+  const withIds = vfArr.filter(\\n+    o => typeof (o as any).fact_id === 'string' || typeof (o as any).id === 'string'\\n+  );\\n+  if (withIds.length >= forEachItemsCount) {\\n+    const recent: any[] = [];\\n+    const seen = new Set<string>();\\n+    for (let i = vfArr.length - 1; i >= 0 && recent.length < forEachItemsCount; i--) {\\n+      const o: any = vfArr[i];\\n+      const key = (o.fact_id || o.id) as string | undefined;\\n+      if (!key || seen.has(key)) continue;\\n+      seen.add(key);\\n+      recent.push(o);\\n+    }\\n+    if (recent.length === forEachItemsCount) {\\n+      const ok = recent.every(o => o && (o.is_valid === true || o.valid === true));\\n+      try {\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          const vals = recent.map(o => (o as any).is_valid ?? (o as any).valid);\\n+          console.error(`[ofAllValid] id-recent verdicts=${JSON.stringify(vals)} ok=${ok}`);\\n+        }\\n+      } catch {}\\n+      return ok;\\n+    }\\n+  }\\n+\\n+  // 3) Last-resort fallback: treat last N entries as current wave\\n+  if (vfArr.length >= forEachItemsCount) {\\n+    const lastN = vfArr.slice(-forEachItemsCount) as any[];\\n+    const ok = lastN.every(o => o && (o.is_valid === true || o.valid === true));\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        const vals = lastN.map(o => (o as any).is_valid ?? (o as any).valid);\\n+        console.error(`[ofAllValid] tail verdicts=${JSON.stringify(vals)} ok=${ok}`);\\n+      }\\n+    } catch {}\\n+    return ok;\\n+  }\\n+\\n+  return false;\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/failure-condition-evaluator.ts\",\"additions\":2,\"deletions\":1,\"changes\":101,\"patch\":\"diff --git a/src/failure-condition-evaluator.ts b/src/failure-condition-evaluator.ts\\nindex f1377257..24e13d10 100644\\n--- a/src/failure-condition-evaluator.ts\\n+++ b/src/failure-condition-evaluator.ts\\n@@ -136,6 +136,7 @@ export class FailureConditionEvaluator {\\n       environment?: Record<string, string>;\\n       previousResults?: Map<string, ReviewSummary>;\\n       authorAssociation?: string;\\n+      workflowInputs?: Record<string, unknown>;\\n     }\\n   ): Promise<boolean> {\\n     // Build context for if evaluation\\n@@ -173,6 +174,9 @@ export class FailureConditionEvaluator {\\n           })()\\n         : {},\\n \\n+      // Workflow inputs (for workflows)\\n+      inputs: contextData?.workflowInputs || {},\\n+\\n       // Required output property (empty for if conditions)\\n       output: {\\n         issues: [],\\n@@ -261,15 +265,6 @@ export class FailureConditionEvaluator {\\n       results.push(...filteredResults, ...checkResults);\\n     }\\n \\n-    try {\\n-      if (checkName === 'B') {\\n-        console.error(\\n-          `üîß Debug: fail_if results for ${checkName}: ${JSON.stringify(results)} context.output=${JSON.stringify(\\n-            context.output\\n-          )}`\\n-        );\\n-      }\\n-    } catch {}\\n     return results;\\n   }\\n \\n@@ -495,6 +490,9 @@ export class FailureConditionEvaluator {\\n         hasChanges: context.hasChanges || false,\\n       };\\n \\n+      // Do not mutate output shape here. Output contracts are defined by providers and\\n+      // workflow outputs. Tests and expressions should rely on those schemas directly.\\n+\\n       // Legacy variables for backward compatibility\\n       const criticalIssues = metadata.criticalIssues;\\n       const errorIssues = metadata.errorIssues;\\n@@ -578,16 +576,82 @@ export class FailureConditionEvaluator {\\n       if (!this.sandbox) {\\n         this.sandbox = this.createSecureSandbox();\\n       }\\n-      let exec: ReturnType<typeof this.sandbox.compile>;\\n+      let result: any;\\n+      // Primary path: SandboxJS\\n       try {\\n-        // Try compiling the raw expression as-is first (supports multi-line logical expressions)\\n-        exec = this.sandbox.compile(`return (${raw});`);\\n-      } catch {\\n-        // Fallback: normalize multi-line statements into a comma-chain expression\\n-        const normalizedExpr = normalize(condition);\\n-        exec = this.sandbox.compile(`return (${normalizedExpr});`);\\n+        let exec: ReturnType<typeof this.sandbox.compile>;\\n+        try {\\n+          exec = this.sandbox.compile(`return (${raw});`);\\n+        } catch {\\n+          const normalizedExpr = normalize(condition);\\n+          exec = this.sandbox.compile(`return (${normalizedExpr});`);\\n+        }\\n+        result = exec(scope).run();\\n+      } catch (_primaryErr) {\\n+        // Fallback path: Node VM for modern syntax (optional chaining, nullish coalescing)\\n+        // Build a minimal, safe context with only whitelisted symbols\\n+        try {\\n+          const vm = require('node:vm');\\n+          const ctx: any = {\\n+            // Scope vars\\n+            output,\\n+            outputs,\\n+            debug: debugData,\\n+            memory: memoryAccessor,\\n+            issues,\\n+            metadata,\\n+            criticalIssues,\\n+            errorIssues,\\n+            totalIssues,\\n+            warningIssues,\\n+            infoIssues,\\n+            checkName,\\n+            schema,\\n+            group,\\n+            branch,\\n+            baseBranch,\\n+            filesChanged,\\n+            filesCount,\\n+            event,\\n+            env,\\n+            // Helpers\\n+            contains,\\n+            startsWith,\\n+            endsWith,\\n+            length,\\n+            always,\\n+            success,\\n+            failure,\\n+            log,\\n+            hasIssue,\\n+            countIssues,\\n+            hasFileMatching,\\n+            hasIssueWith,\\n+            hasFileWith,\\n+            hasMinPermission,\\n+            isOwner,\\n+            isMember,\\n+            isCollaborator,\\n+            isContributor,\\n+            isFirstTimer,\\n+            Math,\\n+            JSON,\\n+          };\\n+          const context = vm.createContext(ctx);\\n+          // Try raw first; if it throws due to semicolons/newlines, normalize\\n+          let code = `(${raw})`;\\n+          try {\\n+            result = new vm.Script(code).runInContext(context, { timeout: 50 });\\n+          } catch {\\n+            const normalizedExpr = normalize(condition);\\n+            code = `(${normalizedExpr})`;\\n+            result = new vm.Script(code).runInContext(context, { timeout: 50 });\\n+          }\\n+        } catch (vmErr) {\\n+          console.error('‚ùå Failed to evaluate expression:', condition, vmErr);\\n+          throw vmErr;\\n+        }\\n       }\\n-      const result = exec(scope).run();\\n       try {\\n         require('./logger').logger.debug(`  fail_if: result=${Boolean(result)}`);\\n       } catch {}\\n@@ -595,7 +659,8 @@ export class FailureConditionEvaluator {\\n       return Boolean(result);\\n     } catch (error) {\\n       console.error('‚ùå Failed to evaluate expression:', condition, error);\\n-      // Re-throw the error so it can be caught at a higher level for error reporting\\n+      // Re-throw the error so it can be caught by evaluateSingleCondition\\n+      // and properly populate the error field in the result\\n       throw error;\\n     }\\n   }\\n\",\"status\":\"modified\"},{\"filename\":\"src/generated/config-schema.ts\",\"additions\":10,\"deletions\":1,\"changes\":371,\"patch\":\"diff --git a/src/generated/config-schema.ts b/src/generated/config-schema.ts\\nindex afa3eee0..47dc639b 100644\\n--- a/src/generated/config-schema.ts\\n+++ b/src/generated/config-schema.ts\\n@@ -30,6 +30,32 @@ export const configSchema = {\\n           description:\\n             'Extends from other configurations - can be file path, HTTP(S) URL, or \\\"default\\\"',\\n         },\\n+        include: {\\n+          anyOf: [\\n+            {\\n+              type: 'string',\\n+            },\\n+            {\\n+              type: 'array',\\n+              items: {\\n+                type: 'string',\\n+              },\\n+            },\\n+          ],\\n+          description:\\n+            'Alias for extends - include from other configurations (backward compatibility)',\\n+        },\\n+        tools: {\\n+          $ref: '#/definitions/Record%3Cstring%2CCustomToolDefinition%3E',\\n+          description: 'Custom tool definitions that can be used in MCP blocks',\\n+        },\\n+        imports: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description: 'Import workflow definitions from external files or URLs',\\n+        },\\n         steps: {\\n           $ref: '#/definitions/Record%3Cstring%2CCheckConfig%3E',\\n           description: 'Step configurations (recommended)',\\n@@ -91,6 +117,10 @@ export const configSchema = {\\n           $ref: '#/definitions/RoutingDefaults',\\n           description: 'Optional routing defaults for retry/goto/run policies',\\n         },\\n+        limits: {\\n+          $ref: '#/definitions/LimitsConfig',\\n+          description: 'Global execution limits',\\n+        },\\n       },\\n       required: ['output', 'version'],\\n       patternProperties: {\\n@@ -101,6 +131,100 @@ export const configSchema = {\\n       type: 'object',\\n       additionalProperties: {},\\n     },\\n+    'Record<string,CustomToolDefinition>': {\\n+      type: 'object',\\n+      additionalProperties: {\\n+        $ref: '#/definitions/CustomToolDefinition',\\n+      },\\n+    },\\n+    CustomToolDefinition: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Tool name - used to reference the tool in MCP blocks',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Description of what the tool does',\\n+        },\\n+        inputSchema: {\\n+          type: 'object',\\n+          properties: {\\n+            type: {\\n+              type: 'string',\\n+              const: 'object',\\n+            },\\n+            properties: {\\n+              $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+            },\\n+            required: {\\n+              type: 'array',\\n+              items: {\\n+                type: 'string',\\n+              },\\n+            },\\n+            additionalProperties: {\\n+              type: 'boolean',\\n+            },\\n+          },\\n+          required: ['type'],\\n+          additionalProperties: false,\\n+          description: 'Input schema for the tool (JSON Schema format)',\\n+          patternProperties: {\\n+            '^x-': {},\\n+          },\\n+        },\\n+        exec: {\\n+          type: 'string',\\n+          description: 'Command to execute - supports Liquid template',\\n+        },\\n+        stdin: {\\n+          type: 'string',\\n+          description: 'Optional stdin input - supports Liquid template',\\n+        },\\n+        transform: {\\n+          type: 'string',\\n+          description: 'Transform the raw output - supports Liquid template',\\n+        },\\n+        transform_js: {\\n+          type: 'string',\\n+          description: 'Transform the output using JavaScript - alternative to transform',\\n+        },\\n+        cwd: {\\n+          type: 'string',\\n+          description: 'Working directory for command execution',\\n+        },\\n+        env: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cstring%3E',\\n+          description: 'Environment variables for the command',\\n+        },\\n+        timeout: {\\n+          type: 'number',\\n+          description: 'Timeout in milliseconds',\\n+        },\\n+        parseJson: {\\n+          type: 'boolean',\\n+          description: 'Whether to parse output as JSON automatically',\\n+        },\\n+        outputSchema: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'Expected output schema for validation',\\n+        },\\n+      },\\n+      required: ['name', 'exec'],\\n+      additionalProperties: false,\\n+      description: 'Custom tool definition for use in MCP blocks',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    'Record<string,string>': {\\n+      type: 'object',\\n+      additionalProperties: {\\n+        type: 'string',\\n+      },\\n+    },\\n     'Record<string,CheckConfig>': {\\n       type: 'object',\\n       additionalProperties: {\\n@@ -207,6 +331,22 @@ export const configSchema = {\\n           type: 'string',\\n           description: 'AI provider to use for this check - overrides global setting',\\n         },\\n+        ai_persona: {\\n+          type: 'string',\\n+          description: \\\"Optional persona hint, prepended to the prompt as 'Persona: <value>'\\\",\\n+        },\\n+        ai_prompt_type: {\\n+          type: 'string',\\n+          description: 'Probe promptType for this check (underscore style)',\\n+        },\\n+        ai_system_prompt: {\\n+          type: 'string',\\n+          description: 'System prompt for this check (underscore style)',\\n+        },\\n+        ai_custom_prompt: {\\n+          type: 'string',\\n+          description: 'Legacy customPrompt (underscore style) ‚Äî deprecated, use ai_system_prompt',\\n+        },\\n         ai_mcp_servers: {\\n           $ref: '#/definitions/Record%3Cstring%2CMcpServerConfig%3E',\\n           description: 'MCP servers for this AI check - overrides global setting',\\n@@ -247,6 +387,11 @@ export const configSchema = {\\n           description:\\n             'Schema type for template rendering (e.g., \\\"code-review\\\", \\\"markdown\\\") or inline JSON schema object - optional',\\n         },\\n+        output_schema: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description:\\n+            \\\"Optional JSON Schema to validate the produced output. If omitted and `schema` is an object, the engine will treat that object as the output_schema for validation purposes while still using string schemas (e.g., 'code-review') for template selection.\\\",\\n+        },\\n         template: {\\n           $ref: '#/definitions/CustomTemplateConfig',\\n           description: 'Custom template configuration - optional',\\n@@ -283,6 +428,17 @@ export const configSchema = {\\n           description:\\n             'Tags for categorizing and filtering checks (e.g., [\\\"local\\\", \\\"fast\\\", \\\"security\\\"])',\\n         },\\n+        criticality: {\\n+          type: 'string',\\n+          enum: ['external', 'internal', 'policy', 'info'],\\n+          description:\\n+            \\\"Operational criticality of this step. Drives default safety policies (contracts, retries, loop budgets) at load time. Behavior can still be overridden explicitly per step via on_*, fail_if, assume/guarantee, etc.\\\\n\\\\n- 'external': interacts with external systems (side effects). Highest safety.\\\\n- 'internal': modifies CI/config/state but not prod. High safety.\\\\n- 'policy': organizational checks (linting, style, doc). Moderate safety.\\\\n- 'info': informational checks. Lowest safety.\\\",\\n+        },\\n+        continue_on_failure: {\\n+          type: 'boolean',\\n+          description:\\n+            \\\"Allow dependents to run even if this step fails. Defaults to false (dependents are gated when this step fails). Similar to GitHub Actions' continue-on-error.\\\",\\n+        },\\n         forEach: {\\n           type: 'boolean',\\n           description: 'Process output as array and run dependent checks for each item',\\n@@ -311,6 +467,41 @@ export const configSchema = {\\n           description:\\n             'Finish routing configuration for forEach checks (runs after ALL iterations complete)',\\n         },\\n+        assume: {\\n+          anyOf: [\\n+            {\\n+              type: 'string',\\n+            },\\n+            {\\n+              type: 'array',\\n+              items: {\\n+                type: 'string',\\n+              },\\n+            },\\n+          ],\\n+          description:\\n+            \\\"Preconditions that must hold before executing the check. If any expression evaluates to false, the check is skipped (skipReason='assume').\\\",\\n+        },\\n+        guarantee: {\\n+          anyOf: [\\n+            {\\n+              type: 'string',\\n+            },\\n+            {\\n+              type: 'array',\\n+              items: {\\n+                type: 'string',\\n+              },\\n+            },\\n+          ],\\n+          description:\\n+            'Postconditions that should hold after executing the check. Expressions are evaluated against the produced result/output; violations are recorded as error issues with ruleId \\\"contract/guarantee_failed\\\".',\\n+        },\\n+        max_runs: {\\n+          type: 'number',\\n+          description:\\n+            'Hard cap on how many times this check may execute within a single engine run. Overrides global limits.max_runs_per_check. Set to 0 or negative to disable for this step.',\\n+        },\\n         message: {\\n           type: 'string',\\n           description: 'Message template for log checks',\\n@@ -395,7 +586,7 @@ export const configSchema = {\\n           type: 'string',\\n           description: 'Session ID for HTTP transport (optional, server may generate one)',\\n         },\\n-        args: {\\n+        command_args: {\\n           type: 'array',\\n           items: {\\n             type: 'string',\\n@@ -422,6 +613,22 @@ export const configSchema = {\\n           type: 'string',\\n           description: 'Default value if timeout occurs or empty input when allow_empty is true',\\n         },\\n+        workflow: {\\n+          type: 'string',\\n+          description: 'Workflow ID or path to workflow file',\\n+        },\\n+        args: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'Arguments/inputs for the workflow',\\n+        },\\n+        overrides: {\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-10692-20779-src_types_config.ts-0-33972%3E%3E',\\n+          description: 'Override specific step configurations in the workflow',\\n+        },\\n+        output_mapping: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cstring%3E',\\n+          description: 'Map workflow outputs to check outputs',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'Configuration for a single check',\\n@@ -445,15 +652,10 @@ export const configSchema = {\\n         'claude-code',\\n         'mcp',\\n         'human-input',\\n+        'workflow',\\n       ],\\n       description: 'Valid check types in configuration',\\n     },\\n-    'Record<string,string>': {\\n-      type: 'object',\\n-      additionalProperties: {\\n-        type: 'string',\\n-      },\\n-    },\\n     EventTrigger: {\\n       type: 'string',\\n       enum: [\\n@@ -492,14 +694,23 @@ export const configSchema = {\\n           type: 'boolean',\\n           description: 'Enable debug mode',\\n         },\\n+        prompt_type: {\\n+          type: 'string',\\n+          description: 'Probe promptType to use (e.g., engineer, code-review, architect)',\\n+        },\\n+        system_prompt: {\\n+          type: 'string',\\n+          description: 'System prompt (baseline preamble). Replaces legacy custom_prompt.',\\n+        },\\n+        custom_prompt: {\\n+          type: 'string',\\n+          description:\\n+            'Probe customPrompt (baseline/system prompt) ‚Äî deprecated, use system_prompt',\\n+        },\\n         skip_code_context: {\\n           type: 'boolean',\\n           description: 'Skip adding code context (diffs, files, PR info) to the prompt',\\n         },\\n-        disable_tools: {\\n-          type: 'boolean',\\n-          description: 'Disable MCP tools - AI will only have access to the prompt text',\\n-        },\\n         mcpServers: {\\n           $ref: '#/definitions/Record%3Cstring%2CMcpServerConfig%3E',\\n           description: 'MCP servers configuration',\\n@@ -521,6 +732,26 @@ export const configSchema = {\\n           description:\\n             'Enable Edit and Create tools for file modification (disabled by default for security)',\\n         },\\n+        allowedTools: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description:\\n+            'Filter allowed tools - supports whitelist, exclusion (!prefix), or raw AI mode (empty array)',\\n+        },\\n+        disableTools: {\\n+          type: 'boolean',\\n+          description: 'Disable all tools for raw AI mode (alternative to allowedTools: [])',\\n+        },\\n+        allowBash: {\\n+          type: 'boolean',\\n+          description: 'Enable bash command execution (shorthand for bashConfig.enabled)',\\n+        },\\n+        bashConfig: {\\n+          $ref: '#/definitions/BashConfig',\\n+          description: 'Advanced bash command execution configuration',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'AI provider configuration',\\n@@ -663,6 +894,47 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n+    BashConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        allow: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description: \\\"Array of permitted command patterns (e.g., ['ls', 'git status'])\\\",\\n+        },\\n+        deny: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description: \\\"Array of blocked command patterns (e.g., ['rm -rf', 'sudo'])\\\",\\n+        },\\n+        noDefaultAllow: {\\n+          type: 'boolean',\\n+          description: 'Disable default safe command list (use with caution)',\\n+        },\\n+        noDefaultDeny: {\\n+          type: 'boolean',\\n+          description: 'Disable default dangerous command blocklist (use with extreme caution)',\\n+        },\\n+        timeout: {\\n+          type: 'number',\\n+          description: 'Execution timeout in milliseconds',\\n+        },\\n+        workingDirectory: {\\n+          type: 'string',\\n+          description: 'Default working directory for command execution',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description:\\n+        \\\"Bash command execution configuration for ProbeAgent Note: Use 'allowBash: true' in AIProviderConfig to enable bash execution\\\",\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n     ClaudeCodeConfig: {\\n       type: 'object',\\n       properties: {\\n@@ -831,6 +1103,14 @@ export const configSchema = {\\n           type: 'string',\\n           description: 'Dynamic remediation list: JS expression returning string[]',\\n         },\\n+        transitions: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/TransitionRule',\\n+          },\\n+          description:\\n+            \\\"Declarative transitions. Evaluated in order; first matching rule wins. If a rule's `to` is null, no goto occurs. When omitted or none match, the engine falls back to goto_js/goto for backward compatibility.\\\",\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'Failure routing configuration per check',\\n@@ -875,6 +1155,30 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n+    TransitionRule: {\\n+      type: 'object',\\n+      properties: {\\n+        when: {\\n+          type: 'string',\\n+          description:\\n+            'JavaScript expression evaluated in the same sandbox as goto_js; truthy enables the rule.',\\n+        },\\n+        to: {\\n+          type: ['string', 'null'],\\n+          description: 'Target step ID, or null to explicitly prevent goto.',\\n+        },\\n+        goto_event: {\\n+          $ref: '#/definitions/EventTrigger',\\n+          description: 'Optional event override when performing goto.',\\n+        },\\n+      },\\n+      required: ['when'],\\n+      additionalProperties: false,\\n+      description: 'Declarative transition rule for on_* blocks.',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n     OnSuccessConfig: {\\n       type: 'object',\\n       properties: {\\n@@ -901,6 +1205,13 @@ export const configSchema = {\\n           type: 'string',\\n           description: 'Dynamic post-success steps: JS expression returning string[]',\\n         },\\n+        transitions: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/TransitionRule',\\n+          },\\n+          description: 'Declarative transitions (see OnFailConfig.transitions).',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'Success routing configuration per check',\\n@@ -934,6 +1245,13 @@ export const configSchema = {\\n           type: 'string',\\n           description: 'Dynamic post-finish steps: JS expression returning string[]',\\n         },\\n+        transitions: {\\n+          type: 'array',\\n+          items: {\\n+            $ref: '#/definitions/TransitionRule',\\n+          },\\n+          description: 'Declarative transitions (see OnFailConfig.transitions).',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description:\\n@@ -942,6 +1260,17 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n+    'Record<string,Partial<interface-src_types_config.ts-10692-20779-src_types_config.ts-0-33972>>':\\n+      {\\n+        type: 'object',\\n+        additionalProperties: {\\n+          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-10692-20779-src_types_config.ts-0-33972%3E',\\n+        },\\n+      },\\n+    'Partial<interface-src_types_config.ts-10692-20779-src_types_config.ts-0-33972>': {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+    },\\n     OutputConfig: {\\n       type: 'object',\\n       properties: {\\n@@ -1293,6 +1622,26 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n+    LimitsConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        max_runs_per_check: {\\n+          type: 'number',\\n+          description:\\n+            'Maximum number of executions per check within a single engine run. Applies to each distinct scope independently for forEach item executions. Set to 0 or negative to disable. Default: 50.',\\n+        },\\n+        max_workflow_depth: {\\n+          type: 'number',\\n+          description:\\n+            'Maximum nesting depth for workflows executed by the state machine engine. Nested workflows are invoked by the workflow provider; this limit prevents accidental infinite recursion. Default: 3.',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Global engine limits',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n   },\\n } as const;\\n export default configSchema;\\n\",\"status\":\"modified\"},{\"filename\":\"src/github-check-service.ts\",\"additions\":1,\"deletions\":1,\"changes\":56,\"patch\":\"diff --git a/src/github-check-service.ts b/src/github-check-service.ts\\nindex b1dee165..8e297a84 100644\\n--- a/src/github-check-service.ts\\n+++ b/src/github-check-service.ts\\n@@ -14,6 +14,7 @@ export interface CheckRunOptions {\\n   name: string;\\n   details_url?: string;\\n   external_id?: string;\\n+  engine_mode?: 'legacy' | 'state-machine'; // M4: Track which engine mode was used\\n }\\n \\n export interface CheckRunAnnotation {\\n@@ -54,12 +55,22 @@ export class GitHubCheckService {\\n \\n   /**\\n    * Create a new check run in queued status\\n+   * M4: Includes engine_mode metadata in summary\\n    */\\n   async createCheckRun(\\n     options: CheckRunOptions,\\n     summary?: CheckRunSummary\\n   ): Promise<{ id: number; url: string }> {\\n     try {\\n+      // M4: Add engine mode metadata to summary if provided\\n+      const enhancedSummary =\\n+        summary && options.engine_mode\\n+          ? {\\n+              ...summary,\\n+              summary: `${summary.summary}\\\\n\\\\n_Engine: ${options.engine_mode}_`,\\n+            }\\n+          : summary;\\n+\\n       const response = await this.octokit.rest.checks.create({\\n         owner: options.owner,\\n         repo: options.repo,\\n@@ -68,11 +79,11 @@ export class GitHubCheckService {\\n         status: 'queued',\\n         details_url: options.details_url,\\n         external_id: options.external_id,\\n-        output: summary\\n+        output: enhancedSummary\\n           ? {\\n-              title: summary.title,\\n-              summary: summary.summary,\\n-              text: summary.text,\\n+              title: enhancedSummary.title,\\n+              summary: enhancedSummary.summary,\\n+              text: enhancedSummary.text,\\n             }\\n           : undefined,\\n       });\\n@@ -318,20 +329,21 @@ export class GitHubCheckService {\\n       const passedConditions = failureResults.filter(result => !result.failed);\\n \\n       if (failedConditions.length > 0) {\\n-        sections.push('### ‚ùå Failed Conditions');\\n+        sections.push('### Failed Conditions');\\n         failedConditions.forEach(condition => {\\n           sections.push(\\n             `- **${condition.conditionName}**: ${condition.message || condition.expression}`\\n           );\\n-          if (condition.severity === 'error') {\\n-            sections.push(`  - ‚ö†Ô∏è **Severity:** Error`);\\n+          if (condition.severity) {\\n+            const icon = this.getSeverityEmoji(condition.severity);\\n+            sections.push(`  - Severity: ${icon} ${condition.severity}`);\\n           }\\n         });\\n         sections.push('');\\n       }\\n \\n       if (passedConditions.length > 0) {\\n-        sections.push('### ‚úÖ Passed Conditions');\\n+        sections.push('### Passed Conditions');\\n         passedConditions.forEach(condition => {\\n           sections.push(\\n             `- **${condition.conditionName}**: ${condition.message || 'Condition passed'}`\\n@@ -344,18 +356,18 @@ export class GitHubCheckService {\\n     // Issues by category section\\n     if (reviewIssues.length > 0) {\\n       const issuesByCategory = this.groupIssuesByCategory(reviewIssues);\\n-      sections.push('## üêõ Issues by Category');\\n+      sections.push('## Issues by Category');\\n \\n       Object.entries(issuesByCategory).forEach(([category, issues]) => {\\n         if (issues.length > 0) {\\n           sections.push(\\n-            `### ${this.getCategoryEmoji(category)} ${category.charAt(0).toUpperCase() + category.slice(1)} (${issues.length})`\\n+            `### ${category.charAt(0).toUpperCase() + category.slice(1)} (${issues.length})`\\n           );\\n \\n           // Show only first 5 issues per category to keep the summary concise\\n           const displayIssues = issues.slice(0, 5);\\n           displayIssues.forEach(issue => {\\n-            const severityIcon = this.getSeverityIcon(issue.severity);\\n+            const severityIcon = this.getSeverityEmoji(issue.severity);\\n             sections.push(`- ${severityIcon} **${issue.file}:${issue.line}** - ${issue.message}`);\\n           });\\n \\n@@ -425,32 +437,16 @@ export class GitHubCheckService {\\n   }\\n \\n   /**\\n-   * Get emoji for issue category\\n-   */\\n-  private getCategoryEmoji(category: string): string {\\n-    const emojiMap: Record<string, string> = {\\n-      security: 'üîê',\\n-      performance: '‚ö°',\\n-      style: 'üé®',\\n-      logic: 'üß†',\\n-      architecture: 'üèóÔ∏è',\\n-      documentation: 'üìö',\\n-      general: 'üìù',\\n-    };\\n-    return emojiMap[category.toLowerCase()] || 'üìù';\\n-  }\\n-\\n-  /**\\n-   * Get icon for issue severity\\n+   * Get emoji for issue severity (allowed; step/category emojis are removed)\\n    */\\n-  private getSeverityIcon(severity: string): string {\\n+  private getSeverityEmoji(severity: string): string {\\n     const iconMap: Record<string, string> = {\\n       critical: 'üö®',\\n       error: '‚ùå',\\n       warning: '‚ö†Ô∏è',\\n       info: '‚ÑπÔ∏è',\\n     };\\n-    return iconMap[severity.toLowerCase()] || '‚ÑπÔ∏è';\\n+    return iconMap[String(severity || '').toLowerCase()] || '';\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/github-comments.ts\",\"additions\":1,\"deletions\":1,\"changes\":22,\"patch\":\"diff --git a/src/github-comments.ts b/src/github-comments.ts\\nindex be84f6ce..e8be7f77 100644\\n--- a/src/github-comments.ts\\n+++ b/src/github-comments.ts\\n@@ -221,8 +221,7 @@ ${content}\\n       const totalScore = items.reduce((sum, item) => sum + (item.score || 0), 0) / items.length;\\n       const totalIssues = items.reduce((sum, item) => sum + (item.issuesFound || 0), 0);\\n \\n-      const emoji = this.getCheckTypeEmoji(groupKey);\\n-      const title = `${emoji} ${this.formatGroupTitle(groupKey, totalScore, totalIssues)}`;\\n+      const title = this.formatGroupTitle(groupKey, totalScore, totalIssues);\\n \\n       const sectionContent = items.map(item => item.content).join('\\\\n\\\\n');\\n       sections.push(this.createCollapsibleSection(title, sectionContent, totalIssues > 0));\\n@@ -403,24 +402,7 @@ ${content}\\n     return 'Critical Issues';\\n   }\\n \\n-  /**\\n-   * Get emoji for check type\\n-   */\\n-  private getCheckTypeEmoji(checkType: string): string {\\n-    const emojiMap: Record<string, string> = {\\n-      performance: 'üìà',\\n-      security: 'üîí',\\n-      architecture: 'üèóÔ∏è',\\n-      style: 'üé®',\\n-      all: 'üîç',\\n-      Excellent: '‚úÖ',\\n-      Good: 'üëç',\\n-      'Needs Improvement': '‚ö†Ô∏è',\\n-      'Critical Issues': 'üö®',\\n-      Unknown: '‚ùì',\\n-    };\\n-    return emojiMap[checkType] || 'üìù';\\n-  }\\n+  // Emoji helper removed: plain titles are used in group headers\\n \\n   /**\\n    * Format group title with score and issue count\\n\",\"status\":\"modified\"},{\"filename\":\"src/index.ts\",\"additions\":1,\"deletions\":1,\"changes\":26,\"patch\":\"diff --git a/src/index.ts b/src/index.ts\\nindex ee22a8fd..90ec81de 100644\\n--- a/src/index.ts\\n+++ b/src/index.ts\\n@@ -711,9 +711,11 @@ async function handleIssueEvent(\\n     prInfo.comments = [];\\n   }\\n \\n-  // Run the checks using CheckExecutionEngine\\n-  const { CheckExecutionEngine } = await import('./check-execution-engine');\\n-  const engine = new CheckExecutionEngine(undefined, octokit);\\n+  // Run the checks using StateMachineExecutionEngine\\n+  const engine = new (await import('./state-machine-execution-engine')).StateMachineExecutionEngine(\\n+    undefined,\\n+    octokit\\n+  );\\n \\n   try {\\n     // Build tag filter from action inputs (if provided)\\n@@ -795,8 +797,22 @@ async function handleIssueEvent(\\n       // Directly use check content without adding extra headers\\n       for (const checks of Object.values(resultsToUse)) {\\n         for (const check of checks) {\\n-          if (check.content && check.content.trim()) {\\n-            commentBody += `${check.content}\\\\n\\\\n`;\\n+          // Try to get content, with fallback to output.text (for custom schemas like issue-assistant)\\n+          let content = check.content?.trim();\\n+          if (!content && check.output) {\\n+            const out = check.output as any;\\n+            if (typeof out === 'string' && out.trim()) {\\n+              content = out.trim();\\n+            } else if (typeof out === 'object') {\\n+              const txt = out.text || out.response || out.message;\\n+              if (typeof txt === 'string' && txt.trim()) {\\n+                content = txt.trim();\\n+              }\\n+            }\\n+          }\\n+\\n+          if (content) {\\n+            commentBody += `${content}\\\\n\\\\n`;\\n           }\\n         }\\n       }\\n\",\"status\":\"modified\"},{\"filename\":\"src/liquid-extensions.ts\",\"additions\":1,\"deletions\":0,\"changes\":2,\"patch\":\"diff --git a/src/liquid-extensions.ts b/src/liquid-extensions.ts\\nindex bc764d42..d215f0e4 100644\\n--- a/src/liquid-extensions.ts\\n+++ b/src/liquid-extensions.ts\\n@@ -269,6 +269,8 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n       return [];\\n     }\\n   });\\n+\\n+  // Removed: merge_sort_by filter (unused)\\n }\\n \\n /**\\n\",\"status\":\"added\"},{\"filename\":\"src/logger.ts\",\"additions\":1,\"deletions\":1,\"changes\":26,\"patch\":\"diff --git a/src/logger.ts b/src/logger.ts\\nindex f97feaf7..6148b77c 100644\\n--- a/src/logger.ts\\n+++ b/src/logger.ts\\n@@ -28,6 +28,7 @@ class Logger {\\n   private level: LogLevel = 'info';\\n   private isJsonLike: boolean = false;\\n   private isTTY: boolean = typeof process !== 'undefined' ? !!process.stderr.isTTY : false;\\n+  private showTimestamps: boolean = true; // default: always show timestamps\\n \\n   configure(\\n     opts: {\\n@@ -78,42 +79,49 @@ class Logger {\\n     return true;\\n   }\\n \\n-  private write(msg: string): void {\\n+  private write(msg: string, level?: LogLevel): void {\\n     // Always route to stderr to keep stdout clean for results\\n     try {\\n-      process.stderr.write(msg + '\\\\n');\\n+      if (this.showTimestamps) {\\n+        const ts = new Date().toISOString();\\n+        const lvl = level ? level : undefined;\\n+        const prefix = lvl ? `[${ts}] [${lvl}]` : `[${ts}]`;\\n+        process.stderr.write(`${prefix} ${msg}\\\\n`);\\n+      } else {\\n+        process.stderr.write(msg + '\\\\n');\\n+      }\\n     } catch {\\n       // Ignore write errors\\n     }\\n   }\\n \\n   info(msg: string): void {\\n-    if (this.shouldLog('info')) this.write(msg);\\n+    if (this.shouldLog('info')) this.write(msg, 'info');\\n   }\\n \\n   warn(msg: string): void {\\n-    if (this.shouldLog('warn')) this.write(msg);\\n+    if (this.shouldLog('warn')) this.write(msg, 'warn');\\n   }\\n \\n   error(msg: string): void {\\n-    if (this.shouldLog('error')) this.write(msg);\\n+    if (this.shouldLog('error')) this.write(msg, 'error');\\n   }\\n \\n   verbose(msg: string): void {\\n-    if (this.shouldLog('verbose')) this.write(msg);\\n+    if (this.shouldLog('verbose')) this.write(msg, 'verbose');\\n   }\\n \\n   debug(msg: string): void {\\n-    if (this.shouldLog('debug')) this.write(msg);\\n+    if (this.shouldLog('debug')) this.write(msg, 'debug');\\n   }\\n \\n   step(msg: string): void {\\n     // High-level phase indicator\\n-    if (this.shouldLog('info')) this.write(`‚ñ∂ ${msg}`);\\n+    if (this.shouldLog('info')) this.write(`‚ñ∂ ${msg}`, 'info');\\n   }\\n \\n   success(msg: string): void {\\n-    if (this.shouldLog('info')) this.write(`‚úî ${msg}`);\\n+    if (this.shouldLog('info')) this.write(`‚úî ${msg}`, 'info');\\n   }\\n }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/memory-store.ts\",\"additions\":1,\"deletions\":0,\"changes\":9,\"patch\":\"diff --git a/src/memory-store.ts b/src/memory-store.ts\\nindex ed48ffe7..71165fc5 100644\\n--- a/src/memory-store.ts\\n+++ b/src/memory-store.ts\\n@@ -31,6 +31,15 @@ export class MemoryStore {\\n     return MemoryStore.instance;\\n   }\\n \\n+  /**\\n+   * Create a new isolated MemoryStore instance that does not affect the\\n+   * process-wide singleton. Useful for nested workflows or tests where\\n+   * state must not leak between runs.\\n+   */\\n+  static createIsolated(config?: MemoryConfig): MemoryStore {\\n+    return new MemoryStore(config);\\n+  }\\n+\\n   /**\\n    * Reset singleton instance (for testing)\\n    */\\n\",\"status\":\"added\"},{\"filename\":\"src/output-formatters.ts\",\"additions\":1,\"deletions\":1,\"changes\":2,\"patch\":\"diff --git a/src/output-formatters.ts b/src/output-formatters.ts\\nindex 02f9751e..6f64b03e 100644\\n--- a/src/output-formatters.ts\\n+++ b/src/output-formatters.ts\\n@@ -16,7 +16,7 @@ export interface AnalysisResult {\\n   executionTime: number;\\n   timestamp: string;\\n   checksExecuted: string[];\\n-  executionStatistics?: import('./check-execution-engine').ExecutionStatistics; // Detailed execution statistics\\n+  executionStatistics?: import('./types/execution').ExecutionStatistics; // Detailed execution statistics\\n   debug?: DebugInfo; // Optional debug information when debug mode is enabled\\n   failureConditions?: FailureConditionResult[]; // Optional failure condition results\\n   isCodeReview?: boolean; // Whether this is a code review context (affects output formatting)\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/ai-check-provider.ts\",\"additions\":3,\"deletions\":1,\"changes\":126,\"patch\":\"diff --git a/src/providers/ai-check-provider.ts b/src/providers/ai-check-provider.ts\\nindex 06093942..9379e2dc 100644\\n--- a/src/providers/ai-check-provider.ts\\n+++ b/src/providers/ai-check-provider.ts\\n@@ -470,25 +470,40 @@ export class AICheckProvider extends CheckProvider {\\n     try {\\n       return await this.liquidEngine.parseAndRender(promptContent, templateContext);\\n     } catch (error) {\\n-      try {\\n-        if (process.env.VISOR_DEBUG === 'true') {\\n-          const lines = promptContent.split(/\\\\r?\\\\n/);\\n-          const preview = lines\\n-            .slice(0, 20)\\n-            .map((l, i) => `${(i + 1).toString().padStart(3, ' ')}| ${l}`)\\n-            .join('\\\\n');\\n-          try {\\n-            process.stderr.write(\\n-              '[prompt-error] First 20 lines of prompt before Liquid render:\\\\n' + preview + '\\\\n'\\n-            );\\n-          } catch {}\\n+      // Always show a helpful snippet with a caret, similar to YAML errors\\n+      const err: any = error || {};\\n+      const lines = String(promptContent || '').split(/\\\\r?\\\\n/);\\n+      const lineNum: number = Number(err.line || err?.token?.line || err?.location?.line || 0);\\n+      const colNum: number = Number(err.col || err?.token?.col || err?.location?.col || 0);\\n+      let snippet = '';\\n+      if (lineNum > 0) {\\n+        const start = Math.max(1, lineNum - 3);\\n+        const end = Math.max(lineNum + 2, lineNum);\\n+        const width = String(end).length;\\n+        for (let i = start; i <= Math.min(end, lines.length); i++) {\\n+          const ln = `${String(i).padStart(width, ' ')} | ${lines[i - 1] ?? ''}`;\\n+          snippet += ln + '\\\\n';\\n+          if (i === lineNum) {\\n+            const caretPad = ' '.repeat(Math.max(0, colNum > 1 ? colNum - 1 : 0) + width + 3);\\n+            snippet += caretPad + '^\\\\n';\\n+          }\\n         }\\n+      } else {\\n+        // Fallback preview of the first 20 lines\\n+        const preview = lines\\n+          .slice(0, 20)\\n+          .map((l, i) => `${(i + 1).toString().padStart(3, ' ')} | ${l}`)\\n+          .join('\\\\n');\\n+        snippet = preview + '\\\\n';\\n+      }\\n+      const msg = `Failed to render prompt template: ${\\n+        error instanceof Error ? error.message : 'Unknown error'\\n+      }`;\\n+      // Print a clear, user-friendly error with context\\n+      try {\\n+        console.error('\\\\n[prompt-error] ' + msg + '\\\\n' + snippet);\\n       } catch {}\\n-      throw new Error(\\n-        `Failed to render prompt template: ${\\n-          error instanceof Error ? error.message : 'Unknown error'\\n-        }`\\n-      );\\n+      throw new Error(msg);\\n     }\\n   }\\n \\n@@ -560,6 +575,18 @@ export class AICheckProvider extends CheckProvider {\\n       if (config.ai.allowEdit !== undefined) {\\n         aiConfig.allowEdit = config.ai.allowEdit as boolean;\\n       }\\n+      if (config.ai.allowedTools !== undefined) {\\n+        aiConfig.allowedTools = config.ai.allowedTools as string[];\\n+      }\\n+      if (config.ai.disableTools !== undefined) {\\n+        aiConfig.disableTools = config.ai.disableTools as boolean;\\n+      }\\n+      if (config.ai.allowBash !== undefined) {\\n+        aiConfig.allowBash = config.ai.allowBash as boolean;\\n+      }\\n+      if (config.ai.bashConfig !== undefined) {\\n+        aiConfig.bashConfig = config.ai.bashConfig as import('../types/config').BashConfig;\\n+      }\\n       if (config.ai.skip_code_context !== undefined) {\\n         // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n         (aiConfig as any).skip_code_context = config.ai.skip_code_context as boolean;\\n@@ -616,11 +643,11 @@ export class AICheckProvider extends CheckProvider {\\n     }\\n \\n     // Pass MCP server config directly to AI service (unless tools are disabled)\\n-    if (Object.keys(mcpServers).length > 0 && !config.ai?.disable_tools) {\\n+    if (Object.keys(mcpServers).length > 0 && !config.ai?.disableTools) {\\n       // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n       (aiConfig as any).mcpServers = mcpServers;\\n       // no noisy diagnostics here\\n-    } else if (config.ai?.disable_tools) {\\n+    } else if (config.ai?.disableTools) {\\n       // silently skip MCP when tools disabled\\n     }\\n \\n@@ -666,8 +693,10 @@ export class AICheckProvider extends CheckProvider {\\n     } catch {}\\n \\n     // Process prompt with Liquid templates and file loading\\n-    // Skip event context (PR diffs, files, etc.) if requested\\n-    const eventContext = config.ai?.skip_code_context ? {} : config.eventContext;\\n+    // Do NOT strip event context on skip_code_context ‚Äî that flag only controls\\n+    // whether we embed PR diffs/large code context later in AIReviewService.\\n+    // Keep repository/comment metadata available for prompts and tests.\\n+    const eventContext = config.eventContext || {};\\n     // Thread stageHistoryBase via eventContext for prompt rendering so\\n     // Liquid templates can get outputs_history_stage (computed from baseline).\\n     const ctxWithStage = {\\n@@ -685,22 +714,37 @@ export class AICheckProvider extends CheckProvider {\\n       (config as any).__outputHistory as Map<string, unknown[]> | undefined\\n     );\\n \\n-    // No implicit prompt mutations here ‚Äî prompts should come from YAML.\\n+    // Optional persona (vendor extension): ai.ai_persona or ai_persona.\\n+    // This is a light-weight preamble, not a rewriting of the user's prompt.\\n+    const aiAny = (config.ai || {}) as any;\\n+    // Persona (underscore only)\\n+    const persona = (aiAny?.ai_persona || (config as any).ai_persona || '').toString().trim();\\n+    const finalPrompt = persona ? `Persona: ${persona}\\\\n\\\\n${processedPrompt}` : processedPrompt;\\n+    // Expose promptType to AIReviewService via env (bridge until ProbeAgent supports it in our SDK surface)\\n+    try {\\n+      const pt = ((config.ai as any)?.promptType || (config as any).ai_prompt_type || '')\\n+        .toString()\\n+        .trim();\\n+      if (pt) process.env.VISOR_PROMPT_TYPE = pt;\\n+    } catch {}\\n \\n     // Test hook: capture the FINAL prompt (with PR context) before provider invocation\\n     try {\\n       const stepName = (config as any).checkName || 'unknown';\\n       const serviceForCapture = new AIReviewService(aiConfig);\\n-      const finalPrompt = await (serviceForCapture as any).buildCustomPrompt(\\n+      const finalPromptCapture = await (serviceForCapture as any).buildCustomPrompt(\\n         prInfo,\\n-        processedPrompt,\\n+        finalPrompt,\\n         config.schema,\\n-        { checkName: (config as any).checkName }\\n+        {\\n+          checkName: (config as any).checkName,\\n+          skipPRContext: (config.ai as any)?.skip_code_context === true,\\n+        }\\n       );\\n       sessionInfo?.hooks?.onPromptCaptured?.({\\n         step: String(stepName),\\n         provider: 'ai',\\n-        prompt: finalPrompt,\\n+        prompt: finalPromptCapture,\\n       });\\n       // capture hook retained; no extra console diagnostics\\n     } catch {}\\n@@ -710,11 +754,29 @@ export class AICheckProvider extends CheckProvider {\\n       const stepName = (config as any).checkName || 'unknown';\\n       const mock = sessionInfo?.hooks?.mockForStep?.(String(stepName));\\n       if (mock !== undefined) {\\n+        // If the mock looks like a ReviewSummary (has issues/content), use it directly\\n+        if (mock && typeof mock === 'object' && ('issues' in (mock as any) || 'content' in (mock as any))) {\\n+          return mock as unknown as ReviewSummary;\\n+        }\\n+        // Otherwise treat it as provider output payload\\n         return { issues: [], output: mock } as ReviewSummary & { output: unknown };\\n       }\\n     } catch {}\\n \\n     // Create AI service with config - environment variables will be used if aiConfig is empty\\n+    try {\\n+      const pt = (aiAny?.prompt_type || (config as any).ai_prompt_type || '').toString().trim();\\n+      if (pt) (aiConfig as any).promptType = pt;\\n+      // Prefer new system_prompt; fall back to legacy custom_prompt for backward compatibility\\n+      const sys = (aiAny?.system_prompt || (config as any).ai_system_prompt || '')\\n+        .toString()\\n+        .trim();\\n+      const legacy = (aiAny?.custom_prompt || (config as any).ai_custom_prompt || '')\\n+        .toString()\\n+        .trim();\\n+      if (sys) (aiConfig as any).systemPrompt = sys;\\n+      else if (legacy) (aiConfig as any).systemPrompt = legacy;\\n+    } catch {}\\n     const service = new AIReviewService(aiConfig);\\n \\n     // Pass the custom prompt and schema - no fallbacks\\n@@ -782,7 +844,7 @@ export class AICheckProvider extends CheckProvider {\\n         }\\n         result = await service.executeReview(\\n           prInfo,\\n-          processedPrompt,\\n+          finalPrompt,\\n           schema,\\n           config.checkName,\\n           config.sessionId\\n@@ -871,9 +933,19 @@ export class AICheckProvider extends CheckProvider {\\n       'ai.timeout',\\n       'ai.mcpServers',\\n       'ai.enableDelegate',\\n+      // legacy persona/prompt keys supported in config\\n+      'ai_persona',\\n+      'ai_prompt_type',\\n+      'ai_custom_prompt',\\n+      'ai_system_prompt',\\n+      // new provider resilience and tools toggles\\n       'ai.retry',\\n       'ai.fallback',\\n       'ai.allowEdit',\\n+      'ai.allowedTools',\\n+      'ai.disableTools',\\n+      'ai.allowBash',\\n+      'ai.bashConfig',\\n       'ai_model',\\n       'ai_provider',\\n       'ai_mcp_servers',\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/check-provider-registry.ts\",\"additions\":1,\"deletions\":1,\"changes\":24,\"patch\":\"diff --git a/src/providers/check-provider-registry.ts b/src/providers/check-provider-registry.ts\\nindex fc4babbb..7b44ae70 100644\\n--- a/src/providers/check-provider-registry.ts\\n+++ b/src/providers/check-provider-registry.ts\\n@@ -12,6 +12,8 @@ import { MemoryCheckProvider } from './memory-check-provider';\\n import { McpCheckProvider } from './mcp-check-provider';\\n import { HumanInputCheckProvider } from './human-input-check-provider';\\n import { ScriptCheckProvider } from './script-check-provider';\\n+import { WorkflowCheckProvider } from './workflow-check-provider';\\n+import { CustomToolDefinition } from '../types/config';\\n \\n /**\\n  * Registry for managing check providers\\n@@ -19,6 +21,7 @@ import { ScriptCheckProvider } from './script-check-provider';\\n export class CheckProviderRegistry {\\n   private providers: Map<string, CheckProvider> = new Map();\\n   private static instance: CheckProviderRegistry;\\n+  private customTools?: Record<string, CustomToolDefinition>;\\n \\n   private constructor() {\\n     // Register default providers\\n@@ -51,6 +54,7 @@ export class CheckProviderRegistry {\\n     this.register(new MemoryCheckProvider());\\n     this.register(new GitHubOpsProvider());\\n     this.register(new HumanInputCheckProvider());\\n+    this.register(new WorkflowCheckProvider());\\n \\n     // Try to register ClaudeCodeCheckProvider - it may fail if dependencies are missing\\n     try {\\n@@ -65,7 +69,12 @@ export class CheckProviderRegistry {\\n \\n     // Try to register McpCheckProvider - it may fail if dependencies are missing\\n     try {\\n-      this.register(new McpCheckProvider());\\n+      const mcpProvider = new McpCheckProvider();\\n+      // Set custom tools if available\\n+      if (this.customTools) {\\n+        mcpProvider.setCustomTools(this.customTools);\\n+      }\\n+      this.register(mcpProvider);\\n     } catch (error) {\\n       console.error(\\n         `Warning: Failed to register McpCheckProvider: ${\\n@@ -143,6 +152,19 @@ export class CheckProviderRegistry {\\n     return Array.from(this.providers.values());\\n   }\\n \\n+  /**\\n+   * Set custom tools that can be used by the MCP provider\\n+   */\\n+  setCustomTools(tools: Record<string, CustomToolDefinition>): void {\\n+    this.customTools = tools;\\n+\\n+    // Update MCP provider if already registered\\n+    const mcpProvider = this.providers.get('mcp') as McpCheckProvider | undefined;\\n+    if (mcpProvider) {\\n+      mcpProvider.setCustomTools(tools);\\n+    }\\n+  }\\n+\\n   /**\\n    * Get providers that are currently available (have required dependencies)\\n    */\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/check-provider.interface.ts\",\"additions\":1,\"deletions\":1,\"changes\":5,\"patch\":\"diff --git a/src/providers/check-provider.interface.ts b/src/providers/check-provider.interface.ts\\nindex 02ca3472..fba5d76e 100644\\n--- a/src/providers/check-provider.interface.ts\\n+++ b/src/providers/check-provider.interface.ts\\n@@ -13,7 +13,8 @@ export interface CheckProviderConfig {\\n   command?: string; // For PR comment triggers\\n   exec?: string; // For command execution (supports Liquid templates)\\n   stdin?: string; // Optional stdin input (supports Liquid templates)\\n-  args?: string[]; // Deprecated: use exec with inline args instead\\n+  args?: string[] | Record<string, unknown>; // string[] deprecated for command args; Record for workflow inputs\\n+  command_args?: string[]; // MCP stdio command arguments\\n   interpreter?: string;\\n   url?: string;\\n   method?: string;\\n@@ -53,6 +54,8 @@ export interface ExecutionContext {\\n    * relying on global execution history.\\n    */\\n   stageHistoryBase?: Record<string, number>;\\n+  /** Workflow inputs - available when executing within a workflow */\\n+  workflowInputs?: Record<string, unknown>;\\n   /** SDK hooks for human input */\\n   hooks?: {\\n     onHumanInput?: (request: HumanInputRequest) => Promise<string>;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/claude-code-check-provider.ts\",\"additions\":1,\"deletions\":0,\"changes\":11,\"patch\":\"diff --git a/src/providers/claude-code-check-provider.ts b/src/providers/claude-code-check-provider.ts\\nindex 75be2b60..e37087e8 100644\\n--- a/src/providers/claude-code-check-provider.ts\\n+++ b/src/providers/claude-code-check-provider.ts\\n@@ -480,6 +480,17 @@ export class ClaudeCodeCheckProvider extends CheckProvider {\\n     dependencyResults?: Map<string, ReviewSummary>,\\n     sessionInfo?: { parentSessionId?: string; reuseSession?: boolean }\\n   ): Promise<ReviewSummary> {\\n+    // Test hook: allow short-circuiting provider with mock output (YAML tests)\\n+    try {\\n+      const stepName = (config as any).checkName || 'claude-code';\\n+      const mock = (sessionInfo as any)?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        if (mock && typeof mock === 'object' && 'issues' in (mock as any)) {\\n+          return mock as unknown as ReviewSummary;\\n+        }\\n+        return { issues: [], output: mock as unknown } as ReviewSummary & { output: unknown };\\n+      }\\n+    } catch {}\\n     // Extract Claude Code configuration\\n     const claudeCodeConfig = (config.claude_code as ClaudeCodeConfig) || {};\\n \\n\",\"status\":\"added\"},{\"filename\":\"src/providers/command-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":67,\"patch\":\"diff --git a/src/providers/command-check-provider.ts b/src/providers/command-check-provider.ts\\nindex 870299d7..0b3a7e81 100644\\n--- a/src/providers/command-check-provider.ts\\n+++ b/src/providers/command-check-provider.ts\\n@@ -6,6 +6,7 @@ import Sandbox from '@nyariv/sandboxjs';\\n import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n import { logger } from '../logger';\\n+import { commandExecutor } from '../utils/command-executor';\\n import {\\n   createPermissionHelpers,\\n   detectLocalMode,\\n@@ -129,6 +130,8 @@ export class CommandCheckProvider extends CheckProvider {\\n       })(),\\n       // New: outputs_raw exposes aggregate values (e.g., full arrays for forEach parents)\\n       outputs_raw: outputsRaw,\\n+      // Workflow inputs (when executing within a workflow)\\n+      inputs: context?.workflowInputs || {},\\n       env: this.getSafeEnvironmentVariables(),\\n     };\\n \\n@@ -161,9 +164,18 @@ export class CommandCheckProvider extends CheckProvider {\\n     // Test hook: mock output for this step (short-circuit execution)\\n     try {\\n       const stepName = (config as any).checkName || 'unknown';\\n-      const mock = context?.hooks?.mockForStep?.(String(stepName));\\n-      if (mock && typeof mock === 'object') {\\n-        const m = mock as { stdout?: string; stderr?: string; exit_code?: number };\\n+      const rawMock = context?.hooks?.mockForStep?.(String(stepName));\\n+      if (rawMock !== undefined) {\\n+        // Normalize primitive mocks into object form\\n+        let mock: any;\\n+        if (typeof rawMock === 'number') {\\n+          mock = { exit_code: Number(rawMock) };\\n+        } else if (typeof rawMock === 'string') {\\n+          mock = { stdout: String(rawMock) };\\n+        } else {\\n+          mock = rawMock as Record<string, unknown>;\\n+        }\\n+        const m = mock as { stdout?: string; stderr?: string; exit_code?: number; exit?: number };\\n         let out: unknown = m.stdout ?? '';\\n         try {\\n           if (\\n@@ -173,19 +185,20 @@ export class CommandCheckProvider extends CheckProvider {\\n             out = JSON.parse(out);\\n           }\\n         } catch {}\\n-        if (m.exit_code && m.exit_code !== 0) {\\n+        const code =\\n+          typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0;\\n+        if (code !== 0) {\\n           return {\\n             issues: [\\n               {\\n                 file: 'command',\\n                 line: 0,\\n                 ruleId: 'command/execution_error',\\n-                message: `Mocked command exited with code ${m.exit_code}`,\\n+                message: `Mocked command exited with code ${code}`,\\n                 severity: 'error',\\n                 category: 'logic',\\n               },\\n             ],\\n-            // Also expose output for assertions\\n             output: out,\\n           } as any;\\n         }\\n@@ -216,11 +229,6 @@ export class CommandCheckProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Execute the script using dynamic import to avoid Jest issues\\n-      const { exec } = await import('child_process');\\n-      const { promisify } = await import('util');\\n-      const execAsync = promisify(exec);\\n-\\n       // Get timeout from config (in seconds) or use default (60 seconds)\\n       const timeoutSeconds = (config.timeout as number) || 60;\\n       const timeoutMs = timeoutSeconds * 1000;\\n@@ -247,16 +255,36 @@ export class CommandCheckProvider extends CheckProvider {\\n \\n       const safeCommand = normalizeNodeEval(renderedCommand);\\n \\n-      const { stdout, stderr } = await execAsync(safeCommand, {\\n+      // Use shared command executor\\n+      const execResult = await commandExecutor.execute(safeCommand, {\\n         env: scriptEnv,\\n         timeout: timeoutMs,\\n-        maxBuffer: 10 * 1024 * 1024, // 10MB buffer\\n       });\\n \\n+      const { stdout, stderr, exitCode } = execResult;\\n+\\n       if (stderr) {\\n         logger.debug(`Command stderr: ${stderr}`);\\n       }\\n \\n+      // Check for non-zero exit code\\n+      if (exitCode !== 0) {\\n+        const errorMessage = stderr || `Command exited with code ${exitCode}`;\\n+        logger.error(`Command failed with exit code ${exitCode}: ${errorMessage}`);\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'command',\\n+              line: 0,\\n+              ruleId: 'command/execution_error',\\n+              message: `Command execution failed: ${errorMessage}`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n       // Keep raw output for transforms\\n       const rawOutput = stdout.trim();\\n \\n@@ -321,6 +349,17 @@ export class CommandCheckProvider extends CheckProvider {\\n             finalOutput = rendered.trim();\\n             logger.verbose(`‚úì Applied Liquid transform successfully (string output)`);\\n           }\\n+\\n+          // Capture Liquid transform in telemetry\\n+          try {\\n+            const span = trace.getSpan(otContext.active());\\n+            if (span) {\\n+              const { captureLiquidEvaluation } = require('../telemetry/state-capture');\\n+              captureLiquidEvaluation(span, transform, transformContext, rendered);\\n+            }\\n+          } catch {\\n+            // Ignore telemetry errors\\n+          }\\n         } catch (error) {\\n           logger.error(\\n             `‚úó Failed to apply Liquid transform: ${error instanceof Error ? error.message : 'Unknown error'}`\\n@@ -351,6 +390,7 @@ export class CommandCheckProvider extends CheckProvider {\\n             pr: templateContext.pr,\\n             files: templateContext.files,\\n             outputs: this.makeOutputsJsonSmart(templateContext.outputs),\\n+            inputs: templateContext.inputs || {},\\n             env: templateContext.env,\\n             permissions: createPermissionHelpers(\\n               resolveAssociationFromEvent((prInfo as any).eventContext, prInfo.authorAssociation),\\n@@ -385,6 +425,7 @@ export class CommandCheckProvider extends CheckProvider {\\n             const pr = scope.pr;\\n             const files = scope.files;\\n             const outputs = scope.outputs;\\n+            const inputs = scope.inputs;\\n             const env = scope.env;\\n             const log = (...args) => { console.log('üîç Debug:', ...args); };\\n             const hasMinPermission = scope.permissions.hasMinPermission;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/custom-tool-executor.ts\",\"additions\":7,\"deletions\":0,\"changes\":241,\"patch\":\"diff --git a/src/providers/custom-tool-executor.ts b/src/providers/custom-tool-executor.ts\\nnew file mode 100644\\nindex 00000000..060b4b37\\n--- /dev/null\\n+++ b/src/providers/custom-tool-executor.ts\\n@@ -0,0 +1,241 @@\\n+import { CustomToolDefinition } from '../types/config';\\n+import { Liquid } from 'liquidjs';\\n+import { createExtendedLiquid } from '../liquid-extensions';\\n+import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n+import Sandbox from '@nyariv/sandboxjs';\\n+import { logger } from '../logger';\\n+import { commandExecutor } from '../utils/command-executor';\\n+import Ajv from 'ajv';\\n+\\n+/**\\n+ * Executes custom tools defined in YAML configuration\\n+ * These tools can be used in MCP blocks as if they were native MCP tools\\n+ */\\n+export class CustomToolExecutor {\\n+  private liquid: Liquid;\\n+  private sandbox?: Sandbox;\\n+  private tools: Map<string, CustomToolDefinition>;\\n+  private ajv: Ajv;\\n+\\n+  constructor(tools?: Record<string, CustomToolDefinition>) {\\n+    this.liquid = createExtendedLiquid({\\n+      cache: false,\\n+      strictFilters: false,\\n+      strictVariables: false,\\n+    });\\n+    this.tools = new Map(Object.entries(tools || {}));\\n+    this.ajv = new Ajv({ allErrors: true, verbose: true });\\n+  }\\n+\\n+  /**\\n+   * Register a custom tool\\n+   */\\n+  registerTool(tool: CustomToolDefinition): void {\\n+    if (!tool.name) {\\n+      throw new Error('Tool must have a name');\\n+    }\\n+    this.tools.set(tool.name, tool);\\n+  }\\n+\\n+  /**\\n+   * Register multiple tools\\n+   */\\n+  registerTools(tools: Record<string, CustomToolDefinition>): void {\\n+    for (const [name, tool] of Object.entries(tools)) {\\n+      // Ensure tool has the correct name\\n+      tool.name = tool.name || name;\\n+      this.registerTool(tool);\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Get all registered tools\\n+   */\\n+  getTools(): CustomToolDefinition[] {\\n+    return Array.from(this.tools.values());\\n+  }\\n+\\n+  /**\\n+   * Get a specific tool by name\\n+   */\\n+  getTool(name: string): CustomToolDefinition | undefined {\\n+    return this.tools.get(name);\\n+  }\\n+\\n+  /**\\n+   * Validate tool input against schema using ajv\\n+   */\\n+  private validateInput(tool: CustomToolDefinition, input: Record<string, unknown>): void {\\n+    if (!tool.inputSchema) {\\n+      return;\\n+    }\\n+\\n+    // Compile and cache the schema validator for this tool\\n+    const validate = this.ajv.compile(tool.inputSchema);\\n+\\n+    // Validate the input\\n+    const valid = validate(input);\\n+\\n+    if (!valid) {\\n+      // Format validation errors for better readability\\n+      const errors = validate.errors\\n+        ?.map(err => {\\n+          if (err.instancePath) {\\n+            return `${err.instancePath}: ${err.message}`;\\n+          }\\n+          return err.message;\\n+        })\\n+        .join(', ');\\n+\\n+      throw new Error(`Input validation failed for tool '${tool.name}': ${errors}`);\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Execute a custom tool\\n+   */\\n+  async execute(\\n+    toolName: string,\\n+    args: Record<string, unknown>,\\n+    context?: {\\n+      pr?: {\\n+        number: number;\\n+        title: string;\\n+        author: string;\\n+        branch: string;\\n+        base: string;\\n+      };\\n+      files?: unknown[];\\n+      outputs?: Record<string, unknown>;\\n+      env?: Record<string, string>;\\n+    }\\n+  ): Promise<unknown> {\\n+    const tool = this.tools.get(toolName);\\n+    if (!tool) {\\n+      throw new Error(`Tool not found: ${toolName}`);\\n+    }\\n+\\n+    // Validate input\\n+    this.validateInput(tool, args);\\n+\\n+    // Build template context\\n+    const templateContext = {\\n+      ...context,\\n+      args,\\n+      input: args,\\n+    };\\n+\\n+    // Render command with Liquid\\n+    const command = await this.liquid.parseAndRender(tool.exec, templateContext);\\n+\\n+    // Render stdin if provided\\n+    let stdin: string | undefined;\\n+    if (tool.stdin) {\\n+      stdin = await this.liquid.parseAndRender(tool.stdin, templateContext);\\n+    }\\n+\\n+    // Execute the command using shared executor\\n+    const env = commandExecutor.buildEnvironment(process.env, tool.env, context?.env);\\n+    const result = await commandExecutor.execute(command, {\\n+      stdin,\\n+      cwd: tool.cwd,\\n+      env,\\n+      timeout: tool.timeout || 30000,\\n+    });\\n+\\n+    // Parse JSON if requested\\n+    let output: unknown = result.stdout;\\n+    if (tool.parseJson) {\\n+      try {\\n+        output = JSON.parse(result.stdout);\\n+      } catch (e) {\\n+        logger.warn(`Failed to parse tool output as JSON: ${e}`);\\n+      }\\n+    }\\n+\\n+    // Apply transform if specified\\n+    if (tool.transform) {\\n+      const transformContext = {\\n+        ...templateContext,\\n+        output,\\n+        stdout: result.stdout,\\n+        stderr: result.stderr,\\n+        exitCode: result.exitCode,\\n+      };\\n+      const transformed = await this.liquid.parseAndRender(tool.transform, transformContext);\\n+      // Try to parse as JSON if it looks like JSON\\n+      if (typeof transformed === 'string' && transformed.trim().startsWith('{')) {\\n+        try {\\n+          output = JSON.parse(transformed);\\n+        } catch {\\n+          output = transformed;\\n+        }\\n+      } else {\\n+        output = transformed;\\n+      }\\n+    }\\n+\\n+    // Apply JavaScript transform if specified\\n+    if (tool.transform_js) {\\n+      output = await this.applyJavaScriptTransform(tool.transform_js, output, {\\n+        ...templateContext,\\n+        stdout: result.stdout,\\n+        stderr: result.stderr,\\n+        exitCode: result.exitCode,\\n+      });\\n+    }\\n+\\n+    return output;\\n+  }\\n+\\n+  /**\\n+   * Apply JavaScript transform to output\\n+   */\\n+  private async applyJavaScriptTransform(\\n+    transformJs: string,\\n+    output: unknown,\\n+    context: Record<string, unknown>\\n+  ): Promise<unknown> {\\n+    if (!this.sandbox) {\\n+      this.sandbox = createSecureSandbox();\\n+    }\\n+\\n+    const code = `\\n+      const output = ${JSON.stringify(output)};\\n+      const context = ${JSON.stringify(context)};\\n+      const args = context.args || {};\\n+      const pr = context.pr || {};\\n+      const files = context.files || [];\\n+      const outputs = context.outputs || {};\\n+      const env = context.env || {};\\n+\\n+      ${transformJs}\\n+    `;\\n+\\n+    try {\\n+      return await compileAndRun(this.sandbox, code, { timeout: 5000 });\\n+    } catch (error) {\\n+      logger.error(`JavaScript transform error: ${error}`);\\n+      throw error;\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Convert custom tools to MCP tool format\\n+   */\\n+  toMcpTools(): Array<{\\n+    name: string;\\n+    description?: string;\\n+    inputSchema?: Record<string, unknown>;\\n+    handler: (args: Record<string, unknown>) => Promise<unknown>;\\n+  }> {\\n+    return Array.from(this.tools.values()).map(tool => ({\\n+      name: tool.name,\\n+      description: tool.description,\\n+      inputSchema: tool.inputSchema as Record<string, unknown>,\\n+      handler: async (args: Record<string, unknown>) => {\\n+        return this.execute(tool.name, args);\\n+      },\\n+    }));\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/providers/github-ops-provider.ts\",\"additions\":2,\"deletions\":2,\"changes\":113,\"patch\":\"diff --git a/src/providers/github-ops-provider.ts b/src/providers/github-ops-provider.ts\\nindex 2e7cef21..97d4bc75 100644\\n--- a/src/providers/github-ops-provider.ts\\n+++ b/src/providers/github-ops-provider.ts\\n@@ -2,7 +2,7 @@ import { CheckProvider, CheckProviderConfig } from './check-provider.interface';\\n import { PRInfo } from '../pr-analyzer';\\n import { ReviewSummary } from '../reviewer';\\n import Sandbox from '@nyariv/sandboxjs';\\n-import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n+import { createSecureSandbox } from '../utils/sandbox';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n import { logger } from '../logger';\\n \\n@@ -24,7 +24,7 @@ export class GitHubOpsProvider extends CheckProvider {\\n   }\\n \\n   getSupportedConfigKeys(): string[] {\\n-    return ['op', 'values', 'value', 'value_js'];\\n+    return ['op', 'values', 'value'];\\n   }\\n \\n   async isAvailable(): Promise<boolean> {\\n@@ -47,7 +47,6 @@ export class GitHubOpsProvider extends CheckProvider {\\n       op: string;\\n       values?: string[] | string;\\n       value?: string;\\n-      value_js?: string;\\n     };\\n \\n     // IMPORTANT: Always prefer authenticated octokit from event context (GitHub App or token)\\n@@ -225,48 +224,83 @@ export class GitHubOpsProvider extends CheckProvider {\\n \\n     let values: string[] = await renderValues(valuesRaw);\\n \\n-    if (cfg.value_js && cfg.value_js.trim()) {\\n-      try {\\n-        // Evaluate user-provided value_js in a restricted sandbox (no process/global exposure)\\n-        const sandbox = this.getSecureSandbox();\\n-\\n-        // Build dependency outputs map (mirrors Liquid context construction)\\n-        const depOutputs: Record<string, unknown> = {};\\n-        if (dependencyResults) {\\n-          for (const [name, result] of dependencyResults.entries()) {\\n-            const summary = result as ReviewSummary & { output?: unknown };\\n-            depOutputs[name] = summary.output !== undefined ? summary.output : summary;\\n+    // Flatten helpers: allow a single Liquid-rendered value to represent\\n+    // a JSON array (e.g., \\\"{{ outputs['issue-assistant'].labels | json }}\\\")\\n+    // or a newline-separated list. This makes explicit YAML configs expressive\\n+    // without re-introducing value_js.\\n+    try {\\n+      const flattened: string[] = [];\\n+      for (const v of values) {\\n+        const t = String(v ?? '').trim();\\n+        if (!t) continue;\\n+        let expanded = false;\\n+        // JSON array expansion\\n+        if (t.startsWith('[') && t.endsWith(']')) {\\n+          try {\\n+            const arr = JSON.parse(t);\\n+            if (Array.isArray(arr)) {\\n+              for (const x of arr) flattened.push(String(x ?? ''));\\n+              expanded = true;\\n+            }\\n+          } catch {}\\n+        }\\n+        if (expanded) continue;\\n+        // Newline-separated fallback (one per line)\\n+        if (t.includes('\\\\n')) {\\n+          for (const line of t.split('\\\\n')) {\\n+            const s = line.trim();\\n+            if (s) flattened.push(s);\\n           }\\n+          expanded = true;\\n         }\\n+        if (!expanded) flattened.push(t);\\n+      }\\n+      values = flattened;\\n+    } catch {}\\n \\n-        const res = compileAndRun<unknown>(\\n-          sandbox,\\n-          cfg.value_js,\\n-          { pr: prInfo, values, outputs: depOutputs },\\n-          { injectLog: true, wrapFunction: true, logPrefix: '[github:value_js]' }\\n-        );\\n-        if (typeof res === 'string') values = [res];\\n-        else if (Array.isArray(res)) values = (res as unknown[]).map(v => String(v));\\n-      } catch (e) {\\n-        const msg = e instanceof Error ? e.message : String(e);\\n+    // Expose dependency outputs to value_js for convenience (generic map)\\n+    const depOutputs: Record<string, unknown> = {};\\n+    if (dependencyResults) {\\n+      for (const [name, result] of dependencyResults.entries()) {\\n+        const summary = result as ReviewSummary & { output?: unknown };\\n+        depOutputs[name] = summary.output !== undefined ? summary.output : summary;\\n+      }\\n+    }\\n+\\n+    // Provider-side normalization replaces legacy value_js usage\\n+    const sanitizeLabel = (s: string) =>\\n+      s\\n+        .replace(/[^A-Za-z0-9:\\\\/\\\\- ]/g, '')\\n+        .replace(/\\\\/{2,}/g, '/')\\n+        .trim();\\n+    values = (Array.isArray(values) ? values : [])\\n+      .map(v => String(v ?? ''))\\n+      .map(sanitizeLabel)\\n+      .filter(Boolean);\\n+\\n+    // Fallback: if values are still empty, try deriving from dependency outputs\\n+    // 1) Common pattern: outputs.<dep>.labels (e.g., from issue-assistant)\\n+    if (values.length === 0 && Object.keys(depOutputs).length > 0) {\\n+      try {\\n+        const lbls: string[] = [];\\n+        for (const obj of Object.values(depOutputs)) {\\n+          const labelsAny = (obj as any)?.labels;\\n+          if (Array.isArray(labelsAny)) {\\n+            for (const v of labelsAny) lbls.push(String(v ?? ''));\\n+          }\\n+        }\\n+        const norm = lbls\\n+          .map(s => s.trim())\\n+          .filter(Boolean)\\n+          .map(s => s.replace(/[^A-Za-z0-9:\\\\/\\\\- ]/g, '').replace(/\\\\/{2,}/g, '/'));\\n+        values = Array.from(new Set(norm));\\n         if (process.env.VISOR_DEBUG === 'true') {\\n-          logger.warn(`[github-ops] value_js_error: ${msg}`);\\n+          logger.info(`[github-ops] derived values from deps.labels: ${JSON.stringify(values)}`);\\n         }\\n-        return {\\n-          issues: [\\n-            {\\n-              file: 'system',\\n-              line: 0,\\n-              ruleId: 'github/value_js_error',\\n-              message: `value_js evaluation failed: ${msg}`,\\n-              severity: 'error',\\n-              category: 'logic',\\n-            },\\n-          ],\\n-        };\\n-      }\\n+      } catch {}\\n     }\\n \\n+    // 2) Fallback: outputs.<dep>.tags based derivation (overview-style)\\n     // Fallback: if values are still empty, try deriving from dependency outputs (common pattern: outputs.<dep>.tags)\\n     if (values.length === 0 && dependencyResults && dependencyResults.size > 0) {\\n       try {\\n@@ -291,8 +325,7 @@ export class GitHubOpsProvider extends CheckProvider {\\n       } catch {}\\n     }\\n \\n-    // Trim, drop empty, and de-duplicate values regardless of source\\n-    values = values.map(v => v.trim()).filter(v => v.length > 0);\\n+    // Trim (already sanitized), drop empty, and de-duplicate values regardless of source\\n     values = Array.from(new Set(values));\\n \\n     try {\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/human-input-check-provider.ts\",\"additions\":5,\"deletions\":1,\"changes\":196,\"patch\":\"diff --git a/src/providers/human-input-check-provider.ts b/src/providers/human-input-check-provider.ts\\nindex ec594320..c49da2e2 100644\\n--- a/src/providers/human-input-check-provider.ts\\n+++ b/src/providers/human-input-check-provider.ts\\n@@ -3,6 +3,8 @@ import { PRInfo } from '../pr-analyzer';\\n import { ReviewSummary } from '../reviewer';\\n import { HumanInputRequest } from '../types/config';\\n import { interactivePrompt, simplePrompt } from '../utils/interactive-prompt';\\n+import { Liquid } from 'liquidjs';\\n+import { createExtendedLiquid } from '../liquid-extensions';\\n import { tryReadStdin } from '../utils/stdin-reader';\\n import * as fs from 'fs';\\n import * as path from 'path';\\n@@ -27,6 +29,7 @@ import * as path from 'path';\\n  * ```\\n  */\\n export class HumanInputCheckProvider extends CheckProvider {\\n+  private liquid?: Liquid;\\n   /**\\n    * @deprecated Use ExecutionContext.cliMessage instead\\n    * Kept for backward compatibility\\n@@ -92,6 +95,74 @@ export class HumanInputCheckProvider extends CheckProvider {\\n     return true;\\n   }\\n \\n+  /** Build a template context for Liquid rendering */\\n+  private buildTemplateContext(\\n+    prInfo: PRInfo,\\n+    dependencyResults?: Map<string, ReviewSummary>,\\n+    outputHistory?: Map<string, unknown[]>,\\n+    _context?: ExecutionContext\\n+  ): Record<string, unknown> {\\n+    const ctx: Record<string, unknown> = {};\\n+    // pr context\\n+    try {\\n+      ctx.pr = {\\n+        number: prInfo.number,\\n+        title: prInfo.title,\\n+        body: prInfo.body,\\n+        author: prInfo.author,\\n+        base: prInfo.base,\\n+        head: prInfo.head,\\n+        files: (prInfo.files || []).map(f => ({\\n+          filename: f.filename,\\n+          status: f.status,\\n+          additions: f.additions,\\n+          deletions: f.deletions,\\n+          changes: f.changes,\\n+        })),\\n+      };\\n+    } catch {}\\n+    // event + env\\n+    try {\\n+      const safeEnv = (() => {\\n+        try {\\n+          const { buildSandboxEnv } = require('../utils/env-exposure');\\n+          return buildSandboxEnv(process.env);\\n+        } catch {\\n+          return {} as Record<string, string>;\\n+        }\\n+      })();\\n+      (ctx as any).event = { event_name: (prInfo as any)?.eventType || 'manual' };\\n+      (ctx as any).env = safeEnv;\\n+    } catch {}\\n+    // utils helpers\\n+    (ctx as any).utils = {\\n+      now: new Date().toISOString(),\\n+      today: new Date().toISOString().split('T')[0],\\n+    };\\n+    // outputs: expose raw outputs from dependency results\\n+    const outputs: Record<string, unknown> = {};\\n+    const outputsRaw: Record<string, unknown> = {};\\n+    if (dependencyResults) {\\n+      for (const [name, res] of dependencyResults.entries()) {\\n+        const summary = res as ReviewSummary & { output?: unknown };\\n+        if (typeof name === 'string' && name.endsWith('-raw')) {\\n+          outputsRaw[name.slice(0, -4)] = summary.output !== undefined ? summary.output : summary;\\n+        } else {\\n+          outputs[name] = summary.output !== undefined ? summary.output : summary;\\n+        }\\n+      }\\n+    }\\n+    ctx.outputs = outputs;\\n+    (ctx as any).outputs_raw = outputsRaw;\\n+    // outputs_history: expose full history if available\\n+    const hist: Record<string, unknown[]> = {};\\n+    if (outputHistory) {\\n+      for (const [k, v] of outputHistory.entries()) hist[k] = Array.isArray(v) ? v : [];\\n+    }\\n+    (ctx as any).outputs_history = hist;\\n+    return ctx;\\n+  }\\n+\\n   /**\\n    * Check if a string looks like a file path\\n    */\\n@@ -104,6 +175,37 @@ export class HumanInputCheckProvider extends CheckProvider {\\n    * Removes potentially dangerous characters while preserving useful input\\n    */\\n   private sanitizeInput(input: string): string {\\n+    // Heuristic: collapse accidental per-character duplication (\\\"stutter\\\") often caused by\\n+    // TTY echo races. We only apply this when most adjacent ASCII chars are doubled.\\n+    const collapseStutter = (s: string): string => {\\n+      if (!s || s.length < 4) return s;\\n+      let dupPairs = 0;\\n+      let pairs = 0;\\n+      for (let i = 0; i + 1 < s.length; i++) {\\n+        const a = s[i];\\n+        const b = s[i + 1];\\n+        if (/^[\\\\x20-\\\\x7E]$/.test(a) && /^[\\\\x20-\\\\x7E]$/.test(b)) {\\n+          pairs++;\\n+          if (a === b) dupPairs++;\\n+        }\\n+      }\\n+      const ratio = pairs > 0 ? dupPairs / pairs : 0;\\n+      if (ratio < 0.5) return s; // keep as-is unless roughly half of pairs are doubled\\n+      let out = '';\\n+      for (let i = 0; i < s.length; i++) {\\n+        const a = s[i];\\n+        const b = i + 1 < s.length ? s[i + 1] : '';\\n+        if (b && a === b) {\\n+          out += a;\\n+          i++; // skip the duplicate\\n+        } else {\\n+          out += a;\\n+        }\\n+      }\\n+      return out;\\n+    };\\n+\\n+    input = collapseStutter(input);\\n     // Remove null bytes (C-string injection)\\n     let sanitized = input.replace(/\\\\0/g, '');\\n \\n@@ -171,13 +273,31 @@ export class HumanInputCheckProvider extends CheckProvider {\\n     config: CheckProviderConfig,\\n     context?: ExecutionContext\\n   ): Promise<string> {\\n-    const prompt = config.prompt || 'Please provide input:';\\n+    // Test runner mock support: if a mock is provided for this step, use it\\n+    try {\\n+      const mockVal = context?.hooks?.mockForStep?.(checkName);\\n+      if (mockVal !== undefined && mockVal !== null) {\\n+        const s = String(mockVal);\\n+        return s;\\n+      }\\n+    } catch {}\\n+    const prompt = (config.prompt as string) || 'Please provide input:';\\n     const placeholder = (config.placeholder as string | undefined) || 'Enter your response...';\\n     const allowEmpty = (config.allow_empty as boolean | undefined) ?? false;\\n     const multiline = (config.multiline as boolean | undefined) ?? false;\\n     const timeout = config.timeout ? config.timeout * 1000 : undefined; // Convert to ms\\n     const defaultValue = config.default as string | undefined;\\n \\n+    // In test/CI modes, never block for input. Use default or empty string.\\n+    const testMode = String(process.env.VISOR_TEST_MODE || '').toLowerCase() === 'true';\\n+    const ciMode =\\n+      String(process.env.CI || '').toLowerCase() === 'true' ||\\n+      String(process.env.GITHUB_ACTIONS || '').toLowerCase() === 'true';\\n+    if (testMode || ciMode) {\\n+      const val = (config.default as string | undefined) || '';\\n+      return val;\\n+    }\\n+\\n     // Get cliMessage from context (new way) or static property (backward compat)\\n     const cliMessage = context?.cliMessage ?? HumanInputCheckProvider.cliMessage;\\n \\n@@ -270,17 +390,85 @@ export class HumanInputCheckProvider extends CheckProvider {\\n     const checkName = config.checkName || 'human-input';\\n \\n     try {\\n+      // Render Liquid templates in prompt/placeholder if any\\n+      try {\\n+        this.liquid =\\n+          this.liquid || createExtendedLiquid({ strictVariables: false, strictFilters: false });\\n+        const tctx = this.buildTemplateContext(\\n+          _prInfo,\\n+          _dependencyResults,\\n+          (config as any).__outputHistory as Map<string, unknown[]> | undefined,\\n+          context\\n+        );\\n+        if (typeof config.prompt === 'string') {\\n+          let rendered = await this.liquid.parseAndRender(config.prompt, tctx);\\n+          // If Liquid markers remain (e.g., due to nested/guarded templates), try a second pass\\n+          if (/\\\\{\\\\{|\\\\{%/.test(rendered)) {\\n+            try {\\n+              rendered = await this.liquid.parseAndRender(rendered, tctx);\\n+            } catch {}\\n+          }\\n+          // Expose the final rendered prompt to the test runner (like AI provider does)\\n+          try {\\n+            const stepName = (config as any).checkName || 'unknown';\\n+            context?.hooks?.onPromptCaptured?.({\\n+              step: String(stepName),\\n+              provider: 'human-input',\\n+              prompt: rendered,\\n+            });\\n+          } catch {}\\n+          config = { ...config, prompt: rendered };\\n+        }\\n+        if (typeof config.placeholder === 'string') {\\n+          let ph = await this.liquid.parseAndRender(config.placeholder as string, tctx);\\n+          if (/\\\\{\\\\{|\\\\{%/.test(ph)) {\\n+            try {\\n+              ph = await this.liquid.parseAndRender(ph, tctx);\\n+            } catch {}\\n+          }\\n+          (config as any).placeholder = ph;\\n+        }\\n+      } catch (e) {\\n+        // Always show Liquid errors with a helpful snippet and caret\\n+        const err: any = e || {};\\n+        const raw = String((config as any)?.prompt || '');\\n+        const lines = raw.split(/\\\\r?\\\\n/);\\n+        const lineNum: number = Number(err.line || err?.token?.line || err?.location?.line || 0);\\n+        const colNum: number = Number(err.col || err?.token?.col || err?.location?.col || 0);\\n+        let snippet = '';\\n+        if (lineNum > 0) {\\n+          const start = Math.max(1, lineNum - 3);\\n+          const end = Math.max(lineNum + 2, lineNum);\\n+          const width = String(end).length;\\n+          for (let i = start; i <= Math.min(end, lines.length); i++) {\\n+            const ln = `${String(i).padStart(width, ' ')} | ${lines[i - 1] ?? ''}`;\\n+            snippet += ln + '\\\\n';\\n+            if (i === lineNum) {\\n+              const caretPad = ' '.repeat(Math.max(0, colNum > 1 ? colNum - 1 : 0) + width + 3);\\n+              snippet += caretPad + '^\\\\n';\\n+            }\\n+          }\\n+        }\\n+        try {\\n+          console.error(\\n+            `‚ö†Ô∏è  human-input: Liquid render failed: ${\\n+              e instanceof Error ? e.message : String(e)\\n+            }\\\\n${snippet}`\\n+          );\\n+        } catch {}\\n+        // Continue with raw strings as a fallback\\n+      }\\n       // Get user input (pass context for non-static state)\\n       const userInput = await this.getUserInput(checkName, config, context);\\n \\n       // Sanitize input to prevent injection attacks in dependent checks\\n       const sanitizedInput = this.sanitizeInput(userInput);\\n \\n-      // Return the input as the check output (stored in output field for dependent checks)\\n+      // Return structured output with timestamp for consistent history/merging\\n       return {\\n         issues: [],\\n-        output: sanitizedInput,\\n-      } as ReviewSummary & { output: string };\\n+        output: { text: sanitizedInput, ts: Date.now() },\\n+      } as ReviewSummary & { output: { text: string; ts: number } };\\n     } catch (error) {\\n       // If there's an error getting input, return an error issue\\n       return {\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/log-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":17,\"patch\":\"diff --git a/src/providers/log-check-provider.ts b/src/providers/log-check-provider.ts\\nindex ebdbd30f..9c6332d1 100644\\n--- a/src/providers/log-check-provider.ts\\n+++ b/src/providers/log-check-provider.ts\\n@@ -1,4 +1,4 @@\\n-import { CheckProvider, CheckProviderConfig } from './check-provider.interface';\\n+import { CheckProvider, CheckProviderConfig, ExecutionContext } from './check-provider.interface';\\n import { PRInfo } from '../pr-analyzer';\\n import { ReviewSummary } from '../reviewer';\\n import { Liquid } from 'liquidjs';\\n@@ -62,7 +62,7 @@ export class LogCheckProvider extends CheckProvider {\\n     prInfo: PRInfo,\\n     config: CheckProviderConfig,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n-    _sessionInfo?: { parentSessionId?: string; reuseSession?: boolean }\\n+    context?: ExecutionContext\\n   ): Promise<ReviewSummary> {\\n     const message = config.message as string;\\n     const level = (config.level as LogLevel) || 'info';\\n@@ -77,7 +77,8 @@ export class LogCheckProvider extends CheckProvider {\\n       includePrContext,\\n       includeDependencies,\\n       includeMetadata,\\n-      config.__outputHistory as Map<string, unknown[]> | undefined\\n+      config.__outputHistory as Map<string, unknown[]> | undefined,\\n+      context\\n     );\\n \\n     // Render the log message template\\n@@ -113,7 +114,8 @@ export class LogCheckProvider extends CheckProvider {\\n     _includePrContext: boolean = true,\\n     _includeDependencies: boolean = true,\\n     includeMetadata: boolean = true,\\n-    outputHistory?: Map<string, unknown[]>\\n+    outputHistory?: Map<string, unknown[]>,\\n+    executionContext?: ExecutionContext\\n   ): Record<string, unknown> {\\n     const context: Record<string, unknown> = {};\\n \\n@@ -194,6 +196,13 @@ export class LogCheckProvider extends CheckProvider {\\n       };\\n     }\\n \\n+    // Add workflow inputs if available\\n+    const workflowInputs = executionContext?.workflowInputs || {};\\n+    logger.debug(\\n+      `[LogProvider] Adding ${Object.keys(workflowInputs).length} workflow inputs to context`\\n+    );\\n+    context.inputs = workflowInputs;\\n+\\n     return context;\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/mcp-check-provider.ts\",\"additions\":2,\"deletions\":1,\"changes\":78,\"patch\":\"diff --git a/src/providers/mcp-check-provider.ts b/src/providers/mcp-check-provider.ts\\nindex 9b44a6d2..8e7a6557 100644\\n--- a/src/providers/mcp-check-provider.ts\\n+++ b/src/providers/mcp-check-provider.ts\\n@@ -11,13 +11,15 @@ import { StreamableHTTPClientTransport } from '@modelcontextprotocol/sdk/client/\\n import Sandbox from '@nyariv/sandboxjs';\\n import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n import { EnvironmentResolver } from '../utils/env-resolver';\\n+import { CustomToolExecutor } from './custom-tool-executor';\\n+import { CustomToolDefinition } from '../types/config';\\n \\n /**\\n  * MCP Check Provider Configuration\\n  */\\n export interface McpCheckConfig extends CheckProviderConfig {\\n-  /** Transport type: stdio (default), sse (legacy), or http (streamable HTTP) */\\n-  transport?: 'stdio' | 'sse' | 'http';\\n+  /** Transport type: stdio (default), sse (legacy), http (streamable HTTP), or custom (YAML-defined tools) */\\n+  transport?: 'stdio' | 'sse' | 'http' | 'custom';\\n   /** Command to execute (for stdio transport) */\\n   command?: string;\\n   /** Command arguments (for stdio transport) */\\n@@ -48,11 +50,12 @@ export interface McpCheckConfig extends CheckProviderConfig {\\n \\n /**\\n  * Check provider that calls MCP tools directly\\n- * Supports stdio, SSE (legacy), and Streamable HTTP transports\\n+ * Supports stdio, SSE (legacy), Streamable HTTP transports, and custom YAML-defined tools\\n  */\\n export class McpCheckProvider extends CheckProvider {\\n   private liquid: Liquid;\\n   private sandbox?: Sandbox;\\n+  private customToolExecutor?: CustomToolExecutor;\\n \\n   constructor() {\\n     super();\\n@@ -63,6 +66,17 @@ export class McpCheckProvider extends CheckProvider {\\n     });\\n   }\\n \\n+  /**\\n+   * Set custom tools for this provider\\n+   */\\n+  setCustomTools(tools: Record<string, CustomToolDefinition>): void {\\n+    if (!this.customToolExecutor) {\\n+      this.customToolExecutor = new CustomToolExecutor(tools);\\n+    } else {\\n+      this.customToolExecutor.registerTools(tools);\\n+    }\\n+  }\\n+\\n   /**\\n    * Create a secure sandbox for JavaScript execution\\n    * - Uses Sandbox.SAFE_GLOBALS which excludes: Function, eval, require, process, etc.\\n@@ -78,7 +92,7 @@ export class McpCheckProvider extends CheckProvider {\\n   }\\n \\n   getDescription(): string {\\n-    return 'Call MCP tools directly using stdio, SSE, or Streamable HTTP transport';\\n+    return 'Call MCP tools directly using stdio, SSE, HTTP, or custom YAML-defined tools';\\n   }\\n \\n   async validateConfig(config: unknown): Promise<boolean> {\\n@@ -129,8 +143,15 @@ export class McpCheckProvider extends CheckProvider {\\n         logger.error(`Invalid URL format for MCP ${transport} transport: ${cfg.url}`);\\n         return false;\\n       }\\n+    } else if (transport === 'custom') {\\n+      // For custom transport, validation is delegated to CustomToolExecutor\\n+      // The tool must exist in the configuration's tools section\\n+      // This will be validated at execution time when the tool is looked up\\n+      logger.debug(`MCP custom transport will validate tool '${cfg.method}' at execution time`);\\n     } else {\\n-      logger.error(`Invalid MCP transport: ${transport}. Must be 'stdio', 'sse', or 'http'`);\\n+      logger.error(\\n+        `Invalid MCP transport: ${transport}. Must be 'stdio', 'sse', 'http', or 'custom'`\\n+      );\\n       return false;\\n     }\\n \\n@@ -184,7 +205,7 @@ export class McpCheckProvider extends CheckProvider {\\n       }\\n \\n       // Create MCP client and execute method\\n-      const result = await this.executeMcpMethod(cfg, methodArgs);\\n+      const result = await this.executeMcpMethod(cfg, methodArgs, prInfo, dependencyResults);\\n \\n       // Apply transforms if specified\\n       let finalOutput = result;\\n@@ -297,12 +318,49 @@ export class McpCheckProvider extends CheckProvider {\\n    */\\n   private async executeMcpMethod(\\n     config: McpCheckConfig,\\n-    methodArgs: Record<string, unknown>\\n+    methodArgs: Record<string, unknown>,\\n+    prInfo?: PRInfo,\\n+    dependencyResults?: Map<string, ReviewSummary>\\n   ): Promise<unknown> {\\n     const transport = config.transport || 'stdio';\\n     const timeout = (config.timeout || 60) * 1000; // Convert to milliseconds\\n \\n-    if (transport === 'stdio') {\\n+    if (transport === 'custom') {\\n+      // Execute custom YAML-defined tool\\n+      if (!this.customToolExecutor) {\\n+        throw new Error(\\n+          'No custom tools available. Define tools in the \\\"tools\\\" section of your configuration.'\\n+        );\\n+      }\\n+\\n+      const tool = this.customToolExecutor.getTool(config.method);\\n+      if (!tool) {\\n+        throw new Error(\\n+          `Custom tool not found: ${config.method}. Available tools: ${this.customToolExecutor\\n+            .getTools()\\n+            .map(t => t.name)\\n+            .join(', ')}`\\n+        );\\n+      }\\n+\\n+      // Build context for custom tool execution\\n+      const context = {\\n+        pr: prInfo\\n+          ? {\\n+              number: prInfo.number,\\n+              title: prInfo.title,\\n+              author: prInfo.author,\\n+              branch: prInfo.head,\\n+              base: prInfo.base,\\n+            }\\n+          : undefined,\\n+        files: prInfo?.files,\\n+        outputs: this.buildOutputContext(dependencyResults),\\n+        env: this.getSafeEnvironmentVariables(),\\n+      };\\n+\\n+      return await this.customToolExecutor.execute(config.method, methodArgs, context);\\n+    } else if (transport === 'stdio') {\\n       return await this.executeStdioMethod(config, methodArgs, timeout);\\n     } else if (transport === 'sse') {\\n       return await this.executeSseMethod(config, methodArgs, timeout);\\n@@ -404,7 +462,7 @@ export class McpCheckProvider extends CheckProvider {\\n   ): Promise<unknown> {\\n     const transport = new StdioClientTransport({\\n       command: config.command!,\\n-      args: config.args,\\n+      args: config.command_args as string[] | undefined,\\n       env: config.env,\\n       cwd: config.workingDirectory,\\n     });\\n@@ -664,7 +722,7 @@ export class McpCheckProvider extends CheckProvider {\\n       'type',\\n       'transport',\\n       'command',\\n-      'args',\\n+      'command_args',\\n       'env',\\n       'workingDirectory',\\n       'url',\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/script-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":25,\"patch\":\"diff --git a/src/providers/script-check-provider.ts b/src/providers/script-check-provider.ts\\nindex 4d8012a6..70d5ae9a 100644\\n--- a/src/providers/script-check-provider.ts\\n+++ b/src/providers/script-check-provider.ts\\n@@ -70,6 +70,14 @@ export class ScriptCheckProvider extends CheckProvider {\\n       (_sessionInfo as any)?.stageHistoryBase as Record<string, number> | undefined,\\n       { attachMemoryReadHelpers: false }\\n     );\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        const hist: any = (ctx as any).outputs_history || {};\\n+        const len = Array.isArray(hist['refine']) ? hist['refine'].length : 0;\\n+\\n+        console.error(`[script] history.refine.len=${len}`);\\n+      }\\n+    } catch {}\\n \\n     // Attach synchronous memory ops consistent with memory provider\\n     const { ops, needsSave } = createSyncMemoryOps(memoryStore);\\n@@ -85,7 +93,7 @@ export class ScriptCheckProvider extends CheckProvider {\\n         { ...ctx },\\n         {\\n           injectLog: true,\\n-          wrapFunction: false,\\n+          wrapFunction: true,\\n           logPrefix: '[script]',\\n         }\\n       );\\n@@ -120,7 +128,20 @@ export class ScriptCheckProvider extends CheckProvider {\\n       logger.warn(`[script] memory save failed: ${e instanceof Error ? e.message : String(e)}`);\\n     }\\n \\n-    return { issues: [], output: result } as ReviewSummary & { output: unknown };\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        const name = String((config as any).checkName || '');\\n+        const t = typeof result;\\n+        console.error(\\n+          `[script-return] ${name} outputType=${t} hasArray=${Array.isArray(result)} hasObj=${result && typeof result === 'object'}`\\n+        );\\n+      }\\n+    } catch {}\\n+    const out: any = { issues: [], output: result } as ReviewSummary & { output: unknown };\\n+    try {\\n+      (out as any).__histTracked = true;\\n+    } catch {}\\n+    return out;\\n   }\\n \\n   getSupportedConfigKeys(): string[] {\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/workflow-check-provider.ts\",\"additions\":18,\"deletions\":0,\"changes\":635,\"patch\":\"diff --git a/src/providers/workflow-check-provider.ts b/src/providers/workflow-check-provider.ts\\nnew file mode 100644\\nindex 00000000..f45c613a\\n--- /dev/null\\n+++ b/src/providers/workflow-check-provider.ts\\n@@ -0,0 +1,635 @@\\n+/**\\n+ * Workflow check provider - executes reusable workflows as checks\\n+ */\\n+\\n+import { CheckProvider, CheckProviderConfig, ExecutionContext } from './check-provider.interface';\\n+import { PRInfo } from '../pr-analyzer';\\n+import { ReviewSummary } from '../reviewer';\\n+import { WorkflowRegistry } from '../workflow-registry';\\n+import { WorkflowExecutor } from '../workflow-executor';\\n+import { logger } from '../logger';\\n+import { WorkflowDefinition, WorkflowExecutionContext } from '../types/workflow';\\n+import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n+import { Liquid } from 'liquidjs';\\n+\\n+/**\\n+ * Provider that executes workflows as checks\\n+ */\\n+export class WorkflowCheckProvider extends CheckProvider {\\n+  private registry: WorkflowRegistry;\\n+  private executor: WorkflowExecutor;\\n+  private liquid: Liquid;\\n+\\n+  constructor() {\\n+    super();\\n+    this.registry = WorkflowRegistry.getInstance();\\n+    this.executor = new WorkflowExecutor();\\n+    this.liquid = new Liquid();\\n+  }\\n+\\n+  getName(): string {\\n+    return 'workflow';\\n+  }\\n+\\n+  getDescription(): string {\\n+    return 'Executes reusable workflow definitions as checks';\\n+  }\\n+\\n+  async validateConfig(config: unknown): Promise<boolean> {\\n+    const cfg = config as CheckProviderConfig & { workflow?: string; config?: string };\\n+\\n+    // Two supported modes:\\n+    // 1) workflow: <id> (pre-registered in WorkflowRegistry via imports)\\n+    // 2) config: <path|url> (load a Visor config file and execute its steps as a workflow)\\n+    if (!cfg.workflow && !cfg.config) {\\n+      logger.error('Workflow provider requires either \\\"workflow\\\" (id) or \\\"config\\\" (path)');\\n+      return false;\\n+    }\\n+\\n+    // If using workflow id, verify presence in registry now\\n+    if (cfg.workflow) {\\n+      if (!this.registry.has(cfg.workflow as string)) {\\n+        logger.error(`Workflow '${cfg.workflow}' not found in registry`);\\n+        return false;\\n+      }\\n+    }\\n+\\n+    // For config path mode we cannot fully validate existence here (no base path);\\n+    // execution will resolve it relative to the parent working directory and fail fast if missing.\\n+    return true;\\n+  }\\n+\\n+  async execute(\\n+    prInfo: PRInfo,\\n+    config: CheckProviderConfig,\\n+    dependencyResults?: Map<string, ReviewSummary>,\\n+    context?: ExecutionContext\\n+  ): Promise<ReviewSummary> {\\n+    const cfg = config as CheckProviderConfig & { workflow?: string; config?: string };\\n+    const isConfigPathMode = !!cfg.config && !cfg.workflow;\\n+\\n+    // Resolve workflow definition\\n+    let workflow: WorkflowDefinition | undefined;\\n+    let workflowId = cfg.workflow as string | undefined;\\n+\\n+    if (isConfigPathMode) {\\n+      const parentCwd = ((context as any)?._parentContext?.workingDirectory ||\\n+        (context as any)?.workingDirectory ||\\n+        process.cwd()) as string;\\n+      workflow = await this.loadWorkflowFromConfigPath(String(cfg.config), parentCwd);\\n+      workflowId = workflow.id;\\n+      logger.info(`Executing workflow from config '${cfg.config}' as '${workflowId}'`);\\n+    } else {\\n+      workflowId = String(cfg.workflow);\\n+      workflow = this.registry.get(workflowId);\\n+      if (!workflow) {\\n+        throw new Error(`Workflow '${workflowId}' not found in registry`);\\n+      }\\n+      logger.info(`Executing workflow '${workflowId}'`);\\n+    }\\n+\\n+    // Prepare inputs\\n+    const inputs = await this.prepareInputs(workflow, config, prInfo, dependencyResults);\\n+\\n+    // Validate inputs\\n+    const validation = this.registry.validateInputs(workflow, inputs);\\n+    if (!validation.valid) {\\n+      const errors = validation.errors?.map(e => `${e.path}: ${e.message}`).join(', ');\\n+      throw new Error(`Invalid workflow inputs: ${errors}`);\\n+    }\\n+\\n+    // Apply overrides to workflow steps if specified\\n+    const modifiedWorkflow = this.applyOverrides(workflow, config);\\n+\\n+    // M3: Check if we're in state-machine mode and should delegate to engine\\n+    const engineMode = (context as any)?._engineMode;\\n+    if (engineMode === 'state-machine') {\\n+      // Delegate to state machine engine for nested workflow execution\\n+      logger.info(`[WorkflowProvider] Delegating workflow '${workflowId}' to state machine engine`);\\n+      return await this.executeViaStateMachine(\\n+        modifiedWorkflow,\\n+        inputs,\\n+        config,\\n+        prInfo,\\n+        dependencyResults,\\n+        context\\n+      );\\n+    }\\n+\\n+    // Legacy mode: Execute the workflow using WorkflowExecutor\\n+    const executionContext: WorkflowExecutionContext = {\\n+      instanceId: `${workflowId}-${Date.now()}`,\\n+      parentCheckId: config.checkName,\\n+      inputs,\\n+      stepResults: new Map(),\\n+    };\\n+\\n+    const result = await this.executor.execute(modifiedWorkflow, executionContext, {\\n+      prInfo,\\n+      dependencyResults,\\n+      context,\\n+    });\\n+\\n+    // Map outputs\\n+    const outputs = this.mapOutputs(result, config.output_mapping as Record<string, string>);\\n+\\n+    // Return the review summary with extended fields\\n+    // Note: These extra fields are used by the execution engine but not part of the base interface\\n+    const summary: ReviewSummary = {\\n+      issues: result.issues || [],\\n+    };\\n+\\n+    // Add extended fields as needed by the engine\\n+    (summary as any).score = result.score || 0;\\n+    (summary as any).confidence = result.confidence || 'medium';\\n+    (summary as any).comments = result.comments || [];\\n+    (summary as any).output = outputs;\\n+    (summary as any).content = this.formatWorkflowResult(workflow, result, outputs);\\n+\\n+    return summary;\\n+  }\\n+\\n+  getSupportedConfigKeys(): string[] {\\n+    return [\\n+      'workflow',\\n+      'config',\\n+      'args',\\n+      'overrides',\\n+      'output_mapping',\\n+      'timeout',\\n+      'env',\\n+      'checkName',\\n+    ];\\n+  }\\n+\\n+  async isAvailable(): Promise<boolean> {\\n+    return true; // Always available\\n+  }\\n+\\n+  getRequirements(): string[] {\\n+    return [];\\n+  }\\n+\\n+  /**\\n+   * Prepare inputs for workflow execution\\n+   */\\n+  private async prepareInputs(\\n+    workflow: WorkflowDefinition,\\n+    config: CheckProviderConfig,\\n+    prInfo: PRInfo,\\n+    dependencyResults?: Map<string, ReviewSummary>\\n+  ): Promise<Record<string, unknown>> {\\n+    const inputs: Record<string, unknown> = {};\\n+\\n+    // Start with default values from workflow definition\\n+    if (workflow.inputs) {\\n+      for (const param of workflow.inputs) {\\n+        if (param.default !== undefined) {\\n+          inputs[param.name] = param.default;\\n+        }\\n+      }\\n+    }\\n+\\n+    // Apply user-provided inputs (args)\\n+    const userInputs = config.args || config.workflow_inputs; // Support both for compatibility\\n+    if (userInputs) {\\n+      for (const [key, value] of Object.entries(userInputs)) {\\n+        // Process value if it's a template or expression\\n+        if (typeof value === 'string') {\\n+          // Check if it's a Liquid template\\n+          if (value.includes('{{') || value.includes('{%')) {\\n+            inputs[key] = await this.liquid.parseAndRender(value, {\\n+              pr: prInfo,\\n+              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n+              env: process.env,\\n+            });\\n+          } else {\\n+            inputs[key] = value;\\n+          }\\n+        } else if (typeof value === 'object' && value !== null && 'expression' in value) {\\n+          // JavaScript expression\\n+          const exprValue = value as { expression: string };\\n+          const sandbox = createSecureSandbox();\\n+          inputs[key] = compileAndRun(\\n+            sandbox,\\n+            exprValue.expression,\\n+            {\\n+              pr: prInfo,\\n+              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n+              env: process.env,\\n+            },\\n+            { injectLog: true, logPrefix: `workflow.input.${key}` }\\n+          );\\n+        } else {\\n+          inputs[key] = value;\\n+        }\\n+      }\\n+    }\\n+\\n+    return inputs;\\n+  }\\n+\\n+  /**\\n+   * Apply overrides to workflow steps\\n+   */\\n+  private applyOverrides(\\n+    workflow: WorkflowDefinition,\\n+    config: CheckProviderConfig\\n+  ): WorkflowDefinition {\\n+    const overrideConfig = config.overrides || config.workflow_overrides; // Support both for compatibility\\n+    if (!overrideConfig) {\\n+      return workflow;\\n+    }\\n+\\n+    // Deep clone the workflow\\n+    const modified = JSON.parse(JSON.stringify(workflow));\\n+\\n+    // Apply overrides\\n+    for (const [stepId, overrides] of Object.entries(overrideConfig)) {\\n+      if (modified.steps[stepId]) {\\n+        // Merge overrides with existing step config\\n+        modified.steps[stepId] = {\\n+          ...modified.steps[stepId],\\n+          ...overrides,\\n+        };\\n+      } else {\\n+        logger.warn(`Cannot override non-existent step '${stepId}' in workflow '${workflow.id}'`);\\n+      }\\n+    }\\n+\\n+    return modified;\\n+  }\\n+\\n+  /**\\n+   * Map workflow outputs to check outputs\\n+   */\\n+  private mapOutputs(result: any, outputMapping?: Record<string, string>): Record<string, unknown> {\\n+    if (!outputMapping) {\\n+      return result.output || {};\\n+    }\\n+\\n+    const mapped: Record<string, unknown> = {};\\n+    const workflowOutputs = result.output || {};\\n+\\n+    for (const [checkOutput, workflowOutput] of Object.entries(outputMapping)) {\\n+      if (workflowOutput in workflowOutputs) {\\n+        mapped[checkOutput] = workflowOutputs[workflowOutput];\\n+      } else if (workflowOutput.includes('.')) {\\n+        // Handle nested paths\\n+        const parts = workflowOutput.split('.');\\n+        let value = workflowOutputs;\\n+        for (const part of parts) {\\n+          value = value?.[part];\\n+          if (value === undefined) break;\\n+        }\\n+        mapped[checkOutput] = value;\\n+      }\\n+    }\\n+\\n+    return mapped;\\n+  }\\n+\\n+  /**\\n+   * Format workflow execution result for display\\n+   */\\n+  /**\\n+   * Execute workflow via state machine engine (M3: nested workflows)\\n+   */\\n+  private async executeViaStateMachine(\\n+    workflow: WorkflowDefinition,\\n+    inputs: Record<string, unknown>,\\n+    config: CheckProviderConfig,\\n+    prInfo: PRInfo,\\n+    dependencyResults?: Map<string, ReviewSummary>,\\n+    context?: ExecutionContext\\n+  ): Promise<ReviewSummary> {\\n+    // Import state machine components\\n+    const {\\n+      projectWorkflowToGraph,\\n+      validateWorkflowDepth,\\n+    } = require('../state-machine/workflow-projection');\\n+    const { StateMachineRunner } = require('../state-machine/runner');\\n+    const { ExecutionJournal } = require('../snapshot-store');\\n+    const { MemoryStore } = require('../memory-store');\\n+    const { v4: uuidv4 } = require('uuid');\\n+\\n+    // Extract parent context if available\\n+    const parentContext = (context as any)?._parentContext;\\n+    const parentState = (context as any)?._parentState;\\n+\\n+    // Validate workflow depth\\n+    const currentDepth = parentState?.flags?.currentWorkflowDepth || 0;\\n+    // Prefer parent state's configured limit; fall back to config.limits if present; else default 3\\n+    const maxDepth =\\n+      parentState?.flags?.maxWorkflowDepth ??\\n+      parentContext?.config?.limits?.max_workflow_depth ??\\n+      3;\\n+    validateWorkflowDepth(currentDepth, maxDepth, workflow.id);\\n+\\n+    // Project workflow to dependency graph\\n+    const { config: workflowConfig, checks: checksMetadata } = projectWorkflowToGraph(\\n+      workflow,\\n+      inputs,\\n+      config.checkName || workflow.id\\n+    );\\n+\\n+    // Build isolated child engine context (separate journal/memory to avoid state contamination)\\n+    // Reuse parent's memory config if available, but never the instance\\n+    const parentMemoryCfg =\\n+      (parentContext?.memory &&\\n+        parentContext.memory.getConfig &&\\n+        parentContext.memory.getConfig()) ||\\n+      parentContext?.config?.memory;\\n+\\n+    const childJournal = new ExecutionJournal();\\n+    const childMemory = MemoryStore.createIsolated(parentMemoryCfg);\\n+    try {\\n+      await childMemory.initialize();\\n+    } catch {}\\n+\\n+    const childContext = {\\n+      mode: 'state-machine' as const,\\n+      config: workflowConfig,\\n+      checks: checksMetadata,\\n+      journal: childJournal,\\n+      memory: childMemory,\\n+      workingDirectory: parentContext?.workingDirectory || process.cwd(),\\n+      // Always use a fresh session for nested workflows to isolate history\\n+      sessionId: uuidv4(),\\n+      event: parentContext?.event || prInfo.eventType,\\n+      debug: parentContext?.debug || false,\\n+      maxParallelism: parentContext?.maxParallelism,\\n+      failFast: parentContext?.failFast,\\n+      // Propagate execution hooks (mocks, octokit, etc.) into the child so\\n+      // nested steps can be mocked/observed by the YAML test runner.\\n+      executionContext: (parentContext as any)?.executionContext,\\n+      // Ensure all workflow steps are considered requested to avoid tag/event filtering surprises\\n+      requestedChecks: Object.keys(checksMetadata),\\n+    };\\n+\\n+    // Create child runner with inherited context\\n+    const runner = new StateMachineRunner(childContext);\\n+    const childState = runner.getState();\\n+\\n+    // Set workflow depth for child\\n+    childState.flags.currentWorkflowDepth = currentDepth + 1;\\n+    childState.flags.maxWorkflowDepth = maxDepth;\\n+\\n+    // Set parent references\\n+    childState.parentContext = parentContext;\\n+    childState.parentScope = parentState?.parentScope;\\n+\\n+    // Execute the child workflow\\n+    logger.info(\\n+      `[WorkflowProvider] Executing nested workflow '${workflow.id}' at depth ${currentDepth + 1}`\\n+    );\\n+    const result = await runner.run();\\n+\\n+    // M3: Check for bubbled events and propagate them to parent\\n+    const bubbledEvents = (childContext as any)._bubbledEvents || [];\\n+    if (bubbledEvents.length > 0 && parentContext) {\\n+      if (parentContext.debug) {\\n+        logger.info(`[WorkflowProvider] Bubbling ${bubbledEvents.length} events to parent context`);\\n+      }\\n+\\n+      // Propagate bubbled events to parent\\n+      if (!parentContext._bubbledEvents) {\\n+        (parentContext as any)._bubbledEvents = [];\\n+      }\\n+      (parentContext as any)._bubbledEvents.push(...bubbledEvents);\\n+    }\\n+\\n+    // Aggregate results from all workflow steps\\n+    const allIssues: any[] = [];\\n+    let totalScore = 0;\\n+    let scoreCount = 0;\\n+\\n+    for (const stepResult of Object.values(result.results)) {\\n+      const typedResult = stepResult as any;\\n+      if (typedResult.issues) {\\n+        allIssues.push(...typedResult.issues);\\n+      }\\n+      if (typedResult.score) {\\n+        totalScore += typedResult.score;\\n+        scoreCount++;\\n+      }\\n+    }\\n+\\n+    // Compute workflow outputs\\n+    const outputs = await this.computeWorkflowOutputsFromState(\\n+      workflow,\\n+      inputs,\\n+      result.results,\\n+      prInfo\\n+    );\\n+\\n+    // Map outputs if output_mapping is specified\\n+    const mappedOutputs = this.mapOutputs(\\n+      { output: outputs },\\n+      config.output_mapping as Record<string, string>\\n+    );\\n+\\n+    // Build aggregated summary\\n+    const summary: ReviewSummary = {\\n+      issues: allIssues,\\n+    };\\n+\\n+    (summary as any).score = scoreCount > 0 ? Math.round(totalScore / scoreCount) : 0;\\n+    (summary as any).confidence = 'medium';\\n+    (summary as any).output = mappedOutputs;\\n+    (summary as any).content = this.formatWorkflowResultFromStateMachine(\\n+      workflow,\\n+      result,\\n+      mappedOutputs\\n+    );\\n+\\n+    return summary;\\n+  }\\n+\\n+  /**\\n+   * Compute workflow outputs from state machine execution results\\n+   */\\n+  private async computeWorkflowOutputsFromState(\\n+    workflow: WorkflowDefinition,\\n+    inputs: Record<string, unknown>,\\n+    groupedResults: Record<string, Array<{ checkName: string; output?: unknown; issues?: any[] }>>,\\n+    prInfo: PRInfo\\n+  ): Promise<Record<string, unknown>> {\\n+    const outputs: Record<string, unknown> = {};\\n+\\n+    if (!workflow.outputs) {\\n+      return outputs;\\n+    }\\n+\\n+    const sandbox = createSecureSandbox();\\n+\\n+    // Flatten GroupedCheckResults (group -> CheckResult[]) to a simple map\\n+    // of checkName -> { output, issues } so workflow-level value_js can\\n+    // reference outputs[\\\"security\\\"].issues, etc.\\n+    const flat: Record<string, { output?: unknown; issues?: any[] }> = {};\\n+    try {\\n+      for (const arr of Object.values(groupedResults || {})) {\\n+        for (const item of arr || []) {\\n+          if (!item) continue;\\n+          const name = (item as any).checkName || (item as any).name;\\n+          if (typeof name === 'string' && name) {\\n+            flat[name] = { output: (item as any).output, issues: (item as any).issues };\\n+          }\\n+        }\\n+      }\\n+    } catch {}\\n+\\n+    for (const output of workflow.outputs) {\\n+      if (output.value_js) {\\n+        // JavaScript expression\\n+        outputs[output.name] = compileAndRun(\\n+          sandbox,\\n+          output.value_js,\\n+          {\\n+            inputs,\\n+            steps: Object.fromEntries(\\n+              Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n+            ),\\n+            outputs: flat,\\n+            pr: prInfo,\\n+          },\\n+          { injectLog: true, logPrefix: `workflow.output.${output.name}` }\\n+        );\\n+      } else if (output.value) {\\n+        // Liquid template\\n+        outputs[output.name] = await this.liquid.parseAndRender(output.value, {\\n+          inputs,\\n+          steps: Object.fromEntries(\\n+            Object.entries(flat).map(([id, result]) => [id, (result as any).output])\\n+          ),\\n+          outputs: flat,\\n+          pr: prInfo,\\n+        });\\n+      }\\n+    }\\n+\\n+    return outputs;\\n+  }\\n+\\n+  /**\\n+   * Format workflow result from state machine execution\\n+   */\\n+  private formatWorkflowResultFromStateMachine(\\n+    workflow: WorkflowDefinition,\\n+    result: any,\\n+    outputs: Record<string, unknown>\\n+  ): string {\\n+    const lines: string[] = [];\\n+\\n+    lines.push(`Workflow: ${workflow.name}`);\\n+    if (workflow.description) {\\n+      lines.push(`Description: ${workflow.description}`);\\n+    }\\n+\\n+    lines.push('');\\n+    lines.push('Execution Summary (State Machine):');\\n+    lines.push(`- Total Steps: ${Object.keys(result.results || {}).length}`);\\n+    lines.push(`- Duration: ${result.statistics?.totalDuration || 0}ms`);\\n+\\n+    if (Object.keys(outputs).length > 0) {\\n+      lines.push('');\\n+      lines.push('Outputs:');\\n+      for (const [key, value] of Object.entries(outputs)) {\\n+        const formatted =\\n+          typeof value === 'object' ? JSON.stringify(value, null, 2) : String(value);\\n+        lines.push(`- ${key}: ${formatted}`);\\n+      }\\n+    }\\n+\\n+    return lines.join('\\\\n');\\n+  }\\n+\\n+  private formatWorkflowResult(\\n+    workflow: WorkflowDefinition,\\n+    result: any,\\n+    outputs: Record<string, unknown>\\n+  ): string {\\n+    const lines: string[] = [];\\n+\\n+    lines.push(`Workflow: ${workflow.name}`);\\n+    if (workflow.description) {\\n+      lines.push(`Description: ${workflow.description}`);\\n+    }\\n+\\n+    lines.push('');\\n+    lines.push('Execution Summary:');\\n+    lines.push(`- Status: ${result.status || 'completed'}`);\\n+    lines.push(`- Score: ${result.score || 0}`);\\n+    lines.push(`- Issues Found: ${result.issues?.length || 0}`);\\n+\\n+    if (result.duration) {\\n+      lines.push(`- Duration: ${result.duration}ms`);\\n+    }\\n+\\n+    if (Object.keys(outputs).length > 0) {\\n+      lines.push('');\\n+      lines.push('Outputs:');\\n+      for (const [key, value] of Object.entries(outputs)) {\\n+        const formatted =\\n+          typeof value === 'object' ? JSON.stringify(value, null, 2) : String(value);\\n+        lines.push(`- ${key}: ${formatted}`);\\n+      }\\n+    }\\n+\\n+    if (result.stepSummaries && result.stepSummaries.length > 0) {\\n+      lines.push('');\\n+      lines.push('Step Results:');\\n+      for (const summary of result.stepSummaries) {\\n+        lines.push(\\n+          `- ${summary.stepId}: ${summary.status} (${summary.issues?.length || 0} issues)`\\n+        );\\n+      }\\n+    }\\n+\\n+    return lines.join('\\\\n');\\n+  }\\n+\\n+  /**\\n+   * Load a Visor config file (with steps/checks) and wrap it as a WorkflowDefinition\\n+   * so it can be executed by the state machine as a nested workflow.\\n+   */\\n+  private async loadWorkflowFromConfigPath(\\n+    sourcePath: string,\\n+    baseDir: string\\n+  ): Promise<WorkflowDefinition> {\\n+    const path = require('node:path');\\n+    const fs = require('node:fs');\\n+    const resolved = path.isAbsolute(sourcePath) ? sourcePath : path.resolve(baseDir, sourcePath);\\n+    if (!fs.existsSync(resolved)) {\\n+      throw new Error(`Workflow config not found at: ${resolved}`);\\n+    }\\n+\\n+    const { ConfigManager } = require('../config');\\n+    const mgr = new ConfigManager();\\n+    // Load as-is without merging bundled defaults; keep child config pure\\n+    const loaded = await mgr.loadConfig(resolved, { validate: false, mergeDefaults: false });\\n+\\n+    const steps: Record<string, any> = (loaded as any).steps || (loaded as any).checks || {};\\n+    if (!steps || Object.keys(steps).length === 0) {\\n+      throw new Error(`Config '${resolved}' does not contain any steps to execute as a workflow`);\\n+    }\\n+\\n+    const id = path.basename(resolved).replace(/\\\\.(ya?ml)$/i, '');\\n+    const name = (loaded as any).name || `Workflow from ${path.basename(resolved)}`;\\n+\\n+    const workflowDef: WorkflowDefinition = {\\n+      id,\\n+      name,\\n+      version: (loaded as any).version || '1.0',\\n+      steps,\\n+      description: (loaded as any).description,\\n+      // Inherit optional triggers if present (not required)\\n+      on: (loaded as any).on,\\n+      // Carry over optional inputs/outputs if present so callers can consume them\\n+      inputs: (loaded as any).inputs,\\n+      outputs: (loaded as any).outputs,\\n+    } as WorkflowDefinition;\\n+\\n+    return workflowDef;\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/reviewer.ts\",\"additions\":1,\"deletions\":1,\"changes\":16,\"patch\":\"diff --git a/src/reviewer.ts b/src/reviewer.ts\\nindex f9b862be..b5127a34 100644\\n--- a/src/reviewer.ts\\n+++ b/src/reviewer.ts\\n@@ -151,8 +151,8 @@ export class PRReviewer {\\n     const { debug = false, config, checks } = options;\\n \\n     if (config && checks && checks.length > 0) {\\n-      const { CheckExecutionEngine } = await import('./check-execution-engine');\\n-      const engine = new CheckExecutionEngine();\\n+      const { StateMachineExecutionEngine } = await import('./state-machine-execution-engine');\\n+      const engine = new StateMachineExecutionEngine();\\n       const { results } = await engine.executeGroupedChecks(\\n         prInfo,\\n         checks,\\n@@ -276,7 +276,12 @@ export class PRReviewer {\\n     repo: string,\\n     prNumber: number,\\n     groupedResults: GroupedCheckResults,\\n-    options: ReviewOptions & { commentId?: string; triggeredBy?: string; commitSha?: string } = {}\\n+    options: ReviewOptions & {\\n+      commentId?: string;\\n+      triggeredBy?: string;\\n+      commitSha?: string;\\n+      octokitOverride?: Octokit;\\n+    } = {}\\n   ): Promise<void> {\\n     // Post separate comments for each group\\n     for (const [groupName, checkResults] of Object.entries(groupedResults)) {\\n@@ -330,7 +335,10 @@ export class PRReviewer {\\n       // Do not post empty comments (possible if content is blank after fallbacks)\\n       if (!comment || !comment.trim()) continue;\\n \\n-      await this.commentManager.updateOrCreateComment(owner, repo, prNumber, comment, {\\n+      const manager = options.octokitOverride\\n+        ? new CommentManager(options.octokitOverride)\\n+        : this.commentManager;\\n+      await manager.updateOrCreateComment(owner, repo, prNumber, comment, {\\n         commentId,\\n         triggeredBy: options.triggeredBy || 'unknown',\\n         allowConcurrentUpdates: false,\\n\",\"status\":\"modified\"},{\"filename\":\"src/sdk.ts\",\"additions\":1,\"deletions\":1,\"changes\":5,\"patch\":\"diff --git a/src/sdk.ts b/src/sdk.ts\\nindex 388e7765..06c63da4 100644\\n--- a/src/sdk.ts\\n+++ b/src/sdk.ts\\n@@ -4,7 +4,7 @@\\n  - Dual ESM/CJS bundle via tsup.\\n */\\n \\n-import { CheckExecutionEngine } from './check-execution-engine';\\n+import { StateMachineExecutionEngine } from './state-machine-execution-engine';\\n import { ConfigManager } from './config';\\n import type { AnalysisResult } from './output-formatters';\\n import type { VisorConfig, TagFilter, HumanInputRequest } from './types/config';\\n@@ -121,7 +121,8 @@ export async function runChecks(opts: RunOptions = {}): Promise<AnalysisResult>\\n       ? resolveChecks(opts.checks, config)\\n       : Object.keys(config.checks || {});\\n \\n-  const engine = new CheckExecutionEngine(opts.cwd);\\n+  // Always use StateMachineExecutionEngine\\n+  const engine = new StateMachineExecutionEngine(opts.cwd);\\n \\n   // Set execution context if provided\\n   if (opts.executionContext) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/state-machine-execution-engine.ts\",\"additions\":27,\"deletions\":0,\"changes\":948,\"patch\":\"diff --git a/src/state-machine-execution-engine.ts b/src/state-machine-execution-engine.ts\\nnew file mode 100644\\nindex 00000000..ed938662\\n--- /dev/null\\n+++ b/src/state-machine-execution-engine.ts\\n@@ -0,0 +1,948 @@\\n+import type { CheckExecutionOptions, ExecutionResult } from './types/execution';\\n+import { AnalysisResult } from './output-formatters';\\n+import type { VisorConfig } from './types/config';\\n+import type { PRInfo } from './pr-analyzer';\\n+import { StateMachineRunner } from './state-machine/runner';\\n+import type { EngineContext } from './types/engine';\\n+import { ExecutionJournal } from './snapshot-store';\\n+import { logger } from './logger';\\n+import type { DebugVisualizerServer } from './debug-visualizer/ws-server';\\n+\\n+/**\\n+ * State machine-based execution engine\\n+ *\\n+ * Production-ready state machine implementation with full observability support.\\n+ * M4: Includes OTEL telemetry and debug visualizer event streaming.\\n+ */\\n+export class StateMachineExecutionEngine {\\n+  private workingDirectory: string;\\n+  private executionContext?: import('./providers/check-provider.interface').ExecutionContext;\\n+  private debugServer?: DebugVisualizerServer;\\n+  private _lastContext?: EngineContext;\\n+  private _lastRunner?: StateMachineRunner;\\n+\\n+  constructor(\\n+    workingDirectory?: string,\\n+    octokit?: import('@octokit/rest').Octokit,\\n+    debugServer?: DebugVisualizerServer\\n+  ) {\\n+    this.workingDirectory = workingDirectory || process.cwd();\\n+    this.debugServer = debugServer;\\n+  }\\n+\\n+  /**\\n+   * Execute checks using the state machine engine\\n+   *\\n+   * Converts CheckExecutionOptions -> executeGroupedChecks() -> AnalysisResult\\n+   */\\n+  async executeChecks(options: CheckExecutionOptions): Promise<AnalysisResult> {\\n+    const startTime = Date.now();\\n+    const timestamp = new Date().toISOString();\\n+\\n+    try {\\n+      // Initialize memory store if configured\\n+      if (options.config?.memory) {\\n+        const { MemoryStore } = await import('./memory-store');\\n+        const memoryStore = MemoryStore.getInstance(options.config.memory);\\n+        await memoryStore.initialize();\\n+        logger.debug('Memory store initialized');\\n+      }\\n+\\n+      // Analyze the repository\\n+      const { GitRepositoryAnalyzer } = await import('./git-repository-analyzer');\\n+      const gitAnalyzer = new GitRepositoryAnalyzer(options.workingDirectory);\\n+      logger.info('Analyzing local git repository...');\\n+      const repositoryInfo = await gitAnalyzer.analyzeRepository();\\n+\\n+      if (!repositoryInfo.isGitRepository) {\\n+        return this.createErrorResult(\\n+          repositoryInfo,\\n+          'Not a git repository or no changes found',\\n+          startTime,\\n+          timestamp,\\n+          options.checks\\n+        );\\n+      }\\n+\\n+      // Convert to PRInfo format for compatibility\\n+      const prInfo = gitAnalyzer.toPRInfo(repositoryInfo);\\n+\\n+      // Propagate event type if provided\\n+      try {\\n+        const evt = (options.webhookContext as any)?.eventType;\\n+        if (evt) (prInfo as any).eventType = evt;\\n+      } catch {}\\n+\\n+      // Apply tag filtering if specified\\n+      const filteredChecks = this.filterChecksByTags(\\n+        options.checks,\\n+        options.config,\\n+        options.tagFilter || options.config?.tag_filter\\n+      );\\n+\\n+      if (filteredChecks.length === 0) {\\n+        logger.warn('No checks match the tag filter criteria');\\n+        return this.createErrorResult(\\n+          repositoryInfo,\\n+          'No checks match the tag filter criteria',\\n+          startTime,\\n+          timestamp,\\n+          options.checks\\n+        );\\n+      }\\n+\\n+      // Execute checks using state machine\\n+      logger.info(`Executing checks: ${filteredChecks.join(', ')}`);\\n+      const executionResult = await this.executeGroupedChecks(\\n+        prInfo,\\n+        filteredChecks,\\n+        options.timeout,\\n+        options.config,\\n+        options.outputFormat,\\n+        options.debug,\\n+        options.maxParallelism,\\n+        options.failFast,\\n+        options.tagFilter\\n+      );\\n+\\n+      // Convert ExecutionResult to AnalysisResult format\\n+      const executionTime = Date.now() - startTime;\\n+\\n+      // Extract review summary from grouped results\\n+      const reviewSummary = this.convertGroupedResultsToReviewSummary(\\n+        executionResult.results,\\n+        executionResult.statistics\\n+      );\\n+\\n+      // Collect debug information when debug mode is enabled\\n+      let debugInfo: import('./output-formatters').DebugInfo | undefined;\\n+      if (options.debug && reviewSummary.debug) {\\n+        debugInfo = {\\n+          provider: reviewSummary.debug.provider,\\n+          model: reviewSummary.debug.model,\\n+          processingTime: reviewSummary.debug.processingTime,\\n+          parallelExecution: options.checks.length > 1,\\n+          checksExecuted: options.checks,\\n+          totalApiCalls: reviewSummary.debug.totalApiCalls || options.checks.length,\\n+          apiCallDetails: reviewSummary.debug.apiCallDetails,\\n+        };\\n+      }\\n+\\n+      // Expose output history snapshot\\n+      try {\\n+        const histSnap = this.getOutputHistorySnapshot();\\n+        (reviewSummary as any).history = histSnap;\\n+      } catch {}\\n+\\n+      return {\\n+        repositoryInfo,\\n+        reviewSummary,\\n+        executionTime,\\n+        timestamp,\\n+        checksExecuted: filteredChecks,\\n+        executionStatistics: executionResult.statistics,\\n+        debug: debugInfo,\\n+      };\\n+    } catch (error) {\\n+      const message = error instanceof Error ? error.message : 'Unknown error occurred';\\n+      logger.error('Error executing checks: ' + message);\\n+\\n+      // In strict test modes, surface errors to callers\\n+      const strictEnv = process.env.VISOR_STRICT_ERRORS === 'true';\\n+      if (strictEnv) {\\n+        throw error;\\n+      }\\n+\\n+      const fallbackRepositoryInfo: import('./git-repository-analyzer').GitRepositoryInfo = {\\n+        title: 'Error during analysis',\\n+        body: `Error: ${message || 'Unknown error'}`,\\n+        author: 'system',\\n+        base: 'main',\\n+        head: 'HEAD',\\n+        files: [],\\n+        totalAdditions: 0,\\n+        totalDeletions: 0,\\n+        isGitRepository: false,\\n+        workingDirectory: options.workingDirectory || process.cwd(),\\n+      };\\n+\\n+      return this.createErrorResult(\\n+        fallbackRepositoryInfo,\\n+        message || 'Unknown error occurred',\\n+        startTime,\\n+        timestamp,\\n+        options.checks\\n+      );\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Get execution context (used by state machine to propagate hooks)\\n+   */\\n+  protected getExecutionContext():\\n+    | import('./providers/check-provider.interface').ExecutionContext\\n+    | undefined {\\n+    return this.executionContext;\\n+  }\\n+\\n+  /**\\n+   * Set execution context for external callers\\n+   */\\n+  public setExecutionContext(\\n+    context: import('./providers/check-provider.interface').ExecutionContext | undefined\\n+  ): void {\\n+    this.executionContext = context;\\n+  }\\n+\\n+  /**\\n+   * Reset per-run state (no-op for state machine engine)\\n+   *\\n+   * The state machine engine is stateless per-run by design.\\n+   * Each execution creates a fresh journal and context.\\n+   * This method exists only for backward compatibility with test framework.\\n+   *\\n+   * @deprecated This is a no-op. State machine engine doesn't maintain per-run state.\\n+   */\\n+  public resetPerRunState(): void {\\n+    // No-op: State machine engine is stateless per-run\\n+    // Each execution creates a fresh journal and context\\n+  }\\n+\\n+  /**\\n+   * Execute grouped checks using the state machine engine\\n+   *\\n+   * M4: Production-ready with full telemetry and debug server support\\n+   */\\n+  async executeGroupedChecks(\\n+    prInfo: PRInfo,\\n+    checks: string[],\\n+    timeout?: number,\\n+    config?: VisorConfig,\\n+    outputFormat?: string,\\n+    debug?: boolean,\\n+    maxParallelism?: number,\\n+    failFast?: boolean,\\n+    tagFilter?: import('./types/config').TagFilter,\\n+    _pauseGate?: () => Promise<void>\\n+  ): Promise<ExecutionResult> {\\n+    if (debug) {\\n+      logger.info('[StateMachine] Using state machine engine');\\n+    }\\n+\\n+    // Create minimal default config if none provided (backward compatibility)\\n+    if (!config) {\\n+      const { ConfigManager } = await import('./config');\\n+      const configManager = new ConfigManager();\\n+      config = await configManager.getDefaultConfig();\\n+      logger.debug('[StateMachine] Using default configuration (no config provided)');\\n+    }\\n+\\n+    // Merge tagFilter into config if provided (test runner passes it separately)\\n+    const configWithTagFilter = tagFilter\\n+      ? {\\n+          ...config,\\n+          tag_filter: tagFilter,\\n+        }\\n+      : config;\\n+\\n+    // Build engine context\\n+    const context = this.buildEngineContext(\\n+      configWithTagFilter,\\n+      prInfo,\\n+      debug,\\n+      maxParallelism,\\n+      failFast,\\n+      checks // Pass the explicit checks list\\n+    );\\n+\\n+    // Copy execution context (hooks, etc.) from legacy engine\\n+    context.executionContext = this.getExecutionContext();\\n+\\n+    // Store context for later access (e.g., getOutputHistorySnapshot)\\n+    this._lastContext = context;\\n+\\n+    // Create and run state machine with debug server support (M4)\\n+    const runner = new StateMachineRunner(context, this.debugServer);\\n+    this._lastRunner = runner;\\n+    const result = await runner.run();\\n+\\n+    if (debug) {\\n+      logger.info('[StateMachine] Execution complete');\\n+    }\\n+\\n+    // Optional grouped-mode PR comment posting (used by YAML tests via execution context)\\n+    try {\\n+      if (\\n+        this.executionContext?.mode?.postGroupedComments &&\\n+        configWithTagFilter?.output?.pr_comment\\n+      ) {\\n+        const { PRReviewer } = await import('./reviewer');\\n+        const reviewer = new PRReviewer(\\n+          (this._lastContext as any)?.executionContext?.octokit as any\\n+        );\\n+\\n+        // Resolve owner/repo from PRInfo.eventContext\\n+        let owner: string | undefined;\\n+        let repo: string | undefined;\\n+        try {\\n+          const anyInfo = prInfo as unknown as {\\n+            eventContext?: { repository?: { owner?: { login?: string }; name?: string } };\\n+          };\\n+          owner = anyInfo?.eventContext?.repository?.owner?.login || owner;\\n+          repo = anyInfo?.eventContext?.repository?.name || repo;\\n+        } catch {}\\n+        owner = owner || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[0];\\n+        repo = repo || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[1];\\n+\\n+        if (owner && repo && (prInfo as any).number) {\\n+          await reviewer.postReviewComment(owner, repo, (prInfo as any).number, result.results, {\\n+            config: configWithTagFilter as any,\\n+            triggeredBy: (prInfo as any).eventType || 'manual',\\n+            commentId: 'visor-review',\\n+            octokitOverride: (prInfo as any)?.eventContext?.octokit,\\n+            commitSha: (prInfo as any)?.eventContext?.pull_request?.head?.sha,\\n+          });\\n+        }\\n+      }\\n+    } catch (err) {\\n+      logger.debug(`[StateMachine] Skipped postGroupedComments due to error: ${err}`);\\n+    }\\n+\\n+    // Cleanup AI sessions after execution\\n+    try {\\n+      const { SessionRegistry } = await import('./session-registry');\\n+      const sessionRegistry = SessionRegistry.getInstance();\\n+      sessionRegistry.clearAllSessions();\\n+    } catch (error) {\\n+      logger.debug(`[StateMachine] Failed to cleanup sessions: ${error}`);\\n+    }\\n+\\n+    return result;\\n+  }\\n+\\n+  /**\\n+   * Build the engine context for state machine execution\\n+   */\\n+  private buildEngineContext(\\n+    config: VisorConfig,\\n+    prInfo: PRInfo,\\n+    debug?: boolean,\\n+    maxParallelism?: number,\\n+    failFast?: boolean,\\n+    requestedChecks?: string[]\\n+  ): EngineContext {\\n+    const { buildEngineContextForRun } = require('./state-machine/context/build-engine-context');\\n+    return buildEngineContextForRun(\\n+      this.workingDirectory,\\n+      config,\\n+      prInfo,\\n+      debug,\\n+      maxParallelism,\\n+      failFast,\\n+      requestedChecks\\n+    );\\n+  }\\n+\\n+  /**\\n+   * Get output history snapshot for test framework compatibility\\n+   * Extracts output history from the journal\\n+   */\\n+  public getOutputHistorySnapshot(): Record<string, unknown[]> {\\n+    // Get the journal from the last execution context\\n+    const journal = (this as any)._lastContext?.journal as ExecutionJournal | undefined;\\n+    if (!journal) {\\n+      logger.debug('[StateMachine][DEBUG] getOutputHistorySnapshot: No journal found');\\n+      return {};\\n+    }\\n+\\n+    const sessionId = (this as any)._lastContext?.sessionId as string | undefined;\\n+    if (!sessionId) {\\n+      logger.debug('[StateMachine][DEBUG] getOutputHistorySnapshot: No sessionId found');\\n+      return {};\\n+    }\\n+\\n+    // Read all journal entries for this session\\n+    const snapshot = journal.beginSnapshot();\\n+    const allEntries = journal.readVisible(sessionId, snapshot, undefined);\\n+\\n+    logger.debug(\\n+      `[StateMachine][DEBUG] getOutputHistorySnapshot: Found ${allEntries.length} journal entries`\\n+    );\\n+\\n+    // Group by checkId and extract outputs\\n+    const outputHistory: Record<string, unknown[]> = {};\\n+    for (const entry of allEntries) {\\n+      const checkId = entry.checkId;\\n+\\n+      if (!outputHistory[checkId]) {\\n+        outputHistory[checkId] = [];\\n+      }\\n+      // Push the output if it exists\\n+      if (entry.result.output !== undefined) {\\n+        outputHistory[checkId].push(entry.result.output);\\n+      }\\n+    }\\n+\\n+    logger.debug(\\n+      `[StateMachine][DEBUG] getOutputHistorySnapshot result: ${JSON.stringify(Object.keys(outputHistory))}`\\n+    );\\n+    for (const [checkId, outputs] of Object.entries(outputHistory)) {\\n+      logger.debug(`[StateMachine][DEBUG]   ${checkId}: ${outputs.length} outputs`);\\n+    }\\n+\\n+    return outputHistory;\\n+  }\\n+\\n+  /**\\n+   * Save a JSON snapshot of the last run's state and journal to a file (experimental).\\n+   * Does not include secrets. Intended for debugging and future resume support.\\n+   */\\n+  public async saveSnapshotToFile(filePath: string): Promise<void> {\\n+    const fs = await import('fs/promises');\\n+    const ctx = this._lastContext;\\n+    const runner = this._lastRunner;\\n+    if (!ctx || !runner) {\\n+      throw new Error('No prior execution context to snapshot');\\n+    }\\n+    const journal = (ctx as any).journal as ExecutionJournal;\\n+    const snapshotId = journal.beginSnapshot();\\n+    const entries = journal.readVisible(ctx.sessionId, snapshotId, undefined);\\n+    const state = runner.getState();\\n+    const serializableState = serializeRunState(state);\\n+    const payload = {\\n+      version: 1,\\n+      sessionId: ctx.sessionId,\\n+      event: ctx.event,\\n+      wave: state.wave,\\n+      state: serializableState,\\n+      journal: entries,\\n+      requestedChecks: (ctx as any).requestedChecks || [],\\n+    } as const;\\n+    await fs.writeFile(filePath, JSON.stringify(payload, null, 2), 'utf8');\\n+  }\\n+\\n+  /**\\n+   * Load a snapshot JSON from file and return it. Resume support can build on this.\\n+   */\\n+  public async loadSnapshotFromFile<T = unknown>(filePath: string): Promise<T> {\\n+    const fs = await import('fs/promises');\\n+    const raw = await fs.readFile(filePath, 'utf8');\\n+    return JSON.parse(raw) as T;\\n+  }\\n+\\n+  /**\\n+   * Filter checks by tag filter\\n+   */\\n+  private filterChecksByTags(\\n+    checks: string[],\\n+    config: VisorConfig | undefined,\\n+    tagFilter: import('./types/config').TagFilter | undefined\\n+  ): string[] {\\n+    // When no tag filter is specified, include only untagged checks by default.\\n+    // Tagged checks are opt-in unless tag_filter is provided.\\n+    return checks.filter(checkName => {\\n+      const checkConfig = config?.checks?.[checkName];\\n+      if (!checkConfig) {\\n+        // If no config for this check, include it by default\\n+        return true;\\n+      }\\n+\\n+      const checkTags = checkConfig.tags || [];\\n+\\n+      // If no tag filter is specified, include only untagged checks.\\n+      if (!tagFilter || (!tagFilter.include && !tagFilter.exclude)) {\\n+        return checkTags.length === 0;\\n+      }\\n+\\n+      // If check has no tags and a tag filter is specified, include it (untagged checks always run)\\n+      if (checkTags.length === 0) {\\n+        return true;\\n+      }\\n+\\n+      // Check exclude tags first (if any exclude tag matches, skip the check)\\n+      if (tagFilter.exclude && tagFilter.exclude.length > 0) {\\n+        const hasExcludedTag = tagFilter.exclude.some(tag => checkTags.includes(tag));\\n+        if (hasExcludedTag) return false;\\n+      }\\n+\\n+      // Check include tags (if specified, at least one must match)\\n+      if (tagFilter.include && tagFilter.include.length > 0) {\\n+        const hasIncludedTag = tagFilter.include.some(tag => checkTags.includes(tag));\\n+        if (!hasIncludedTag) return false;\\n+      }\\n+\\n+      return true;\\n+    });\\n+  }\\n+\\n+  /**\\n+   * Create an error result in AnalysisResult format\\n+   */\\n+  private createErrorResult(\\n+    repositoryInfo: import('./git-repository-analyzer').GitRepositoryInfo,\\n+    errorMessage: string,\\n+    startTime: number,\\n+    timestamp: string,\\n+    checksExecuted: string[]\\n+  ): AnalysisResult {\\n+    const executionTime = Date.now() - startTime;\\n+\\n+    return {\\n+      repositoryInfo,\\n+      reviewSummary: {\\n+        issues: [\\n+          {\\n+            file: 'system',\\n+            line: 0,\\n+            endLine: undefined,\\n+            ruleId: 'system/error',\\n+            message: errorMessage,\\n+            severity: 'error',\\n+            category: 'logic',\\n+            suggestion: undefined,\\n+            replacement: undefined,\\n+          },\\n+        ],\\n+      },\\n+      executionTime,\\n+      timestamp,\\n+      checksExecuted,\\n+    };\\n+  }\\n+\\n+  /**\\n+   * Convert GroupedCheckResults to ReviewSummary\\n+   * Aggregates all check results into a single ReviewSummary\\n+   */\\n+  private convertGroupedResultsToReviewSummary(\\n+    groupedResults: import('./reviewer').GroupedCheckResults,\\n+    statistics?: import('./types/execution').ExecutionStatistics\\n+  ): import('./reviewer').ReviewSummary {\\n+    const { convertToReviewSummary } = require('./state-machine/execution/summary');\\n+    return (convertToReviewSummary as any)(groupedResults as any, statistics as any) as any;\\n+  }\\n+\\n+  /**\\n+   * Evaluate failure conditions for a check result\\n+   *\\n+   * This method provides backward compatibility with the legacy engine by\\n+   * delegating to the FailureConditionEvaluator.\\n+   *\\n+   * @param checkName - The name of the check being evaluated\\n+   * @param reviewSummary - The review summary containing check results\\n+   * @param config - The Visor configuration containing failure conditions\\n+   * @param previousOutputs - Optional previous check outputs for cross-check conditions\\n+   * @param authorAssociation - Optional GitHub author association for permission checks\\n+   * @returns Array of failure condition evaluation results\\n+   */\\n+  async evaluateFailureConditions(\\n+    checkName: string,\\n+    reviewSummary: import('./reviewer').ReviewSummary,\\n+    config: VisorConfig,\\n+    previousOutputs?: Record<string, import('./reviewer').ReviewSummary>,\\n+    authorAssociation?: string\\n+  ): Promise<import('./types/config').FailureConditionResult[]> {\\n+    const { FailureConditionEvaluator } = await import('./failure-condition-evaluator');\\n+    const evaluator = new FailureConditionEvaluator();\\n+    const { addEvent } = await import('./telemetry/trace-helpers');\\n+    const { addFailIfTriggered } = await import('./telemetry/metrics');\\n+\\n+    // Extract check configuration\\n+    const checkConfig = config.checks?.[checkName];\\n+    if (!checkConfig) {\\n+      return [];\\n+    }\\n+\\n+    // Schema can be string or Record<string, unknown>, convert to string for evaluation\\n+    const rawSchema = checkConfig.schema || 'code-review';\\n+    const checkSchema = typeof rawSchema === 'string' ? rawSchema : 'code-review';\\n+    const checkGroup = checkConfig.group || 'default';\\n+\\n+    // Handle both fail_if (simple string) and failure_conditions (complex object)\\n+    const results: import('./types/config').FailureConditionResult[] = [];\\n+\\n+    // Evaluate global fail_if\\n+    if (config.fail_if) {\\n+      const failed = await evaluator.evaluateSimpleCondition(\\n+        checkName,\\n+        checkSchema,\\n+        checkGroup,\\n+        reviewSummary,\\n+        config.fail_if,\\n+        previousOutputs || {}\\n+      );\\n+\\n+      // Telemetry events + metric\\n+      try {\\n+        addEvent('fail_if.evaluated', {\\n+          'visor.check.id': checkName,\\n+          scope: 'global',\\n+          expression: String(config.fail_if),\\n+          result: failed ? 'triggered' : 'not_triggered',\\n+        });\\n+        if (failed) {\\n+          addEvent('fail_if.triggered', {\\n+            'visor.check.id': checkName,\\n+            scope: 'global',\\n+            expression: String(config.fail_if),\\n+          });\\n+          addFailIfTriggered(checkName, 'global');\\n+        }\\n+      } catch {}\\n+\\n+      results.push({\\n+        conditionName: 'global_fail_if',\\n+        failed,\\n+        expression: config.fail_if,\\n+        message: failed ? `Global failure condition met: ${config.fail_if}` : undefined,\\n+        severity: 'error',\\n+        haltExecution: false,\\n+      });\\n+    }\\n+\\n+    // Evaluate check-specific fail_if (overrides global if present)\\n+    if (checkConfig.fail_if) {\\n+      const failed = await evaluator.evaluateSimpleCondition(\\n+        checkName,\\n+        checkSchema,\\n+        checkGroup,\\n+        reviewSummary,\\n+        checkConfig.fail_if,\\n+        previousOutputs || {}\\n+      );\\n+\\n+      // Telemetry events + metric\\n+      try {\\n+        addEvent('fail_if.evaluated', {\\n+          'visor.check.id': checkName,\\n+          scope: 'check',\\n+          expression: String(checkConfig.fail_if),\\n+          result: failed ? 'triggered' : 'not_triggered',\\n+        });\\n+        if (failed) {\\n+          addEvent('fail_if.triggered', {\\n+            'visor.check.id': checkName,\\n+            scope: 'check',\\n+            expression: String(checkConfig.fail_if),\\n+          });\\n+          addFailIfTriggered(checkName, 'check');\\n+        }\\n+      } catch {}\\n+\\n+      results.push({\\n+        conditionName: `${checkName}_fail_if`,\\n+        failed,\\n+        expression: checkConfig.fail_if,\\n+        message: failed ? `Check failure condition met: ${checkConfig.fail_if}` : undefined,\\n+        severity: 'error',\\n+        haltExecution: false,\\n+      });\\n+    }\\n+\\n+    // Also evaluate legacy failure_conditions if present\\n+    const globalConditions = config.failure_conditions;\\n+    const checkConditions = checkConfig.failure_conditions;\\n+\\n+    if (globalConditions || checkConditions) {\\n+      const legacyResults = await evaluator.evaluateConditions(\\n+        checkName,\\n+        checkSchema,\\n+        checkGroup,\\n+        reviewSummary,\\n+        globalConditions,\\n+        checkConditions,\\n+        previousOutputs,\\n+        authorAssociation\\n+      );\\n+      results.push(...legacyResults);\\n+    }\\n+\\n+    return results;\\n+  }\\n+\\n+  /**\\n+   * Get repository status\\n+   * @returns Repository status information\\n+   */\\n+  async getRepositoryStatus(): Promise<{\\n+    isGitRepository: boolean;\\n+    branch?: string;\\n+    hasChanges: boolean;\\n+    filesChanged?: number;\\n+  }> {\\n+    try {\\n+      const { GitRepositoryAnalyzer } = await import('./git-repository-analyzer');\\n+      const analyzer = new GitRepositoryAnalyzer(this.workingDirectory);\\n+      const info = await analyzer.analyzeRepository();\\n+\\n+      return {\\n+        isGitRepository: info.isGitRepository,\\n+        branch: info.head, // Use head as branch name\\n+        hasChanges: info.isGitRepository && (info.files?.length > 0 || false),\\n+        filesChanged: info.isGitRepository ? info.files?.length || 0 : 0,\\n+      };\\n+    } catch {\\n+      return {\\n+        isGitRepository: false,\\n+        hasChanges: false,\\n+      };\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Check if current directory is a git repository\\n+   * @returns True if git repository, false otherwise\\n+   */\\n+  async isGitRepository(): Promise<boolean> {\\n+    const status = await this.getRepositoryStatus();\\n+    return status.isGitRepository;\\n+  }\\n+\\n+  /**\\n+   * Get list of available check types\\n+   * @returns Array of check type names\\n+   */\\n+  static getAvailableCheckTypes(): string[] {\\n+    const { CheckProviderRegistry } = require('./providers/check-provider-registry');\\n+    const registry = CheckProviderRegistry.getInstance();\\n+    return registry.getAvailableProviders();\\n+  }\\n+\\n+  /**\\n+   * Validate check types and return valid/invalid lists\\n+   * @param checks - Array of check type names to validate\\n+   * @returns Object with valid and invalid check types\\n+   */\\n+  static validateCheckTypes(checks: string[]): { valid: string[]; invalid: string[] } {\\n+    const availableTypes = StateMachineExecutionEngine.getAvailableCheckTypes();\\n+    const valid: string[] = [];\\n+    const invalid: string[] = [];\\n+\\n+    for (const check of checks) {\\n+      if (availableTypes.includes(check)) {\\n+        valid.push(check);\\n+      } else {\\n+        invalid.push(check);\\n+      }\\n+    }\\n+\\n+    return { valid, invalid };\\n+  }\\n+\\n+  /**\\n+   * Render check content using the appropriate template\\n+   *\\n+   * This method handles template rendering for check results, supporting:\\n+   * - Plain schema: returns raw content without template processing\\n+   * - Custom templates: from inline content or file\\n+   * - Built-in schema templates: from output/{schema}/template.liquid\\n+   */\\n+  private async renderCheckContent(\\n+    checkName: string,\\n+    reviewSummary: import('./reviewer').ReviewSummary,\\n+    checkConfig: any,\\n+    _prInfo?: PRInfo\\n+  ): Promise<string> {\\n+    // Import the liquid template system\\n+    const { createExtendedLiquid } = await import('./liquid-extensions');\\n+    const fs = await import('fs/promises');\\n+    const path = await import('path');\\n+\\n+    // Determine template to use\\n+    const schema = checkConfig.schema || 'plain';\\n+    let templateContent: string;\\n+\\n+    if (checkConfig.template) {\\n+      // Custom template\\n+      if (checkConfig.template.content) {\\n+        templateContent = checkConfig.template.content;\\n+      } else if (checkConfig.template.file) {\\n+        // Validate template file path for security\\n+        const templateFile = checkConfig.template.file;\\n+\\n+        // Check for absolute paths\\n+        if (path.isAbsolute(templateFile)) {\\n+          throw new Error('Template path must be relative to project directory');\\n+        }\\n+\\n+        // Check for .. segments\\n+        if (templateFile.includes('..')) {\\n+          throw new Error('Template path cannot contain \\\"..\\\" segments');\\n+        }\\n+\\n+        // Check for home directory references\\n+        if (templateFile.startsWith('~')) {\\n+          throw new Error('Template path cannot reference home directory');\\n+        }\\n+\\n+        // Check for null bytes\\n+        if (templateFile.includes('\\\\0')) {\\n+          throw new Error('Template path contains invalid characters');\\n+        }\\n+\\n+        // Check for whitespace-only paths\\n+        if (templateFile.trim() === '') {\\n+          throw new Error('Template path must be a non-empty string');\\n+        }\\n+\\n+        // Check for .liquid extension\\n+        if (!templateFile.endsWith('.liquid')) {\\n+          throw new Error('Template file must have .liquid extension');\\n+        }\\n+\\n+        // Resolve path relative to working directory\\n+        const { GitRepositoryAnalyzer } = await import('./git-repository-analyzer');\\n+        const gitAnalyzer = new GitRepositoryAnalyzer(this.workingDirectory);\\n+        const repoInfo = await gitAnalyzer.analyzeRepository();\\n+        const workingDir = repoInfo.workingDirectory;\\n+\\n+        const resolvedPath = path.resolve(workingDir, templateFile);\\n+        templateContent = await fs.readFile(resolvedPath, 'utf-8');\\n+      } else {\\n+        throw new Error('Custom template must specify either \\\"file\\\" or \\\"content\\\"');\\n+      }\\n+    } else if (schema === 'plain') {\\n+      // Plain schema - return raw content directly\\n+      return reviewSummary.issues?.[0]?.message || '';\\n+    } else {\\n+      // Use built-in schema template\\n+      const sanitizedSchema = schema.replace(/[^a-zA-Z0-9-]/g, '');\\n+      if (!sanitizedSchema) {\\n+        throw new Error('Invalid schema name');\\n+      }\\n+      const templatePath = path.join(__dirname, `output/${sanitizedSchema}/template.liquid`);\\n+      templateContent = await fs.readFile(templatePath, 'utf-8');\\n+    }\\n+\\n+    // Create liquid instance with extended functionality\\n+    const liquid = createExtendedLiquid({\\n+      trimTagLeft: false,\\n+      trimTagRight: false,\\n+      trimOutputLeft: false,\\n+      trimOutputRight: false,\\n+      greedy: false,\\n+    });\\n+\\n+    // Prepare template data\\n+    const templateData = {\\n+      issues: reviewSummary.issues || [],\\n+      checkName: checkName,\\n+    };\\n+\\n+    const rendered = await liquid.parseAndRender(templateContent, templateData);\\n+    return rendered.trim();\\n+  }\\n+\\n+  /**\\n+   * Format the status column for execution statistics\\n+   * Used by execution-statistics-formatting tests\\n+   */\\n+  private formatStatusColumn(stats: import('./types/execution').CheckExecutionStats): string {\\n+    if (stats.skipped) {\\n+      // Format skip reason\\n+      if (stats.skipReason === 'if_condition') {\\n+        return '‚è≠ if';\\n+      } else if (stats.skipReason === 'fail_fast') {\\n+        return '‚è≠ ff';\\n+      } else if (stats.skipReason === 'dependency_failed') {\\n+        return '‚è≠ dep';\\n+      }\\n+      return '‚è≠';\\n+    }\\n+\\n+    const totalRuns = stats.totalRuns;\\n+    const successfulRuns = stats.successfulRuns;\\n+    const failedRuns = stats.failedRuns;\\n+\\n+    if (failedRuns > 0 && successfulRuns > 0) {\\n+      // Mixed results\\n+      return `‚úî/‚úñ ${successfulRuns}/${totalRuns}`;\\n+    } else if (failedRuns > 0) {\\n+      // All failed\\n+      return totalRuns === 1 ? '‚úñ' : `‚úñ √ó${totalRuns}`;\\n+    } else {\\n+      // All successful\\n+      return totalRuns === 1 ? '‚úî' : `‚úî √ó${totalRuns}`;\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Format the details column for execution statistics\\n+   * Used by execution-statistics-formatting tests\\n+   */\\n+  private formatDetailsColumn(stats: import('./types/execution').CheckExecutionStats): string {\\n+    const parts: string[] = [];\\n+\\n+    // Add outputs produced\\n+    if (stats.outputsProduced !== undefined && stats.outputsProduced > 0) {\\n+      parts.push(`‚Üí${stats.outputsProduced}`);\\n+    }\\n+\\n+    // Add critical issues\\n+    if (stats.issuesBySeverity.critical > 0) {\\n+      parts.push(`${stats.issuesBySeverity.critical}üî¥`);\\n+    }\\n+\\n+    // Add error issues (only if no critical)\\n+    if (stats.issuesBySeverity.error > 0 && stats.issuesBySeverity.critical === 0) {\\n+      parts.push(`${stats.issuesBySeverity.error}‚ùå`);\\n+    }\\n+\\n+    // Add warnings\\n+    if (stats.issuesBySeverity.warning > 0) {\\n+      parts.push(`${stats.issuesBySeverity.warning}‚ö†Ô∏è`);\\n+    }\\n+\\n+    // Add info issues (only if no critical/error/warning)\\n+    if (\\n+      stats.issuesBySeverity.info > 0 &&\\n+      stats.issuesBySeverity.critical === 0 &&\\n+      stats.issuesBySeverity.error === 0 &&\\n+      stats.issuesBySeverity.warning === 0\\n+    ) {\\n+      parts.push(`${stats.issuesBySeverity.info}üí°`);\\n+    }\\n+\\n+    // Add error message if present\\n+    if (stats.errorMessage) {\\n+      parts.push(this.truncate(stats.errorMessage, 40));\\n+    }\\n+\\n+    // Add skip condition if present\\n+    if (stats.skipCondition) {\\n+      parts.push(this.truncate(stats.skipCondition, 40));\\n+    }\\n+\\n+    return parts.join(' ');\\n+  }\\n+\\n+  /**\\n+   * Truncate a string to a maximum length\\n+   * Used by formatDetailsColumn\\n+   */\\n+  private truncate(str: string, maxLength: number): string {\\n+    if (str.length <= maxLength) {\\n+      return str;\\n+    }\\n+    return str.substring(0, maxLength - 3) + '...';\\n+  }\\n+}\\n+\\n+/** Convert RunState with Maps/Sets into a JSON-safe form */\\n+function serializeRunState(state: import('./types/engine').RunState) {\\n+  return {\\n+    ...state,\\n+    levelQueue: state.levelQueue,\\n+    eventQueue: state.eventQueue,\\n+    activeDispatches: Array.from(state.activeDispatches.entries()),\\n+    completedChecks: Array.from(state.completedChecks.values()),\\n+    stats: Array.from(state.stats.entries()),\\n+    historyLog: state.historyLog,\\n+    forwardRunGuards: Array.from(state.forwardRunGuards.values()),\\n+    currentLevelChecks: Array.from(state.currentLevelChecks.values()),\\n+    pendingRunScopes: Array.from((state.pendingRunScopes || new Map()).entries()).map(([k, v]) => [\\n+      k,\\n+      v,\\n+    ]),\\n+  };\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/state-machine/context/build-engine-context.ts\",\"additions\":4,\"deletions\":0,\"changes\":112,\"patch\":\"diff --git a/src/state-machine/context/build-engine-context.ts b/src/state-machine/context/build-engine-context.ts\\nnew file mode 100644\\nindex 00000000..6c856725\\n--- /dev/null\\n+++ b/src/state-machine/context/build-engine-context.ts\\n@@ -0,0 +1,112 @@\\n+import type { VisorConfig, EventTrigger } from '../../types/config';\\n+import type { PRInfo } from '../../pr-analyzer';\\n+import type { EngineContext, CheckMetadata } from '../../types/engine';\\n+import { ExecutionJournal } from '../../snapshot-store';\\n+import { MemoryStore } from '../../memory-store';\\n+import { v4 as uuidv4 } from 'uuid';\\n+import { logger } from '../../logger';\\n+import type { VisorConfig as VCfg, CheckConfig as CfgCheck } from '../../types/config';\\n+\\n+/**\\n+ * Apply minimal criticality defaults in-place.\\n+ * This is a no-behavior-change scaffold: we only default missing\\n+ * check.criticality to 'policy' so downstream code can rely on a value.\\n+ * Future mapping (retries/loop budgets) can build on this without\\n+ * changing existing behavior.\\n+ */\\n+function applyCriticalityDefaults(cfg: VCfg): void {\\n+  const checks = cfg.checks || {};\\n+  for (const id of Object.keys(checks)) {\\n+    const c: CfgCheck = (checks as any)[id] as CfgCheck;\\n+    if (!c.criticality) (c.criticality as any) = 'policy';\\n+    // For 'info' checks, default continue_on_failure to true if unset.\\n+    if (c.criticality === 'info' && typeof c.continue_on_failure === 'undefined')\\n+      c.continue_on_failure = true;\\n+  }\\n+}\\n+\\n+/**\\n+ * Pure helper to build an EngineContext for a state-machine run.\\n+ * Extracted to reduce StateMachineExecutionEngine size; behavior unchanged.\\n+ */\\n+export function buildEngineContextForRun(\\n+  workingDirectory: string,\\n+  config: VisorConfig,\\n+  prInfo: PRInfo,\\n+  debug?: boolean,\\n+  maxParallelism?: number,\\n+  failFast?: boolean,\\n+  requestedChecks?: string[]\\n+): EngineContext {\\n+  // Deep clone provided config to avoid cross-run mutations between tests/runs\\n+  const clonedConfig: VisorConfig = JSON.parse(JSON.stringify(config));\\n+\\n+  // Build check metadata\\n+  const checks: Record<string, CheckMetadata> = {};\\n+\\n+  // Fill in minimal defaults derived from criticality (no behavior change)\\n+  applyCriticalityDefaults(clonedConfig);\\n+\\n+  // If config has checks, use them\\n+  for (const [checkId, checkConfig] of Object.entries(clonedConfig.checks || {})) {\\n+    checks[checkId] = {\\n+      tags: checkConfig.tags || [],\\n+      triggers: (Array.isArray(checkConfig.on) ? checkConfig.on : [checkConfig.on]).filter(\\n+        Boolean\\n+      ) as EventTrigger[],\\n+      group: checkConfig.group,\\n+      providerType: checkConfig.type || 'ai',\\n+      dependencies: checkConfig.depends_on || [],\\n+    };\\n+  }\\n+\\n+  // Backward compatibility: synthesize minimal check configs for requested checks\\n+  // that don't exist in the config (e.g., legacy test mode with empty config)\\n+  if (requestedChecks && requestedChecks.length > 0) {\\n+    for (const checkName of requestedChecks) {\\n+      if (!checks[checkName] && !clonedConfig.checks?.[checkName]) {\\n+        // Synthesize a minimal check config for this legacy check name\\n+        logger.debug(`[StateMachine] Synthesizing minimal config for legacy check: ${checkName}`);\\n+\\n+        // Add to config.checks so providers can find it\\n+        if (!clonedConfig.checks) {\\n+          clonedConfig.checks = {};\\n+        }\\n+        clonedConfig.checks[checkName] = {\\n+          type: 'ai',\\n+          prompt: `Perform ${checkName} analysis`,\\n+        } as any;\\n+\\n+        // Add metadata\\n+        checks[checkName] = {\\n+          tags: [],\\n+          triggers: [],\\n+          group: 'default',\\n+          providerType: 'ai',\\n+          dependencies: [],\\n+        };\\n+      }\\n+    }\\n+  }\\n+\\n+  // Initialize journal and memory\\n+  const journal = new ExecutionJournal();\\n+  const memory = MemoryStore.getInstance(clonedConfig.memory);\\n+\\n+  return {\\n+    mode: 'state-machine',\\n+    config: clonedConfig,\\n+    checks,\\n+    journal,\\n+    memory,\\n+    workingDirectory,\\n+    sessionId: uuidv4(),\\n+    event: prInfo.eventType,\\n+    debug,\\n+    maxParallelism,\\n+    failFast,\\n+    requestedChecks: requestedChecks && requestedChecks.length > 0 ? requestedChecks : undefined,\\n+    // Store prInfo for later access (e.g., in getOutputHistorySnapshot)\\n+    prInfo,\\n+  };\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/state-machine/dispatch/dependency-gating.ts\",\"additions\":5,\"deletions\":0,\"changes\":147,\"patch\":\"diff --git a/src/state-machine/dispatch/dependency-gating.ts b/src/state-machine/dispatch/dependency-gating.ts\\nnew file mode 100644\\nindex 00000000..6e64c60a\\n--- /dev/null\\n+++ b/src/state-machine/dispatch/dependency-gating.ts\\n@@ -0,0 +1,147 @@\\n+import type { EngineContext, RunState } from '../../types/engine';\\n+import type { ReviewSummary } from '../../reviewer';\\n+\\n+/**\\n+ * Build dependency results for a check with a specific scope.\\n+ * Extracted from LevelDispatch to centralize dependency resolution logic.\\n+ */\\n+export function buildDependencyResultsWithScope(\\n+  checkId: string,\\n+  checkConfig: any,\\n+  context: EngineContext,\\n+  scope: Array<{ check: string; index: number }>\\n+): Map<string, ReviewSummary> {\\n+  const dependencyResults = new Map<string, ReviewSummary>();\\n+\\n+  const dependencies = checkConfig.depends_on || [];\\n+  const depList = Array.isArray(dependencies) ? dependencies : [dependencies];\\n+\\n+  const currentIndex = scope.length > 0 ? scope[scope.length - 1].index : undefined;\\n+\\n+  for (const depId of depList) {\\n+    if (!depId) continue;\\n+    try {\\n+      const snapshotId = context.journal.beginSnapshot();\\n+      const visible = context.journal.readVisible(\\n+        context.sessionId,\\n+        snapshotId,\\n+        context.event as any\\n+      );\\n+      const sameScope = (\\n+        a: Array<{ check: string; index: number }>,\\n+        b: Array<{ check: string; index: number }>\\n+      ): boolean => {\\n+        if (a.length !== b.length) return false;\\n+        for (let i = 0; i < a.length; i++)\\n+          if (a[i].check !== b[i].check || a[i].index !== b[i].index) return false;\\n+        return true;\\n+      };\\n+      const matches = visible.filter(e => e.checkId === depId && sameScope(e.scope as any, scope));\\n+      let journalResult = (\\n+        matches.length > 0 ? matches[matches.length - 1].result : undefined\\n+      ) as any;\\n+\\n+      if (\\n+        journalResult &&\\n+        Array.isArray(journalResult.forEachItems) &&\\n+        currentIndex !== undefined\\n+      ) {\\n+        const perItemSummary: any = (journalResult.forEachItemResults &&\\n+          journalResult.forEachItemResults[currentIndex]) || { issues: [] };\\n+        const perItemOutput = journalResult.forEachItems[currentIndex];\\n+        const combined = { ...perItemSummary, output: perItemOutput } as ReviewSummary;\\n+        dependencyResults.set(depId, combined);\\n+        continue;\\n+      }\\n+\\n+      if (!journalResult) {\\n+        try {\\n+          const { ContextView } = require('../../snapshot-store');\\n+          const rawContextView = new ContextView(\\n+            context.journal,\\n+            context.sessionId,\\n+            snapshotId,\\n+            [],\\n+            context.event\\n+          );\\n+          const raw = rawContextView.get(depId);\\n+          if (raw && Array.isArray((raw as any).forEachItems) && currentIndex !== undefined) {\\n+            const perItemSummary: any = ((raw as any).forEachItemResults &&\\n+              (raw as any).forEachItemResults[currentIndex]) || { issues: [] };\\n+            const perItemOutput = (raw as any).forEachItems[currentIndex];\\n+            journalResult = { ...perItemSummary, output: perItemOutput } as ReviewSummary;\\n+          }\\n+        } catch {\\n+          // ignore\\n+        }\\n+      }\\n+\\n+      if (journalResult) {\\n+        dependencyResults.set(depId, journalResult as ReviewSummary);\\n+      }\\n+    } catch {\\n+      // ignore individual dep failures\\n+    }\\n+  }\\n+\\n+  // Also expose raw outputs for all checks as a best-effort convenience\\n+  try {\\n+    const snapshotId = context.journal.beginSnapshot();\\n+    const allEntries = context.journal.readVisible(\\n+      context.sessionId,\\n+      snapshotId,\\n+      context.event as any\\n+    );\\n+    const allCheckNames = Array.from(new Set(allEntries.map((e: any) => e.checkId)));\\n+\\n+    for (const checkName of allCheckNames) {\\n+      try {\\n+        const { ContextView } = require('../../snapshot-store');\\n+        const rawContextView = new ContextView(\\n+          context.journal,\\n+          context.sessionId,\\n+          snapshotId,\\n+          scope,\\n+          context.event\\n+        );\\n+        const jr = rawContextView.get(checkName);\\n+        if (jr) dependencyResults.set(checkName, jr as ReviewSummary);\\n+      } catch {}\\n+    }\\n+\\n+    for (const checkName of allCheckNames) {\\n+      const checkCfg = context.config.checks?.[checkName];\\n+      if (checkCfg?.forEach) {\\n+        try {\\n+          const { ContextView } = require('../../snapshot-store');\\n+          const rawContextView = new ContextView(\\n+            context.journal,\\n+            context.sessionId,\\n+            snapshotId,\\n+            [],\\n+            context.event\\n+          );\\n+          const rawResult = rawContextView.get(checkName);\\n+          if (rawResult && (rawResult as any).forEachItems) {\\n+            const rawKey = `${checkName}-raw`;\\n+            dependencyResults.set(rawKey, {\\n+              issues: [],\\n+              output: (rawResult as any).forEachItems,\\n+            } as ReviewSummary);\\n+          }\\n+        } catch {}\\n+      }\\n+    }\\n+  } catch {}\\n+\\n+  return dependencyResults;\\n+}\\n+\\n+export function buildDependencyResults(\\n+  checkId: string,\\n+  checkConfig: any,\\n+  context: EngineContext,\\n+  _state: RunState\\n+): Map<string, ReviewSummary> {\\n+  return buildDependencyResultsWithScope(checkId, checkConfig, context, []);\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/state-machine/dispatch/execution-invoker.ts\",\"additions\":15,\"deletions\":0,\"changes\":516,\"patch\":\"diff --git a/src/state-machine/dispatch/execution-invoker.ts b/src/state-machine/dispatch/execution-invoker.ts\\nnew file mode 100644\\nindex 00000000..bac5fa92\\n--- /dev/null\\n+++ b/src/state-machine/dispatch/execution-invoker.ts\\n@@ -0,0 +1,516 @@\\n+import type { EngineContext, EngineEvent, EngineState, RunState } from '../../types/engine';\\n+import type { ReviewIssue, ReviewSummary } from '../../reviewer';\\n+import type { CheckExecutionStats } from '../../types/execution';\\n+import type { CheckConfig } from '../../types/config';\\n+import { logger } from '../../logger';\\n+import { withActiveSpan } from '../../telemetry/trace-helpers';\\n+import { emitMermaidFromMarkdown } from '../../utils/mermaid-telemetry';\\n+import { emitNdjsonFallback } from '../../telemetry/fallback-ndjson';\\n+import { buildOutputHistoryFromJournal } from './history-snapshot';\\n+import { buildDependencyResultsWithScope } from './dependency-gating';\\n+import { renderTemplateContent } from './template-renderer';\\n+import { hasFatalIssues } from './stats-manager';\\n+import { handleRouting } from '../states/routing';\\n+import { executeCheckWithForEachItems } from './foreach-processor';\\n+import { updateStats } from './stats-manager';\\n+\\n+/**\\n+ * Execute a single check with provider integration (non-forEach path, but handles forEach parent outputs\\n+ * and can redirect to forEach processor upstream).\\n+ *\\n+ * Note: for deciding whether to run, this function relies on the provided evaluateIf callback.\\n+ */\\n+export async function executeSingleCheck(\\n+  checkId: string,\\n+  context: EngineContext,\\n+  state: RunState,\\n+  emitEvent: (event: EngineEvent) => void,\\n+  transition: (newState: EngineState) => void,\\n+  evaluateIf: (\\n+    checkId: string,\\n+    checkConfig: CheckConfig,\\n+    context: EngineContext,\\n+    state: RunState\\n+  ) => Promise<boolean>,\\n+  scopeOverride?: Array<{ check: string; index: number }>\\n+): Promise<ReviewSummary> {\\n+  const checkConfig = context.config.checks?.[checkId];\\n+\\n+  // Evaluate 'if' condition before execution\\n+  if (checkConfig?.if) {\\n+    const shouldRun = await evaluateIf(checkId, checkConfig, context, state);\\n+    if (!shouldRun) {\\n+      logger.info(\\n+        `‚è≠  Skipped (if: ${checkConfig.if.substring(0, 40)}${checkConfig.if.length > 40 ? '...' : ''})`\\n+      );\\n+      const emptyResult: ReviewSummary = { issues: [] };\\n+      try {\\n+        Object.defineProperty(emptyResult as any, '__skipped', {\\n+          value: 'if_condition',\\n+          enumerable: false,\\n+        });\\n+      } catch {}\\n+      state.completedChecks.add(checkId);\\n+      const stats: CheckExecutionStats = {\\n+        checkName: checkId,\\n+        totalRuns: 0,\\n+        successfulRuns: 0,\\n+        failedRuns: 0,\\n+        skippedRuns: 0,\\n+        skipped: true,\\n+        skipReason: 'if_condition',\\n+        skipCondition: checkConfig.if,\\n+        totalDuration: 0,\\n+        issuesFound: 0,\\n+        issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n+      };\\n+      state.stats.set(checkId, stats);\\n+      logger.info(`[LevelDispatch] Recorded skip stats for ${checkId}: skipReason=if_condition`);\\n+      try {\\n+        context.journal.commitEntry({\\n+          sessionId: context.sessionId,\\n+          checkId,\\n+          result: emptyResult as any,\\n+          event: context.event || 'manual',\\n+          scope: [],\\n+        });\\n+      } catch (error) {\\n+        logger.warn(`[LevelDispatch] Failed to commit skipped result to journal: ${error}`);\\n+      }\\n+      emitEvent({ type: 'CheckCompleted', checkId, scope: [], result: emptyResult });\\n+      return emptyResult;\\n+    }\\n+  }\\n+\\n+  const dependencies = checkConfig?.depends_on || [];\\n+  const depList = Array.isArray(dependencies) ? dependencies : [dependencies];\\n+\\n+  // Dependency gating with continue_on_failure and OR groups\\n+  const failedChecks = (state as any).failedChecks as Set<string> | undefined;\\n+  const tokens = depList.filter(Boolean) as string[];\\n+  const groupSatisfied = (token: string): boolean => {\\n+    const options = token.includes('|')\\n+      ? token\\n+          .split('|')\\n+          .map(s => s.trim())\\n+          .filter(Boolean)\\n+      : [token];\\n+    for (const opt of options) {\\n+      const depCfg: any = context.config.checks?.[opt];\\n+      const cont = !!(depCfg && depCfg.continue_on_failure === true);\\n+      const st = state.stats.get(opt);\\n+      const wasMarkedFailed = !!(failedChecks && failedChecks.has(opt));\\n+      const skipped = !!(st && (st as any).skipped === true);\\n+      const failedOnly = !!(st && (st.failedRuns || 0) > 0 && (st.successfulRuns || 0) === 0);\\n+      const satisfied = !skipped && ((!failedOnly && !wasMarkedFailed) || cont);\\n+      if (satisfied) return true;\\n+    }\\n+    return false;\\n+  };\\n+\\n+  if (tokens.length > 0) {\\n+    let allOk = true;\\n+    for (const t of tokens) {\\n+      if (!groupSatisfied(t)) {\\n+        allOk = false;\\n+        break;\\n+      }\\n+    }\\n+    if (!allOk) {\\n+      const emptyResult: ReviewSummary = { issues: [] };\\n+      try {\\n+        Object.defineProperty(emptyResult as any, '__skipped', {\\n+          value: 'dependency_failed',\\n+          enumerable: false,\\n+        });\\n+      } catch {}\\n+      state.completedChecks.add(checkId);\\n+      (state as any).failedChecks = (state as any).failedChecks || new Set<string>();\\n+      (state as any).failedChecks.add(checkId);\\n+      const stats: CheckExecutionStats = {\\n+        checkName: checkId,\\n+        totalRuns: 0,\\n+        successfulRuns: 0,\\n+        failedRuns: 0,\\n+        skippedRuns: 0,\\n+        skipped: true,\\n+        skipReason: 'dependency_failed',\\n+        totalDuration: 0,\\n+        issuesFound: 0,\\n+        issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n+      };\\n+      state.stats.set(checkId, stats);\\n+      try {\\n+        context.journal.commitEntry({\\n+          sessionId: context.sessionId,\\n+          checkId,\\n+          result: emptyResult as any,\\n+          event: context.event || 'manual',\\n+          scope: [],\\n+        });\\n+      } catch (error) {\\n+        logger.warn(`[LevelDispatch] Failed to commit empty result to journal: ${error}`);\\n+      }\\n+      emitEvent({ type: 'CheckCompleted', checkId, scope: [], result: emptyResult });\\n+      return emptyResult;\\n+    }\\n+  }\\n+\\n+  let forEachParent: string | undefined;\\n+  let forEachItems: unknown[] | undefined;\\n+  for (const depId of depList) {\\n+    if (!depId) continue;\\n+    try {\\n+      const snapshotId = context.journal.beginSnapshot();\\n+      const { ContextView } = require('../../snapshot-store');\\n+      const contextView = new ContextView(\\n+        context.journal,\\n+        context.sessionId,\\n+        snapshotId,\\n+        [],\\n+        context.event\\n+      );\\n+      const depResult: any = contextView.get(depId);\\n+      if (depResult?.forEachItems && Array.isArray(depResult.forEachItems)) {\\n+        forEachParent = depId;\\n+        forEachItems = depResult.forEachItems;\\n+        break;\\n+      }\\n+    } catch {}\\n+  }\\n+\\n+  // If there's a forEach parent and items, decide fanout here (map vs reduce)\\n+  if (forEachParent && forEachItems !== undefined) {\\n+    let fanoutMode: 'map' | 'reduce' = 'reduce';\\n+    const explicit = (checkConfig as any)?.fanout as 'map' | 'reduce' | undefined;\\n+    if (explicit === 'map' || explicit === 'reduce') fanoutMode = explicit;\\n+    else {\\n+      const providerType = context.checks[checkId]?.providerType || '';\\n+      const reduceProviders = new Set(['log', 'memory', 'script', 'workflow', 'noop']);\\n+      fanoutMode = reduceProviders.has(providerType) ? 'reduce' : 'map';\\n+    }\\n+    if (fanoutMode === 'map') {\\n+      if ((forEachItems as unknown[]).length === 0) {\\n+        logger.info(`‚è≠  Skipped (forEach parent \\\"${forEachParent}\\\" has 0 items)`);\\n+        const emptyResult: ReviewSummary = { issues: [] };\\n+        try {\\n+          Object.defineProperty(emptyResult as any, '__skipped', {\\n+            value: 'forEach_empty',\\n+            enumerable: false,\\n+          });\\n+        } catch {}\\n+        state.completedChecks.add(checkId);\\n+        // Mark as failed for dependents if parent failed\\n+        let derivedSkipReason: 'forEach_empty' | 'dependency_failed' = 'forEach_empty';\\n+        try {\\n+          const parentFailed =\\n+            !!((state as any).failedChecks && (state as any).failedChecks.has(forEachParent)) ||\\n+            (() => {\\n+              const s = state.stats.get(forEachParent);\\n+              return !!(s && (s.failedRuns || 0) > 0);\\n+            })();\\n+          if (parentFailed) derivedSkipReason = 'dependency_failed';\\n+        } catch {}\\n+        const stats: CheckExecutionStats = {\\n+          checkName: checkId,\\n+          totalRuns: 0,\\n+          successfulRuns: 0,\\n+          failedRuns: 0,\\n+          skippedRuns: 0,\\n+          skipped: true,\\n+          skipReason: derivedSkipReason,\\n+          totalDuration: 0,\\n+          issuesFound: 0,\\n+          issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n+        };\\n+        state.stats.set(checkId, stats);\\n+        try {\\n+          context.journal.commitEntry({\\n+            sessionId: context.sessionId,\\n+            checkId,\\n+            result: emptyResult as any,\\n+            event: context.event || 'manual',\\n+            scope: [],\\n+          });\\n+        } catch (error) {\\n+          logger.warn(`[LevelDispatch] Failed to commit empty result to journal: ${error}`);\\n+        }\\n+        emitEvent({ type: 'CheckCompleted', checkId, scope: [], result: emptyResult });\\n+        return emptyResult;\\n+      }\\n+      return await executeCheckWithForEachItems(\\n+        checkId,\\n+        forEachParent,\\n+        forEachItems as unknown[],\\n+        context,\\n+        state,\\n+        emitEvent,\\n+        transition\\n+      );\\n+    }\\n+    // fanout reduce ‚Üí fall through to normal single execution\\n+  }\\n+\\n+  // Normal execution without forEach\\n+  const scope: Array<{ check: string; index: number }> = scopeOverride || [];\\n+  emitEvent({ type: 'CheckScheduled', checkId, scope });\\n+\\n+  const startTime = Date.now();\\n+  const dispatch: any = {\\n+    id: `${checkId}-${Date.now()}`,\\n+    checkId,\\n+    scope,\\n+    provider: context.checks[checkId]?.providerType || 'unknown',\\n+    startMs: startTime,\\n+    attempts: 1,\\n+  };\\n+  state.activeDispatches.set(checkId, dispatch);\\n+\\n+  try {\\n+    if (!checkConfig) throw new Error(`Check configuration not found: ${checkId}`);\\n+    const providerType = checkConfig.type || 'ai';\\n+    const providerRegistry =\\n+      require('../../providers/check-provider-registry').CheckProviderRegistry.getInstance();\\n+    const provider = providerRegistry.getProviderOrThrow(providerType);\\n+\\n+    const outputHistory = buildOutputHistoryFromJournal(context);\\n+    const providerConfig: any = {\\n+      type: providerType,\\n+      checkName: checkId,\\n+      prompt: checkConfig.prompt,\\n+      exec: checkConfig.exec,\\n+      schema: checkConfig.schema,\\n+      group: checkConfig.group,\\n+      focus: checkConfig.focus || mapCheckNameToFocus(checkId),\\n+      transform: checkConfig.transform,\\n+      transform_js: checkConfig.transform_js,\\n+      env: checkConfig.env,\\n+      forEach: checkConfig.forEach,\\n+      ...checkConfig,\\n+      eventContext: (context.prInfo as any)?.eventContext || {},\\n+      __outputHistory: outputHistory,\\n+      ai: {\\n+        ...(checkConfig.ai || {}),\\n+        timeout: checkConfig.ai?.timeout || 600000,\\n+        debug: !!context.debug,\\n+      },\\n+    };\\n+\\n+    const dependencyResults = buildDependencyResultsWithScope(checkId, checkConfig, context, scope);\\n+    const prInfo: any = context.prInfo || {\\n+      number: 1,\\n+      title: 'State Machine Execution',\\n+      author: 'system',\\n+      eventType: context.event || 'manual',\\n+      eventContext: {},\\n+      files: [],\\n+      commits: [],\\n+    };\\n+    const executionContext = {\\n+      ...context.executionContext,\\n+      _engineMode: context.mode,\\n+      _parentContext: context,\\n+      _parentState: state,\\n+    };\\n+\\n+    try {\\n+      emitNdjsonFallback('visor.provider', {\\n+        'visor.check.id': checkId,\\n+        'visor.provider.type': providerType,\\n+      });\\n+    } catch {}\\n+\\n+    const result = await withActiveSpan(\\n+      `visor.check.${checkId}`,\\n+      { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+      async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n+    );\\n+\\n+    const enrichedIssues = (result.issues || []).map((issue: ReviewIssue) => ({\\n+      ...issue,\\n+      checkName: checkId,\\n+      ruleId: `${checkId}/${issue.ruleId || 'unknown'}`,\\n+      group: checkConfig.group,\\n+      schema: typeof checkConfig.schema === 'object' ? 'custom' : checkConfig.schema,\\n+      template: checkConfig.template,\\n+      timestamp: Date.now(),\\n+    }));\\n+    const enrichedResult: any = { ...result, issues: enrichedIssues };\\n+\\n+    // Handle forEach:true output from provider (convert to items)\\n+    let isForEach = false;\\n+    let forEachItemsLocal: unknown[] | undefined;\\n+    if (checkConfig.forEach) {\\n+      const output = (result as any).output;\\n+      if (Array.isArray(output)) {\\n+        isForEach = true;\\n+        forEachItemsLocal = output;\\n+        enrichedResult.isForEach = true;\\n+        enrichedResult.forEachItems = output;\\n+      } else {\\n+        if (context.debug)\\n+          logger.warn(\\n+            `[LevelDispatch] Check ${checkId} has forEach:true but output is not an array: ${typeof output}, converting to single-item array`\\n+          );\\n+        isForEach = true;\\n+        forEachItemsLocal = [output];\\n+        enrichedResult.isForEach = true;\\n+        enrichedResult.forEachItems = [output];\\n+      }\\n+    }\\n+    if ((result as any).isForEach) enrichedResult.isForEach = true;\\n+    if ((result as any).forEachItems) enrichedResult.forEachItems = (result as any).forEachItems;\\n+    if ((result as any).forEachItemResults)\\n+      enrichedResult.forEachItemResults = (result as any).forEachItemResults;\\n+    if ((result as any).forEachFatalMask)\\n+      enrichedResult.forEachFatalMask = (result as any).forEachFatalMask;\\n+\\n+    let renderedContent: string | undefined;\\n+    try {\\n+      renderedContent = await renderTemplateContent(checkId, checkConfig, enrichedResult);\\n+      if (renderedContent) emitMermaidFromMarkdown(checkId, renderedContent, 'content');\\n+    } catch (error) {\\n+      logger.warn(`[LevelDispatch] Failed to render template for ${checkId}: ${error}`);\\n+    }\\n+\\n+    let outputWithTimestamp: any = undefined;\\n+    if ((result as any).output !== undefined) {\\n+      const output = (result as any).output;\\n+      if (output !== null && typeof output === 'object' && !Array.isArray(output))\\n+        outputWithTimestamp = { ...output, ts: Date.now() };\\n+      else outputWithTimestamp = output;\\n+    }\\n+\\n+    const enrichedResultWithContent = renderedContent\\n+      ? { ...enrichedResult, content: renderedContent }\\n+      : enrichedResult;\\n+    const enrichedResultWithTimestamp =\\n+      outputWithTimestamp !== undefined\\n+        ? { ...enrichedResultWithContent, output: outputWithTimestamp }\\n+        : enrichedResultWithContent;\\n+\\n+    state.completedChecks.add(checkId);\\n+    const currentWaveCompletions = (state as any).currentWaveCompletions as Set<string> | undefined;\\n+    if (currentWaveCompletions) currentWaveCompletions.add(checkId);\\n+\\n+    // Process routing (fail_if, on_success, on_fail) BEFORE storing in journal\\n+    // Same behavior as inlined version in LevelDispatch so routing errors are captured.\\n+    try {\\n+      logger.info(`[LevelDispatch] Calling handleRouting for ${checkId}`);\\n+    } catch {}\\n+    await handleRouting(context, state, transition, emitEvent, {\\n+      checkId,\\n+      scope,\\n+      result: enrichedResult,\\n+      checkConfig: checkConfig as CheckConfig,\\n+      success: !hasFatalIssues(enrichedResult),\\n+    });\\n+\\n+    try {\\n+      const commitResult: any = {\\n+        ...enrichedResult,\\n+        ...(renderedContent ? { content: renderedContent } : {}),\\n+        ...((result as any).output !== undefined\\n+          ? outputWithTimestamp !== undefined\\n+            ? { output: outputWithTimestamp }\\n+            : { output: (result as any).output }\\n+          : {}),\\n+      };\\n+      context.journal.commitEntry({\\n+        sessionId: context.sessionId,\\n+        checkId,\\n+        result: commitResult,\\n+        event: context.event || 'manual',\\n+        scope,\\n+      });\\n+    } catch (error) {\\n+      logger.warn(`[LevelDispatch] Failed to commit to journal: ${error}`);\\n+    }\\n+\\n+    // Update stats for this single-run execution to ensure visibility\\n+    try {\\n+      const duration = Date.now() - startTime;\\n+      updateStats([{ checkId, result: enrichedResult as any, duration }], state, false);\\n+    } catch {}\\n+\\n+    if (isForEach) {\\n+      try {\\n+        const existing = state.stats.get(checkId);\\n+        const aggStats: CheckExecutionStats = existing || {\\n+          checkName: checkId,\\n+          totalRuns: 0,\\n+          successfulRuns: 0,\\n+          failedRuns: 0,\\n+          skippedRuns: 0,\\n+          skipped: false,\\n+          totalDuration: 0,\\n+          issuesFound: 0,\\n+          issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n+        };\\n+        aggStats.totalRuns++;\\n+        const hasFatal = hasFatalIssues(enrichedResultWithTimestamp as any);\\n+        if (hasFatal) aggStats.failedRuns++;\\n+        else aggStats.successfulRuns++;\\n+        const items = (enrichedResultWithTimestamp as any).forEachItems;\\n+        if (Array.isArray(items)) aggStats.outputsProduced = items.length;\\n+        state.stats.set(checkId, aggStats);\\n+      } catch {}\\n+    }\\n+\\n+    if (isForEach && forEachItemsLocal && Array.isArray(forEachItemsLocal)) {\\n+      for (let itemIndex = 0; itemIndex < forEachItemsLocal.length; itemIndex++) {\\n+        const itemScope: Array<{ check: string; index: number }> = [\\n+          { check: checkId, index: itemIndex },\\n+        ];\\n+        const item = forEachItemsLocal[itemIndex];\\n+        try {\\n+          context.journal.commitEntry({\\n+            sessionId: context.sessionId,\\n+            checkId,\\n+            result: { issues: [], output: item } as any,\\n+            event: context.event || 'manual',\\n+            scope: itemScope,\\n+          });\\n+        } catch (error) {\\n+          logger.warn(\\n+            `[LevelDispatch] Failed to commit per-item journal for ${checkId} item ${itemIndex}: ${error}`\\n+          );\\n+        }\\n+      }\\n+    }\\n+\\n+    state.activeDispatches.delete(checkId);\\n+    emitEvent({\\n+      type: 'CheckCompleted',\\n+      checkId,\\n+      scope,\\n+      result: {\\n+        ...enrichedResult,\\n+        output: (result as any).output,\\n+        content: renderedContent || (result as any).content,\\n+      },\\n+    });\\n+    return enrichedResult;\\n+  } catch (error) {\\n+    const err = error instanceof Error ? error : new Error(String(error));\\n+    logger.error(`[LevelDispatch] Error executing check ${checkId}: ${err.message}`);\\n+    state.activeDispatches.delete(checkId);\\n+    emitEvent({\\n+      type: 'CheckErrored',\\n+      checkId,\\n+      scope,\\n+      error: { message: err.message, stack: err.stack, name: err.name },\\n+    });\\n+    throw err;\\n+  }\\n+}\\n+\\n+function mapCheckNameToFocus(checkName: string): string {\\n+  const focusMap: Record<string, string> = {\\n+    security: 'security',\\n+    performance: 'performance',\\n+    style: 'style',\\n+    architecture: 'architecture',\\n+  };\\n+  return focusMap[checkName] || 'all';\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/state-machine/dispatch/foreach-processor.ts\",\"additions\":17,\"deletions\":0,\"changes\":586,\"patch\":\"diff --git a/src/state-machine/dispatch/foreach-processor.ts b/src/state-machine/dispatch/foreach-processor.ts\\nnew file mode 100644\\nindex 00000000..43baf994\\n--- /dev/null\\n+++ b/src/state-machine/dispatch/foreach-processor.ts\\n@@ -0,0 +1,586 @@\\n+import type { EngineContext, EngineEvent, EngineState, RunState } from '../../types/engine';\\n+import type { ReviewIssue, ReviewSummary } from '../../reviewer';\\n+import { logger } from '../../logger';\\n+import { emitNdjsonSpanWithEvents } from '../../telemetry/fallback-ndjson';\\n+import { withActiveSpan } from '../../telemetry/trace-helpers';\\n+import type { CheckProviderConfig } from '../../providers/check-provider.interface';\\n+import { buildOutputHistoryFromJournal } from './history-snapshot';\\n+import { buildDependencyResultsWithScope } from './dependency-gating';\\n+import { updateStats, hasFatalIssues } from './stats-manager';\\n+import { handleRouting, checkLoopBudget, evaluateGoto } from '../states/routing';\\n+\\n+/**\\n+ * Execute a check once per forEach item (map fanout path).\\n+ * Extracted from LevelDispatch without behavior change.\\n+ */\\n+export async function executeCheckWithForEachItems(\\n+  checkId: string,\\n+  forEachParent: string,\\n+  forEachItems: unknown[],\\n+  context: EngineContext,\\n+  state: RunState,\\n+  emitEvent: (event: EngineEvent) => void,\\n+  transition: (newState: EngineState) => void\\n+): Promise<ReviewSummary> {\\n+  // Tactical correctness fix: re-read the parent's aggregated forEachItems\\n+  try {\\n+    const snapId = context.journal.beginSnapshot();\\n+    const visible = context.journal.readVisible(context.sessionId, snapId, context.event as any);\\n+    let latestItems: unknown[] | undefined;\\n+    for (let i = visible.length - 1; i >= 0; i--) {\\n+      const e = visible[i];\\n+      if (e.checkId === forEachParent && Array.isArray(e.scope) && e.scope.length === 0) {\\n+        const r: any = e.result;\\n+        if (r && Array.isArray(r.forEachItems)) {\\n+          latestItems = r.forEachItems as unknown[];\\n+          break;\\n+        }\\n+      }\\n+    }\\n+    if (Array.isArray(latestItems)) {\\n+      if (context.debug) {\\n+        try {\\n+          const prevLen = Array.isArray(forEachItems) ? (forEachItems as any[]).length : 0;\\n+          const newLen = latestItems.length;\\n+          if (prevLen !== newLen) {\\n+            logger.info(\\n+              `[LevelDispatch] Refreshing forEachItems for ${checkId}: from parent '${forEachParent}' latestItems=${newLen} (was ${prevLen})`\\n+            );\\n+          }\\n+        } catch {}\\n+      }\\n+      forEachItems = latestItems as unknown[];\\n+    }\\n+  } catch (e) {\\n+    if (context.debug) {\\n+      logger.warn(\\n+        `[LevelDispatch] Failed to refresh forEachItems from journal for ${forEachParent}: ${e}`\\n+      );\\n+    }\\n+  }\\n+\\n+  const checkConfig = context.config.checks?.[checkId];\\n+  if (!checkConfig) {\\n+    throw new Error(`Check configuration not found: ${checkId}`);\\n+  }\\n+\\n+  logger.info(\\n+    `[LevelDispatch][DEBUG] executeCheckWithForEachItems: checkId=${checkId}, forEachParent=${forEachParent}, items=${forEachItems.length}`\\n+  );\\n+  logger.info(\\n+    `[LevelDispatch][DEBUG] forEachItems: ${JSON.stringify(forEachItems).substring(0, 200)}`\\n+  );\\n+\\n+  const allIssues: ReviewIssue[] = [];\\n+  const perItemResults: ReviewSummary[] = [];\\n+  const allOutputs: unknown[] = [];\\n+  const allContents: string[] = [];\\n+  const perIterationDurations: number[] = [];\\n+\\n+  for (let itemIndex = 0; itemIndex < forEachItems.length; itemIndex++) {\\n+    const iterationStartMs = Date.now();\\n+    const scope: Array<{ check: string; index: number }> = [\\n+      { check: forEachParent, index: itemIndex },\\n+    ];\\n+\\n+    const forEachItem = forEachItems[itemIndex];\\n+    logger.info(\\n+      `[LevelDispatch][DEBUG] Starting iteration ${itemIndex} of ${checkId}, parent=${forEachParent}, item=${JSON.stringify(forEachItem)?.substring(0, 100)}`\\n+    );\\n+\\n+    const shouldSkipDueToParentFailure =\\n+      (forEachItem as any)?.__failed === true || (forEachItem as any)?.__skip === true;\\n+    if (shouldSkipDueToParentFailure) {\\n+      logger.info(\\n+        `‚è≠  Skipped ${checkId} iteration ${itemIndex} (forEach parent \\\"${forEachParent}\\\" iteration ${itemIndex} marked as failed)`\\n+      );\\n+      const iterationDurationMs = Date.now() - iterationStartMs;\\n+      perIterationDurations.push(iterationDurationMs);\\n+      perItemResults.push({ issues: [] });\\n+      allOutputs.push({ __skip: true });\\n+      continue;\\n+    }\\n+\\n+    try {\\n+      emitNdjsonSpanWithEvents(\\n+        'visor.foreach.item',\\n+        {\\n+          'visor.check.id': checkId,\\n+          'visor.foreach.index': itemIndex,\\n+          'visor.foreach.total': forEachItems.length,\\n+        },\\n+        []\\n+      );\\n+    } catch (error) {\\n+      logger.warn(`[LevelDispatch] Failed to emit foreach.item span: ${error}`);\\n+    }\\n+\\n+    emitEvent({ type: 'CheckScheduled', checkId, scope });\\n+\\n+    const dispatch = {\\n+      id: `${checkId}-${itemIndex}-${Date.now()}`,\\n+      checkId,\\n+      scope,\\n+      provider: context.checks[checkId]?.providerType || 'unknown',\\n+      startMs: Date.now(),\\n+      attempts: 1,\\n+      foreachIndex: itemIndex,\\n+    };\\n+    state.activeDispatches.set(`${checkId}-${itemIndex}`, dispatch as any);\\n+\\n+    try {\\n+      const providerType = checkConfig.type || 'ai';\\n+      const providerRegistry =\\n+        require('../../providers/check-provider-registry').CheckProviderRegistry.getInstance();\\n+      const provider = providerRegistry.getProviderOrThrow(providerType);\\n+\\n+      const outputHistory = buildOutputHistoryFromJournal(context);\\n+\\n+      const providerConfig: CheckProviderConfig = {\\n+        type: providerType,\\n+        checkName: checkId,\\n+        prompt: checkConfig.prompt,\\n+        exec: checkConfig.exec,\\n+        schema: checkConfig.schema,\\n+        group: checkConfig.group,\\n+        focus:\\n+          checkConfig.focus ||\\n+          ((): string => {\\n+            const focusMap: Record<string, string> = {\\n+              security: 'security',\\n+              performance: 'performance',\\n+              style: 'style',\\n+              architecture: 'architecture',\\n+            };\\n+            return focusMap[checkId] || 'all';\\n+          })(),\\n+        transform: checkConfig.transform,\\n+        transform_js: checkConfig.transform_js,\\n+        env: checkConfig.env,\\n+        forEach: checkConfig.forEach,\\n+        ...checkConfig,\\n+        eventContext: (context.prInfo as any)?.eventContext || {},\\n+        __outputHistory: outputHistory,\\n+        ai: {\\n+          ...(checkConfig.ai || {}),\\n+          timeout: checkConfig.ai?.timeout || 600000,\\n+          debug: !!context.debug,\\n+        },\\n+      };\\n+\\n+      const dependencyResults = buildDependencyResultsWithScope(\\n+        checkId,\\n+        checkConfig,\\n+        context,\\n+        scope\\n+      );\\n+\\n+      // Per-item dependency gating for map fanout\\n+      try {\\n+        const rawDeps = (checkConfig as any)?.depends_on || [];\\n+        const depList = Array.isArray(rawDeps) ? rawDeps : [rawDeps];\\n+        if (depList.length > 0) {\\n+          const groupSatisfied = (token: string): boolean => {\\n+            if (typeof token !== 'string') return true;\\n+            const orOptions = token.includes('|')\\n+              ? token\\n+                  .split('|')\\n+                  .map(s => s.trim())\\n+                  .filter(Boolean)\\n+              : [token];\\n+            for (const opt of orOptions) {\\n+              const dr = dependencyResults.get(opt) as ReviewSummary | undefined;\\n+              const depCfg = context.config.checks?.[opt];\\n+              const cont = !!(depCfg && (depCfg as any).continue_on_failure === true);\\n+              let failed = false;\\n+              let skipped = false;\\n+              if (!dr) failed = true;\\n+              else {\\n+                const out: any = (dr as any).output;\\n+                const fatal = hasFatalIssues(dr as any);\\n+                failed = fatal || (!!out && typeof out === 'object' && out.__failed === true);\\n+                skipped = !!(out && typeof out === 'object' && out.__skip === true);\\n+              }\\n+              const satisfied = !skipped && (!failed || cont);\\n+              if (satisfied) return true;\\n+            }\\n+            return false;\\n+          };\\n+\\n+          let allSatisfied = true;\\n+          for (const token of depList) {\\n+            if (!groupSatisfied(token as any)) {\\n+              allSatisfied = false;\\n+              break;\\n+            }\\n+          }\\n+\\n+          if (!allSatisfied) {\\n+            if (context.debug) {\\n+              logger.info(\\n+                `[LevelDispatch] Skipping ${checkId} iteration ${itemIndex} due to unsatisfied dependency group(s)`\\n+              );\\n+            }\\n+            const iterationDurationMs = Date.now() - iterationStartMs;\\n+            perIterationDurations.push(iterationDurationMs);\\n+            perItemResults.push({ issues: [] });\\n+            allOutputs.push({ __skip: true });\\n+            continue;\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      const prInfo: any = context.prInfo || {\\n+        number: 1,\\n+        title: 'State Machine Execution',\\n+        author: 'system',\\n+        eventType: context.event || 'manual',\\n+        eventContext: {},\\n+        files: [],\\n+        commits: [],\\n+      };\\n+      const executionContext = {\\n+        ...context.executionContext,\\n+        _engineMode: context.mode,\\n+        _parentContext: context,\\n+        _parentState: state,\\n+      };\\n+\\n+      const result = await withActiveSpan(\\n+        `visor.check.${checkId}`,\\n+        { 'visor.check.id': checkId, 'visor.check.type': providerType },\\n+        async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n+      );\\n+\\n+      const enrichedIssues = (result.issues || []).map((issue: ReviewIssue) => ({\\n+        ...issue,\\n+        checkName: checkId,\\n+        ruleId: `${checkId}/${issue.ruleId || 'unknown'}`,\\n+        group: checkConfig.group,\\n+        schema: typeof checkConfig.schema === 'object' ? 'custom' : checkConfig.schema,\\n+        template: checkConfig.template,\\n+        timestamp: Date.now(),\\n+      }));\\n+\\n+      const enrichedResult: any = { ...result, issues: enrichedIssues };\\n+\\n+      // Update stats for this iteration\\n+      const iterationDurationMs = Date.now() - iterationStartMs;\\n+      perIterationDurations.push(iterationDurationMs);\\n+      updateStats(\\n+        [{ checkId, result: enrichedResult as any, duration: iterationDurationMs }],\\n+        state,\\n+        true\\n+      );\\n+\\n+      // Commit per-iteration journal result (scoped)\\n+      try {\\n+        context.journal.commitEntry({\\n+          sessionId: context.sessionId,\\n+          checkId,\\n+          result: enrichedResult as any,\\n+          event: context.event || 'manual',\\n+          scope,\\n+        });\\n+        logger.info(\\n+          `[LevelDispatch][DEBUG] Committing to journal: checkId=${checkId}, scope=${JSON.stringify(scope)}, hasOutput=${enrichedResult.output !== undefined}`\\n+        );\\n+      } catch (error) {\\n+        logger.warn(`[LevelDispatch] Failed to commit per-iteration result to journal: ${error}`);\\n+      }\\n+\\n+      // Record per-item summary and aggregate outputs/contents\\n+      perItemResults.push(enrichedResult);\\n+      if ((enrichedResult as any).content)\\n+        allContents.push(String((enrichedResult as any).content));\\n+      if ((enrichedResult as any).output !== undefined)\\n+        allOutputs.push((enrichedResult as any).output);\\n+      allIssues.push(...(enrichedResult.issues || []));\\n+\\n+      // Emit completion for iteration\\n+      emitEvent({ type: 'CheckCompleted', checkId, scope, result: enrichedResult });\\n+    } catch (error) {\\n+      const iterationDurationMs = Date.now() - iterationStartMs;\\n+      perIterationDurations.push(iterationDurationMs);\\n+      const err = error instanceof Error ? error : new Error(String(error));\\n+      logger.error(\\n+        `[LevelDispatch] Error executing ${checkId} iteration ${itemIndex}: ${err.message}`\\n+      );\\n+      updateStats(\\n+        [\\n+          {\\n+            checkId,\\n+            result: {\\n+              issues: [\\n+                {\\n+                  severity: 'error',\\n+                  category: 'logic',\\n+                  ruleId: `${checkId}/error`,\\n+                  file: 'system',\\n+                  line: 0,\\n+                  message: err.message,\\n+                  timestamp: Date.now(),\\n+                } as any,\\n+              ],\\n+            } as any,\\n+            error: err,\\n+            duration: iterationDurationMs,\\n+          },\\n+        ],\\n+        state,\\n+        true\\n+      );\\n+      allOutputs.push({ __failed: true });\\n+      perItemResults.push({\\n+        issues: [\\n+          {\\n+            severity: 'error',\\n+            category: 'logic',\\n+            ruleId: `${checkId}/error`,\\n+            file: 'system',\\n+            line: 0,\\n+            message: err.message,\\n+            timestamp: Date.now(),\\n+          } as any,\\n+        ],\\n+      });\\n+      emitEvent({\\n+        type: 'CheckErrored',\\n+        checkId,\\n+        scope,\\n+        error: { message: err.message, stack: err.stack, name: err.name },\\n+      });\\n+    } finally {\\n+      state.activeDispatches.delete(`${checkId}-${itemIndex}`);\\n+    }\\n+  }\\n+\\n+  // Update forEach metadata in stats (same behavior)\\n+  const checkStats = state.stats.get(checkId);\\n+  if (checkStats) {\\n+    checkStats.outputsProduced = allOutputs.length;\\n+    checkStats.perIterationDuration = perIterationDurations;\\n+    const previewItems = allOutputs.slice(0, 3).map(item => {\\n+      const str = typeof item === 'string' ? item : (JSON.stringify(item) ?? 'undefined');\\n+      return str.length > 50 ? str.substring(0, 50) + '...' : str;\\n+    });\\n+    checkStats.forEachPreview =\\n+      allOutputs.length > 3 ? [...previewItems, `...${allOutputs.length - 3} more`] : previewItems;\\n+    state.stats.set(checkId, checkStats);\\n+    if (checkStats.totalRuns > 0 && checkStats.failedRuns === checkStats.totalRuns) {\\n+      logger.info(\\n+        `[LevelDispatch] forEach check ${checkId} failed completely (${checkStats.failedRuns}/${checkStats.totalRuns} iterations failed)`\\n+      );\\n+      (state as any).failedChecks = (state as any).failedChecks || new Set<string>();\\n+      (state as any).failedChecks.add(checkId);\\n+    }\\n+  }\\n+\\n+  // Aggregated result\\n+  const aggregatedResult: any = {\\n+    issues: allIssues,\\n+    isForEach: true,\\n+    forEachItems: allOutputs,\\n+    forEachItemResults: perItemResults,\\n+    ...(allContents.length > 0 ? { content: allContents.join('\\\\n') } : {}),\\n+  };\\n+\\n+  logger.info(\\n+    `[LevelDispatch][DEBUG] Aggregated result for ${checkId}: forEachItems.length=${allOutputs.length}, results=${perItemResults.length}`\\n+  );\\n+  logger.info(`[LevelDispatch][DEBUG] allOutputs: ${JSON.stringify(allOutputs).substring(0, 200)}`);\\n+\\n+  // Route aggregated child before commit\\n+  try {\\n+    logger.info(`[LevelDispatch] Calling handleRouting for ${checkId}`);\\n+  } catch {}\\n+  try {\\n+    state.completedChecks.add(checkId);\\n+    const currentWaveCompletions = (state as any).currentWaveCompletions as Set<string> | undefined;\\n+    if (currentWaveCompletions) currentWaveCompletions.add(checkId);\\n+\\n+    await handleRouting(context, state, transition, emitEvent, {\\n+      checkId,\\n+      scope: [],\\n+      result: aggregatedResult as any,\\n+      checkConfig: checkConfig as any,\\n+      success: !hasFatalIssues(aggregatedResult as any),\\n+    });\\n+  } catch (error) {\\n+    logger.warn(`[LevelDispatch] Routing error for aggregated forEach ${checkId}: ${error}`);\\n+  }\\n+\\n+  try {\\n+    context.journal.commitEntry({\\n+      sessionId: context.sessionId,\\n+      checkId,\\n+      result: aggregatedResult as any,\\n+      event: context.event || 'manual',\\n+      scope: [],\\n+    });\\n+    logger.info(`[LevelDispatch][DEBUG] Committed aggregated result to journal with scope=[]`);\\n+  } catch (error) {\\n+    logger.warn(`[LevelDispatch] Failed to commit aggregated forEach result to journal: ${error}`);\\n+  }\\n+\\n+  emitEvent({ type: 'CheckCompleted', checkId, scope: [], result: aggregatedResult });\\n+\\n+  // on_finish for forEach parent after children complete (as before)\\n+  const parentCheckConfig = context.config.checks?.[forEachParent];\\n+  logger.info(\\n+    `[LevelDispatch][DEBUG] Checking on_finish for forEach parent ${forEachParent}: has_on_finish=${!!parentCheckConfig?.on_finish}, is_forEach=${!!parentCheckConfig?.forEach}`\\n+  );\\n+\\n+  if (parentCheckConfig?.on_finish && parentCheckConfig.forEach) {\\n+    logger.info(\\n+      `[LevelDispatch] Processing on_finish for forEach parent ${forEachParent} after children complete`\\n+    );\\n+    try {\\n+      const snapshotId = context.journal.beginSnapshot();\\n+      const { ContextView } = require('../../snapshot-store');\\n+      const contextView = new ContextView(\\n+        context.journal,\\n+        context.sessionId,\\n+        snapshotId,\\n+        [],\\n+        context.event\\n+      );\\n+      const parentResult = contextView.get(forEachParent);\\n+\\n+      if (parentResult) {\\n+        logger.info(\\n+          `[LevelDispatch] Found parent result for ${forEachParent}, evaluating on_finish`\\n+        );\\n+\\n+        const onFinish = parentCheckConfig.on_finish;\\n+\\n+        let queuedForward = false;\\n+        logger.info(\\n+          `[LevelDispatch] on_finish.run: ${onFinish.run?.length || 0} targets, targets=${JSON.stringify(onFinish.run || [])}`\\n+        );\\n+        if (onFinish.run && onFinish.run.length > 0) {\\n+          for (const targetCheck of onFinish.run) {\\n+            logger.info(`[LevelDispatch] Processing on_finish.run target: ${targetCheck}`);\\n+            logger.info(\\n+              `[LevelDispatch] Loop budget check: routingLoopCount=${state.routingLoopCount}, max_loops=${context.config.routing?.max_loops ?? 10}`\\n+            );\\n+            if (checkLoopBudget(context, state, 'on_finish', 'run')) {\\n+              const errorIssue: ReviewIssue = {\\n+                file: 'system',\\n+                line: 0,\\n+                ruleId: `${forEachParent}/routing/loop_budget_exceeded`,\\n+                message: `Routing loop budget exceeded (max_loops=${context.config.routing?.max_loops ?? 10}) during on_finish run`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              };\\n+              parentResult.issues = [...(parentResult.issues || []), errorIssue];\\n+              try {\\n+                context.journal.commitEntry({\\n+                  sessionId: context.sessionId,\\n+                  checkId: forEachParent,\\n+                  result: parentResult as any,\\n+                  event: context.event || 'manual',\\n+                  scope: [],\\n+                });\\n+              } catch (err) {\\n+                logger.warn(\\n+                  `[LevelDispatch] Failed to commit parent result with loop budget error: ${err}`\\n+                );\\n+              }\\n+              return aggregatedResult;\\n+            }\\n+            state.routingLoopCount++;\\n+            emitEvent({\\n+              type: 'ForwardRunRequested',\\n+              target: targetCheck,\\n+              scope: [],\\n+              origin: 'run',\\n+            });\\n+            queuedForward = true;\\n+          }\\n+        }\\n+\\n+        // Debug for goto_js\\n+        if (context.debug || true) {\\n+          logger.info(\\n+            `[LevelDispatch] Evaluating on_finish.goto_js for forEach parent: ${forEachParent}`\\n+          );\\n+          if (onFinish.goto_js)\\n+            logger.info(`[LevelDispatch] goto_js code: ${onFinish.goto_js.substring(0, 200)}`);\\n+          try {\\n+            const snapshotId2 = context.journal.beginSnapshot();\\n+            const { ContextView: CV } = require('../../snapshot-store');\\n+            const view = new CV(context.journal, context.sessionId, snapshotId2, [], undefined);\\n+            const vfHist = view.getHistory('validate-fact') || [];\\n+            logger.info(`[LevelDispatch] history['validate-fact'] length: ${vfHist.length}`);\\n+            const all = context.journal.readVisible(context.sessionId, snapshotId2, undefined);\\n+            const keys = Array.from(new Set(all.map((e: any) => e.checkId)));\\n+            logger.info(`[LevelDispatch] history keys: ${keys.join(', ')}`);\\n+          } catch {}\\n+        }\\n+\\n+        const gotoTarget = await (evaluateGoto as any)(\\n+          onFinish.goto_js,\\n+          onFinish.goto,\\n+          forEachParent,\\n+          parentCheckConfig,\\n+          parentResult,\\n+          context,\\n+          state\\n+        );\\n+        if (context.debug || true)\\n+          logger.info(`[LevelDispatch] goto_js evaluation result: ${gotoTarget || 'null'}`);\\n+\\n+        if (gotoTarget) {\\n+          if (queuedForward && gotoTarget === forEachParent) {\\n+            logger.info(\\n+              `[LevelDispatch] on_finish.goto to self (${gotoTarget}) deferred, will process after WaveRetry`\\n+            );\\n+            emitEvent({ type: 'WaveRetry', reason: 'on_finish' });\\n+          } else {\\n+            if (checkLoopBudget(context, state, 'on_finish', 'goto')) {\\n+              const errorIssue: ReviewIssue = {\\n+                file: 'system',\\n+                line: 0,\\n+                ruleId: `${forEachParent}/routing/loop_budget_exceeded`,\\n+                message: `Routing loop budget exceeded (max_loops=${context.config.routing?.max_loops ?? 10}) during on_finish goto`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              };\\n+              parentResult.issues = [...(parentResult.issues || []), errorIssue];\\n+              try {\\n+                context.journal.commitEntry({\\n+                  sessionId: context.sessionId,\\n+                  checkId: forEachParent,\\n+                  result: parentResult as any,\\n+                  event: context.event || 'manual',\\n+                  scope: [],\\n+                });\\n+              } catch {}\\n+              return aggregatedResult;\\n+            }\\n+            state.routingLoopCount++;\\n+            emitEvent({ type: 'ForwardRunRequested', target: gotoTarget, origin: 'goto' });\\n+          }\\n+        }\\n+\\n+        // If we enqueued any on_finish.run targets, request a WaveRetry so the\\n+        // next wave re-evaluates guards/ifs across the full plan. Guard to\\n+        // avoid duplicate retries for the same parent within the same wave.\\n+        if (queuedForward) {\\n+          const guardKey = `waveRetry:on_finish:${forEachParent}:wave:${state.wave}`;\\n+          logger.info(\\n+            `[LevelDispatch] Checking WaveRetry guard: ${guardKey}, has=${!!(state as any).forwardRunGuards?.has(guardKey)}`\\n+          );\\n+          if (!(state as any).forwardRunGuards?.has(guardKey)) {\\n+            (state as any).forwardRunGuards?.add(guardKey);\\n+            logger.info(`[LevelDispatch] Emitting WaveRetry event for on_finish.run targets`);\\n+            emitEvent({ type: 'WaveRetry', reason: 'on_finish' });\\n+          }\\n+        }\\n+      }\\n+    } catch {}\\n+  }\\n+\\n+  return aggregatedResult;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/state-machine/dispatch/history-snapshot.ts\",\"additions\":1,\"deletions\":0,\"changes\":30,\"patch\":\"diff --git a/src/state-machine/dispatch/history-snapshot.ts b/src/state-machine/dispatch/history-snapshot.ts\\nnew file mode 100644\\nindex 00000000..4b501ef0\\n--- /dev/null\\n+++ b/src/state-machine/dispatch/history-snapshot.ts\\n@@ -0,0 +1,30 @@\\n+import type { EngineContext } from '../../types/engine';\\n+import { logger } from '../../logger';\\n+\\n+/**\\n+ * Build output history Map from journal for template rendering\\n+ * This matches the format expected by AI providers.\\n+ * Moved from LevelDispatch to reduce file size and improve reuse.\\n+ */\\n+export function buildOutputHistoryFromJournal(context: EngineContext): Map<string, unknown[]> {\\n+  const outputHistory = new Map<string, unknown[]>();\\n+\\n+  try {\\n+    const snapshot = context.journal.beginSnapshot();\\n+    const allEntries = context.journal.readVisible(context.sessionId, snapshot, undefined);\\n+\\n+    for (const entry of allEntries) {\\n+      const checkId = entry.checkId;\\n+      if (!outputHistory.has(checkId)) {\\n+        outputHistory.set(checkId, []);\\n+      }\\n+      if (entry.result.output !== undefined) {\\n+        outputHistory.get(checkId)!.push(entry.result.output);\\n+      }\\n+    }\\n+  } catch (error) {\\n+    logger.debug(`[LevelDispatch] Error building output history: ${error}`);\\n+  }\\n+\\n+  return outputHistory;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/state-machine/dispatch/renderer-schema.ts\",\"additions\":1,\"deletions\":0,\"changes\":35,\"patch\":\"diff --git a/src/state-machine/dispatch/renderer-schema.ts b/src/state-machine/dispatch/renderer-schema.ts\\nnew file mode 100644\\nindex 00000000..f98f29fc\\n--- /dev/null\\n+++ b/src/state-machine/dispatch/renderer-schema.ts\\n@@ -0,0 +1,35 @@\\n+import { logger } from '../../logger';\\n+\\n+/**\\n+ * Load a built-in JSON Schema for a named renderer (e.g., \\\"code-review\\\",\\n+ * \\\"issue-assistant\\\", \\\"overview\\\", \\\"plain\\\"). Returns undefined when not found.\\n+ *\\n+ * Mirrors template resolution in template-renderer.ts, but targets schema.json.\\n+ */\\n+export async function loadRendererSchema(name: string): Promise<any | undefined> {\\n+  try {\\n+    const fs = await import('fs/promises');\\n+    const path = await import('path');\\n+    const sanitized = String(name).replace(/[^a-zA-Z0-9-]/g, '');\\n+    if (!sanitized) return undefined;\\n+\\n+    const candidates = [\\n+      // When running from dist\\n+      path.join(__dirname, '..', '..', 'output', sanitized, 'schema.json'),\\n+      // When running from a checkout with output/ folder copied to CWD\\n+      path.join(process.cwd(), 'output', sanitized, 'schema.json'),\\n+    ];\\n+\\n+    for (const p of candidates) {\\n+      try {\\n+        const raw = await fs.readFile(p, 'utf-8');\\n+        return JSON.parse(raw);\\n+      } catch {}\\n+    }\\n+  } catch (e) {\\n+    try {\\n+      logger.warn(`[schema-loader] Failed to load renderer schema '${name}': ${String(e)}`);\\n+    } catch {}\\n+  }\\n+  return undefined;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/state-machine/dispatch/stats-manager.ts\",\"additions\":3,\"deletions\":0,\"changes\":95,\"patch\":\"diff --git a/src/state-machine/dispatch/stats-manager.ts b/src/state-machine/dispatch/stats-manager.ts\\nnew file mode 100644\\nindex 00000000..8d04436e\\n--- /dev/null\\n+++ b/src/state-machine/dispatch/stats-manager.ts\\n@@ -0,0 +1,95 @@\\n+import type { ReviewSummary } from '../../reviewer';\\n+import type { RunState } from '../../types/engine';\\n+import type { CheckExecutionStats } from '../../types/execution';\\n+\\n+export function hasFatalIssues(result: ReviewSummary): boolean {\\n+  if (!result.issues) return false;\\n+  return result.issues.some(issue => {\\n+    const ruleId = issue.ruleId || '';\\n+    return (\\n+      ruleId.endsWith('/error') ||\\n+      ruleId.includes('/execution_error') ||\\n+      ruleId.endsWith('_fail_if')\\n+    );\\n+  });\\n+}\\n+\\n+export function shouldFailFast(\\n+  results: Array<{ checkId: string; result: ReviewSummary; error?: Error }>\\n+): boolean {\\n+  for (const { result } of results) {\\n+    if (!result || !result.issues) continue;\\n+    if (hasFatalIssues(result)) return true;\\n+  }\\n+  return false;\\n+}\\n+\\n+export function updateStats(\\n+  results: Array<{ checkId: string; result: ReviewSummary; error?: Error; duration?: number }>,\\n+  state: RunState,\\n+  isForEachIteration: boolean = false\\n+): void {\\n+  for (const { checkId, result, error, duration } of results) {\\n+    const existing = state.stats.get(checkId);\\n+\\n+    const stats: CheckExecutionStats = existing || {\\n+      checkName: checkId,\\n+      totalRuns: 0,\\n+      successfulRuns: 0,\\n+      failedRuns: 0,\\n+      skippedRuns: 0,\\n+      skipped: false,\\n+      totalDuration: 0,\\n+      issuesFound: 0,\\n+      issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n+    };\\n+\\n+    // If this check was previously marked as skipped and now executed,\\n+    // clear the skipped flag and any skip counters to ensure the run is visible.\\n+    if (stats.skipped) {\\n+      stats.skipped = false;\\n+      stats.skippedRuns = 0;\\n+      (stats as any).skipReason = undefined;\\n+      (stats as any).skipCondition = undefined;\\n+    }\\n+\\n+    stats.totalRuns++;\\n+    if (typeof duration === 'number' && Number.isFinite(duration)) {\\n+      stats.totalDuration += duration;\\n+    }\\n+\\n+    const hasExecutionFailure = !!error || hasFatalIssues(result);\\n+    if (error) {\\n+      stats.failedRuns++;\\n+      // Note: do not set errorMessage; caller aggregates a single top-level issue.\\n+    } else if (hasExecutionFailure) {\\n+      stats.failedRuns++;\\n+      // Marking complete failure for non-forEach\\n+      // Per-iteration failures are handled in foreach path.\\n+      if (!isForEachIteration) {\\n+        (state as any).failedChecks = (state as any).failedChecks || new Set<string>();\\n+        (state as any).failedChecks.add(checkId);\\n+      }\\n+    } else {\\n+      stats.successfulRuns++;\\n+    }\\n+\\n+    if (result.issues) {\\n+      stats.issuesFound += result.issues.length;\\n+      for (const issue of result.issues) {\\n+        if (issue.severity === 'critical') stats.issuesBySeverity.critical++;\\n+        else if (issue.severity === 'error') stats.issuesBySeverity.error++;\\n+        else if (issue.severity === 'warning') stats.issuesBySeverity.warning++;\\n+        else if (issue.severity === 'info') stats.issuesBySeverity.info++;\\n+      }\\n+    }\\n+\\n+    if (stats.outputsProduced === undefined) {\\n+      const forEachItems = (result as any).forEachItems;\\n+      if (Array.isArray(forEachItems)) stats.outputsProduced = forEachItems.length;\\n+      else if ((result as any).output !== undefined) stats.outputsProduced = 1;\\n+    }\\n+\\n+    state.stats.set(checkId, stats);\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/state-machine/dispatch/template-renderer.ts\",\"additions\":2,\"deletions\":0,\"changes\":70,\"patch\":\"diff --git a/src/state-machine/dispatch/template-renderer.ts b/src/state-machine/dispatch/template-renderer.ts\\nnew file mode 100644\\nindex 00000000..0a9f3924\\n--- /dev/null\\n+++ b/src/state-machine/dispatch/template-renderer.ts\\n@@ -0,0 +1,70 @@\\n+import { logger } from '../../logger';\\n+import type { ReviewSummary } from '../../reviewer';\\n+\\n+/**\\n+ * Render template content for a check using our extended Liquid environment.\\n+ * Extracted from LevelDispatch for reuse and testability.\\n+ */\\n+export async function renderTemplateContent(\\n+  checkId: string,\\n+  checkConfig: any,\\n+  reviewSummary: ReviewSummary\\n+): Promise<string | undefined> {\\n+  try {\\n+    const { createExtendedLiquid } = await import('../../liquid-extensions');\\n+    const fs = await import('fs/promises');\\n+    const path = await import('path');\\n+\\n+    const schemaRaw = checkConfig.schema || 'plain';\\n+    const schema = typeof schemaRaw === 'string' ? schemaRaw : 'code-review';\\n+\\n+    let templateContent: string | undefined;\\n+\\n+    if (checkConfig.template && checkConfig.template.content) {\\n+      templateContent = String(checkConfig.template.content);\\n+    } else if (checkConfig.template && checkConfig.template.file) {\\n+      const file = String(checkConfig.template.file);\\n+      const resolved = path.resolve(process.cwd(), file);\\n+      templateContent = await fs.readFile(resolved, 'utf-8');\\n+    } else if (schema && schema !== 'plain') {\\n+      const sanitized = String(schema).replace(/[^a-zA-Z0-9-]/g, '');\\n+      if (sanitized) {\\n+        const candidatePaths = [\\n+          path.join(__dirname, '..', '..', 'output', sanitized, 'template.liquid'),\\n+          path.join(process.cwd(), 'output', sanitized, 'template.liquid'),\\n+        ];\\n+        for (const p of candidatePaths) {\\n+          try {\\n+            templateContent = await fs.readFile(p, 'utf-8');\\n+            if (templateContent) break;\\n+          } catch {\\n+            // try next\\n+          }\\n+        }\\n+      }\\n+    }\\n+\\n+    if (!templateContent) return undefined;\\n+\\n+    const liquid = createExtendedLiquid({\\n+      trimTagLeft: false,\\n+      trimTagRight: false,\\n+      trimOutputLeft: false,\\n+      trimOutputRight: false,\\n+      greedy: false,\\n+    });\\n+\\n+    const templateData: Record<string, unknown> = {\\n+      issues: reviewSummary.issues || [],\\n+      checkName: checkId,\\n+      output: (reviewSummary as any).output,\\n+    };\\n+\\n+    const rendered = await liquid.parseAndRender(templateContent, templateData);\\n+    return rendered.trim();\\n+  } catch (error) {\\n+    const msg = error instanceof Error ? error.message : String(error);\\n+    logger.error(`[LevelDispatch] Failed to render template for ${checkId}: ${msg}`);\\n+    return undefined;\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/state-machine/execution/summary.ts\",\"additions\":2,\"deletions\":0,\"changes\":45,\"patch\":\"diff --git a/src/state-machine/execution/summary.ts b/src/state-machine/execution/summary.ts\\nnew file mode 100644\\nindex 00000000..997f7018\\n--- /dev/null\\n+++ b/src/state-machine/execution/summary.ts\\n@@ -0,0 +1,45 @@\\n+import type { ReviewIssue, GroupedCheckResults, ReviewSummary } from '../../reviewer';\\n+import type { ExecutionStatistics } from '../../types/execution';\\n+\\n+/**\\n+ * Pure helper to convert grouped results + statistics into a flat ReviewSummary.\\n+ * Extracted to reduce StateMachineExecutionEngine size; behavior unchanged.\\n+ */\\n+export function convertToReviewSummary(\\n+  groupedResults: GroupedCheckResults,\\n+  statistics?: ExecutionStatistics\\n+): ReviewSummary {\\n+  const allIssues: ReviewIssue[] = [];\\n+\\n+  // Aggregate issues from all check results\\n+  for (const checkResults of Object.values(groupedResults)) {\\n+    for (const checkResult of checkResults) {\\n+      if (checkResult.issues && checkResult.issues.length > 0) {\\n+        allIssues.push(...checkResult.issues);\\n+      }\\n+    }\\n+  }\\n+\\n+  // Convert errors from execution statistics into issues\\n+  if (statistics) {\\n+    for (const checkStats of statistics.checks) {\\n+      if (checkStats.errorMessage) {\\n+        allIssues.push({\\n+          file: 'system',\\n+          line: 0,\\n+          endLine: undefined,\\n+          ruleId: 'system/error',\\n+          message: checkStats.errorMessage,\\n+          severity: 'error',\\n+          category: 'logic',\\n+          suggestion: undefined,\\n+          replacement: undefined,\\n+        });\\n+      }\\n+    }\\n+  }\\n+\\n+  return {\\n+    issues: allIssues,\\n+  };\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/state-machine/runner.ts\",\"additions\":15,\"deletions\":0,\"changes\":534,\"patch\":\"diff --git a/src/state-machine/runner.ts b/src/state-machine/runner.ts\\nnew file mode 100644\\nindex 00000000..5258c6b8\\n--- /dev/null\\n+++ b/src/state-machine/runner.ts\\n@@ -0,0 +1,534 @@\\n+/**\\n+ * State Machine Runner - Core orchestration\\n+ *\\n+ * Implements the main event loop that drives state transitions.\\n+ * Supports OTEL telemetry and debug visualizer event streaming (M4).\\n+ */\\n+\\n+import type {\\n+  EngineContext,\\n+  RunState,\\n+  EngineEvent,\\n+  EngineState,\\n+  SerializedError,\\n+} from '../types/engine';\\n+import { logger } from '../logger';\\n+import type { ExecutionResult } from '../types/execution';\\n+import { GroupedCheckResults } from '../reviewer';\\n+import { withActiveSpan, addEvent as addOtelEvent } from '../telemetry/trace-helpers';\\n+import type { DebugVisualizerServer } from '../debug-visualizer/ws-server';\\n+\\n+// Import state handlers\\n+import { handleInit } from './states/init';\\n+import { handlePlanReady } from './states/plan-ready';\\n+import { handleWavePlanning } from './states/wave-planning';\\n+import { handleLevelDispatch } from './states/level-dispatch';\\n+import { handleCheckRunning } from './states/check-running';\\n+import { handleCompleted } from './states/completed';\\n+import { handleError } from './states/error';\\n+\\n+/**\\n+ * Main state machine runner\\n+ */\\n+export class StateMachineRunner {\\n+  private context: EngineContext;\\n+  private state: RunState;\\n+  private debugServer?: DebugVisualizerServer;\\n+\\n+  constructor(context: EngineContext, debugServer?: DebugVisualizerServer) {\\n+    this.context = context;\\n+    this.state = this.initializeState();\\n+    this.debugServer = debugServer;\\n+  }\\n+\\n+  /**\\n+   * Initialize the run state\\n+   */\\n+  private initializeState(): RunState {\\n+    // Pull defaults from config where available\\n+    const DEFAULT_MAX_WORKFLOW_DEPTH = 3;\\n+    const configuredMaxDepth =\\n+      (this.context && this.context.config && this.context.config.limits\\n+        ? this.context.config.limits.max_workflow_depth\\n+        : undefined) ?? DEFAULT_MAX_WORKFLOW_DEPTH;\\n+    return {\\n+      currentState: 'Init',\\n+      wave: 0,\\n+      levelQueue: [],\\n+      eventQueue: [],\\n+      activeDispatches: new Map(),\\n+      completedChecks: new Set(),\\n+      flags: {\\n+        failFastTriggered: false,\\n+        forwardRunRequested: false,\\n+        // Maximum nesting depth for nested workflows (configurable)\\n+        maxWorkflowDepth: configuredMaxDepth,\\n+        currentWorkflowDepth: 0, // Start at root level\\n+      },\\n+      stats: new Map(),\\n+      historyLog: [],\\n+      forwardRunGuards: new Set(),\\n+      currentLevelChecks: new Set(),\\n+      routingLoopCount: 0,\\n+      pendingRunScopes: new Map(),\\n+    };\\n+  }\\n+\\n+  /**\\n+   * Execute the state machine\\n+   */\\n+  async run(): Promise<ExecutionResult> {\\n+    try {\\n+      // Emit initial state transition event\\n+      this.emitEvent({ type: 'StateTransition', from: 'Init', to: 'Init' });\\n+\\n+      // Main event loop\\n+      while (!this.isTerminalState(this.state.currentState)) {\\n+        const currentState = this.state.currentState;\\n+\\n+        if (this.context.debug) {\\n+          logger.info(`[StateMachine] State: ${currentState}, Wave: ${this.state.wave}`);\\n+        }\\n+\\n+        // Execute current state handler\\n+        await this.executeState(currentState);\\n+\\n+        // Check for errors\\n+        if (this.state.currentState === 'Error') {\\n+          break;\\n+        }\\n+      }\\n+\\n+      // Return final results\\n+      return this.buildExecutionResult();\\n+    } catch (error) {\\n+      const errorMsg = error instanceof Error ? error.message : String(error);\\n+      logger.error(`[StateMachine] Fatal error: ${errorMsg}`);\\n+      const serializedError: SerializedError = {\\n+        message: error instanceof Error ? error.message : String(error),\\n+        stack: error instanceof Error ? error.stack : undefined,\\n+        name: error instanceof Error ? error.name : undefined,\\n+      };\\n+      this.emitEvent({ type: 'Shutdown', error: serializedError });\\n+      throw error;\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Execute a specific state handler\\n+   * M4: Wraps each state execution in an OTEL span for observability\\n+   */\\n+  private async executeState(state: EngineState): Promise<void> {\\n+    // M4: Wrap state execution in OTEL span\\n+    return withActiveSpan(\\n+      `engine.state.${state.toLowerCase()}`,\\n+      {\\n+        state: state,\\n+        engine_mode: this.context.mode,\\n+        wave: this.state.wave,\\n+        session_id: this.context.sessionId,\\n+      },\\n+      async () => {\\n+        try {\\n+          switch (state) {\\n+            case 'Init':\\n+              await handleInit(this.context, this.state, this.transition.bind(this));\\n+              break;\\n+            case 'PlanReady':\\n+              await handlePlanReady(this.context, this.state, this.transition.bind(this));\\n+              break;\\n+            case 'WavePlanning':\\n+              await handleWavePlanning(this.context, this.state, this.transition.bind(this));\\n+              break;\\n+            case 'LevelDispatch':\\n+              await handleLevelDispatch(\\n+                this.context,\\n+                this.state,\\n+                this.transition.bind(this),\\n+                this.emitEvent.bind(this)\\n+              );\\n+              break;\\n+            case 'CheckRunning':\\n+              await handleCheckRunning(\\n+                this.context,\\n+                this.state,\\n+                this.transition.bind(this),\\n+                this.emitEvent.bind(this)\\n+              );\\n+              break;\\n+            case 'Routing':\\n+              // Routing is handled inline by CheckRunning for now\\n+              throw new Error('Routing state should be handled by CheckRunning');\\n+            case 'Completed':\\n+              await handleCompleted(this.context, this.state);\\n+              break;\\n+            case 'Error':\\n+              await handleError(this.context, this.state);\\n+              break;\\n+            default:\\n+              throw new Error(`Unknown state: ${state}`);\\n+          }\\n+        } catch (error) {\\n+          const errorMsg = error instanceof Error ? error.message : String(error);\\n+          logger.error(`[StateMachine] Error in state ${state}: ${errorMsg}`);\\n+          const serializedError: SerializedError = {\\n+            message: error instanceof Error ? error.message : String(error),\\n+            stack: error instanceof Error ? error.stack : undefined,\\n+            name: error instanceof Error ? error.name : undefined,\\n+          };\\n+          this.emitEvent({ type: 'Shutdown', error: serializedError });\\n+          this.state.currentState = 'Error';\\n+          throw error; // Re-throw to trigger span error recording\\n+        }\\n+      }\\n+    );\\n+  }\\n+\\n+  /**\\n+   * Transition to a new state\\n+   * M4: Emits OTEL span for the transition with state metadata\\n+   */\\n+  private transition(newState: EngineState): void {\\n+    const oldState = this.state.currentState;\\n+    this.state.currentState = newState;\\n+\\n+    // Emit state transition event\\n+    const transitionEvent = { type: 'StateTransition' as const, from: oldState, to: newState };\\n+    this.emitEvent(transitionEvent);\\n+\\n+    // M4: Emit OTEL event for state transition\\n+    try {\\n+      addOtelEvent('engine.state_transition', {\\n+        state_from: oldState,\\n+        state_to: newState,\\n+        engine_mode: this.context.mode,\\n+        wave: this.state.wave,\\n+        session_id: this.context.sessionId,\\n+      });\\n+    } catch (_err) {\\n+      // Ignore telemetry errors\\n+    }\\n+\\n+    if (this.context.debug) {\\n+      logger.info(`[StateMachine] Transition: ${oldState} -> ${newState}`);\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Emit an engine event\\n+   * M4: Streams events to debug visualizer for time-travel debugging\\n+   */\\n+  private emitEvent(event: EngineEvent): void {\\n+    this.state.historyLog.push(event);\\n+\\n+    // Queue events that require processing by WavePlanning\\n+    if (event.type === 'ForwardRunRequested' || event.type === 'WaveRetry') {\\n+      this.state.eventQueue.push(event);\\n+    }\\n+\\n+    // M4: Stream event to debug visualizer for live monitoring\\n+    if (this.debugServer) {\\n+      try {\\n+        this.streamEventToDebugServer(event);\\n+      } catch (_err) {\\n+        // Ignore debug server errors\\n+      }\\n+    }\\n+\\n+    if (this.context.debug && event.type !== 'StateTransition') {\\n+      logger.debug(`[StateMachine] Event: ${event.type}`);\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Stream an engine event to debug visualizer (M4)\\n+   * Converts EngineEvent to ProcessedSpan format for visualization\\n+   */\\n+  private streamEventToDebugServer(event: EngineEvent): void {\\n+    if (!this.debugServer) return;\\n+\\n+    // Convert EngineEvent to a ProcessedSpan-like structure\\n+    const timestamp = process.hrtime();\\n+    const span = {\\n+      traceId: this.context.sessionId,\\n+      spanId: `${event.type}-${Date.now()}`,\\n+      name: `engine.event.${event.type.toLowerCase()}`,\\n+      startTime: timestamp,\\n+      endTime: timestamp,\\n+      duration: 0,\\n+      attributes: {\\n+        event_type: event.type,\\n+        engine_mode: this.context.mode,\\n+        wave: this.state.wave,\\n+        session_id: this.context.sessionId,\\n+        ...this.extractEventAttributes(event),\\n+      },\\n+      events: [],\\n+      status: 'ok' as const,\\n+    };\\n+\\n+    this.debugServer.emitSpan(span);\\n+  }\\n+\\n+  /**\\n+   * Extract type-specific attributes from engine events\\n+   */\\n+  private extractEventAttributes(event: EngineEvent): Record<string, any> {\\n+    switch (event.type) {\\n+      case 'StateTransition':\\n+        return { state_from: event.from, state_to: event.to };\\n+      case 'CheckScheduled':\\n+      case 'CheckCompleted':\\n+      case 'CheckErrored':\\n+        return {\\n+          check_id: event.checkId,\\n+          scope: event.scope?.join('.') || '',\\n+        };\\n+      case 'ForwardRunRequested':\\n+        return {\\n+          target: event.target,\\n+          goto_event: event.gotoEvent,\\n+          scope: event.scope?.join('.') || '',\\n+        };\\n+      case 'WaveRetry':\\n+        return { reason: event.reason };\\n+      case 'Shutdown':\\n+        return {\\n+          error: event.error?.message,\\n+        };\\n+      default:\\n+        return {};\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Check if a state is terminal\\n+   */\\n+  private isTerminalState(state: EngineState): boolean {\\n+    return state === 'Completed' || state === 'Error';\\n+  }\\n+\\n+  /**\\n+   * Build the final execution result\\n+   */\\n+  private buildExecutionResult(): ExecutionResult {\\n+    const stats = Array.from(this.state.stats.values());\\n+    // Surface execution errors first for clearer reporting and to satisfy\\n+    // tests that read the first entry for error messages.\\n+    stats.sort((a, b) => (b.errorMessage ? 1 : 0) - (a.errorMessage ? 1 : 0));\\n+\\n+    // Build results from completed checks by reading from journal\\n+    const results: GroupedCheckResults = this.aggregateResultsFromJournal();\\n+\\n+    // Aggregate total duration\\n+    let totalDuration = 0;\\n+    for (const stat of stats) {\\n+      totalDuration = Math.max(totalDuration, stat.totalDuration);\\n+    }\\n+\\n+    // Normalize statistics: ensure successfulRuns + failedRuns == totalRuns\\n+    // The updateStats function in level-dispatch.ts already counts all executions correctly,\\n+    // including forEach iterations, retries, and goto re-runs. We just need to ensure consistency.\\n+    try {\\n+      for (const s of stats) {\\n+        // Final clamp: ensure successfulRuns + failedRuns == totalRuns\\n+        // Prefer to preserve failures and adjust successes accordingly.\\n+        const sumSF = (s.successfulRuns || 0) + (s.failedRuns || 0);\\n+        if (s.totalRuns !== undefined && sumSF !== s.totalRuns) {\\n+          if (sumSF > s.totalRuns) {\\n+            // Too many counted: clamp successes to fill remaining after failures\\n+            const failures = Math.min(s.failedRuns || 0, s.totalRuns);\\n+            s.failedRuns = failures;\\n+            s.successfulRuns = Math.max(0, s.totalRuns - failures);\\n+          } else {\\n+            // Not enough counted: assume remaining were successful\\n+            s.successfulRuns = (s.successfulRuns || 0) + (s.totalRuns - sumSF);\\n+          }\\n+        }\\n+      }\\n+    } catch {}\\n+\\n+    // Debug: log stats breakdown for debugging\\n+    if (this.context.debug) {\\n+      logger.info('[StateMachine][Stats] Final statistics breakdown:');\\n+      for (const s of stats) {\\n+        logger.info(\\n+          `  ${s.checkName}: totalRuns=${s.totalRuns}, successful=${s.successfulRuns}, failed=${s.failedRuns}`\\n+        );\\n+      }\\n+      logger.info(\\n+        `[StateMachine][Stats] Total: ${this.state.stats.size} configured, ${stats.reduce((sum, s) => sum + s.totalRuns, 0)} executions`\\n+      );\\n+    }\\n+\\n+    return {\\n+      results,\\n+      statistics: {\\n+        totalChecksConfigured: this.state.stats.size,\\n+        totalExecutions: stats.reduce((sum, s) => sum + s.totalRuns, 0),\\n+        successfulExecutions: stats.reduce((sum, s) => sum + s.successfulRuns, 0),\\n+        failedExecutions: stats.reduce((sum, s) => sum + s.failedRuns, 0),\\n+        skippedChecks: stats.filter(s => s.skipped).length,\\n+        totalDuration,\\n+        checks: stats,\\n+      },\\n+    };\\n+  }\\n+\\n+  /**\\n+   * Aggregate results from journal into GroupedCheckResults format\\n+   * This matches the format returned by the legacy engine\\n+   */\\n+  private aggregateResultsFromJournal(): GroupedCheckResults {\\n+    const groupedResults: GroupedCheckResults = {};\\n+\\n+    // Read all journal entries for this session\\n+    const allEntries = this.context.journal.readVisible(\\n+      this.context.sessionId,\\n+      this.context.journal.beginSnapshot(),\\n+      undefined\\n+    );\\n+\\n+    // Group entries by check ID to handle forEach iterations\\n+    const checkEntries = new Map<string, typeof allEntries>();\\n+    for (const entry of allEntries) {\\n+      const existing = checkEntries.get(entry.checkId) || [];\\n+      existing.push(entry);\\n+      checkEntries.set(entry.checkId, existing);\\n+    }\\n+\\n+    // Process each check\\n+    for (const [checkId, entries] of checkEntries) {\\n+      const checkConfig = this.context.config.checks?.[checkId];\\n+\\n+      // Special handling for system errors (validation, cycles, etc.)\\n+      if (!checkConfig && checkId === 'system') {\\n+        const latestEntry = entries[entries.length - 1];\\n+        if (latestEntry && latestEntry.result.issues) {\\n+          // Add system issues to a 'system' group\\n+          if (!groupedResults['system']) {\\n+            groupedResults['system'] = [];\\n+          }\\n+          groupedResults['system'].push({\\n+            checkName: 'system',\\n+            content: '',\\n+            group: 'system',\\n+            output: undefined,\\n+            debug: undefined,\\n+            issues: latestEntry.result.issues,\\n+          });\\n+        }\\n+        continue;\\n+      }\\n+\\n+      if (!checkConfig) continue;\\n+\\n+      // Determine group: use explicit group or fall back to check name\\n+      const group = checkConfig.group || checkId;\\n+\\n+      // For forEach checks, we aggregate all iterations\\n+      // For non-forEach, we take the latest entry\\n+      let content = '';\\n+      let output: unknown = undefined;\\n+      const allIssues: any[] = [];\\n+      let debug: any = undefined;\\n+\\n+      if (checkConfig.forEach && entries.length > 1) {\\n+        // Aggregate all forEach iterations\\n+        const contents: string[] = [];\\n+        for (const entry of entries) {\\n+          if (entry.result.content) {\\n+            contents.push(entry.result.content);\\n+          }\\n+          if (entry.result.issues) {\\n+            allIssues.push(...entry.result.issues);\\n+          }\\n+          if (entry.result.debug) {\\n+            debug = entry.result.debug;\\n+          }\\n+          // For forEach, output is typically an array from the last iteration\\n+          if (entry.result.output !== undefined) {\\n+            output = entry.result.output;\\n+          }\\n+        }\\n+        content = contents.join('\\\\n');\\n+      } else {\\n+        // For non-forEach or single-entry forEach, take the latest entry\\n+        const latestEntry = entries[entries.length - 1];\\n+        if (latestEntry) {\\n+          content = latestEntry.result.content || '';\\n+          output = latestEntry.result.output;\\n+          if (latestEntry.result.issues) {\\n+            allIssues.push(...latestEntry.result.issues);\\n+          }\\n+          debug = latestEntry.result.debug;\\n+        }\\n+      }\\n+\\n+      // Create CheckResult\\n+      const checkResult: import('../reviewer').CheckResult = {\\n+        checkName: checkId,\\n+        content,\\n+        group,\\n+        output,\\n+        debug,\\n+        issues: allIssues,\\n+      };\\n+\\n+      // Add to appropriate group\\n+      if (!groupedResults[group]) {\\n+        groupedResults[group] = [];\\n+      }\\n+      groupedResults[group].push(checkResult);\\n+    }\\n+\\n+    // Apply issue suppression if enabled\\n+    const suppressionEnabled = this.context.config.output?.suppressionEnabled ?? true;\\n+    if (suppressionEnabled) {\\n+      const { IssueFilter } = require('../issue-filter');\\n+      const filter = new IssueFilter(true);\\n+\\n+      // Filter issues in each check result\\n+      for (const group of Object.keys(groupedResults)) {\\n+        for (const checkResult of groupedResults[group]) {\\n+          if (checkResult.issues && checkResult.issues.length > 0) {\\n+            checkResult.issues = filter.filterIssues(\\n+              checkResult.issues,\\n+              this.context.workingDirectory\\n+            );\\n+          }\\n+        }\\n+      }\\n+    }\\n+\\n+    return groupedResults;\\n+  }\\n+\\n+  /**\\n+   * Get current run state (for debugging/testing)\\n+   */\\n+  getState(): RunState {\\n+    return this.state;\\n+  }\\n+\\n+  /**\\n+   * Bubble an event to parent context (nested workflows support)\\n+   * This allows nested workflows to trigger re-runs in parent workflows\\n+   */\\n+  bubbleEventToParent(event: EngineEvent): void {\\n+    if (this.state.parentContext && this.state.parentContext.mode === 'state-machine') {\\n+      // Emit event to parent's event queue\\n+      if (this.context.debug) {\\n+        logger.info(`[StateMachine] Bubbling event to parent: ${event.type}`);\\n+      }\\n+\\n+      // We need to access the parent runner to bubble events\\n+      // For now, we'll store events in a queue that can be retrieved\\n+      // The parent will poll for bubbled events after child execution\\n+      if (!this.state.parentContext._bubbledEvents) {\\n+        (this.state.parentContext as any)._bubbledEvents = [];\\n+      }\\n+      (this.state.parentContext as any)._bubbledEvents.push(event);\\n+    }\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/state-machine/states/check-running.ts\",\"additions\":1,\"deletions\":0,\"changes\":24,\"patch\":\"diff --git a/src/state-machine/states/check-running.ts b/src/state-machine/states/check-running.ts\\nnew file mode 100644\\nindex 00000000..6ee42dac\\n--- /dev/null\\n+++ b/src/state-machine/states/check-running.ts\\n@@ -0,0 +1,24 @@\\n+/**\\n+ * CheckRunning State Handler\\n+ *\\n+ * Responsibilities:\\n+ * - Execute provider logic for a check\\n+ * - Assemble CheckResult\\n+ * - Transition to Routing state\\n+ *\\n+ * M1: This state is handled inline by LevelDispatch\\n+ * M2: Will be a proper state when we add routing\\n+ */\\n+\\n+import type { EngineContext, RunState, EngineState, EngineEvent } from '../../types/engine';\\n+\\n+export async function handleCheckRunning(\\n+  _context: EngineContext,\\n+  _state: RunState,\\n+  transition: (newState: EngineState) => void,\\n+  _emitEvent: (event: EngineEvent) => void\\n+): Promise<void> {\\n+  // M2: This state is not used yet - LevelDispatch handles execution inline with routing\\n+  // The routing happens directly in LevelDispatch after check execution\\n+  transition('WavePlanning');\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/state-machine/states/completed.ts\",\"additions\":1,\"deletions\":0,\"changes\":34,\"patch\":\"diff --git a/src/state-machine/states/completed.ts b/src/state-machine/states/completed.ts\\nnew file mode 100644\\nindex 00000000..6a22137f\\n--- /dev/null\\n+++ b/src/state-machine/states/completed.ts\\n@@ -0,0 +1,34 @@\\n+/**\\n+ * Completed State Handler\\n+ *\\n+ * Responsibilities:\\n+ * - Finalize stats\\n+ * - Flush telemetry\\n+ * - Update GitHub CheckRuns\\n+ * - Terminal state\\n+ */\\n+\\n+import type { EngineContext, RunState } from '../../types/engine';\\n+import { logger } from '../../logger';\\n+\\n+export async function handleCompleted(context: EngineContext, state: RunState): Promise<void> {\\n+  if (context.debug) {\\n+    logger.info('[Completed] Execution complete');\\n+    logger.info(`[Completed] Total waves: ${state.wave + 1}`);\\n+    logger.info(`[Completed] Checks completed: ${state.completedChecks.size}`);\\n+    logger.info(`[Completed] Stats collected: ${state.stats.size}`);\\n+  }\\n+\\n+  // Finalize GitHub checks if configured\\n+  if (context.gitHubChecks) {\\n+    // GitHub check finalization happens in the main engine\\n+    if (context.debug) {\\n+      logger.info('[Completed] GitHub checks will be finalized by main engine');\\n+    }\\n+  }\\n+\\n+  // Flush telemetry\\n+  // M4: Will add structured event streaming to debug visualizer\\n+\\n+  // Terminal state - no transition\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/state-machine/states/error.ts\",\"additions\":1,\"deletions\":0,\"changes\":32,\"patch\":\"diff --git a/src/state-machine/states/error.ts b/src/state-machine/states/error.ts\\nnew file mode 100644\\nindex 00000000..11c1d03e\\n--- /dev/null\\n+++ b/src/state-machine/states/error.ts\\n@@ -0,0 +1,32 @@\\n+/**\\n+ * Error State Handler\\n+ *\\n+ * Responsibilities:\\n+ * - Capture fatal errors\\n+ * - Unwind gracefully\\n+ * - Terminal state\\n+ */\\n+\\n+import type { EngineContext, RunState } from '../../types/engine';\\n+import { logger } from '../../logger';\\n+\\n+export async function handleError(context: EngineContext, state: RunState): Promise<void> {\\n+  logger.error('[Error] State machine entered error state');\\n+\\n+  // Check if there's an error in the event queue\\n+  const errorEvent = state.eventQueue.find(e => e.type === 'Shutdown' && e.error);\\n+  if (errorEvent && errorEvent.type === 'Shutdown' && errorEvent.error) {\\n+    logger.error(`[Error] Fatal error: ${errorEvent.error.message}`);\\n+    if (errorEvent.error.stack) {\\n+      logger.error(`[Error] Stack: ${errorEvent.error.stack}`);\\n+    }\\n+  }\\n+\\n+  // Log stats for debugging\\n+  if (context.debug) {\\n+    logger.info(`[Error] Completed ${state.completedChecks.size} checks before error`);\\n+    logger.info(`[Error] Active dispatches: ${state.activeDispatches.size}`);\\n+  }\\n+\\n+  // Terminal state - no transition\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/state-machine/states/init.ts\",\"additions\":2,\"deletions\":0,\"changes\":48,\"patch\":\"diff --git a/src/state-machine/states/init.ts b/src/state-machine/states/init.ts\\nnew file mode 100644\\nindex 00000000..b965feb3\\n--- /dev/null\\n+++ b/src/state-machine/states/init.ts\\n@@ -0,0 +1,48 @@\\n+/**\\n+ * Init State Handler\\n+ *\\n+ * Responsibilities:\\n+ * - Validate configuration\\n+ * - Initialize services (journal, memory, GitHub checks)\\n+ * - Transition to PlanReady\\n+ */\\n+\\n+import type { EngineContext, RunState, EngineState } from '../../types/engine';\\n+import { logger } from '../../logger';\\n+\\n+export async function handleInit(\\n+  context: EngineContext,\\n+  state: RunState,\\n+  transition: (newState: EngineState) => void\\n+): Promise<void> {\\n+  if (context.debug) {\\n+    logger.info('[Init] Initializing state machine...');\\n+  }\\n+\\n+  // Validate configuration\\n+  if (!context.config) {\\n+    throw new Error('Configuration is required');\\n+  }\\n+\\n+  // Initialize memory store if needed\\n+  if (context.memory) {\\n+    await context.memory.initialize();\\n+  }\\n+\\n+  // Initialize GitHub checks if configured\\n+  if (context.gitHubChecks) {\\n+    // GitHub check initialization happens in the main engine\\n+    // We just validate it's available here\\n+    if (context.debug) {\\n+      logger.info('[Init] GitHub checks service available');\\n+    }\\n+  }\\n+\\n+  // Reset journal for this session\\n+  if (context.debug) {\\n+    logger.info(`[Init] Session ID: ${context.sessionId}`);\\n+  }\\n+\\n+  // Transition to PlanReady\\n+  transition('PlanReady');\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/state-machine/states/level-dispatch.ts\",\"additions\":77,\"deletions\":0,\"changes\":2762,\"patch\":\"diff --git a/src/state-machine/states/level-dispatch.ts b/src/state-machine/states/level-dispatch.ts\\nnew file mode 100644\\nindex 00000000..8096453c\\n--- /dev/null\\n+++ b/src/state-machine/states/level-dispatch.ts\\n@@ -0,0 +1,2762 @@\\n+/**\\n+ * LevelDispatch State Handler\\n+ *\\n+ * Responsibilities:\\n+ * - Pop next topological level from queue\\n+ * - Spawn tasks up to maxParallelism\\n+ * - Handle session reuse barriers\\n+ * - Support fail-fast\\n+ * - Support debug pause\\n+ * - Execute actual provider logic\\n+ * - Transition back to WavePlanning or handle errors\\n+ *\\n+ * M2: Integrates actual provider execution and routing\\n+ */\\n+\\n+import type {\\n+  EngineContext,\\n+  RunState,\\n+  EngineState,\\n+  EngineEvent,\\n+  DispatchRecord,\\n+} from '../../types/engine';\\n+import { logger } from '../../logger';\\n+import type { ReviewSummary, ReviewIssue } from '../../reviewer';\\n+import type { CheckExecutionStats } from '../../types/execution';\\n+import type { CheckProviderConfig } from '../../providers/check-provider.interface';\\n+import type { CheckConfig } from '../../types/config';\\n+import { handleRouting, checkLoopBudget } from './routing';\\n+import { withActiveSpan } from '../../telemetry/trace-helpers';\\n+import { emitMermaidFromMarkdown } from '../../utils/mermaid-telemetry';\\n+import { emitNdjsonSpanWithEvents, emitNdjsonFallback } from '../../telemetry/fallback-ndjson';\\n+import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n+\\n+/**\\n+ * Map check name to focus for AI provider\\n+ * This is a fallback when focus is not explicitly configured\\n+ */\\n+function mapCheckNameToFocus(checkName: string): string {\\n+  const focusMap: Record<string, string> = {\\n+    security: 'security',\\n+    performance: 'performance',\\n+    style: 'style',\\n+    architecture: 'architecture',\\n+  };\\n+\\n+  return focusMap[checkName] || 'all';\\n+}\\n+\\n+/**\\n+ * Build output history Map from journal for template rendering\\n+ * This matches the format expected by AI providers\\n+ */\\n+function buildOutputHistoryFromJournal(context: EngineContext): Map<string, unknown[]> {\\n+  const outputHistory = new Map<string, unknown[]>();\\n+\\n+  try {\\n+    const snapshot = context.journal.beginSnapshot();\\n+    const allEntries = context.journal.readVisible(context.sessionId, snapshot, undefined);\\n+\\n+    // Group by checkId and extract outputs\\n+    for (const entry of allEntries) {\\n+      const checkId = entry.checkId;\\n+      if (!outputHistory.has(checkId)) {\\n+        outputHistory.set(checkId, []);\\n+      }\\n+      // Push the output if it exists\\n+      if (entry.result.output !== undefined) {\\n+        outputHistory.get(checkId)!.push(entry.result.output);\\n+      }\\n+    }\\n+  } catch (error) {\\n+    // Silently fail - return empty map\\n+    logger.debug(`[LevelDispatch] Error building output history: ${error}`);\\n+  }\\n+\\n+  return outputHistory;\\n+}\\n+\\n+/**\\n+ * Evaluate 'if' condition for a check\\n+ *\\n+ * Note: For routing loops to work correctly, 'outputs' should only include\\n+ * results from the CURRENT wave, not from previous waves. This allows\\n+ * checks to re-execute after routing (goto/on_fail/on_success) triggers.\\n+ */\\n+async function evaluateIfCondition(\\n+  checkId: string,\\n+  checkConfig: CheckConfig,\\n+  context: EngineContext,\\n+  state: RunState\\n+): Promise<boolean> {\\n+  const ifExpression = checkConfig.if;\\n+  if (!ifExpression) {\\n+    return true; // No condition means always run\\n+  }\\n+\\n+  try {\\n+    const evaluator = new FailureConditionEvaluator();\\n+\\n+    // Build previous results for condition evaluation\\n+    // Default: include only results from the CURRENT wave. This prevents\\n+    // stale data from causing routing loops in normal execution.\\n+    const previousResults = new Map<string, ReviewSummary>();\\n+\\n+    const currentWaveCompletions = (state as any).currentWaveCompletions as Set<string> | undefined;\\n+    const useGlobalOutputs = !!((state as any).flags && (state as any).flags.forwardRunActive);\\n+\\n+    if (useGlobalOutputs) {\\n+      // Forward-run wave: allow guards to consult latest outputs from the entire\\n+      // journal so follow-up steps (e.g., post-verified after run-review) can\\n+      // see the outputs produced in the prior wave that scheduled this forward-run.\\n+      try {\\n+        const snapshotId = context.journal.beginSnapshot();\\n+        const ContextView = require('../../snapshot-store').ContextView;\\n+        const contextView = new ContextView(\\n+          context.journal,\\n+          context.sessionId,\\n+          snapshotId,\\n+          [],\\n+          context.event\\n+        );\\n+        for (const key of Object.keys(context.checks || {})) {\\n+          const jr = contextView.get(key);\\n+          if (jr) previousResults.set(key, jr as ReviewSummary);\\n+        }\\n+      } catch {\\n+        // Fallback to current-wave only if any error occurs\\n+      }\\n+    } else if (currentWaveCompletions) {\\n+      // Current-wave-only results\\n+      for (const key of currentWaveCompletions) {\\n+        try {\\n+          const snapshotId = context.journal.beginSnapshot();\\n+          const ContextView = require('../../snapshot-store').ContextView;\\n+          const contextView = new ContextView(\\n+            context.journal,\\n+            context.sessionId,\\n+            snapshotId,\\n+            [],\\n+            context.event\\n+          );\\n+          const journalResult = contextView.get(key);\\n+          if (journalResult) {\\n+            previousResults.set(key, journalResult as ReviewSummary);\\n+          }\\n+        } catch {\\n+          // Silently skip - will use empty result\\n+        }\\n+      }\\n+    }\\n+    // Fallback: if no wave tracking, use empty outputs (allows all checks to run)\\n+\\n+    // Build context data for if condition evaluation\\n+    // Create a snapshot of process.env to ensure current values are captured\\n+    // This is critical for test stages that set environment variables dynamically\\n+    const envSnapshot: Record<string, string> = {};\\n+    for (const [key, value] of Object.entries(process.env)) {\\n+      if (value !== undefined) {\\n+        envSnapshot[key] = value;\\n+      }\\n+    }\\n+\\n+    // Merge config.env into environment (config.env takes precedence)\\n+    if (context.config.env) {\\n+      for (const [key, value] of Object.entries(context.config.env)) {\\n+        if (value !== undefined && value !== null) {\\n+          envSnapshot[key] = String(value);\\n+        }\\n+      }\\n+    }\\n+\\n+    const contextData = {\\n+      previousResults,\\n+      event: context.event || 'manual',\\n+      branch: (context.prInfo as any)?.branch,\\n+      baseBranch: (context.prInfo as any)?.baseBranch,\\n+      filesChanged: context.prInfo?.files?.map(f => f.filename),\\n+      environment: envSnapshot,\\n+    };\\n+\\n+    const shouldRun = await evaluator.evaluateIfCondition(checkId, ifExpression, contextData);\\n+    return shouldRun;\\n+  } catch (error) {\\n+    const msg = error instanceof Error ? error.message : String(error);\\n+    logger.error(`Failed to evaluate if expression for check '${checkId}': ${msg}`);\\n+    // Fail-secure: if condition evaluation fails, skip execution\\n+    return false;\\n+  }\\n+}\\n+\\n+export async function handleLevelDispatch(\\n+  context: EngineContext,\\n+  state: RunState,\\n+  transition: (newState: EngineState) => void,\\n+  emitEvent: (event: EngineEvent) => void\\n+): Promise<void> {\\n+  // Pop next level from queue\\n+  const level = state.levelQueue.shift();\\n+\\n+  if (!level) {\\n+    // No more levels - go back to wave planning\\n+    if (context.debug) {\\n+      logger.info('[LevelDispatch] No more levels in queue');\\n+    }\\n+    transition('WavePlanning');\\n+    return;\\n+  }\\n+\\n+  if (context.debug) {\\n+    logger.info(\\n+      `[LevelDispatch] Executing level ${level.level} with ${level.parallel.length} checks`\\n+    );\\n+  }\\n+\\n+  // Update current level tracking\\n+  state.currentLevel = level.level;\\n+  state.currentLevelChecks = new Set(level.parallel);\\n+\\n+  // Emit level ready event\\n+  emitEvent({ type: 'LevelReady', level, wave: state.wave });\\n+\\n+  const maxParallelism = context.maxParallelism || 10;\\n+  const results: Array<{ checkId: string; result: ReviewSummary; error?: Error }> = [];\\n+\\n+  // Group checks by session provider to enforce session reuse barriers\\n+  const sessionGroups = groupBySession(level.parallel, context);\\n+\\n+  // Execute each session group sequentially, but checks within group in parallel\\n+  for (const group of sessionGroups) {\\n+    const groupResults = await executeCheckGroup(\\n+      group,\\n+      context,\\n+      state,\\n+      maxParallelism,\\n+      emitEvent,\\n+      transition\\n+    );\\n+    results.push(...groupResults);\\n+\\n+    // Check fail-fast\\n+    if (context.failFast && shouldFailFast(results)) {\\n+      logger.warn('[LevelDispatch] Fail-fast triggered');\\n+      state.flags.failFastTriggered = true;\\n+      break;\\n+    }\\n+  }\\n+\\n+  // Emit level depleted event\\n+  emitEvent({ type: 'LevelDepleted', level: level.level, wave: state.wave });\\n+\\n+  // Update stats - exclude only aggregated forEach results and explicit skip stubs\\n+  // Previously skipped checks that now execute must be included so updateStats can\\n+  // clear the skipped flag and count this run.\\n+  const nonForEachResults = results.filter(r => {\\n+    if ((r.result as any).isForEach) return false;\\n+    if ((r.result as any).__skipped) return false;\\n+    return true;\\n+  });\\n+  updateStats(nonForEachResults, state);\\n+\\n+  // Check if fail-fast was triggered\\n+  if (state.flags.failFastTriggered) {\\n+    // Skip remaining levels\\n+    state.levelQueue = [];\\n+    if (context.debug) {\\n+      logger.info('[LevelDispatch] Fail-fast triggered, clearing level queue');\\n+    }\\n+  }\\n+\\n+  // Clear current level tracking\\n+  state.currentLevelChecks.clear();\\n+\\n+  // Transition back to WavePlanning\\n+  transition('WavePlanning');\\n+}\\n+\\n+/**\\n+ * Group checks by session provider to enforce sequential execution\\n+ */\\n+function groupBySession(checks: string[], context: EngineContext): string[][] {\\n+  // M2: Group checks that share a session provider\\n+  const sessionProviderMap = new Map<string, string[]>();\\n+  const noSessionChecks: string[] = [];\\n+\\n+  for (const checkId of checks) {\\n+    const metadata = context.checks[checkId];\\n+    const sessionProvider = metadata?.sessionProvider;\\n+\\n+    if (sessionProvider) {\\n+      const group = sessionProviderMap.get(sessionProvider) || [];\\n+      group.push(checkId);\\n+      sessionProviderMap.set(sessionProvider, group);\\n+    } else {\\n+      noSessionChecks.push(checkId);\\n+    }\\n+  }\\n+\\n+  // Return groups: first all session groups (sequential), then independent checks\\n+  const groups: string[][] = [];\\n+\\n+  // Add session groups (each group runs sequentially relative to other session groups)\\n+  for (const group of sessionProviderMap.values()) {\\n+    groups.push(group);\\n+  }\\n+\\n+  // Add independent checks as one group (can run in parallel)\\n+  if (noSessionChecks.length > 0) {\\n+    groups.push(noSessionChecks);\\n+  }\\n+\\n+  return groups;\\n+}\\n+\\n+/**\\n+ * Execute a group of checks in parallel (up to maxParallelism)\\n+ */\\n+async function executeCheckGroup(\\n+  checks: string[],\\n+  context: EngineContext,\\n+  state: RunState,\\n+  maxParallelism: number,\\n+  emitEvent: (event: EngineEvent) => void,\\n+  transition: (newState: EngineState) => void\\n+): Promise<Array<{ checkId: string; result: ReviewSummary; error?: Error; duration?: number }>> {\\n+  const results: Array<{\\n+    checkId: string;\\n+    result: ReviewSummary;\\n+    error?: Error;\\n+    duration?: number;\\n+  }> = [];\\n+\\n+  // Deduplicate checks within the same level to avoid double execution when\\n+  // routing or OR-dependency expansion accidentally introduces duplicates.\\n+  const seen = new Set<string>();\\n+  const uniqueChecks: string[] = [];\\n+  for (const id of checks) {\\n+    if (!seen.has(id)) {\\n+      seen.add(id);\\n+      uniqueChecks.push(id);\\n+    }\\n+  }\\n+\\n+  // Execute with limited parallelism\\n+  const pool: Promise<void>[] = [];\\n+\\n+  for (const checkId of uniqueChecks) {\\n+    // If forward-run provided explicit per-item scopes, schedule one execution per scope\\n+    const scopedRuns: Array<Array<{ check: string; index: number }>> =\\n+      (state.pendingRunScopes && state.pendingRunScopes.get(checkId)) || [];\\n+    // Guard: do not execute the same check more than once within a single wave\\n+    try {\\n+      const currentWaveCompletions = (state as any).currentWaveCompletions as\\n+        | Set<string>\\n+        | undefined;\\n+      if (currentWaveCompletions && currentWaveCompletions.has(checkId)) {\\n+        if (context.debug) {\\n+          logger.info(`[LevelDispatch] Skipping ${checkId}: already completed in current wave`);\\n+        }\\n+        continue;\\n+      }\\n+    } catch {}\\n+\\n+    // Wait if pool is full\\n+    if (pool.length >= maxParallelism) {\\n+      await Promise.race(pool);\\n+      // Remove completed promises\\n+      pool.splice(\\n+        0,\\n+        pool.length,\\n+        ...pool.filter(p => {\\n+          const settled = (p as any)._settled;\\n+          return !settled;\\n+        })\\n+      );\\n+    }\\n+\\n+    const runOnce = async (scopeOverride?: Array<{ check: string; index: number }>) => {\\n+      const startTime = Date.now();\\n+      try {\\n+        const result = await executeSingleCheck(\\n+          checkId,\\n+          context,\\n+          state,\\n+          emitEvent,\\n+          transition,\\n+          scopeOverride\\n+        );\\n+        const duration = Date.now() - startTime;\\n+        results.push({ checkId, result, duration });\\n+      } catch (error) {\\n+        const duration = Date.now() - startTime;\\n+        const err = error instanceof Error ? error : new Error(String(error));\\n+        logger.error(`[LevelDispatch] Error executing check ${checkId}: ${err.message}`);\\n+        results.push({ checkId, result: { issues: [] }, error: err, duration });\\n+      }\\n+    };\\n+\\n+    // If we have explicit scopes, schedule one run per scope; otherwise run once (default scope)\\n+    const promise = (async () => {\\n+      if (scopedRuns.length > 0) {\\n+        for (const sc of scopedRuns) {\\n+          await runOnce(sc);\\n+        }\\n+        // Clear consumed scopes\\n+        try {\\n+          state.pendingRunScopes?.delete(checkId);\\n+        } catch {}\\n+      } else {\\n+        await runOnce();\\n+      }\\n+    })();\\n+\\n+    // Mark promise as settled when done\\n+    promise\\n+      .then(() => {\\n+        (promise as any)._settled = true;\\n+      })\\n+      .catch(() => {\\n+        (promise as any)._settled = true;\\n+      });\\n+\\n+    pool.push(promise);\\n+  }\\n+\\n+  // Wait for all remaining checks\\n+  await Promise.all(pool);\\n+\\n+  return results;\\n+}\\n+\\n+/**\\n+ * Execute a check multiple times for forEach items\\n+ */\\n+async function executeCheckWithForEachItems(\\n+  checkId: string,\\n+  forEachParent: string,\\n+  forEachItems: unknown[],\\n+  context: EngineContext,\\n+  state: RunState,\\n+  emitEvent: (event: EngineEvent) => void,\\n+  transition: (newState: EngineState) => void\\n+): Promise<ReviewSummary> {\\n+  // Tactical correctness fix: re-read the parent's aggregated forEachItems\\n+  // from a fresh journal snapshot to avoid stale items during goto/on_finish\\n+  // retries. Prefer the shallowest (root-scope) entry via getRaw().\\n+  try {\\n+    const snapId = context.journal.beginSnapshot();\\n+    // Read latest aggregated (root-scope) parent entry directly from journal.\\n+    // ContextView.getRaw() returns the shallowest entry but not necessarily the latest;\\n+    // here we explicitly pick the last committed root-scope entry for correctness.\\n+    const visible = context.journal.readVisible(context.sessionId, snapId, context.event as any);\\n+    let latestItems: unknown[] | undefined;\\n+    for (let i = visible.length - 1; i >= 0; i--) {\\n+      const e = visible[i];\\n+      if (e.checkId === forEachParent && Array.isArray(e.scope) && e.scope.length === 0) {\\n+        const r: any = e.result;\\n+        if (r && Array.isArray(r.forEachItems)) {\\n+          latestItems = r.forEachItems as unknown[];\\n+          break;\\n+        }\\n+      }\\n+    }\\n+    if (Array.isArray(latestItems)) {\\n+      if (context.debug) {\\n+        try {\\n+          const prevLen = Array.isArray(forEachItems) ? (forEachItems as any[]).length : 0;\\n+          const newLen = latestItems.length;\\n+          if (prevLen !== newLen) {\\n+            logger.info(\\n+              `[LevelDispatch] Refreshing forEachItems for ${checkId}: ` +\\n+                `from parent '${forEachParent}' latestItems=${newLen} (was ${prevLen})`\\n+            );\\n+          }\\n+        } catch {}\\n+      }\\n+      forEachItems = latestItems as unknown[];\\n+    }\\n+  } catch (e) {\\n+    // Non-fatal: proceed with provided forEachItems\\n+    if (context.debug) {\\n+      logger.warn(\\n+        `[LevelDispatch] Failed to refresh forEachItems from journal for ${forEachParent}: ${e}`\\n+      );\\n+    }\\n+  }\\n+  const checkConfig = context.config.checks?.[checkId];\\n+  if (!checkConfig) {\\n+    throw new Error(`Check configuration not found: ${checkId}`);\\n+  }\\n+\\n+  // DEBUG: Log forEach execution\\n+  logger.info(\\n+    `[LevelDispatch][DEBUG] executeCheckWithForEachItems: checkId=${checkId}, forEachParent=${forEachParent}, items=${forEachItems.length}`\\n+  );\\n+  logger.info(\\n+    `[LevelDispatch][DEBUG] forEachItems: ${JSON.stringify(forEachItems).substring(0, 200)}`\\n+  );\\n+\\n+  const allIssues: ReviewIssue[] = [];\\n+  const perItemResults: ReviewSummary[] = [];\\n+  const allOutputs: unknown[] = [];\\n+  const allContents: string[] = [];\\n+  const perIterationDurations: number[] = [];\\n+\\n+  // Execute check once per forEach item\\n+  for (let itemIndex = 0; itemIndex < forEachItems.length; itemIndex++) {\\n+    const iterationStartMs = Date.now();\\n+    const scope: Array<{ check: string; index: number }> = [\\n+      { check: forEachParent, index: itemIndex },\\n+    ];\\n+\\n+    const forEachItem = forEachItems[itemIndex];\\n+    logger.info(\\n+      `[LevelDispatch][DEBUG] Starting iteration ${itemIndex} of ${checkId}, parent=${forEachParent}, item=${JSON.stringify(forEachItem)?.substring(0, 100)}`\\n+    );\\n+\\n+    // Check if the forEach item indicates a failure\\n+    // When a check fails (via fail_if or execution error), it may set a flag in the output\\n+    // Skip this iteration if the parent iteration failed\\n+    const shouldSkipDueToParentFailure =\\n+      (forEachItem as any)?.__failed === true || (forEachItem as any)?.__skip === true;\\n+\\n+    if (shouldSkipDueToParentFailure) {\\n+      // Parent iteration failed - skip this iteration\\n+      logger.info(\\n+        `‚è≠  Skipped ${checkId} iteration ${itemIndex} (forEach parent \\\"${forEachParent}\\\" iteration ${itemIndex} marked as failed)`\\n+      );\\n+\\n+      // Track this as a skipped iteration in stats\\n+      const iterationDurationMs = Date.now() - iterationStartMs;\\n+      perIterationDurations.push(iterationDurationMs);\\n+\\n+      // Add empty result to maintain array alignment\\n+      perItemResults.push({ issues: [] });\\n+      // Propagate an explicit skip marker so downstream dependents can also skip this branch\\n+      allOutputs.push({ __skip: true });\\n+\\n+      continue; // Skip to next iteration\\n+    }\\n+\\n+    // Emit visor.foreach.item span for telemetry\\n+    try {\\n+      emitNdjsonSpanWithEvents(\\n+        'visor.foreach.item',\\n+        {\\n+          'visor.check.id': checkId,\\n+          'visor.foreach.index': itemIndex,\\n+          'visor.foreach.total': forEachItems.length,\\n+        },\\n+        []\\n+      );\\n+    } catch (error) {\\n+      logger.warn(`[LevelDispatch] Failed to emit foreach.item span: ${error}`);\\n+    }\\n+\\n+    // Emit scheduled event\\n+    emitEvent({ type: 'CheckScheduled', checkId, scope });\\n+\\n+    // Create dispatch record\\n+    const dispatch: DispatchRecord = {\\n+      id: `${checkId}-${itemIndex}-${Date.now()}`,\\n+      checkId,\\n+      scope,\\n+      provider: context.checks[checkId]?.providerType || 'unknown',\\n+      startMs: Date.now(),\\n+      attempts: 1,\\n+      foreachIndex: itemIndex,\\n+    };\\n+\\n+    state.activeDispatches.set(`${checkId}-${itemIndex}`, dispatch);\\n+\\n+    try {\\n+      // Get provider\\n+      const providerType = checkConfig.type || 'ai';\\n+      const providerRegistry =\\n+        require('../../providers/check-provider-registry').CheckProviderRegistry.getInstance();\\n+      const provider = providerRegistry.getProviderOrThrow(providerType);\\n+\\n+      // Build output history for template rendering\\n+      const outputHistory = buildOutputHistoryFromJournal(context);\\n+\\n+      // Build provider configuration\\n+      const providerConfig: CheckProviderConfig = {\\n+        type: providerType,\\n+        checkName: checkId,\\n+        prompt: checkConfig.prompt,\\n+        exec: checkConfig.exec,\\n+        schema: checkConfig.schema,\\n+        group: checkConfig.group,\\n+        focus: checkConfig.focus || mapCheckNameToFocus(checkId),\\n+        transform: checkConfig.transform,\\n+        transform_js: checkConfig.transform_js,\\n+        env: checkConfig.env,\\n+        forEach: checkConfig.forEach,\\n+        ...checkConfig,\\n+        eventContext: (context.prInfo as any)?.eventContext || {},\\n+        __outputHistory: outputHistory,\\n+        ai: {\\n+          ...(checkConfig.ai || {}),\\n+          timeout: checkConfig.ai?.timeout || 600000,\\n+          debug: !!context.debug,\\n+        },\\n+      };\\n+\\n+      // Build dependency results with scope\\n+      const dependencyResults = buildDependencyResultsWithScope(\\n+        checkId,\\n+        checkConfig,\\n+        context,\\n+        scope\\n+      );\\n+\\n+      // Per-item dependency gating for map fanout: honor OR dependencies and continue_on_failure\\n+      try {\\n+        const rawDeps = (checkConfig as any)?.depends_on || [];\\n+        const depList = Array.isArray(rawDeps) ? rawDeps : [rawDeps];\\n+        if (depList.length > 0) {\\n+          const groupSatisfied = (token: string): boolean => {\\n+            if (typeof token !== 'string') return true;\\n+            const orOptions = token.includes('|')\\n+              ? token\\n+                  .split('|')\\n+                  .map(s => s.trim())\\n+                  .filter(Boolean)\\n+              : [token];\\n+            for (const opt of orOptions) {\\n+              const dr = dependencyResults.get(opt) as ReviewSummary | undefined;\\n+              const depCfg = context.config.checks?.[opt];\\n+              const cont = !!(depCfg && (depCfg as any).continue_on_failure === true);\\n+              let failed = false;\\n+              let skipped = false;\\n+              if (!dr) {\\n+                failed = true; // missing result => not satisfied\\n+              } else {\\n+                const out: any = (dr as any).output;\\n+                const fatal = hasFatalIssues(dr as any);\\n+                failed = fatal || (!!out && typeof out === 'object' && out.__failed === true);\\n+                skipped = !!(out && typeof out === 'object' && out.__skip === true);\\n+              }\\n+              const satisfied = !skipped && (!failed || cont);\\n+              if (satisfied) return true; // any option satisfies the group\\n+            }\\n+            return false;\\n+          };\\n+\\n+          let allSatisfied = true;\\n+          for (const token of depList) {\\n+            if (!groupSatisfied(token as any)) {\\n+              allSatisfied = false;\\n+              break;\\n+            }\\n+          }\\n+\\n+          if (!allSatisfied) {\\n+            // Skip this iteration without executing provider; maintain per-item alignment\\n+            if (context.debug) {\\n+              logger.info(\\n+                `[LevelDispatch] Skipping ${checkId} iteration ${itemIndex} due to unsatisfied dependency group(s)`\\n+              );\\n+            }\\n+            const iterationDurationMs = Date.now() - iterationStartMs;\\n+            perIterationDurations.push(iterationDurationMs);\\n+            perItemResults.push({ issues: [] });\\n+            allOutputs.push({ __skip: true });\\n+            // Do not call updateStats here: iteration did not execute\\n+            continue;\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      // Build PR info (use real prInfo from context if available, otherwise use defaults)\\n+      const prInfo: any = context.prInfo || {\\n+        number: 1,\\n+        title: 'State Machine Execution',\\n+        author: 'system',\\n+        eventType: context.event || 'manual',\\n+        eventContext: {},\\n+        files: [],\\n+        commits: [],\\n+      };\\n+\\n+      // Build execution context\\n+      const executionContext = {\\n+        ...context.executionContext,\\n+        _engineMode: context.mode,\\n+        _parentContext: context,\\n+        _parentState: state,\\n+      };\\n+\\n+      // Evaluate assume contract for this iteration (design-by-contract)\\n+      try {\\n+        const assumeExpr = (checkConfig as any)?.assume as string | string[] | undefined;\\n+        if (assumeExpr) {\\n+          const evaluator = new FailureConditionEvaluator();\\n+          const exprs = Array.isArray(assumeExpr) ? assumeExpr : [assumeExpr];\\n+          let ok = true;\\n+          for (const ex of exprs) {\\n+            const res = await evaluator.evaluateIfCondition(checkId, ex, {\\n+              event: context.event || 'manual',\\n+              previousResults: dependencyResults as any,\\n+            } as any);\\n+            if (!res) {\\n+              ok = false;\\n+              break;\\n+            }\\n+          }\\n+          if (!ok) {\\n+            logger.info(\\n+              `‚è≠  Skipped (assume: ${String(Array.isArray(assumeExpr) ? assumeExpr[0] : assumeExpr).substring(0, 40)}${String(Array.isArray(assumeExpr) ? assumeExpr[0] : assumeExpr).length > 40 ? '...' : ''})`\\n+            );\\n+            const iterationDurationMs = Date.now() - iterationStartMs;\\n+            perIterationDurations.push(iterationDurationMs);\\n+            perItemResults.push({ issues: [] });\\n+            allOutputs.push({ __skip: true });\\n+            continue;\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      // Emit provider telemetry\\n+      try {\\n+        emitNdjsonFallback('visor.provider', {\\n+          'visor.check.id': checkId,\\n+          'visor.provider.type': providerType,\\n+        });\\n+      } catch {}\\n+\\n+      // Execute provider with telemetry\\n+      const itemResult = await withActiveSpan(\\n+        `visor.check.${checkId}`,\\n+        {\\n+          'visor.check.id': checkId,\\n+          'visor.check.type': providerType,\\n+          'visor.foreach.index': itemIndex,\\n+        },\\n+        async () => provider.execute(prInfo, providerConfig, dependencyResults, executionContext)\\n+      );\\n+\\n+      // Enrich issues\\n+      const enrichedIssues = (itemResult.issues || []).map((issue: ReviewIssue) => ({\\n+        ...issue,\\n+        checkName: checkId,\\n+        ruleId: `${checkId}/${issue.ruleId || 'unknown'}`,\\n+        group: checkConfig.group,\\n+        schema: typeof checkConfig.schema === 'object' ? 'custom' : checkConfig.schema,\\n+        template: checkConfig.template,\\n+        timestamp: Date.now(),\\n+      }));\\n+\\n+      // Track output BEFORE creating enrichedResult\\n+      let output = (itemResult as any).output;\\n+      let content = (itemResult as any).content;\\n+\\n+      // Generate default content from issues if no content was provided\\n+      if (!content && enrichedIssues.length > 0) {\\n+        content = enrichedIssues\\n+          .map(\\n+            (i: ReviewIssue) =>\\n+              `- **${i.severity.toUpperCase()}**: ${i.message} (${i.file}:${i.line})`\\n+          )\\n+          .join('\\\\n');\\n+      }\\n+\\n+      // Check if this iteration has fatal issues (execution failures)\\n+      const iterationHasFatalIssues = enrichedIssues.some((issue: ReviewIssue) => {\\n+        const ruleId = issue.ruleId || '';\\n+        return (\\n+          ruleId.endsWith('/error') || // System errors\\n+          ruleId.includes('/execution_error') || // Command failures\\n+          ruleId.endsWith('_fail_if') // fail_if triggered\\n+        );\\n+      });\\n+\\n+      // If this iteration failed, mark the output so dependent forEach iterations can skip it\\n+      if (\\n+        iterationHasFatalIssues &&\\n+        output !== undefined &&\\n+        output !== null &&\\n+        typeof output === 'object'\\n+      ) {\\n+        output = { ...output, __failed: true };\\n+      } else if (iterationHasFatalIssues) {\\n+        // If output is primitive or undefined, wrap it in an object with __failed flag\\n+        output = { __value: output, __failed: true };\\n+      }\\n+\\n+      // DEBUG: Log output for this iteration\\n+      logger.info(\\n+        `[LevelDispatch][DEBUG] Iteration ${itemIndex}: output=${JSON.stringify(output)?.substring(0, 100)}, hasFatalIssues=${iterationHasFatalIssues}`\\n+      );\\n+\\n+      const enrichedResult: ReviewSummary = {\\n+        ...itemResult,\\n+        issues: enrichedIssues,\\n+        ...(content ? { content } : {}),\\n+      };\\n+\\n+      // JSON Schema validation for per-item outputs when a schema is provided\\n+      try {\\n+        let schemaObj =\\n+          (typeof checkConfig.schema === 'object' ? (checkConfig.schema as any) : undefined) ||\\n+          (checkConfig as any).output_schema;\\n+        // If schema is a known renderer string, attempt to load its JSON Schema\\n+        if (!schemaObj && typeof (checkConfig as any).schema === 'string') {\\n+          try {\\n+            const { loadRendererSchema } = await import('../dispatch/renderer-schema');\\n+            schemaObj = await loadRendererSchema((checkConfig as any).schema as string);\\n+          } catch {}\\n+        }\\n+        const itemOutput = output;\\n+        if (schemaObj && itemOutput !== undefined) {\\n+          const Ajv = require('ajv');\\n+          const ajv = new Ajv({ allErrors: true, allowUnionTypes: true, strict: false });\\n+          const validate = ajv.compile(schemaObj);\\n+          const valid = validate(itemOutput);\\n+          if (!valid) {\\n+            const errs = (validate.errors || [])\\n+              .slice(0, 3)\\n+              .map((e: any) => e.message)\\n+              .join('; ');\\n+            const issue: ReviewIssue = {\\n+              file: 'contract',\\n+              line: 0,\\n+              ruleId: `contract/schema_validation_failed`,\\n+              message: `Output schema validation failed${errs ? `: ${errs}` : ''}`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+              checkName: checkId,\\n+              group: checkConfig.group,\\n+              schema: 'json-schema',\\n+              timestamp: Date.now(),\\n+            } as any;\\n+            enrichedResult.issues = [...(enrichedResult.issues || []), issue];\\n+            if (Array.isArray(enrichedIssues)) {\\n+              enrichedIssues.push(issue);\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      // Evaluate guarantee contract (non-fatal): append error issues on violation\\n+      try {\\n+        const guaranteeExpr = (checkConfig as any)?.guarantee as string | string[] | undefined;\\n+        if (guaranteeExpr) {\\n+          const evaluator = new FailureConditionEvaluator();\\n+          const exprs = Array.isArray(guaranteeExpr) ? guaranteeExpr : [guaranteeExpr];\\n+          for (const ex of exprs) {\\n+            const holds = await evaluator.evaluateIfCondition(checkId, ex, {\\n+              previousResults: dependencyResults as any,\\n+              event: context.event || 'manual',\\n+            } as any);\\n+            if (!holds) {\\n+              const issue: ReviewIssue = {\\n+                file: 'contract',\\n+                line: 0,\\n+                ruleId: `contract/guarantee_failed`,\\n+                message: `Guarantee failed: ${ex}`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+                checkName: checkId,\\n+                group: checkConfig.group,\\n+                schema:\\n+                  typeof checkConfig.schema === 'object' ? 'custom' : (checkConfig.schema as any),\\n+                timestamp: Date.now(),\\n+              } as any;\\n+              enrichedResult.issues = [...(enrichedResult.issues || []), issue];\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      // Evaluate fail_if for this forEach iteration\\n+      if (checkConfig.fail_if) {\\n+        try {\\n+          const evaluator = new FailureConditionEvaluator();\\n+          // Build outputs map for fail_if evaluation (use dependency results as previous outputs)\\n+          const failed = await evaluator.evaluateSimpleCondition(\\n+            checkId,\\n+            typeof checkConfig.schema === 'object' ? 'custom' : checkConfig.schema || '',\\n+            checkConfig.group || '',\\n+            enrichedResult,\\n+            checkConfig.fail_if,\\n+            Object.fromEntries(dependencyResults.entries()) as Record<string, ReviewSummary>\\n+          );\\n+\\n+          if (failed) {\\n+            logger.warn(\\n+              `[LevelDispatch] fail_if triggered for ${checkId} iteration ${itemIndex}: ${checkConfig.fail_if}`\\n+            );\\n+\\n+            // Add fail_if issue to the result\\n+            const failIssue: ReviewIssue = {\\n+              file: 'system',\\n+              line: 0,\\n+              ruleId: `${checkId}/${checkId}_fail_if`,\\n+              message: `Check failure condition met: ${checkConfig.fail_if}`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+              checkName: checkId,\\n+              group: checkConfig.group,\\n+              schema: typeof checkConfig.schema === 'object' ? 'custom' : checkConfig.schema,\\n+              timestamp: Date.now(),\\n+            };\\n+\\n+            enrichedResult.issues = [...(enrichedResult.issues || []), failIssue];\\n+            enrichedIssues.push(failIssue);\\n+            allIssues.push(failIssue);\\n+\\n+            // Re-check if iteration has fatal issues after adding fail_if issue\\n+            const nowHasFatalIssues = enrichedResult.issues.some((issue: ReviewIssue) => {\\n+              const ruleId = issue.ruleId || '';\\n+              return (\\n+                ruleId.endsWith('/error') ||\\n+                ruleId.includes('/execution_error') ||\\n+                ruleId.endsWith('_fail_if')\\n+              );\\n+            });\\n+\\n+            // Update output with __failed flag if needed\\n+            if (\\n+              nowHasFatalIssues &&\\n+              output !== undefined &&\\n+              output !== null &&\\n+              typeof output === 'object' &&\\n+              !(output as any).__failed\\n+            ) {\\n+              output = { ...output, __failed: true };\\n+            } else if (nowHasFatalIssues && !(output as any)?.__failed) {\\n+              output = { __value: output, __failed: true };\\n+            }\\n+          }\\n+        } catch (error) {\\n+          const msg = error instanceof Error ? error.message : String(error);\\n+          logger.error(\\n+            `[LevelDispatch] Error evaluating fail_if for ${checkId} iteration ${itemIndex}: ${msg}`\\n+          );\\n+        }\\n+      }\\n+\\n+      // Store per-item result\\n+      perItemResults.push(enrichedResult);\\n+      allIssues.push(...enrichedIssues);\\n+      allOutputs.push(output);\\n+\\n+      // Track content\\n+      if (typeof content === 'string' && content.trim()) {\\n+        allContents.push(content.trim());\\n+      }\\n+\\n+      // Store in journal with scope - EXPLICITLY include output field\\n+      try {\\n+        const journalEntry = {\\n+          sessionId: context.sessionId,\\n+          checkId,\\n+          result: { ...enrichedResult, output } as any,\\n+          event: context.event || 'manual',\\n+          scope,\\n+        };\\n+        // DEBUG: Log journal entry\\n+        logger.info(\\n+          `[LevelDispatch][DEBUG] Committing to journal: checkId=${checkId}, scope=${JSON.stringify(scope)}, hasOutput=${output !== undefined}`\\n+        );\\n+        context.journal.commitEntry(journalEntry);\\n+      } catch (error) {\\n+        logger.warn(`[LevelDispatch] Failed to commit to journal: ${error}`);\\n+      }\\n+\\n+      state.activeDispatches.delete(`${checkId}-${itemIndex}`);\\n+\\n+      // Emit completed event\\n+      emitEvent({\\n+        type: 'CheckCompleted',\\n+        checkId,\\n+        scope,\\n+        result: {\\n+          ...enrichedResult,\\n+          output,\\n+        },\\n+      });\\n+\\n+      // Track duration for this iteration\\n+      const iterationDurationMs = Date.now() - iterationStartMs;\\n+      perIterationDurations.push(iterationDurationMs);\\n+\\n+      // Track statistics for this forEach iteration\\n+      updateStats(\\n+        [{ checkId, result: enrichedResult, duration: iterationDurationMs }],\\n+        state,\\n+        true\\n+      );\\n+    } catch (error) {\\n+      // Track duration for failed iteration\\n+      const iterationDurationMs = Date.now() - iterationStartMs;\\n+      perIterationDurations.push(iterationDurationMs);\\n+      const err = error instanceof Error ? error : new Error(String(error));\\n+      logger.error(\\n+        `[LevelDispatch] Error executing check ${checkId} item ${itemIndex}: ${err.message}`\\n+      );\\n+\\n+      state.activeDispatches.delete(`${checkId}-${itemIndex}`);\\n+\\n+      // Emit error event\\n+      emitEvent({\\n+        type: 'CheckErrored',\\n+        checkId,\\n+        scope,\\n+        error: {\\n+          message: err.message,\\n+          stack: err.stack,\\n+          name: err.name,\\n+        },\\n+      });\\n+\\n+      // Add error to results\\n+      const errorIssue: ReviewIssue = {\\n+        file: '',\\n+        line: 0,\\n+        ruleId: `${checkId}/error`,\\n+        message: err.message,\\n+        severity: 'error',\\n+        category: 'logic',\\n+      };\\n+\\n+      allIssues.push(errorIssue);\\n+      perItemResults.push({ issues: [errorIssue] });\\n+\\n+      // Track statistics for this failed forEach iteration\\n+      updateStats(\\n+        [{ checkId, result: { issues: [errorIssue] }, error: err, duration: iterationDurationMs }],\\n+        state,\\n+        true\\n+      );\\n+    }\\n+  }\\n+\\n+  // Mark as completed\\n+  state.completedChecks.add(checkId);\\n+\\n+  // Update forEach metadata in stats\\n+  const checkStats = state.stats.get(checkId);\\n+  if (checkStats) {\\n+    checkStats.outputsProduced = allOutputs.length;\\n+    checkStats.perIterationDuration = perIterationDurations;\\n+\\n+    // Create preview of forEach items (first 3 + indicator for more)\\n+    const previewItems = allOutputs.slice(0, 3).map(item => {\\n+      const str = typeof item === 'string' ? item : (JSON.stringify(item) ?? 'undefined');\\n+      return str.length > 50 ? str.substring(0, 50) + '...' : str;\\n+    });\\n+\\n+    if (allOutputs.length > 3) {\\n+      checkStats.forEachPreview = [...previewItems, `...${allOutputs.length - 3} more`];\\n+    } else {\\n+      checkStats.forEachPreview = previewItems;\\n+    }\\n+\\n+    state.stats.set(checkId, checkStats);\\n+\\n+    // Check if ALL iterations failed (complete failure)\\n+    // If so, mark check as failed so dependents can be skipped\\n+    if (checkStats.totalRuns > 0 && checkStats.failedRuns === checkStats.totalRuns) {\\n+      logger.info(\\n+        `[LevelDispatch] forEach check ${checkId} failed completely (${checkStats.failedRuns}/${checkStats.totalRuns} iterations failed)`\\n+      );\\n+      // Mark in state so dependents know this check failed\\n+      if (!(state as any).failedChecks) {\\n+        (state as any).failedChecks = new Set<string>();\\n+      }\\n+      (state as any).failedChecks.add(checkId);\\n+    }\\n+  }\\n+\\n+  // Return aggregated result\\n+  const aggregatedResult: any = {\\n+    issues: allIssues,\\n+    isForEach: true,\\n+    forEachItems: allOutputs,\\n+    forEachItemResults: perItemResults,\\n+    // Include aggregated content from all iterations\\n+    ...(allContents.length > 0 ? { content: allContents.join('\\\\n') } : {}),\\n+  };\\n+\\n+  // DEBUG: Log aggregated result\\n+  logger.info(\\n+    `[LevelDispatch][DEBUG] Aggregated result for ${checkId}: forEachItems.length=${allOutputs.length}, results=${perItemResults.length}`\\n+  );\\n+  logger.info(`[LevelDispatch][DEBUG] allOutputs: ${JSON.stringify(allOutputs).substring(0, 200)}`);\\n+\\n+  // Store aggregated result in journal (without scope - this is the parent-level result)\\n+  // Before storing, process routing for the aggregated child check so on_success/on_fail\\n+  // can schedule follow-up actions (e.g., per-item remediation) based on forEach results.\\n+  try {\\n+    logger.info(`[LevelDispatch] Calling handleRouting for ${checkId}`);\\n+  } catch {}\\n+  try {\\n+    // Mark completion prior to routing so guards see this as completed in the wave\\n+    state.completedChecks.add(checkId);\\n+    const currentWaveCompletions = (state as any).currentWaveCompletions as Set<string> | undefined;\\n+    if (currentWaveCompletions) currentWaveCompletions.add(checkId);\\n+\\n+    await handleRouting(context, state, transition, emitEvent, {\\n+      checkId,\\n+      scope: [],\\n+      result: aggregatedResult as any,\\n+      checkConfig: checkConfig as any,\\n+      success: !hasFatalIssues(aggregatedResult as any),\\n+    });\\n+  } catch (error) {\\n+    logger.warn(`[LevelDispatch] Routing error for aggregated forEach ${checkId}: ${error}`);\\n+  }\\n+\\n+  try {\\n+    context.journal.commitEntry({\\n+      sessionId: context.sessionId,\\n+      checkId,\\n+      result: aggregatedResult as any,\\n+      event: context.event || 'manual',\\n+      scope: [],\\n+    });\\n+    logger.info(`[LevelDispatch][DEBUG] Committed aggregated result to journal with scope=[]`);\\n+  } catch (error) {\\n+    logger.warn(`[LevelDispatch] Failed to commit aggregated forEach result to journal: ${error}`);\\n+  }\\n+\\n+  // Note: We intentionally do not increment totals here for the aggregated\\n+  // child invocation; per-iteration stats were recorded above via updateStats.\\n+  // Each iteration was counted separately, so the totalRuns already reflects\\n+  // the correct number of executions.\\n+\\n+  // Emit completed event for aggregated result\\n+  emitEvent({\\n+    type: 'CheckCompleted',\\n+    checkId,\\n+    scope: [],\\n+    result: aggregatedResult,\\n+  });\\n+\\n+  const parentCheckConfig = context.config.checks?.[forEachParent];\\n+\\n+  // Process on_finish for forEach PARENT after all forEach children complete\\n+  // The forEach parent is the one that produced the forEachItems\\n+  logger.info(\\n+    `[LevelDispatch][DEBUG] Checking on_finish for forEach parent ${forEachParent}: has_on_finish=${!!parentCheckConfig?.on_finish}, is_forEach=${!!parentCheckConfig?.forEach}`\\n+  );\\n+\\n+  if (parentCheckConfig?.on_finish && parentCheckConfig.forEach) {\\n+    logger.info(\\n+      `[LevelDispatch] Processing on_finish for forEach parent ${forEachParent} after children complete`\\n+    );\\n+\\n+    // Get the parent check's result from journal\\n+    try {\\n+      const snapshotId = context.journal.beginSnapshot();\\n+      const contextView = new (require('../../snapshot-store').ContextView)(\\n+        context.journal,\\n+        context.sessionId,\\n+        snapshotId,\\n+        [],\\n+        context.event\\n+      );\\n+      const parentResult = contextView.get(forEachParent);\\n+\\n+      if (parentResult) {\\n+        logger.info(\\n+          `[LevelDispatch] Found parent result for ${forEachParent}, evaluating on_finish`\\n+        );\\n+\\n+        // Evaluate on_finish routing (goto/goto_js) for the forEach parent\\n+        const onFinish = parentCheckConfig.on_finish;\\n+\\n+        // Process on_finish.run (if any). When we enqueue forward runs via\\n+        // on_finish from within LevelDispatch (i.e., forEach parent path), we\\n+        // must also request a WaveRetry so that checks whose execution depends\\n+        // on updated memory/side-effects (but are not direct dependents of the\\n+        // scheduled target) can re-evaluate their `if` conditions next wave.\\n+        // This mirrors routing.ts behavior and is necessary for flows where an\\n+        // aggregator sets flags consumed by later checks (e.g., posting steps).\\n+        let queuedForward = false;\\n+        logger.info(\\n+          `[LevelDispatch] on_finish.run: ${onFinish.run?.length || 0} targets, targets=${JSON.stringify(onFinish.run || [])}`\\n+        );\\n+        if (onFinish.run && onFinish.run.length > 0) {\\n+          for (const targetCheck of onFinish.run) {\\n+            logger.info(`[LevelDispatch] Processing on_finish.run target: ${targetCheck}`);\\n+            logger.info(\\n+              `[LevelDispatch] Loop budget check: routingLoopCount=${state.routingLoopCount}, max_loops=${context.config.routing?.max_loops ?? 10}`\\n+            );\\n+            // Check loop budget before scheduling\\n+            if (checkLoopBudget(context, state, 'on_finish', 'run')) {\\n+              const errorIssue: ReviewIssue = {\\n+                file: 'system',\\n+                line: 0,\\n+                ruleId: `${forEachParent}/routing/loop_budget_exceeded`,\\n+                message: `Routing loop budget exceeded (max_loops=${context.config.routing?.max_loops ?? 10}) during on_finish run`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              };\\n+              // Add error to parent result (not child aggregatedResult)\\n+              parentResult.issues = [...(parentResult.issues || []), errorIssue];\\n+              // Update parent result in journal with the error\\n+              try {\\n+                context.journal.commitEntry({\\n+                  sessionId: context.sessionId,\\n+                  checkId: forEachParent,\\n+                  result: parentResult as any,\\n+                  event: context.event || 'manual',\\n+                  scope: [],\\n+                });\\n+              } catch (err) {\\n+                logger.warn(\\n+                  `[LevelDispatch] Failed to commit parent result with loop budget error: ${err}`\\n+                );\\n+              }\\n+              return aggregatedResult; // ABORT\\n+            }\\n+\\n+            // Increment loop count\\n+            state.routingLoopCount++;\\n+\\n+            emitEvent({\\n+              type: 'ForwardRunRequested',\\n+              target: targetCheck,\\n+              scope: [],\\n+              origin: 'run',\\n+            });\\n+            queuedForward = true;\\n+          }\\n+        }\\n+\\n+        // Declarative transitions override goto/goto_js when present.\\n+        // Mirror routing.ts behavior for the forEach-parent on_finish path.\\n+        try {\\n+          const { evaluateTransitions } = await import('./routing');\\n+          const transTarget = await evaluateTransitions(\\n+            (onFinish as any).transitions,\\n+            forEachParent,\\n+            parentCheckConfig as any,\\n+            parentResult as any,\\n+            context,\\n+            state\\n+          );\\n+          if (transTarget !== undefined) {\\n+            if (transTarget) {\\n+              // Loop budget guard\\n+              if (checkLoopBudget(context, state, 'on_finish', 'goto')) {\\n+                const errorIssue: ReviewIssue = {\\n+                  file: 'system',\\n+                  line: 0,\\n+                  ruleId: `${forEachParent}/routing/loop_budget_exceeded`,\\n+                  message: `Routing loop budget exceeded (max_loops=${context.config.routing?.max_loops ?? 10}) during on_finish transitions`,\\n+                  severity: 'error',\\n+                  category: 'logic',\\n+                };\\n+                parentResult.issues = [...(parentResult.issues || []), errorIssue];\\n+                try {\\n+                  context.journal.commitEntry({\\n+                    sessionId: context.sessionId,\\n+                    checkId: forEachParent,\\n+                    result: parentResult as any,\\n+                    event: context.event || 'manual',\\n+                    scope: [],\\n+                  });\\n+                } catch {}\\n+                return aggregatedResult; // abort further routing\\n+              }\\n+              state.routingLoopCount++;\\n+              emitEvent({\\n+                type: 'ForwardRunRequested',\\n+                target: transTarget.to,\\n+                scope: [],\\n+                origin: 'goto_js',\\n+                gotoEvent: (transTarget as any).goto_event,\\n+              });\\n+              queuedForward = true;\\n+            }\\n+            // Whether null (explicit no-op) or a target, transitions override goto/goto_js\\n+            // Also request a WaveRetry if we queued something (handled below)\\n+            if (queuedForward) {\\n+              // no-op here; WaveRetry emitted after this block\\n+            }\\n+            return aggregatedResult;\\n+          }\\n+        } catch (e) {\\n+          logger.error(\\n+            `[LevelDispatch] Error evaluating on_finish transitions for ${forEachParent}: ${e instanceof Error ? e.message : String(e)}`\\n+          );\\n+        }\\n+\\n+        // Evaluate goto_js and schedule routing if transitions did not match\\n+        const { evaluateGoto } = await import('./routing');\\n+\\n+        // Debug logging for forEach on_finish.goto_js evaluation\\n+        if (context.debug || true) {\\n+          logger.info(\\n+            `[LevelDispatch] Evaluating on_finish.goto_js for forEach parent: ${forEachParent}`\\n+          );\\n+          if (onFinish.goto_js) {\\n+            logger.info(`[LevelDispatch] goto_js code: ${onFinish.goto_js.substring(0, 200)}`);\\n+          }\\n+          try {\\n+            const snapshotId = context.journal.beginSnapshot();\\n+            const view = new (require('../../snapshot-store').ContextView)(\\n+              context.journal,\\n+              context.sessionId,\\n+              snapshotId,\\n+              [],\\n+              undefined\\n+            );\\n+            const vfHist = view.getHistory('validate-fact') || [];\\n+            logger.info(`[LevelDispatch] history['validate-fact'] length: ${vfHist.length}`);\\n+            const all = context.journal.readVisible(context.sessionId, snapshotId, undefined);\\n+            const keys = Array.from(new Set(all.map((e: any) => e.checkId)));\\n+            logger.info(`[LevelDispatch] history keys: ${keys.join(', ')}`);\\n+          } catch {}\\n+        }\\n+\\n+        const gotoTarget = await (evaluateGoto as any)(\\n+          onFinish.goto_js,\\n+          onFinish.goto,\\n+          forEachParent,\\n+          parentCheckConfig,\\n+          parentResult,\\n+          context,\\n+          state\\n+        );\\n+\\n+        if (context.debug || true) {\\n+          logger.info(`[LevelDispatch] goto_js evaluation result: ${gotoTarget || 'null'}`);\\n+        }\\n+\\n+        if (gotoTarget) {\\n+          // If we also queued on_finish.run and the goto target is the same\\n+          // forEach parent, defer this self-goto to avoid premature preemption\\n+          // before reducers (on_finish.run) have updated shared state (e.g.,\\n+          // memory). The goto will be scheduled AFTER the WaveRetry completes\\n+          // and checks can re-evaluate their conditions.\\n+          if (queuedForward && gotoTarget === forEachParent) {\\n+            logger.info(\\n+              `[LevelDispatch] on_finish.goto to self (${gotoTarget}) deferred, will process after WaveRetry`\\n+            );\\n+            // Still schedule the goto, but it will execute after aggregate completes\\n+            // and WaveRetry processes. This ensures the goto happens AFTER memory\\n+            // is updated by aggregate.\\n+            // Note: We don't schedule it immediately to avoid preemption, but we\\n+            // DO schedule it so it processes after the wave retry.\\n+          }\\n+          // Always schedule the goto (even if deferred) - it will execute in order\\n+          // Check loop budget before scheduling goto\\n+          if (checkLoopBudget(context, state, 'on_finish', 'goto')) {\\n+            const errorIssue: ReviewIssue = {\\n+              file: 'system',\\n+              line: 0,\\n+              ruleId: `${forEachParent}/routing/loop_budget_exceeded`,\\n+              message: `Routing loop budget exceeded (max_loops=${context.config.routing?.max_loops ?? 10}) during on_finish goto`,\\n+    \\n\\n... [TRUNCATED: Diff too large (100.5KB), showing first 50KB] ...\",\"status\":\"added\"},{\"filename\":\"src/state-machine/states/plan-ready.ts\",\"additions\":14,\"deletions\":0,\"changes\":477,\"patch\":\"diff --git a/src/state-machine/states/plan-ready.ts b/src/state-machine/states/plan-ready.ts\\nnew file mode 100644\\nindex 00000000..3b53d07b\\n--- /dev/null\\n+++ b/src/state-machine/states/plan-ready.ts\\n@@ -0,0 +1,477 @@\\n+/**\\n+ * PlanReady State Handler\\n+ *\\n+ * Responsibilities:\\n+ * - Build dependency graph using DependencyResolver\\n+ * - Validate graph (check for cycles)\\n+ * - Compute check metadata (tags, sessions, triggers)\\n+ * - Transition to WavePlanning\\n+ */\\n+\\n+import type { EngineContext, RunState, EngineState } from '../../types/engine';\\n+import { DependencyResolver } from '../../dependency-resolver';\\n+import { logger } from '../../logger';\\n+\\n+export async function handlePlanReady(\\n+  context: EngineContext,\\n+  state: RunState,\\n+  transition: (newState: EngineState) => void\\n+): Promise<void> {\\n+  if (context.debug) {\\n+    logger.info('[PlanReady] Building dependency graph...');\\n+    if (context.requestedChecks) {\\n+      logger.info(`[PlanReady] Requested checks: ${context.requestedChecks.join(', ')}`);\\n+    }\\n+    if (context.config.tag_filter) {\\n+      logger.info(\\n+        `[PlanReady] Tag filter: include=${JSON.stringify(context.config.tag_filter.include)}, exclude=${JSON.stringify(context.config.tag_filter.exclude)}`\\n+      );\\n+    } else {\\n+      logger.info('[PlanReady] No tag filter specified - will include only untagged checks');\\n+    }\\n+  }\\n+\\n+  // Filter checks based on requested checks list, event triggers, and tags BEFORE building dependency graph\\n+  //\\n+  // Filtering order (matches legacy engine):\\n+  // 1. Explicit check list (requestedChecks) - if provided, expand with transitive dependencies first\\n+  // 2. Event filtering: Only include checks where:\\n+  //    - checkConfig.on is undefined (runs on any event), OR\\n+  //    - checkConfig.on includes context.event\\n+  // 3. Tag filtering (matches legacy engine behavior):\\n+  //    - When no tag filter is specified, include only untagged checks by default\\n+  //    - Tagged checks are opt-in unless tag_filter is provided\\n+  //    - If exclude tags specified, exclude checks with any matching tag\\n+  //    - If include tags specified, include checks with at least one matching tag OR untagged checks\\n+  const eventTrigger = context.event;\\n+  const tagFilter = context.config.tag_filter;\\n+\\n+  // Expand requested checks with transitive dependencies (matches legacy engine)\\n+  const expandWithTransitives = (rootChecks: string[]): Set<string> | null => {\\n+    const expanded = new Set<string>(rootChecks);\\n+\\n+    const allowByTags = (checkId: string): boolean => {\\n+      if (!tagFilter) return true;\\n+      const cfg = context.config.checks?.[checkId];\\n+      const tags: string[] = cfg?.tags || [];\\n+      if (tagFilter.exclude && tagFilter.exclude.some(t => tags.includes(t))) return false;\\n+      if (tagFilter.include && tagFilter.include.length > 0) {\\n+        return tagFilter.include.some(t => tags.includes(t));\\n+      }\\n+      return true;\\n+    };\\n+\\n+    const allowByEvent = (checkId: string): boolean => {\\n+      const cfg = context.config.checks?.[checkId];\\n+      const triggers = cfg?.on || [];\\n+      if (!triggers || triggers.length === 0) return true;\\n+      const current = eventTrigger || 'manual';\\n+      return triggers.includes(current as any);\\n+    };\\n+\\n+    const visit = (checkId: string): string | null => {\\n+      const cfg = context.config.checks?.[checkId];\\n+      if (!cfg || !cfg.depends_on) return null;\\n+\\n+      const depTokens = Array.isArray(cfg.depends_on) ? cfg.depends_on : [cfg.depends_on];\\n+      const expandDep = (tok: string): string[] => {\\n+        if (tok.includes('|')) {\\n+          return tok\\n+            .split('|')\\n+            .map(s => s.trim())\\n+            .filter(Boolean);\\n+        }\\n+        return [tok];\\n+      };\\n+\\n+      const deps = depTokens.flatMap(expandDep);\\n+      for (const depId of deps) {\\n+        // Check if dependency exists - if not, return error\\n+        if (!context.config.checks?.[depId]) {\\n+          return `Check \\\"${checkId}\\\" depends on \\\"${depId}\\\" but \\\"${depId}\\\" is not defined`;\\n+        }\\n+        if (!allowByTags(depId)) continue;\\n+        if (!allowByEvent(depId)) continue;\\n+        if (!expanded.has(depId)) {\\n+          expanded.add(depId);\\n+          const err = visit(depId);\\n+          if (err) return err;\\n+        }\\n+      }\\n+      return null;\\n+    };\\n+\\n+    for (const checkId of rootChecks) {\\n+      const err = visit(checkId);\\n+      if (err) {\\n+        // Record validation error\\n+        const validationIssue: any = {\\n+          file: 'system',\\n+          line: 0,\\n+          message: err,\\n+          category: 'logic',\\n+          severity: 'error',\\n+          ruleId: 'system/error',\\n+        };\\n+\\n+        context.journal.commitEntry({\\n+          sessionId: context.sessionId,\\n+          scope: [],\\n+          checkId: 'system',\\n+          result: {\\n+            issues: [validationIssue],\\n+            output: undefined,\\n+          },\\n+        });\\n+\\n+        return null; // Signal error by returning null\\n+      }\\n+    }\\n+\\n+    return expanded;\\n+  };\\n+\\n+  const requestedChecksSet = context.requestedChecks\\n+    ? expandWithTransitives(context.requestedChecks)\\n+    : undefined;\\n+\\n+  // Check if dependency validation failed during expansion\\n+  if (context.requestedChecks && requestedChecksSet === null) {\\n+    logger.error(`[PlanReady] Dependency validation failed during expansion`);\\n+    // Transition to Completed since error was already recorded\\n+    state.currentState = 'Completed';\\n+    return;\\n+  }\\n+\\n+  if (context.debug && requestedChecksSet && context.requestedChecks) {\\n+    const added = Array.from(requestedChecksSet).filter(c => !context.requestedChecks!.includes(c));\\n+    if (added.length > 0) {\\n+      logger.info(\\n+        `[PlanReady] Expanded requested checks with transitive dependencies: ${added.join(', ')}`\\n+      );\\n+    }\\n+  }\\n+\\n+  const filteredChecks: Record<string, import('../../types/config').CheckConfig> = {};\\n+\\n+  // Identify checks that are only meant to run via routing (on_* .run targets).\\n+  // These should not be part of the initial graph unless explicitly requested.\\n+  const routingRunTargets = new Set<string>();\\n+  for (const [, cfg] of Object.entries(context.config.checks || {})) {\\n+    const onFinish = (cfg as any).on_finish || {};\\n+    const onSuccess = (cfg as any).on_success || {};\\n+    const onFail = (cfg as any).on_fail || {};\\n+    const collect = (arr?: string[]) => {\\n+      if (Array.isArray(arr)) {\\n+        for (const t of arr) if (typeof t === 'string' && t) routingRunTargets.add(t);\\n+      }\\n+    };\\n+    collect(onFinish.run);\\n+    collect(onSuccess.run);\\n+    collect(onFail.run);\\n+  }\\n+\\n+  for (const [checkId, checkConfig] of Object.entries(context.config.checks || {})) {\\n+    // 1. Filter by explicit check list (if provided, now includes transitive dependencies)\\n+    if (requestedChecksSet && !requestedChecksSet.has(checkId)) {\\n+      if (context.debug) {\\n+        logger.info(\\n+          `[PlanReady] Skipping check '${checkId}': not in expanded requested checks list`\\n+        );\\n+      }\\n+      continue;\\n+    }\\n+    // 1b. Exclude checks that are intended to be started by routing (run targets)\\n+    // unless they were explicitly requested. This avoids running aggregators/routers\\n+    // at wave 0; they will be scheduled by ForwardRunRequested from on_* handlers.\\n+    if (!requestedChecksSet && routingRunTargets.has(checkId)) {\\n+      if (context.debug) {\\n+        logger.info(\\n+          `[PlanReady] Skipping check '${checkId}': routing-run target (will be scheduled by on_*.run)`\\n+        );\\n+      }\\n+      continue;\\n+    }\\n+    // Check if event trigger matches (same logic as event-mapper.ts shouldRunCheck)\\n+    // If 'on' is not specified, the check can run on any event\\n+    if (checkConfig.on && eventTrigger && !checkConfig.on.includes(eventTrigger)) {\\n+      if (context.debug) {\\n+        logger.info(\\n+          `[PlanReady] Skipping check '${checkId}': on=${JSON.stringify(checkConfig.on)}, event=${eventTrigger}`\\n+        );\\n+      }\\n+      continue;\\n+    }\\n+\\n+    // Tag filtering (matches legacy CheckExecutionEngine.filterChecksByTags logic)\\n+    const checkTags = checkConfig.tags || [];\\n+    const isTagged = checkTags.length > 0;\\n+\\n+    if (tagFilter) {\\n+      // Check exclude tags first (if any exclude tag matches, skip the check)\\n+      if (tagFilter.exclude && tagFilter.exclude.length > 0) {\\n+        const hasExcludedTag = tagFilter.exclude.some(tag => checkTags.includes(tag));\\n+        if (hasExcludedTag) {\\n+          if (context.debug) {\\n+            logger.info(`[PlanReady] Skipping check '${checkId}': excluded by tag filter`);\\n+          }\\n+          continue;\\n+        }\\n+      }\\n+\\n+      // Check include tags (if specified, at least one must match OR check is untagged)\\n+      if (tagFilter.include && tagFilter.include.length > 0) {\\n+        const hasIncludedTag = tagFilter.include.some(tag => checkTags.includes(tag));\\n+        if (!hasIncludedTag && isTagged) {\\n+          if (context.debug) {\\n+            logger.info(`[PlanReady] Skipping check '${checkId}': not included by tag filter`);\\n+          }\\n+          continue;\\n+        }\\n+      }\\n+    } else {\\n+      // No tag filter specified: include only untagged checks by default\\n+      // Tagged checks are opt-in unless tag_filter is provided\\n+      if (isTagged) {\\n+        if (context.debug) {\\n+          logger.info(\\n+            `[PlanReady] Skipping check '${checkId}': tagged but no tag filter specified`\\n+          );\\n+        }\\n+        continue;\\n+      }\\n+    }\\n+\\n+    filteredChecks[checkId] = checkConfig;\\n+  }\\n+\\n+  if (context.debug) {\\n+    const totalChecks = Object.keys(context.config.checks || {}).length;\\n+    const filteredCount = Object.keys(filteredChecks).length;\\n+    logger.info(\\n+      `[PlanReady] Filtered ${totalChecks} checks to ${filteredCount} based on event=${eventTrigger}`\\n+    );\\n+  }\\n+\\n+  // Forward-closure across dependents is useful when running ‚Äúall‚Äù checks with no explicit roots,\\n+  // but it must NOT run when the caller provided an explicit requested list (tests that expect\\n+  // only a subset would be surprised). Only apply this when no requestedChecks are set.\\n+  if (!context.requestedChecks || context.requestedChecks.length === 0) {\\n+    const dependentsMap = new Map<string, string[]>();\\n+    for (const [cid, cfg] of Object.entries(context.config.checks || {})) {\\n+      const deps = (cfg.depends_on || []) as string[];\\n+      const depList = Array.isArray(deps) ? deps : [deps];\\n+      for (const raw of depList) {\\n+        if (typeof raw !== 'string') continue;\\n+        const tokens = raw.includes('|')\\n+          ? raw\\n+              .split('|')\\n+              .map(s => s.trim())\\n+              .filter(Boolean)\\n+          : [raw];\\n+        for (const dep of tokens) {\\n+          if (!dependentsMap.has(dep)) dependentsMap.set(dep, []);\\n+          dependentsMap.get(dep)!.push(cid);\\n+        }\\n+      }\\n+    }\\n+\\n+    const queue: string[] = Object.keys(filteredChecks);\\n+    const seenForward = new Set(queue);\\n+    while (queue.length > 0) {\\n+      const cur = queue.shift()!;\\n+      const kids = dependentsMap.get(cur) || [];\\n+      for (const child of kids) {\\n+        if (seenForward.has(child)) continue;\\n+        // Only add child if it passes event/tag filtering (reuse policy)\\n+        const cfg = context.config.checks?.[child];\\n+        if (!cfg) continue;\\n+        if (cfg.on && eventTrigger && !cfg.on.includes(eventTrigger)) continue;\\n+        const tags = cfg.tags || [];\\n+        const isTagged = tags.length > 0;\\n+        if (!tagFilter && isTagged) continue;\\n+        if (tagFilter) {\\n+          if (tagFilter.exclude && tagFilter.exclude.length > 0) {\\n+            const hasExcluded = tagFilter.exclude.some(t => tags.includes(t));\\n+            if (hasExcluded) continue;\\n+          }\\n+          if (tagFilter.include && tagFilter.include.length > 0) {\\n+            const hasIncluded = tagFilter.include.some(t => tags.includes(t));\\n+            if (!hasIncluded && isTagged) continue;\\n+          }\\n+        }\\n+        filteredChecks[child] = cfg;\\n+        seenForward.add(child);\\n+        queue.push(child);\\n+        if (context.debug)\\n+          logger.info(`[PlanReady] Added dependent '${child}' via forward-closure from '${cur}'`);\\n+      }\\n+    }\\n+  }\\n+\\n+  // Helper to check if dependencies are satisfied\\n+  // For OR dependencies (pipe syntax), at least one must be in filteredChecks\\n+  const areDependenciesSatisfied = (dependencies: string[]): boolean => {\\n+    for (const dep of dependencies) {\\n+      // Check for OR dependency (pipe syntax)\\n+      if (dep.includes('|')) {\\n+        const orOptions = dep\\n+          .split('|')\\n+          .map(s => s.trim())\\n+          .filter(Boolean);\\n+        // At least one option must exist in filtered checks\\n+        const hasAtLeastOne = orOptions.some(opt => filteredChecks[opt] !== undefined);\\n+        if (!hasAtLeastOne) {\\n+          return false;\\n+        }\\n+      } else {\\n+        // Regular dependency - must exist in filtered checks\\n+        if (filteredChecks[dep] === undefined) {\\n+          return false;\\n+        }\\n+      }\\n+    }\\n+    return true;\\n+  };\\n+\\n+  // Second pass: Remove checks whose dependencies are not satisfied\\n+  // Note: When tag filtering is active, we allow soft dependencies - checks can run\\n+  // even if some dependencies are filtered out by tags. They just won't have those outputs.\\n+  const finalChecks: Record<string, import('../../types/config').CheckConfig> = {};\\n+  for (const [checkId, checkConfig] of Object.entries(filteredChecks)) {\\n+    const dependencies = checkConfig.depends_on || [];\\n+\\n+    // Only enforce dependency satisfaction when NO tag filter is active\\n+    // When tag filtering is active, allow checks to run with partial dependencies (soft dependencies)\\n+    if (dependencies.length > 0 && !tagFilter && !areDependenciesSatisfied(dependencies)) {\\n+      if (context.debug) {\\n+        logger.info(\\n+          `[PlanReady] Skipping check '${checkId}': unsatisfied dependencies ${JSON.stringify(dependencies)}`\\n+        );\\n+      }\\n+      continue;\\n+    }\\n+    finalChecks[checkId] = checkConfig;\\n+  }\\n+\\n+  if (context.debug && Object.keys(finalChecks).length !== Object.keys(filteredChecks).length) {\\n+    logger.info(\\n+      `[PlanReady] Removed ${Object.keys(filteredChecks).length - Object.keys(finalChecks).length} checks due to unsatisfied dependencies`\\n+    );\\n+  }\\n+\\n+  // Extract dependencies from final filtered check configurations\\n+  const checkDependencies: Record<string, string[]> = {};\\n+\\n+  for (const [checkId, checkConfig] of Object.entries(finalChecks)) {\\n+    // Expand OR groups (pipe syntax) for dependency resolution\\n+    // For OR groups, only include options that exist in finalChecks\\n+    // e.g., \\\"issue-assistant|comment-assistant\\\" becomes [\\\"comment-assistant\\\"] if issue-assistant was filtered out\\n+    // For regular dependencies, also filter to only include checks that exist (soft dependencies when tag filtering)\\n+    const dependencies = (checkConfig.depends_on || []).flatMap((d: string) => {\\n+      if (typeof d === 'string' && d.includes('|')) {\\n+        // OR dependency - filter to only include available checks\\n+        const orOptions = d\\n+          .split('|')\\n+          .map(s => s.trim())\\n+          .filter(Boolean)\\n+          .filter(opt => finalChecks[opt] !== undefined); // Only include if check exists\\n+        return orOptions;\\n+      } else {\\n+        // Regular dependency - when tag filtering is active, filter to only available checks (soft dependencies)\\n+        // When no tag filtering, include all dependencies (validation happens in graph builder)\\n+        if (tagFilter && finalChecks[d] === undefined) {\\n+          if (context.debug) {\\n+            logger.info(\\n+              `[PlanReady] Soft dependency '${d}' of check '${checkId}' filtered out by tags - check will run without it`\\n+            );\\n+          }\\n+          return []; // Filter out unavailable dependency\\n+        }\\n+        return [d];\\n+      }\\n+    });\\n+    checkDependencies[checkId] = dependencies;\\n+  }\\n+\\n+  // Build dependency graph\\n+  let graph;\\n+  try {\\n+    graph = DependencyResolver.buildDependencyGraph(checkDependencies);\\n+  } catch (error) {\\n+    const errorMsg = error instanceof Error ? error.message : String(error);\\n+    logger.error(`[PlanReady] Dependency validation failed: ${errorMsg}`);\\n+\\n+    // Record validation error as an issue\\n+    const validationIssue: any = {\\n+      file: 'system',\\n+      line: 0,\\n+      message: errorMsg,\\n+      category: 'logic',\\n+      severity: 'error',\\n+      ruleId: 'system/error',\\n+    };\\n+\\n+    // Record in journal as a system check\\n+    context.journal.commitEntry({\\n+      sessionId: context.sessionId,\\n+      scope: [],\\n+      checkId: 'system',\\n+      result: {\\n+        issues: [validationIssue],\\n+        output: undefined,\\n+      },\\n+    });\\n+\\n+    // Transition to Completed\\n+    state.currentState = 'Completed';\\n+    return;\\n+  }\\n+\\n+  // Validate graph - check for cycles\\n+  if (graph.hasCycles) {\\n+    const cycleNodes = graph.cycleNodes?.join(' -> ') || 'unknown';\\n+    const errorMsg = `Dependency cycle detected: ${cycleNodes}`;\\n+    logger.error(`[PlanReady] ${errorMsg}`);\\n+\\n+    // Record cycle error as an issue\\n+    const cycleIssue: any = {\\n+      file: 'system',\\n+      line: 0,\\n+      message: errorMsg,\\n+      category: 'logic',\\n+      severity: 'error',\\n+      ruleId: 'system/error',\\n+    };\\n+\\n+    // Record in journal as a system check\\n+    context.journal.commitEntry({\\n+      sessionId: context.sessionId,\\n+      scope: [],\\n+      checkId: 'system',\\n+      result: {\\n+        issues: [cycleIssue],\\n+        output: undefined,\\n+      },\\n+    });\\n+\\n+    // Transition to Completed\\n+    state.currentState = 'Completed';\\n+    return;\\n+  }\\n+\\n+  if (context.debug) {\\n+    logger.info(\\n+      `[PlanReady] Graph built with ${graph.nodes.size} checks, ${graph.executionOrder.length} levels`\\n+    );\\n+  }\\n+\\n+  // Store graph in context (mutate for now, can refactor later)\\n+  (context as any).dependencyGraph = graph;\\n+\\n+  // Initialize wave 0\\n+  state.wave = 0;\\n+\\n+  // Transition to WavePlanning\\n+  transition('WavePlanning');\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/state-machine/states/routing.ts\",\"additions\":42,\"deletions\":0,\"changes\":1510,\"patch\":\"diff --git a/src/state-machine/states/routing.ts b/src/state-machine/states/routing.ts\\nnew file mode 100644\\nindex 00000000..5752a5bb\\n--- /dev/null\\n+++ b/src/state-machine/states/routing.ts\\n@@ -0,0 +1,1510 @@\\n+/**\\n+ * Routing State Handler\\n+ *\\n+ * Responsibilities:\\n+ * - Evaluate fail_if conditions after check execution\\n+ * - Process on_success, on_fail, on_finish triggers\\n+ * - Enqueue ForwardRunRequested events for goto\\n+ * - Enqueue WaveRetry events for routing loops\\n+ * - Transition back to WavePlanning or Completed\\n+ *\\n+ * M2: Core routing logic implementation\\n+ */\\n+\\n+import type { EngineContext, RunState, EngineState, EngineEvent } from '../../types/engine';\\n+import type { ReviewSummary, ReviewIssue } from '../../reviewer';\\n+import type { CheckConfig, OnFailConfig, TransitionRule } from '../../types/config';\\n+import { logger } from '../../logger';\\n+import { FailureConditionEvaluator } from '../../failure-condition-evaluator';\\n+import { createSecureSandbox, compileAndRun } from '../../utils/sandbox';\\n+import { MemoryStore } from '../../memory-store';\\n+\\n+/**\\n+ * Check if any configured check depends (directly or via OR dependency) on the\\n+ * given `checkId`. Used to decide whether a forEach parent has downstream\\n+ * dependents, in which case its on_finish should defer to the LevelDispatch\\n+ * post-children hook.\\n+ */\\n+// hasDependents helper removed (unused)\\n+\\n+/**\\n+ * Check if any dependent of `checkId` would execute with map fanout.\\n+ * Used to decide whether a forEach parent's on_finish should be deferred to\\n+ * the LevelDispatch post-children hook (only necessary when map-fanout children\\n+ * will run per item).\\n+ */\\n+function hasMapFanoutDependents(context: EngineContext, checkId: string): boolean {\\n+  const checks = context.config.checks || {};\\n+  const reduceProviders = new Set(['log', 'memory', 'script', 'workflow', 'noop']);\\n+\\n+  for (const [cid, cfg] of Object.entries(checks)) {\\n+    if (cid === checkId) continue;\\n+    const rawDeps = (cfg as any).depends_on || [];\\n+    const depList = Array.isArray(rawDeps) ? rawDeps : [rawDeps];\\n+    // Does this check depend on our target?\\n+    let depends = false;\\n+    for (const dep of depList) {\\n+      if (typeof dep !== 'string') continue;\\n+      if (dep.includes('|')) {\\n+        const opts = dep\\n+          .split('|')\\n+          .map(s => s.trim())\\n+          .filter(Boolean);\\n+        if (opts.includes(checkId)) {\\n+          depends = true;\\n+          break;\\n+        }\\n+      } else if (dep === checkId) {\\n+        depends = true;\\n+        break;\\n+      }\\n+    }\\n+    if (!depends) continue;\\n+\\n+    // Determine this dependent's fanout mode\\n+    const explicit = (cfg as any).fanout as 'map' | 'reduce' | undefined;\\n+    if (explicit === 'map') return true;\\n+    if (explicit === 'reduce') continue;\\n+\\n+    // Infer default based on provider type\\n+    const providerType = context.checks[cid]?.providerType || (checks as any)[cid]?.type || '';\\n+    const inferred: 'map' | 'reduce' = reduceProviders.has(providerType) ? 'reduce' : 'map';\\n+    if (inferred === 'map') return true;\\n+  }\\n+  return false;\\n+}\\n+\\n+/** Classify failure type to inform retry policy mapping */\\n+function classifyFailure(result: ReviewSummary): 'none' | 'logical' | 'execution' {\\n+  const issues = result?.issues || [];\\n+  if (!issues || issues.length === 0) return 'none';\\n+  // Heuristics:\\n+  // - logical: fail_if, contract/guarantee_failed, explicit ruleIds ending with _fail_if\\n+  // - execution: provider/command errors, forEach/execution_error, sandbox_runner_error\\n+  let hasLogical = false;\\n+  let hasExecution = false;\\n+  for (const iss of issues) {\\n+    const id = String((iss as any).ruleId || '');\\n+    const msg = String((iss as any).message || '');\\n+    if (\\n+      id.endsWith('_fail_if') ||\\n+      id.includes('contract/guarantee_failed') ||\\n+      id.includes('contract/schema_validation_failed')\\n+    )\\n+      hasLogical = true;\\n+    if (id.includes('/execution_error') || msg.includes('Command execution failed'))\\n+      hasExecution = true;\\n+    if (id.includes('forEach/execution_error') || msg.includes('sandbox_runner_error'))\\n+      hasExecution = true;\\n+  }\\n+  if (hasLogical && !hasExecution) return 'logical';\\n+  if (hasExecution && !hasLogical) return 'execution';\\n+  // Mixed or unknown: treat as execution to avoid suppressing retries unexpectedly\\n+  return hasExecution ? 'execution' : 'logical';\\n+}\\n+\\n+function getCriticality(context: EngineContext, checkId: string): CheckConfig['criticality'] {\\n+  const cfg = context.config.checks?.[checkId];\\n+  return (cfg && (cfg as any).criticality) || 'policy';\\n+}\\n+\\n+/**\\n+ * Context for a check that just completed and needs routing evaluation\\n+ */\\n+interface RoutingContext {\\n+  checkId: string;\\n+  scope: Array<{ check: string; index: number }>;\\n+  result: ReviewSummary;\\n+  checkConfig: CheckConfig;\\n+  success: boolean; // true if no fatal issues\\n+}\\n+\\n+/**\\n+ * Create memory helpers for sandbox context\\n+ */\\n+function createMemoryHelpers() {\\n+  const memoryStore = MemoryStore.getInstance();\\n+  return {\\n+    get: (key: string, ns?: string) => memoryStore.get(key, ns),\\n+    has: (key: string, ns?: string) => memoryStore.has(key, ns),\\n+    getAll: (ns?: string) => memoryStore.getAll(ns),\\n+    set: (key: string, value: unknown, ns?: string) => {\\n+      const nsName = ns || memoryStore.getDefaultNamespace();\\n+      const data: Map<string, Map<string, unknown>> = (memoryStore as any)['data'];\\n+      if (!data.has(nsName)) data.set(nsName, new Map());\\n+      data.get(nsName)!.set(key, value);\\n+    },\\n+    clear: (ns?: string) => {\\n+      const data: Map<string, Map<string, unknown>> = (memoryStore as any)['data'];\\n+      if (ns) data.delete(ns);\\n+      else data.clear();\\n+    },\\n+    increment: (key: string, amount = 1, ns?: string) => {\\n+      const nsName = ns || memoryStore.getDefaultNamespace();\\n+      const data: Map<string, Map<string, unknown>> = (memoryStore as any)['data'];\\n+      if (!data.has(nsName)) data.set(nsName, new Map());\\n+      const nsMap = data.get(nsName)!;\\n+      const current = nsMap.get(key);\\n+      const numCurrent = typeof current === 'number' ? current : 0;\\n+      const newValue = numCurrent + amount;\\n+      nsMap.set(key, newValue);\\n+      return newValue;\\n+    },\\n+  };\\n+}\\n+\\n+/**\\n+ * Handle routing state - evaluate conditions and decide next actions\\n+ */\\n+export async function handleRouting(\\n+  context: EngineContext,\\n+  state: RunState,\\n+  transition: (newState: EngineState) => void,\\n+  emitEvent: (event: EngineEvent) => void,\\n+  routingContext: RoutingContext\\n+): Promise<void> {\\n+  const { checkId, scope, result, checkConfig, success } = routingContext;\\n+\\n+  // Always log routing entry for debugging E2E expectations\\n+  logger.info(`[Routing] Evaluating routing for check: ${checkId}, success: ${success}`);\\n+\\n+  // Step 1: Evaluate fail_if conditions\\n+  const failIfTriggered = await evaluateFailIf(checkId, result, checkConfig, context, state);\\n+\\n+  if (failIfTriggered) {\\n+    if (context.debug) {\\n+      logger.info(`[Routing] fail_if triggered for ${checkId}`);\\n+    }\\n+\\n+    // Treat as failure for routing purposes\\n+    await processOnFail(checkId, scope, result, checkConfig, context, state, emitEvent);\\n+  } else if (success) {\\n+    // Step 2: Process on_success routing\\n+    await processOnSuccess(checkId, scope, result, checkConfig, context, state, emitEvent);\\n+  } else {\\n+    // Step 3: Process on_fail routing\\n+    await processOnFail(checkId, scope, result, checkConfig, context, state, emitEvent);\\n+  }\\n+\\n+  // Step 4: on_finish\\n+  // Process on_finish here for:\\n+  //  - non-forEach checks\\n+  //  - forEach parents that do NOT have map-fanout dependents\\n+  //    (reduce-only dependents don't need the post-children barrier)\\n+  const shouldProcessOnFinishHere =\\n+    !!checkConfig.on_finish &&\\n+    (checkConfig.forEach !== true || !hasMapFanoutDependents(context, checkId));\\n+  if (checkConfig.on_finish) {\\n+    logger.info(\\n+      `[Routing] on_finish decision for ${checkId}: forEach=${!!checkConfig.forEach}, processHere=${shouldProcessOnFinishHere}`\\n+    );\\n+  }\\n+  if (shouldProcessOnFinishHere) {\\n+    await processOnFinish(checkId, scope, result, checkConfig, context, state, emitEvent);\\n+  }\\n+\\n+  // Transition back to WavePlanning to process queued events\\n+  transition('WavePlanning');\\n+}\\n+\\n+/**\\n+ * Process on_finish routing\\n+ */\\n+async function processOnFinish(\\n+  checkId: string,\\n+  scope: Array<{ check: string; index: number }>,\\n+  result: ReviewSummary,\\n+  checkConfig: CheckConfig,\\n+  context: EngineContext,\\n+  state: RunState,\\n+  emitEvent: (event: EngineEvent) => void\\n+): Promise<void> {\\n+  const onFinish = checkConfig.on_finish;\\n+\\n+  if (!onFinish) {\\n+    return; // No on_finish configuration\\n+  }\\n+\\n+  // Log at info level so it's visible in test output\\n+  logger.info(`Processing on_finish for ${checkId}`);\\n+  let queuedForward = false;\\n+  // Process on_finish.run\\n+  if (onFinish.run && onFinish.run.length > 0) {\\n+    // Check if current check is a forEach parent with items\\n+    const currentCheckIsForEach = checkConfig.forEach === true;\\n+    const forEachItems = currentCheckIsForEach ? (result as any).forEachItems : undefined;\\n+    const hasForEachItems = Array.isArray(forEachItems) && forEachItems.length > 0;\\n+\\n+    for (const targetCheck of onFinish.run) {\\n+      // Check loop budget before scheduling\\n+      if (checkLoopBudget(context, state, 'on_finish', 'run')) {\\n+        const errorIssue: ReviewIssue = {\\n+          file: 'system',\\n+          line: 0,\\n+          ruleId: `${checkId}/routing/loop_budget_exceeded`,\\n+          message: `Routing loop budget exceeded (max_loops=${context.config.routing?.max_loops ?? DEFAULT_MAX_LOOPS}) during on_finish run`,\\n+          severity: 'error',\\n+          category: 'logic',\\n+        };\\n+        result.issues = [...(result.issues || []), errorIssue];\\n+        return;\\n+      }\\n+\\n+      // Handle fanout: check if target has fanout configuration\\n+      const targetConfig = context.config.checks?.[targetCheck];\\n+      const fanoutMode = targetConfig?.fanout || 'reduce'; // default to reduce\\n+\\n+      if (context.debug) {\\n+        logger.info(\\n+          `[Routing] on_finish.run: scheduling ${targetCheck} with fanout=${fanoutMode}, hasForEachItems=${hasForEachItems}`\\n+        );\\n+      }\\n+\\n+      // If current check has forEach items and target is map fanout, emit one event per item\\n+      if (fanoutMode === 'map' && hasForEachItems) {\\n+        // Map fanout: emit one ForwardRunRequested per forEach item\\n+        for (let itemIndex = 0; itemIndex < forEachItems!.length; itemIndex++) {\\n+          // Increment loop count for each item\\n+          state.routingLoopCount++;\\n+\\n+          const itemScope: Array<{ check: string; index: number }> = [\\n+            { check: checkId, index: itemIndex },\\n+          ];\\n+\\n+          emitEvent({\\n+            type: 'ForwardRunRequested',\\n+            target: targetCheck,\\n+            scope: itemScope,\\n+            origin: 'run',\\n+          });\\n+          queuedForward = true;\\n+        }\\n+      } else {\\n+        // Reduce fanout (or no forEach context): emit with empty scope once\\n+        // Increment loop count\\n+        state.routingLoopCount++;\\n+\\n+        emitEvent({\\n+          type: 'ForwardRunRequested',\\n+          target: targetCheck,\\n+          scope: [],\\n+          origin: 'run',\\n+        });\\n+        queuedForward = true;\\n+      }\\n+    }\\n+  }\\n+\\n+  // Process on_finish.run_js\\n+  if (onFinish.run_js) {\\n+    const dynamicTargets = await evaluateRunJs(\\n+      onFinish.run_js,\\n+      checkId,\\n+      checkConfig,\\n+      result,\\n+      context,\\n+      state\\n+    );\\n+\\n+    for (const targetCheck of dynamicTargets) {\\n+      // Check loop budget before scheduling\\n+      if (checkLoopBudget(context, state, 'on_finish', 'run')) {\\n+        const errorIssue: ReviewIssue = {\\n+          file: 'system',\\n+          line: 0,\\n+          ruleId: `${checkId}/routing/loop_budget_exceeded`,\\n+          message: `Routing loop budget exceeded (max_loops=${context.config.routing?.max_loops ?? DEFAULT_MAX_LOOPS}) during on_finish run`,\\n+          severity: 'error',\\n+          category: 'logic',\\n+        };\\n+        result.issues = [...(result.issues || []), errorIssue];\\n+        return;\\n+      }\\n+\\n+      if (context.debug) {\\n+        logger.info(`[Routing] on_finish.run_js: scheduling ${targetCheck}`);\\n+      }\\n+\\n+      // Increment loop count\\n+      state.routingLoopCount++;\\n+\\n+      emitEvent({\\n+        type: 'ForwardRunRequested',\\n+        target: targetCheck,\\n+        scope,\\n+        origin: 'run_js',\\n+      });\\n+      queuedForward = true;\\n+    }\\n+  }\\n+\\n+  // Declarative transitions (override goto/goto_js when present)\\n+  const finishTransTarget = await evaluateTransitions(\\n+    onFinish.transitions,\\n+    checkId,\\n+    checkConfig,\\n+    result,\\n+    context,\\n+    state\\n+  );\\n+  if (finishTransTarget !== undefined) {\\n+    if (finishTransTarget) {\\n+      if (checkLoopBudget(context, state, 'on_finish', 'goto')) {\\n+        const errorIssue: ReviewIssue = {\\n+          file: 'system',\\n+          line: 0,\\n+          ruleId: `${checkId}/routing/loop_budget_exceeded`,\\n+          message: `Routing loop budget exceeded (max_loops=${context.config.routing?.max_loops ?? DEFAULT_MAX_LOOPS}) during on_finish goto`,\\n+          severity: 'error',\\n+          category: 'logic',\\n+        };\\n+        result.issues = [...(result.issues || []), errorIssue];\\n+        return;\\n+      }\\n+      state.routingLoopCount++;\\n+      emitEvent({\\n+        type: 'ForwardRunRequested',\\n+        target: finishTransTarget.to,\\n+        scope,\\n+        origin: 'goto_js',\\n+        gotoEvent: finishTransTarget.goto_event,\\n+      });\\n+    }\\n+    return; // transitions override goto/goto_js\\n+  }\\n+\\n+  // Process on_finish.goto / goto_js\\n+  const gotoTarget = await evaluateGoto(\\n+    onFinish.goto_js,\\n+    onFinish.goto,\\n+    checkId,\\n+    checkConfig,\\n+    result,\\n+    context,\\n+    state\\n+  );\\n+\\n+  if (gotoTarget) {\\n+    // Check loop budget before scheduling goto\\n+    if (checkLoopBudget(context, state, 'on_finish', 'goto')) {\\n+      const errorIssue: ReviewIssue = {\\n+        file: 'system',\\n+        line: 0,\\n+        ruleId: `${checkId}/routing/loop_budget_exceeded`,\\n+        message: `Routing loop budget exceeded (max_loops=${context.config.routing?.max_loops ?? DEFAULT_MAX_LOOPS}) during on_finish goto`,\\n+        severity: 'error',\\n+        category: 'logic',\\n+      };\\n+      result.issues = [...(result.issues || []), errorIssue];\\n+      return;\\n+    }\\n+\\n+    if (context.debug) {\\n+      logger.info(`[Routing] on_finish.goto: ${gotoTarget}`);\\n+    }\\n+\\n+    // Increment loop count\\n+    state.routingLoopCount++;\\n+\\n+    // Enqueue forward run event\\n+    emitEvent({\\n+      type: 'ForwardRunRequested',\\n+      target: gotoTarget,\\n+      scope,\\n+      origin: 'goto_js',\\n+    });\\n+\\n+    // Mark that we've seen a forward run\\n+    state.flags.forwardRunRequested = true;\\n+  }\\n+\\n+  // If we scheduled any forward-run targets via on_finish, request a wave retry so\\n+  // dependent checks (with if conditions) can re-evaluate after the forward-run completes.\\n+  // Guard: only enqueue once per originating check per wave to avoid loops.\\n+  if (queuedForward) {\\n+    const guardKey = `waveRetry:on_finish:${checkId}:wave:${state.wave}`;\\n+    if (!(state as any).forwardRunGuards?.has(guardKey)) {\\n+      (state as any).forwardRunGuards?.add(guardKey);\\n+      emitEvent({ type: 'WaveRetry', reason: 'on_finish' });\\n+    }\\n+  }\\n+}\\n+\\n+/**\\n+ * Evaluate fail_if conditions for a check\\n+ */\\n+// Returns true only when a check-level fail_if is triggered.\\n+// Global fail_if records an issue for summary/reporting but MUST NOT gate routing.\\n+async function evaluateFailIf(\\n+  checkId: string,\\n+  result: ReviewSummary,\\n+  checkConfig: CheckConfig,\\n+  context: EngineContext,\\n+  state: RunState\\n+): Promise<boolean> {\\n+  const config = context.config;\\n+\\n+  // Check for fail_if at global or check level\\n+  const globalFailIf = config.fail_if;\\n+  const checkFailIf = checkConfig.fail_if;\\n+\\n+  if (!globalFailIf && !checkFailIf) {\\n+    return false; // No fail_if conditions\\n+  }\\n+\\n+  const evaluator = new FailureConditionEvaluator();\\n+\\n+  // Build outputs record from state\\n+  const outputsRecord: Record<string, ReviewSummary> = {};\\n+  for (const [key] of state.stats.entries()) {\\n+    // Try to get the actual result from context.journal if available\\n+    try {\\n+      const snapshotId = context.journal.beginSnapshot();\\n+      const contextView = new (require('../../snapshot-store').ContextView)(\\n+        context.journal,\\n+        context.sessionId,\\n+        snapshotId,\\n+        [],\\n+        context.event\\n+      );\\n+      const journalResult = contextView.get(key);\\n+      if (journalResult) {\\n+        outputsRecord[key] = journalResult as ReviewSummary;\\n+      }\\n+    } catch {\\n+      // Fallback to empty result\\n+      outputsRecord[key] = { issues: [] };\\n+    }\\n+  }\\n+\\n+  const checkSchema = typeof checkConfig.schema === 'object' ? 'custom' : checkConfig.schema || '';\\n+  const checkGroup = checkConfig.group || '';\\n+\\n+  // Evaluate global fail_if (non-gating)\\n+  if (globalFailIf) {\\n+    try {\\n+      const failed = await evaluator.evaluateSimpleCondition(\\n+        checkId,\\n+        checkSchema,\\n+        checkGroup,\\n+        result,\\n+        globalFailIf,\\n+        outputsRecord\\n+      );\\n+\\n+      if (failed) {\\n+        logger.warn(`[Routing] Global fail_if triggered for ${checkId}: ${globalFailIf}`);\\n+\\n+        // Add fail_if issue to result\\n+        const failIssue: ReviewIssue = {\\n+          file: 'system',\\n+          line: 0,\\n+          ruleId: 'global_fail_if',\\n+          message: `Global failure condition met: ${globalFailIf}`,\\n+          severity: 'error',\\n+          category: 'logic',\\n+        };\\n+\\n+        result.issues = [...(result.issues || []), failIssue];\\n+        // IMPORTANT: do not gate routing on global fail_if\\n+        // This condition contributes to overall run status but should not\\n+        // block dependents from executing when the producing check succeeded.\\n+        // Continue evaluating check-level fail_if below.\\n+      }\\n+    } catch (error) {\\n+      const msg = error instanceof Error ? error.message : String(error);\\n+      logger.error(`[Routing] Error evaluating global fail_if: ${msg}`);\\n+    }\\n+  }\\n+\\n+  // Evaluate check-specific fail_if\\n+  if (checkFailIf) {\\n+    try {\\n+      const failed = await evaluator.evaluateSimpleCondition(\\n+        checkId,\\n+        checkSchema,\\n+        checkGroup,\\n+        result,\\n+        checkFailIf,\\n+        outputsRecord\\n+      );\\n+\\n+      if (failed) {\\n+        logger.warn(`[Routing] Check fail_if triggered for ${checkId}: ${checkFailIf}`);\\n+\\n+        // Add fail_if issue to result\\n+        const failIssue: ReviewIssue = {\\n+          file: 'system',\\n+          line: 0,\\n+          ruleId: `${checkId}_fail_if`,\\n+          message: `Check failure condition met: ${checkFailIf}`,\\n+          severity: 'error',\\n+          category: 'logic',\\n+        };\\n+\\n+        result.issues = [...(result.issues || []), failIssue];\\n+        return true;\\n+      }\\n+    } catch (error) {\\n+      const msg = error instanceof Error ? error.message : String(error);\\n+      logger.error(`[Routing] Error evaluating check fail_if: ${msg}`);\\n+    }\\n+  }\\n+\\n+  return false;\\n+}\\n+\\n+/**\\n+ * Check if routing loop budget is exceeded\\n+ */\\n+export function checkLoopBudget(\\n+  context: EngineContext,\\n+  state: RunState,\\n+  origin: 'on_success' | 'on_fail' | 'on_finish',\\n+  action: 'run' | 'goto'\\n+): boolean {\\n+  const maxLoops = context.config.routing?.max_loops ?? DEFAULT_MAX_LOOPS;\\n+\\n+  if (state.routingLoopCount >= maxLoops) {\\n+    const msg = `Routing loop budget exceeded (max_loops=${maxLoops}) during ${origin} ${action}`;\\n+    logger.error(`[Routing] ${msg}`);\\n+    return true; // Budget exceeded\\n+  }\\n+\\n+  return false; // Budget OK\\n+}\\n+\\n+/**\\n+ * Process on_success routing\\n+ */\\n+async function processOnSuccess(\\n+  checkId: string,\\n+  scope: Array<{ check: string; index: number }>,\\n+  result: ReviewSummary,\\n+  checkConfig: CheckConfig,\\n+  context: EngineContext,\\n+  state: RunState,\\n+  emitEvent: (event: EngineEvent) => void\\n+): Promise<void> {\\n+  const onSuccess = checkConfig.on_success;\\n+\\n+  if (!onSuccess) {\\n+    return; // No on_success configuration\\n+  }\\n+\\n+  if (context.debug) {\\n+    logger.info(`[Routing] Processing on_success for ${checkId}`);\\n+  }\\n+\\n+  // Process on_success.run\\n+  if (onSuccess.run && onSuccess.run.length > 0) {\\n+    // Detect forEach context based on the actual result (aggregated map execution)\\n+    const resForEachItems: any[] | undefined =\\n+      (result && (result as any).forEachItems) || undefined;\\n+    const hasForEachItems = Array.isArray(resForEachItems) && resForEachItems.length > 0;\\n+\\n+    for (const targetCheck of onSuccess.run) {\\n+      // Check loop budget before scheduling\\n+      if (checkLoopBudget(context, state, 'on_success', 'run')) {\\n+        // Add error issue to result\\n+        const errorIssue: ReviewIssue = {\\n+          file: 'system',\\n+          line: 0,\\n+          ruleId: `${checkId}/routing/loop_budget_exceeded`,\\n+          message: `Routing loop budget exceeded (max_loops=${context.config.routing?.max_loops ?? DEFAULT_MAX_LOOPS}) during on_success run`,\\n+          severity: 'error',\\n+          category: 'logic',\\n+        };\\n+        result.issues = [...(result.issues || []), errorIssue];\\n+        return; // Stop processing\\n+      }\\n+\\n+      // Handle fanout: check if target has fanout configuration\\n+      const targetConfig = context.config.checks?.[targetCheck];\\n+      const fanoutMode = targetConfig?.fanout || 'reduce'; // default to reduce\\n+\\n+      if (context.debug) {\\n+        logger.info(\\n+          `[Routing] on_success.run: scheduling ${targetCheck} with fanout=${fanoutMode}, hasForEachItems=${hasForEachItems}`\\n+        );\\n+      }\\n+\\n+      // If current check has forEach items and target is map fanout, emit one event per item\\n+      if (fanoutMode === 'map' && hasForEachItems) {\\n+        // Map fanout: emit one ForwardRunRequested per forEach item\\n+        for (let itemIndex = 0; itemIndex < resForEachItems!.length; itemIndex++) {\\n+          // Increment loop count for each item\\n+          state.routingLoopCount++;\\n+\\n+          const itemScope: Array<{ check: string; index: number }> = [\\n+            { check: checkId, index: itemIndex },\\n+          ];\\n+\\n+          emitEvent({\\n+            type: 'ForwardRunRequested',\\n+            target: targetCheck,\\n+            scope: itemScope,\\n+            origin: 'run',\\n+          });\\n+        }\\n+      } else {\\n+        // Reduce fanout (or no forEach context): emit with empty scope once\\n+        // Increment loop count\\n+        state.routingLoopCount++;\\n+\\n+        emitEvent({\\n+          type: 'ForwardRunRequested',\\n+          target: targetCheck,\\n+          scope,\\n+          origin: 'run',\\n+        });\\n+      }\\n+    }\\n+  }\\n+\\n+  // Process on_success.run_js\\n+  if (onSuccess.run_js) {\\n+    const dynamicTargets = await evaluateRunJs(\\n+      onSuccess.run_js,\\n+      checkId,\\n+      checkConfig,\\n+      result,\\n+      context,\\n+      state\\n+    );\\n+\\n+    for (const targetCheck of dynamicTargets) {\\n+      // Check loop budget before scheduling\\n+      if (checkLoopBudget(context, state, 'on_success', 'run')) {\\n+        const errorIssue: ReviewIssue = {\\n+          file: 'system',\\n+          line: 0,\\n+          ruleId: `${checkId}/routing/loop_budget_exceeded`,\\n+          message: `Routing loop budget exceeded (max_loops=${context.config.routing?.max_loops ?? 10}) during on_success run`,\\n+          severity: 'error',\\n+          category: 'logic',\\n+        };\\n+        result.issues = [...(result.issues || []), errorIssue];\\n+        return;\\n+      }\\n+\\n+      if (context.debug) {\\n+        logger.info(`[Routing] on_success.run_js: scheduling ${targetCheck}`);\\n+      }\\n+\\n+      // Increment loop count\\n+      state.routingLoopCount++;\\n+\\n+      emitEvent({\\n+        type: 'ForwardRunRequested',\\n+        target: targetCheck,\\n+        scope,\\n+        origin: 'run_js',\\n+      });\\n+    }\\n+  }\\n+\\n+  // Declarative transitions for on_success (override goto/goto_js when present)\\n+  const successTransTarget = await evaluateTransitions(\\n+    onSuccess.transitions,\\n+    checkId,\\n+    checkConfig,\\n+    result,\\n+    context,\\n+    state\\n+  );\\n+  if (successTransTarget !== undefined) {\\n+    if (successTransTarget) {\\n+      if (checkLoopBudget(context, state, 'on_success', 'goto')) {\\n+        const errorIssue: ReviewIssue = {\\n+          file: 'system',\\n+          line: 0,\\n+          ruleId: `${checkId}/routing/loop_budget_exceeded`,\\n+          message: `Routing loop budget exceeded (max_loops=${context.config.routing?.max_loops ?? DEFAULT_MAX_LOOPS}) during on_success goto`,\\n+          severity: 'error',\\n+          category: 'logic',\\n+        };\\n+        result.issues = [...(result.issues || []), errorIssue];\\n+        return;\\n+      }\\n+      state.routingLoopCount++;\\n+      emitEvent({\\n+        type: 'ForwardRunRequested',\\n+        target: successTransTarget.to,\\n+        scope,\\n+        origin: 'goto_js',\\n+        gotoEvent: successTransTarget.goto_event,\\n+      });\\n+      state.flags.forwardRunRequested = true;\\n+    }\\n+    return;\\n+  }\\n+\\n+  // Process on_success.goto / goto_js\\n+  const gotoTarget = await evaluateGoto(\\n+    onSuccess.goto_js,\\n+    onSuccess.goto,\\n+    checkId,\\n+    checkConfig,\\n+    result,\\n+    context,\\n+    state\\n+  );\\n+\\n+  if (gotoTarget) {\\n+    // Check loop budget before scheduling goto\\n+    if (checkLoopBudget(context, state, 'on_success', 'goto')) {\\n+      const errorIssue: ReviewIssue = {\\n+        file: 'system',\\n+        line: 0,\\n+        ruleId: `${checkId}/routing/loop_budget_exceeded`,\\n+        message: `Routing loop budget exceeded (max_loops=${context.config.routing?.max_loops ?? 10}) during on_success goto`,\\n+        severity: 'error',\\n+        category: 'logic',\\n+      };\\n+      result.issues = [...(result.issues || []), errorIssue];\\n+      return;\\n+    }\\n+\\n+    if (context.debug) {\\n+      logger.info(`[Routing] on_success.goto: ${gotoTarget}`);\\n+    }\\n+\\n+    // Increment loop count\\n+    state.routingLoopCount++;\\n+\\n+    // Enqueue forward run event with optional event override\\n+    emitEvent({\\n+      type: 'ForwardRunRequested',\\n+      target: gotoTarget,\\n+      gotoEvent: onSuccess.goto_event,\\n+      scope,\\n+      origin: 'goto_js',\\n+    });\\n+\\n+    // Mark that we've seen a forward run\\n+    state.flags.forwardRunRequested = true;\\n+  }\\n+}\\n+\\n+/**\\n+ * Process on_fail routing\\n+ */\\n+async function processOnFail(\\n+  checkId: string,\\n+  scope: Array<{ check: string; index: number }>,\\n+  result: ReviewSummary,\\n+  checkConfig: CheckConfig,\\n+  context: EngineContext,\\n+  state: RunState,\\n+  emitEvent: (event: EngineEvent) => void\\n+): Promise<void> {\\n+  // Merge defaults with check-specific on_fail\\n+  const defaults = context.config.routing?.defaults?.on_fail || {};\\n+  const onFail: OnFailConfig | undefined = checkConfig.on_fail\\n+    ? { ...defaults, ...checkConfig.on_fail }\\n+    : undefined;\\n+\\n+  if (!onFail) {\\n+    return; // No on_fail configuration\\n+  }\\n+\\n+  if (context.debug) {\\n+    logger.info(`[Routing] Processing on_fail for ${checkId}`);\\n+  }\\n+\\n+  // Process on_fail.run\\n+  if (onFail.run && onFail.run.length > 0) {\\n+    // Detect forEach context based on the actual aggregated result\\n+    const resForEachItems: any[] | undefined =\\n+      (result && (result as any).forEachItems) || undefined;\\n+    const hasForEachItems = Array.isArray(resForEachItems) && resForEachItems.length > 0;\\n+\\n+    for (const targetCheck of onFail.run) {\\n+      // Check loop budget before scheduling\\n+      if (checkLoopBudget(context, state, 'on_fail', 'run')) {\\n+        const errorIssue: ReviewIssue = {\\n+          file: 'system',\\n+          line: 0,\\n+          ruleId: `${checkId}/routing/loop_budget_exceeded`,\\n+          message: `Routing loop budget exceeded (max_loops=${context.config.routing?.max_loops ?? 10}) during on_fail run`,\\n+          severity: 'error',\\n+          category: 'logic',\\n+        };\\n+        result.issues = [...(result.issues || []), errorIssue];\\n+        return;\\n+      }\\n+\\n+      // Handle fanout: check if target has fanout configuration\\n+      const targetConfig = context.config.checks?.[targetCheck];\\n+      const fanoutMode = targetConfig?.fanout || 'reduce'; // default to reduce\\n+\\n+      if (context.debug) {\\n+        logger.info(\\n+          `[Routing] on_fail.run: scheduling ${targetCheck} with fanout=${fanoutMode}, hasForEachItems=${hasForEachItems}`\\n+        );\\n+      }\\n+\\n+      // If current check ran in forEach context, schedule remediation per item\\n+      if (hasForEachItems) {\\n+        for (let itemIndex = 0; itemIndex < resForEachItems!.length; itemIndex++) {\\n+          const itemOut = resForEachItems![itemIndex] as any;\\n+          // Only remediate failed iterations if __failed is present; otherwise run for all\\n+          if (\\n+            itemOut &&\\n+            typeof itemOut === 'object' &&\\n+            itemOut.__failed !== true &&\\n+            fanoutMode !== 'map'\\n+          ) {\\n+            // For reduce targets, skip successful iterations to avoid redundant runs\\n+            continue;\\n+          }\\n+\\n+          state.routingLoopCount++;\\n+          const itemScope: Array<{ check: string; index: number }> = [\\n+            { check: checkId, index: itemIndex },\\n+          ];\\n+          emitEvent({\\n+            type: 'ForwardRunRequested',\\n+            target: targetCheck,\\n+            scope: itemScope,\\n+            origin: 'run',\\n+          });\\n+        }\\n+      } else {\\n+        // No forEach context: preserve current scope (if any)\\n+        state.routingLoopCount++;\\n+        emitEvent({\\n+          type: 'ForwardRunRequested',\\n+          target: targetCheck,\\n+          scope,\\n+          origin: 'run',\\n+        });\\n+      }\\n+    }\\n+  }\\n+\\n+  // Process on_fail.run_js\\n+  if (onFail.run_js) {\\n+    const dynamicTargets = await evaluateRunJs(\\n+      onFail.run_js,\\n+      checkId,\\n+      checkConfig,\\n+      result,\\n+      context,\\n+      state\\n+    );\\n+\\n+    for (const targetCheck of dynamicTargets) {\\n+      // Check loop budget before scheduling\\n+      if (checkLoopBudget(context, state, 'on_fail', 'run')) {\\n+        const errorIssue: ReviewIssue = {\\n+          file: 'system',\\n+          line: 0,\\n+          ruleId: `${checkId}/routing/loop_budget_exceeded`,\\n+          message: `Routing loop budget exceeded (max_loops=${context.config.routing?.max_loops ?? 10}) during on_fail run`,\\n+          severity: 'error',\\n+          category: 'logic',\\n+        };\\n+        result.issues = [...(result.issues || []), errorIssue];\\n+        return;\\n+      }\\n+\\n+      if (context.debug) {\\n+        logger.info(`[Routing] on_fail.run_js: scheduling ${targetCheck}`);\\n+      }\\n+\\n+      // Increment loop count\\n+      state.routingLoopCount++;\\n+\\n+      emitEvent({\\n+        type: 'ForwardRunRequested',\\n+        target: targetCheck,\\n+        scope,\\n+        origin: 'run_js',\\n+      });\\n+    }\\n+  }\\n+\\n+  // Process on_fail.retry (schedule retry of the current check)\\n+  if (onFail.retry && typeof onFail.retry.max === 'number' && onFail.retry.max > 0) {\\n+    // Criticality mapping: for 'external' and 'internal', avoid automatic\\n+    // retries for logical failures (fail_if/guarantee violations).\\n+    const crit = getCriticality(context, checkId);\\n+    const failureKind = classifyFailure(result);\\n+    if ((crit === 'external' || crit === 'internal') && failureKind === 'logical') {\\n+      if (context.debug) {\\n+        logger.info(\\n+          `[Routing] on_fail.retry suppressed for ${checkId} (criticality=${crit}, failure=logical)`\\n+        );\\n+      }\\n+      // Skip retry scheduling\\n+    } else {\\n+      const max = Math.max(0, onFail.retry.max || 0);\\n+      // Initialize retry attempt map on state\\n+      if (!(state as any).retryAttempts) (state as any).retryAttempts = new Map<string, number>();\\n+      const attemptsMap: Map<string, number> = (state as any).retryAttempts;\\n+\\n+      const makeKey = (sc: Array<{ check: string; index: number }> | undefined) => {\\n+        const keyScope = sc && sc.length > 0 ? JSON.stringify(sc) : 'root';\\n+        return `${checkId}::${keyScope}`;\\n+      };\\n+\\n+      const scheduleRetryForScope = (sc: Array<{ check: string; index: number }> | undefined) => {\\n+        const key = makeKey(sc);\\n+        const used = attemptsMap.get(key) || 0;\\n+        if (used >= max) return; // budget exhausted\\n+        attemptsMap.set(key, used + 1);\\n+\\n+        // Increment loop count and schedule forward run for the same check\\n+        state.routingLoopCount++;\\n+        emitEvent({\\n+          type: 'ForwardRunRequested',\\n+          target: checkId,\\n+          scope: sc || [],\\n+          origin: 'run',\\n+        });\\n+      };\\n+\\n+      const resForEachItems: any[] | undefined =\\n+        (result && (result as any).forEachItems) || undefined;\\n+      const hasForEachItems = Array.isArray(resForEachItems) && resForEachItems.length > 0;\\n+\\n+      if (hasForEachItems) {\\n+        for (let i = 0; i < resForEachItems!.length; i++) {\\n+          const itemOut = resForEachItems![i] as any;\\n+          // Only retry failed iterations (marked by __failed)\\n+          if (itemOut && typeof itemOut === 'object' && itemOut.__failed === true) {\\n+            const sc: Array<{ check: string; index: number }> = [{ check: checkId, index: i }];\\n+            scheduleRetryForScope(sc);\\n+          }\\n+        }\\n+      } else {\\n+        scheduleRetryForScope(scope);\\n+      }\\n+\\n+      // Note: backoff.delay_ms and mode are intentionally not awaited here; the\\n+      // state-machine processes retries as subsequent waves. If needed later, we\\n+      // can insert a timed wait in the orchestrator layer.\\n+    }\\n+  }\\n+\\n+  // Declarative transitions for on_fail (override goto/goto_js when present)\\n+  const failTransTarget = await evaluateTransitions(\\n+    onFail.transitions,\\n+    checkId,\\n+    checkConfig,\\n+    result,\\n+    context,\\n+    state\\n+  );\\n+  if (failTransTarget !== undefined) {\\n+    if (failTransTarget) {\\n+      if (checkLoopBudget(context, state, 'on_fail', 'goto')) {\\n+        const errorIssue: ReviewIssue = {\\n+          file: 'system',\\n+          line: 0,\\n+          ruleId: `${checkId}/routing/loop_budget_exceeded`,\\n+          message: `Routing loop budget exceeded (max_loops=${context.config.routing?.max_loops ?? DEFAULT_MAX_LOOPS}) during on_fail goto`,\\n+          severity: 'error',\\n+          category: 'logic',\\n+        };\\n+        result.issues = [...(result.issues || []), errorIssue];\\n+        return;\\n+      }\\n+      state.routingLoopCount++;\\n+      emitEvent({\\n+        type: 'ForwardRunRequested',\\n+        target: failTransTarget.to,\\n+        scope,\\n+        origin: 'goto_js',\\n+        gotoEvent: failTransTarget.goto_event,\\n+      });\\n+      state.flags.forwardRunRequested = true;\\n+    }\\n+    return;\\n+  }\\n+\\n+  // Process on_fail.goto / goto_js\\n+  const gotoTarget = await evaluateGoto(\\n+    onFail.goto_js,\\n+    onFail.goto,\\n+    checkId,\\n+    checkConfig,\\n+    result,\\n+    context,\\n+    state\\n+  );\\n+\\n+  if (gotoTarget) {\\n+    // Check loop budget before scheduling goto\\n+    if (checkLoopBudget(context, state, 'on_fail', 'goto')) {\\n+      const errorIssue: ReviewIssue = {\\n+        file: 'system',\\n+        line: 0,\\n+        ruleId: `${checkId}/routing/loop_budget_exceeded`,\\n+        message: `Routing loop budget exceeded (max_loops=${context.config.routing?.max_loops ?? 10}) during on_fail goto`,\\n+        severity: 'error',\\n+        category: 'logic',\\n+      };\\n+      result.issues = [...(result.issues || []), errorIssue];\\n+      return;\\n+    }\\n+\\n+    if (context.debug) {\\n+      logger.info(`[Routing] on_fail.goto: ${gotoTarget}`);\\n+    }\\n+\\n+    // Increment loop count\\n+    state.routingLoopCount++;\\n+\\n+    // Enqueue forward run event with optional event override\\n+    emitEvent({\\n+      type: 'ForwardRunRequested',\\n+      target: gotoTarget,\\n+      gotoEvent: onFail.goto_event,\\n+      scope,\\n+      origin: 'goto_js',\\n+    });\\n+\\n+    // Mark that we've seen a forward run\\n+    state.flags.forwardRunRequested = true;\\n+  }\\n+}\\n+\\n+/**\\n+ * Evaluate run_js expression to get dynamic check targets\\n+ */\\n+async function evaluateRunJs(\\n+  runJs: string,\\n+  checkId: string,\\n+  checkConfig: CheckConfig,\\n+  result: ReviewSummary,\\n+  context: EngineContext,\\n+  _state: RunState\\n+): Promise<string[]> {\\n+  try {\\n+    const sandbox = createSecureSandbox();\\n+\\n+    // Build outputs record and outputs_history\\n+    const snapshotId = context.journal.beginSnapshot();\\n+    const contextView = new (require('../../snapshot-store').ContextView)(\\n+      context.journal,\\n+      context.sessionId,\\n+      snapshotId,\\n+      [],\\n+      context.event\\n+    );\\n+\\n+    const outputsRecord: Record<string, any> = {};\\n+    const outputsHistory: Record<string, any[]> = {};\\n+\\n+    // Get all visible journal entries to build complete history\\n+    const allEntries = context.journal.readVisible(context.sessionId, snapshotId, context.event);\\n+    const uniqueCheckIds = new Set(allEntries.map(e => e.checkId));\\n+\\n+    for (const checkIdFromJournal of uniqueCheckIds) {\\n+      try {\\n+        // Get current output for this check\\n+        const journalResult = contextView.get(checkIdFromJournal);\\n+        if (journalResult) {\\n+          // Prefer the output field if present, otherwise use the full result\\n+          outputsRecord[checkIdFromJournal] =\\n+            journalResult.output !== undefined ? journalResult.output : journalResult;\\n+        }\\n+      } catch {\\n+        outputsRecord[checkIdFromJournal] = { issues: [] };\\n+      }\\n+\\n+      // Build history for this check\\n+      try {\\n+        const history = contextView.getHistory(checkIdFromJournal);\\n+        if (history && history.length > 0) {\\n+          // Extract outputs from history (prefer output field if available)\\n+          outputsHistory[checkIdFromJournal] = history.map((r: any) =>\\n+            r.output !== undefined ? r.output : r\\n+          );\\n+        }\\n+      } catch {\\n+        // Ignore history errors\\n+      }\\n+    }\\n+\\n+    // Add history as a property on outputs object for convenient access\\n+    outputsRecord.history = outputsHistory;\\n+\\n+    // Compute minimal forEach metadata for run_js parity\\n+    let forEachMeta: any = undefined;\\n+    try {\\n+      const hist = outputsHistory[checkId] || [];\\n+      const lastArr = (hist as any[])\\n+        .slice()\\n+        .reverse()\\n+        .find((x: any) => Array.isArray(x));\\n+      if (checkConfig.forEach === true && Array.isArray(lastArr)) {\\n+        forEachMeta = {\\n+          is_parent: true,\\n+          last_wave_size: lastArr.length,\\n+          last_items: lastArr,\\n+        };\\n+      }\\n+    } catch {}\\n+\\n+    const scopeObj: any = {\\n+      step: {\\n+        id: checkId,\\n+        tags: checkConfig.tags || [],\\n+        group: checkConfig.group,\\n+      },\\n+      outputs: outputsRecord,\\n+      outputs_history: outputsHistory,\\n+      output: (result as any)?.output,\\n+      memory: createMemoryHelpers(),\\n+      event: {\\n+        name: context.event || 'manual',\\n+      },\\n+      forEach: forEachMeta,\\n+    };\\n+\\n+    const code = `\\n+      const step = scope.step;\\n+      const outputs = scope.outputs;\\n+      const outputs_history = scope.outputs_history;\\n+      const output = scope.output;\\n+      const memory = scope.memory;\\n+      const event = scope.event;\\n+      const forEach = scope.forEach;\\n+      const log = (...args) => console.log('üîç Debug:', ...args);\\n+      const __fn = () => {\\n+        ${runJs}\\n+      };\\n+      const __res = __fn();\\n+      return Array.isArray(__res) ? __res.filter(x => typeof x === 'string' && x) : [];\\n+    `;\\n+\\n+    try {\\n+      const evalResult = compileAndRun<string[]>(\\n+        sandbox,\\n+        code,\\n+        { scope: scopeObj },\\n+        { injectLog: false, wrapFunction: false }\\n+      );\\n+      return Array.isArray(evalResult) ? evalResult.filter(Boolean) : [];\\n+    } catch (_e) {\\n+      // Fallback to VM for modern syntax used inside run_js\\n+      try {\\n+        const vm = require('node:vm');\\n+        const context = vm.createContext({ scope: scopeObj, console: { log: () => {} } });\\n+        const src = `(() => { ${runJs}\\\\n })()`;\\n+        const val = new vm.Script(src).runInContext(context, { timeout: 100 });\\n+        return Array.isArray(val) ? val.filter((x: any) => typeof x === 'string' && x) : [];\\n+      } catch (_vmErr) {\\n+        return [];\\n+      }\\n+    }\\n+  } catch (error) {\\n+    const msg = error instanceof Error ? error.message : String(error);\\n+    logger.error(`[Routing] Error evaluating run_js: ${msg}`);\\n+    return [];\\n+  }\\n+}\\n+\\n+/**\\n+ * Evaluate goto_js or return static goto target\\n+ */\\n+export async function evaluateGoto(\\n+  gotoJs: string | undefined,\\n+  gotoStatic: string | undefined,\\n+  checkId: string,\\n+  checkConfig: CheckConfig,\\n+  result: ReviewSummary,\\n+  context: EngineContext,\\n+  _state: RunState\\n+): Promise<string | null> {\\n+  // Evaluate goto_js first\\n+  if (gotoJs) {\\n+    try {\\n+      const sandbox = createSecureSandbox();\\n+\\n+      // Build outputs record and outputs_history from the full session snapshot.\\n+      // Do not filter by event here ‚Äî on_finish (especially forEach post-children) may\\n+      // need to see results committed under different event triggers within the same run.\\n+      const snapshotId = context.journal.beginSnapshot();\\n+      const contextView = new (require('../../snapshot-store').ContextView)(\\n+        context.journal,\\n+        context.sessionId,\\n+        snapshotId,\\n+        [],\\n+        undefined\\n+      );\\n+\\n+      const outputsRecord: Record<string, any> = {};\\n+      const outputsHistory: Record<string, any[]> = {};\\n+\\n+      // Get all visible journal entries to build complete history\\n+      const allEntries = context.journal.readVisible(context.sessionId, snapshotId, undefined);\\n+      const uniqueCheckIds = new Set(allEntries.map(e => e.checkId));\\n+\\n+      for (const checkIdFromJournal of uniqueCheckIds) {\\n+        try {\\n+          // Get current output for this check\\n+          const journalResult = contextView.get(checkIdFromJournal);\\n+          if (journalResult) {\\n+            // Prefer the output field if present, otherwise use the full result\\n+            outputsRecord[checkIdFromJournal] =\\n+              journalResult.output !== undefined ? journalResult.output : journalResult;\\n+          }\\n+        } catch {\\n+          outputsRecord[checkIdFromJournal] = { issues: [] };\\n+        }\\n+\\n+        // Build history for this check\\n+        try {\\n+          const history = contextView.getHistory(checkIdFromJournal);\\n+          if (history && history.length > 0) {\\n+            // Extract outputs from history (prefer output field if available)\\n+            outputsHistory[checkIdFromJournal] = history.map((r: any) =>\\n+              r.output !== undefined ? r.output : r\\n+            );\\n+          }\\n+        } catch {\\n+          // Ignore history errors\\n+        }\\n+      }\\n+\\n+      // Add history as a property on outputs object for convenient access\\n+      outputsRecord.history = outputsHistory;\\n+\\n+      // Compute minimal forEach metadata for convenience in goto_js\\n+      // - last_wave_size: number of items in the latest root-scope array output\\n+      // - last_items: the latest array output itself (if available)\\n+      let forEachMeta: any = undefined;\\n+      try {\\n+        const hist = outputsHistory[checkId] || [];\\n+        const lastArr = (hist as any[])\\n+          .slice()\\n+          .reverse()\\n+          .find((x: any) => Array.isArray(x));\\n+        if (checkConfig.forEach === true && Array.isArray(lastArr)) {\\n+          forEachMeta = {\\n+            is_parent: true,\\n+            last_wave_size: lastArr.length,\\n+            last_items: lastArr,\\n+          };\\n+        }\\n+      } catch {}\\n+\\n+      const scopeObj: any = {\\n+        step: {\\n+          id: checkId,\\n+          tags: checkConfig.tags || [],\\n+          group: checkConfig.group,\\n+        },\\n+        outputs: outputsRecord,\\n+        outputs_history: outputsHistory,\\n+        output: (result as any)?.output,\\n+        memory: createMemoryHelpers(),\\n+        event: {\\n+          name: context.event || 'manual',\\n+        },\\n+        forEach: forEachMeta,\\n+      };\\n+\\n+      // Debug: Log outputs_history\\n+      if (context.debug) {\\n+        logger.info(\\n+          `[Routing] evaluateGoto: checkId=${checkId}, outputs_history keys=${Object.keys(outputsHistory).join(',')}`\\n+        );\\n+        for (const [key, values] of Object.entries(outputsHistory)) {\\n+          logger.info(`[Routing]   ${key}: ${values.length} items`);\\n+        }\\n+      }\\n+\\n+      const code = `\\n+        const step = scope.step;\\n+        const outputs = scope.outputs;\\n+        const outputs_history = scope.outputs_history;\\n+        const output = scope.output;\\n+        const memory = scope.memory;\\n+        const event = scope.event;\\n+        const forEach = scope.forEach;\\n+        const log = (...args) => console.log('üîç Debug:', ...args);\\n+        ${gotoJs}\\n+      `;\\n+\\n+      try {\\n+        const evalResult = compileAndRun<string | null>(\\n+          sandbox,\\n+          code,\\n+          { scope: scopeObj },\\n+          { injectLog: false, wrapFunction: true }\\n+        );\\n+\\n+        if (context.debug) {\\n+          logger.info(`[Routing] evaluateGoto result: ${evalResult}`);\\n+        }\\n+\\n+        if (typeof evalResult === 'string' && evalResult) {\\n+          return evalResult;\\n+        }\\n+      } catch (_e) {\\n+        // Fallback to VM for modern syntax in goto_js\\n+        try {\\n+          const vm = require('node:vm');\\n+          const contextObj = {\\n+            step: scopeObj.step,\\n+            outputs: scopeObj.outputs,\\n+            outputs_history: scopeObj.outputs_history,\\n+            output: scopeObj.output,\\n+            memory: scopeObj.memory,\\n+            event: scopeObj.event,\\n+            forEach: (scopeObj as any).forEach,\\n+          };\\n+          const vmctx = vm.createContext(contextObj);\\n+          const src = `(() => { ${gotoJs}\\\\n })()`;\\n+          const res = new vm.Script(src).runInContext(vmctx, { timeout: 100 });\\n+          if (typeof res === 'string' && res) return res;\\n+        } catch (_vmErr) {\\n+          // ignore; fall through to static goto\\n+        }\\n+      }\\n+    } catch (error) {\\n+      const msg = error instanceof Error ? error.message : String(error);\\n+      logger.error(`[Routing] Error evaluating goto_js: ${msg}`);\\n+\\n+      // Fall back to static goto if available\\n+      if (gotoStatic) {\\n+        logger.info(`[Routing] Falling back to static goto: ${gotoStatic}`);\\n+        return gotoStatic;\\n+      }\\n+    }\\n+  }\\n+\\n+  // Return static goto\\n+  return gotoStatic || null;\\n+}\\n+// Default values (used only when config is absent)\\n+const DEFAULT_MAX_LOOPS = 10;\\n+\\n+/**\\n+ * Evaluate declarative transitions. Returns:\\n+ *  - { to, goto_event } when a rule matches,\\n+ *  - null (explicit) when a rule matches with to=null,\\n+ *  - undefined when no rule matched or transitions is empty.\\n+ */\\n+export async function evaluateTransitions(\\n+  transitions: TransitionRule[] | undefined,\\n+  checkId: string,\\n+  checkConfig: CheckConfig,\\n+  result: ReviewSummary,\\n+  context: EngineContext,\\n+  _state: RunState\\n+): Promise<\\n+  { to: string; goto_event?: import('../../types/config').EventTrigger } | null | undefined\\n+> {\\n+  if (!transitions || transitions.length === 0) return undefined;\\n+  try {\\n+    const sandbox = createSecureSandbox();\\n+\\n+    // Build outputs record and outputs_history from the full session snapshot\\n+    const snapshotId = context.journal.beginSnapshot();\\n+    const ContextView = (require('../../snapshot-store') as any).ContextView;\\n+    const view = new ContextView(context.journal, context.sessionId, snapshotId, [], undefined);\\n+\\n+    const outputsRecord: Record<string, any> = {};\\n+    const outputsHistory: Record<string, any[]> = {};\\n+    const allEntries = context.journal.readVisible(context.sessionId, snapshotId, undefined);\\n+    const uniqueCheckIds = new Set(allEntries.map((e: any) => e.checkId));\\n+    for (const cid of uniqueCheckIds) {\\n+      try {\\n+        const jr = view.get(cid);\\n+        if (jr) outputsRecord[cid] = jr.output !== undefined ? jr.output : jr;\\n+      } catch {}\\n+      try {\\n+        const hist = view.getHistory(cid);\\n+        if (hist && hist.length > 0) {\\n+          outputsHistory[cid] = hist.map((r: any) => (r.output !== undefined ? r.output : r));\\n+        }\\n+      } catch {}\\n+    }\\n+    outputsRecord.history = outputsHistory;\\n+\\n+    const scopeObj: any = {\\n+      step: { id: checkId, tags: checkConfig.tags || [], group: checkConfig.group },\\n+      outputs: outputsRecord,\\n+      outputs_history: outputsHistory,\\n+      output: (result as any)?.output,\\n+      memory: createMemoryHelpers(),\\n+      event: { name: context.event || 'manual' },\\n+    };\\n+\\n+    for (const rule of transitions) {\\n+      const helpers = `\\n+        const any = (arr, pred) => Array.isArray(arr) && arr.some(x => pred(x));\\n+        const all = (arr, pred) => Array.isArray(arr) && arr.every(x => pred(x));\\n+        const none = (arr, pred) => Array.isArray(arr) && !arr.some(x => pred(x));\\n+        const count = (arr, pred) => Array.isArray(arr) ? arr.filter(x => pred(x)).length : 0;\\n+      `;\\n+      // Mirror the variable exposure pattern used in goto_js/run_js so that\\n+      // `when:` expressions can reference `outputs`, `outputs_history`, `event`, etc.\\n+      const code = `\\n+        ${helpers}\\n+        const step = scope.step;\\n+        const outputs = scope.outputs;\\n+        const outputs_history = scope.outputs_history;\\n+        const output = scope.output;\\n+        const memory = scope.memory;\\n+        const event = scope.event;\\n+        const __eval = () => { return (${rule.when}); };\\n+        return __eval();\\n+      `;\\n+      let matched: boolean | undefined;\\n+      try {\\n+        matched = compileAndRun<boolean>(\\n+          sandbox,\\n+          code,\\n+          { scope: scopeObj },\\n+          { injectLog: false, wrapFunction: false }\\n+        );\\n+      } catch (_e) {\\n+        // Fallback: Node VM for modern syntax like optional chaining and ??\\n+        try {\\n+          const vm = require('node:vm');\\n+          const helpersFns = {\\n+            any: (arr: any[], pred: (x: any) => boolean) => Array.isArray(arr) && arr.some(pred),\\n+            all: (arr: any[], pred: (x: any) => boolean) => Array.isArray(arr) && arr.every(pred),\\n+            none: (arr: any[], pred: (x: any) => boolean) => Array.isArray(arr) && !arr.some(pred),\\n+            count: (arr: any[], pred: (x: any) => boolean) =>\\n+              Array.isArray(arr) ? arr.filter(pred).length : 0,\\n+          };\\n+          const context = vm.createContext({\\n+            step: scopeObj.step,\\n+            outputs: scopeObj.outputs,\\n+            outputs_history: scopeObj.outputs_history,\\n+            output: scopeObj.output,\\n+            memory: scopeObj.memory,\\n+            event: scopeObj.event,\\n+            ...helpersFns,\\n+          });\\n+          const res = new vm.Script(`(${rule.when})`).runInContext(context, { timeout: 50 });\\n+          matched = !!res;\\n+        } catch (_vmErr) {\\n+          matched = false;\\n+        }\\n+      }\\n+      if (matched\\n\\n... [TRUNCATED: Diff too large (50.4KB), showing first 50KB] ...\",\"status\":\"added\"},{\"filename\":\"src/state-machine/states/wave-planning.ts\",\"additions\":15,\"deletions\":0,\"changes\":511,\"patch\":\"diff --git a/src/state-machine/states/wave-planning.ts b/src/state-machine/states/wave-planning.ts\\nnew file mode 100644\\nindex 00000000..d22e7628\\n--- /dev/null\\n+++ b/src/state-machine/states/wave-planning.ts\\n@@ -0,0 +1,511 @@\\n+/**\\n+ * WavePlanning State Handler\\n+ *\\n+ * Responsibilities:\\n+ * - Inspect event queue for forward runs, goto events, etc.\\n+ * - Determine next wave's execution levels\\n+ * - Queue topological levels for execution\\n+ * - Transition to LevelDispatch or Completed\\n+ *\\n+ * M2: Adds goto/on_finish/on_fail routing with deduplication\\n+ */\\n+\\n+import type { EngineContext, RunState, EngineState, EngineEvent } from '../../types/engine';\\n+import { logger } from '../../logger';\\n+import { DependencyResolver } from '../../dependency-resolver';\\n+\\n+export async function handleWavePlanning(\\n+  context: EngineContext,\\n+  state: RunState,\\n+  transition: (newState: EngineState) => void\\n+): Promise<void> {\\n+  if (context.debug) {\\n+    logger.info(`[WavePlanning] Planning wave ${state.wave}...`);\\n+  }\\n+\\n+  // Reset forward-run active flag at the beginning of each planning cycle.\\n+  // It will be set to true only when we schedule a wave spawned by a forward-run request.\\n+  try {\\n+    (state as any).flags = (state as any).flags || {};\\n+    (state as any).flags.forwardRunActive = false;\\n+  } catch {}\\n+\\n+  // Check if we have a dependency graph\\n+  if (!context.dependencyGraph) {\\n+    throw new Error('Dependency graph not available');\\n+  }\\n+\\n+  // M3: Process bubbled events from child workflows\\n+  const bubbledEvents = (context as any)._bubbledEvents || [];\\n+  if (bubbledEvents.length > 0) {\\n+    if (context.debug) {\\n+      logger.info(\\n+        `[WavePlanning] Processing ${bubbledEvents.length} bubbled events from child workflows`\\n+      );\\n+    }\\n+\\n+    // Merge bubbled events into our event queue\\n+    for (const event of bubbledEvents) {\\n+      state.eventQueue.push(event);\\n+    }\\n+\\n+    // Clear bubbled events\\n+    (context as any)._bubbledEvents = [];\\n+  }\\n+\\n+  // M2: Process event queue for forward run requests\\n+  // IMPORTANT: Only process forward run requests if the current wave's levelQueue is empty\\n+  // This ensures we complete all scheduled checks before processing routing events\\n+  const forwardRunRequests = state.eventQueue.filter(\\n+    e => e.type === 'ForwardRunRequested'\\n+  ) as Array<Extract<EngineEvent, { type: 'ForwardRunRequested' }>>;\\n+\\n+  // Process forward-run requests.\\n+  // - GOTO-originated requests (goto/goto_js) preempt remaining work to jump back.\\n+  // - RUN-originated requests (run/run_js) are processed after the current wave drains,\\n+  //   so dependents in this wave (e.g., validate-fact after extract-facts) still run first.\\n+  if (\\n+    forwardRunRequests.length > 0 &&\\n+    (state.levelQueue.length === 0 ||\\n+      forwardRunRequests.some(r => r.origin === 'goto' || r.origin === 'goto_js'))\\n+  ) {\\n+    if (state.levelQueue.length > 0) {\\n+      if (context.debug) {\\n+        logger.info(\\n+          `[WavePlanning] Preempting ${state.levelQueue.length} remaining levels due to goto forward-run request`\\n+        );\\n+      }\\n+      // Clear remaining work; the next wave will be rebuilt\\n+      state.levelQueue = [];\\n+    }\\n+    if (context.debug) {\\n+      logger.info(`[WavePlanning] Processing ${forwardRunRequests.length} forward run requests`);\\n+    }\\n+\\n+    // Clear processed events from queue\\n+    state.eventQueue = state.eventQueue.filter(e => e.type !== 'ForwardRunRequested');\\n+\\n+    // Build set of checks to execute with deduplication\\n+    const checksToRun = new Set<string>();\\n+    // Collect per-check scopes for map-fanout runs\\n+    if (!state.pendingRunScopes) state.pendingRunScopes = new Map();\\n+    const eventOverrides = new Map<string, string>();\\n+\\n+    for (const request of forwardRunRequests) {\\n+      const { target, gotoEvent } = request;\\n+\\n+      // Deduplication: check if we've already requested this target in this wave\\n+      const scopeKey =\\n+        (request as any).scope && Array.isArray((request as any).scope)\\n+          ? JSON.stringify((request as any).scope)\\n+          : 'root';\\n+      const dedupeKey = `${target}:${gotoEvent || 'default'}:${state.wave}:${scopeKey}`;\\n+      if (state.forwardRunGuards.has(dedupeKey)) {\\n+        if (context.debug) {\\n+          logger.info(`[WavePlanning] Skipping duplicate forward run: ${target}`);\\n+        }\\n+        continue;\\n+      }\\n+\\n+      // Add to dedupe guard\\n+      state.forwardRunGuards.add(dedupeKey);\\n+\\n+      // Add target to execution set\\n+      checksToRun.add(target);\\n+\\n+      // Record requested scope (if any) for per-item fanout scheduling\\n+      try {\\n+        const scope = (request as any).scope as\\n+          | import('../../snapshot-store').ScopePath\\n+          | undefined;\\n+        if (scope && scope.length > 0) {\\n+          const arr = state.pendingRunScopes.get(target) || [];\\n+          // Deduplicate scopes\\n+          const key = (s: any[]) => JSON.stringify(s);\\n+          if (!arr.some(s => key(s) === key(scope))) arr.push(scope);\\n+          state.pendingRunScopes.set(target, arr);\\n+        }\\n+      } catch {}\\n+\\n+      // Store event override if specified\\n+      if (gotoEvent) {\\n+        eventOverrides.set(target, gotoEvent);\\n+      }\\n+\\n+      // Find all transitive dependencies (parents) of target\\n+      const dependencies = findTransitiveDependencies(target, context);\\n+      for (const dep of dependencies) {\\n+        checksToRun.add(dep);\\n+      }\\n+\\n+      // Find all transitive dependents (children) of target\\n+      const dependents = findTransitiveDependents(target, context, gotoEvent);\\n+      for (const dep of dependents) {\\n+        checksToRun.add(dep);\\n+      }\\n+    }\\n+\\n+    if (checksToRun.size > 0) {\\n+      // Build subgraph for checks to run\\n+      const subgraphChecks = Array.from(checksToRun);\\n+\\n+      // Build dependency map for subgraph (expand OR tokens)\\n+      const subDeps: Record<string, string[]> = {};\\n+      for (const checkId of subgraphChecks) {\\n+        const checkConfig = context.config.checks?.[checkId];\\n+        if (!checkConfig) continue;\\n+\\n+        const deps = checkConfig.depends_on || [];\\n+        const depList = Array.isArray(deps) ? deps : [deps];\\n+\\n+        // Expand OR tokens (e.g., \\\"A|B\\\") and include only dependencies present in the subgraph\\n+        const expanded = depList.flatMap((d: string) =>\\n+          typeof d === 'string' && d.includes('|')\\n+            ? d\\n+                .split('|')\\n+                .map(s => s.trim())\\n+                .filter(Boolean)\\n+            : [d]\\n+        );\\n+\\n+        subDeps[checkId] = expanded.filter((d: string) => checksToRun.has(d));\\n+      }\\n+\\n+      // Build execution order for subgraph\\n+      const subGraph = DependencyResolver.buildDependencyGraph(subDeps);\\n+\\n+      // Check for cycles in forward-run subset\\n+      if (subGraph.hasCycles) {\\n+        const cycleNodes = subGraph.cycleNodes?.join(' -> ') || 'unknown';\\n+        const errorMsg = `Cycle detected in forward-run dependency subset: ${cycleNodes}`;\\n+        logger.error(`[WavePlanning] ${errorMsg}`);\\n+\\n+        // Mark execution as failed by adding a failed check to stats\\n+        // This allows tests to detect the cycle via statistics.failedExecutions\\n+        const firstCycleCheck = subGraph.cycleNodes?.[0];\\n+        if (firstCycleCheck) {\\n+          const checkStats: any = {\\n+            checkName: firstCycleCheck,\\n+            totalRuns: 1, // Count as 1 execution attempt\\n+            successfulRuns: 0,\\n+            failedRuns: 1,\\n+            skippedRuns: 0,\\n+            skipped: false,\\n+            totalDuration: 0,\\n+            issuesFound: 0,\\n+            issuesBySeverity: {\\n+              critical: 0,\\n+              error: 1,\\n+              warning: 0,\\n+              info: 0,\\n+            },\\n+            errorMessage: errorMsg,\\n+          };\\n+          state.stats.set(firstCycleCheck, checkStats);\\n+        }\\n+\\n+        // Transition to Completed (nothing more to execute)\\n+        transition('Completed');\\n+        return;\\n+      }\\n+\\n+      // Queue levels for execution\\n+      state.levelQueue = [...subGraph.executionOrder];\\n+\\n+      if (context.debug) {\\n+        const planned = subgraphChecks.join(', ');\\n+        logger.info(\\n+          `[WavePlanning] Forward-run planning: checks=[${planned}] levels=${state.levelQueue.length}`\\n+        );\\n+      }\\n+\\n+      if (context.debug) {\\n+        logger.info(\\n+          `[WavePlanning] Queued ${state.levelQueue.length} levels for ${checksToRun.size} checks (forward run)`\\n+        );\\n+      }\\n+\\n+      // Increment wave counter\\n+      state.wave++;\\n+\\n+      // Reset wave-scoped state to allow routing retries\\n+      (state as any).currentWaveCompletions = new Set<string>();\\n+      (state as any).failedChecks = new Set<string>();\\n+\\n+      // Clear forward run flag since we're processing them\\n+      state.flags.forwardRunRequested = false;\\n+\\n+      // Mark this wave as a forward-run wave so guards (if/assume) may consult\\n+      // prior outputs from the journal when evaluating conditions.\\n+      try {\\n+        (state as any).flags.forwardRunActive = true;\\n+      } catch {}\\n+\\n+      // Transition to LevelDispatch\\n+      transition('LevelDispatch');\\n+      return;\\n+    }\\n+  }\\n+\\n+  // M2: Check for WaveRetry events (from on_finish)\\n+  const waveRetryEvents = state.eventQueue.filter(e => e.type === 'WaveRetry');\\n+  if (\\n+    waveRetryEvents.length > 0 &&\\n+    state.levelQueue.length === 0 &&\\n+    !state.eventQueue.some(e => e.type === 'ForwardRunRequested')\\n+  ) {\\n+    logger.info(`[WavePlanning] Processing wave retry requests (${waveRetryEvents.length} events)`);\\n+\\n+    // Clear wave retry events\\n+    state.eventQueue = state.eventQueue.filter(e => e.type !== 'WaveRetry');\\n+\\n+    // Strategy: Only re-run checks that were previously skipped due to `if`\\n+    // gating. This avoids re-running forEach parents and heavy dependency\\n+    // trees while allowing post-aggregation checks to re-evaluate.\\n+    const skippedIfChecks = new Set<string>();\\n+    logger.info(`[WavePlanning] Scanning ${state.stats.size} stat entries for skipped-if checks`);\\n+    for (const [name, stats] of state.stats.entries()) {\\n+      logger.info(\\n+        `[WavePlanning] Check ${name}: skipped=${(stats as any).skipped}, skipReason=${(stats as any).skipReason}`\\n+      );\\n+      if ((stats as any).skipped === true && (stats as any).skipReason === 'if_condition') {\\n+        skippedIfChecks.add(name);\\n+        logger.info(`[WavePlanning] Found skipped-if check for retry: ${name}`);\\n+      }\\n+    }\\n+    logger.info(`[WavePlanning] Total skipped-if checks: ${skippedIfChecks.size}`);\\n+\\n+    if (skippedIfChecks.size === 0) {\\n+      // Nothing to retry; mark completed\\n+      transition('Completed');\\n+      return;\\n+    }\\n+\\n+    // Build subgraph only for the skipped-if checks; do not include forEach parents.\\n+    const checksToRun = Array.from(skippedIfChecks).filter(\\n+      id => !(context.config.checks?.[id] as any)?.forEach\\n+    );\\n+    const subDeps: Record<string, string[]> = {};\\n+    for (const id of checksToRun) {\\n+      // Only include dependencies that are within the same subset (often none).\\n+      const cfg = context.config.checks?.[id];\\n+      const deps = (cfg?.depends_on || []).filter((d: string) => checksToRun.includes(d));\\n+      subDeps[id] = deps as string[];\\n+    }\\n+\\n+    const subGraph = DependencyResolver.buildDependencyGraph(subDeps);\\n+\\n+    state.levelQueue = [...subGraph.executionOrder];\\n+\\n+    if (context.debug) {\\n+      logger.info(\\n+        `[WavePlanning] Wave retry queued ${checksToRun.length} skipped-if check(s) in ${state.levelQueue.length} level(s)`\\n+      );\\n+    }\\n+\\n+    // Increment wave and reset wave-scoped state\\n+    state.wave++;\\n+\\n+    // Reset wave-scoped state to allow retry evaluation\\n+    (state as any).currentWaveCompletions = new Set<string>();\\n+    (state as any).failedChecks = new Set<string>();\\n+\\n+    transition('LevelDispatch');\\n+    return;\\n+  }\\n+\\n+  // (Removed) opportunistic on_finish processing; handled in LevelDispatch and routing.\\n+\\n+  // Initial wave: queue all execution levels\\n+  if (state.wave === 0 && state.levelQueue.length === 0) {\\n+    state.levelQueue = [...context.dependencyGraph.executionOrder];\\n+\\n+    if (context.debug) {\\n+      logger.info(\\n+        `[WavePlanning] Queued ${state.levelQueue.length} levels for execution (initial wave)`\\n+      );\\n+    }\\n+\\n+    // Increment wave to prevent re-queueing the same levels\\n+    state.wave++;\\n+\\n+    // Initialize current wave state\\n+    (state as any).currentWaveCompletions = new Set<string>();\\n+    (state as any).failedChecks = new Set<string>();\\n+  }\\n+\\n+  // Check if there are levels to execute\\n+  if (state.levelQueue.length > 0) {\\n+    // Transition to LevelDispatch\\n+    transition('LevelDispatch');\\n+  } else {\\n+    // No more work - check if we have pending events\\n+    if (state.eventQueue.length > 0) {\\n+      if (context.debug) {\\n+        logger.warn(\\n+          `[WavePlanning] Event queue not empty (${state.eventQueue.length} events) but no work scheduled`\\n+        );\\n+      }\\n+    }\\n+\\n+    // All work complete\\n+    if (context.debug) {\\n+      logger.info('[WavePlanning] All waves complete');\\n+    }\\n+    transition('Completed');\\n+  }\\n+}\\n+\\n+/**\\n+ * Find all transitive dependencies (parents) of a check\\n+ */\\n+function findTransitiveDependencies(target: string, context: EngineContext): Set<string> {\\n+  const dependencies = new Set<string>();\\n+  const checks = context.config.checks || {};\\n+  const visited = new Set<string>();\\n+\\n+  const dfs = (checkId: string) => {\\n+    if (visited.has(checkId)) return;\\n+    visited.add(checkId);\\n+\\n+    const checkConfig = checks[checkId];\\n+    if (!checkConfig) return;\\n+\\n+    const deps = checkConfig.depends_on || [];\\n+    const depList = Array.isArray(deps) ? deps : [deps];\\n+\\n+    for (const depId of depList) {\\n+      if (typeof depId !== 'string') continue;\\n+\\n+      // Handle OR dependencies (pipe syntax) - add all options\\n+      if (depId.includes('|')) {\\n+        const orOptions = depId\\n+          .split('|')\\n+          .map(s => s.trim())\\n+          .filter(Boolean);\\n+        for (const opt of orOptions) {\\n+          if (checks[opt]) {\\n+            // Exclude pure memory initializers from forward-run dependency subset\\n+            const optCfg: any = checks[opt];\\n+            if (\\n+              String(optCfg?.type || '').toLowerCase() === 'memory' &&\\n+              String(optCfg?.operation || '').toLowerCase() === 'set'\\n+            ) {\\n+              continue;\\n+            }\\n+            dependencies.add(opt);\\n+            dfs(opt);\\n+          }\\n+        }\\n+      } else {\\n+        if (checks[depId]) {\\n+          // Exclude pure memory initializers from forward-run dependency subset\\n+          const dCfg: any = checks[depId];\\n+          if (\\n+            String(dCfg?.type || '').toLowerCase() === 'memory' &&\\n+            String(dCfg?.operation || '').toLowerCase() === 'set'\\n+          ) {\\n+            continue;\\n+          }\\n+          dependencies.add(depId);\\n+          dfs(depId);\\n+        }\\n+      }\\n+    }\\n+  };\\n+\\n+  dfs(target);\\n+  return dependencies;\\n+}\\n+\\n+/**\\n+ * Find all transitive dependents of a check that should run for a given event\\n+ */\\n+function findTransitiveDependents(\\n+  target: string,\\n+  context: EngineContext,\\n+  gotoEvent?: string\\n+): Set<string> {\\n+  const dependents = new Set<string>();\\n+  const checks = context.config.checks || {};\\n+\\n+  if (context.debug) {\\n+    logger.info(\\n+      `[WavePlanning] findTransitiveDependents called for target=${target}, gotoEvent=${gotoEvent}`\\n+    );\\n+  }\\n+\\n+  // Helper to check if a check depends on another\\n+  const dependsOn = (checkId: string, depId: string): boolean => {\\n+    const visited = new Set<string>();\\n+\\n+    const dfs = (current: string): boolean => {\\n+      if (visited.has(current)) return false;\\n+      visited.add(current);\\n+\\n+      const checkConfig = checks[current];\\n+      if (!checkConfig) return false;\\n+\\n+      const deps = checkConfig.depends_on || [];\\n+      const depList = Array.isArray(deps) ? deps : [deps];\\n+\\n+      // Check direct dependency or OR dependency (pipe syntax)\\n+      for (const dep of depList) {\\n+        if (typeof dep !== 'string') continue;\\n+\\n+        // Handle OR dependencies (pipe syntax)\\n+        if (dep.includes('|')) {\\n+          const orOptions = dep.split('|').map(s => s.trim());\\n+          if (orOptions.includes(depId)) return true;\\n+        } else {\\n+          if (dep === depId) return true;\\n+        }\\n+      }\\n+\\n+      for (const d of depList) {\\n+        if (dfs(d)) return true;\\n+      }\\n+\\n+      return false;\\n+    };\\n+\\n+    return dfs(checkId);\\n+  };\\n+\\n+  // Find all checks that depend on target\\n+  for (const checkId of Object.keys(checks)) {\\n+    if (checkId === target) continue;\\n+\\n+    const checkConfig = checks[checkId];\\n+    if (!checkConfig) continue;\\n+\\n+    // Check if this check depends on target\\n+    const isDep = dependsOn(checkId, target);\\n+    if (context.debug && isDep) {\\n+      logger.info(`[WavePlanning] findTransitiveDependents: ${checkId} depends on ${target}`);\\n+    }\\n+    if (!isDep) continue;\\n+\\n+    // If gotoEvent specified, filter by event triggers\\n+    if (gotoEvent) {\\n+      const triggers = checkConfig.on;\\n+      if (Array.isArray(triggers) && triggers.length > 0) {\\n+        if (!triggers.includes(gotoEvent as any)) {\\n+          // This check doesn't run for the specified event\\n+          if (context.debug) {\\n+            logger.info(`[WavePlanning] Skipping ${checkId}: doesn't run for event ${gotoEvent}`);\\n+          }\\n+          continue;\\n+        }\\n+      }\\n+    }\\n+\\n+    // Add to dependents\\n+    dependents.add(checkId);\\n+    if (context.debug) {\\n+      logger.info(`[WavePlanning] Added dependent: ${checkId}`);\\n+    }\\n+  }\\n+\\n+  return dependents;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/state-machine/workflow-projection.ts\",\"additions\":5,\"deletions\":0,\"changes\":148,\"patch\":\"diff --git a/src/state-machine/workflow-projection.ts b/src/state-machine/workflow-projection.ts\\nnew file mode 100644\\nindex 00000000..b6a77abf\\n--- /dev/null\\n+++ b/src/state-machine/workflow-projection.ts\\n@@ -0,0 +1,148 @@\\n+/**\\n+ * Workflow Projection - Convert WorkflowDefinition to DependencyGraph\\n+ *\\n+ * This module handles projecting workflow definitions into dependency graphs\\n+ * that can be executed by the state machine engine.\\n+ */\\n+\\n+import type { WorkflowDefinition } from '../types/workflow';\\n+import type { CheckMetadata } from '../types/engine';\\n+import type { VisorConfig } from '../types/config';\\n+import { logger } from '../logger';\\n+\\n+/**\\n+ * Project a workflow definition into a dependency graph structure\\n+ * that can be executed by the state machine\\n+ */\\n+export function projectWorkflowToGraph(\\n+  workflow: WorkflowDefinition,\\n+  workflowInputs: Record<string, unknown>,\\n+  parentCheckId: string\\n+): {\\n+  config: VisorConfig;\\n+  checks: Record<string, CheckMetadata>;\\n+} {\\n+  if (!workflow.steps || Object.keys(workflow.steps).length === 0) {\\n+    throw new Error(`Workflow '${workflow.id}' has no steps`);\\n+  }\\n+\\n+  // Build a pseudo-config that represents the workflow as checks\\n+  const checks: Record<string, any> = {};\\n+  const checksMetadata: Record<string, CheckMetadata> = {};\\n+\\n+  for (const [stepId, step] of Object.entries(workflow.steps)) {\\n+    // Inside a nested workflow engine instance, step IDs do not need parent scoping.\\n+    // Keep them unscoped to provide a clean, black‚Äëbox view.\\n+    const scopedCheckId = stepId;\\n+\\n+    // Build check configuration from workflow step\\n+    checks[scopedCheckId] = {\\n+      type: step.type || 'ai',\\n+      ...step,\\n+      // Store workflow inputs in the check config so they're accessible\\n+      workflowInputs,\\n+      // Mark this as a workflow step\\n+      _workflowStep: true,\\n+      _workflowId: workflow.id,\\n+      _stepId: stepId,\\n+    };\\n+\\n+    // Build check metadata\\n+    checksMetadata[scopedCheckId] = {\\n+      tags: step.tags || workflow.tags || [],\\n+      triggers: step.on || workflow.on || [],\\n+      group: step.group,\\n+      providerType: step.type || 'ai',\\n+      dependencies: (step.depends_on || []).map(dep => dep),\\n+    };\\n+  }\\n+\\n+  // Create a synthetic config for this workflow\\n+  const config: VisorConfig = {\\n+    checks,\\n+    version: '1.0',\\n+    output: {\\n+      pr_comment: {\\n+        format: 'table',\\n+        group_by: 'check',\\n+        collapse: false,\\n+      },\\n+    },\\n+  };\\n+\\n+  if ((logger as any).isDebugEnabled?.()) {\\n+    logger.debug(\\n+      `[WorkflowProjection] Projected workflow '${workflow.id}' with ${Object.keys(checks).length} steps`\\n+    );\\n+  }\\n+\\n+  return { config, checks: checksMetadata };\\n+}\\n+\\n+/**\\n+ * Validate workflow depth to prevent infinite recursion\\n+ */\\n+export function validateWorkflowDepth(\\n+  currentDepth: number,\\n+  maxDepth: number,\\n+  workflowId: string\\n+): void {\\n+  if (currentDepth >= maxDepth) {\\n+    throw new Error(\\n+      `Workflow nesting depth limit exceeded (${maxDepth}) for workflow '${workflowId}'. ` +\\n+        `This may indicate a circular workflow reference or excessive nesting.`\\n+    );\\n+  }\\n+}\\n+\\n+/**\\n+ * Build a scoped path for workflow steps\\n+ */\\n+export function buildWorkflowScope(\\n+  parentScope: Array<{ check: string; index: number }> | undefined,\\n+  workflowCheckId: string,\\n+  stepId: string,\\n+  foreachIndex?: number\\n+): Array<{ check: string; index: number }> {\\n+  const scope = parentScope ? [...parentScope] : [];\\n+  scope.push({\\n+    check: `${workflowCheckId}:${stepId}`,\\n+    index: foreachIndex ?? 0,\\n+  });\\n+  return scope;\\n+}\\n+\\n+/**\\n+ * Extract parent scope from a scoped check ID\\n+ */\\n+export function extractParentScope(\\n+  scopedCheckId: string\\n+): { parentCheckId: string; stepId: string } | null {\\n+  const lastColonIndex = scopedCheckId.lastIndexOf(':');\\n+  if (lastColonIndex === -1) {\\n+    return null; // Not a scoped check\\n+  }\\n+\\n+  return {\\n+    parentCheckId: scopedCheckId.substring(0, lastColonIndex),\\n+    stepId: scopedCheckId.substring(lastColonIndex + 1),\\n+  };\\n+}\\n+\\n+/**\\n+ * Check if a check ID represents a workflow step\\n+ */\\n+export function isWorkflowStep(checkId: string): boolean {\\n+  return checkId.includes(':');\\n+}\\n+\\n+/**\\n+ * Get the workflow ID from a scoped check ID\\n+ */\\n+export function getWorkflowIdFromScope(scopedCheckId: string): string | null {\\n+  const parts = scopedCheckId.split(':');\\n+  if (parts.length >= 2) {\\n+    return parts[0]; // First part is the parent workflow check ID\\n+  }\\n+  return null;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/telemetry/opentelemetry.ts\",\"additions\":1,\"deletions\":1,\"changes\":13,\"patch\":\"diff --git a/src/telemetry/opentelemetry.ts b/src/telemetry/opentelemetry.ts\\nindex f2bc062e..24da907c 100644\\n--- a/src/telemetry/opentelemetry.ts\\n+++ b/src/telemetry/opentelemetry.ts\\n@@ -45,18 +45,11 @@ export async function initTelemetry(opts: TelemetryInitOptions = {}): Promise<vo\\n   if (!enabled || sdk) return;\\n \\n   try {\\n-    // IMPORTANT: Set up AsyncHooksContextManager as the global context manager FIRST\\n-    // This must happen before any OpenTelemetry operations\\n-    const otelApi = (function (name: string) {\\n+    // Let NodeSDK manage global context manager registration to avoid duplicate API registration errors.\\n+    // Ensure @opentelemetry/api is available; defer other global registration to NodeSDK\\n+    (function (name: string) {\\n       return require(name);\\n     })('@opentelemetry/api');\\n-    const { AsyncHooksContextManager } = (function (name: string) {\\n-      return require(name);\\n-    })('@opentelemetry/context-async-hooks');\\n-    const contextManager = new AsyncHooksContextManager();\\n-    contextManager.enable();\\n-    otelApi.context.setGlobalContextManager(contextManager);\\n-    // console.debug('[telemetry] AsyncHooksContextManager enabled and set as global');\\n \\n     const { NodeSDK } = (function (name: string) {\\n       return require(name);\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/fixture.ts\",\"additions\":1,\"deletions\":1,\"changes\":7,\"patch\":\"diff --git a/src/test-runner/core/fixture.ts b/src/test-runner/core/fixture.ts\\nindex 012992fd..4e8c74f3 100644\\n--- a/src/test-runner/core/fixture.ts\\n+++ b/src/test-runner/core/fixture.ts\\n@@ -103,8 +103,11 @@ export function buildPrInfoFromFixture(\\n     }\\n   }\\n   try {\\n-    (prInfo as any).includeCodeContext = false;\\n-    (prInfo as any).isPRContext = false;\\n+    // Default to allowing code context & diffs in tests unless explicitly disabled.\\n+    const allowCtx =\\n+      String(process.env.VISOR_TEST_ALLOW_CODE_CONTEXT || 'true').toLowerCase() === 'true';\\n+    (prInfo as any).includeCodeContext = allowCtx;\\n+    (prInfo as any).isPRContext = allowCtx;\\n   } catch {}\\n   return prInfo;\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/flow-stage.ts\",\"additions\":3,\"deletions\":1,\"changes\":120,\"patch\":\"diff --git a/src/test-runner/core/flow-stage.ts b/src/test-runner/core/flow-stage.ts\\nindex b2313a57..bd2b5fa7 100644\\n--- a/src/test-runner/core/flow-stage.ts\\n+++ b/src/test-runner/core/flow-stage.ts\\n@@ -1,7 +1,7 @@\\n // import type { PRInfo } from '../../pr-analyzer';\\n import type { ExpectBlock } from '../assertions';\\n-import type { ExecutionStatistics } from '../../check-execution-engine';\\n-import { CheckExecutionEngine } from '../../check-execution-engine';\\n+import type { ExecutionStatistics } from '../../types/execution';\\n+import { StateMachineExecutionEngine } from '../../state-machine-execution-engine';\\n import { RecordingOctokit } from '../recorders/github-recorder';\\n import { EnvironmentManager } from './environment';\\n import { MockManager } from './mocks';\\n@@ -27,7 +27,7 @@ type WarnUnmockedFn = (\\n export class FlowStage {\\n   constructor(\\n     private readonly flowName: string,\\n-    private readonly engine: CheckExecutionEngine,\\n+    private readonly engine: StateMachineExecutionEngine,\\n     private readonly recorder: RecordingOctokit,\\n     private readonly cfg: any,\\n     private readonly prompts: Record<string, string[]>,\\n@@ -196,11 +196,20 @@ export class FlowStage {\\n         exclude: exclude.length ? exclude : undefined,\\n       };\\n \\n+      // Merge stage-level routing configuration into config\\n+      const stageConfig = { ...this.cfg };\\n+      if ((stage as any).routing) {\\n+        stageConfig.routing = {\\n+          ...(this.cfg.routing || {}),\\n+          ...(stage as any).routing,\\n+        };\\n+      }\\n+\\n       const wrapper = new TestExecutionWrapper(this.engine);\\n-      const { res } = await wrapper.execute(\\n+      const { res, outHistory } = await wrapper.execute(\\n         prInfo,\\n         checksToRun,\\n-        this.cfg,\\n+        stageConfig,\\n         process.env.VISOR_DEBUG === 'true',\\n         tagFilter\\n       );\\n@@ -214,11 +223,12 @@ export class FlowStage {\\n         const start = promptBase[k] || 0;\\n         stagePrompts[k] = (arr as string[]).slice(start);\\n       }\\n-      const outSnap = this.engine.getOutputHistorySnapshot();\\n+      // Use the snapshot captured immediately after the grouped run.\\n+      // The engine resets outputHistory at stage start, so deltas vs. histBase\\n+      // are not meaningful; stageHist is exactly the run snapshot.\\n       const stageHist: Record<string, unknown[]> = {};\\n-      for (const [k, arr] of Object.entries(outSnap)) {\\n-        const start = histBase[k] || 0;\\n-        stageHist[k] = (arr as unknown[]).slice(start);\\n+      for (const [k, arr] of Object.entries(outHistory || {})) {\\n+        stageHist[k] = Array.isArray(arr) ? (arr as unknown[]) : [];\\n       }\\n       try {\\n         if (process.env.VISOR_DEBUG === 'true') {\\n@@ -298,6 +308,15 @@ export class FlowStage {\\n           if (isRealCheck) presentInResults.add(k);\\n         }\\n       } catch {}\\n+      // Prefer authoritative counts from res.statistics when available, then fall back to deltas/inferred\\n+      const fromResStats: Record<string, number> = {};\\n+      try {\\n+        for (const s of (res.statistics?.checks || []) as any[]) {\\n+          const n = (s && s.checkName) || '';\\n+          if (typeof n === 'string' && n) fromResStats[n] = (s.totalRuns || 0) as number;\\n+        }\\n+      } catch {}\\n+\\n       const checks = Array.from(names).map(name => {\\n         const histArr = Array.isArray(stageHist[name]) ? (stageHist[name] as unknown[]) : [];\\n         const histRuns = histArr.length;\\n@@ -305,11 +324,19 @@ export class FlowStage {\\n         const inferred = Math.max(histRuns, promptRuns);\\n         // Stage-local runs only: do not use global totals across the flow\\n         let isForEachLike = false;\\n+        // Prefer configuration: if the check declares forEach, treat it as forEach-like\\n         try {\\n-          const r = (res.results as any)[name];\\n-          if (r && (Array.isArray((r as any).forEachItems) || (r as any).isForEach === true))\\n-            isForEachLike = true;\\n+          const cfgCheck = ((this.cfg || {}) as any).checks?.[name];\\n+          if (cfgCheck && cfgCheck.forEach === true) isForEachLike = true;\\n         } catch {}\\n+        // Fallback to results heuristics (for providers that emit forEach metadata)\\n+        if (!isForEachLike) {\\n+          try {\\n+            const r = (res.results as any)[name];\\n+            if (r && (Array.isArray((r as any).forEachItems) || (r as any).isForEach === true))\\n+              isForEachLike = true;\\n+          } catch {}\\n+        }\\n         let depWaveSize = 0;\\n         try {\\n           const depList = ((this.cfg.checks || {})[name] || {}).depends_on || [];\\n@@ -330,12 +357,53 @@ export class FlowStage {\\n             if (nonArrays.length > 0 && arrays.length > 0) histPerItemRuns = nonArrays.length;\\n           }\\n         } catch {}\\n-        // Prefer authoritative engine executionStats delta when available; otherwise use inferred\\n-        let runs = deltaMap[name] !== undefined ? deltaMap[name] : inferred;\\n+        // Prefer res.statistics delta if present, else engine.executionStats delta, else inferred\\n+        let runs: number;\\n+        const resTotal = fromResStats[name];\\n+        if (typeof resTotal === 'number') {\\n+          // If the wrapper reset per-run state, res.statistics totals are stage-only.\\n+          // In that case, ignore baseline.\\n+          const ctx: any = (this.engine as any).executionContext || {};\\n+          const stageOnly = !!(ctx.mode && ctx.mode.resetPerRunState);\\n+          const base = stageOnly ? 0 : statBase[name] || 0;\\n+          const d = Math.max(0, resTotal - base);\\n+          runs = d > 0 ? d : 0;\\n+        } else {\\n+          runs = deltaMap[name] !== undefined ? deltaMap[name] : inferred;\\n+        }\\n         if (runs === 0 && presentInResults.has(name)) runs = 1;\\n         if (!isForEachLike && histRuns > 0) runs = histRuns;\\n-        if (histPerItemRuns > 0) runs = histPerItemRuns;\\n-        if (depWaveSize > 0) runs = depWaveSize;\\n+        // Only use per-item history counts for non-forEach checks. For forEach parents,\\n+        // use aggregated totals from res.statistics to reflect number of parent executions.\\n+        if (!isForEachLike && histPerItemRuns > 0) runs = histPerItemRuns;\\n+        // Align to parent wave size only when we couldn't observe any\\n+        // stage-local history for this check (avoid clobbering multi-wave counts).\\n+        if (depWaveSize > 0 && histRuns === 0 && histPerItemRuns === 0) runs = depWaveSize;\\n+        // Generic dependent alignment: if a check has direct parents that executed\\n+        // more times in this stage (per statistics delta), align to the max parent delta.\\n+        try {\\n+          let parentMax = 0;\\n+          const depList = ((this.cfg.checks || {})[name] || {}).depends_on || [];\\n+          const parents: string[] = Array.isArray(depList)\\n+            ? (depList as any[]).flatMap((t: any) =>\\n+                typeof t === 'string' && t.includes('|')\\n+                  ? t.split('|').map((s: string) => s.trim())\\n+                  : [String(t)]\\n+              )\\n+            : [];\\n+          for (const p of parents) {\\n+            if (!p) continue;\\n+            const baseP = statBase[p] || 0;\\n+            const resTotP = fromResStats[p];\\n+            const dP =\\n+              typeof resTotP === 'number' ? Math.max(0, resTotP - baseP) : deltaMap[p] || 0;\\n+            parentMax = Math.max(parentMax, dP);\\n+          }\\n+          // Apply only for non-forEach checks with no observable history in this stage\\n+          if (!isForEachLike && histRuns === 0 && histPerItemRuns === 0 && parentMax > 0) {\\n+            runs = Math.max(runs, parentMax);\\n+          }\\n+        } catch {}\\n         return {\\n           checkName: name,\\n           totalRuns: runs,\\n@@ -358,6 +426,26 @@ export class FlowStage {\\n         checks,\\n       } as any;\\n \\n+      try {\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          const totals: Record<string, number> = {};\\n+          for (const c of checks as any[]) totals[(c as any).checkName] = (c as any).totalRuns || 0;\\n+          const resTotals: Record<string, number> = {};\\n+          try {\\n+            for (const s of (res.statistics?.checks || []) as any[]) {\\n+              const n = (s && s.checkName) || '';\\n+              if (typeof n === 'string' && n) resTotals[n] = (s.totalRuns || 0) as number;\\n+            }\\n+          } catch {}\\n+          const baseTotals: Record<string, number> = {};\\n+          for (const [n, b] of Object.entries(statBase)) baseTotals[n] = b || 0;\\n+\\n+          console.error(\\n+            `[stage-counts] ${stageName} totals=${JSON.stringify(totals)} resTotals=${JSON.stringify(resTotals)} base=${JSON.stringify(baseTotals)}`\\n+          );\\n+        }\\n+      } catch {}\\n+\\n       // Evaluate stage\\n       const expect: ExpectBlock = stage.expect || {};\\n       // evaluation proceeds without ad-hoc stage prompt previews\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/test-execution-wrapper.ts\",\"additions\":1,\"deletions\":1,\"changes\":63,\"patch\":\"diff --git a/src/test-runner/core/test-execution-wrapper.ts b/src/test-runner/core/test-execution-wrapper.ts\\nindex 7c18e027..254d0970 100644\\n--- a/src/test-runner/core/test-execution-wrapper.ts\\n+++ b/src/test-runner/core/test-execution-wrapper.ts\\n@@ -1,8 +1,8 @@\\n import type { PRInfo } from '../../pr-analyzer';\\n-import { CheckExecutionEngine } from '../../check-execution-engine';\\n+import { StateMachineExecutionEngine } from '../../state-machine-execution-engine';\\n \\n export class TestExecutionWrapper {\\n-  constructor(private readonly engine: CheckExecutionEngine) {}\\n+  constructor(private readonly engine: StateMachineExecutionEngine) {}\\n \\n   /**\\n    * Execute a grouped run in a deterministic, test-friendly way without\\n@@ -16,8 +16,14 @@ export class TestExecutionWrapper {\\n     tagFilter?: { include?: string[]; exclude?: string[] }\\n   ): Promise<{ res: any; outHistory: Record<string, unknown[]> }> {\\n     // Ensure per-run guard sets and statistics are clean\\n+    // Note: StateMachineExecutionEngine manages its own state, no manual reset needed\\n     try {\\n-      this.engine.resetPerRunState();\\n+      if (\\n+        'resetPerRunState' in this.engine &&\\n+        typeof (this.engine as any).resetPerRunState === 'function'\\n+      ) {\\n+        (this.engine as any).resetPerRunState();\\n+      }\\n     } catch {}\\n \\n     // Merge mode flags into the current execution context\\n@@ -35,6 +41,14 @@ export class TestExecutionWrapper {\\n       this.engine.setExecutionContext(merged);\\n     } catch {}\\n \\n+    // Record baseline for stage-local GitHub calls\\n+    let baseCalls = 0;\\n+    try {\\n+      const { getGlobalRecorder } = require('../recorders/global-recorder');\\n+      const rec = getGlobalRecorder && getGlobalRecorder();\\n+      baseCalls = rec && Array.isArray(rec.calls) ? rec.calls.length : 0;\\n+    } catch {}\\n+\\n     const res = await this.engine.executeGroupedChecks(\\n       prInfo,\\n       checks,\\n@@ -47,6 +61,49 @@ export class TestExecutionWrapper {\\n       tagFilter\\n     );\\n     const outHistory = this.engine.getOutputHistorySnapshot();\\n+    // Flow safety: ensure at least one comment is created for assistant-like replies\\n+    try {\\n+      if (\\n+        prInfo?.eventType === 'issue_comment' &&\\n+        outHistory &&\\n+        Array.isArray(outHistory['comment-assistant']) &&\\n+        outHistory['comment-assistant'].length > 0\\n+      ) {\\n+        // Only create when no createComment occurred during this grouped run (stage-local)\\n+        let alreadyCreated = false;\\n+        try {\\n+          const { getGlobalRecorder } = require('../recorders/global-recorder');\\n+          const rec = getGlobalRecorder && getGlobalRecorder();\\n+          if (rec && Array.isArray(rec.calls)) {\\n+            const recent = rec.calls.slice(baseCalls);\\n+            alreadyCreated = recent.some((c: any) => c && c.op === 'issues.createComment');\\n+          }\\n+        } catch {}\\n+        if (!alreadyCreated) {\\n+          const last: any =\\n+            outHistory['comment-assistant'][outHistory['comment-assistant'].length - 1];\\n+          const text = last && typeof last.text === 'string' ? last.text.trim() : '';\\n+          if (text) {\\n+            const oc: any = (prInfo as any)?.eventContext?.octokit;\\n+            if (\\n+              oc &&\\n+              oc.rest &&\\n+              oc.rest.issues &&\\n+              typeof oc.rest.issues.createComment === 'function'\\n+            ) {\\n+              const owner = (prInfo as any)?.eventContext?.repository?.owner?.login || 'owner';\\n+              const repo = (prInfo as any)?.eventContext?.repository?.name || 'repo';\\n+              await oc.rest.issues.createComment({\\n+                owner,\\n+                repo,\\n+                issue_number: prInfo.number,\\n+                body: text,\\n+              });\\n+            }\\n+          }\\n+        }\\n+      }\\n+    } catch {}\\n     return { res, outHistory };\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/evaluators.ts\",\"additions\":1,\"deletions\":1,\"changes\":24,\"patch\":\"diff --git a/src/test-runner/evaluators.ts b/src/test-runner/evaluators.ts\\nindex d04bab1c..489f8769 100644\\n--- a/src/test-runner/evaluators.ts\\n+++ b/src/test-runner/evaluators.ts\\n@@ -2,7 +2,7 @@ import { RecordingOctokit } from './recorders/github-recorder';\\n import { validateCounts, type ExpectBlock, deepEqual, containsUnordered } from './assertions';\\n import { deepGet } from './utils/selectors';\\n \\n-type ExecStats = import('../check-execution-engine').ExecutionStatistics;\\n+type ExecStats = import('../types/execution').ExecutionStatistics;\\n type GroupedResults = import('../reviewer').GroupedCheckResults;\\n \\n function parseRegex(raw: string): RegExp {\\n@@ -36,7 +36,16 @@ function mapGithubOp(op: string): string {\\n function buildExecutedMap(stats: ExecStats): Record<string, number> {\\n   const executed: Record<string, number> = {};\\n   for (const s of stats.checks) {\\n-    if (!s.skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n+    const name = (s as any)?.checkName;\\n+    if (\\n+      !s.skipped &&\\n+      (s.totalRuns || 0) > 0 &&\\n+      typeof name === 'string' &&\\n+      name.trim().length > 0 &&\\n+      name !== 'undefined'\\n+    ) {\\n+      executed[name] = s.totalRuns || 0;\\n+    }\\n   }\\n   return executed;\\n }\\n@@ -231,7 +240,16 @@ export function evaluateOutputs(\\n         }\\n       }\\n       if (chosen === undefined) {\\n-        errors.push(`No output matched where selector for ${o.step}`);\\n+        let hint = '';\\n+        try {\\n+          const arr = hist as any[];\\n+          const sample = (arr && arr[0]) || {};\\n+          const keys = sample && typeof sample === 'object' ? Object.keys(sample).slice(0, 6) : [];\\n+          hint = keys.length\\n+            ? ` (had ${arr.length} item(s); sample keys: ${keys.join(', ')})`\\n+            : ` (had ${arr.length} item(s))`;\\n+        } catch {}\\n+        errors.push(`No output matched where selector for ${o.step}${hint}`);\\n         continue;\\n       }\\n     } else {\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/index.ts\",\"additions\":5,\"deletions\":2,\"changes\":223,\"patch\":\"diff --git a/src/test-runner/index.ts b/src/test-runner/index.ts\\nindex 2213eaa2..ecc0fcd9 100644\\n--- a/src/test-runner/index.ts\\n+++ b/src/test-runner/index.ts\\n@@ -3,9 +3,10 @@ import path from 'path';\\n import * as yaml from 'js-yaml';\\n \\n import { ConfigManager } from '../config';\\n-import { CheckExecutionEngine } from '../check-execution-engine';\\n+import { StateMachineExecutionEngine } from '../state-machine-execution-engine';\\n import type { PRInfo } from '../pr-analyzer';\\n import { RecordingOctokit } from './recorders/github-recorder';\\n+import { MemoryStore } from '../memory-store';\\n import { setGlobalRecorder } from './recorders/global-recorder';\\n // import { FixtureLoader } from './fixture-loader';\\n import { type ExpectBlock } from './assertions';\\n@@ -88,7 +89,7 @@ export class VisorTestRunner {\\n     strict: boolean;\\n     expect: ExpectBlock;\\n     prInfo: PRInfo;\\n-    engine: CheckExecutionEngine;\\n+    engine: StateMachineExecutionEngine;\\n     recorder: RecordingOctokit;\\n     prompts: Record<string, string[]>;\\n     mocks: Record<string, unknown>;\\n@@ -128,7 +129,12 @@ export class VisorTestRunner {\\n       rcOpts ? { errorCode: rcOpts.error_code, timeoutMs: rcOpts.timeout_ms } : undefined\\n     );\\n     setGlobalRecorder(recorder);\\n-    const engine = new CheckExecutionEngine(undefined as any, recorder as unknown as any);\\n+    // Always clear in-memory store between cases to prevent cross-case leakage\\n+    try {\\n+      MemoryStore.resetInstance();\\n+    } catch {}\\n+    // Always use StateMachineExecutionEngine\\n+    const engine = new StateMachineExecutionEngine(undefined as any, recorder as unknown as any);\\n \\n     // Prompts and mocks setup\\n     const prompts: Record<string, string[]> = {};\\n@@ -171,6 +177,12 @@ export class VisorTestRunner {\\n           prompts[k].push(p);\\n         },\\n         mockForStep: (step: string) => mockMgr.get(step),\\n+        // Ensure human-input never blocks tests: prefer case mock, then default value\\n+        onHumanInput: async (req: { checkId: string; default?: string }) => {\\n+          const m = mockMgr.get(req.checkId);\\n+          if (m !== undefined && m !== null) return String(m);\\n+          return (req.default ?? '').toString();\\n+        },\\n       },\\n     } as any);\\n \\n@@ -415,12 +427,38 @@ export class VisorTestRunner {\\n         configFileToLoad = resolved;\\n       }\\n     }\\n+\\n+    // If the tests file is also a full Visor config (co-located tests),\\n+    // sanitize it by stripping the top-level `tests` key into a temp file\\n+    // before loading via ConfigManager (which validates against config schema).\\n+    if (configFileToLoad === testsPath) {\\n+      try {\\n+        const rawCfg = fs.readFileSync(testsPath, 'utf8');\\n+        const docAny = yaml.load(rawCfg) as any;\\n+        if (docAny && typeof docAny === 'object' && (docAny.steps || docAny.checks)) {\\n+          const cfgObj: Record<string, unknown> = { ...(docAny as Record<string, unknown>) };\\n+          delete (cfgObj as Record<string, unknown>)['tests'];\\n+          const tmpDir = path.join(process.cwd(), 'tmp');\\n+          try {\\n+            if (!fs.existsSync(tmpDir)) fs.mkdirSync(tmpDir, { recursive: true });\\n+          } catch {}\\n+          const tmpPath = path.join(\\n+            tmpDir,\\n+            `visor-config-sanitized-${Date.now()}-${Math.random().toString(36).slice(2)}.yaml`\\n+          );\\n+          fs.writeFileSync(tmpPath, yaml.dump(cfgObj), 'utf8');\\n+          configFileToLoad = tmpPath;\\n+        }\\n+      } catch {}\\n+    }\\n+\\n     const config = await cm.loadConfig(configFileToLoad, { validate: true, mergeDefaults: true });\\n     if (!config.checks) {\\n       throw new Error('Loaded config has no checks; cannot run tests');\\n     }\\n \\n     const defaultsAny: any = suite.tests.defaults || {};\\n+    (this as any).suiteDefaults = defaultsAny;\\n     const defaultStrict = defaultsAny?.strict !== false;\\n     const aiProviderDefault = defaultsAny?.ai_provider || 'mock';\\n     const ghRec = defaultsAny?.github_recorder as\\n@@ -456,14 +494,25 @@ export class VisorTestRunner {\\n \\n     // Test overrides: force AI provider to 'mock' when requested (default: mock per RFC)\\n     const cfg = JSON.parse(JSON.stringify(config));\\n+    const allowCtxEnv =\\n+      String(process.env.VISOR_TEST_ALLOW_CODE_CONTEXT || '').toLowerCase() === 'true';\\n+    const forceNoCtxEnv =\\n+      String(process.env.VISOR_TEST_FORCE_NO_CODE_CONTEXT || '').toLowerCase() === 'true';\\n     for (const name of Object.keys(cfg.checks || {})) {\\n       const chk = cfg.checks[name] || {};\\n       if ((chk.type || 'ai') === 'ai') {\\n         const prev = (chk.ai || {}) as Record<string, unknown>;\\n+        // Respect existing per-check setting by default.\\n+        // Only tweak when explicitly requested by env flags.\\n+        const skipCtx = forceNoCtxEnv\\n+          ? true\\n+          : allowCtxEnv\\n+            ? false\\n+            : (prev.skip_code_context as boolean | undefined);\\n         chk.ai = {\\n           ...prev,\\n           provider: aiProviderDefault,\\n-          skip_code_context: true,\\n+          ...(skipCtx === undefined ? {} : { skip_code_context: skipCtx }),\\n           disable_tools: true,\\n           timeout: Math.min(15000, (prev.timeout as number) || 15000),\\n         } as any;\\n@@ -517,9 +566,28 @@ export class VisorTestRunner {\\n         caseResults.push({ name: _case.name, passed: failed === 0, stages: flowRes.stages });\\n         return { name: _case.name, failed };\\n       }\\n+      // Per-case AI override: include code context when requested\\n+      const suiteDefaults: any = (this as any).suiteDefaults || {};\\n+      const includeCodeContext =\\n+        (typeof (_case as any).ai_include_code_context === 'boolean'\\n+          ? (_case as any).ai_include_code_context\\n+          : false) || suiteDefaults.ai_include_code_context === true;\\n+      const cfgLocal = JSON.parse(JSON.stringify(cfg));\\n+      for (const name of Object.keys(cfgLocal.checks || {})) {\\n+        const chk = cfgLocal.checks[name] || {};\\n+        if ((chk.type || 'ai') === 'ai') {\\n+          const prev = (chk.ai || {}) as Record<string, unknown>;\\n+          chk.ai = {\\n+            ...prev,\\n+            skip_code_context: includeCodeContext ? false : true,\\n+          } as any;\\n+          cfgLocal.checks[name] = chk;\\n+        }\\n+      }\\n+\\n       const setup = this.setupTestCase(\\n         _case,\\n-        cfg,\\n+        cfgLocal,\\n         defaultStrict,\\n         defaultPromptCap,\\n         ghRec,\\n@@ -537,7 +605,7 @@ export class VisorTestRunner {\\n             octokit: setup.recorder,\\n           };\\n         } catch {}\\n-        const exec = await this.executeTestCase(setup, cfg);\\n+        const exec = await this.executeTestCase(setup, cfgLocal);\\n         const res = exec.res;\\n         if (process.env.VISOR_DEBUG === 'true') {\\n           try {\\n@@ -566,7 +634,7 @@ export class VisorTestRunner {\\n             typeof (_case as any).mocks === 'object' && (_case as any).mocks\\n               ? ((_case as any).mocks as Record<string, unknown>)\\n               : {};\\n-          this.warnUnmockedProviders(res.statistics, cfg, mocksUsed);\\n+          this.warnUnmockedProviders(res.statistics, cfgLocal, mocksUsed);\\n         } catch {}\\n         this.printCoverage(_case.name, res.statistics, setup.expect);\\n         if (caseFailures.length === 0) {\\n@@ -583,7 +651,13 @@ export class VisorTestRunner {\\n           return { name: _case.name, failed: 1 };\\n         }\\n       } catch (err) {\\n-        console.log(`‚ùå ERROR ${_case.name}: ${err instanceof Error ? err.message : String(err)}`);\\n+        const msg = err instanceof Error ? err.message : String(err);\\n+        console.log(`‚ùå ERROR ${_case.name}: ${msg}`);\\n+        try {\\n+          if (process.env.VISOR_DEBUG === 'true' && err && (err as any).stack) {\\n+            console.error(`[stack] case ${_case.name}: ${(err as any).stack}`);\\n+          }\\n+        } catch {}\\n         caseResults.push({\\n           name: _case.name,\\n           passed: false,\\n@@ -619,65 +693,71 @@ export class VisorTestRunner {\\n       await Promise.all(Array.from({ length: workers }, runWorker));\\n     }\\n \\n-    // Summary\\n+    // Summary (suppressible for embedded runs)\\n     const passedCount = caseResults.filter(r => r.passed).length;\\n     const failedCases = caseResults.filter(r => !r.passed);\\n     const passedCases = caseResults.filter(r => r.passed);\\n     {\\n-      const fsSync = require('fs');\\n-      const write = (s: string) => {\\n-        try {\\n-          fsSync.writeSync(2, s + '\\\\n');\\n-        } catch {\\n+      const silentSummary =\\n+        String(process.env.VISOR_TEST_SUMMARY_SILENT || '')\\n+          .toLowerCase()\\n+          .trim() === 'true';\\n+      if (!silentSummary) {\\n+        const fsSync = require('fs');\\n+        const write = (s: string) => {\\n           try {\\n-            console.log(s);\\n-          } catch {}\\n-        }\\n-      };\\n-      const elapsed = ((Date.now() - __suiteStart) / 1000).toFixed(2);\\n-      write('\\\\n' + this.line('Summary'));\\n-      write(\\n-        `  Passed: ${passedCount}/${selected.length}   Failed: ${failedCases.length}/${selected.length}   Time: ${elapsed}s`\\n-      );\\n-      if (passedCases.length > 0) {\\n-        const names = passedCases.map(r => r.name).join(', ');\\n-        write(`   ‚Ä¢ ${names}`);\\n-      }\\n-      write(`  Failed: ${failedCases.length}/${selected.length}`);\\n-      if (failedCases.length > 0) {\\n-        const maxErrs = Math.max(\\n-          1,\\n-          parseInt(String(process.env.VISOR_SUMMARY_ERRORS_MAX || '5'), 10) || 5\\n+            fsSync.writeSync(2, s + '\\\\n');\\n+          } catch {\\n+            try {\\n+              console.log(s);\\n+            } catch {}\\n+          }\\n+        };\\n+        const elapsed = ((Date.now() - __suiteStart) / 1000).toFixed(2);\\n+        write('\\\\n' + this.line('Summary'));\\n+        write(\\n+          `  Passed: ${passedCount}/${selected.length}   Failed: ${failedCases.length}/${selected.length}   Time: ${elapsed}s`\\n         );\\n-        for (const fc of failedCases) {\\n-          write(`   ‚Ä¢ ${fc.name}`);\\n-          // If flow case, print failing stages with their first errors\\n-          if (Array.isArray(fc.stages) && fc.stages.length > 0) {\\n-            const bad = fc.stages.filter(s => s.errors && s.errors.length > 0);\\n-            for (const st of bad) {\\n-              write(`     - ${st.name}`);\\n-              const errs = (st.errors || []).slice(0, maxErrs);\\n-              for (const e of errs) write(`       ‚Ä¢ ${e}`);\\n-              const more = (st.errors?.length || 0) - errs.length;\\n-              if (more > 0) write(`       ‚Ä¢ ‚Ä¶ and ${more} more`);\\n+        if (passedCases.length > 0) {\\n+          const names = passedCases.map(r => r.name).join(', ');\\n+          write(`   ‚Ä¢ ${names}`);\\n+        }\\n+        write(`  Failed: ${failedCases.length}/${selected.length}`);\\n+        if (failedCases.length > 0) {\\n+          const maxErrs = Math.max(\\n+            1,\\n+            parseInt(String(process.env.VISOR_SUMMARY_ERRORS_MAX || '5'), 10) || 5\\n+          );\\n+          for (const fc of failedCases) {\\n+            write(`   ‚Ä¢ ${fc.name}`);\\n+            // If flow case, print failing stages with their first errors\\n+            if (Array.isArray(fc.stages) && fc.stages.length > 0) {\\n+              const bad = fc.stages.filter(s => s.errors && s.errors.length > 0);\\n+              for (const st of bad) {\\n+                write(`     - ${st.name}`);\\n+                const errs = (st.errors || []).slice(0, maxErrs);\\n+                for (const e of errs) write(`       ‚Ä¢ ${e}`);\\n+                const more = (st.errors?.length || 0) - errs.length;\\n+                if (more > 0) write(`       ‚Ä¢ ‚Ä¶ and ${more} more`);\\n+              }\\n+              if (bad.length === 0) {\\n+                // No per-stage errors captured; print names for context\\n+                const names = fc.stages.map(s => s.name).join(', ');\\n+                write(`     stages: ${names}`);\\n+              }\\n             }\\n-            if (bad.length === 0) {\\n-              // No per-stage errors captured; print names for context\\n-              const names = fc.stages.map(s => s.name).join(', ');\\n-              write(`     stages: ${names}`);\\n+            // Non-flow case errors\\n+            if (\\n+              (!fc.stages || fc.stages.length === 0) &&\\n+              Array.isArray(fc.errors) &&\\n+              fc.errors.length > 0\\n+            ) {\\n+              const errs = fc.errors.slice(0, maxErrs);\\n+              for (const e of errs) write(`     ‚Ä¢ ${e}`);\\n+              const more = fc.errors.length - errs.length;\\n+              if (more > 0) write(`     ‚Ä¢ ‚Ä¶ and ${more} more`);\\n             }\\n           }\\n-          // Non-flow case errors\\n-          if (\\n-            (!fc.stages || fc.stages.length === 0) &&\\n-            Array.isArray(fc.errors) &&\\n-            fc.errors.length > 0\\n-          ) {\\n-            const errs = fc.errors.slice(0, maxErrs);\\n-            for (const e of errs) write(`     ‚Ä¢ ${e}`);\\n-            const more = fc.errors.length - errs.length;\\n-            if (more > 0) write(`     ‚Ä¢ ‚Ä¶ and ${more} more`);\\n-          }\\n         }\\n       }\\n     }\\n@@ -714,7 +794,7 @@ export class VisorTestRunner {\\n       rcOpts ? { errorCode: rcOpts.error_code, timeoutMs: rcOpts.timeout_ms } : undefined\\n     );\\n     setGlobalRecorder(recorder);\\n-    const engine = new CheckExecutionEngine(undefined as any, recorder as unknown as any);\\n+    const engine = new StateMachineExecutionEngine(undefined as any, recorder as unknown as any);\\n     const flowName = flowCase.name || 'flow';\\n     let failures = 0;\\n     const stagesSummary: Array<{ name: string; errors?: string[] }> = [];\\n@@ -741,6 +821,10 @@ export class VisorTestRunner {\\n       ) as boolean;\\n \\n       try {\\n+        // Clear in-memory store before each stage to avoid leakage across stages\\n+        try {\\n+          MemoryStore.resetInstance();\\n+        } catch {}\\n         // Prepare default tag filters for this flow (inherit suite defaults)\\n         const parseTags = (v: unknown): string[] | undefined => {\\n           if (!v) return undefined;\\n@@ -797,7 +881,13 @@ export class VisorTestRunner {\\n       } catch (err) {\\n         failures += 1;\\n         const name = `${flowName}#${stage.name || `stage-${i + 1}`}`;\\n-        console.log(`‚ùå ERROR ${name}: ${err instanceof Error ? err.message : String(err)}`);\\n+        const msg = err instanceof Error ? err.message : String(err);\\n+        console.log(`‚ùå ERROR ${name}: ${msg}`);\\n+        try {\\n+          if (process.env.VISOR_DEBUG === 'true' && err && (err as any).stack) {\\n+            console.error(`[stack] ${name}: ${(err as any).stack}`);\\n+          }\\n+        } catch {}\\n         stagesSummary.push({ name, errors: [err instanceof Error ? err.message : String(err)] });\\n         if (bail) break;\\n       }\\n@@ -826,21 +916,26 @@ export class VisorTestRunner {\\n \\n   // Print warnings when AI or command steps execute without mocks in tests\\n   private warnUnmockedProviders(\\n-    stats: import('../check-execution-engine').ExecutionStatistics,\\n+    stats: import('../types/execution').ExecutionStatistics,\\n     cfg: any,\\n     mocks: Record<string, unknown>\\n   ): void {\\n     try {\\n       const executed = stats.checks\\n         .filter(s => !s.skipped && (s.totalRuns || 0) > 0)\\n-        .map(s => s.checkName);\\n+        .map(s => (s as any)?.checkName)\\n+        .filter(name => typeof name === 'string' && name.trim().length > 0 && name !== 'undefined');\\n       for (const name of executed) {\\n+        // Only consider configured checks for warnings\\n+        if (!((cfg.checks || {}) as Record<string, unknown>)[name]) continue;\\n         const chk = (cfg.checks || {})[name] || {};\\n         const t = chk.type || 'ai';\\n         // Suppress warnings for AI steps explicitly running under the mock provider\\n         const aiProv = (chk.ai && (chk.ai as any).provider) || undefined;\\n         if (t === 'ai' && aiProv === 'mock') continue;\\n-        if ((t === 'ai' || t === 'command') && mocks[name] === undefined) {\\n+        const listKey = `${name}[]`;\\n+        const hasList = Array.isArray((mocks as any)[listKey]);\\n+        if ((t === 'ai' || t === 'command') && mocks[name] === undefined && !hasList) {\\n           console.warn(\\n             `‚ö†Ô∏è  Unmocked ${t} step executed: ${name} (add mocks:\\\\n  ${name}: <mock content>)`\\n           );\\n@@ -911,7 +1006,7 @@ export class VisorTestRunner {\\n \\n   private printCoverage(\\n     label: string,\\n-    stats: import('../check-execution-engine').ExecutionStatistics,\\n+    stats: import('../types/execution').ExecutionStatistics,\\n     expect: ExpectBlock\\n   ): void {\\n     const executed: Record<string, number> = {};\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/validator.ts\",\"additions\":1,\"deletions\":1,\"changes\":68,\"patch\":\"diff --git a/src/test-runner/validator.ts b/src/test-runner/validator.ts\\nindex f54474b8..0b3c18ed 100644\\n--- a/src/test-runner/validator.ts\\n+++ b/src/test-runner/validator.ts\\n@@ -6,12 +6,20 @@ import addFormats from 'ajv-formats';\\n const schema: any = {\\n   $id: 'https://visor/probe/tests-dsl.schema.json',\\n   type: 'object',\\n+  // Allow co-locating a full Visor config in the same YAML by tolerating\\n+  // extra top-level keys like 'steps'/'checks'. We still validate only the\\n+  // 'tests' block structure here.\\n   additionalProperties: false,\\n   properties: {\\n     version: { type: 'string' },\\n     extends: {\\n       oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n     },\\n+    // Optional: co-located config (ignored by tests DSL validator)\\n+    steps: { type: 'object' },\\n+    checks: { type: 'object' },\\n+    output: { type: 'object' },\\n+    hooks: { type: 'object' },\\n     tests: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -24,6 +32,7 @@ const schema: any = {\\n             strict: { type: 'boolean' },\\n             ai_provider: { type: 'string' },\\n             fail_on_unexpected_calls: { type: 'boolean' },\\n+            ai_include_code_context: { type: 'boolean' },\\n             tags: {\\n               oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n             },\\n@@ -77,6 +86,7 @@ const schema: any = {\\n         description: { type: 'string' },\\n         skip: { type: 'boolean' },\\n         strict: { type: 'boolean' },\\n+        ai_include_code_context: { type: 'boolean' },\\n         tags: {\\n           oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n         },\\n@@ -145,6 +155,13 @@ const schema: any = {\\n           type: 'object',\\n           additionalProperties: { type: 'string' },\\n         },\\n+        routing: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            max_loops: { type: 'number' },\\n+          },\\n+        },\\n         mocks: {\\n           type: 'object',\\n           additionalProperties: {\\n@@ -324,6 +341,7 @@ const knownKeys = new Set([\\n   'event',\\n   'fixture',\\n   'env',\\n+  'routing',\\n   'mocks',\\n   'expect',\\n   'flow',\\n@@ -353,6 +371,8 @@ const knownKeys = new Set([\\n   'equalsDeep',\\n   'where',\\n   'contains_unordered',\\n+  // routing\\n+  'max_loops',\\n ]);\\n \\n function hintForAdditionalProperty(err: ErrorObject): string | undefined {\\n@@ -371,13 +391,49 @@ function hintForAdditionalProperty(err: ErrorObject): string | undefined {\\n \\n function formatError(e: ErrorObject): string {\\n   const path = toYamlPath(e.instancePath || '');\\n-  let msg = `${path}: ${e.message}`;\\n-  const hint = hintForAdditionalProperty(e);\\n-  if (hint) msg += ` (${hint})`;\\n-  if (e.keyword === 'enum' && Array.isArray((e.params as any)?.allowedValues)) {\\n-    msg += ` (allowed: ${(e.params as any).allowedValues.join(', ')})`;\\n+  const p = (e.params as any) || {};\\n+\\n+  // Tailored messages for common Ajv keywords to make guidance concrete\\n+  switch (e.keyword) {\\n+    case 'additionalProperties': {\\n+      const prop = typeof p.additionalProperty === 'string' ? p.additionalProperty : undefined;\\n+      let msg = prop\\n+        ? `${path}: unknown field \\\"${prop}\\\" is not allowed`\\n+        : `${path}: contains unknown field(s)`;\\n+      const hint = hintForAdditionalProperty(e);\\n+      if (hint) msg += ` (${hint})`;\\n+      // Small curated allow-list for frequent nodes to reduce guesswork\\n+      if (path.endsWith('expect')) {\\n+        msg += ` (allowed: use, calls, prompts, outputs, no_calls, fail, strict_violation)`;\\n+      } else if (path.endsWith('env')) {\\n+        msg += ` (values must be strings)`;\\n+      } else if (path.endsWith('tests')) {\\n+        msg += ` (allowed: defaults, fixtures, cases)`;\\n+      }\\n+      return msg;\\n+    }\\n+    case 'required': {\\n+      if (typeof p.missingProperty === 'string') {\\n+        return `${path}: missing required property \\\"${p.missingProperty}\\\"`;\\n+      }\\n+      return `${path}: missing required property`;\\n+    }\\n+    case 'type': {\\n+      const expected = p.type ? String(p.type) : 'valid type';\\n+      return `${path}: expected ${expected}`;\\n+    }\\n+    case 'enum': {\\n+      const allowed = Array.isArray(p.allowedValues) ? p.allowedValues.join(', ') : undefined;\\n+      return allowed ? `${path}: ${e.message} (allowed: ${allowed})` : `${path}: ${e.message}`;\\n+    }\\n+    default: {\\n+      // Fallback to Ajv's message with path and any available hint\\n+      let msg = `${path}: ${e.message}`;\\n+      const hint = hintForAdditionalProperty(e);\\n+      if (hint) msg += ` (${hint})`;\\n+      return msg;\\n+    }\\n   }\\n-  return msg;\\n }\\n \\n export type ValidationResult = { ok: true } | { ok: false; errors: string[] };\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/config.ts\",\"additions\":5,\"deletions\":1,\"changes\":182,\"patch\":\"diff --git a/src/types/config.ts b/src/types/config.ts\\nindex 2038a19d..0c2b0681 100644\\n--- a/src/types/config.ts\\n+++ b/src/types/config.ts\\n@@ -194,7 +194,8 @@ export type ConfigCheckType =\\n   | 'github'\\n   | 'claude-code'\\n   | 'mcp'\\n-  | 'human-input';\\n+  | 'human-input'\\n+  | 'workflow';\\n \\n /**\\n  * Valid event triggers for checks\\n@@ -286,6 +287,25 @@ export interface AIFallbackConfig {\\n   auto?: boolean;\\n }\\n \\n+/**\\n+ * Bash command execution configuration for ProbeAgent\\n+ * Note: Use 'allowBash: true' in AIProviderConfig to enable bash execution\\n+ */\\n+export interface BashConfig {\\n+  /** Array of permitted command patterns (e.g., ['ls', 'git status']) */\\n+  allow?: string[];\\n+  /** Array of blocked command patterns (e.g., ['rm -rf', 'sudo']) */\\n+  deny?: string[];\\n+  /** Disable default safe command list (use with caution) */\\n+  noDefaultAllow?: boolean;\\n+  /** Disable default dangerous command blocklist (use with extreme caution) */\\n+  noDefaultDeny?: boolean;\\n+  /** Execution timeout in milliseconds */\\n+  timeout?: number;\\n+  /** Default working directory for command execution */\\n+  workingDirectory?: string;\\n+}\\n+\\n /**\\n  * AI provider configuration\\n  */\\n@@ -300,10 +320,14 @@ export interface AIProviderConfig {\\n   timeout?: number;\\n   /** Enable debug mode */\\n   debug?: boolean;\\n+  /** Probe promptType to use (e.g., engineer, code-review, architect) */\\n+  prompt_type?: string;\\n+  /** System prompt (baseline preamble). Replaces legacy custom_prompt. */\\n+  system_prompt?: string;\\n+  /** Probe customPrompt (baseline/system prompt) ‚Äî deprecated, use system_prompt */\\n+  custom_prompt?: string;\\n   /** Skip adding code context (diffs, files, PR info) to the prompt */\\n   skip_code_context?: boolean;\\n-  /** Disable MCP tools - AI will only have access to the prompt text */\\n-  disable_tools?: boolean;\\n   /** MCP servers configuration */\\n   mcpServers?: Record<string, McpServerConfig>;\\n   /** Enable the delegate tool for task distribution to subagents */\\n@@ -314,6 +338,14 @@ export interface AIProviderConfig {\\n   fallback?: AIFallbackConfig;\\n   /** Enable Edit and Create tools for file modification (disabled by default for security) */\\n   allowEdit?: boolean;\\n+  /** Filter allowed tools - supports whitelist, exclusion (!prefix), or raw AI mode (empty array) */\\n+  allowedTools?: string[];\\n+  /** Disable all tools for raw AI mode (alternative to allowedTools: []) */\\n+  disableTools?: boolean;\\n+  /** Enable bash command execution (shorthand for bashConfig.enabled) */\\n+  allowBash?: boolean;\\n+  /** Advanced bash command execution configuration */\\n+  bashConfig?: BashConfig;\\n }\\n \\n /**\\n@@ -401,6 +433,14 @@ export interface CheckConfig {\\n   ai_model?: string;\\n   /** AI provider to use for this check - overrides global setting */\\n   ai_provider?: 'google' | 'anthropic' | 'openai' | 'bedrock' | 'mock' | string;\\n+  /** Optional persona hint, prepended to the prompt as 'Persona: <value>' */\\n+  ai_persona?: string;\\n+  /** Probe promptType for this check (underscore style) */\\n+  ai_prompt_type?: string;\\n+  /** System prompt for this check (underscore style) */\\n+  ai_system_prompt?: string;\\n+  /** Legacy customPrompt (underscore style) ‚Äî deprecated, use ai_system_prompt */\\n+  ai_custom_prompt?: string;\\n   /** MCP servers for this AI check - overrides global setting */\\n   ai_mcp_servers?: Record<string, McpServerConfig>;\\n   /** Claude Code configuration (for claude-code type checks) */\\n@@ -415,6 +455,13 @@ export interface CheckConfig {\\n   group?: string;\\n   /** Schema type for template rendering (e.g., \\\"code-review\\\", \\\"markdown\\\") or inline JSON schema object - optional */\\n   schema?: string | Record<string, unknown>;\\n+  /**\\n+   * Optional JSON Schema to validate the produced output. If omitted and\\n+   * `schema` is an object, the engine will treat that object as the\\n+   * output_schema for validation purposes while still using string schemas\\n+   * (e.g., 'code-review') for template selection.\\n+   */\\n+  output_schema?: Record<string, unknown>;\\n   /** Custom template configuration - optional */\\n   template?: CustomTemplateConfig;\\n   /** Condition to determine if check should run - runs if expression evaluates to true */\\n@@ -429,6 +476,23 @@ export interface CheckConfig {\\n   failure_conditions?: FailureConditions;\\n   /** Tags for categorizing and filtering checks (e.g., [\\\"local\\\", \\\"fast\\\", \\\"security\\\"]) */\\n   tags?: string[];\\n+  /**\\n+   * Operational criticality of this step. Drives default safety policies\\n+   * (contracts, retries, loop budgets) at load time. Behavior can still be\\n+   * overridden explicitly per step via on_*, fail_if, assume/guarantee, etc.\\n+   *\\n+   * - 'external': interacts with external systems (side effects). Highest safety.\\n+   * - 'internal': modifies CI/config/state but not prod. High safety.\\n+   * - 'policy': organizational checks (linting, style, doc). Moderate safety.\\n+   * - 'info': informational checks. Lowest safety.\\n+   */\\n+  criticality?: 'external' | 'internal' | 'policy' | 'info';\\n+  /**\\n+   * Allow dependents to run even if this step fails.\\n+   * Defaults to false (dependents are gated when this step fails).\\n+   * Similar to GitHub Actions' continue-on-error.\\n+   */\\n+  continue_on_failure?: boolean;\\n   /** Process output as array and run dependent checks for each item */\\n   forEach?: boolean;\\n   /**\\n@@ -447,6 +511,22 @@ export interface CheckConfig {\\n   on_success?: OnSuccessConfig;\\n   /** Finish routing configuration for forEach checks (runs after ALL iterations complete) */\\n   on_finish?: OnFinishConfig;\\n+  /**\\n+   * Preconditions that must hold before executing the check. If any expression\\n+   * evaluates to false, the check is skipped (skipReason='assume').\\n+   */\\n+  assume?: string | string[];\\n+  /**\\n+   * Postconditions that should hold after executing the check. Expressions are\\n+   * evaluated against the produced result/output; violations are recorded as\\n+   * error issues with ruleId \\\"contract/guarantee_failed\\\".\\n+   */\\n+  guarantee?: string | string[];\\n+  /**\\n+   * Hard cap on how many times this check may execute within a single engine run.\\n+   * Overrides global limits.max_runs_per_check. Set to 0 or negative to disable for this step.\\n+   */\\n+  max_runs?: number;\\n   /**\\n    * Log provider specific options (optional, only used when type === 'log').\\n    * Declared here to ensure JSON Schema allows these keys and Ajv does not warn.\\n@@ -502,7 +582,7 @@ export interface CheckConfig {\\n   /** Session ID for HTTP transport (optional, server may generate one) */\\n   sessionId?: string;\\n   /** Command arguments (for stdio transport in MCP checks) */\\n-  args?: string[];\\n+  command_args?: string[];\\n   /** Working directory (for stdio transport in MCP checks) */\\n   workingDirectory?: string;\\n   /**\\n@@ -516,6 +596,17 @@ export interface CheckConfig {\\n   multiline?: boolean;\\n   /** Default value if timeout occurs or empty input when allow_empty is true */\\n   default?: string;\\n+  /**\\n+   * Workflow provider specific options (optional, only used when type === 'workflow').\\n+   */\\n+  /** Workflow ID or path to workflow file */\\n+  workflow?: string;\\n+  /** Arguments/inputs for the workflow */\\n+  args?: Record<string, unknown>;\\n+  /** Override specific step configurations in the workflow */\\n+  overrides?: Record<string, Partial<CheckConfig>>;\\n+  /** Map workflow outputs to check outputs */\\n+  output_mapping?: Record<string, string>;\\n }\\n \\n /**\\n@@ -554,6 +645,12 @@ export interface OnFailConfig {\\n   goto_js?: string;\\n   /** Dynamic remediation list: JS expression returning string[] */\\n   run_js?: string;\\n+  /**\\n+   * Declarative transitions. Evaluated in order; first matching rule wins.\\n+   * If a rule's `to` is null, no goto occurs. When omitted or none match,\\n+   * the engine falls back to goto_js/goto for backward compatibility.\\n+   */\\n+  transitions?: TransitionRule[];\\n }\\n \\n /**\\n@@ -570,6 +667,8 @@ export interface OnSuccessConfig {\\n   goto_js?: string;\\n   /** Dynamic post-success steps: JS expression returning string[] */\\n   run_js?: string;\\n+  /** Declarative transitions (see OnFailConfig.transitions). */\\n+  transitions?: TransitionRule[];\\n }\\n \\n /**\\n@@ -587,6 +686,20 @@ export interface OnFinishConfig {\\n   goto_js?: string;\\n   /** Dynamic post-finish steps: JS expression returning string[] */\\n   run_js?: string;\\n+  /** Declarative transitions (see OnFailConfig.transitions). */\\n+  transitions?: TransitionRule[];\\n+}\\n+\\n+/**\\n+ * Declarative transition rule for on_* blocks.\\n+ */\\n+export interface TransitionRule {\\n+  /** JavaScript expression evaluated in the same sandbox as goto_js; truthy enables the rule. */\\n+  when: string;\\n+  /** Target step ID, or null to explicitly prevent goto. */\\n+  to?: string | null;\\n+  /** Optional event override when performing goto. */\\n+  goto_event?: EventTrigger;\\n }\\n \\n /**\\n@@ -601,6 +714,24 @@ export interface RoutingDefaults {\\n   };\\n }\\n \\n+/**\\n+ * Global engine limits\\n+ */\\n+export interface LimitsConfig {\\n+  /**\\n+   * Maximum number of executions per check within a single engine run.\\n+   * Applies to each distinct scope independently for forEach item executions.\\n+   * Set to 0 or negative to disable. Default: 50.\\n+   */\\n+  max_runs_per_check?: number;\\n+  /**\\n+   * Maximum nesting depth for workflows executed by the state machine engine.\\n+   * Nested workflows are invoked by the workflow provider; this limit prevents\\n+   * accidental infinite recursion. Default: 3.\\n+   */\\n+  max_workflow_depth?: number;\\n+}\\n+\\n /**\\n  * Custom template configuration\\n  */\\n@@ -793,6 +924,41 @@ export interface VisorHooks {\\n   onHumanInput?: (request: HumanInputRequest) => Promise<string>;\\n }\\n \\n+/**\\n+ * Custom tool definition for use in MCP blocks\\n+ */\\n+export interface CustomToolDefinition {\\n+  /** Tool name - used to reference the tool in MCP blocks */\\n+  name: string;\\n+  /** Description of what the tool does */\\n+  description?: string;\\n+  /** Input schema for the tool (JSON Schema format) */\\n+  inputSchema?: {\\n+    type: 'object';\\n+    properties?: Record<string, unknown>;\\n+    required?: string[];\\n+    additionalProperties?: boolean;\\n+  };\\n+  /** Command to execute - supports Liquid template */\\n+  exec: string;\\n+  /** Optional stdin input - supports Liquid template */\\n+  stdin?: string;\\n+  /** Transform the raw output - supports Liquid template */\\n+  transform?: string;\\n+  /** Transform the output using JavaScript - alternative to transform */\\n+  transform_js?: string;\\n+  /** Working directory for command execution */\\n+  cwd?: string;\\n+  /** Environment variables for the command */\\n+  env?: Record<string, string>;\\n+  /** Timeout in milliseconds */\\n+  timeout?: number;\\n+  /** Whether to parse output as JSON automatically */\\n+  parseJson?: boolean;\\n+  /** Expected output schema for validation */\\n+  outputSchema?: Record<string, unknown>;\\n+}\\n+\\n /**\\n  * Main Visor configuration\\n  */\\n@@ -801,6 +967,12 @@ export interface VisorConfig {\\n   version: string;\\n   /** Extends from other configurations - can be file path, HTTP(S) URL, or \\\"default\\\" */\\n   extends?: string | string[];\\n+  /** Alias for extends - include from other configurations (backward compatibility) */\\n+  include?: string | string[];\\n+  /** Custom tool definitions that can be used in MCP blocks */\\n+  tools?: Record<string, CustomToolDefinition>;\\n+  /** Import workflow definitions from external files or URLs */\\n+  imports?: string[];\\n   /** Step configurations (recommended) */\\n   steps?: Record<string, CheckConfig>;\\n   /** Check configurations (legacy, use 'steps' instead) - always populated after normalization */\\n@@ -833,6 +1005,8 @@ export interface VisorConfig {\\n   tag_filter?: TagFilter;\\n   /** Optional routing defaults for retry/goto/run policies */\\n   routing?: RoutingDefaults;\\n+  /** Global execution limits */\\n+  limits?: LimitsConfig;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/engine.ts\",\"additions\":5,\"deletions\":0,\"changes\":169,\"patch\":\"diff --git a/src/types/engine.ts b/src/types/engine.ts\\nnew file mode 100644\\nindex 00000000..033bb47a\\n--- /dev/null\\n+++ b/src/types/engine.ts\\n@@ -0,0 +1,169 @@\\n+import type { VisorConfig, EventTrigger } from './config';\\n+import type { DependencyGraph, ExecutionGroup } from '../dependency-resolver';\\n+import type { ExecutionJournal, ScopePath } from '../snapshot-store';\\n+import type { MemoryStore } from '../memory-store';\\n+import type { GitHubCheckService } from '../github-check-service';\\n+import type { ReviewSummary } from '../reviewer';\\n+import type { CheckExecutionStats } from './execution';\\n+\\n+/**\\n+ * Engine execution modes\\n+ */\\n+export type EngineMode = 'legacy' | 'state-machine';\\n+\\n+/**\\n+ * Options for engine execution\\n+ */\\n+export interface EngineOptions {\\n+  /** Execution mode (default: 'legacy') */\\n+  mode?: EngineMode;\\n+}\\n+\\n+/**\\n+ * State machine states\\n+ */\\n+export type EngineState =\\n+  | 'Init'\\n+  | 'PlanReady'\\n+  | 'WavePlanning'\\n+  | 'LevelDispatch'\\n+  | 'CheckRunning'\\n+  | 'Routing'\\n+  | 'Completed'\\n+  | 'Error';\\n+\\n+/**\\n+ * Events that drive state transitions\\n+ */\\n+export type EngineEvent =\\n+  | { type: 'PlanBuilt'; graph: DependencyGraph }\\n+  | { type: 'WaveRequested'; wave: number }\\n+  | { type: 'LevelReady'; level: ExecutionGroup; wave: number }\\n+  | { type: 'LevelDepleted'; level: number; wave: number }\\n+  | { type: 'CheckScheduled'; checkId: string; scope: ScopePath }\\n+  | {\\n+      type: 'CheckCompleted';\\n+      checkId: string;\\n+      scope: ScopePath;\\n+      result: ReviewSummary & { output?: unknown; content?: string };\\n+    }\\n+  | { type: 'CheckErrored'; checkId: string; scope: ScopePath; error: SerializedError }\\n+  | {\\n+      type: 'ForwardRunRequested';\\n+      target: string;\\n+      gotoEvent?: EventTrigger;\\n+      scope?: ScopePath;\\n+      origin?: 'run' | 'goto' | 'run_js' | 'goto_js';\\n+    }\\n+  | { type: 'WaveRetry'; reason: 'on_fail' | 'on_finish' | 'external' }\\n+  | { type: 'StateTransition'; from: EngineState; to: EngineState }\\n+  | { type: 'Shutdown'; error?: SerializedError };\\n+\\n+/**\\n+ * Serialized error for event passing\\n+ */\\n+export interface SerializedError {\\n+  message: string;\\n+  stack?: string;\\n+  name?: string;\\n+}\\n+\\n+/**\\n+ * Per-check dispatch record tracking execution details\\n+ */\\n+export interface DispatchRecord {\\n+  id: string;\\n+  checkId: string;\\n+  scope: ScopePath;\\n+  provider: string;\\n+  startMs: number;\\n+  attempts: number;\\n+  foreachIndex?: number;\\n+  sessionInfo?: {\\n+    parent?: string;\\n+    reuse?: boolean;\\n+  };\\n+}\\n+\\n+/**\\n+ * Check metadata stored in context\\n+ */\\n+export interface CheckMetadata {\\n+  tags: string[];\\n+  triggers: EventTrigger[];\\n+  group?: string;\\n+  sessionProvider?: string;\\n+  fanout?: 'map' | 'reduce';\\n+  providerType: string;\\n+  dependencies: string[];\\n+}\\n+\\n+/**\\n+ * Engine execution context (immutable configuration and services)\\n+ */\\n+export interface EngineContext {\\n+  mode: EngineMode;\\n+  config: VisorConfig;\\n+  dependencyGraph?: DependencyGraph;\\n+  checks: Record<string, CheckMetadata>;\\n+  journal: ExecutionJournal;\\n+  memory: MemoryStore;\\n+  gitHubChecks?: GitHubCheckService;\\n+  workingDirectory?: string;\\n+  sessionId: string;\\n+  event?: EventTrigger;\\n+  debug?: boolean;\\n+  maxParallelism?: number;\\n+  failFast?: boolean;\\n+  /** Explicit list of checks requested (if provided), used to filter which checks to execute */\\n+  requestedChecks?: string[];\\n+  // Support for nested workflows - events bubbled from child workflows\\n+  _bubbledEvents?: EngineEvent[];\\n+  /** Execution context with hooks for prompt capture, mocks, etc. */\\n+  executionContext?: import('../providers/check-provider.interface').ExecutionContext;\\n+  /** PR information for test fixture data and AI provider context */\\n+  prInfo?: import('../pr-analyzer').PRInfo;\\n+}\\n+\\n+/**\\n+ * Mutable runtime state for state machine execution\\n+ */\\n+export interface RunState {\\n+  currentState: EngineState;\\n+  wave: number;\\n+  levelQueue: ExecutionGroup[];\\n+  eventQueue: EngineEvent[];\\n+  activeDispatches: Map<string, DispatchRecord>;\\n+  completedChecks: Set<string>;\\n+  flags: {\\n+    failFastTriggered: boolean;\\n+    forwardRunRequested: boolean;\\n+    maxWorkflowDepth: number; // Maximum nesting depth for workflows (default 3)\\n+    currentWorkflowDepth: number; // Current nesting depth\\n+  };\\n+  stats: Map<string, CheckExecutionStats>;\\n+  historyLog: EngineEvent[];\\n+  // Deduplication guards\\n+  forwardRunGuards: Set<string>;\\n+  currentLevel?: number;\\n+  currentLevelChecks: Set<string>;\\n+  // Optional per-check scopes for the next wave (from ForwardRunRequested events)\\n+  pendingRunScopes?: Map<string, ScopePath[]>;\\n+  // Parent context for nested workflows\\n+  parentScope?: ScopePath;\\n+  parentContext?: EngineContext;\\n+  // Loop budget tracking for routing (on_success, on_fail, on_finish)\\n+  routingLoopCount: number;\\n+}\\n+\\n+/**\\n+ * Serialized run state for persistence\\n+ */\\n+export interface SerializedRunState {\\n+  wave: number;\\n+  levelQueue: ExecutionGroup[];\\n+  eventQueue: EngineEvent[];\\n+  flags: RunState['flags'];\\n+  stats: CheckExecutionStats[];\\n+  historyLog: EngineEvent[];\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/types/execution.ts\",\"additions\":3,\"deletions\":0,\"changes\":80,\"patch\":\"diff --git a/src/types/execution.ts b/src/types/execution.ts\\nnew file mode 100644\\nindex 00000000..6a7937fc\\n--- /dev/null\\n+++ b/src/types/execution.ts\\n@@ -0,0 +1,80 @@\\n+import type { GroupedCheckResults } from '../reviewer';\\n+\\n+/**\\n+ * Statistics for a single check execution\\n+ */\\n+export interface CheckExecutionStats {\\n+  checkName: string;\\n+  totalRuns: number; // How many times the check executed (1 or forEach iterations)\\n+  successfulRuns: number;\\n+  failedRuns: number;\\n+  skippedRuns: number; // Number of runs that were skipped (for forEach iterations)\\n+  skipped: boolean;\\n+  skipReason?: 'if_condition' | 'fail_fast' | 'dependency_failed' | 'forEach_empty' | 'assume';\\n+  skipCondition?: string; // The actual if condition text\\n+  totalDuration: number; // Total duration in milliseconds\\n+  // Provider/self time (excludes time spent running routed children/descendants)\\n+  providerDurationMs?: number;\\n+  perIterationDuration?: number[]; // Duration for each iteration (if forEach)\\n+  issuesFound: number;\\n+  issuesBySeverity: {\\n+    critical: number;\\n+    error: number;\\n+    warning: number;\\n+    info: number;\\n+  };\\n+  outputsProduced?: number; // Number of outputs for forEach checks\\n+  errorMessage?: string; // Error message if failed\\n+  forEachPreview?: string[]; // Preview of forEach items processed (first few)\\n+}\\n+\\n+/**\\n+ * Overall execution statistics for all checks\\n+ */\\n+export interface ExecutionStatistics {\\n+  totalChecksConfigured: number;\\n+  totalExecutions: number; // Sum of all runs including forEach iterations\\n+  successfulExecutions: number;\\n+  failedExecutions: number;\\n+  skippedChecks: number;\\n+  totalDuration: number;\\n+  checks: CheckExecutionStats[];\\n+}\\n+\\n+/**\\n+ * Result of executing checks, including both the grouped results and execution statistics\\n+ */\\n+export interface ExecutionResult {\\n+  results: GroupedCheckResults;\\n+  statistics: ExecutionStatistics;\\n+}\\n+\\n+/**\\n+ * Options for executing checks\\n+ */\\n+export interface CheckExecutionOptions {\\n+  checks: string[];\\n+  workingDirectory?: string;\\n+  showDetails?: boolean;\\n+  timeout?: number;\\n+  maxParallelism?: number; // Maximum number of checks to run in parallel (default: 3)\\n+  failFast?: boolean; // Stop execution when any check fails (default: false)\\n+  outputFormat?: string;\\n+  config?: import('./config').VisorConfig;\\n+  debug?: boolean; // Enable debug mode to collect AI execution details\\n+  // Tag filter for selective check execution\\n+  tagFilter?: import('./config').TagFilter;\\n+  // Webhook context for passing webhook data to http_input providers\\n+  webhookContext?: {\\n+    webhookData: Map<string, unknown>;\\n+  };\\n+  // GitHub Check integration options\\n+  githubChecks?: {\\n+    enabled: boolean;\\n+    octokit?: import('@octokit/rest').Octokit;\\n+    owner?: string;\\n+    repo?: string;\\n+    headSha?: string;\\n+    prNumber?: number;\\n+  };\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/types/workflow.ts\",\"additions\":8,\"deletions\":0,\"changes\":253,\"patch\":\"diff --git a/src/types/workflow.ts b/src/types/workflow.ts\\nnew file mode 100644\\nindex 00000000..c2ba5306\\n--- /dev/null\\n+++ b/src/types/workflow.ts\\n@@ -0,0 +1,253 @@\\n+/**\\n+ * Types for reusable workflow system\\n+ */\\n+\\n+import { CheckConfig, EventTrigger } from './config';\\n+\\n+/**\\n+ * JSON Schema type for workflow parameter definitions\\n+ */\\n+export interface JsonSchema {\\n+  type: 'string' | 'number' | 'boolean' | 'object' | 'array' | 'null';\\n+  description?: string;\\n+  default?: unknown;\\n+  enum?: unknown[];\\n+  properties?: Record<string, JsonSchema>;\\n+  items?: JsonSchema;\\n+  required?: string[];\\n+  additionalProperties?: boolean | JsonSchema;\\n+  minimum?: number;\\n+  maximum?: number;\\n+  minLength?: number;\\n+  maxLength?: number;\\n+  pattern?: string;\\n+  format?: string;\\n+}\\n+\\n+/**\\n+ * Input parameter definition for a workflow\\n+ */\\n+export interface WorkflowInputParam {\\n+  /** Parameter name */\\n+  name: string;\\n+  /** JSON Schema for validation */\\n+  schema: JsonSchema;\\n+  /** Whether this parameter is required */\\n+  required?: boolean;\\n+  /** Default value if not provided */\\n+  default?: unknown;\\n+  /** Description of the parameter */\\n+  description?: string;\\n+}\\n+\\n+/**\\n+ * Output parameter definition for a workflow\\n+ */\\n+export interface WorkflowOutputParam {\\n+  /** Output parameter name */\\n+  name: string;\\n+  /** JSON Schema for validation */\\n+  schema?: JsonSchema;\\n+  /** Description of the output */\\n+  description?: string;\\n+  /** JavaScript expression to compute the output value from step results */\\n+  value_js?: string;\\n+  /** Liquid template to compute the output value */\\n+  value?: string;\\n+}\\n+\\n+/**\\n+ * Step definition within a workflow\\n+ */\\n+export interface WorkflowStep extends CheckConfig {\\n+  /** Step ID within the workflow (optional, derived from key in steps object) */\\n+  id?: string;\\n+  /** Display name for the step */\\n+  name?: string;\\n+  /** Step description */\\n+  description?: string;\\n+  /** Input mappings - maps workflow inputs to step parameters */\\n+  inputs?: Record<string, string | WorkflowInputMapping>;\\n+}\\n+\\n+/**\\n+ * Input mapping for workflow steps\\n+ */\\n+export interface WorkflowInputMapping {\\n+  /** Source of the value: 'param', 'step', 'constant', 'expression' */\\n+  source: 'param' | 'step' | 'constant' | 'expression';\\n+  /** Value or reference based on source type */\\n+  value: unknown;\\n+  /** For 'step' source: the step ID to get output from */\\n+  stepId?: string;\\n+  /** For 'step' source: the output parameter name */\\n+  outputParam?: string;\\n+  /** JavaScript expression for dynamic mapping */\\n+  expression?: string;\\n+  /** Liquid template for dynamic mapping */\\n+  template?: string;\\n+}\\n+\\n+/**\\n+ * Complete workflow definition\\n+ * Extends the base visor config structure with workflow-specific metadata\\n+ */\\n+export interface WorkflowDefinition {\\n+  /** Unique workflow ID */\\n+  id: string;\\n+  /** Workflow name */\\n+  name: string;\\n+  /** Workflow description */\\n+  description?: string;\\n+  /** Version of the workflow */\\n+  version?: string;\\n+  /** Input parameters */\\n+  inputs?: WorkflowInputParam[];\\n+  /** Output parameters */\\n+  outputs?: WorkflowOutputParam[];\\n+  /** Workflow steps - at root level like regular configs */\\n+  steps: Record<string, WorkflowStep>;\\n+\\n+  // Optional metadata\\n+  /** Tags for categorization */\\n+  tags?: string[];\\n+  /** Events that can trigger this workflow */\\n+  on?: EventTrigger[];\\n+  /** Default configuration values for steps */\\n+  defaults?: Partial<CheckConfig>;\\n+  /** Category for organizing workflows */\\n+  category?: string;\\n+  /** Author information */\\n+  author?: {\\n+    name?: string;\\n+    email?: string;\\n+    url?: string;\\n+  };\\n+  /** License information */\\n+  license?: string;\\n+  /** Example usage */\\n+  examples?: WorkflowExample[];\\n+\\n+  /**\\n+   * Test checks for this workflow (only used when running standalone)\\n+   * These are NOT imported when the workflow is imported into another config\\n+   */\\n+  tests?: Record<string, CheckConfig>;\\n+}\\n+\\n+/**\\n+ * Example usage of a workflow\\n+ */\\n+export interface WorkflowExample {\\n+  /** Example name */\\n+  name: string;\\n+  /** Example description */\\n+  description?: string;\\n+  /** Input values for the example */\\n+  inputs: Record<string, unknown>;\\n+  /** Expected outputs (for documentation) */\\n+  expectedOutputs?: Record<string, unknown>;\\n+}\\n+\\n+/**\\n+ * Reference to a workflow in check configuration\\n+ */\\n+export interface WorkflowReference {\\n+  /** Workflow ID or path to import */\\n+  workflow: string;\\n+  /** Input parameter values */\\n+  inputs?: Record<string, unknown>;\\n+  /** Override specific step configurations */\\n+  overrides?: Record<string, Partial<CheckConfig>>;\\n+  /** Map workflow outputs to check outputs */\\n+  outputMapping?: Record<string, string>;\\n+}\\n+\\n+/**\\n+ * Workflow execution context\\n+ */\\n+export interface WorkflowExecutionContext {\\n+  /** Workflow instance ID */\\n+  instanceId: string;\\n+  /** Parent check ID if workflow is used as a step */\\n+  parentCheckId?: string;\\n+  /** Input values provided */\\n+  inputs: Record<string, unknown>;\\n+  /** Step results accumulated during execution */\\n+  stepResults: Map<string, unknown>;\\n+  /** Output values computed */\\n+  outputs?: Record<string, unknown>;\\n+  /** Execution metadata */\\n+  metadata?: {\\n+    startTime: number;\\n+    endTime?: number;\\n+    duration?: number;\\n+    status: 'running' | 'completed' | 'failed' | 'skipped';\\n+    error?: string;\\n+  };\\n+}\\n+\\n+/**\\n+ * Workflow validation result\\n+ */\\n+export interface WorkflowValidationResult {\\n+  /** Whether the workflow is valid */\\n+  valid: boolean;\\n+  /** Validation errors */\\n+  errors?: Array<{\\n+    path: string;\\n+    message: string;\\n+    value?: unknown;\\n+  }>;\\n+  /** Validation warnings */\\n+  warnings?: Array<{\\n+    path: string;\\n+    message: string;\\n+  }>;\\n+}\\n+\\n+/**\\n+ * Workflow registry entry\\n+ */\\n+export interface WorkflowRegistryEntry {\\n+  /** Workflow definition */\\n+  definition: WorkflowDefinition;\\n+  /** Source of the workflow (file path, URL, or 'inline') */\\n+  source: string;\\n+  /** When the workflow was registered */\\n+  registeredAt: Date;\\n+  /** Usage statistics */\\n+  usage?: {\\n+    count: number;\\n+    lastUsed?: Date;\\n+    averageDuration?: number;\\n+  };\\n+}\\n+\\n+/**\\n+ * Options for importing workflows\\n+ */\\n+export interface WorkflowImportOptions {\\n+  /** Base path for resolving relative imports */\\n+  basePath?: string;\\n+  /** Whether to validate workflows on import */\\n+  validate?: boolean;\\n+  /** Whether to override existing workflows */\\n+  override?: boolean;\\n+  /** Custom validators */\\n+  validators?: Array<(workflow: WorkflowDefinition) => WorkflowValidationResult>;\\n+}\\n+\\n+/**\\n+ * Workflow execution options\\n+ */\\n+export interface WorkflowExecutionOptions {\\n+  /** Maximum execution time in milliseconds */\\n+  timeout?: number;\\n+  /** Whether to continue on step failure */\\n+  continueOnError?: boolean;\\n+  /** Debug mode */\\n+  debug?: boolean;\\n+  /** Dry run - validate but don't execute */\\n+  dryRun?: boolean;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/utils/command-executor.ts\",\"additions\":6,\"deletions\":0,\"changes\":185,\"patch\":\"diff --git a/src/utils/command-executor.ts b/src/utils/command-executor.ts\\nnew file mode 100644\\nindex 00000000..b058f260\\n--- /dev/null\\n+++ b/src/utils/command-executor.ts\\n@@ -0,0 +1,185 @@\\n+import { exec } from 'child_process';\\n+import { promisify } from 'util';\\n+import { logger } from '../logger';\\n+\\n+export interface CommandExecutionOptions {\\n+  stdin?: string;\\n+  cwd?: string;\\n+  env?: Record<string, string>;\\n+  timeout?: number;\\n+}\\n+\\n+export interface CommandExecutionResult {\\n+  stdout: string;\\n+  stderr: string;\\n+  exitCode: number;\\n+}\\n+\\n+/**\\n+ * Shared utility for executing shell commands\\n+ * Used by both CommandCheckProvider and CustomToolExecutor\\n+ */\\n+export class CommandExecutor {\\n+  private static instance: CommandExecutor;\\n+\\n+  private constructor() {}\\n+\\n+  static getInstance(): CommandExecutor {\\n+    if (!CommandExecutor.instance) {\\n+      CommandExecutor.instance = new CommandExecutor();\\n+    }\\n+    return CommandExecutor.instance;\\n+  }\\n+\\n+  /**\\n+   * Execute a shell command with optional stdin, environment, and timeout\\n+   */\\n+  async execute(\\n+    command: string,\\n+    options: CommandExecutionOptions = {}\\n+  ): Promise<CommandExecutionResult> {\\n+    const execAsync = promisify(exec);\\n+    const timeout = options.timeout || 30000;\\n+\\n+    // If stdin is provided, we need to handle it differently\\n+    if (options.stdin) {\\n+      return this.executeWithStdin(command, options);\\n+    }\\n+\\n+    // For commands without stdin, use the simpler promisified version\\n+    try {\\n+      const result = await execAsync(command, {\\n+        cwd: options.cwd,\\n+        env: options.env as NodeJS.ProcessEnv,\\n+        timeout,\\n+      });\\n+\\n+      return {\\n+        stdout: result.stdout || '',\\n+        stderr: result.stderr || '',\\n+        exitCode: 0,\\n+      };\\n+    } catch (error) {\\n+      return this.handleExecutionError(error, timeout);\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Execute command with stdin input\\n+   */\\n+  private executeWithStdin(\\n+    command: string,\\n+    options: CommandExecutionOptions\\n+  ): Promise<CommandExecutionResult> {\\n+    return new Promise((resolve, reject) => {\\n+      const childProcess = exec(\\n+        command,\\n+        {\\n+          cwd: options.cwd,\\n+          env: options.env as NodeJS.ProcessEnv,\\n+          timeout: options.timeout || 30000,\\n+        },\\n+        (error, stdout, stderr) => {\\n+          // Check if the process was killed due to timeout\\n+          if (\\n+            error &&\\n+            error.killed &&\\n+            ((error as NodeJS.ErrnoException).code === 'ETIMEDOUT' || error.signal === 'SIGTERM')\\n+          ) {\\n+            reject(new Error(`Command timed out after ${options.timeout || 30000}ms`));\\n+          } else {\\n+            resolve({\\n+              stdout: stdout || '',\\n+              stderr: stderr || '',\\n+              exitCode: error ? error.code || 1 : 0,\\n+            });\\n+          }\\n+        }\\n+      );\\n+\\n+      // Write stdin and close\\n+      if (options.stdin && childProcess.stdin) {\\n+        childProcess.stdin.write(options.stdin);\\n+        childProcess.stdin.end();\\n+      }\\n+    });\\n+  }\\n+\\n+  /**\\n+   * Handle execution errors consistently\\n+   */\\n+  private handleExecutionError(error: unknown, timeout: number): CommandExecutionResult {\\n+    const execError = error as NodeJS.ErrnoException & {\\n+      stdout?: string;\\n+      stderr?: string;\\n+      killed?: boolean;\\n+      code?: string | number;\\n+      signal?: string;\\n+    };\\n+\\n+    // Check if the process was killed due to timeout\\n+    // Node.js sets killed: true and signal: 'SIGTERM' when timeout expires\\n+    if (execError.killed && (execError.code === 'ETIMEDOUT' || execError.signal === 'SIGTERM')) {\\n+      throw new Error(`Command timed out after ${timeout}ms`);\\n+    }\\n+\\n+    // Extract exit code - it might be a string or number\\n+    let exitCode = 1;\\n+    if (execError.code) {\\n+      exitCode = typeof execError.code === 'string' ? parseInt(execError.code, 10) : execError.code;\\n+    }\\n+\\n+    return {\\n+      stdout: execError.stdout || '',\\n+      stderr: execError.stderr || '',\\n+      exitCode,\\n+    };\\n+  }\\n+\\n+  /**\\n+   * Build safe environment variables by merging process.env with custom env\\n+   * Ensures all values are strings (no undefined)\\n+   */\\n+  buildEnvironment(\\n+    baseEnv: NodeJS.ProcessEnv = process.env,\\n+    ...customEnvs: Array<Record<string, string> | undefined>\\n+  ): Record<string, string> {\\n+    const result: Record<string, string> = {};\\n+\\n+    // Start with base environment, filtering out undefined values\\n+    for (const [key, value] of Object.entries(baseEnv)) {\\n+      if (value !== undefined) {\\n+        result[key] = value;\\n+      }\\n+    }\\n+\\n+    // Merge custom environments\\n+    for (const customEnv of customEnvs) {\\n+      if (customEnv) {\\n+        Object.assign(result, customEnv);\\n+      }\\n+    }\\n+\\n+    return result;\\n+  }\\n+\\n+  /**\\n+   * Log command execution for debugging\\n+   */\\n+  logExecution(command: string, options: CommandExecutionOptions): void {\\n+    const debugInfo = [\\n+      `Executing command: ${command}`,\\n+      options.cwd ? `cwd: ${options.cwd}` : null,\\n+      options.stdin ? 'with stdin' : null,\\n+      options.timeout ? `timeout: ${options.timeout}ms` : null,\\n+      options.env ? `env vars: ${Object.keys(options.env).length}` : null,\\n+    ]\\n+      .filter(Boolean)\\n+      .join(', ');\\n+\\n+    logger.debug(debugInfo);\\n+  }\\n+}\\n+\\n+// Export singleton instance for convenience\\n+export const commandExecutor = CommandExecutor.getInstance();\\n\",\"status\":\"added\"},{\"filename\":\"src/utils/config-loader.ts\",\"additions\":1,\"deletions\":1,\"changes\":30,\"patch\":\"diff --git a/src/utils/config-loader.ts b/src/utils/config-loader.ts\\nindex ef3c1aa0..be893e48 100644\\n--- a/src/utils/config-loader.ts\\n+++ b/src/utils/config-loader.ts\\n@@ -157,6 +157,13 @@ export class ConfigLoader {\\n         throw new Error(`Invalid YAML in configuration file: ${resolvedPath}`);\\n       }\\n \\n+      // Normalize 'include' (alias) to 'extends' for nested chains\\n+      if ((config as any).include && !(config as any).extends) {\\n+        const inc = (config as any).include;\\n+        (config as any).extends = Array.isArray(inc) ? inc : [inc];\\n+        delete (config as any).include;\\n+      }\\n+\\n       // Update base directory for nested extends\\n       const previousBaseDir = this.options.baseDir;\\n       this.options.baseDir = path.dirname(resolvedPath);\\n@@ -300,12 +307,26 @@ export class ConfigLoader {\\n         throw new Error('Invalid default configuration');\\n       }\\n \\n+      // Alias: support 'include' as 'extends' in packaged defaults\\n+      if ((config as any).include && !(config as any).extends) {\\n+        const inc = (config as any).include;\\n+        (config as any).extends = Array.isArray(inc) ? inc : [inc];\\n+        delete (config as any).include;\\n+      }\\n+\\n       // Normalize 'checks' and 'steps' for backward compatibility\\n       config = this.normalizeStepsAndChecks(config);\\n \\n       // Default configs shouldn't have extends, but handle it just in case\\n       if (config.extends) {\\n-        return await this.processExtends(config);\\n+        // Ensure relative paths (e.g., ./code-review.yaml) resolve from the defaults directory\\n+        const previousBaseDir = this.options.baseDir;\\n+        try {\\n+          this.options.baseDir = path.dirname(defaultConfigPath);\\n+          return await this.processExtends(config);\\n+        } finally {\\n+          this.options.baseDir = previousBaseDir;\\n+        }\\n       }\\n \\n       return config;\\n@@ -481,10 +502,11 @@ export class ConfigLoader {\\n    * Ensures both keys are present and contain the same data\\n    */\\n   private normalizeStepsAndChecks(config: Partial<VisorConfig>): Partial<VisorConfig> {\\n-    // If both are present, 'steps' takes precedence\\n+    // If both are present, merge with 'steps' taking precedence on key conflicts\\n     if (config.steps && config.checks) {\\n-      // Use steps as the source of truth\\n-      config.checks = config.steps;\\n+      const merged = { ...(config.checks as Record<string, unknown>), ...(config.steps as any) };\\n+      config.checks = merged as any;\\n+      config.steps = merged as any;\\n     } else if (config.steps && !config.checks) {\\n       // Copy steps to checks for internal compatibility\\n       config.checks = config.steps;\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/config-merger.ts\",\"additions\":1,\"deletions\":1,\"changes\":25,\"patch\":\"diff --git a/src/utils/config-merger.ts b/src/utils/config-merger.ts\\nindex b659a537..0b6085b3 100644\\n--- a/src/utils/config-merger.ts\\n+++ b/src/utils/config-merger.ts\\n@@ -40,8 +40,29 @@ export class ConfigMerger {\\n       result.checks = this.mergeChecks(parent.checks || {}, child.checks);\\n     }\\n \\n-    // Note: extends should not be in the final merged config\\n-    // It's only used during the loading process\\n+    // Merge steps as well (some configs provide only 'steps' and rely on normalization later)\\n+    // This preserves step definitions across extends chains before normalization.\\n+    if ((child as any).steps) {\\n+      const parentSteps = ((parent as any).steps || {}) as Record<string, CheckConfig>;\\n+      const childSteps = ((child as any).steps || {}) as Record<string, CheckConfig>;\\n+      (result as any).steps = this.mergeChecks(parentSteps, childSteps);\\n+    }\\n+\\n+    // Merge custom tools\\n+    if (child.tools) {\\n+      result.tools = this.mergeObjects(parent.tools || {}, child.tools);\\n+    }\\n+\\n+    // Merge workflow imports (concatenate arrays)\\n+    if (child.imports) {\\n+      const parentImports = parent.imports || [];\\n+      const childImports = child.imports || [];\\n+      // Combine and deduplicate\\n+      result.imports = [...new Set([...parentImports, ...childImports])];\\n+    }\\n+\\n+    // Note: extends/include should not be in the final merged config\\n+    // They are only used during the loading process\\n \\n     return result;\\n   }\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/interactive-prompt.ts\",\"additions\":7,\"deletions\":6,\"changes\":447,\"patch\":\"diff --git a/src/utils/interactive-prompt.ts b/src/utils/interactive-prompt.ts\\nindex 8228b1d9..d8e2f3ca 100644\\n--- a/src/utils/interactive-prompt.ts\\n+++ b/src/utils/interactive-prompt.ts\\n@@ -1,9 +1,32 @@\\n /**\\n- * Interactive terminal prompting with beautiful UI\\n+ * Interactive terminal prompting (minimal TTY UI)\\n  */\\n \\n import * as readline from 'readline';\\n \\n+// Global, process-wide guard to ensure we never open two readline prompts at once.\\n+// This is crucial because the engine may (due to routing) attempt to schedule\\n+// a second human-input step while the first is still waiting. Two concurrent\\n+// readline instances on the same TTY cause duplicated keystrokes and other\\n+// erratic behavior. We serialize prompts with a tiny async mutex.\\n+let activePrompt = false;\\n+const waiters: Array<() => void> = [];\\n+\\n+async function acquirePromptLock(): Promise<void> {\\n+  if (!activePrompt) {\\n+    activePrompt = true;\\n+    return;\\n+  }\\n+  await new Promise<void>(resolve => waiters.push(resolve));\\n+  activePrompt = true;\\n+}\\n+\\n+function releasePromptLock(): void {\\n+  activePrompt = false;\\n+  const next = waiters.shift();\\n+  if (next) next();\\n+}\\n+\\n export interface PromptOptions {\\n   /** The prompt text to display */\\n   prompt: string;\\n@@ -18,252 +41,236 @@ export interface PromptOptions {\\n   /** Allow empty input */\\n   allowEmpty?: boolean;\\n }\\n-\\n-// ANSI color codes\\n-const colors = {\\n-  reset: '\\\\x1b[0m',\\n-  dim: '\\\\x1b[2m',\\n-  bold: '\\\\x1b[1m',\\n-  cyan: '\\\\x1b[36m',\\n-  green: '\\\\x1b[32m',\\n-  yellow: '\\\\x1b[33m',\\n-  gray: '\\\\x1b[90m',\\n-};\\n-\\n-// Box drawing characters (with ASCII fallback)\\n-const supportsUnicode = process.env.LANG?.includes('UTF-8') || process.platform === 'darwin';\\n-\\n-const box = supportsUnicode\\n-  ? {\\n-      topLeft: '‚îå',\\n-      topRight: '‚îê',\\n-      bottomLeft: '‚îî',\\n-      bottomRight: '‚îò',\\n-      horizontal: '‚îÄ',\\n-      vertical: '‚îÇ',\\n-      leftT: '‚îú',\\n-      rightT: '‚î§',\\n-    }\\n-  : {\\n-      topLeft: '+',\\n-      topRight: '+',\\n-      bottomLeft: '+',\\n-      bottomRight: '+',\\n-      horizontal: '-',\\n-      vertical: '|',\\n-      leftT: '+',\\n-      rightT: '+',\\n-    };\\n-\\n-/**\\n- * Format time in mm:ss\\n- */\\n-function formatTime(ms: number): string {\\n-  const seconds = Math.ceil(ms / 1000);\\n-  const mins = Math.floor(seconds / 60);\\n-  const secs = seconds % 60;\\n-  return `${mins}:${secs.toString().padStart(2, '0')}`;\\n-}\\n-\\n-/**\\n- * Draw a horizontal line\\n- */\\n-function drawLine(char: string, width: number): string {\\n-  return char.repeat(width);\\n-}\\n-\\n-/**\\n- * Wrap text to fit within a given width\\n- */\\n-function wrapText(text: string, width: number): string[] {\\n-  const words = text.split(' ');\\n-  const lines: string[] = [];\\n-  let currentLine = '';\\n-\\n-  for (const word of words) {\\n-    if (currentLine.length + word.length + 1 <= width) {\\n-      currentLine += (currentLine ? ' ' : '') + word;\\n-    } else {\\n-      if (currentLine) lines.push(currentLine);\\n-      currentLine = word;\\n-    }\\n-  }\\n-  if (currentLine) lines.push(currentLine);\\n-\\n-  return lines;\\n-}\\n-\\n-/**\\n- * Display the prompt UI\\n- */\\n-function displayPromptUI(options: PromptOptions, remainingMs?: number): void {\\n-  const width = Math.min(process.stdout.columns || 80, 80) - 4;\\n-  const icon = supportsUnicode ? 'üí¨' : '>';\\n-\\n-  console.log('\\\\n'); // Add some spacing\\n-\\n-  // Top border\\n-  console.log(`${box.topLeft}${drawLine(box.horizontal, width + 2)}${box.topRight}`);\\n-\\n-  // Title\\n-  console.log(\\n-    `${box.vertical} ${colors.bold}${icon} Human Input Required${colors.reset}${' '.repeat(\\n-      width - 22\\n-    )} ${box.vertical}`\\n-  );\\n-\\n-  // Separator\\n-  console.log(`${box.leftT}${drawLine(box.horizontal, width + 2)}${box.rightT}`);\\n-\\n-  // Empty line\\n-  console.log(`${box.vertical} ${' '.repeat(width)} ${box.vertical}`);\\n-\\n-  // Prompt text (wrapped)\\n-  const promptLines = wrapText(options.prompt, width - 2);\\n-  for (const line of promptLines) {\\n-    console.log(\\n-      `${box.vertical} ${colors.cyan}${line}${colors.reset}${' '.repeat(\\n-        width - line.length\\n-      )} ${box.vertical}`\\n-    );\\n-  }\\n-\\n-  // Empty line\\n-  console.log(`${box.vertical} ${' '.repeat(width)} ${box.vertical}`);\\n-\\n-  // Instructions\\n-  const instruction = options.multiline\\n-    ? '(Type your response, press Ctrl+D when done)'\\n-    : '(Type your response and press Enter)';\\n-  console.log(\\n-    `${box.vertical} ${colors.dim}${instruction}${colors.reset}${' '.repeat(\\n-      width - instruction.length\\n-    )} ${box.vertical}`\\n-  );\\n-\\n-  // Placeholder if provided\\n-  if (options.placeholder && !options.multiline) {\\n-    console.log(\\n-      `${box.vertical} ${colors.dim}${options.placeholder}${colors.reset}${' '.repeat(\\n-        width - options.placeholder.length\\n-      )} ${box.vertical}`\\n-    );\\n-  }\\n-\\n-  // Empty line\\n-  console.log(`${box.vertical} ${' '.repeat(width)} ${box.vertical}`);\\n-\\n-  // Timeout indicator\\n-  if (remainingMs !== undefined && options.timeout) {\\n-    const timeIcon = supportsUnicode ? '‚è± ' : 'Time: ';\\n-    const timeStr = `${timeIcon} ${formatTime(remainingMs)} remaining`;\\n-    console.log(\\n-      `${box.vertical} ${colors.yellow}${timeStr}${colors.reset}${' '.repeat(\\n-        width - timeStr.length\\n-      )} ${box.vertical}`\\n-    );\\n-  }\\n-\\n-  // Bottom border\\n-  console.log(`${box.bottomLeft}${drawLine(box.horizontal, width + 2)}${box.bottomRight}`);\\n-\\n-  console.log(''); // Empty line before input\\n-  process.stdout.write(`${colors.green}>${colors.reset} `);\\n-}\\n-\\n /**\\n  * Prompt user for input with a beautiful interactive UI\\n  */\\n export async function interactivePrompt(options: PromptOptions): Promise<string> {\\n+  await acquirePromptLock();\\n   return new Promise((resolve, reject) => {\\n-    let input = '';\\n-    let timeoutId: NodeJS.Timeout | undefined;\\n-    let countdownInterval: NodeJS.Timeout | undefined;\\n-    let remainingMs = options.timeout;\\n+    const dbg = process.env.VISOR_DEBUG === 'true';\\n+    try {\\n+      if (dbg) {\\n+        const counts: Record<string, number> = {\\n+          data: process.stdin.listenerCount('data'),\\n+          end: process.stdin.listenerCount('end'),\\n+          error: process.stdin.listenerCount('error'),\\n+          readable: process.stdin.listenerCount('readable'),\\n+          close: process.stdin.listenerCount('close'),\\n+        } as any;\\n+        console.error(\\n+          `[human-input] starting prompt: isTTY=${!!process.stdin.isTTY} active=${activePrompt} waiters=${waiters.length} listeners=${JSON.stringify(counts)}`\\n+        );\\n+      }\\n+    } catch {}\\n+    // Ensure stdin is in a sane state for a fresh interactive session\\n+    try {\\n+      if (process.stdin.isTTY && typeof (process.stdin as any).setRawMode === 'function') {\\n+        // We use line-based input; disable raw mode just in case\\n+        (process.stdin as any).setRawMode(false);\\n+      }\\n+      // Always resume stdin before creating the interface\\n+      process.stdin.resume();\\n+    } catch {}\\n \\n-    const rl = readline.createInterface({\\n-      input: process.stdin,\\n-      output: process.stdout,\\n-      terminal: true,\\n-    });\\n+    // Ensure encoding is set for predictable behavior\\n+    try {\\n+      process.stdin.setEncoding('utf8');\\n+    } catch {}\\n+\\n+    let rl: readline.Interface | undefined;\\n \\n-    // Display initial UI\\n-    displayPromptUI(options, remainingMs);\\n+    const allowEmpty = options.allowEmpty ?? false;\\n+    const multiline = options.multiline ?? false;\\n+    const defaultValue = options.defaultValue;\\n \\n+    let timeoutId: NodeJS.Timeout | undefined;\\n     const cleanup = () => {\\n       if (timeoutId) clearTimeout(timeoutId);\\n-      if (countdownInterval) clearInterval(countdownInterval);\\n-      rl.close();\\n+      try {\\n+        rl?.removeAllListeners();\\n+      } catch {}\\n+      try {\\n+        rl?.close();\\n+      } catch {}\\n+      // Hardening: make sure no stray listeners remain on stdin between loops\\n+      // Do not blanket-remove listeners from process.stdin; a fresh readline\\n+      // instance will manage its own listeners. Over-removing here can leave\\n+      // the next interface in a bad state (no keypress events).\\n+      try {\\n+        if (process.stdin.isTTY && typeof (process.stdin as any).setRawMode === 'function') {\\n+          (process.stdin as any).setRawMode(false);\\n+        }\\n+      } catch {}\\n+      try {\\n+        process.stdin.pause();\\n+      } catch {}\\n+      // Release the global lock so a queued prompt (if any) may proceed\\n+      try {\\n+        releasePromptLock();\\n+      } catch {}\\n+      // If stdout/stderr were temporarily wrapped by the question handler, restore them now\\n+      try {\\n+        if ((process.stdout as any).__restoreWrites) {\\n+          (process.stdout as any).__restoreWrites();\\n+        }\\n+      } catch {}\\n+      try {\\n+        if ((process.stderr as any).__restoreWrites) {\\n+          (process.stderr as any).__restoreWrites();\\n+        }\\n+      } catch {}\\n+      try {\\n+        if (dbg) {\\n+          const counts: Record<string, number> = {\\n+            data: process.stdin.listenerCount('data'),\\n+            end: process.stdin.listenerCount('end'),\\n+            error: process.stdin.listenerCount('error'),\\n+            readable: process.stdin.listenerCount('readable'),\\n+            close: process.stdin.listenerCount('close'),\\n+          } as any;\\n+          console.error(\\n+            `[human-input] cleanup: isTTY=${!!process.stdin.isTTY} active=false waiters=${waiters.length} listeners=${JSON.stringify(counts)}`\\n+          );\\n+        }\\n+      } catch {}\\n     };\\n-\\n     const finish = (value: string) => {\\n       cleanup();\\n-      console.log(''); // New line after input\\n       resolve(value);\\n     };\\n \\n-    // Setup timeout if specified\\n-    if (options.timeout) {\\n+    // Optional timeout (no default)\\n+    if (options.timeout && options.timeout > 0) {\\n       timeoutId = setTimeout(() => {\\n         cleanup();\\n-        console.log(`\\\\n${colors.yellow}‚è±  Timeout reached${colors.reset}`);\\n-        if (options.defaultValue !== undefined) {\\n-          console.log(\\n-            `${colors.gray}Using default value: ${options.defaultValue}${colors.reset}\\\\n`\\n-          );\\n-          resolve(options.defaultValue);\\n-        } else {\\n-          reject(new Error('Input timeout'));\\n-        }\\n+        if (defaultValue !== undefined) return resolve(defaultValue);\\n+        return reject(new Error('Input timeout'));\\n       }, options.timeout);\\n-\\n-      // Update countdown every second\\n-      if (remainingMs) {\\n-        countdownInterval = setInterval(() => {\\n-          remainingMs = remainingMs! - 1000;\\n-          if (remainingMs <= 0) {\\n-            if (countdownInterval) clearInterval(countdownInterval);\\n-          }\\n-        }, 1000);\\n-      }\\n     }\\n \\n-    if (options.multiline) {\\n-      // Multiline mode: collect lines until EOF (Ctrl+D)\\n+    // Print minimal header with dashed separators\\n+    const header: string[] = [];\\n+    if (options.prompt && options.prompt.trim()) header.push(options.prompt.trim());\\n+    if (multiline) header.push('(Ctrl+D to submit)');\\n+    if (options.placeholder && !multiline) header.push(options.placeholder);\\n+    const width = Math.max(\\n+      20,\\n+      Math.min((process.stdout && (process.stdout as any).columns) || 80, 100)\\n+    );\\n+    const dash = '-'.repeat(width);\\n+    try {\\n+      console.log('\\\\n' + dash);\\n+      if (header.length) console.log(header.join('\\\\n'));\\n+      console.log(dash);\\n+    } catch {}\\n+\\n+    // No echo-suppression hacks ‚Äî we fix the root cause below by using raw-mode\\n+    // input for single-line prompts, so the terminal never replays the line.\\n+\\n+    if (multiline) {\\n+      rl = readline.createInterface({\\n+        input: process.stdin,\\n+        output: process.stdout,\\n+        terminal: true,\\n+      });\\n+      let buf = '';\\n+      process.stdout.write('> ');\\n       rl.on('line', line => {\\n-        input += (input ? '\\\\n' : '') + line;\\n+        buf += (buf ? '\\\\n' : '') + line;\\n+        process.stdout.write('> ');\\n       });\\n-\\n       rl.on('close', () => {\\n-        cleanup();\\n-        const trimmed = input.trim();\\n-        if (!trimmed && !options.allowEmpty) {\\n-          console.log(`${colors.yellow}‚ö†  Empty input not allowed${colors.reset}`);\\n-          reject(new Error('Empty input not allowed'));\\n-        } else {\\n-          finish(trimmed);\\n+        const trimmed = buf.trim();\\n+        if (!trimmed && !allowEmpty && defaultValue === undefined) {\\n+          return reject(new Error('Empty input not allowed'));\\n         }\\n+        return finish(trimmed || defaultValue || '');\\n+      });\\n+      rl.on('SIGINT', () => {\\n+        try {\\n+          // Print a clean newline and exit immediately with 130 (SIGINT)\\n+          process.stdout.write('\\\\n');\\n+        } catch {}\\n+        cleanup();\\n+        process.exit(130);\\n       });\\n     } else {\\n-      // Single line mode\\n-      rl.question('', answer => {\\n-        const trimmed = answer.trim();\\n-        if (!trimmed && !options.allowEmpty && !options.defaultValue) {\\n+      // Root cause fix: raw-mode single-line input without readline echo.\\n+      const readLineRaw = async (): Promise<string> => {\\n+        return new Promise<string>(resolveRaw => {\\n+          let buf = '';\\n+          const onData = (chunk: Buffer) => {\\n+            const s = chunk.toString('utf8');\\n+            for (let i = 0; i < s.length; i++) {\\n+              const ch = s[i];\\n+              const code = s.charCodeAt(i);\\n+              if (ch === '\\\\n' || ch === '\\\\r') {\\n+                try {\\n+                  process.stdout.write('\\\\n');\\n+                } catch {}\\n+                teardown();\\n+                resolveRaw(buf);\\n+                return;\\n+              }\\n+              if (ch === '\\\\b' || code === 127) {\\n+                if (buf.length > 0) {\\n+                  buf = buf.slice(0, -1);\\n+                  try {\\n+                    process.stdout.write('\\\\b \\\\b');\\n+                  } catch {}\\n+                }\\n+                continue;\\n+              }\\n+              if (code === 3) {\\n+                // Ctrl+C\\n+                try {\\n+                  process.stdout.write('\\\\n');\\n+                } catch {}\\n+                teardown();\\n+                process.exit(130);\\n+              }\\n+              if (code >= 32) {\\n+                buf += ch;\\n+                try {\\n+                  process.stdout.write(ch);\\n+                } catch {}\\n+              }\\n+            }\\n+          };\\n+          const teardown = () => {\\n+            try {\\n+              process.stdin.off('data', onData);\\n+            } catch {}\\n+            try {\\n+              if (process.stdin.isTTY && typeof (process.stdin as any).setRawMode === 'function') {\\n+                (process.stdin as any).setRawMode(false);\\n+              }\\n+            } catch {}\\n+          };\\n+          try {\\n+            if (process.stdin.isTTY && typeof (process.stdin as any).setRawMode === 'function') {\\n+              (process.stdin as any).setRawMode(true);\\n+            }\\n+          } catch {}\\n+          process.stdin.on('data', onData);\\n+          try {\\n+            process.stdout.write('> ');\\n+          } catch {}\\n+        });\\n+      };\\n+      (async () => {\\n+        const answer = await readLineRaw();\\n+        const trimmed = (answer || '').trim();\\n+        if (!trimmed && !allowEmpty && defaultValue === undefined) {\\n           cleanup();\\n-          console.log(`${colors.yellow}‚ö†  Empty input not allowed${colors.reset}`);\\n-          reject(new Error('Empty input not allowed'));\\n-        } else {\\n-          finish(trimmed || options.defaultValue || '');\\n+          return reject(new Error('Empty input not allowed'));\\n         }\\n+        return finish(trimmed || defaultValue || '');\\n+      })().catch(err => {\\n+        cleanup();\\n+        reject(err);\\n       });\\n     }\\n-\\n-    // Handle Ctrl+C\\n-    rl.on('SIGINT', () => {\\n-      cleanup();\\n-      console.log('\\\\n\\\\n' + colors.yellow + '‚ö†  Cancelled by user' + colors.reset);\\n-      reject(new Error('Cancelled by user'));\\n-    });\\n   });\\n }\\n \\n@@ -277,6 +284,14 @@ export async function simplePrompt(prompt: string): Promise<string> {\\n       output: process.stdout,\\n     });\\n \\n+    rl.on('SIGINT', () => {\\n+      try {\\n+        process.stdout.write('\\\\n');\\n+      } catch {}\\n+      rl.close();\\n+      process.exit(130);\\n+    });\\n+\\n     rl.question(`${prompt}\\\\n> `, answer => {\\n       rl.close();\\n       resolve(answer.trim());\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/sandbox.ts\",\"additions\":1,\"deletions\":1,\"changes\":44,\"patch\":\"diff --git a/src/utils/sandbox.ts b/src/utils/sandbox.ts\\nindex d2010fb1..9914a3a5 100644\\n--- a/src/utils/sandbox.ts\\n+++ b/src/utils/sandbox.ts\\n@@ -24,12 +24,24 @@ export function createSecureSandbox(): Sandbox {\\n     ...Sandbox.SAFE_GLOBALS,\\n     Math,\\n     JSON,\\n-    // Provide console with limited surface. Calls are harmless in CI logs and\\n-    // help with debugging value_js / transform_js expressions.\\n+    // Provide console with limited surface. Use trampolines so that any test\\n+    // spies (e.g., jest.spyOn(console, 'log')) see calls made inside the sandbox.\\n     console: {\\n-      log: console.log,\\n-      warn: console.warn,\\n-      error: console.error,\\n+      log: (...args: unknown[]) => {\\n+        try {\\n+          (console as any).log(...args);\\n+        } catch {}\\n+      },\\n+      warn: (...args: unknown[]) => {\\n+        try {\\n+          (console as any).warn(...args);\\n+        } catch {}\\n+      },\\n+      error: (...args: unknown[]) => {\\n+        try {\\n+          (console as any).error(...args);\\n+        } catch {}\\n+      },\\n     },\\n   } as Record<string, unknown>;\\n \\n@@ -185,9 +197,27 @@ export function compileAndRun<T = unknown>(\\n   const header = inject\\n     ? `const __lp = ${JSON.stringify(safePrefix)}; const log = (...a) => { try { console.log(__lp, ...a); } catch {} };\\\\n`\\n     : '';\\n+  // When wrapping, execute user code inside an IIFE and return its value.\\n+  // This reliably captures the value of the last expression or any explicit\\n+  // return statements inside the script, without requiring the caller to\\n+  // manually `return` at top level.\\n+  // Wrapper heuristic:\\n+  // - If the snippet contains an explicit `return`, semicolons or newlines (likely a block),\\n+  //   run it inside an IIFE so `return` works:  (() => { code })()\\n+  // - Otherwise treat it as a pure expression and return its value directly.\\n+  const src = String(userCode);\\n+  const looksLikeBlock = /\\\\breturn\\\\b/.test(src) || /;/.test(src) || /\\\\n/.test(src);\\n+  // Heuristic: if the snippet itself looks like an IIFE/callable expression\\n+  // (e.g., `(() => { ... })()` or `(function(){ ... })()`), return its value\\n+  // directly to avoid swallowing the result by nesting it inside another block.\\n+  const looksLikeIife = /\\\\)\\\\s*\\\\(\\\\s*\\\\)\\\\s*;?$/.test(src.trim());\\n   const body = opts.wrapFunction\\n-    ? `const __fn = () => {\\\\n${userCode}\\\\n};\\\\nreturn __fn();\\\\n`\\n-    : `${userCode}`;\\n+    ? looksLikeBlock\\n+      ? looksLikeIife\\n+        ? `return (\\\\n${src}\\\\n);\\\\n`\\n+        : `return (() => {\\\\n${src}\\\\n})();\\\\n`\\n+      : `return (\\\\n${src}\\\\n);\\\\n`\\n+    : `${src}`;\\n   const code = `${header}${body}`;\\n   let exec: ReturnType<typeof sandbox.compile>;\\n   try {\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/template-context.ts\",\"additions\":1,\"deletions\":1,\"changes\":3,\"patch\":\"diff --git a/src/utils/template-context.ts b/src/utils/template-context.ts\\nindex 9af9baef..17118e36 100644\\n--- a/src/utils/template-context.ts\\n+++ b/src/utils/template-context.ts\\n@@ -70,7 +70,8 @@ export function buildProviderTemplateContext(\\n         const name = checkName.slice(0, -4);\\n         outputsRaw[name] = summary.output !== undefined ? summary.output : summary;\\n       } else {\\n-        outputs[checkName] = summary.output !== undefined ? summary.output : summary;\\n+        const extracted = summary.output !== undefined ? summary.output : summary;\\n+        outputs[checkName] = extracted;\\n       }\\n     }\\n   }\\n\",\"status\":\"modified\"},{\"filename\":\"src/webhook-server.ts\",\"additions\":1,\"deletions\":1,\"changes\":8,\"patch\":\"diff --git a/src/webhook-server.ts b/src/webhook-server.ts\\nindex a2c6fb1f..5480d5e0 100644\\n--- a/src/webhook-server.ts\\n+++ b/src/webhook-server.ts\\n@@ -5,7 +5,7 @@ import * as crypto from 'crypto';\\n import { HttpServerConfig, VisorConfig } from './types/config';\\n import { Liquid } from 'liquidjs';\\n import { createExtendedLiquid } from './liquid-extensions';\\n-import { CheckExecutionEngine } from './check-execution-engine';\\n+import { StateMachineExecutionEngine } from './state-machine-execution-engine';\\n \\n export interface WebhookPayload {\\n   endpoint: string;\\n@@ -26,7 +26,7 @@ export class WebhookServer {\\n   private config: HttpServerConfig;\\n   private liquid: Liquid;\\n   private webhookData: Map<string, unknown> = new Map();\\n-  private executionEngine?: CheckExecutionEngine;\\n+  private executionEngine?: StateMachineExecutionEngine;\\n   private visorConfig?: VisorConfig;\\n   private isGitHubActions: boolean;\\n \\n@@ -42,7 +42,7 @@ export class WebhookServer {\\n   /**\\n    * Set the execution engine for triggering checks on webhook receipt\\n    */\\n-  public setExecutionEngine(engine: CheckExecutionEngine): void {\\n+  public setExecutionEngine(engine: StateMachineExecutionEngine): void {\\n     this.executionEngine = engine;\\n   }\\n \\n@@ -508,7 +508,7 @@ export class WebhookServer {\\n export function createWebhookServer(\\n   config: HttpServerConfig,\\n   visorConfig?: VisorConfig,\\n-  executionEngine?: CheckExecutionEngine\\n+  executionEngine?: StateMachineExecutionEngine\\n ): WebhookServer {\\n   const server = new WebhookServer(config, visorConfig);\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-executor.ts\",\"additions\":14,\"deletions\":0,\"changes\":488,\"patch\":\"diff --git a/src/workflow-executor.ts b/src/workflow-executor.ts\\nnew file mode 100644\\nindex 00000000..751e7f16\\n--- /dev/null\\n+++ b/src/workflow-executor.ts\\n@@ -0,0 +1,488 @@\\n+/**\\n+ * Workflow executor for running workflow definitions\\n+ */\\n+\\n+import {\\n+  WorkflowDefinition,\\n+  WorkflowExecutionContext,\\n+  WorkflowStep,\\n+  WorkflowInputMapping,\\n+  WorkflowExecutionOptions,\\n+} from './types/workflow';\\n+import { PRInfo } from './pr-analyzer';\\n+import { ReviewSummary } from './reviewer';\\n+import { CheckProviderRegistry } from './providers/check-provider-registry';\\n+import { CheckProviderConfig, ExecutionContext } from './providers/check-provider.interface';\\n+import { DependencyResolver } from './dependency-resolver';\\n+import { logger } from './logger';\\n+import { createSecureSandbox, compileAndRun } from './utils/sandbox';\\n+import { Liquid } from 'liquidjs';\\n+\\n+/**\\n+ * Workflow execution result\\n+ */\\n+export interface WorkflowExecutionResult {\\n+  success: boolean;\\n+  score?: number;\\n+  confidence?: 'high' | 'medium' | 'low';\\n+  issues?: any[];\\n+  comments?: any[];\\n+  output?: Record<string, unknown>;\\n+  status: 'completed' | 'failed' | 'skipped';\\n+  duration?: number;\\n+  error?: string;\\n+  stepSummaries?: Array<{\\n+    stepId: string;\\n+    status: 'success' | 'failed' | 'skipped';\\n+    issues?: any[];\\n+    output?: unknown;\\n+  }>;\\n+}\\n+\\n+/**\\n+ * Execution options passed to workflow executor\\n+ */\\n+interface WorkflowRunOptions {\\n+  prInfo: PRInfo;\\n+  dependencyResults?: Map<string, ReviewSummary>;\\n+  context?: ExecutionContext;\\n+  options?: WorkflowExecutionOptions;\\n+}\\n+\\n+/**\\n+ * Executes workflow definitions\\n+ */\\n+export class WorkflowExecutor {\\n+  private providerRegistry: CheckProviderRegistry | null = null;\\n+  private liquid: Liquid;\\n+\\n+  constructor() {\\n+    // Don't call CheckProviderRegistry.getInstance() here to avoid circular dependency\\n+    // during registry initialization (since WorkflowCheckProvider is registered in the registry)\\n+    this.liquid = new Liquid();\\n+  }\\n+\\n+  /**\\n+   * Lazy-load the provider registry to avoid circular dependency during initialization\\n+   */\\n+  private getProviderRegistry(): CheckProviderRegistry {\\n+    if (!this.providerRegistry) {\\n+      this.providerRegistry = CheckProviderRegistry.getInstance();\\n+    }\\n+    return this.providerRegistry;\\n+  }\\n+\\n+  /**\\n+   * Execute a workflow\\n+   */\\n+  public async execute(\\n+    workflow: WorkflowDefinition,\\n+    executionContext: WorkflowExecutionContext,\\n+    runOptions: WorkflowRunOptions\\n+  ): Promise<WorkflowExecutionResult> {\\n+    const startTime = Date.now();\\n+    executionContext.metadata = {\\n+      startTime,\\n+      status: 'running',\\n+    };\\n+\\n+    try {\\n+      // Resolve step execution order\\n+      const executionOrder = this.resolveExecutionOrder(workflow);\\n+      logger.debug(`Workflow ${workflow.id} execution order: ${executionOrder.join(' -> ')}`);\\n+\\n+      // Execute steps in order\\n+      const stepResults = new Map<string, ReviewSummary>();\\n+      const stepSummaries: Array<{\\n+        stepId: string;\\n+        status: 'success' | 'failed' | 'skipped';\\n+        issues?: any[];\\n+        output?: unknown;\\n+      }> = [];\\n+\\n+      for (const stepId of executionOrder) {\\n+        const step = workflow.steps[stepId];\\n+\\n+        // Check if step should be executed (evaluate 'if' condition)\\n+        if (step.if) {\\n+          const shouldRun = this.evaluateCondition(step.if, {\\n+            inputs: executionContext.inputs,\\n+            outputs: Object.fromEntries(stepResults),\\n+            pr: runOptions.prInfo,\\n+          });\\n+\\n+          if (!shouldRun) {\\n+            logger.info(`Skipping step '${stepId}' due to condition: ${step.if}`);\\n+            stepSummaries.push({\\n+              stepId,\\n+              status: 'skipped',\\n+            });\\n+            continue;\\n+          }\\n+        }\\n+\\n+        // Prepare step configuration\\n+        const stepConfig = await this.prepareStepConfig(\\n+          step,\\n+          stepId,\\n+          executionContext,\\n+          stepResults,\\n+          workflow\\n+        );\\n+\\n+        // Execute the step\\n+        try {\\n+          logger.info(`Executing workflow step '${stepId}'`);\\n+          // Extend context with workflow inputs\\n+          const stepContext: ExecutionContext = {\\n+            ...runOptions.context,\\n+            workflowInputs: executionContext.inputs,\\n+          };\\n+          const result = await this.executeStep(\\n+            stepConfig,\\n+            runOptions.prInfo,\\n+            stepResults,\\n+            stepContext\\n+          );\\n+\\n+          stepResults.set(stepId, result);\\n+          stepSummaries.push({\\n+            stepId,\\n+            status: 'success',\\n+            issues: result.issues,\\n+            output: (result as any).output,\\n+          });\\n+        } catch (error) {\\n+          const errorMessage = error instanceof Error ? error.message : String(error);\\n+          logger.error(`Step '${stepId}' failed: ${errorMessage}`);\\n+\\n+          stepSummaries.push({\\n+            stepId,\\n+            status: 'failed',\\n+            output: { error: errorMessage },\\n+          });\\n+\\n+          if (!runOptions.options?.continueOnError) {\\n+            throw new Error(`Workflow step '${stepId}' failed: ${errorMessage}`);\\n+          }\\n+        }\\n+      }\\n+\\n+      // Compute workflow outputs\\n+      const outputs = await this.computeOutputs(\\n+        workflow,\\n+        executionContext,\\n+        stepResults,\\n+        runOptions.prInfo\\n+      );\\n+      executionContext.outputs = outputs;\\n+\\n+      // Aggregate results\\n+      const aggregated = this.aggregateResults(stepResults);\\n+\\n+      const endTime = Date.now();\\n+      executionContext.metadata.endTime = endTime;\\n+      executionContext.metadata.duration = endTime - startTime;\\n+      executionContext.metadata.status = 'completed';\\n+\\n+      return {\\n+        success: true,\\n+        score: aggregated.score,\\n+        confidence: aggregated.confidence,\\n+        issues: aggregated.issues,\\n+        comments: aggregated.comments,\\n+        output: outputs,\\n+        status: 'completed',\\n+        duration: endTime - startTime,\\n+        stepSummaries,\\n+      };\\n+    } catch (error) {\\n+      const endTime = Date.now();\\n+      executionContext.metadata.endTime = endTime;\\n+      executionContext.metadata.duration = endTime - startTime;\\n+      executionContext.metadata.status = 'failed';\\n+      executionContext.metadata.error = error instanceof Error ? error.message : String(error);\\n+\\n+      return {\\n+        success: false,\\n+        status: 'failed',\\n+        duration: endTime - startTime,\\n+        error: error instanceof Error ? error.message : String(error),\\n+      };\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Resolve step execution order based on dependencies\\n+   */\\n+  private resolveExecutionOrder(workflow: WorkflowDefinition): string[] {\\n+    // Build dependency map\\n+    const dependencies: Record<string, string[]> = {};\\n+    for (const [stepId, step] of Object.entries(workflow.steps)) {\\n+      dependencies[stepId] = step.depends_on || [];\\n+    }\\n+\\n+    // Use static DependencyResolver\\n+    const graph = DependencyResolver.buildDependencyGraph(dependencies);\\n+\\n+    if (graph.hasCycles) {\\n+      throw new Error(\\n+        `Circular dependency detected in workflow steps: ${graph.cycleNodes?.join(' -> ')}`\\n+      );\\n+    }\\n+\\n+    // Flatten execution groups to get linear order\\n+    const order: string[] = [];\\n+    for (const group of graph.executionOrder) {\\n+      order.push(...group.parallel);\\n+    }\\n+\\n+    return order;\\n+  }\\n+\\n+  /**\\n+   * Prepare step configuration with input mappings\\n+   */\\n+  private async prepareStepConfig(\\n+    step: WorkflowStep,\\n+    stepId: string,\\n+    executionContext: WorkflowExecutionContext,\\n+    stepResults: Map<string, ReviewSummary>,\\n+    workflow: WorkflowDefinition\\n+  ): Promise<CheckProviderConfig> {\\n+    const config: CheckProviderConfig = {\\n+      ...step,\\n+      type: step.type || 'ai',\\n+      checkName: `${executionContext.instanceId}:${stepId}`,\\n+    };\\n+\\n+    // Process input mappings\\n+    if (step.inputs) {\\n+      for (const [inputName, mapping] of Object.entries(step.inputs)) {\\n+        const value = await this.resolveInputMapping(\\n+          mapping,\\n+          executionContext,\\n+          stepResults,\\n+          workflow\\n+        );\\n+        (config as any)[inputName] = value;\\n+      }\\n+    }\\n+\\n+    return config;\\n+  }\\n+\\n+  /**\\n+   * Resolve input mapping to actual value\\n+   */\\n+  private async resolveInputMapping(\\n+    mapping: string | WorkflowInputMapping,\\n+    executionContext: WorkflowExecutionContext,\\n+    stepResults: Map<string, ReviewSummary>,\\n+    _workflow: WorkflowDefinition\\n+  ): Promise<unknown> {\\n+    // Simple string mapping - treat as parameter reference\\n+    if (typeof mapping === 'string') {\\n+      return executionContext.inputs[mapping];\\n+    }\\n+\\n+    // Complex mapping\\n+    if (typeof mapping === 'object' && mapping !== null && 'source' in mapping) {\\n+      const typedMapping = mapping as WorkflowInputMapping;\\n+\\n+      switch (typedMapping.source) {\\n+        case 'param':\\n+          // Reference to workflow input parameter\\n+          return executionContext.inputs[String(typedMapping.value)];\\n+\\n+        case 'step':\\n+          // Reference to another step's output\\n+          if (!typedMapping.stepId) {\\n+            throw new Error('Step input mapping requires stepId');\\n+          }\\n+          const stepResult = stepResults.get(typedMapping.stepId);\\n+          if (!stepResult) {\\n+            throw new Error(`Step '${typedMapping.stepId}' has not been executed yet`);\\n+          }\\n+          const output = (stepResult as any).output;\\n+          if (typedMapping.outputParam && output) {\\n+            return output[typedMapping.outputParam];\\n+          }\\n+          return output;\\n+\\n+        case 'constant':\\n+          // Constant value\\n+          return typedMapping.value;\\n+\\n+        case 'expression':\\n+          // JavaScript expression\\n+          if (!typedMapping.expression) {\\n+            throw new Error('Expression mapping requires expression field');\\n+          }\\n+          const sandbox = createSecureSandbox();\\n+          return compileAndRun(\\n+            sandbox,\\n+            typedMapping.expression,\\n+            {\\n+              inputs: executionContext.inputs,\\n+              outputs: Object.fromEntries(stepResults),\\n+              steps: Object.fromEntries(\\n+                Array.from(stepResults.entries()).map(([id, result]) => [\\n+                  id,\\n+                  (result as any).output,\\n+                ])\\n+              ),\\n+            },\\n+            { injectLog: true, logPrefix: 'workflow.input.expression' }\\n+          );\\n+\\n+        default:\\n+          throw new Error(`Unknown input mapping source: ${typedMapping.source}`);\\n+      }\\n+    }\\n+\\n+    // Handle Liquid template in mapping\\n+    if (typeof mapping === 'object' && mapping !== null && 'template' in mapping) {\\n+      const typedMapping = mapping as WorkflowInputMapping;\\n+      if (typedMapping.template) {\\n+        return await this.liquid.parseAndRender(typedMapping.template, {\\n+          inputs: executionContext.inputs,\\n+          outputs: Object.fromEntries(stepResults),\\n+        });\\n+      }\\n+    }\\n+\\n+    // Return as-is\\n+    return mapping;\\n+  }\\n+\\n+  /**\\n+   * Execute a single step\\n+   */\\n+  private async executeStep(\\n+    config: CheckProviderConfig,\\n+    prInfo: PRInfo,\\n+    dependencyResults: Map<string, ReviewSummary>,\\n+    context?: ExecutionContext\\n+  ): Promise<ReviewSummary> {\\n+    const provider = await this.getProviderRegistry().getProvider(config.type);\\n+    if (!provider) {\\n+      throw new Error(`Provider '${config.type}' not found`);\\n+    }\\n+\\n+    return await provider.execute(prInfo, config, dependencyResults, context);\\n+  }\\n+\\n+  /**\\n+   * Compute workflow outputs\\n+   */\\n+  private async computeOutputs(\\n+    workflow: WorkflowDefinition,\\n+    executionContext: WorkflowExecutionContext,\\n+    stepResults: Map<string, ReviewSummary>,\\n+    prInfo: PRInfo\\n+  ): Promise<Record<string, unknown>> {\\n+    const outputs: Record<string, unknown> = {};\\n+\\n+    if (!workflow.outputs) {\\n+      return outputs;\\n+    }\\n+\\n+    for (const output of workflow.outputs) {\\n+      if (output.value_js) {\\n+        // JavaScript expression\\n+        const sandbox = createSecureSandbox();\\n+        outputs[output.name] = compileAndRun(\\n+          sandbox,\\n+          output.value_js,\\n+          {\\n+            inputs: executionContext.inputs,\\n+            steps: Object.fromEntries(\\n+              Array.from(stepResults.entries()).map(([id, result]) => [id, (result as any).output])\\n+            ),\\n+            outputs: Object.fromEntries(stepResults),\\n+            pr: prInfo,\\n+          },\\n+          { injectLog: true, logPrefix: `workflow.output.${output.name}` }\\n+        );\\n+      } else if (output.value) {\\n+        // Liquid template\\n+        outputs[output.name] = await this.liquid.parseAndRender(output.value, {\\n+          inputs: executionContext.inputs,\\n+          steps: Object.fromEntries(\\n+            Array.from(stepResults.entries()).map(([id, result]) => [id, (result as any).output])\\n+          ),\\n+          outputs: Object.fromEntries(stepResults),\\n+          pr: prInfo,\\n+        });\\n+      }\\n+    }\\n+\\n+    return outputs;\\n+  }\\n+\\n+  /**\\n+   * Aggregate results from all steps\\n+   */\\n+  private aggregateResults(stepResults: Map<string, ReviewSummary>): {\\n+    score: number;\\n+    confidence: 'high' | 'medium' | 'low';\\n+    issues: any[];\\n+    comments: any[];\\n+  } {\\n+    let totalScore = 0;\\n+    let scoreCount = 0;\\n+    const allIssues: any[] = [];\\n+    const allComments: any[] = [];\\n+    let minConfidence: 'high' | 'medium' | 'low' = 'high';\\n+\\n+    for (const result of stepResults.values()) {\\n+      const extResult = result as any;\\n+      if (typeof extResult.score === 'number') {\\n+        totalScore += extResult.score;\\n+        scoreCount++;\\n+      }\\n+\\n+      if (result.issues) {\\n+        allIssues.push(...result.issues);\\n+      }\\n+\\n+      if (extResult.comments) {\\n+        allComments.push(...extResult.comments);\\n+      }\\n+\\n+      if (extResult.confidence) {\\n+        if (\\n+          extResult.confidence === 'low' ||\\n+          (extResult.confidence === 'medium' && minConfidence === 'high')\\n+        ) {\\n+          minConfidence = extResult.confidence;\\n+        }\\n+      }\\n+    }\\n+\\n+    return {\\n+      score: scoreCount > 0 ? Math.round(totalScore / scoreCount) : 0,\\n+      confidence: minConfidence,\\n+      issues: allIssues,\\n+      comments: allComments,\\n+    };\\n+  }\\n+\\n+  /**\\n+   * Evaluate a condition expression\\n+   */\\n+  private evaluateCondition(condition: string, context: any): boolean {\\n+    try {\\n+      const sandbox = createSecureSandbox();\\n+      const result = compileAndRun(sandbox, condition, context, {\\n+        injectLog: true,\\n+        logPrefix: 'workflow.condition',\\n+      });\\n+      return Boolean(result);\\n+    } catch (error) {\\n+      logger.warn(`Failed to evaluate condition '${condition}': ${error}`);\\n+      return false;\\n+    }\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/workflow-registry.ts\",\"additions\":13,\"deletions\":0,\"changes\":454,\"patch\":\"diff --git a/src/workflow-registry.ts b/src/workflow-registry.ts\\nnew file mode 100644\\nindex 00000000..2cbe3a89\\n--- /dev/null\\n+++ b/src/workflow-registry.ts\\n@@ -0,0 +1,454 @@\\n+/**\\n+ * Workflow registry for managing reusable workflow definitions\\n+ */\\n+\\n+import {\\n+  WorkflowDefinition,\\n+  WorkflowRegistryEntry,\\n+  WorkflowValidationResult,\\n+  WorkflowImportOptions,\\n+  JsonSchema,\\n+} from './types/workflow';\\n+import { promises as fs } from 'fs';\\n+import * as path from 'path';\\n+import * as yaml from 'js-yaml';\\n+import { logger } from './logger';\\n+import { DependencyResolver } from './dependency-resolver';\\n+import Ajv from 'ajv';\\n+import addFormats from 'ajv-formats';\\n+\\n+/**\\n+ * Registry for managing workflow definitions\\n+ */\\n+export class WorkflowRegistry {\\n+  private static instance: WorkflowRegistry;\\n+  private workflows: Map<string, WorkflowRegistryEntry> = new Map();\\n+  private ajv: Ajv;\\n+\\n+  private constructor() {\\n+    this.ajv = new Ajv({ allErrors: true, strict: false });\\n+    addFormats(this.ajv);\\n+  }\\n+\\n+  /**\\n+   * Get the singleton instance of the workflow registry\\n+   */\\n+  public static getInstance(): WorkflowRegistry {\\n+    if (!WorkflowRegistry.instance) {\\n+      WorkflowRegistry.instance = new WorkflowRegistry();\\n+    }\\n+    return WorkflowRegistry.instance;\\n+  }\\n+\\n+  /**\\n+   * Register a workflow definition\\n+   */\\n+  public register(\\n+    workflow: WorkflowDefinition,\\n+    source: string = 'inline',\\n+    options?: { override?: boolean }\\n+  ): WorkflowValidationResult {\\n+    // Validate the workflow\\n+    const validation = this.validateWorkflow(workflow);\\n+    if (!validation.valid) {\\n+      return validation;\\n+    }\\n+\\n+    // Check if workflow already exists\\n+    if (this.workflows.has(workflow.id) && !options?.override) {\\n+      return {\\n+        valid: false,\\n+        errors: [\\n+          {\\n+            path: 'id',\\n+            message: `Workflow with ID '${workflow.id}' already exists`,\\n+            value: workflow.id,\\n+          },\\n+        ],\\n+      };\\n+    }\\n+\\n+    // Register the workflow\\n+    this.workflows.set(workflow.id, {\\n+      definition: workflow,\\n+      source,\\n+      registeredAt: new Date(),\\n+      usage: {\\n+        count: 0,\\n+      },\\n+    });\\n+\\n+    logger.debug(`Registered workflow '${workflow.id}' from ${source}`);\\n+    return { valid: true };\\n+  }\\n+\\n+  /**\\n+   * Get a workflow by ID\\n+   */\\n+  public get(id: string): WorkflowDefinition | undefined {\\n+    const entry = this.workflows.get(id);\\n+    if (entry) {\\n+      // Update usage statistics\\n+      entry.usage = entry.usage || { count: 0 };\\n+      entry.usage.count++;\\n+      entry.usage.lastUsed = new Date();\\n+    }\\n+    return entry?.definition;\\n+  }\\n+\\n+  /**\\n+   * Check if a workflow exists\\n+   */\\n+  public has(id: string): boolean {\\n+    return this.workflows.has(id);\\n+  }\\n+\\n+  /**\\n+   * List all registered workflows\\n+   */\\n+  public list(): WorkflowDefinition[] {\\n+    return Array.from(this.workflows.values()).map(entry => entry.definition);\\n+  }\\n+\\n+  /**\\n+   * Get workflow metadata\\n+   */\\n+  public getMetadata(id: string): WorkflowRegistryEntry | undefined {\\n+    return this.workflows.get(id);\\n+  }\\n+\\n+  /**\\n+   * Remove a workflow from the registry\\n+   */\\n+  public unregister(id: string): boolean {\\n+    return this.workflows.delete(id);\\n+  }\\n+\\n+  /**\\n+   * Clear all workflows\\n+   */\\n+  public clear(): void {\\n+    this.workflows.clear();\\n+  }\\n+\\n+  /**\\n+   * Import workflows from a file or URL\\n+   */\\n+  public async import(\\n+    source: string,\\n+    options?: WorkflowImportOptions\\n+  ): Promise<WorkflowValidationResult[]> {\\n+    const results: WorkflowValidationResult[] = [];\\n+\\n+    try {\\n+      // Load the workflow file\\n+      const content = await this.loadWorkflowContent(source, options?.basePath);\\n+      const data = this.parseWorkflowContent(content, source);\\n+\\n+      // Handle both single workflow and multiple workflows\\n+      const workflows: WorkflowDefinition[] = Array.isArray(data) ? data : [data];\\n+\\n+      for (const workflow of workflows) {\\n+        // Validate if requested\\n+        if (options?.validate !== false) {\\n+          const validation = this.validateWorkflow(workflow);\\n+          if (!validation.valid) {\\n+            results.push(validation);\\n+            continue;\\n+          }\\n+\\n+          // Run custom validators if provided\\n+          if (options?.validators) {\\n+            for (const validator of options.validators) {\\n+              const customValidation = validator(workflow);\\n+              if (!customValidation.valid) {\\n+                results.push(customValidation);\\n+                continue;\\n+              }\\n+            }\\n+          }\\n+        }\\n+\\n+        // Strip out 'tests' field before registering - tests are only for standalone execution\\n+        const workflowWithoutTests = { ...workflow };\\n+        delete (workflowWithoutTests as any).tests;\\n+\\n+        // Register the workflow (without tests)\\n+        const result = this.register(workflowWithoutTests, source, { override: options?.override });\\n+        results.push(result);\\n+      }\\n+    } catch (error) {\\n+      results.push({\\n+        valid: false,\\n+        errors: [\\n+          {\\n+            path: 'source',\\n+            message: `Failed to import workflows from '${source}': ${error instanceof Error ? error.message : String(error)}`,\\n+            value: source,\\n+          },\\n+        ],\\n+      });\\n+    }\\n+\\n+    return results;\\n+  }\\n+\\n+  /**\\n+   * Import multiple workflow sources\\n+   */\\n+  public async importMany(\\n+    sources: string[],\\n+    options?: WorkflowImportOptions\\n+  ): Promise<Map<string, WorkflowValidationResult[]>> {\\n+    const results = new Map<string, WorkflowValidationResult[]>();\\n+\\n+    for (const source of sources) {\\n+      const importResults = await this.import(source, options);\\n+      results.set(source, importResults);\\n+    }\\n+\\n+    return results;\\n+  }\\n+\\n+  /**\\n+   * Validate a workflow definition\\n+   */\\n+  public validateWorkflow(workflow: WorkflowDefinition): WorkflowValidationResult {\\n+    const errors: Array<{ path: string; message: string; value?: unknown }> = [];\\n+    const warnings: Array<{ path: string; message: string }> = [];\\n+\\n+    // Validate required fields\\n+    if (!workflow.id) {\\n+      errors.push({ path: 'id', message: 'Workflow ID is required' });\\n+    }\\n+\\n+    if (!workflow.name) {\\n+      errors.push({ path: 'name', message: 'Workflow name is required' });\\n+    }\\n+\\n+    if (!workflow.steps || Object.keys(workflow.steps).length === 0) {\\n+      errors.push({ path: 'steps', message: 'Workflow must have at least one step' });\\n+    }\\n+\\n+    // Validate input parameters\\n+    if (workflow.inputs) {\\n+      for (let i = 0; i < workflow.inputs.length; i++) {\\n+        const input = workflow.inputs[i];\\n+        if (!input.name) {\\n+          errors.push({ path: `inputs[${i}].name`, message: 'Input parameter name is required' });\\n+        }\\n+        if (!input.schema) {\\n+          warnings.push({\\n+            path: `inputs[${i}].schema`,\\n+            message: 'Input parameter schema is recommended',\\n+          });\\n+        }\\n+      }\\n+    }\\n+\\n+    // Validate output parameters\\n+    if (workflow.outputs) {\\n+      for (let i = 0; i < workflow.outputs.length; i++) {\\n+        const output = workflow.outputs[i];\\n+        if (!output.name) {\\n+          errors.push({ path: `outputs[${i}].name`, message: 'Output parameter name is required' });\\n+        }\\n+        if (!output.value && !output.value_js) {\\n+          errors.push({\\n+            path: `outputs[${i}]`,\\n+            message: 'Output parameter must have either value or value_js',\\n+          });\\n+        }\\n+      }\\n+    }\\n+\\n+    // Validate steps\\n+    for (const [stepId, step] of Object.entries(workflow.steps || {})) {\\n+      // Validate step dependencies\\n+      if (step.depends_on) {\\n+        for (const dep of step.depends_on) {\\n+          if (!workflow.steps[dep]) {\\n+            errors.push({\\n+              path: `steps.${stepId}.depends_on`,\\n+              message: `Step '${stepId}' depends on non-existent step '${dep}'`,\\n+              value: dep,\\n+            });\\n+          }\\n+        }\\n+      }\\n+\\n+      // Validate input mappings\\n+      if (step.inputs) {\\n+        for (const [inputName, mapping] of Object.entries(step.inputs)) {\\n+          if (typeof mapping === 'object' && mapping !== null && 'source' in mapping) {\\n+            const typedMapping = mapping as any;\\n+            if (typedMapping.source === 'step' && !typedMapping.stepId) {\\n+              errors.push({\\n+                path: `steps.${stepId}.inputs.${inputName}`,\\n+                message: 'Step input mapping with source \\\"step\\\" must have stepId',\\n+              });\\n+            }\\n+            if (typedMapping.source === 'param') {\\n+              // Validate that the parameter exists\\n+              const paramExists = workflow.inputs?.some(p => p.name === typedMapping.value);\\n+              if (!paramExists) {\\n+                errors.push({\\n+                  path: `steps.${stepId}.inputs.${inputName}`,\\n+                  message: `Step input references non-existent parameter '${typedMapping.value}'`,\\n+                  value: typedMapping.value,\\n+                });\\n+              }\\n+            }\\n+          }\\n+        }\\n+      }\\n+    }\\n+\\n+    // Check for circular dependencies\\n+    const circularDeps = this.detectCircularDependencies(workflow);\\n+    if (circularDeps.length > 0) {\\n+      errors.push({\\n+        path: 'steps',\\n+        message: `Circular dependencies detected: ${circularDeps.join(' -> ')}`,\\n+      });\\n+    }\\n+\\n+    return {\\n+      valid: errors.length === 0,\\n+      errors: errors.length > 0 ? errors : undefined,\\n+      warnings: warnings.length > 0 ? warnings : undefined,\\n+    };\\n+  }\\n+\\n+  /**\\n+   * Validate input values against workflow input schema\\n+   */\\n+  public validateInputs(\\n+    workflow: WorkflowDefinition,\\n+    inputs: Record<string, unknown>\\n+  ): WorkflowValidationResult {\\n+    const errors: Array<{ path: string; message: string; value?: unknown }> = [];\\n+\\n+    if (!workflow.inputs) {\\n+      return { valid: true };\\n+    }\\n+\\n+    // Check required inputs\\n+    for (const param of workflow.inputs) {\\n+      if (param.required !== false && !(param.name in inputs) && param.default === undefined) {\\n+        errors.push({\\n+          path: `inputs.${param.name}`,\\n+          message: `Required input '${param.name}' is missing`,\\n+        });\\n+      }\\n+    }\\n+\\n+    // Validate input schemas\\n+    for (const param of workflow.inputs) {\\n+      if (param.name in inputs && param.schema) {\\n+        const value = inputs[param.name];\\n+        const valid = this.validateAgainstSchema(value, param.schema);\\n+        if (!valid.valid) {\\n+          errors.push({\\n+            path: `inputs.${param.name}`,\\n+            message: valid.error || 'Invalid input value',\\n+            value,\\n+          });\\n+        }\\n+      }\\n+    }\\n+\\n+    return {\\n+      valid: errors.length === 0,\\n+      errors: errors.length > 0 ? errors : undefined,\\n+    };\\n+  }\\n+\\n+  /**\\n+   * Load workflow content from file or URL\\n+   */\\n+  private async loadWorkflowContent(source: string, basePath?: string): Promise<string> {\\n+    // Handle URLs\\n+    if (source.startsWith('http://') || source.startsWith('https://')) {\\n+      const response = await fetch(source);\\n+      if (!response.ok) {\\n+        throw new Error(`Failed to fetch workflow from ${source}: ${response.statusText}`);\\n+      }\\n+      return await response.text();\\n+    }\\n+\\n+    // Handle file paths\\n+    const filePath = path.isAbsolute(source)\\n+      ? source\\n+      : path.resolve(basePath || process.cwd(), source);\\n+    return await fs.readFile(filePath, 'utf-8');\\n+  }\\n+\\n+  /**\\n+   * Parse workflow content (YAML or JSON)\\n+   */\\n+  private parseWorkflowContent(content: string, source: string): any {\\n+    // Try JSON first\\n+    try {\\n+      return JSON.parse(content);\\n+    } catch {\\n+      // Try YAML\\n+      try {\\n+        return yaml.load(content);\\n+      } catch (error) {\\n+        throw new Error(\\n+          `Failed to parse workflow file ${source}: ${error instanceof Error ? error.message : String(error)}`\\n+        );\\n+      }\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Detect circular dependencies in workflow steps using DependencyResolver\\n+   */\\n+  private detectCircularDependencies(workflow: WorkflowDefinition): string[] {\\n+    // Build dependency map\\n+    const dependencies: Record<string, string[]> = {};\\n+    for (const [stepId, step] of Object.entries(workflow.steps || {})) {\\n+      dependencies[stepId] = step.depends_on || [];\\n+    }\\n+\\n+    try {\\n+      // Use DependencyResolver to check for cycles\\n+      const graph = DependencyResolver.buildDependencyGraph(dependencies);\\n+\\n+      if (graph.hasCycles && graph.cycleNodes) {\\n+        return graph.cycleNodes;\\n+      }\\n+\\n+      return [];\\n+    } catch {\\n+      // DependencyResolver throws error for non-existent dependencies\\n+      // This should be caught by the dependency validation in validateWorkflow\\n+      // Return empty array here and let the validation handle it\\n+      return [];\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Validate a value against a JSON schema\\n+   */\\n+  private validateAgainstSchema(\\n+    value: unknown,\\n+    schema: JsonSchema\\n+  ): { valid: boolean; error?: string } {\\n+    try {\\n+      const validate = this.ajv.compile(schema as any);\\n+      const valid = validate(value);\\n+      if (!valid) {\\n+        const errors = validate.errors\\n+          ?.map(e => `${e.instancePath || '/'}: ${e.message}`)\\n+          .join(', ');\\n+        return { valid: false, error: errors };\\n+      }\\n+      return { valid: true };\\n+    } catch (error) {\\n+      return { valid: false, error: error instanceof Error ? error.message : String(error) };\\n+    }\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"tests/e2e/fact-validation-memory-e2e.test.ts\",\"additions\":1,\"deletions\":3,\"changes\":135,\"patch\":\"diff --git a/tests/e2e/fact-validation-memory-e2e.test.ts b/tests/e2e/fact-validation-memory-e2e.test.ts\\nindex 3ba368b5..9abdce9c 100644\\n--- a/tests/e2e/fact-validation-memory-e2e.test.ts\\n+++ b/tests/e2e/fact-validation-memory-e2e.test.ts\\n@@ -1,119 +1,62 @@\\n import { CheckExecutionEngine } from '../../src/check-execution-engine';\\n import type { VisorConfig } from '../../src/types/config';\\n \\n-describe('Fact Validation Flow (memory, fast e2e)', () => {\\n-  const makeConfig = (retryLimit: number, ns = 'fact-validation'): Partial<VisorConfig> => ({\\n+describe('Fact Validation Flow (history-driven, fast e2e)', () => {\\n+  const makeConfig = (retryWaves: number): Partial<VisorConfig> => ({\\n     version: '1.0',\\n     checks: {\\n-      'init-attempt': {\\n-        type: 'memory',\\n-        operation: 'set',\\n-        namespace: ns,\\n-        key: 'attempt',\\n-        value: 0,\\n-      },\\n-      'init-limit': {\\n-        type: 'memory',\\n-        operation: 'set',\\n-        namespace: ns,\\n-        key: 'retry_limit',\\n-        value: retryLimit,\\n-        depends_on: ['init-attempt'],\\n-      },\\n-      'seed-facts': {\\n-        type: 'memory',\\n-        operation: 'set',\\n-        namespace: ns,\\n-        key: 'facts',\\n-        value_js: 'Array.from({length:6},(_,i)=>({id:`fact-${i+1}`, claim:`claim-${i+1}`}))',\\n-        depends_on: ['init-limit'],\\n-      },\\n-\\n       'extract-facts': {\\n-        type: 'memory',\\n-        operation: 'get',\\n-        namespace: ns,\\n-        key: 'facts',\\n+        type: 'script',\\n         forEach: true,\\n-        depends_on: ['seed-facts'],\\n+        content: `\\n+          // Produce 6 facts\\n+          return Array.from({length:6},(_,i)=>({ id: 'fact-'+(i+1), claim: 'claim-'+(i+1) }));\\n+        `,\\n         on_finish: {\\n-          run: ['comment-assistant', 'aggregate-validations'],\\n           goto_js: `\\n-            const NS = '${ns}';\\n-            const allValid = memory.get('all_valid', NS) === true;\\n-            const limit = Number(memory.get('retry_limit', NS) || 0);\\n-            let attempt = Number(memory.get('attempt', NS) || 0);\\n-            if (!allValid && attempt < limit) {\\n-              memory.increment('attempt', 1, NS);\\n-              return 'extract-facts';\\n-            }\\n-            return null;\\n+            // Re-run until the last wave is all valid, capped by one retry.\\n+            const hist = outputs.history || {};\\n+            const items = (forEach && forEach.last_wave_size) ? forEach.last_wave_size : 1;\\n+            const perAll = (hist['validate-fact'] || []).filter((x) => !Array.isArray(x));\\n+            const waves = items > 0 ? Math.floor(perAll.length / items) : 0;\\n+            const last = items > 0 ? perAll.slice(-items) : [];\\n+            const allOk = last.length === items && last.every(v => v && (v.is_valid === true || v.valid === true));\\n+            log('[goto_js] items=', items, 'perAll=', perAll.length, 'waves=', waves, 'lastOk=', allOk);\\n+            const maxWaves = 1 + ${retryWaves};\\n+            return (!allOk && waves < maxWaves) ? 'extract-facts' : null;\\n           `,\\n         },\\n       },\\n \\n       'validate-fact': {\\n         type: 'script',\\n-        namespace: ns,\\n         depends_on: ['extract-facts'],\\n         fanout: 'map',\\n         content: `\\n-          const NS = '${ns}';\\n-          const attempt = memory.get('attempt', NS) || 0;\\n+          // Determine current wave from history\\n+          const arrs = outputs.history['extract-facts'] || [];\\n+          const lastArr = arrs.filter(Array.isArray).slice(-1)[0] || [];\\n+          const items = lastArr.length || 1;\\n+          const per = (outputs.history['validate-fact'] || []).filter(x => !Array.isArray(x));\\n+          const waves = Math.floor(per.length / items);\\n+\\n+          // Current item\\n           const f = outputs['extract-facts'];\\n           const n = Number((f.id||'').split('-')[1]||'0');\\n+\\n+          // First wave: facts 1..3 are invalid, rest valid. Later waves: all valid.\\n           const invalidOnFirst = (n >= 1 && n <= 3);\\n-          const is_valid = attempt >= 1 ? true : !invalidOnFirst;\\n+          const is_valid = waves >= 1 ? true : !invalidOnFirst;\\n           return { fact_id: f.id, claim: f.claim, is_valid, confidence: 'high', evidence: is_valid ? 'ok' : 'bad' };\\n         `,\\n       },\\n-\\n-      'aggregate-validations': {\\n-        type: 'script',\\n-        namespace: ns,\\n-        content: `\\n-          const NS = '${ns}';\\n-          const nested = outputs.history['validate-fact'] || [];\\n-          const vals = Array.isArray(nested) ? nested.flatMap(x => Array.isArray(x) ? x : [x]) : [];\\n-          const byId = new Map(); for (const v of vals) { if (v && v.fact_id) byId.set(v.fact_id, v); }\\n-          const uniq = Array.from(byId.values());\\n-          const invalid = uniq.filter(v => v && ((v.is_valid === false) || (v.evidence === 'bad')));\\n-          const all_valid = invalid.length === 0;\\n-          memory.set('all_valid', all_valid, NS);\\n-          { const prev = Number(memory.get('total_validations', NS) || 0); memory.set('total_validations', prev + uniq.length, NS); }\\n-          const attempt = memory.get('attempt', NS) || 0;\\n-          return { total: uniq.length, invalid: invalid.length, all_valid, attempt };\\n-        `,\\n-      },\\n-\\n-      'comment-assistant': {\\n-        type: 'script',\\n-        namespace: ns,\\n-        content: `\\n-          const NS = '${ns}';\\n-          const prev = outputs.history['comment-assistant'] || [];\\n-          const allFactsNested = outputs.history['validate-fact'] || [];\\n-          const allFacts = Array.isArray(allFactsNested) ? allFactsNested.flatMap(x => Array.isArray(x) ? x : [x]) : [];\\n-          // Collect unique set of fact_ids that were ever invalid across waves\\n-          const failedIds = new Set();\\n-          for (const f of allFacts) {\\n-            if (f && f.fact_id && (f.is_valid === false || f.evidence === 'bad')) failedIds.add(f.fact_id);\\n-          }\\n-          const failed = Array.from(failedIds);\\n-          memory.set('comment_prev_count', Array.isArray(prev) ? prev.length : 0, NS);\\n-          memory.set('failed_from_history', failed.length, NS);\\n-          return { prev_count: Array.isArray(prev) ? prev.length : 0, failed_total: failed.length };\\n-        `,\\n-      },\\n+      // no aggregator step needed in this variant\\n     },\\n   });\\n \\n-  it('no retry: per-item validations run once (√ó6), attempt=0', async () => {\\n-    // Clean memory between tests\\n-    const { MemoryStore } = require('../../src/memory-store');\\n-    MemoryStore.resetInstance();\\n+  it('no retry: per-item validations run once (√ó6)', async () => {\\n     const engine = new CheckExecutionEngine();\\n-    const cfg = makeConfig(0, 'fact-validation-n0');\\n+    const cfg = makeConfig(0);\\n     const result = await engine.executeChecks({\\n       checks: ['extract-facts', 'validate-fact'],\\n       config: cfg as VisorConfig,\\n@@ -125,16 +68,11 @@ describe('Fact Validation Flow (memory, fast e2e)', () => {\\n     for (const c of (result as any).executionStatistics?.checks || []) byName[c.checkName] = c;\\n     expect(byName['extract-facts'].outputsProduced).toBe(6);\\n     expect(byName['validate-fact'].totalRuns).toBe(6);\\n-    const store = MemoryStore.getInstance();\\n-    expect(store.get('total_validations', 'fact-validation-n0')).toBe(6);\\n-    expect(store.get('attempt', 'fact-validation-n0')).toBe(0);\\n   });\\n \\n-  it('one retry: per-item validations run twice (√ó12), attempt=1', async () => {\\n-    const { MemoryStore } = require('../../src/memory-store');\\n-    MemoryStore.resetInstance();\\n+  it('one retry: per-item validations run twice (√ó12)', async () => {\\n     const engine = new CheckExecutionEngine();\\n-    const cfg = makeConfig(1, 'fact-validation-n1');\\n+    const cfg = makeConfig(1);\\n     const result = await engine.executeChecks({\\n       checks: ['extract-facts', 'validate-fact'],\\n       config: cfg as VisorConfig,\\n@@ -146,12 +84,5 @@ describe('Fact Validation Flow (memory, fast e2e)', () => {\\n     for (const c of (result as any).executionStatistics?.checks || []) byName[c.checkName] = c;\\n     expect(byName['extract-facts'].outputsProduced).toBe(6);\\n     expect(byName['validate-fact'].totalRuns).toBe(12);\\n-    const store = MemoryStore.getInstance();\\n-    expect(store.get('total_validations', 'fact-validation-n1')).toBeGreaterThanOrEqual(6);\\n-    expect(store.get('attempt', 'fact-validation-n1')).toBe(1);\\n-    // Comment assistant should have seen its previous result on retry\\n-    expect(store.get('comment_prev_count', 'fact-validation-n1')).toBeGreaterThanOrEqual(0);\\n-    // And it should have access to all failed facts across history (3 invalid on first wave)\\n-    expect(store.get('failed_from_history', 'fact-validation-n1')).toBe(3);\\n   });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/e2e/foreach-conditional-chain.test.ts\",\"additions\":1,\"deletions\":2,\"changes\":86,\"patch\":\"diff --git a/tests/e2e/foreach-conditional-chain.test.ts b/tests/e2e/foreach-conditional-chain.test.ts\\nindex ef736ebf..8dcc7fa1 100644\\n--- a/tests/e2e/foreach-conditional-chain.test.ts\\n+++ b/tests/e2e/foreach-conditional-chain.test.ts\\n@@ -36,7 +36,7 @@ describe('E2E: forEach with Conditional Chain', () => {\\n \\n     const finalOptions = {\\n       ...options,\\n-      env: { ...cleanEnv, VISOR_DEBUG: 'true' },\\n+      env: { ...cleanEnv, VISOR_DEBUG: 'true', VISOR_STATE_MACHINE: '1' },\\n       encoding: 'utf-8',\\n       stdio: ['pipe', 'pipe', 'pipe'],\\n     };\\n@@ -184,24 +184,13 @@ checks:\\n     //   - outputs[\\\"check-a\\\"] = undefined (only 2 items, out of bounds)\\n     //   - outputs[\\\"check-b\\\"] = undefined (only 1 item, out of bounds)\\n \\n-    const result = execCLI(['--config', configPath, '--output', 'table', '--debug'], {\\n+    const result = execCLI(['--config', configPath, '--output', 'table'], {\\n       cwd: testDir,\\n     });\\n \\n-    // With the forEach branching fix, all forEach parents should be unwrapped consistently\\n-    // The logger should show single objects for both check-a and check-b in each iteration\\n-    // Note: Due to conditionals, some iterations may skip checks, but when they do run,\\n-    // they should always see unwrapped (single object) outputs\\n-\\n-    // Verify the debug output shows forEach execution\\n-    expect(result).toMatch(/depends on forEach check/);\\n-    expect(result).toMatch(/executing \\\\d+ times/);\\n-\\n-    // The final aggregated output should show:\\n-    // - check-a: array of 2 objects (for items where typeA matched)\\n-    // - check-b: single object (unwrapped array of 1, where typeB matched)\\n-    expect(result).toContain('check-a:');\\n-    expect(result).toContain('check-b:');\\n+    // Verify execution completes successfully without errors\\n+    // Since all checks complete successfully with no issues, we expect clean output\\n+    expect(result).toMatch(/No issues found/);\\n   });\\n \\n   it('should document the expected behavior', () => {\\n@@ -243,37 +232,25 @@ checks:\\n     // 3. check-b executes 1 time (for typeB item: id 2)\\n     // 4. final-check executes 3 times (once per root-check item)\\n \\n-    const result = execCLI(['--config', configPath, '--output', 'table', '--debug'], {\\n+    const result = execCLI(['--config', configPath, '--output', 'json'], {\\n       cwd: testDir,\\n     });\\n \\n-    // Verify forEach execution debug messages\\n-    expect(result).toMatch(/depends on forEach check/);\\n-    expect(result).toMatch(/executing (\\\\d+) times/);\\n-\\n-    // Verify root-check execution\\n-    expect(result).toMatch(/root-check/);\\n-\\n-    // Verify conditional execution\\n-    // check-a should run for items matching typeA\\n-    expect(result).toMatch(/check-a/);\\n+    // Parse JSON output to verify structure\\n+    const output = JSON.parse(result);\\n \\n-    // check-b should run for items matching typeB\\n-    expect(result).toMatch(/check-b/);\\n-\\n-    // final-check should run 3 times (once per forEach item)\\n-    expect(result).toMatch(/final-check/);\\n+    // Verify all checks completed successfully\\n+    // The forEach execution should result in:\\n+    // - root-check: 3 iterations (one for each item)\\n+    // - check-a: 2 iterations (for typeA items)\\n+    // - check-b: 1 iteration (for typeB item)\\n+    // - final-check: 3 iterations (one for each root-check item)\\n \\n-    // Verify the output shows the forEach branching pattern\\n-    // The debug output should show multiple executions\\n-    const executionMatches = result.match(/executing (\\\\d+) times/g);\\n-    expect(executionMatches).toBeTruthy();\\n-    expect(executionMatches!.length).toBeGreaterThan(0);\\n+    expect(output).toBeDefined();\\n \\n-    // Verify outputs are present in final-check\\n-    expect(result).toMatch(/check-a:/);\\n-    expect(result).toMatch(/check-b:/);\\n-    expect(result).toMatch(/root-check:/);\\n+    // Since this is a log check with no issues, we just verify no errors occurred\\n+    const issues = output.issues || [];\\n+    expect(Array.isArray(issues)).toBe(true);\\n   });\\n \\n   it('should properly aggregate forEach results after all iterations', () => {\\n@@ -283,33 +260,14 @@ checks:\\n     // - check-a: array of 2 items (executed for 2 typeA items)\\n     // - check-b: array of 1 item (executed for 1 typeB item)\\n \\n-    const result = execCLI(['--config', configPath, '--output', 'table', '--debug'], {\\n+    const result = execCLI(['--config', configPath, '--output', 'table'], {\\n       cwd: testDir,\\n     });\\n \\n-    // Verify all checks completed successfully\\n-    expect(result).toMatch(/root-check/);\\n-    expect(result).toMatch(/check-a/);\\n-    expect(result).toMatch(/check-b/);\\n-    expect(result).toMatch(/final-check/);\\n-\\n-    // Verify forEach completion messages\\n-    expect(result).toMatch(/Completed forEach execution for check \\\"check-a\\\"/);\\n-    expect(result).toMatch(/Completed forEach execution for check \\\"check-b\\\"/);\\n-    expect(result).toMatch(/Completed forEach execution for check \\\"final-check\\\"/);\\n-\\n-    // Verify the checks were executed\\n-    // Note: \\\"Checks Executed\\\" only appears in Analysis Summary when there are issues\\n-    // Since there are no issues, we verify execution via completion messages instead\\n-    expect(result).toMatch(/Dependency-aware execution completed successfully/);\\n-\\n-    // The summary should show all checks executed (in debug output)\\n-    expect(result).toContain('root-check');\\n-    expect(result).toContain('check-a');\\n-    expect(result).toContain('check-b');\\n-    expect(result).toContain('final-check');\\n+    // Verify execution completed successfully\\n+    expect(result).toBeDefined();\\n \\n     // No issues should be found since all checks complete successfully\\n-    expect(result).toMatch(/No issues found|Total Issues.*0/);\\n+    expect(result).toMatch(/No issues found/);\\n   });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/e2e/foreach-on-finish.test.ts\",\"additions\":0,\"deletions\":32,\"changes\":1132,\"patch\":\"diff --git a/tests/e2e/foreach-on-finish.test.ts b/tests/e2e/foreach-on-finish.test.ts\\ndeleted file mode 100644\\nindex 28b9b144..00000000\\n--- a/tests/e2e/foreach-on-finish.test.ts\\n+++ /dev/null\\n@@ -1,1132 +0,0 @@\\n-import { describe, it, expect, beforeAll, afterAll } from '@jest/globals';\\n-import * as path from 'path';\\n-import * as fs from 'fs';\\n-import * as os from 'os';\\n-import { execSync } from 'child_process';\\n-\\n-/**\\n- * E2E Test: forEach with on_finish hook\\n- *\\n- * Structure:\\n- * 1. extract-items (forEach: true, on_finish) ‚Üí returns JSON array [1, 2, 3]\\n- * 2. process-item (depends_on: [extract-items]) ‚Üí processes each item\\n- * 3. aggregate-check (triggered by on_finish.run) ‚Üí aggregates results after ALL iterations\\n- *\\n- * This test verifies that on_finish triggers AFTER all dependent iterations complete.\\n- */\\n-describe('E2E: forEach with on_finish', () => {\\n-  let originalCwd: string;\\n-\\n-  beforeAll(() => {\\n-    originalCwd = process.cwd();\\n-  });\\n-\\n-  afterAll(() => {\\n-    if (originalCwd) {\\n-      try {\\n-        process.chdir(originalCwd);\\n-      } catch {\\n-        // Ignore errors\\n-      }\\n-    }\\n-  });\\n-  let testDir: string;\\n-  let configPath: string;\\n-  let cliCommand: string;\\n-  let cliArgsPrefix: string[];\\n-\\n-  // Helper function to execute CLI with clean environment\\n-  // Returns combined stdout + stderr\\n-  const execCLI = (args: string[], options: any = {}): string => {\\n-    const cleanEnv = { ...process.env } as NodeJS.ProcessEnv;\\n-    delete cleanEnv.JEST_WORKER_ID;\\n-    delete cleanEnv.NODE_ENV;\\n-    delete cleanEnv.GITHUB_ACTIONS;\\n-    delete cleanEnv.GIT_DIR;\\n-    delete cleanEnv.GIT_WORK_TREE;\\n-    delete cleanEnv.GIT_INDEX_FILE;\\n-    delete cleanEnv.GIT_PREFIX;\\n-    delete cleanEnv.GIT_COMMON_DIR;\\n-\\n-    // Use shell to merge stderr into stdout\\n-    const shellCmd = `${cliCommand} ${[...cliArgsPrefix, '--cli', ...args].join(' ')} 2>&1`;\\n-    const finalOptions = {\\n-      ...options,\\n-      env: { ...cleanEnv, VISOR_DEBUG: 'true' },\\n-      encoding: 'utf-8',\\n-      shell: true,\\n-    };\\n-    try {\\n-      const out = execSync(shellCmd, finalOptions) as unknown as string | Buffer;\\n-      return typeof out === 'string' ? out : (out as Buffer).toString('utf-8');\\n-    } catch (error: any) {\\n-      // When command fails, still return output\\n-      const output = error?.stdout || error?.output;\\n-      if (output) {\\n-        return Buffer.isBuffer(output) ? output.toString('utf-8') : String(output);\\n-      }\\n-      throw error;\\n-    }\\n-  };\\n-\\n-  beforeAll(() => {\\n-    // Create temp directory for test\\n-    testDir = fs.mkdtempSync(path.join(os.tmpdir(), 'visor-e2e-foreach-on-finish-'));\\n-\\n-    // Use dist/index.js (ncc bundled) if available, otherwise use ts-node with src/index.ts\\n-    const distCli = path.join(__dirname, '../../dist/index.js');\\n-    if (fs.existsSync(distCli)) {\\n-      cliCommand = 'node';\\n-      cliArgsPrefix = [distCli];\\n-    } else {\\n-      const tsNodeRegister = require.resolve('ts-node/register', {\\n-        paths: [path.resolve(__dirname, '../../')],\\n-      });\\n-      cliCommand = 'node';\\n-      cliArgsPrefix = ['-r', tsNodeRegister, path.join(__dirname, '../../src/index.ts')];\\n-    }\\n-\\n-    // Create test config with on_finish\\n-    const config = `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  extract-items:\\n-    type: command\\n-    exec: echo '[{\\\"id\\\":1,\\\"value\\\":\\\"A\\\"},{\\\"id\\\":2,\\\"value\\\":\\\"B\\\"},{\\\"id\\\":3,\\\"value\\\":\\\"C\\\"}]'\\n-    output_format: json\\n-    forEach: true\\n-    on_finish:\\n-      run: [log-on-finish]\\n-      goto_js: 'return null;'\\n-\\n-  process-item:\\n-    type: command\\n-    depends_on: [extract-items]\\n-    exec: >\\n-      echo '{\\\"processed_id\\\": {{ outputs[\\\"extract-items\\\"].id }}, \\\"processed_value\\\": \\\"{{ outputs[\\\"extract-items\\\"].value }}\\\"}'\\n-    output_format: json\\n-\\n-  log-on-finish:\\n-    type: log\\n-    message: |\\n-      === on_finish triggered ===\\n-      This should run AFTER all process-item iterations complete\\n-      Items processed: {{ outputs[\\\"process-item\\\"] | size }}\\n-\\n-output:\\n-  pr_comment:\\n-    format: table\\n-    group_by: check\\n-    collapse: false\\n-`;\\n-\\n-    configPath = path.join(testDir, '.visor.yaml');\\n-    fs.writeFileSync(configPath, config);\\n-\\n-    // Create a minimal package.json\\n-    fs.writeFileSync(\\n-      path.join(testDir, 'package.json'),\\n-      JSON.stringify({ name: 'test', version: '1.0.0' })\\n-    );\\n-\\n-    // Initialize git repo and create a commit\\n-    execSync('git init && git config user.email \\\"test@test.com\\\" && git config user.name \\\"Test\\\"', {\\n-      cwd: testDir,\\n-    });\\n-    fs.writeFileSync(path.join(testDir, 'test.txt'), 'test content');\\n-    execSync('git add . && git -c core.hooksPath=/dev/null commit -m \\\"Initial commit\\\"', {\\n-      cwd: testDir,\\n-    });\\n-  });\\n-\\n-  afterAll(() => {\\n-    if (testDir && fs.existsSync(testDir)) {\\n-      fs.rmSync(testDir, { recursive: true, force: true });\\n-    }\\n-  });\\n-\\n-  it('should detect on_finish hook on forEach check', () => {\\n-    // Run visor in test directory\\n-    const result = execCLI(['--config', configPath], { cwd: testDir });\\n-\\n-    // Since on_finish is in MVP state (logs TODO messages),\\n-    // we verify the check executed successfully without errors\\n-    expect(result).toContain('No issues found');\\n-    // The mere fact that it completes without error means on_finish was processed\\n-  });\\n-\\n-  it('should trigger on_finish after all forEach dependents complete', () => {\\n-    // Run visor in test directory\\n-    const result = execCLI(['--config', configPath], { cwd: testDir });\\n-\\n-    // For MVP implementation, verify execution completes successfully\\n-    // The on_finish hook is detected and processed (even if run/goto_js are TODO)\\n-    expect(result).toContain('No issues found');\\n-\\n-    // Verify forEach executed (3 items processed)\\n-    expect(result).toMatch(/Found 3 items for forEach|3.*forEach.*item/i);\\n-  });\\n-\\n-  it('should skip on_finish for empty forEach arrays', () => {\\n-    // Create config with empty array\\n-    const emptyConfig = `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  extract-empty:\\n-    type: command\\n-    exec: echo '[]'\\n-    output_format: json\\n-    forEach: true\\n-    on_finish:\\n-      run: []\\n-\\n-output:\\n-  pr_comment:\\n-    format: table\\n-    group_by: check\\n-    collapse: false\\n-`;\\n-    const emptyConfigPath = path.join(testDir, '.visor-empty.yaml');\\n-    fs.writeFileSync(emptyConfigPath, emptyConfig);\\n-\\n-    // Run visor with empty array config\\n-    const result = execCLI(['--config', emptyConfigPath], { cwd: testDir });\\n-\\n-    // Verify execution completes (empty forEach arrays are valid and should skip iterations)\\n-    expect(result).toContain('No issues found');\\n-    // For empty arrays, forEach still processes but with 0 items - this is valid\\n-    // We just verify it doesn't crash\\n-  });\\n-\\n-  it('should provide forEach stats in on_finish context', () => {\\n-    // Run visor in test directory with debug output\\n-    const result = execCLI(['--config', configPath], { cwd: testDir });\\n-\\n-    // For MVP, verify forEach executed with correct item count\\n-    // Full on_finish context (with run/goto_js execution) is pending\\n-    expect(result).toContain('No issues found');\\n-    expect(result).toMatch(/Found 3 items for forEach|3.*forEach.*item/i);\\n-  });\\n-\\n-  it('should complete full routing flow: forEach ‚Üí validation ‚Üí aggregation ‚Üí routing', () => {\\n-    // Create a complete flow config\\n-    const fullFlowConfig = `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  extract-facts:\\n-    type: command\\n-    exec: echo '[{\\\"claim\\\":\\\"Fact 1\\\",\\\"valid\\\":true},{\\\"claim\\\":\\\"Fact 2\\\",\\\"valid\\\":false},{\\\"claim\\\":\\\"Fact 3\\\",\\\"valid\\\":true}]'\\n-    output_format: json\\n-    forEach: true\\n-    on_finish:\\n-      run: [aggregate-validations]\\n-      goto_js: |\\n-        const allValid = outputs['validate-fact'].every(f => f.valid === true);\\n-        if (allValid) {\\n-          return null;\\n-        } else {\\n-          return 'retry-check';\\n-        }\\n-\\n-  validate-fact:\\n-    type: command\\n-    depends_on: [extract-facts]\\n-    exec: echo '{\\\"valid\\\":{{ outputs[\\\"extract-facts\\\"].valid }},\\\"claim\\\":\\\"{{ outputs[\\\"extract-facts\\\"].claim }}\\\"}'\\n-    output_format: json\\n-\\n-  aggregate-validations:\\n-    type: command\\n-    exec: echo '{\\\"aggregated\\\":true}'\\n-    output_format: json\\n-\\n-  retry-check:\\n-    type: command\\n-    exec: echo '{\\\"retry\\\":true}'\\n-    output_format: json\\n-\\n-output:\\n-  pr_comment:\\n-    format: table\\n-    group_by: check\\n-    collapse: false\\n-`;\\n-    const fullFlowPath = path.join(testDir, '.visor-full-flow.yaml');\\n-    fs.writeFileSync(fullFlowPath, fullFlowConfig);\\n-\\n-    const result = execCLI(['--config', fullFlowPath], { cwd: testDir });\\n-\\n-    // Verify all checks executed\\n-    expect(result).toMatch(/extract-facts|validate-fact/i);\\n-    // Verify aggregation ran\\n-    expect(result).toMatch(/aggregate-validations|aggregated/i);\\n-    // Verify routing occurred (should route to retry-check since not all valid)\\n-    expect(result).toMatch(/retry-check|retry/i);\\n-  });\\n-\\n-  it('should retry with memory: invalid result ‚Üí increment attempt ‚Üí retry ‚Üí success', () => {\\n-    // Create retry flow config with memory\\n-    const retryConfig = `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  init-memory:\\n-    type: memory\\n-    operation: set\\n-    key: attempt_count\\n-    value: 0\\n-    namespace: test\\n-\\n-  extract-items:\\n-    type: command\\n-    depends_on: [init-memory]\\n-    exec: echo '[{\\\"id\\\":1,\\\"status\\\":\\\"invalid\\\"}]'\\n-    output_format: json\\n-    forEach: true\\n-    on_finish:\\n-      run: [check-status]\\n-      goto_js: |\\n-        const attempt = memory.get('attempt_count', 'test') || 0;\\n-        const allValid = outputs['validate-status'].every(s => s.status === 'valid');\\n-\\n-        if (allValid) {\\n-          return null;\\n-        }\\n-\\n-        if (attempt >= 2) {\\n-          return null; // Max attempts reached\\n-        }\\n-\\n-        memory.increment('attempt_count', 1, 'test');\\n-        return 'extract-items';\\n-\\n-  validate-status:\\n-    type: command\\n-    depends_on: [extract-items]\\n-    exec: |\\n-      ATTEMPT=$(echo '{{ memory.get(\\\"attempt_count\\\", \\\"test\\\") }}' | grep -o '[0-9]' || echo 0)\\n-      if [ \\\"$ATTEMPT\\\" -ge \\\"1\\\" ]; then\\n-        echo '{\\\"status\\\":\\\"valid\\\",\\\"id\\\":{{ outputs[\\\"extract-items\\\"].id }}}'\\n-      else\\n-        echo '{\\\"status\\\":\\\"invalid\\\",\\\"id\\\":{{ outputs[\\\"extract-items\\\"].id }}}'\\n-      fi\\n-    output_format: json\\n-\\n-  check-status:\\n-    type: memory\\n-    operation: get\\n-    key: attempt_count\\n-    namespace: test\\n-\\n-output:\\n-  pr_comment:\\n-    format: table\\n-    group_by: check\\n-    collapse: false\\n-`;\\n-    const retryPath = path.join(testDir, '.visor-retry.yaml');\\n-    fs.writeFileSync(retryPath, retryConfig);\\n-\\n-    const result = execCLI(['--config', retryPath], { cwd: testDir });\\n-\\n-    // Verify execution completed (should retry and eventually succeed or hit max attempts)\\n-    expect(result).toBeDefined();\\n-  });\\n-\\n-  it('should respect goto_event override', () => {\\n-    // Create config with goto_event override\\n-    const gotoEventConfig = `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  extract-items:\\n-    type: command\\n-    exec: echo '[{\\\"id\\\":1}]'\\n-    output_format: json\\n-    forEach: true\\n-    on_finish:\\n-      goto: retry-check\\n-      goto_event: manual\\n-\\n-  process-item:\\n-    type: command\\n-    depends_on: [extract-items]\\n-    exec: echo '{\\\"processed\\\":true}'\\n-    output_format: json\\n-\\n-  retry-check:\\n-    type: command\\n-    exec: echo '{\\\"retried\\\":true}'\\n-    output_format: json\\n-\\n-output:\\n-  pr_comment:\\n-    format: table\\n-    group_by: check\\n-    collapse: false\\n-`;\\n-    const gotoEventPath = path.join(testDir, '.visor-goto-event.yaml');\\n-    fs.writeFileSync(gotoEventPath, gotoEventConfig);\\n-\\n-    const result = execCLI(['--config', gotoEventPath], { cwd: testDir });\\n-\\n-    // Verify execution completed\\n-    expect(result).toBeDefined();\\n-  });\\n-\\n-  it('should stop routing after max attempts (loop safety)', () => {\\n-    // Create config that would loop infinitely without max_loops\\n-    const loopConfig = `\\n-version: \\\"1.0\\\"\\n-max_loops: 3\\n-checks:\\n-  extract-items:\\n-    type: command\\n-    exec: echo '[{\\\"id\\\":1}]'\\n-    output_format: json\\n-    forEach: true\\n-    on_finish:\\n-      goto_js: |\\n-        // Always route back - should hit max_loops\\n-        return 'extract-items';\\n-\\n-  process-item:\\n-    type: command\\n-    depends_on: [extract-items]\\n-    exec: echo '{\\\"processed\\\":true}'\\n-    output_format: json\\n-\\n-output:\\n-  pr_comment:\\n-    format: table\\n-    group_by: check\\n-    collapse: false\\n-`;\\n-    const loopPath = path.join(testDir, '.visor-loop.yaml');\\n-    fs.writeFileSync(loopPath, loopConfig);\\n-\\n-    const result = execCLI(['--config', loopPath], { cwd: testDir });\\n-\\n-    // Should complete (not infinite loop) and show max loops message\\n-    expect(result).toBeDefined();\\n-    // May contain loop-related messages\\n-  });\\n-\\n-  it('should track output history correctly across loops', () => {\\n-    // Create config that accesses history across multiple loops\\n-    const historyConfig = `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  init-counter:\\n-    type: memory\\n-    operation: set\\n-    key: loop_count\\n-    value: 0\\n-    namespace: test\\n-\\n-  extract-items:\\n-    type: command\\n-    depends_on: [init-counter]\\n-    exec: echo '[{\\\"id\\\":1},{\\\"id\\\":2}]'\\n-    output_format: json\\n-    forEach: true\\n-    on_finish:\\n-      run: [check-history]\\n-      goto_js: |\\n-        const loopCount = memory.get('loop_count', 'test') || 0;\\n-        if (loopCount >= 1) {\\n-          return null;\\n-        }\\n-        memory.increment('loop_count', 1, 'test');\\n-        return 'extract-items';\\n-\\n-  process-item:\\n-    type: command\\n-    depends_on: [extract-items]\\n-    exec: echo '{\\\"processed\\\":{{ outputs[\\\"extract-items\\\"].id }},\\\"loop\\\":{{ memory.get(\\\"loop_count\\\", \\\"test\\\") }}}'\\n-    output_format: json\\n-\\n-  check-history:\\n-    type: memory\\n-    operation: set\\n-    key: history_size\\n-    namespace: test\\n-    value_js: '(Array.isArray(outputs_history[\\\\'process-item\\\\']) ? outputs_history[\\\\'process-item\\\\'].length : 1)'\\n-\\n-output:\\n-  pr_comment:\\n-    format: table\\n-    group_by: check\\n-    collapse: false\\n-`;\\n-    const historyPath = path.join(testDir, '.visor-history.yaml');\\n-    fs.writeFileSync(historyPath, historyConfig);\\n-\\n-    const result = execCLI(['--config', historyPath], { cwd: testDir });\\n-\\n-    // Verify execution completed\\n-    expect(result).toBeDefined();\\n-  });\\n-\\n-  // ============================================================================\\n-  // PHASE 5 TESTS: Full Fact Validation Flow (Tasks 5.1-5.4)\\n-  // ============================================================================\\n-\\n-  describe('Phase 5.1: Full Fact Validation Flow', () => {\\n-    it('should complete full cycle: assistant ‚Üí extract ‚Üí validate ‚Üí aggregate ‚Üí post', () => {\\n-      // Test the complete fact validation flow from start to finish\\n-      const fullCycleConfig = `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  init-memory:\\n-    type: memory\\n-    operation: set\\n-    key: attempt_count\\n-    value: 0\\n-    namespace: fact-validation\\n-\\n-  assistant:\\n-    type: command\\n-    depends_on: [init-memory]\\n-    exec: echo 'The config file is .visor.yaml and tests use Jest'\\n-\\n-  extract-facts:\\n-    type: command\\n-    depends_on: [assistant]\\n-    exec: echo '[{\\\"id\\\":\\\"f1\\\",\\\"claim\\\":\\\"Config is .visor.yaml\\\",\\\"valid\\\":true},{\\\"id\\\":\\\"f2\\\",\\\"claim\\\":\\\"Tests use Jest\\\",\\\"valid\\\":true}]'\\n-    output_format: json\\n-    forEach: true\\n-    on_finish:\\n-      run: [aggregate-validations]\\n-      goto_js: |\\n-        const allValid = memory.get('all_valid', 'fact-validation');\\n-        if (allValid) {\\n-          return null; // Proceed to posting\\n-        }\\n-        return null; // No retry needed for this test\\n-\\n-  validate-fact:\\n-    type: command\\n-    depends_on: [extract-facts]\\n-    exec: echo '{\\\"fact_id\\\":\\\"{{ outputs[\\\"extract-facts\\\"].id }}\\\",\\\"valid\\\":{{ outputs[\\\"extract-facts\\\"].valid }}}'\\n-    output_format: json\\n-\\n-  aggregate-validations:\\n-    type: memory\\n-    operation: set\\n-    namespace: fact-validation\\n-    key: all_valid\\n-    value_js: \\\"Array.isArray(outputs_history['validate-fact']) ? outputs_history['validate-fact'].every(v => v.valid === true) : true\\\"\\n-\\n-  post-response:\\n-    type: log\\n-    depends_on: [extract-facts]\\n-    if: \\\"memory.get('all_valid', 'fact-validation') === true\\\"\\n-    message: '‚úÖ Posted verified response: {{ outputs[\\\"assistant\\\"] }}'\\n-\\n-output:\\n-  pr_comment:\\n-    format: table\\n-    group_by: check\\n-    collapse: false\\n-`;\\n-      const fullCyclePath = path.join(testDir, '.visor-full-cycle.yaml');\\n-      fs.writeFileSync(fullCyclePath, fullCycleConfig);\\n-\\n-      const result = execCLI(['--config', fullCyclePath], { cwd: testDir });\\n-\\n-      // Verify all checks executed in correct order\\n-      expect(result).toContain('init-memory');\\n-      expect(result).toContain('assistant');\\n-      expect(result).toContain('extract-facts');\\n-      expect(result).toContain('validate-fact');\\n-      expect(result).toContain('aggregate-validations');\\n-      expect(result).toContain('‚úÖ Posted verified response');\\n-    });\\n-\\n-    it('should handle all facts valid ‚Üí direct post', () => {\\n-      // All facts pass validation, should post immediately without retry\\n-      const allValidConfig = `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  extract-facts:\\n-    type: command\\n-    exec: echo '[{\\\"id\\\":\\\"f1\\\",\\\"claim\\\":\\\"Valid fact 1\\\"},{\\\"id\\\":\\\"f2\\\",\\\"claim\\\":\\\"Valid fact 2\\\"}]'\\n-    output_format: json\\n-    forEach: true\\n-    on_finish:\\n-      run: [aggregate]\\n-      goto_js: |\\n-        const allValid = memory.get('all_valid', 'fact-validation');\\n-        return allValid ? null : 'extract-facts';\\n-\\n-  validate-fact:\\n-    type: command\\n-    depends_on: [extract-facts]\\n-    exec: echo '{\\\"fact_id\\\":\\\"{{ outputs[\\\"extract-facts\\\"].id }}\\\",\\\"is_valid\\\":true}'\\n-    output_format: json\\n-\\n-  aggregate:\\n-    type: script\\n-    namespace: fact-validation\\n-    content: |\\n-      const validations = outputs.history['validate-fact'] || [];\\n-      const allValid = validations.every(v => v.is_valid === true);\\n-      memory.set('all_valid', allValid, 'fact-validation');\\n-      return { all_valid: allValid };\\n-\\n-  post-verified:\\n-    type: log\\n-    depends_on: [extract-facts]\\n-    if: \\\"memory.get('all_valid', 'fact-validation') === true\\\"\\n-    message: 'Posted verified response - all facts valid'\\n-\\n-output:\\n-  pr_comment:\\n-    format: table\\n-    group_by: check\\n-    collapse: false\\n-`;\\n-      const allValidPath = path.join(testDir, '.visor-all-valid.yaml');\\n-      fs.writeFileSync(allValidPath, allValidConfig);\\n-\\n-      const result = execCLI(['--config', allValidPath], { cwd: testDir });\\n-\\n-      // Should post directly without retry\\n-      expect(result).toContain('Posted verified response');\\n-      // Should not route back (no retry)\\n-      expect(result).not.toMatch(/Routed to:/i);\\n-      expect(result).toMatch(/Checks:\\\\s*4 configured\\\\s*‚Üí\\\\s*7 executions/);\\n-    });\\n-\\n-    it('should handle some facts invalid ‚Üí retry ‚Üí validate ‚Üí post', () => {\\n-      // Some facts fail first time, retry with correction, then succeed\\n-      const retrySuccessConfig = `\\n-version: \\\"1.0\\\"\\n-max_loops: 3\\n-checks:\\n-  init-memory:\\n-    type: memory\\n-    operation: set\\n-    key: attempt\\n-    value: 0\\n-    namespace: fact-validation\\n-\\n-  extract-facts:\\n-    type: command\\n-    depends_on: [init-memory]\\n-    exec: |\\n-      ATTEMPT=$(echo '{{ memory.get(\\\"attempt\\\", \\\"fact-validation\\\") }}' | grep -o '[0-9]' || echo 0)\\n-      if [ \\\"$ATTEMPT\\\" -ge \\\"1\\\" ]; then\\n-        echo '[{\\\"id\\\":\\\"f1\\\",\\\"claim\\\":\\\"Corrected fact\\\",\\\"valid\\\":true}]'\\n-      else\\n-        echo '[{\\\"id\\\":\\\"f1\\\",\\\"claim\\\":\\\"Wrong fact\\\",\\\"valid\\\":false}]'\\n-      fi\\n-    output_format: json\\n-    forEach: true\\n-    on_finish:\\n-      run: [aggregate]\\n-      goto_js: |\\n-        const allValid = memory.get('all_valid', 'fact-validation');\\n-        const attempt = memory.get('attempt', 'fact-validation') || 0;\\n-        if (allValid) {\\n-          return null;\\n-        }\\n-        if (attempt >= 1) {\\n-          return null; // Give up\\n-        }\\n-        memory.increment('attempt', 1, 'fact-validation');\\n-        return 'extract-facts';\\n-\\n-  validate-fact:\\n-    type: command\\n-    depends_on: [extract-facts]\\n-    exec: echo '{\\\"fact_id\\\":\\\"{{ outputs[\\\"extract-facts\\\"].id }}\\\",\\\"is_valid\\\":{{ outputs[\\\"extract-facts\\\"].valid }}}'\\n-    output_format: json\\n-\\n-  aggregate:\\n-    type: script\\n-    namespace: fact-validation\\n-    content: |\\n-      const validations = outputs.history['validate-fact'] || [];\\n-      const allValid = validations.every(v => v.is_valid === true);\\n-      memory.set('all_valid', allValid, 'fact-validation');\\n-      return { all_valid: allValid };\\n-\\n-  post-verified:\\n-    type: log\\n-    depends_on: [extract-facts]\\n-    if: \\\"memory.get('all_valid', 'fact-validation') === true\\\"\\n-    message: 'Posted after successful retry'\\n-\\n-output:\\n-  pr_comment:\\n-    format: table\\n-    group_by: check\\n-    collapse: false\\n-`;\\n-      const retrySuccessPath = path.join(testDir, '.visor-retry-success.yaml');\\n-      fs.writeFileSync(retrySuccessPath, retrySuccessConfig);\\n-\\n-      const result = execCLI(['--config', retrySuccessPath], { cwd: testDir });\\n-\\n-      // Should retry and eventually post\\n-      expect(result).toContain('Posted after successful retry');\\n-    });\\n-  });\\n-\\n-  describe('Phase 5.2: Retry Logic', () => {\\n-    it('should retry once with validation context when facts invalid', () => {\\n-      // Invalid facts detected, verify memory increment and retry\\n-      const retryOnceConfig = `\\n-version: \\\"1.0\\\"\\n-max_loops: 3\\n-checks:\\n-  init-memory:\\n-    type: memory\\n-    operation: set\\n-    key: attempt\\n-    value: 0\\n-    namespace: fact-validation\\n-\\n-  extract-facts:\\n-    type: command\\n-    depends_on: [init-memory]\\n-    exec: echo '[{\\\"id\\\":\\\"f1\\\",\\\"valid\\\":false}]'\\n-    output_format: json\\n-    forEach: true\\n-    on_finish:\\n-      run: [aggregate]\\n-      goto_js: |\\n-        const allValid = memory.get('all_valid', 'fact-validation');\\n-        const attempt = memory.get('attempt', 'fact-validation') || 0;\\n-        if (allValid || attempt >= 1) {\\n-          return null;\\n-        }\\n-        memory.increment('attempt', 1, 'fact-validation');\\n-        return 'extract-facts';\\n-\\n-  validate-fact:\\n-    type: command\\n-    depends_on: [extract-facts]\\n-    exec: echo '{\\\"is_valid\\\":{{ outputs[\\\"extract-facts\\\"].valid }}}'\\n-    output_format: json\\n-\\n-  aggregate:\\n-    type: script\\n-    namespace: fact-validation\\n-    content: |\\n-      const validations = outputs.history['validate-fact'] || [];\\n-      const allValid = validations.every(v => v.is_valid === true);\\n-      memory.set('all_valid', allValid, 'fact-validation');\\n-      memory.set('invalid_count', validations.filter(v => !v.is_valid).length, 'fact-validation');\\n-      return { all_valid: allValid, invalid: validations.filter(v => !v.is_valid).length };\\n-\\n-  log-retry:\\n-    type: log\\n-    depends_on: [extract-facts]\\n-    if: \\\"memory.get('attempt', 'fact-validation') > 0\\\"\\n-    message: 'Retried due to {{ memory.get(\\\"invalid_count\\\", \\\"fact-validation\\\") }} invalid facts'\\n-\\n-output:\\n-  pr_comment:\\n-    format: table\\n-    group_by: check\\n-    collapse: false\\n-`;\\n-      const retryOncePath = path.join(testDir, '.visor-retry-once.yaml');\\n-      fs.writeFileSync(retryOncePath, retryOnceConfig);\\n-\\n-      const result = execCLI(['--config', retryOncePath], { cwd: testDir });\\n-\\n-      // Should see retry-related output or complete execution\\n-      expect(result).toBeDefined();\\n-      // Verify it executed without crashing\\n-      expect(result).toMatch(/log-retry|aggregate|extract-facts/i);\\n-    });\\n-\\n-    it('should stop after max attempts and post warning', () => {\\n-      // Invalid facts on both attempts, should give up\\n-      const maxAttemptsConfig = `\\n-version: \\\"1.0\\\"\\n-max_loops: 3\\n-checks:\\n-  init-memory:\\n-    type: memory\\n-    operation: set\\n-    key: attempt\\n-    value: 0\\n-    namespace: fact-validation\\n-\\n-  extract-facts:\\n-    type: command\\n-    depends_on: [init-memory]\\n-    exec: echo '[{\\\"id\\\":\\\"f1\\\",\\\"valid\\\":false}]'\\n-    output_format: json\\n-    forEach: true\\n-    on_finish:\\n-      run: [aggregate]\\n-      goto_js: |\\n-        const allValid = memory.get('all_valid', 'fact-validation');\\n-        const attempt = memory.get('attempt', 'fact-validation') || 0;\\n-        if (allValid) {\\n-          return null;\\n-        }\\n-        if (attempt >= 1) {\\n-          return null; // Max attempts reached\\n-        }\\n-        memory.increment('attempt', 1, 'fact-validation');\\n-        return 'extract-facts';\\n-\\n-  validate-fact:\\n-    type: command\\n-    depends_on: [extract-facts]\\n-    exec: echo '{\\\"is_valid\\\":false}'\\n-    output_format: json\\n-\\n-  aggregate:\\n-    type: script\\n-    namespace: fact-validation\\n-    content: |\\n-      const validations = outputs.history['validate-fact'] || [];\\n-      const allValid = validations.every(v => v.is_valid === true);\\n-      memory.set('all_valid', allValid, 'fact-validation');\\n-      return { all_valid: allValid };\\n-\\n-  post-warning:\\n-    type: log\\n-    depends_on: [extract-facts]\\n-    if: |\\n-      memory.get('all_valid', 'fact-validation') === false &&\\n-      memory.get('attempt', 'fact-validation') >= 1\\n-    message: '‚ö†Ô∏è Warning: Could not verify all facts after max attempts'\\n-\\n-output:\\n-  pr_comment:\\n-    format: table\\n-    group_by: check\\n-    collapse: false\\n-`;\\n-      const maxAttemptsPath = path.join(testDir, '.visor-max-attempts.yaml');\\n-      fs.writeFileSync(maxAttemptsPath, maxAttemptsConfig);\\n-\\n-      const result = execCLI(['--config', maxAttemptsPath], { cwd: testDir });\\n-\\n-      // Should complete execution and have warning check in output\\n-      expect(result).toBeDefined();\\n-      expect(result).toMatch(/post-warning|Warning|aggregate/i);\\n-    });\\n-\\n-    it('should not exceed max_loops', () => {\\n-      // Test that max_loops prevents infinite retries\\n-      const maxLoopsConfig = `\\n-version: \\\"1.0\\\"\\n-max_loops: 2\\n-checks:\\n-  extract-facts:\\n-    type: command\\n-    exec: echo '[{\\\"id\\\":\\\"f1\\\"}]'\\n-    output_format: json\\n-    forEach: true\\n-    on_finish:\\n-      goto_js: |\\n-        // Always retry - should be stopped by max_loops\\n-        return 'extract-facts';\\n-\\n-  validate-fact:\\n-    type: command\\n-    depends_on: [extract-facts]\\n-    exec: echo '{\\\"valid\\\":false}'\\n-    output_format: json\\n-\\n-output:\\n-  pr_comment:\\n-    format: table\\n-    group_by: check\\n-    collapse: false\\n-`;\\n-      const maxLoopsPath = path.join(testDir, '.visor-max-loops.yaml');\\n-      fs.writeFileSync(maxLoopsPath, maxLoopsConfig);\\n-\\n-      const result = execCLI(['--config', maxLoopsPath], { cwd: testDir });\\n-\\n-      // Should stop due to max_loops\\n-      expect(result).toBeDefined();\\n-      // Should not loop indefinitely (test completes)\\n-    });\\n-  });\\n-\\n-  describe('Phase 5.3: Empty Facts / Edge Cases', () => {\\n-    it('should handle no facts extracted ‚Üí direct post', () => {\\n-      // Response has no verifiable facts, should skip validation\\n-      const noFactsConfig = `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  assistant:\\n-    type: command\\n-    exec: echo 'Just a greeting with no facts'\\n-\\n-  extract-facts:\\n-    type: command\\n-    depends_on: [assistant]\\n-    exec: echo '[]'\\n-    output_format: json\\n-    forEach: true\\n-    on_finish:\\n-      run: [aggregate]\\n-\\n-  validate-fact:\\n-    type: command\\n-    depends_on: [extract-facts]\\n-    exec: echo '{\\\"valid\\\":true}'\\n-    output_format: json\\n-\\n-  aggregate:\\n-    type: script\\n-    namespace: fact-validation\\n-    content: |\\n-      const validations = outputs.history['validate-fact'] || [];\\n-      memory.set('fact_count', validations.length, 'fact-validation');\\n-      memory.set('all_valid', validations.length === 0, 'fact-validation');\\n-      return { fact_count: validations.length, no_facts: validations.length === 0 };\\n-\\n-  post-direct:\\n-    type: log\\n-    depends_on: [extract-facts]\\n-    if: \\\"memory.get('fact_count', 'fact-validation') === 0\\\"\\n-    message: 'Posted directly - no facts to validate'\\n-\\n-output:\\n-  pr_comment:\\n-    format: table\\n-    group_by: check\\n-    collapse: false\\n-`;\\n-      const noFactsPath = path.join(testDir, '.visor-no-facts.yaml');\\n-      fs.writeFileSync(noFactsPath, noFactsConfig);\\n-\\n-      const result = execCLI(['--config', noFactsPath], { cwd: testDir });\\n-\\n-      // Should complete and have post-direct check in output\\n-      expect(result).toBeDefined();\\n-      expect(result).toMatch(/post-direct|aggregate|assistant/i);\\n-    });\\n-\\n-    it('should handle empty forEach array', () => {\\n-      // forEach outputs empty array [], dependent checks should not run\\n-      const emptyArrayConfig = `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  extract-facts:\\n-    type: command\\n-    exec: echo '[]'\\n-    output_format: json\\n-    forEach: true\\n-    on_finish:\\n-      run: [log-finish]\\n-\\n-  validate-fact:\\n-    type: command\\n-    depends_on: [extract-facts]\\n-    exec: echo '{\\\"valid\\\":true}'\\n-    output_format: json\\n-\\n-  log-finish:\\n-    type: log\\n-    message: 'on_finish triggered after empty forEach'\\n-\\n-output:\\n-  pr_comment:\\n-    format: table\\n-    group_by: check\\n-    collapse: false\\n-`;\\n-      const emptyArrayPath = path.join(testDir, '.visor-empty-array.yaml');\\n-      fs.writeFileSync(emptyArrayPath, emptyArrayConfig);\\n-\\n-      const result = execCLI(['--config', emptyArrayPath], { cwd: testDir });\\n-\\n-      // Should complete successfully\\n-      expect(result).toBeDefined();\\n-      // Verify forEach had 0 items\\n-      expect(result).toMatch(/Found 0 items for forEach|no items from.*extract-facts/i);\\n-      // Dependent check should be skipped\\n-      expect(result).toMatch(/validate-fact.*‚è≠|Skipped.*validate-fact/i);\\n-    });\\n-\\n-    it('should handle malformed fact extraction', () => {\\n-      // Fact extraction returns invalid JSON, should handle gracefully\\n-      const malformedConfig = `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  extract-facts:\\n-    type: command\\n-    exec: echo 'INVALID JSON {'\\n-    transform_js: |\\n-      try {\\n-        return JSON.parse(output);\\n-      } catch (e) {\\n-        log('Parse error:', e.message);\\n-        return [];\\n-      }\\n-    forEach: true\\n-    on_finish:\\n-      run: [handle-error]\\n-\\n-  validate-fact:\\n-    type: command\\n-    depends_on: [extract-facts]\\n-    exec: echo '{\\\"valid\\\":true}'\\n-    output_format: json\\n-\\n-  handle-error:\\n-    type: script\\n-    namespace: fact-validation\\n-    content: |\\n-      const items = outputs['extract-facts'] || [];\\n-      memory.set('parse_error', Array.isArray(items) && items.length === 0, 'fact-validation');\\n-      return { handled_error: true };\\n-\\n-  post-fallback:\\n-    type: log\\n-    depends_on: [extract-facts]\\n-    if: \\\"memory.get('parse_error', 'fact-validation') === true\\\"\\n-    message: 'Posted with fallback due to parse error'\\n-\\n-output:\\n-  pr_comment:\\n-    format: table\\n-    group_by: check\\n-    collapse: false\\n-`;\\n-      const malformedPath = path.join(testDir, '.visor-malformed.yaml');\\n-      fs.writeFileSync(malformedPath, malformedConfig);\\n-\\n-      const result = execCLI(['--config', malformedPath], { cwd: testDir });\\n-\\n-      // Should handle error gracefully and not crash\\n-      expect(result).toBeDefined();\\n-      expect(result).toMatch(/post-fallback|handle-error|extract-facts/i);\\n-    });\\n-  });\\n-\\n-  describe('Phase 5.4: Validation Disabled', () => {\\n-    it('should bypass validation when ENABLE_FACT_VALIDATION=false', () => {\\n-      // Validation disabled, should skip validation checks entirely\\n-      const disabledConfig = `\\n-version: \\\"1.0\\\"\\n-env:\\n-  ENABLE_FACT_VALIDATION: \\\"false\\\"\\n-\\n-checks:\\n-  assistant:\\n-    type: command\\n-    exec: echo 'Response with facts'\\n-\\n-  extract-facts:\\n-    type: command\\n-    depends_on: [assistant]\\n-    if: \\\"env.ENABLE_FACT_VALIDATION === 'true'\\\"\\n-    exec: echo '[]'\\n-    output_format: json\\n-    forEach: true\\n-\\n-  validate-fact:\\n-    type: command\\n-    depends_on: [extract-facts]\\n-    if: \\\"env.ENABLE_FACT_VALIDATION === 'true'\\\"\\n-    exec: echo '{\\\"valid\\\":true}'\\n-    output_format: json\\n-\\n-  post-direct:\\n-    type: log\\n-    depends_on: [assistant]\\n-    if: \\\"env.ENABLE_FACT_VALIDATION !== 'true'\\\"\\n-    message: 'Posted directly - validation disabled'\\n-\\n-output:\\n-  pr_comment:\\n-    format: table\\n-    group_by: check\\n-    collapse: false\\n-`;\\n-      const disabledPath = path.join(testDir, '.visor-disabled.yaml');\\n-      fs.writeFileSync(disabledPath, disabledConfig);\\n-\\n-      const result = execCLI(['--config', disabledPath], { cwd: testDir });\\n-\\n-      // Should skip validation and post directly\\n-      expect(result).toBeDefined();\\n-      expect(result).toMatch(/post-direct|assistant/i);\\n-      // Validation checks should be skipped (shown as skipped in output)\\n-      expect(result).toMatch(/‚è≠.*Skipped.*if.*env\\\\.ENABLE_FACT_VALIDATION/i);\\n-      // Should see post-direct executed\\n-      expect(result).toMatch(/Posted directly - validation disabled/i);\\n-    });\\n-\\n-    it('should use conditional checks correctly', () => {\\n-      // Verify if conditions work correctly for both enabled and disabled states\\n-      const conditionalConfig = `\\n-version: \\\"1.0\\\"\\n-env:\\n-  ENABLE_FACT_VALIDATION: \\\"true\\\"\\n-\\n-checks:\\n-  assistant:\\n-    type: command\\n-    exec: echo 'Response'\\n-\\n-  extract-facts:\\n-    type: command\\n-    depends_on: [assistant]\\n-    if: \\\"env.ENABLE_FACT_VALIDATION === 'true'\\\"\\n-    exec: echo '[{\\\"id\\\":\\\"f1\\\"}]'\\n-    output_format: json\\n-    forEach: true\\n-    on_finish:\\n-      run: [aggregate]\\n-\\n-  validate-fact:\\n-    type: command\\n-    depends_on: [extract-facts]\\n-    if: \\\"env.ENABLE_FACT_VALIDATION === 'true'\\\"\\n-    exec: echo '{\\\"valid\\\":true}'\\n-    output_format: json\\n-\\n-  aggregate:\\n-    type: script\\n-    namespace: fact-validation\\n-    content: |\\n-      memory.set('validation_ran', true, 'fact-validation');\\n-      return { validation_ran: true };\\n-\\n-  post-verified:\\n-    type: log\\n-    depends_on: [extract-facts]\\n-    if: \\\"env.ENABLE_FACT_VALIDATION === 'true'\\\"\\n-    message: 'Posted via validation path'\\n-\\n-  post-direct:\\n-    type: log\\n-    depends_on: [assistant]\\n-    if: \\\"env.ENABLE_FACT_VALIDATION !== 'true'\\\"\\n-    message: 'Posted via direct path'\\n-\\n-output:\\n-  pr_comment:\\n-    format: table\\n-    group_by: check\\n-    collapse: false\\n-`;\\n-      const conditionalPath = path.join(testDir, '.visor-conditional.yaml');\\n-      fs.writeFileSync(conditionalPath, conditionalConfig);\\n-\\n-      const result = execCLI(['--config', conditionalPath], { cwd: testDir });\\n-\\n-      // With validation enabled, should use validation path\\n-      expect(result).toBeDefined();\\n-      expect(result).toMatch(/post-verified|aggregate|extract-facts/i);\\n-      expect(result).not.toMatch(/post-direct.*direct path/i);\\n-      // Should run validation checks\\n-      expect(result).toMatch(/extract-facts/i);\\n-      expect(result).toMatch(/validate-fact/i);\\n-    });\\n-  });\\n-});\\n\",\"status\":\"removed\"},{\"filename\":\"tests/e2e/on-finish-loop-budget-e2e.test.ts\",\"additions\":1,\"deletions\":0,\"changes\":5,\"patch\":\"diff --git a/tests/e2e/on-finish-loop-budget-e2e.test.ts b/tests/e2e/on-finish-loop-budget-e2e.test.ts\\nindex 3c917bf6..7ebdfc5e 100644\\n--- a/tests/e2e/on-finish-loop-budget-e2e.test.ts\\n+++ b/tests/e2e/on-finish-loop-budget-e2e.test.ts\\n@@ -65,6 +65,11 @@ checks:\\n       run: [child-log]\\n       goto: other-log\\n \\n+  process-item:\\n+    type: log\\n+    message: Processing item\\n+    depends_on: [parent]\\n+\\n   child-log:\\n     type: log\\n     message: CHILD\\n\",\"status\":\"added\"},{\"filename\":\"tests/e2e/session-reuse-e2e.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":7,\"patch\":\"diff --git a/tests/e2e/session-reuse-e2e.test.ts b/tests/e2e/session-reuse-e2e.test.ts\\nindex 53b4e523..053814ab 100644\\n--- a/tests/e2e/session-reuse-e2e.test.ts\\n+++ b/tests/e2e/session-reuse-e2e.test.ts\\n@@ -202,9 +202,10 @@ fail_fast: false\\n     expect(result.reviewSummary.issues).toBeDefined();\\n     expect(Array.isArray(result.reviewSummary.issues)).toBe(true);\\n \\n-    // Verify debug information is available\\n-    expect(result.debug).toBeDefined();\\n-    expect(result.debug?.checksExecuted).toEqual(result.checksExecuted);\\n+    // Verify debug information when available\\n+    if (result.debug) {\\n+      expect(result.debug.checksExecuted).toEqual(result.checksExecuted);\\n+    }\\n \\n     // Verify session reuse affected parallelism\\n     // security-analysis and security-remediation should run sequentially\\n\",\"status\":\"modified\"},{\"filename\":\"tests/e2e/telemetry-mermaid-e2e.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/tests/e2e/telemetry-mermaid-e2e.test.ts b/tests/e2e/telemetry-mermaid-e2e.test.ts\\nindex e21531ef..796fac7f 100644\\n--- a/tests/e2e/telemetry-mermaid-e2e.test.ts\\n+++ b/tests/e2e/telemetry-mermaid-e2e.test.ts\\n@@ -21,7 +21,9 @@ describe('Telemetry E2E ‚Äî Mermaid diagram telemetry (full code)', () => {\\n   beforeAll(() => {\\n     fs.mkdirSync(tempDir, { recursive: true });\\n     fs.mkdirSync(tracesDir, { recursive: true });\\n+  });\\n \\n+  beforeEach(() => {\\n     // Build a check that renders a template containing a mermaid block\\n     const cfg = {\\n       version: '1.0',\\n@@ -38,9 +40,7 @@ describe('Telemetry E2E ‚Äî Mermaid diagram telemetry (full code)', () => {\\n     } as const;\\n \\n     fs.writeFileSync(configPath, yaml.dump(cfg), 'utf8');\\n-  });\\n \\n-  beforeEach(() => {\\n     mockConsoleLog = jest.fn();\\n     mockConsoleError = jest.fn();\\n     mockProcessExit = jest.fn();\\n\",\"status\":\"modified\"},{\"filename\":\"tests/engine/criticality-guarantee-retry.engine.test.ts\",\"additions\":2,\"deletions\":0,\"changes\":51,\"patch\":\"diff --git a/tests/engine/criticality-guarantee-retry.engine.test.ts b/tests/engine/criticality-guarantee-retry.engine.test.ts\\nnew file mode 100644\\nindex 00000000..cb194a97\\n--- /dev/null\\n+++ b/tests/engine/criticality-guarantee-retry.engine.test.ts\\n@@ -0,0 +1,51 @@\\n+import { StateMachineExecutionEngine } from '../../src/state-machine-execution-engine';\\n+import type { VisorConfig } from '../../src/types/config';\\n+\\n+describe('Engine: guarantee + retry across criticality modes', () => {\\n+  const baseCfg: Partial<VisorConfig> = {\\n+    version: '1.0',\\n+    output: { pr_comment: { enabled: false } } as any,\\n+  };\\n+\\n+  const makeCheck = (criticality?: 'external' | 'internal' | 'policy' | 'info') => ({\\n+    type: 'script',\\n+    content: 'return { ok: false };',\\n+    // Logical failure (policy violation)\\n+    fail_if: 'true',\\n+    on_fail: { retry: { max: 2 } },\\n+    ...(criticality ? { criticality } : {}),\\n+  });\\n+\\n+  it('policy: retries are honored for logical failure (fail_if)', async () => {\\n+    const cfg = {\\n+      ...baseCfg,\\n+      checks: { p: makeCheck('policy' as any) },\\n+    } as VisorConfig;\\n+    const engine = new StateMachineExecutionEngine();\\n+    const res = await engine.executeChecks({ checks: ['p'], config: cfg, debug: false });\\n+    const st = (res.executionStatistics?.checks || []).find(s => s.checkName === 'p');\\n+    expect(st?.totalRuns || 0).toBeGreaterThan(1); // retries occurred\\n+  });\\n+\\n+  it('internal: retries suppressed for logical failure', async () => {\\n+    const cfg = {\\n+      ...baseCfg,\\n+      checks: { c: makeCheck('internal') },\\n+    } as VisorConfig;\\n+    const engine = new StateMachineExecutionEngine();\\n+    const res = await engine.executeChecks({ checks: ['c'], config: cfg, debug: false });\\n+    const st = (res.executionStatistics?.checks || []).find(s => s.checkName === 'c');\\n+    expect(st?.totalRuns).toBe(1);\\n+  });\\n+\\n+  it('external: retries suppressed for logical failure', async () => {\\n+    const cfg = {\\n+      ...baseCfg,\\n+      checks: { e: makeCheck('external') },\\n+    } as VisorConfig;\\n+    const engine = new StateMachineExecutionEngine();\\n+    const res = await engine.executeChecks({ checks: ['e'], config: cfg, debug: false });\\n+    const st = (res.executionStatistics?.checks || []).find(s => s.checkName === 'e');\\n+    expect(st?.totalRuns).toBe(1);\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/engine/criticality-retry.engine.test.ts\",\"additions\":1,\"deletions\":0,\"changes\":32,\"patch\":\"diff --git a/tests/engine/criticality-retry.engine.test.ts b/tests/engine/criticality-retry.engine.test.ts\\nnew file mode 100644\\nindex 00000000..349dd6b7\\n--- /dev/null\\n+++ b/tests/engine/criticality-retry.engine.test.ts\\n@@ -0,0 +1,32 @@\\n+import { describe, it, expect } from '@jest/globals';\\n+import { StateMachineExecutionEngine } from '../../src/state-machine-execution-engine';\\n+import type { VisorConfig } from '../../src/types/config';\\n+\\n+describe('Engine: criticality mapping for on_fail.retry', () => {\\n+  it(\\\"suppresses retry on logical failure when criticality is 'external'\\\", async () => {\\n+    const config: VisorConfig = {\\n+      version: '1.0',\\n+      checks: {\\n+        step: {\\n+          type: 'log',\\n+          message: 'hello',\\n+          level: 'info',\\n+          // Force a logical failure via fail_if\\n+          fail_if: 'true',\\n+          criticality: 'external',\\n+          on_fail: {\\n+            retry: { max: 3 },\\n+          },\\n+        },\\n+      },\\n+      output: { format: 'json' },\\n+    } as any;\\n+\\n+    const engine = new StateMachineExecutionEngine();\\n+    const result = await engine.executeChecks({ checks: ['step'], config, debug: false });\\n+    const st = (result.executionStatistics?.checks || []).find(s => s.checkName === 'step');\\n+    expect(st?.totalRuns).toBe(1); // initial run only, retries suppressed\\n+    expect(st?.successfulRuns || 0).toBe(0);\\n+    expect(st?.failedRuns || 0).toBeGreaterThanOrEqual(1);\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/engine/foreach-on-finish.engine.test.ts\",\"additions\":6,\"deletions\":0,\"changes\":199,\"patch\":\"diff --git a/tests/engine/foreach-on-finish.engine.test.ts b/tests/engine/foreach-on-finish.engine.test.ts\\nnew file mode 100644\\nindex 00000000..181beeb7\\n--- /dev/null\\n+++ b/tests/engine/foreach-on-finish.engine.test.ts\\n@@ -0,0 +1,199 @@\\n+import { describe, it, expect } from '@jest/globals';\\n+import { StateMachineExecutionEngine } from '../../src/state-machine-execution-engine';\\n+import { MemoryStore } from '../../src/memory-store';\\n+import type { VisorConfig } from '../../src/types/config';\\n+\\n+describe('Engine: forEach with on_finish (native)', () => {\\n+  // No filesystem or git needed; run engine natively with in-memory config\\n+\\n+  it('all facts valid ‚Üí direct post (no retry)', async () => {\\n+    MemoryStore.resetInstance();\\n+    // No shell exec needed for this native test\\n+\\n+    const config: VisorConfig = {\\n+      version: '1.0',\\n+      checks: {\\n+        'extract-facts': {\\n+          type: 'script',\\n+          content:\\n+            'return [{id:\\\"f1\\\", claim:\\\"Valid 1\\\", valid:true}, {id:\\\"f2\\\", claim:\\\"Valid 2\\\", valid:true}];',\\n+          forEach: true,\\n+          on_finish: {\\n+            run: ['aggregate'],\\n+          },\\n+        },\\n+        'validate-fact': {\\n+          type: 'script',\\n+          depends_on: ['extract-facts'],\\n+          fanout: 'map',\\n+          content:\\n+            'return { fact_id: outputs[\\\"extract-facts\\\"].id, is_valid: outputs[\\\"extract-facts\\\"].valid };',\\n+        },\\n+        aggregate: {\\n+          type: 'script',\\n+          namespace: 'fact-validation',\\n+          // Compute all_valid from the most recent cycle of validate-fact.\\n+          // Derive the last cycle size from the latest extract-facts array entry.\\n+          content: [\\n+            \\\"const vf = (outputs.history['validate-fact']||[]).filter(v => v && typeof v === 'object');\\\",\\n+            \\\"const ex = (outputs.history['extract-facts']||[]);\\\",\\n+            'let lastSize = 0; for (let i = ex.length - 1; i >= 0 && lastSize === 0; i--) { if (Array.isArray(ex[i])) { lastSize = ex[i].length; } }',\\n+            'const recent = lastSize > 0 ? vf.slice(-lastSize) : vf;',\\n+            'const allValid = recent.length > 0 && recent.every(i => i && i.is_valid === true);',\\n+            \\\"memory.set('all_valid', allValid, 'fact-validation');\\\",\\n+            'return { all_valid: allValid };',\\n+          ].join('\\\\n'),\\n+        },\\n+        'post-verified': {\\n+          type: 'log',\\n+          depends_on: ['extract-facts'],\\n+          if: \\\"memory.get('all_valid', 'fact-validation') === true\\\",\\n+          message: '‚úÖ Posted verified response',\\n+        },\\n+      },\\n+    } as any;\\n+\\n+    const engine = new StateMachineExecutionEngine();\\n+    const prInfo: any = {\\n+      number: 1,\\n+      title: 't',\\n+      author: 'a',\\n+      base: 'main',\\n+      head: 'branch',\\n+      files: [],\\n+      totalAdditions: 0,\\n+      totalDeletions: 0,\\n+      eventType: 'manual',\\n+    };\\n+    const result = await engine.executeGroupedChecks(\\n+      prInfo,\\n+      // Do not request 'aggregate' explicitly; it runs via on_finish.run\\n+      ['extract-facts', 'validate-fact', 'post-verified'],\\n+      undefined,\\n+      config,\\n+      'table',\\n+      false\\n+    );\\n+\\n+    // Validate statistics\\n+    const stats = result.statistics!;\\n+    const byName: Record<string, any> = {};\\n+    for (const s of stats.checks || []) byName[s.checkName] = s;\\n+\\n+    expect(byName['extract-facts']).toBeDefined();\\n+    expect(byName['validate-fact']).toBeDefined();\\n+    expect(byName['aggregate']).toBeDefined();\\n+    expect(byName['post-verified']).toBeDefined();\\n+    expect(byName['post-verified'].successfulRuns || 0).toBeGreaterThanOrEqual(1);\\n+\\n+    // Validate output history: 2 validations\\n+    const hist = engine.getOutputHistorySnapshot() as any;\\n+    const entries = hist['validate-fact'] || [];\\n+    expect(Array.isArray(entries)).toBe(true);\\n+    expect(entries.length).toBe(2);\\n+  });\\n+\\n+  it('some facts invalid ‚Üí single retry ‚Üí post', async () => {\\n+    MemoryStore.resetInstance();\\n+    const config: VisorConfig = {\\n+      version: '1.0',\\n+      checks: {\\n+        'init-memory': {\\n+          type: 'memory',\\n+          operation: 'set',\\n+          key: 'attempt',\\n+          value: 0,\\n+          namespace: 'fact-validation',\\n+        },\\n+        'extract-facts': {\\n+          type: 'script',\\n+          depends_on: ['init-memory'],\\n+          content: [\\n+            \\\"const att = memory.get('attempt','fact-validation') || 0;\\\",\\n+            \\\"if (att >= 1) return [{id:'f1', claim:'Corrected', valid:true}];\\\",\\n+            \\\"return [{id:'f1', claim:'Wrong', valid:false}];\\\",\\n+          ].join('\\\\n'),\\n+          forEach: true,\\n+          on_finish: {\\n+            run: ['aggregate'],\\n+            goto_js: [\\n+              \\\"const ok = memory.get('all_valid', 'fact-validation');\\\",\\n+              \\\"const att = memory.get('attempt', 'fact-validation') || 0;\\\",\\n+              'if (ok) return null;',\\n+              'if (att >= 1) return null;',\\n+              \\\"memory.increment('attempt', 1, 'fact-validation');\\\",\\n+              \\\"return 'extract-facts';\\\",\\n+            ].join('\\\\n'),\\n+          },\\n+        },\\n+        'validate-fact': {\\n+          type: 'script',\\n+          depends_on: ['extract-facts'],\\n+          fanout: 'map',\\n+          content:\\n+            'return { fact_id: outputs[\\\"extract-facts\\\"].id, is_valid: outputs[\\\"extract-facts\\\"].valid };',\\n+        },\\n+        aggregate: {\\n+          type: 'script',\\n+          namespace: 'fact-validation',\\n+          content: [\\n+            \\\"const vf = (outputs.history['validate-fact']||[]).filter(v => v && typeof v === 'object');\\\",\\n+            \\\"const ex = (outputs.history['extract-facts']||[]);\\\",\\n+            'let lastSize = 0; for (let i = ex.length - 1; i >= 0 && lastSize === 0; i--) { if (Array.isArray(ex[i])) { lastSize = ex[i].length; } }',\\n+            'const recent = lastSize > 0 ? vf.slice(-lastSize) : vf;',\\n+            'const allValid = recent.length > 0 && recent.every(i => i && i.is_valid === true);',\\n+            \\\"memory.set('all_valid', allValid, 'fact-validation');\\\",\\n+            'return { all_valid: allValid };',\\n+          ].join('\\\\n'),\\n+        },\\n+        'post-verified': {\\n+          type: 'log',\\n+          depends_on: ['extract-facts'],\\n+          if: \\\"memory.get('all_valid', 'fact-validation') === true\\\",\\n+          message: 'Posted after successful retry',\\n+        },\\n+      },\\n+    } as any;\\n+\\n+    const engine = new StateMachineExecutionEngine();\\n+    const prInfo: any = {\\n+      number: 1,\\n+      title: 't',\\n+      author: 'a',\\n+      base: 'main',\\n+      head: 'branch',\\n+      files: [],\\n+      totalAdditions: 0,\\n+      totalDeletions: 0,\\n+      eventType: 'manual',\\n+    };\\n+    const result = await engine.executeGroupedChecks(\\n+      prInfo,\\n+      // Do not request 'aggregate' explicitly; it runs via on_finish.run\\n+      ['init-memory', 'extract-facts', 'validate-fact', 'post-verified'],\\n+      undefined,\\n+      config,\\n+      'table',\\n+      false\\n+    );\\n+\\n+    const stats = result.statistics!;\\n+    const byName: Record<string, any> = {};\\n+    for (const s of stats.checks || []) byName[s.checkName] = s;\\n+\\n+    // extract-facts should be re-run at least once\\n+    expect(byName['extract-facts'].totalRuns || 0).toBeGreaterThanOrEqual(1);\\n+    // post should eventually run\\n+    expect(byName['post-verified'].successfulRuns || 0).toBeGreaterThanOrEqual(1);\\n+\\n+    // Validate history flipped to valid in the last entry\\n+    const hist = engine.getOutputHistorySnapshot() as any;\\n+    const vals = hist['validate-fact'] || [];\\n+    expect(Array.isArray(vals)).toBe(true);\\n+    expect(vals.length).toBeGreaterThanOrEqual(1);\\n+    const last = vals[vals.length - 1];\\n+    const lastItems = last && last.isForEach ? last.forEachItems || [] : last ? [last] : [];\\n+    expect(lastItems.length).toBeGreaterThanOrEqual(1);\\n+    expect(lastItems[lastItems.length - 1]?.is_valid).toBe(true);\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/engine/schema-validation-foreach.engine.test.ts\",\"additions\":2,\"deletions\":0,\"changes\":43,\"patch\":\"diff --git a/tests/engine/schema-validation-foreach.engine.test.ts b/tests/engine/schema-validation-foreach.engine.test.ts\\nnew file mode 100644\\nindex 00000000..a81c8dd1\\n--- /dev/null\\n+++ b/tests/engine/schema-validation-foreach.engine.test.ts\\n@@ -0,0 +1,43 @@\\n+import { StateMachineExecutionEngine } from '../../src/state-machine-execution-engine';\\n+import type { VisorConfig } from '../../src/types/config';\\n+\\n+describe('Schema validation for forEach map (script -> script)', () => {\\n+  it('adds schema_validation_failed for invalid per-item outputs and surfaces in summary', async () => {\\n+    const cfg: VisorConfig = {\\n+      version: '1.0',\\n+      output: { pr_comment: { enabled: false } } as any,\\n+      checks: {\\n+        list: {\\n+          type: 'script',\\n+          forEach: true,\\n+          content: 'return [{x:1},{x:\\\"bad\\\"}]',\\n+        },\\n+        mapStep: {\\n+          type: 'script',\\n+          depends_on: ['list'],\\n+          fanout: 'map',\\n+          content: 'return { x: outputs[\\\"list\\\"].x };',\\n+          schema: {\\n+            type: 'object',\\n+            properties: { x: { type: 'number' } },\\n+            required: ['x'],\\n+            additionalProperties: true,\\n+          },\\n+        },\\n+      },\\n+    } as any;\\n+\\n+    const engine = new StateMachineExecutionEngine();\\n+    const res = await engine.executeChecks({\\n+      checks: ['list', 'mapStep'],\\n+      config: cfg,\\n+      debug: false,\\n+    });\\n+    const issues = (res.reviewSummary.issues || []).filter(\\n+      i =>\\n+        String(i.ruleId || '').includes('contract/schema_validation_failed') &&\\n+        i.checkName === 'mapStep'\\n+    );\\n+    expect(issues.length).toBeGreaterThanOrEqual(1);\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/integration/criticality-noncritical-continue.integration.test.ts\",\"additions\":1,\"deletions\":0,\"changes\":36,\"patch\":\"diff --git a/tests/integration/criticality-noncritical-continue.integration.test.ts b/tests/integration/criticality-noncritical-continue.integration.test.ts\\nnew file mode 100644\\nindex 00000000..8aa4ba0a\\n--- /dev/null\\n+++ b/tests/integration/criticality-noncritical-continue.integration.test.ts\\n@@ -0,0 +1,36 @@\\n+import { StateMachineExecutionEngine } from '../../src/state-machine-execution-engine';\\n+import type { VisorConfig } from '../../src/types/config';\\n+\\n+describe('Criticality integration: info continues despite failure', () => {\\n+  it('dependent runs when dependency is info and fails logically', async () => {\\n+    const cfg: VisorConfig = {\\n+      version: '1.0',\\n+      output: { format: 'json' },\\n+      checks: {\\n+        a: {\\n+          type: 'log',\\n+          message: 'A running',\\n+          level: 'info',\\n+          fail_if: 'true', // logical failure\\n+          criticality: 'info',\\n+        },\\n+        b: {\\n+          type: 'log',\\n+          message: 'B should still run',\\n+          level: 'info',\\n+          depends_on: ['a'],\\n+        },\\n+      },\\n+    } as any;\\n+\\n+    const engine = new StateMachineExecutionEngine();\\n+    const res = await engine.executeChecks({ checks: ['a', 'b'], config: cfg, debug: false });\\n+    const stats = res.executionStatistics?.checks || [];\\n+    const by = Object.fromEntries(stats.map(s => [s.checkName, s]));\\n+\\n+    expect(by['a']?.totalRuns).toBe(1);\\n+    expect(by['a']?.failedRuns || 0).toBeGreaterThanOrEqual(1);\\n+    // info should allow dependent b to run\\n+    expect(by['b']?.totalRuns).toBe(1);\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/integration/depends-on-continue-on-failure.test.ts\",\"additions\":2,\"deletions\":0,\"changes\":68,\"patch\":\"diff --git a/tests/integration/depends-on-continue-on-failure.test.ts b/tests/integration/depends-on-continue-on-failure.test.ts\\nnew file mode 100644\\nindex 00000000..a4289c18\\n--- /dev/null\\n+++ b/tests/integration/depends-on-continue-on-failure.test.ts\\n@@ -0,0 +1,68 @@\\n+import { CheckExecutionEngine } from '../../src/check-execution-engine';\\n+import type { VisorConfig } from '../../src/types/config';\\n+\\n+describe('depends_on gating with continue_on_failure', () => {\\n+  test('blocks dependent when dependency fails by default', async () => {\\n+    const engine = new CheckExecutionEngine(process.cwd());\\n+    const cfg: VisorConfig = {\\n+      version: '1.0',\\n+      checks: {\\n+        failer: {\\n+          type: 'command',\\n+          // Emit a non-zero exit via node process.exit(1)\\n+          exec: \\\"node -e 'process.exit(1)'\\\",\\n+          on: ['manual'],\\n+        },\\n+        dep: {\\n+          type: 'command',\\n+          exec: 'node -e \\\"console.log(JSON.stringify({\\\\\\\\\\\"ok\\\\\\\\\\\":true}))\\\"',\\n+          depends_on: ['failer'],\\n+          on: ['manual'],\\n+        },\\n+      },\\n+      output: { pr_comment: { format: 'markdown', group_by: 'check', collapse: false } },\\n+    } as any;\\n+\\n+    const res = await engine.executeChecks({\\n+      checks: ['failer', 'dep'],\\n+      config: cfg,\\n+      webhookContext: { eventType: 'manual' } as any,\\n+    });\\n+\\n+    const hist = (res.reviewSummary as any).history || {};\\n+    const depHist = Array.isArray(hist['dep']) ? hist['dep'] : [];\\n+    expect(depHist.length).toBe(0);\\n+  });\\n+\\n+  test('allows dependent when dependency has continue_on_failure: true', async () => {\\n+    const engine = new CheckExecutionEngine(process.cwd());\\n+    const cfg: VisorConfig = {\\n+      version: '1.0',\\n+      checks: {\\n+        failer: {\\n+          type: 'command',\\n+          exec: \\\"node -e 'process.exit(1)'\\\",\\n+          on: ['manual'],\\n+          continue_on_failure: true,\\n+        },\\n+        dep: {\\n+          type: 'command',\\n+          exec: 'node -e \\\"console.log(JSON.stringify({\\\\\\\\\\\"ok\\\\\\\\\\\":true}))\\\"',\\n+          depends_on: ['failer'],\\n+          on: ['manual'],\\n+        },\\n+      },\\n+      output: { pr_comment: { format: 'markdown', group_by: 'check', collapse: false } },\\n+    } as any;\\n+\\n+    const res = await engine.executeChecks({\\n+      checks: ['failer', 'dep'],\\n+      config: cfg,\\n+      webhookContext: { eventType: 'manual' } as any,\\n+    });\\n+\\n+    const hist = (res.reviewSummary as any).history || {};\\n+    const depHist = Array.isArray(hist['dep']) ? hist['dep'] : [];\\n+    expect(depHist.length).toBe(1);\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/integration/forward-run-dedupe-success.test.ts\",\"additions\":2,\"deletions\":0,\"changes\":62,\"patch\":\"diff --git a/tests/integration/forward-run-dedupe-success.test.ts b/tests/integration/forward-run-dedupe-success.test.ts\\nnew file mode 100644\\nindex 00000000..dc4d7136\\n--- /dev/null\\n+++ b/tests/integration/forward-run-dedupe-success.test.ts\\n@@ -0,0 +1,62 @@\\n+import { CheckExecutionEngine } from '../../src/check-execution-engine';\\n+import type { VisorConfig } from '../../src/types/config';\\n+\\n+describe('forward-run dedupe after success (multi-turn)', () => {\\n+  test('finish runs exactly once after final refine success', async () => {\\n+    const engine = new CheckExecutionEngine(process.cwd());\\n+\\n+    const cfg: VisorConfig = {\\n+      version: '1.0',\\n+      checks: {\\n+        ask: {\\n+          type: 'script',\\n+          content: `({ ok: true })`,\\n+          on: ['manual'],\\n+        },\\n+        refine: {\\n+          type: 'script',\\n+          depends_on: ['ask'],\\n+          on: ['manual'],\\n+          // Compute attempt from refine.history; flip refined=true on 3rd run\\n+          content: `\\n+(() => {\\n+  const histRef = (outputs && outputs.history && outputs.history['refine']) || [];\\n+  const attempt = Array.isArray(histRef) ? histRef.length + 1 : 1;\\n+  log('[refine.content]', 'histRefLen=', Array.isArray(histRef) ? histRef.length : 'NA', 'attempt=', attempt);\\n+  if (attempt === 1) return { refined: false, text: 'Which CI platform and trigger conditions?' };\\n+  if (attempt === 2) return { refined: false, text: 'What Node version and commands should run?' };\\n+  return { refined: true, text: 'Set up GitHub Actions workflow: on push to main, use Node 18.x, cache npm, run npm ci && npm test.' };\\n+})()\\n+`,\\n+          fail_if: `\\n+            log('[refine.fail_if]', 'refined?', output && output.refined, 'text=', output && output.text);\\n+            output && output.refined !== true\\n+          `,\\n+          on_fail: { goto: 'ask' },\\n+        },\\n+        finish: {\\n+          type: 'script',\\n+          depends_on: ['refine'],\\n+          on: ['manual'],\\n+          content: `({ text: (outputs['refine'] && outputs['refine'].text) || '' })`,\\n+        },\\n+      },\\n+      output: { pr_comment: { format: 'markdown', group_by: 'check', collapse: false } },\\n+    } as any;\\n+\\n+    const res = await engine.executeChecks({\\n+      checks: ['ask', 'refine', 'finish'],\\n+      config: cfg,\\n+      webhookContext: { eventType: 'manual' } as any,\\n+    });\\n+\\n+    const hist = (res.reviewSummary as any).history || {};\\n+    const askRuns = Array.isArray(hist['ask']) ? hist['ask'].length : 0;\\n+    const refineRuns = Array.isArray(hist['refine']) ? hist['refine'].length : 0;\\n+    const finishRuns = Array.isArray(hist['finish']) ? hist['finish'].length : 0;\\n+\\n+    expect(askRuns).toBeGreaterThanOrEqual(3);\\n+    expect(refineRuns).toBe(3);\\n+    expect(finishRuns).toBe(1);\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/integration/github-workflow.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":14,\"patch\":\"diff --git a/tests/integration/github-workflow.test.ts b/tests/integration/github-workflow.test.ts\\nindex b319805a..a6f2707e 100644\\n--- a/tests/integration/github-workflow.test.ts\\n+++ b/tests/integration/github-workflow.test.ts\\n@@ -41,13 +41,15 @@ jest.mock('../../src/check-execution-engine', () => {\\n         .fn()\\n         .mockImplementation(async (_prInfo, _checks, _unused1, _config, _unused2, _debug) => {\\n           // Return ExecutionResult format\\n+          // State machine groups by checkName when no explicit group is set\\n+          const checkName = _checks[0] || 'security-review';\\n           return {\\n             results: {\\n-              default: [\\n+              [checkName]: [\\n                 {\\n-                  checkName: 'security-review',\\n+                  checkName: checkName,\\n                   content: `## Security Issues Found\\\\n\\\\n- **CRITICAL**: Potential hardcoded API key detected (src/test.ts:10)\\\\n- **WARNING**: Consider using a more efficient data structure (src/test.ts:25)\\\\n\\\\n## Suggestions\\\\n\\\\n- Consider adding input validation\\\\n- Add unit tests for new functionality`,\\n-                  group: 'default',\\n+                  group: checkName,\\n                   debug: {\\n                     provider: 'google',\\n                     model: 'gemini-2.0-flash-exp',\\n@@ -267,13 +269,13 @@ describe('GitHub PR Workflow Integration', () => {\\n       });\\n \\n       // Verify review structure (new GroupedCheckResults format)\\n+      // State machine groups by checkName when no explicit group is set\\n       expect(review).toEqual(\\n         expect.objectContaining({\\n-          default: expect.arrayContaining([\\n+          'security-review': expect.arrayContaining([\\n             expect.objectContaining({\\n               checkName: 'security-review',\\n-              content: expect.stringContaining('Security Issues Found'),\\n-              group: 'default',\\n+              group: 'security-review',\\n             }),\\n           ]),\\n         })\\n\",\"status\":\"modified\"},{\"filename\":\"tests/integration/on-fail-no-cascade.test.ts\",\"additions\":1,\"deletions\":0,\"changes\":35,\"patch\":\"diff --git a/tests/integration/on-fail-no-cascade.test.ts b/tests/integration/on-fail-no-cascade.test.ts\\nnew file mode 100644\\nindex 00000000..f28dc2a5\\n--- /dev/null\\n+++ b/tests/integration/on-fail-no-cascade.test.ts\\n@@ -0,0 +1,35 @@\\n+import { CheckExecutionEngine } from '../../src/check-execution-engine';\\n+import type { VisorConfig } from '../../src/types/config';\\n+\\n+describe('on_fail forward-run does not cascade success chains', () => {\\n+  it('skips finish when refine fails via fail_if', async () => {\\n+    const engine = new CheckExecutionEngine(process.cwd());\\n+    const cfg: VisorConfig = {\\n+      version: '1.0',\\n+      checks: {\\n+        ask: { type: 'command', exec: 'echo ask', on_success: { goto: 'refine' }, on: ['manual'] },\\n+        refine: {\\n+          type: 'command',\\n+          depends_on: ['ask'],\\n+          exec: 'node -e \\\"console.log(JSON.stringify({refined:false}))\\\"',\\n+          fail_if: 'output && output.refined !== true',\\n+          on_fail: { goto: 'ask' },\\n+          on_success: { goto: 'finish' },\\n+          on: ['manual'],\\n+        },\\n+        finish: { type: 'command', depends_on: ['refine'], exec: 'echo done', on: ['manual'] },\\n+      },\\n+      output: { pr_comment: { format: 'markdown', group_by: 'check', collapse: false } },\\n+      routing: { max_loops: 0 },\\n+    } as any;\\n+\\n+    const res = await engine.executeChecks({\\n+      checks: ['ask', 'refine', 'finish'],\\n+      config: cfg,\\n+      webhookContext: { eventType: 'manual' } as any,\\n+    });\\n+    const hist = (res.reviewSummary as any).history || {};\\n+    const finishHist = Array.isArray(hist['finish']) ? hist['finish'] : [];\\n+    expect(finishHist.length).toBe(0);\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/integration/snapshot-visibility-integration.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":3,\"patch\":\"diff --git a/tests/integration/snapshot-visibility-integration.test.ts b/tests/integration/snapshot-visibility-integration.test.ts\\nindex 2df585b1..b71b8c52 100644\\n--- a/tests/integration/snapshot-visibility-integration.test.ts\\n+++ b/tests/integration/snapshot-visibility-integration.test.ts\\n@@ -23,8 +23,7 @@ describe('Snapshot Visibility Integration', () => {\\n         },\\n         consumer: {\\n           type: 'script',\\n-          depends_on: ['producer'],\\n-          // Note: with script provider, we provide explicit dependency\\n+          // NO depends_on - we test snapshot visibility via goto routing\\n           content: `\\n             // Read producer output via snapshot-provided outputs\\n             const value = outputs[\\\"producer\\\"]?.msg;\\n\",\"status\":\"modified\"},{\"filename\":\"tests/integration/suppression.test.ts\",\"additions\":2,\"deletions\":3,\"changes\":152,\"patch\":\"diff --git a/tests/integration/suppression.test.ts b/tests/integration/suppression.test.ts\\nindex 9275f002..055fd3db 100644\\n--- a/tests/integration/suppression.test.ts\\n+++ b/tests/integration/suppression.test.ts\\n@@ -1,23 +1,10 @@\\n import { IssueFilter } from '../../src/issue-filter';\\n-import { ReviewIssue, ReviewSummary } from '../../src/reviewer';\\n+import { ReviewIssue } from '../../src/reviewer';\\n import { CheckExecutionEngine } from '../../src/check-execution-engine';\\n-import { VisorConfig } from '../../src/types/config';\\n-import { DependencyGraph } from '../../src/dependency-resolver';\\n import * as fs from 'fs';\\n import * as path from 'path';\\n import * as os from 'os';\\n \\n-// Type for accessing private methods and properties in tests\\n-interface CheckExecutionEngineWithPrivates {\\n-  config: Partial<VisorConfig>;\\n-  aggregateDependencyAwareResults(\\n-    results: Map<string, ReviewSummary>,\\n-    dependencyGraph: DependencyGraph,\\n-    debug: boolean,\\n-    stoppedEarly: boolean\\n-  ): ReviewSummary;\\n-}\\n-\\n describe('Issue Suppression Integration Tests', () => {\\n   let tempDir: string;\\n \\n@@ -64,60 +51,36 @@ function test() {\\n       execSync('git add .', { cwd: tempDir });\\n       execSync('git -c core.hooksPath=/dev/null commit -m \\\"test\\\"', { cwd: tempDir });\\n \\n-      const engine = new CheckExecutionEngine(tempDir);\\n-\\n-      // Store the original config so we can pass it through\\n-      (engine as unknown as CheckExecutionEngineWithPrivates).config = {\\n-        output: {\\n-          suppressionEnabled: true,\\n-          pr_comment: {\\n-            format: 'table',\\n-            group_by: 'check',\\n-            collapse: false,\\n-          },\\n-        },\\n-      };\\n+      new CheckExecutionEngine(tempDir);\\n \\n-      // Test the filtering directly with the aggregateDependencyAwareResults method\\n-      const mockResults = new Map();\\n-      mockResults.set('test-check', {\\n-        issues: [\\n-          {\\n-            file: 'file1.js',\\n-            line: 3,\\n-            ruleId: 'security/hardcoded-password',\\n-            message: 'Hardcoded password - should be suppressed',\\n-            severity: 'error',\\n-            category: 'security',\\n-          },\\n-          {\\n-            file: 'file2.js',\\n-            line: 3,\\n-            ruleId: 'security/hardcoded-api-key',\\n-            message: 'Hardcoded API key - should NOT be suppressed',\\n-            severity: 'error',\\n-            category: 'security',\\n-          },\\n-        ],\\n-      });\\n-\\n-      const mockDependencyGraph: DependencyGraph = {\\n-        executionOrder: [{ level: 0, parallel: ['test-check'] }],\\n-        nodes: new Map([\\n-          ['test-check', { id: 'test-check', dependencies: [], dependents: [], depth: 0 }],\\n-        ]),\\n-        hasCycles: false,\\n-      };\\n+      // Create test issues directly - testing the IssueFilter integration\\n+      const testIssues: ReviewIssue[] = [\\n+        {\\n+          file: 'file1.js',\\n+          line: 3,\\n+          ruleId: 'security/hardcoded-password',\\n+          message: 'Hardcoded password - should be suppressed',\\n+          severity: 'error',\\n+          category: 'security',\\n+        },\\n+        {\\n+          file: 'file2.js',\\n+          line: 3,\\n+          ruleId: 'security/hardcoded-api-key',\\n+          message: 'Hardcoded API key - should NOT be suppressed',\\n+          severity: 'error',\\n+          category: 'security',\\n+        },\\n+      ];\\n \\n-      // Call the method directly\\n-      const result = (\\n-        engine as unknown as CheckExecutionEngineWithPrivates\\n-      ).aggregateDependencyAwareResults(mockResults, mockDependencyGraph, false, false);\\n+      // Apply suppression filter directly (simulating what the engine does)\\n+      const filter = new IssueFilter(true);\\n+      const filteredIssues = filter.filterIssues(testIssues, tempDir);\\n \\n       // Verify that only file2 issue remains (file1 was suppressed)\\n-      expect(result.issues).toHaveLength(1);\\n-      expect(result.issues![0].file).toBe('file2.js');\\n-      expect(result.issues![0].message).toContain('should NOT be suppressed');\\n+      expect(filteredIssues).toHaveLength(1);\\n+      expect(filteredIssues[0].file).toBe('file2.js');\\n+      expect(filteredIssues[0].message).toContain('should NOT be suppressed');\\n     });\\n \\n     it('should respect suppressionEnabled config', async () => {\\n@@ -140,51 +103,30 @@ function test() {\\n       execSync('git add .', { cwd: tempDir });\\n       execSync('git -c core.hooksPath=/dev/null commit -m \\\"test\\\"', { cwd: tempDir });\\n \\n-      const engine = new CheckExecutionEngine(tempDir);\\n-\\n-      // Set config to disable suppression\\n-      (engine as unknown as CheckExecutionEngineWithPrivates).config = {\\n-        output: {\\n-          suppressionEnabled: false,\\n-          pr_comment: {\\n-            format: 'table',\\n-            group_by: 'check',\\n-            collapse: false,\\n-          },\\n-        },\\n-      };\\n-\\n-      // Test the filtering directly with the aggregateDependencyAwareResults method\\n-      const mockResults = new Map();\\n-      mockResults.set('test-check', {\\n-        issues: [\\n-          {\\n-            file: 'test.js',\\n-            line: 3,\\n-            ruleId: 'security/hardcoded-password',\\n-            message: 'Hardcoded password',\\n-            severity: 'error',\\n-            category: 'security',\\n-          },\\n-        ],\\n-      });\\n-\\n-      const mockDependencyGraph: DependencyGraph = {\\n-        executionOrder: [{ level: 0, parallel: ['test-check'] }],\\n-        nodes: new Map([\\n-          ['test-check', { id: 'test-check', dependencies: [], dependents: [], depth: 0 }],\\n-        ]),\\n-        hasCycles: false,\\n+      // Test with suppression disabled\\n+      const testIssue: ReviewIssue = {\\n+        file: 'test.js',\\n+        line: 3,\\n+        ruleId: 'security/hardcoded-password',\\n+        message: 'Hardcoded password',\\n+        severity: 'error',\\n+        category: 'security',\\n       };\\n \\n-      // Call the method directly\\n-      const result = (\\n-        engine as unknown as CheckExecutionEngineWithPrivates\\n-      ).aggregateDependencyAwareResults(mockResults, mockDependencyGraph, false, false);\\n+      // Apply filter with suppression disabled\\n+      const filterDisabled = new IssueFilter(false);\\n+      const resultDisabled = filterDisabled.filterIssues([testIssue], tempDir);\\n \\n       // Should NOT suppress when disabled\\n-      expect(result.issues).toHaveLength(1);\\n-      expect(result.issues![0].message).toBe('Hardcoded password');\\n+      expect(resultDisabled).toHaveLength(1);\\n+      expect(resultDisabled[0].message).toBe('Hardcoded password');\\n+\\n+      // Test with suppression enabled (for comparison)\\n+      const filterEnabled = new IssueFilter(true);\\n+      const resultEnabled = filterEnabled.filterIssues([testIssue], tempDir);\\n+\\n+      // Should suppress when enabled (since file has visor-disable comment)\\n+      expect(resultEnabled).toHaveLength(0);\\n     });\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/manual/README.md\",\"additions\":2,\"deletions\":0,\"changes\":64,\"patch\":\"diff --git a/tests/manual/README.md b/tests/manual/README.md\\nnew file mode 100644\\nindex 00000000..5f3bab0f\\n--- /dev/null\\n+++ b/tests/manual/README.md\\n@@ -0,0 +1,64 @@\\n+# Manual Tests\\n+\\n+This directory contains manual tests that require real API keys and make actual API calls. These tests are skipped by default in CI/CD.\\n+\\n+## Running Manual Tests\\n+\\n+### Prerequisites\\n+\\n+Set the required environment variables:\\n+\\n+```bash\\n+export ANTHROPIC_API_KEY=\\\"your-api-key-here\\\"\\n+export RUN_MANUAL_TESTS=true\\n+```\\n+\\n+### Run Bash Configuration Tests\\n+\\n+```bash\\n+# Run all manual tests\\n+npm test -- tests/manual/bash-config-manual.test.ts\\n+\\n+# Or use the helper script\\n+npm run test:manual:bash\\n+```\\n+\\n+## Test Coverage\\n+\\n+### `bash-config-manual.test.ts`\\n+\\n+Tests the bash command execution configuration with real ProbeAgent calls:\\n+\\n+1. **allowBash: true** - Validates basic bash execution with default safe commands\\n+2. **bashConfig options** - Tests custom allow/deny lists and timeout\\n+3. **workingDirectory** - Verifies custom working directory is respected\\n+4. **Default behavior** - Confirms bash is disabled by default\\n+\\n+## Notes\\n+\\n+- These tests make real API calls and may incur costs\\n+- Tests are automatically skipped unless `RUN_MANUAL_TESTS=true`\\n+- Each test has a 60-second timeout\\n+- Debug mode is enabled to show ProbeAgent interactions\\n+\\n+## Expected Output\\n+\\n+When tests pass, you should see:\\n+\\n+```\\n+üìù Testing allowBash: true\\n+‚úÖ allowBash test completed\\n+üìä Result: { overallScore: 100, ... }\\n+\\n+üìù Testing allowBash with bashConfig\\n+‚úÖ bashConfig test completed\\n+üìä Result: { overallScore: 100, ... }\\n+\\n+üìù Testing bashConfig.workingDirectory\\n+‚úÖ workingDirectory test completed\\n+üìä Result: { overallScore: 100, ... }\\n+\\n+üìù Testing without allowBash (default behavior)\\n+‚úÖ No bash test completed\\n+üìä Result: { overallScore: 100, ... }\\n+```\\n\",\"status\":\"added\"},{\"filename\":\"tests/manual/bash-config-manual.test.ts\",\"additions\":7,\"deletions\":0,\"changes\":235,\"patch\":\"diff --git a/tests/manual/bash-config-manual.test.ts b/tests/manual/bash-config-manual.test.ts\\nnew file mode 100644\\nindex 00000000..bf304269\\n--- /dev/null\\n+++ b/tests/manual/bash-config-manual.test.ts\\n@@ -0,0 +1,235 @@\\n+/**\\n+ * Manual test for bash configuration with ProbeAgent\\n+ *\\n+ * This test validates that bash configuration options are properly passed\\n+ * to ProbeAgent and can execute bash commands when enabled.\\n+ *\\n+ * Run with: npm test -- tests/manual/bash-config-manual.test.ts\\n+ *\\n+ * Prerequisites:\\n+ * - ANTHROPIC_API_KEY environment variable must be set\\n+ * - This test actually calls the AI API and may incur costs\\n+ */\\n+\\n+import { AIReviewService } from '../../src/ai-review-service';\\n+import { PRInfo } from '../../src/pr-analyzer';\\n+\\n+// Skip this test in CI/CD - only run manually\\n+const runManualTests = process.env.RUN_MANUAL_TESTS === 'true';\\n+\\n+describe('Bash Configuration Manual Tests', () => {\\n+  // Create a minimal PR info for testing\\n+  const mockPRInfo: PRInfo = {\\n+    number: 1,\\n+    title: 'Test PR',\\n+    body: 'Test PR body',\\n+    author: 'test-user',\\n+    base: 'main',\\n+    head: 'feature',\\n+    files: [\\n+      {\\n+        filename: 'test.ts',\\n+        status: 'modified',\\n+        additions: 10,\\n+        deletions: 5,\\n+        changes: 15,\\n+        patch: '+console.log(\\\"test\\\");',\\n+      },\\n+    ],\\n+    totalAdditions: 10,\\n+    totalDeletions: 5,\\n+  };\\n+\\n+  beforeAll(() => {\\n+    if (!runManualTests) {\\n+      console.log('‚è≠Ô∏è  Skipping manual tests. Set RUN_MANUAL_TESTS=true to run.');\\n+    }\\n+  });\\n+\\n+  (runManualTests ? describe : describe.skip)('With API Key', () => {\\n+    it('should execute bash commands when allowBash is true', async () => {\\n+      const hasAnthropicKey = !!process.env.ANTHROPIC_API_KEY;\\n+      const hasGoogleKey = !!process.env.GOOGLE_API_KEY;\\n+\\n+      if (!hasAnthropicKey && !hasGoogleKey) {\\n+        console.warn('‚ö†Ô∏è  No API key set (ANTHROPIC_API_KEY or GOOGLE_API_KEY), skipping test');\\n+        return;\\n+      }\\n+\\n+      const provider = hasAnthropicKey ? 'anthropic' : 'google';\\n+      const model = hasAnthropicKey ? 'claude-3-5-sonnet-20241022' : 'gemini-2.0-flash-exp';\\n+\\n+      const service = new AIReviewService({\\n+        provider: provider as any,\\n+        model,\\n+        allowBash: true,\\n+        debug: true,\\n+      });\\n+\\n+      const prompt = `\\n+You have access to bash commands. Please:\\n+1. Run 'echo \\\"Hello from bash\\\"'\\n+2. Run 'pwd' to show the current directory\\n+3. Confirm that bash commands are working\\n+\\n+Return a JSON response with your findings.\\n+`;\\n+\\n+      console.log('\\\\nüìù Testing allowBash: true');\\n+      const result = await service.executeReview(mockPRInfo, prompt);\\n+\\n+      // Check that we got a response\\n+      expect(result).toBeDefined();\\n+      console.log('‚úÖ allowBash test completed');\\n+      console.log('üìä Result:', JSON.stringify(result, null, 2));\\n+    }, 60000); // 60 second timeout\\n+\\n+    it('should pass bashConfig options to ProbeAgent', async () => {\\n+      const hasAnthropicKey = !!process.env.ANTHROPIC_API_KEY;\\n+      const hasGoogleKey = !!process.env.GOOGLE_API_KEY;\\n+\\n+      if (!hasAnthropicKey && !hasGoogleKey) {\\n+        console.warn('‚ö†Ô∏è  No API key set, skipping test');\\n+        return;\\n+      }\\n+\\n+      const provider = hasAnthropicKey ? 'anthropic' : 'google';\\n+      const model = hasAnthropicKey ? 'claude-3-5-sonnet-20241022' : 'gemini-2.0-flash-exp';\\n+\\n+      const service = new AIReviewService({\\n+        provider: provider as any,\\n+        model,\\n+        allowBash: true,\\n+        bashConfig: {\\n+          allow: ['echo', 'pwd', 'ls'],\\n+          timeout: 5000,\\n+        },\\n+        debug: true,\\n+      });\\n+\\n+      const prompt = `\\n+You have access to bash commands with custom configuration.\\n+Try running these commands:\\n+1. echo \\\"Test with custom allow list\\\"\\n+2. pwd\\n+3. ls\\n+\\n+Summarize what commands worked and return JSON.\\n+`;\\n+\\n+      console.log('\\\\nüìù Testing allowBash with bashConfig');\\n+      const result = await service.executeReview(mockPRInfo, prompt);\\n+\\n+      expect(result).toBeDefined();\\n+      console.log('‚úÖ bashConfig test completed');\\n+      console.log('üìä Result:', JSON.stringify(result, null, 2));\\n+    }, 60000);\\n+\\n+    it('should respect custom working directory', async () => {\\n+      const hasAnthropicKey = !!process.env.ANTHROPIC_API_KEY;\\n+      const hasGoogleKey = !!process.env.GOOGLE_API_KEY;\\n+\\n+      if (!hasAnthropicKey && !hasGoogleKey) {\\n+        console.warn('‚ö†Ô∏è  No API key set, skipping test');\\n+        return;\\n+      }\\n+\\n+      const provider = hasAnthropicKey ? 'anthropic' : 'google';\\n+      const model = hasAnthropicKey ? 'claude-3-5-sonnet-20241022' : 'gemini-2.0-flash-exp';\\n+\\n+      const service = new AIReviewService({\\n+        provider: provider as any,\\n+        model,\\n+        allowBash: true,\\n+        bashConfig: {\\n+          workingDirectory: '/tmp',\\n+        },\\n+        debug: true,\\n+      });\\n+\\n+      const prompt = `\\n+Run 'pwd' to show the current working directory.\\n+The working directory should be /tmp.\\n+Return JSON with the pwd output.\\n+`;\\n+\\n+      console.log('\\\\nüìù Testing bashConfig.workingDirectory');\\n+      const result = await service.executeReview(mockPRInfo, prompt);\\n+\\n+      expect(result).toBeDefined();\\n+      console.log('‚úÖ workingDirectory test completed');\\n+      console.log('üìä Result:', JSON.stringify(result, null, 2));\\n+    }, 60000);\\n+\\n+    it('should work without bash when allowBash is not set', async () => {\\n+      const hasAnthropicKey = !!process.env.ANTHROPIC_API_KEY;\\n+      const hasGoogleKey = !!process.env.GOOGLE_API_KEY;\\n+\\n+      if (!hasAnthropicKey && !hasGoogleKey) {\\n+        console.warn('‚ö†Ô∏è  No API key set, skipping test');\\n+        return;\\n+      }\\n+\\n+      const provider = hasAnthropicKey ? 'anthropic' : 'google';\\n+      const model = hasAnthropicKey ? 'claude-3-5-sonnet-20241022' : 'gemini-2.0-flash-exp';\\n+\\n+      const service = new AIReviewService({\\n+        provider: provider as any,\\n+        model,\\n+        // allowBash not set - should default to false\\n+        debug: true,\\n+      });\\n+\\n+      const prompt = `\\n+Analyze this test file and provide feedback.\\n+You should NOT have access to bash commands.\\n+Return JSON with your analysis.\\n+`;\\n+\\n+      console.log('\\\\nüìù Testing without allowBash (default behavior)');\\n+      const result = await service.executeReview(mockPRInfo, prompt);\\n+\\n+      expect(result).toBeDefined();\\n+      console.log('‚úÖ No bash test completed');\\n+      console.log('üìä Result:', JSON.stringify(result, null, 2));\\n+    }, 60000);\\n+  });\\n+\\n+  describe('Configuration Validation', () => {\\n+    it('should accept allowBash boolean', () => {\\n+      expect(() => {\\n+        new AIReviewService({\\n+          provider: 'mock',\\n+          allowBash: true,\\n+        });\\n+      }).not.toThrow();\\n+    });\\n+\\n+    it('should accept bashConfig object', () => {\\n+      expect(() => {\\n+        new AIReviewService({\\n+          provider: 'mock',\\n+          allowBash: true,\\n+          bashConfig: {\\n+            allow: ['ls', 'pwd'],\\n+            deny: ['rm'],\\n+            timeout: 30000,\\n+            workingDirectory: '/tmp',\\n+          },\\n+        });\\n+      }).not.toThrow();\\n+    });\\n+\\n+    it('should accept both allowBash and bashConfig', () => {\\n+      expect(() => {\\n+        new AIReviewService({\\n+          provider: 'mock',\\n+          allowBash: true,\\n+          bashConfig: {\\n+            allow: ['git status'],\\n+          },\\n+        });\\n+      }).not.toThrow();\\n+    });\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/override.tests.yaml\",\"additions\":1,\"deletions\":0,\"changes\":32,\"patch\":\"diff --git a/tests/override.tests.yaml b/tests/override.tests.yaml\\nnew file mode 100644\\nindex 00000000..8dbf5e3f\\n--- /dev/null\\n+++ b/tests/override.tests.yaml\\n@@ -0,0 +1,32 @@\\n+version: \\\"1.0\\\"\\n+extends: \\\"../override.yaml\\\"\\n+\\n+tests:\\n+  defaults:\\n+    strict: false\\n+    ai_provider: mock\\n+  cases:\\n+    - name: override-appendPrompt-security\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview body\\\"\\n+          tags: { label: feature, review-effort: 2 }\\n+        security: { issues: [] }\\n+        readability: { issues: [] }\\n+      expect:\\n+        calls:\\n+          - step: security\\n+            exactly: 1\\n+          - step: readability\\n+            exactly: 1\\n+        prompts:\\n+          - step: security\\n+            contains:\\n+              - \\\"hard-coded credentials\\\"\\n+              - \\\"secret manager\\\"\\n+          - step: readability\\n+            contains:\\n+              - \\\"readability review\\\"\\n+              - \\\"naming\\\"\\n\",\"status\":\"added\"},{\"filename\":\"tests/setup.ts\",\"additions\":1,\"deletions\":1,\"changes\":20,\"patch\":\"diff --git a/tests/setup.ts b/tests/setup.ts\\nindex 79a10416..812e00d3 100644\\n--- a/tests/setup.ts\\n+++ b/tests/setup.ts\\n@@ -67,12 +67,16 @@ afterEach(() => {\\n // Set global Jest timeout for all tests\\n jest.setTimeout(10000); // 10 seconds max per test\\n \\n-// Configure console to reduce noise in test output\\n+// Configure console to reduce noise in test output.\\n+// When VISOR_DEBUG=true or VISOR_TEST_SHOW_LOGS=true, keep real console to allow debugging logs.\\n const originalConsole = global.console;\\n-global.console = {\\n-  ...originalConsole,\\n-  log: jest.fn(), // Silence console.log during tests\\n-  warn: jest.fn(),\\n-  error: jest.fn(),\\n-  debug: jest.fn(),\\n-} as Console;\\n+const SHOW_LOGS = process.env.VISOR_DEBUG === 'true' || process.env.VISOR_TEST_SHOW_LOGS === 'true';\\n+if (!SHOW_LOGS) {\\n+  global.console = {\\n+    ...originalConsole,\\n+    log: jest.fn(), // Silence console.log during tests\\n+    warn: jest.fn(),\\n+    error: jest.fn(),\\n+    debug: jest.fn(),\\n+  } as Console;\\n+}\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/assume-optional-chaining.test.ts\",\"additions\":2,\"deletions\":0,\"changes\":49,\"patch\":\"diff --git a/tests/unit/assume-optional-chaining.test.ts b/tests/unit/assume-optional-chaining.test.ts\\nnew file mode 100644\\nindex 00000000..71d5c6f9\\n--- /dev/null\\n+++ b/tests/unit/assume-optional-chaining.test.ts\\n@@ -0,0 +1,49 @@\\n+import { StateMachineExecutionEngine } from '../../src/state-machine-execution-engine';\\n+import type { VisorConfig } from '../../src/types/config';\\n+\\n+describe('assume: optional chaining and nullish coalescing', () => {\\n+  it('runs when (outputs[dep]?.x?.length ?? 0) > 0 is true', async () => {\\n+    const cfg: VisorConfig = {\\n+      version: '1.0',\\n+      output: { pr_comment: { enabled: false } } as any,\\n+      checks: {\\n+        dep: { type: 'script', content: 'return { x: [\\\"y\\\"] };' } as any,\\n+        a: {\\n+          type: 'script',\\n+          depends_on: ['dep'],\\n+          assume: \\\"(outputs['dep']?.x?.length ?? 0) > 0\\\",\\n+          content: 'return { ok: true };',\\n+        } as any,\\n+      },\\n+    } as any;\\n+\\n+    const engine = new StateMachineExecutionEngine();\\n+    const res = await engine.executeChecks({ checks: ['dep', 'a'], config: cfg, debug: false });\\n+    const byName: Record<string, any> = {};\\n+    for (const s of res.executionStatistics?.checks || []) byName[s.checkName] = s;\\n+    expect(byName['a']?.totalRuns).toBe(1);\\n+  });\\n+\\n+  it('skips when expression evaluates to 0', async () => {\\n+    const cfg: VisorConfig = {\\n+      version: '1.0',\\n+      output: { pr_comment: { enabled: false } } as any,\\n+      checks: {\\n+        dep: { type: 'script', content: 'return { };' } as any,\\n+        a: {\\n+          type: 'script',\\n+          depends_on: ['dep'],\\n+          assume: \\\"(outputs['dep']?.x?.length ?? 0) > 0\\\",\\n+          content: 'return { ok: true };',\\n+        } as any,\\n+      },\\n+    } as any;\\n+\\n+    const engine = new StateMachineExecutionEngine();\\n+    const res = await engine.executeChecks({ checks: ['dep', 'a'], config: cfg, debug: false });\\n+    const byName: Record<string, any> = {};\\n+    for (const s of res.executionStatistics?.checks || []) byName[s.checkName] = s;\\n+    // totalRuns stays undefined or 0 when skipped via assume\\n+    expect(byName['a']?.totalRuns || 0).toBe(0);\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/unit/check-execution-engine-dependencies.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":19,\"patch\":\"diff --git a/tests/unit/check-execution-engine-dependencies.test.ts b/tests/unit/check-execution-engine-dependencies.test.ts\\nindex aa24520a..db8704e0 100644\\n--- a/tests/unit/check-execution-engine-dependencies.test.ts\\n+++ b/tests/unit/check-execution-engine-dependencies.test.ts\\n@@ -259,7 +259,7 @@ describe('CheckExecutionEngine - Dependencies', () => {\\n \\n       // Should return error result instead of throwing\\n       expect(result.reviewSummary.issues).toHaveLength(1);\\n-      expect(result.reviewSummary.issues![0].message).toContain('Circular dependencies detected');\\n+      expect(result.reviewSummary.issues![0].message).toContain('Dependency cycle detected');\\n       expect(mockProvider.execute).not.toHaveBeenCalled();\\n     });\\n \\n@@ -290,7 +290,8 @@ describe('CheckExecutionEngine - Dependencies', () => {\\n \\n       // Should return error result instead of throwing\\n       expect(result.reviewSummary.issues).toHaveLength(1);\\n-      expect(result.reviewSummary.issues![0].message).toContain('Dependency validation failed');\\n+      expect(result.reviewSummary.issues![0].message).toContain('depends on');\\n+      expect(result.reviewSummary.issues![0].message).toContain('not defined');\\n       expect(mockProvider.execute).not.toHaveBeenCalled();\\n     });\\n \\n@@ -339,14 +340,16 @@ describe('CheckExecutionEngine - Dependencies', () => {\\n       });\\n \\n       expect(result.reviewSummary.issues).toBeDefined();\\n-      expect(mockProvider.execute).toHaveBeenCalledTimes(2);\\n \\n-      // Should have error issues from failed security check\\n-      const errorIssues = (result.reviewSummary.issues || []).filter(issue =>\\n-        issue.ruleId?.includes('error')\\n+      // State machine behavior: when a check throws, only that check is called\\n+      // The dependent check is skipped because its dependency failed\\n+      expect(mockProvider.execute).toHaveBeenCalledTimes(1);\\n+\\n+      // Should have error issue from failed security check\\n+      const errorIssues = (result.reviewSummary.issues || []).filter(\\n+        issue => issue.message?.includes('Security check failed') || issue.ruleId?.includes('error')\\n       );\\n-      expect(errorIssues).toHaveLength(1);\\n-      expect(errorIssues[0].message).toContain('Security check failed');\\n+      expect(errorIssues.length).toBeGreaterThan(0);\\n     });\\n \\n     it('should include dependency execution statistics in debug output', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/cli/check-execution-engine.test.ts\",\"additions\":3,\"deletions\":3,\"changes\":183,\"patch\":\"diff --git a/tests/unit/cli/check-execution-engine.test.ts b/tests/unit/cli/check-execution-engine.test.ts\\nindex f63e0d09..ab781d77 100644\\n--- a/tests/unit/cli/check-execution-engine.test.ts\\n+++ b/tests/unit/cli/check-execution-engine.test.ts\\n@@ -21,6 +21,7 @@ describe('CheckExecutionEngine', () => {\\n   let mockGitAnalyzer: jest.Mocked<GitRepositoryAnalyzer>;\\n   let mockReviewer: jest.Mocked<PRReviewer>;\\n   let mockRegistry: jest.Mocked<CheckProviderRegistry>;\\n+  let mockAIProvider: any;\\n \\n   const mockRepositoryInfo: GitRepositoryInfo = {\\n     title: 'Test Repository',\\n@@ -73,10 +74,31 @@ describe('CheckExecutionEngine', () => {\\n     mockGitAnalyzer = new GitRepositoryAnalyzer() as jest.Mocked<GitRepositoryAnalyzer>;\\n     mockReviewer = new PRReviewer(null as any) as jest.Mocked<PRReviewer>;\\n \\n-    // Mock registry to return false for hasProvider so it falls back to PRReviewer\\n+    // Mock the AI provider to return a simple review summary\\n+    mockAIProvider = {\\n+      getName: jest.fn().mockReturnValue('ai'),\\n+      getDescription: jest.fn().mockReturnValue('AI provider'),\\n+      validateConfig: jest.fn().mockResolvedValue(true),\\n+      getSupportedConfigKeys: jest.fn().mockReturnValue([]),\\n+      isAvailable: jest.fn().mockResolvedValue(true),\\n+      getRequirements: jest.fn().mockReturnValue([]),\\n+      execute: jest.fn().mockImplementation(async () => ({\\n+        issues: [\\n+          {\\n+            category: 'security',\\n+            message: 'Potential security issue',\\n+            severity: 'error',\\n+            file: 'src/test.ts',\\n+            line: 10,\\n+          },\\n+        ],\\n+      })),\\n+    };\\n+\\n+    // Mock registry to return the AI provider\\n     mockRegistry = {\\n-      hasProvider: jest.fn().mockReturnValue(false),\\n-      getProviderOrThrow: jest.fn(),\\n+      hasProvider: jest.fn().mockReturnValue(true),\\n+      getProviderOrThrow: jest.fn().mockReturnValue(mockAIProvider),\\n       getAvailableProviders: jest.fn().mockReturnValue(['ai', 'tool', 'script', 'webhook']),\\n     } as any;\\n \\n@@ -94,13 +116,15 @@ describe('CheckExecutionEngine', () => {\\n   describe('Constructor', () => {\\n     it('should initialize with default working directory', () => {\\n       const engine = new CheckExecutionEngine();\\n-      expect(GitRepositoryAnalyzer).toHaveBeenCalledWith(process.cwd());\\n+      // State machine engine just stores the working directory, doesn't create analyzer in constructor\\n+      expect(engine).toBeDefined();\\n     });\\n \\n     it('should initialize with custom working directory', () => {\\n       const customDir = '/custom/work/dir';\\n       const engine = new CheckExecutionEngine(customDir);\\n-      expect(GitRepositoryAnalyzer).toHaveBeenCalledWith(customDir);\\n+      // State machine engine just stores the working directory, doesn't create analyzer in constructor\\n+      expect(engine).toBeDefined();\\n     });\\n   });\\n \\n@@ -142,19 +166,10 @@ describe('CheckExecutionEngine', () => {\\n       const result = await checkEngine.executeChecks(options);\\n \\n       expect(mockGitAnalyzer.analyzeRepository).toHaveBeenCalled();\\n-      expect(mockReviewer.reviewPR).toHaveBeenCalledWith(\\n-        'local',\\n-        'repository',\\n-        0,\\n-        expect.any(Object),\\n-        expect.objectContaining({\\n-          focus: 'all',\\n-          format: 'table',\\n-        })\\n-      );\\n \\n+      // Verify the result structure and content\\n       expect(result.repositoryInfo).toEqual(mockRepositoryInfo);\\n-      expect(result.reviewSummary).toEqual(mockReviewSummary);\\n+      expect(result.reviewSummary.issues).toBeDefined();\\n       expect(result.checksExecuted).toEqual(['security', 'performance']);\\n       expect(result.executionTime).toBeGreaterThanOrEqual(0);\\n       expect(result.timestamp).toBeDefined();\\n@@ -165,18 +180,11 @@ describe('CheckExecutionEngine', () => {\\n         checks: ['security'],\\n       };\\n \\n-      await checkEngine.executeChecks(options);\\n-\\n-      expect(mockReviewer.reviewPR).toHaveBeenCalledWith(\\n-        'local',\\n-        'repository',\\n-        0,\\n-        expect.any(Object),\\n-        expect.objectContaining({\\n-          focus: 'security',\\n-          format: 'table',\\n-        })\\n-      );\\n+      const result = await checkEngine.executeChecks(options);\\n+\\n+      // Verify the check was executed\\n+      expect(result.checksExecuted).toEqual(['security']);\\n+      expect(result.reviewSummary.issues).toBeDefined();\\n     });\\n \\n     it('should handle single performance check', async () => {\\n@@ -184,18 +192,11 @@ describe('CheckExecutionEngine', () => {\\n         checks: ['performance'],\\n       };\\n \\n-      await checkEngine.executeChecks(options);\\n-\\n-      expect(mockReviewer.reviewPR).toHaveBeenCalledWith(\\n-        'local',\\n-        'repository',\\n-        0,\\n-        expect.any(Object),\\n-        expect.objectContaining({\\n-          focus: 'performance',\\n-          format: 'table',\\n-        })\\n-      );\\n+      const result = await checkEngine.executeChecks(options);\\n+\\n+      // Verify the check was executed\\n+      expect(result.checksExecuted).toEqual(['performance']);\\n+      expect(result.reviewSummary.issues).toBeDefined();\\n     });\\n \\n     it('should handle single style check', async () => {\\n@@ -203,18 +204,11 @@ describe('CheckExecutionEngine', () => {\\n         checks: ['style'],\\n       };\\n \\n-      await checkEngine.executeChecks(options);\\n-\\n-      expect(mockReviewer.reviewPR).toHaveBeenCalledWith(\\n-        'local',\\n-        'repository',\\n-        0,\\n-        expect.any(Object),\\n-        expect.objectContaining({\\n-          focus: 'style',\\n-          format: 'table',\\n-        })\\n-      );\\n+      const result = await checkEngine.executeChecks(options);\\n+\\n+      // Verify the check was executed\\n+      expect(result.checksExecuted).toEqual(['style']);\\n+      expect(result.reviewSummary.issues).toBeDefined();\\n     });\\n \\n     it('should pass timeout option to check execution', async () => {\\n@@ -223,11 +217,11 @@ describe('CheckExecutionEngine', () => {\\n         timeout: 300000, // 5 minutes\\n       };\\n \\n-      await checkEngine.executeChecks(options);\\n+      const result = await checkEngine.executeChecks(options);\\n \\n-      // Since we're using PRReviewer, the timeout should be stored but we can't directly test it\\n-      // without mocking the AI service. For now, we just verify the call happened\\n-      expect(mockReviewer.reviewPR).toHaveBeenCalled();\\n+      // Verify the execution completed successfully\\n+      expect(result.checksExecuted).toEqual(['security']);\\n+      expect(result.reviewSummary.issues).toBeDefined();\\n     });\\n \\n     it('should handle timeout option with default value', async () => {\\n@@ -236,9 +230,11 @@ describe('CheckExecutionEngine', () => {\\n         timeout: undefined, // Should use default (600000ms)\\n       };\\n \\n-      await checkEngine.executeChecks(options);\\n+      const result = await checkEngine.executeChecks(options);\\n \\n-      expect(mockReviewer.reviewPR).toHaveBeenCalled();\\n+      // Verify the execution completed successfully\\n+      expect(result.checksExecuted).toEqual(['performance']);\\n+      expect(result.reviewSummary.issues).toBeDefined();\\n     });\\n \\n     it('should accept various timeout values', async () => {\\n@@ -252,8 +248,9 @@ describe('CheckExecutionEngine', () => {\\n           timeout,\\n         };\\n \\n-        await checkEngine.executeChecks(options);\\n-        expect(mockReviewer.reviewPR).toHaveBeenCalled();\\n+        const result = await checkEngine.executeChecks(options);\\n+        expect(result.checksExecuted).toEqual(['all']);\\n+        expect(result.reviewSummary.issues).toBeDefined();\\n       }\\n     });\\n \\n@@ -262,18 +259,11 @@ describe('CheckExecutionEngine', () => {\\n         checks: ['all'],\\n       };\\n \\n-      await checkEngine.executeChecks(options);\\n-\\n-      expect(mockReviewer.reviewPR).toHaveBeenCalledWith(\\n-        'local',\\n-        'repository',\\n-        0,\\n-        expect.any(Object),\\n-        expect.objectContaining({\\n-          focus: 'all',\\n-          format: 'table',\\n-        })\\n-      );\\n+      const result = await checkEngine.executeChecks(options);\\n+\\n+      // Verify the check was executed\\n+      expect(result.checksExecuted).toEqual(['all']);\\n+      expect(result.reviewSummary.issues).toBeDefined();\\n     });\\n \\n     it('should handle architecture check (mapped to all)', async () => {\\n@@ -281,18 +271,11 @@ describe('CheckExecutionEngine', () => {\\n         checks: ['architecture'],\\n       };\\n \\n-      await checkEngine.executeChecks(options);\\n-\\n-      expect(mockReviewer.reviewPR).toHaveBeenCalledWith(\\n-        'local',\\n-        'repository',\\n-        0,\\n-        expect.any(Object),\\n-        expect.objectContaining({\\n-          focus: 'all',\\n-          format: 'table',\\n-        })\\n-      );\\n+      const result = await checkEngine.executeChecks(options);\\n+\\n+      // Verify the check was executed\\n+      expect(result.checksExecuted).toEqual(['architecture']);\\n+      expect(result.reviewSummary.issues).toBeDefined();\\n     });\\n \\n     it('should handle multiple mixed checks', async () => {\\n@@ -300,18 +283,11 @@ describe('CheckExecutionEngine', () => {\\n         checks: ['security', 'performance', 'style'],\\n       };\\n \\n-      await checkEngine.executeChecks(options);\\n-\\n-      expect(mockReviewer.reviewPR).toHaveBeenCalledWith(\\n-        'local',\\n-        'repository',\\n-        0,\\n-        expect.any(Object),\\n-        expect.objectContaining({\\n-          focus: 'all',\\n-          format: 'table',\\n-        })\\n-      );\\n+      const result = await checkEngine.executeChecks(options);\\n+\\n+      // Verify all checks were executed\\n+      expect(result.checksExecuted).toEqual(['security', 'performance', 'style']);\\n+      expect(result.reviewSummary.issues).toBeDefined();\\n     });\\n \\n     it('should handle non-git repository', async () => {\\n@@ -350,7 +326,18 @@ describe('CheckExecutionEngine', () => {\\n     });\\n \\n     it('should handle reviewer errors', async () => {\\n-      mockReviewer.reviewPR.mockRejectedValue(new Error('Review failed'));\\n+      // Update the mock provider to throw an error\\n+      const mockAIProviderWithError = {\\n+        getName: jest.fn().mockReturnValue('ai'),\\n+        getDescription: jest.fn().mockReturnValue('AI provider'),\\n+        validateConfig: jest.fn().mockResolvedValue(true),\\n+        getSupportedConfigKeys: jest.fn().mockReturnValue([]),\\n+        isAvailable: jest.fn().mockResolvedValue(true),\\n+        getRequirements: jest.fn().mockReturnValue([]),\\n+        execute: jest.fn().mockRejectedValue(new Error('Review failed')),\\n+      };\\n+\\n+      mockRegistry.getProviderOrThrow.mockReturnValue(mockAIProviderWithError);\\n \\n       const options: CheckExecutionOptions = {\\n         checks: ['security'],\\n@@ -359,7 +346,7 @@ describe('CheckExecutionEngine', () => {\\n       const result = await checkEngine.executeChecks(options);\\n \\n       expect(result.reviewSummary.issues).toHaveLength(1);\\n-      expect(result.reviewSummary.issues![0].message).toBe('Review failed');\\n+      expect(result.reviewSummary.issues![0].message).toContain('Review failed');\\n       expect(result.reviewSummary.issues![0].ruleId).toBe('system/error');\\n     });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/config-criticality.test.ts\",\"additions\":1,\"deletions\":0,\"changes\":29,\"patch\":\"diff --git a/tests/unit/config-criticality.test.ts b/tests/unit/config-criticality.test.ts\\nnew file mode 100644\\nindex 00000000..6f3d4054\\n--- /dev/null\\n+++ b/tests/unit/config-criticality.test.ts\\n@@ -0,0 +1,29 @@\\n+import { buildEngineContextForRun } from '../../src/state-machine/context/build-engine-context';\\n+import type { VisorConfig } from '../../src/types/config';\\n+\\n+describe('Config criticality defaults', () => {\\n+  it('defaults missing check.criticality to policy', () => {\\n+    const cfg: VisorConfig = {\\n+      version: '1',\\n+      output: { format: 'json' },\\n+      checks: {\\n+        a: { type: 'log', message: 'hi' },\\n+        b: { type: 'log', message: 'there', criticality: 'external' },\\n+      },\\n+    } as any;\\n+\\n+    const ctx = buildEngineContextForRun(process.cwd(), cfg, {\\n+      eventType: 'manual',\\n+      owner: 'o',\\n+      repo: 'r',\\n+      prNumber: 0,\\n+      branch: 'main',\\n+      baseSha: 'x',\\n+      headSha: 'y',\\n+      commitMessage: '',\\n+    } as any);\\n+\\n+    expect(ctx.config.checks!.a!.criticality).toBe('policy');\\n+    expect(ctx.config.checks!.b!.criticality).toBe('external');\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/unit/config-extends.test.ts\",\"additions\":1,\"deletions\":0,\"changes\":21,\"patch\":\"diff --git a/tests/unit/config-extends.test.ts b/tests/unit/config-extends.test.ts\\nindex 7b6d6e67..b112a869 100644\\n--- a/tests/unit/config-extends.test.ts\\n+++ b/tests/unit/config-extends.test.ts\\n@@ -79,8 +79,11 @@ describe('Config Extends Functionality', () => {\\n         checks: {\\n           remote: {\\n             type: 'http',\\n+            criticality: 'external',\\n             url: 'https://example.com/webhook',\\n             body: '{\\\"test\\\": \\\"data\\\"}',\\n+            assume: 'true',\\n+            guarantee: 'true',\\n             on: ['pr_opened'],\\n           },\\n         },\\n@@ -533,8 +536,11 @@ describe('Config Extends Functionality', () => {\\n         checks: {\\n           custom: {\\n             type: 'http',\\n+            criticality: 'external',\\n             url: 'https://example.com',\\n             body: '{\\\"test\\\": \\\"data\\\"}',\\n+            assume: 'true',\\n+            guarantee: 'true',\\n             on: ['pr_opened'],\\n           },\\n         },\\n@@ -968,8 +974,11 @@ describe('Config Extends Functionality', () => {\\n         checks: {\\n           top: {\\n             type: 'http',\\n+            criticality: 'external',\\n             url: 'https://example.com',\\n             body: '{\\\"test\\\": \\\"data\\\"}',\\n+            assume: 'true',\\n+            guarantee: 'true',\\n             on: ['pr_opened'],\\n           },\\n         },\\n@@ -1020,8 +1029,11 @@ describe('Config Extends Functionality', () => {\\n         checks: {\\n           remote: {\\n             type: 'http',\\n+            criticality: 'external',\\n             url: 'https://example.com',\\n             body: '{\\\"test\\\": \\\"data\\\"}',\\n+            assume: 'true',\\n+            guarantee: 'true',\\n             on: ['pr_opened'],\\n           },\\n         },\\n@@ -1543,8 +1555,11 @@ describe('Config Extends Functionality', () => {\\n           },\\n           performance: {\\n             type: 'http',\\n+            criticality: 'external',\\n             url: 'https://perf.example.com/analyze',\\n             body: '{\\\"test\\\": \\\"data\\\"}',\\n+            assume: 'true',\\n+            guarantee: 'true',\\n             on: ['pr_opened'],\\n             group: 'performance',\\n           },\\n@@ -1773,8 +1788,11 @@ describe('Config Extends Functionality', () => {\\n         checks: {\\n           level2: {\\n             type: 'http',\\n+            criticality: 'external',\\n             url: 'https://example.com',\\n             body: '{\\\"test\\\": \\\"data\\\"}',\\n+            assume: 'true',\\n+            guarantee: 'true',\\n             on: ['pr_opened'],\\n           },\\n         },\\n@@ -1841,8 +1859,11 @@ describe('Config Extends Functionality', () => {\\n         checks: {\\n           child: {\\n             type: 'http',\\n+            criticality: 'external',\\n             url: 'https://example.com',\\n             body: '{\\\"test\\\": \\\"data\\\"}',\\n+            assume: 'true',\\n+            guarantee: 'true',\\n             on: ['pr_opened'],\\n           },\\n         },\\n\",\"status\":\"added\"},{\"filename\":\"tests/unit/cron-scheduler.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":10,\"patch\":\"diff --git a/tests/unit/cron-scheduler.test.ts b/tests/unit/cron-scheduler.test.ts\\nindex b0bd765d..46dc168c 100644\\n--- a/tests/unit/cron-scheduler.test.ts\\n+++ b/tests/unit/cron-scheduler.test.ts\\n@@ -1,17 +1,17 @@\\n import { CronScheduler } from '../../src/cron-scheduler';\\n import * as cron from 'node-cron';\\n-import { CheckExecutionEngine } from '../../src/check-execution-engine';\\n+import { StateMachineExecutionEngine } from '../../src/state-machine-execution-engine';\\n import { VisorConfig } from '../../src/types/config';\\n \\n // Mock node-cron\\n jest.mock('node-cron');\\n \\n-// Mock CheckExecutionEngine\\n-jest.mock('../../src/check-execution-engine');\\n+// Mock StateMachineExecutionEngine\\n+jest.mock('../../src/state-machine-execution-engine');\\n \\n describe('CronScheduler', () => {\\n   let scheduler: CronScheduler;\\n-  let mockExecutionEngine: jest.Mocked<CheckExecutionEngine>;\\n+  let mockExecutionEngine: jest.Mocked<StateMachineExecutionEngine>;\\n   let mockConfig: VisorConfig;\\n   let mockCronTasks: Map<string, { start: jest.Mock; stop: jest.Mock }>;\\n \\n@@ -26,7 +26,7 @@ describe('CronScheduler', () => {\\n       isGitRepository: jest.fn(),\\n       evaluateFailureConditions: jest.fn(),\\n       getRepositoryStatus: jest.fn(),\\n-    } as unknown as jest.Mocked<CheckExecutionEngine>;\\n+    } as unknown as jest.Mocked<StateMachineExecutionEngine>;\\n \\n     mockConfig = {\\n       version: '1.0',\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/custom-tools.test.ts\",\"additions\":11,\"deletions\":0,\"changes\":364,\"patch\":\"diff --git a/tests/unit/custom-tools.test.ts b/tests/unit/custom-tools.test.ts\\nnew file mode 100644\\nindex 00000000..b3c4c2fa\\n--- /dev/null\\n+++ b/tests/unit/custom-tools.test.ts\\n@@ -0,0 +1,364 @@\\n+import { CustomToolExecutor } from '../../src/providers/custom-tool-executor';\\n+import { CustomToolDefinition } from '../../src/types/config';\\n+\\n+describe('CustomToolExecutor', () => {\\n+  let executor: CustomToolExecutor;\\n+\\n+  beforeEach(() => {\\n+    executor = new CustomToolExecutor();\\n+  });\\n+\\n+  describe('tool registration', () => {\\n+    it('should register a custom tool', () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'test-tool',\\n+        description: 'A test tool',\\n+        exec: 'echo \\\"hello\\\"',\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const registeredTool = executor.getTool('test-tool');\\n+\\n+      expect(registeredTool).toBeDefined();\\n+      expect(registeredTool?.name).toBe('test-tool');\\n+      expect(registeredTool?.description).toBe('A test tool');\\n+    });\\n+\\n+    it('should register multiple tools', () => {\\n+      const tools: Record<string, CustomToolDefinition> = {\\n+        tool1: {\\n+          name: 'tool1',\\n+          exec: 'echo \\\"tool1\\\"',\\n+        },\\n+        tool2: {\\n+          name: 'tool2',\\n+          exec: 'echo \\\"tool2\\\"',\\n+        },\\n+      };\\n+\\n+      executor.registerTools(tools);\\n+\\n+      expect(executor.getTools()).toHaveLength(2);\\n+      expect(executor.getTool('tool1')).toBeDefined();\\n+      expect(executor.getTool('tool2')).toBeDefined();\\n+    });\\n+\\n+    it('should throw error when registering tool without name', () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: '',\\n+        exec: 'echo \\\"test\\\"',\\n+      };\\n+\\n+      expect(() => executor.registerTool(tool)).toThrow('Tool must have a name');\\n+    });\\n+  });\\n+\\n+  describe('input validation', () => {\\n+    it('should validate required fields', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'validate-tool',\\n+        exec: 'echo \\\"test\\\"',\\n+        inputSchema: {\\n+          type: 'object',\\n+          properties: {\\n+            name: { type: 'string' },\\n+            age: { type: 'number' },\\n+          },\\n+          required: ['name'],\\n+        },\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+\\n+      // Missing required field should throw\\n+      await expect(executor.execute('validate-tool', { age: 25 }, {})).rejects.toThrow(\\n+        \\\"Input validation failed for tool 'validate-tool': must have required property 'name'\\\"\\n+      );\\n+\\n+      // With required field should work\\n+      await expect(\\n+        executor.execute('validate-tool', { name: 'John', age: 25 }, {})\\n+      ).resolves.toBeDefined();\\n+    });\\n+\\n+    it('should reject unknown properties when additionalProperties is false', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'strict-tool',\\n+        exec: 'echo \\\"test\\\"',\\n+        inputSchema: {\\n+          type: 'object',\\n+          properties: {\\n+            name: { type: 'string' },\\n+          },\\n+          additionalProperties: false,\\n+        },\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+\\n+      await expect(\\n+        executor.execute('strict-tool', { name: 'John', extra: 'field' }, {})\\n+      ).rejects.toThrow('Input validation failed');\\n+    });\\n+\\n+    it('should validate data types according to JSON Schema', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'typed-tool',\\n+        exec: 'echo \\\"test\\\"',\\n+        inputSchema: {\\n+          type: 'object',\\n+          properties: {\\n+            name: { type: 'string' },\\n+            age: { type: 'number' },\\n+            active: { type: 'boolean' },\\n+            tags: {\\n+              type: 'array',\\n+              items: { type: 'string' },\\n+            },\\n+          },\\n+          required: ['name', 'age'],\\n+        },\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+\\n+      // Invalid: age is string instead of number\\n+      await expect(executor.execute('typed-tool', { name: 'John', age: '25' }, {})).rejects.toThrow(\\n+        'Input validation failed'\\n+      );\\n+\\n+      // Invalid: active is string instead of boolean\\n+      await expect(\\n+        executor.execute('typed-tool', { name: 'John', age: 25, active: 'yes' }, {})\\n+      ).rejects.toThrow('Input validation failed');\\n+\\n+      // Invalid: tags contains numbers instead of strings\\n+      await expect(\\n+        executor.execute('typed-tool', { name: 'John', age: 25, tags: [1, 2, 3] }, {})\\n+      ).rejects.toThrow('Input validation failed');\\n+\\n+      // Valid: all types are correct\\n+      await expect(\\n+        executor.execute(\\n+          'typed-tool',\\n+          { name: 'John', age: 25, active: true, tags: ['a', 'b'] },\\n+          {}\\n+        )\\n+      ).resolves.toBeDefined();\\n+    });\\n+  });\\n+\\n+  describe('tool execution', () => {\\n+    it('should execute a simple command', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'echo-tool',\\n+        exec: 'echo \\\"Hello World\\\"',\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const result = await executor.execute('echo-tool', {}, {});\\n+\\n+      expect(result).toContain('Hello World');\\n+    });\\n+\\n+    it('should pass arguments via Liquid templates', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'greet-tool',\\n+        exec: 'echo \\\"Hello {{ args.name }}\\\"',\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const result = await executor.execute('greet-tool', { name: 'Alice' }, {});\\n+\\n+      expect(result).toContain('Hello Alice');\\n+    });\\n+\\n+    it('should handle stdin input', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'cat-tool',\\n+        exec: 'cat',\\n+        stdin: 'Input from stdin: {{ args.message }}',\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const result = await executor.execute('cat-tool', { message: 'test message' }, {});\\n+\\n+      expect(result).toContain('Input from stdin: test message');\\n+    });\\n+\\n+    it('should parse JSON output when requested', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'json-tool',\\n+        exec: 'echo \\\\'{\\\"status\\\": \\\"success\\\", \\\"count\\\": 42}\\\\'',\\n+        parseJson: true,\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const result = await executor.execute('json-tool', {}, {});\\n+\\n+      expect(result).toEqual({ status: 'success', count: 42 });\\n+    });\\n+\\n+    it('should apply Liquid transform to output', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'transform-tool',\\n+        exec: 'echo \\\"raw output\\\"',\\n+        transform: '{ \\\"processed\\\": \\\"{{ output | strip }}\\\" }',\\n+        parseJson: true,\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const result = await executor.execute('transform-tool', {}, {});\\n+\\n+      expect(result).toEqual({ processed: 'raw output' });\\n+    });\\n+\\n+    it('should apply JavaScript transform to output', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'js-transform-tool',\\n+        exec: 'echo \\\"10\\\"',\\n+        transform_js: 'return parseInt(output.trim()) * 2;',\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const result = await executor.execute('js-transform-tool', {}, {});\\n+\\n+      expect(result).toBe(20);\\n+    });\\n+\\n+    it('should handle command timeout', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'slow-tool',\\n+        exec: 'sleep 5',\\n+        timeout: 100, // 100ms timeout\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+\\n+      // The command should throw a timeout error\\n+      await expect(executor.execute('slow-tool', {}, {})).rejects.toThrow(\\n+        'Command timed out after 100ms'\\n+      );\\n+    });\\n+\\n+    it('should throw error for non-existent tool', async () => {\\n+      await expect(executor.execute('non-existent', {}, {})).rejects.toThrow(\\n+        'Tool not found: non-existent'\\n+      );\\n+    });\\n+  });\\n+\\n+  describe('context usage', () => {\\n+    it('should provide PR context to tools', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'pr-tool',\\n+        exec: 'echo \\\"PR #{{ pr.number }}: {{ pr.title }}\\\"',\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const result = await executor.execute(\\n+        'pr-tool',\\n+        {},\\n+        {\\n+          pr: {\\n+            number: 123,\\n+            title: 'Add new feature',\\n+            author: 'john',\\n+            branch: 'feature/test',\\n+            base: 'main',\\n+          },\\n+        }\\n+      );\\n+\\n+      expect(result).toContain('PR #123: Add new feature');\\n+    });\\n+\\n+    it('should provide file list to tools', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'file-tool',\\n+        exec: 'echo \\\"Files: {{ files | size }}\\\"',\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const result = await executor.execute(\\n+        'file-tool',\\n+        {},\\n+        {\\n+          files: ['file1.js', 'file2.ts', 'file3.py'],\\n+        }\\n+      );\\n+\\n+      expect(result).toContain('Files: 3');\\n+    });\\n+\\n+    it('should provide outputs from previous checks', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'output-tool',\\n+        exec: 'echo \\\"Previous result: {{ outputs.check1 }}\\\"',\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const result = await executor.execute(\\n+        'output-tool',\\n+        {},\\n+        {\\n+          outputs: {\\n+            check1: 'success',\\n+            check2: 'pending',\\n+          },\\n+        }\\n+      );\\n+\\n+      expect(result).toContain('Previous result: success');\\n+    });\\n+  });\\n+\\n+  describe('MCP tool conversion', () => {\\n+    it('should convert custom tools to MCP tool format', () => {\\n+      const tools: Record<string, CustomToolDefinition> = {\\n+        tool1: {\\n+          name: 'tool1',\\n+          description: 'First tool',\\n+          exec: 'echo \\\"tool1\\\"',\\n+          inputSchema: {\\n+            type: 'object',\\n+            properties: {\\n+              param: { type: 'string' },\\n+            },\\n+          },\\n+        },\\n+        tool2: {\\n+          name: 'tool2',\\n+          description: 'Second tool',\\n+          exec: 'echo \\\"tool2\\\"',\\n+        },\\n+      };\\n+\\n+      executor.registerTools(tools);\\n+      const mcpTools = executor.toMcpTools();\\n+\\n+      expect(mcpTools).toHaveLength(2);\\n+      expect(mcpTools[0].name).toBe('tool1');\\n+      expect(mcpTools[0].description).toBe('First tool');\\n+      expect(mcpTools[0].inputSchema).toBeDefined();\\n+      expect(mcpTools[0].handler).toBeDefined();\\n+\\n+      expect(mcpTools[1].name).toBe('tool2');\\n+      expect(mcpTools[1].description).toBe('Second tool');\\n+    });\\n+\\n+    it('should execute through MCP tool handler', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'handler-tool',\\n+        exec: 'echo \\\"Result: {{ args.value }}\\\"',\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const mcpTools = executor.toMcpTools();\\n+      const handler = mcpTools[0].handler;\\n+\\n+      const result = await handler({ value: 'test' });\\n+      expect(result).toContain('Result: test');\\n+    });\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/unit/engine-onfinish-utils.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":3,\"patch\":\"diff --git a/tests/unit/engine-onfinish-utils.test.ts b/tests/unit/engine-onfinish-utils.test.ts\\nindex 5e16e78f..ee4d1db6 100644\\n--- a/tests/unit/engine-onfinish-utils.test.ts\\n+++ b/tests/unit/engine-onfinish-utils.test.ts\\n@@ -19,7 +19,7 @@ describe('OnFinish utils', () => {\\n     expect(p.outputsHistoryForContext.a).toHaveLength(1);\\n   });\\n \\n-  test('composeOnFinishContext includes memory/env and step metadata', () => {\\n+  test('composeOnFinishContext includes env and step metadata', () => {\\n     const ctx = composeOnFinishContext(\\n       undefined,\\n       'extract-facts',\\n@@ -39,7 +39,6 @@ describe('OnFinish utils', () => {\\n     );\\n     expect(ctx.step.id).toBe('extract-facts');\\n     expect(Array.isArray(ctx.outputs_history.validate)).toBe(true);\\n-    expect(typeof ctx.memory.get).toBe('function');\\n     expect(ctx.event.name).toBe('issue_opened');\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/execution-statistics-formatting.test.ts\",\"additions\":1,\"deletions\":0,\"changes\":16,\"patch\":\"diff --git a/tests/unit/execution-statistics-formatting.test.ts b/tests/unit/execution-statistics-formatting.test.ts\\nindex a3a37c2c..1c25f04a 100644\\n--- a/tests/unit/execution-statistics-formatting.test.ts\\n+++ b/tests/unit/execution-statistics-formatting.test.ts\\n@@ -15,6 +15,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 1,\\n         successfulRuns: 1,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 1000,\\n         issuesFound: 0,\\n@@ -31,6 +32,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 5,\\n         successfulRuns: 5,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 5000,\\n         issuesFound: 0,\\n@@ -47,6 +49,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 3,\\n         successfulRuns: 0,\\n         failedRuns: 3,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 3000,\\n         issuesFound: 0,\\n@@ -63,6 +66,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 5,\\n         successfulRuns: 3,\\n         failedRuns: 2,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 5000,\\n         issuesFound: 0,\\n@@ -79,6 +83,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 0,\\n         successfulRuns: 0,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: true,\\n         skipReason: 'if_condition',\\n         totalDuration: 0,\\n@@ -96,6 +101,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 0,\\n         successfulRuns: 0,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: true,\\n         skipReason: 'fail_fast',\\n         totalDuration: 0,\\n@@ -113,6 +119,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 0,\\n         successfulRuns: 0,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: true,\\n         skipReason: 'dependency_failed',\\n         totalDuration: 0,\\n@@ -132,6 +139,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 1,\\n         successfulRuns: 1,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 1000,\\n         issuesFound: 0,\\n@@ -149,6 +157,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 1,\\n         successfulRuns: 1,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 1000,\\n         issuesFound: 3,\\n@@ -165,6 +174,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 1,\\n         successfulRuns: 1,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 1000,\\n         issuesFound: 12,\\n@@ -181,6 +191,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 1,\\n         successfulRuns: 1,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 1000,\\n         issuesFound: 5,\\n@@ -197,6 +208,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 1,\\n         successfulRuns: 1,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 1000,\\n         issuesFound: 7,\\n@@ -214,6 +226,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 1,\\n         successfulRuns: 0,\\n         failedRuns: 1,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 1000,\\n         issuesFound: 0,\\n@@ -231,6 +244,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 1,\\n         successfulRuns: 0,\\n         failedRuns: 1,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 1000,\\n         issuesFound: 0,\\n@@ -249,6 +263,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 0,\\n         successfulRuns: 0,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: true,\\n         skipReason: 'if_condition',\\n         skipCondition: 'branch == \\\"main\\\"',\\n@@ -267,6 +282,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 5,\\n         successfulRuns: 5,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 5000,\\n         issuesFound: 15,\\n\",\"status\":\"added\"},{\"filename\":\"tests/unit/foreach-custom-schema-integration.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":43,\"patch\":\"diff --git a/tests/unit/foreach-custom-schema-integration.test.ts b/tests/unit/foreach-custom-schema-integration.test.ts\\nindex 51206287..8b78b170 100644\\n--- a/tests/unit/foreach-custom-schema-integration.test.ts\\n+++ b/tests/unit/foreach-custom-schema-integration.test.ts\\n@@ -3,18 +3,36 @@ import { CommandCheckProvider } from '../../src/providers/command-check-provider\\n import { CheckProviderConfig } from '../../src/providers/check-provider.interface';\\n import { PRInfo } from '../../src/pr-analyzer';\\n import { ReviewSummary } from '../../src/reviewer';\\n+import { CommandExecutionResult, CommandExecutionOptions } from '../../src/utils/command-executor';\\n+\\n+// First, create the mock functions with the factory pattern\\n+jest.mock('../../src/utils/command-executor', () => {\\n+  const mockExecute =\\n+    jest.fn<\\n+      (command: string, options?: CommandExecutionOptions) => Promise<CommandExecutionResult>\\n+    >();\\n+  const mockBuildEnvironment = jest.fn().mockReturnValue({});\\n+\\n+  return {\\n+    commandExecutor: {\\n+      execute: mockExecute,\\n+      buildEnvironment: mockBuildEnvironment,\\n+    },\\n+    // Export mocks for test access\\n+    __mockExecute: mockExecute,\\n+    __mockBuildEnvironment: mockBuildEnvironment,\\n+  };\\n+});\\n \\n-// Mock child_process\\n-const mockExec = jest.fn() as jest.MockedFunction<any>;\\n-const mockPromisify = jest.fn().mockReturnValue(mockExec);\\n-\\n-jest.mock('child_process', () => ({\\n-  exec: jest.fn(),\\n-}));\\n-\\n-jest.mock('util', () => ({\\n-  promisify: mockPromisify,\\n-}));\\n+// Import the mocked module to get the mock functions\\n+const mockModule = jest.requireMock('../../src/utils/command-executor') as {\\n+  __mockExecute: jest.MockedFunction<\\n+    (command: string, options?: CommandExecutionOptions) => Promise<CommandExecutionResult>\\n+  >;\\n+  __mockBuildEnvironment: jest.MockedFunction<() => Record<string, string>>;\\n+};\\n+const mockExecute = mockModule.__mockExecute;\\n+// mockBuildEnvironment is defined but not used in tests\\n \\n /**\\n  * Test: forEach with Custom Schema Integration\\n@@ -69,9 +87,10 @@ describe('forEach with Custom Schema Integration', () => {\\n       ],\\n     } as ReviewSummary & { output: unknown });\\n \\n-    mockExec.mockResolvedValue({\\n+    mockExecute.mockResolvedValue({\\n       stdout: 'test\\\\n',\\n       stderr: '',\\n+      exitCode: 0,\\n     });\\n \\n     const result = await provider.execute(mockPRInfo, config, dependencyResults);\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/github-comments.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":10,\"patch\":\"diff --git a/tests/unit/github-comments.test.ts b/tests/unit/github-comments.test.ts\\nindex 7b7fd179..f5649528 100644\\n--- a/tests/unit/github-comments.test.ts\\n+++ b/tests/unit/github-comments.test.ts\\n@@ -339,8 +339,9 @@ describe('CommentManager', () => {\\n \\n       const result = commentManager.formatGroupedResults(results, 'check');\\n \\n-      expect(result).toContain('üìà performance Review (Score: 75/100) - 5 issues found');\\n-      expect(result).toContain('üîí security Review (Score: 90/100) - 1 issues found');\\n+      // Titles no longer include step/category emojis; assert plain headings\\n+      expect(result).toContain('performance Review (Score: 75/100) - 5 issues found');\\n+      expect(result).toContain('security Review (Score: 90/100) - 1 issues found');\\n       expect(result).toContain('<details open>'); // Should expand sections with issues\\n     });\\n \\n@@ -352,8 +353,9 @@ describe('CommentManager', () => {\\n \\n       const result = commentManager.formatGroupedResults(results, 'severity');\\n \\n-      expect(result).toContain('üëç Good Review');\\n-      expect(result).toContain('üö® Critical Issues Review');\\n+      // Group headers do not include severity emojis; plain titles are used\\n+      expect(result).toContain('Good Review');\\n+      expect(result).toContain('Critical Issues Review');\\n     });\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/liquid-template-extensions.test.ts\",\"additions\":2,\"deletions\":0,\"changes\":46,\"patch\":\"diff --git a/tests/unit/liquid-template-extensions.test.ts b/tests/unit/liquid-template-extensions.test.ts\\nnew file mode 100644\\nindex 00000000..0fd9fc4d\\n--- /dev/null\\n+++ b/tests/unit/liquid-template-extensions.test.ts\\n@@ -0,0 +1,46 @@\\n+import { StateMachineExecutionEngine } from '../../src/state-machine-execution-engine';\\n+import type { VisorConfig } from '../../src/types/config';\\n+\\n+describe('Liquid template extensions in LevelDispatch', () => {\\n+  it('renders template with custom filters (safe_label) using createExtendedLiquid', async () => {\\n+    const engine = new StateMachineExecutionEngine();\\n+\\n+    const cfg: VisorConfig = {\\n+      version: '1.0',\\n+      checks: {\\n+        tmpl: {\\n+          type: 'log',\\n+          message: 'hello',\\n+          // Use filter provided by createExtendedLiquid to prove we are not using raw Liquid\\n+          template: { content: \\\"{{ 'foo@bar' | safe_label }}\\\" },\\n+          group: 'default',\\n+        },\\n+      },\\n+    } as any;\\n+\\n+    const prInfo: any = {\\n+      number: 0,\\n+      title: 't',\\n+      author: 'a',\\n+      base: 'main',\\n+      head: 'branch',\\n+      files: [],\\n+      totalAdditions: 0,\\n+      totalDeletions: 0,\\n+      eventType: 'manual',\\n+    };\\n+\\n+    const { results } = await engine.executeGroupedChecks(\\n+      prInfo,\\n+      ['tmpl'],\\n+      undefined,\\n+      cfg,\\n+      'table',\\n+      false\\n+    );\\n+\\n+    const group = results['default'];\\n+    expect(Array.isArray(group)).toBe(true);\\n+    expect(group![0].content).toContain('foobar'); // '@' removed by safe_label\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/unit/mcp-provider.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":41,\"patch\":\"diff --git a/tests/unit/mcp-provider.test.ts b/tests/unit/mcp-provider.test.ts\\nindex 1fa1cb38..1e546782 100644\\n--- a/tests/unit/mcp-provider.test.ts\\n+++ b/tests/unit/mcp-provider.test.ts\\n@@ -17,7 +17,8 @@ describe('MCP Check Provider', () => {\\n       const description = provider.getDescription();\\n       expect(description).toContain('stdio');\\n       expect(description).toContain('SSE');\\n-      expect(description).toContain('Streamable HTTP');\\n+      expect(description).toContain('HTTP');\\n+      expect(description).toContain('custom');\\n     });\\n \\n     it('should be available', async () => {\\n@@ -156,6 +157,44 @@ describe('MCP Check Provider', () => {\\n       });\\n     });\\n \\n+    describe('custom transport', () => {\\n+      it('should accept custom transport with method', async () => {\\n+        const config = {\\n+          type: 'mcp',\\n+          transport: 'custom',\\n+          method: 'my-custom-tool',\\n+        };\\n+\\n+        const result = await provider.validateConfig(config);\\n+        expect(result).toBe(true);\\n+      });\\n+\\n+      it('should reject custom transport without method', async () => {\\n+        const config = {\\n+          type: 'mcp',\\n+          transport: 'custom',\\n+        };\\n+\\n+        const result = await provider.validateConfig(config);\\n+        expect(result).toBe(false);\\n+      });\\n+\\n+      it('should accept custom transport with methodArgs', async () => {\\n+        const config = {\\n+          type: 'mcp',\\n+          transport: 'custom',\\n+          method: 'my-custom-tool',\\n+          methodArgs: {\\n+            param1: 'value1',\\n+            param2: 123,\\n+          },\\n+        };\\n+\\n+        const result = await provider.validateConfig(config);\\n+        expect(result).toBe(true);\\n+      });\\n+    });\\n+\\n     describe('invalid transport', () => {\\n       it('should reject invalid transport type', async () => {\\n         const config = {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/ai-check-provider.test.ts\",\"additions\":5,\"deletions\":0,\"changes\":160,\"patch\":\"diff --git a/tests/unit/providers/ai-check-provider.test.ts b/tests/unit/providers/ai-check-provider.test.ts\\nindex 7d433767..d0b94086 100644\\n--- a/tests/unit/providers/ai-check-provider.test.ts\\n+++ b/tests/unit/providers/ai-check-provider.test.ts\\n@@ -243,6 +243,162 @@ describe('AICheckProvider', () => {\\n         allowEdit: true,\\n       });\\n     });\\n+\\n+    it('should pass allowedTools and disableTools flags to service', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'analyze code structure',\\n+        ai: {\\n+          provider: 'anthropic',\\n+          model: 'claude-3-opus',\\n+          allowedTools: ['Read', 'Grep', 'Glob'],\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'anthropic',\\n+        model: 'claude-3-opus',\\n+        allowedTools: ['Read', 'Grep', 'Glob'],\\n+      });\\n+    });\\n+\\n+    it('should pass disableTools flag to service', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'explain architecture',\\n+        ai: {\\n+          provider: 'openai',\\n+          model: 'gpt-4',\\n+          disableTools: true,\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'openai',\\n+        model: 'gpt-4',\\n+        disableTools: true,\\n+      });\\n+    });\\n+\\n+    it('should pass allowBash to service', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'analyze git status',\\n+        ai: {\\n+          provider: 'anthropic',\\n+          model: 'claude-3-opus',\\n+          allowBash: true,\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'anthropic',\\n+        model: 'claude-3-opus',\\n+        allowBash: true,\\n+      });\\n+    });\\n+\\n+    it('should pass bashConfig with allowBash to service', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'analyze git status with custom config',\\n+        ai: {\\n+          provider: 'anthropic',\\n+          model: 'claude-3-opus',\\n+          allowBash: true,\\n+          bashConfig: {\\n+            allow: ['git status', 'ls'],\\n+            timeout: 30000,\\n+          },\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'anthropic',\\n+        model: 'claude-3-opus',\\n+        allowBash: true,\\n+        bashConfig: {\\n+          allow: ['git status', 'ls'],\\n+          timeout: 30000,\\n+        },\\n+      });\\n+    });\\n   });\\n \\n   describe('getSupportedConfigKeys', () => {\\n@@ -255,6 +411,10 @@ describe('AICheckProvider', () => {\\n       expect(keys).toContain('ai.model');\\n       expect(keys).toContain('ai.enableDelegate');\\n       expect(keys).toContain('ai.allowEdit');\\n+      expect(keys).toContain('ai.allowedTools');\\n+      expect(keys).toContain('ai.disableTools');\\n+      expect(keys).toContain('ai.allowBash');\\n+      expect(keys).toContain('ai.bashConfig');\\n     });\\n   });\\n \\n\",\"status\":\"added\"},{\"filename\":\"tests/unit/providers/check-provider-registry.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/tests/unit/providers/check-provider-registry.test.ts b/tests/unit/providers/check-provider-registry.test.ts\\nindex 5c0d5fc3..8e5ec3d77 100644\\n--- a/tests/unit/providers/check-provider-registry.test.ts\\n+++ b/tests/unit/providers/check-provider-registry.test.ts\\n@@ -173,8 +173,8 @@ describe('CheckProviderRegistry', () => {\\n       const providers = registry.getAllProviders();\\n       expect(providers).toContain(provider1);\\n       expect(providers).toContain(provider2);\\n-      // Reset adds 13 default providers (ai, command, script, http, http_input, http_client, noop, log, memory, github, claude-code, mcp, human-input) + 2 custom = 15 total\\n-      expect(providers.length).toBe(15);\\n+      // Reset adds 14 default providers (ai, command, script, http, http_input, http_client, noop, log, memory, github, claude-code, mcp, human-input, workflow) + 2 custom = 16 total\\n+      expect(providers.length).toBe(16);\\n     });\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/command-check-provider.test.ts\",\"additions\":2,\"deletions\":2,\"changes\":136,\"patch\":\"diff --git a/tests/unit/providers/command-check-provider.test.ts b/tests/unit/providers/command-check-provider.test.ts\\nindex 51703b84..3bf11a84 100644\\n--- a/tests/unit/providers/command-check-provider.test.ts\\n+++ b/tests/unit/providers/command-check-provider.test.ts\\n@@ -3,19 +3,39 @@ import { CommandCheckProvider } from '../../../src/providers/command-check-provi\\n import { CheckProviderConfig } from '../../../src/providers/check-provider.interface';\\n import { PRInfo } from '../../../src/pr-analyzer';\\n import { ReviewSummary } from '../../../src/reviewer';\\n+import {\\n+  CommandExecutionResult,\\n+  CommandExecutionOptions,\\n+} from '../../../src/utils/command-executor';\\n+\\n+// Mock the command executor with factory pattern\\n+jest.mock('../../../src/utils/command-executor', () => {\\n+  const mockExecute =\\n+    jest.fn<\\n+      (command: string, options?: CommandExecutionOptions) => Promise<CommandExecutionResult>\\n+    >();\\n+  const mockBuildEnvironment = jest.fn().mockReturnValue({});\\n+\\n+  return {\\n+    commandExecutor: {\\n+      execute: mockExecute,\\n+      buildEnvironment: mockBuildEnvironment,\\n+    },\\n+    // Export mocks for test access\\n+    __mockExecute: mockExecute,\\n+    __mockBuildEnvironment: mockBuildEnvironment,\\n+  };\\n+});\\n \\n-// Mock child_process and util\\n-// eslint-disable-next-line @typescript-eslint/no-explicit-any\\n-const mockExec = jest.fn() as jest.MockedFunction<any>;\\n-const mockPromisify = jest.fn().mockReturnValue(mockExec);\\n-\\n-jest.mock('child_process', () => ({\\n-  exec: jest.fn(),\\n-}));\\n-\\n-jest.mock('util', () => ({\\n-  promisify: mockPromisify,\\n-}));\\n+// Import the mocked module to get the mock functions\\n+const mockModule = jest.requireMock('../../../src/utils/command-executor') as {\\n+  __mockExecute: jest.MockedFunction<\\n+    (command: string, options?: CommandExecutionOptions) => Promise<CommandExecutionResult>\\n+  >;\\n+  __mockBuildEnvironment: jest.MockedFunction<() => Record<string, string>>;\\n+};\\n+const mockExecute = mockModule.__mockExecute;\\n+// mockBuildEnvironment is defined but not used in tests\\n \\n describe('CommandCheckProvider', () => {\\n   let provider: CommandCheckProvider;\\n@@ -121,9 +141,10 @@ describe('CommandCheckProvider', () => {\\n         exec: 'echo \\\"hello world\\\"',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'hello world\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n@@ -132,10 +153,9 @@ describe('CommandCheckProvider', () => {\\n       expect(result.issues).toEqual([]);\\n       // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n       expect((result as any).output).toBe('hello world');\\n-      expect(mockExec).toHaveBeenCalledWith('echo \\\"hello world\\\"', {\\n+      expect(mockExecute).toHaveBeenCalledWith('echo \\\"hello world\\\"', {\\n         env: expect.any(Object),\\n         timeout: 60000,\\n-        maxBuffer: 10 * 1024 * 1024,\\n       });\\n     });\\n \\n@@ -145,9 +165,10 @@ describe('CommandCheckProvider', () => {\\n         exec: 'echo \\\\'{\\\"items\\\": [\\\"a\\\", \\\"b\\\", \\\"c\\\"]}\\\\'',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: '{\\\"items\\\": [\\\"a\\\", \\\"b\\\", \\\"c\\\"]}\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n@@ -164,7 +185,7 @@ describe('CommandCheckProvider', () => {\\n         exec: 'nonexistent-command',\\n       };\\n \\n-      mockExec.mockRejectedValue(new Error('Command not found'));\\n+      mockExecute.mockRejectedValue(new Error('Command not found'));\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n \\n@@ -191,7 +212,7 @@ describe('CommandCheckProvider', () => {\\n         stdout: 'partial output',\\n       });\\n \\n-      mockExec.mockRejectedValue(error);\\n+      mockExecute.mockRejectedValue(error);\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n \\n@@ -215,9 +236,10 @@ describe('CommandCheckProvider', () => {\\n         exec: 'echo \\\"invalid json {\\\"',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'invalid json {\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n@@ -237,14 +259,18 @@ describe('CommandCheckProvider', () => {\\n         exec: 'echo \\\"PR: {{ pr.title }} by {{ pr.author }}\\\"',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'PR: Test PR by testuser\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n \\n-      expect(mockExec).toHaveBeenCalledWith('echo \\\"PR: Test PR by testuser\\\"', expect.any(Object));\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"PR: Test PR by testuser\\\"',\\n+        expect.any(Object)\\n+      );\\n       expect((result as any).output).toBe('PR: Test PR by testuser');\\n     });\\n \\n@@ -254,14 +280,15 @@ describe('CommandCheckProvider', () => {\\n         exec: 'echo \\\"Files: {{ fileCount }}\\\"',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'Files: 2\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n \\n-      expect(mockExec).toHaveBeenCalledWith('echo \\\"Files: 2\\\"', expect.any(Object));\\n+      expect(mockExecute).toHaveBeenCalledWith('echo \\\"Files: 2\\\"', expect.any(Object));\\n       expect((result as any).output).toBe('Files: 2');\\n     });\\n \\n@@ -271,15 +298,16 @@ describe('CommandCheckProvider', () => {\\n         exec: 'echo \\\"static command\\\"',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'static command\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       // eslint-disable-next-line @typescript-eslint/no-unused-vars\\n       const result = await provider.execute(mockPRInfo, config);\\n \\n-      expect(mockExec).toHaveBeenCalledWith('echo \\\"static command\\\"', expect.any(Object));\\n+      expect(mockExecute).toHaveBeenCalledWith('echo \\\"static command\\\"', expect.any(Object));\\n     });\\n   });\\n \\n@@ -293,19 +321,19 @@ describe('CommandCheckProvider', () => {\\n         },\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'test_value\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       await provider.execute(mockPRInfo, config);\\n \\n-      expect(mockExec).toHaveBeenCalledWith('echo $TEST_VAR', {\\n+      expect(mockExecute).toHaveBeenCalledWith('echo $TEST_VAR', {\\n         env: expect.objectContaining({\\n           TEST_VAR: 'test_value',\\n         }),\\n         timeout: 60000,\\n-        maxBuffer: 10 * 1024 * 1024,\\n       });\\n     });\\n \\n@@ -325,14 +353,19 @@ describe('CommandCheckProvider', () => {\\n         exec: 'env',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'output\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       await provider.execute(mockPRInfo, config);\\n \\n-      const envArg = mockExec.mock.calls[0][1].env;\\n+      expect(mockExecute.mock.calls).toHaveLength(1);\\n+      const callArgs = mockExecute.mock.calls[0];\\n+      expect(callArgs).toBeDefined();\\n+      expect(callArgs[1]).toBeDefined();\\n+      const envArg = callArgs[1]!.env;\\n       expect(envArg).toHaveProperty('CI_BUILD_NUMBER', '123');\\n       expect(envArg).toHaveProperty('GITHUB_REPOSITORY', 'test/repo');\\n       expect(envArg).toHaveProperty('NODE_VERSION', '18.0.0');\\n@@ -353,9 +386,10 @@ describe('CommandCheckProvider', () => {\\n         transform: '{{ output.data | join: \\\",\\\" }}',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: '{\\\"data\\\": [1, 2, 3]}\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n@@ -370,9 +404,10 @@ describe('CommandCheckProvider', () => {\\n         transform: '{{ invalid.liquid.syntax !! }}',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'test\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n@@ -394,9 +429,10 @@ describe('CommandCheckProvider', () => {\\n         transform: '{\\\"transformed\\\": \\\"{{ output }}\\\"}',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'raw text\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n@@ -428,14 +464,15 @@ describe('CommandCheckProvider', () => {\\n         ],\\n       });\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'Dep count: 1\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config, dependencyResults);\\n \\n-      expect(mockExec).toHaveBeenCalledWith('echo \\\"Dep count: 1\\\"', expect.any(Object));\\n+      expect(mockExecute).toHaveBeenCalledWith('echo \\\"Dep count: 1\\\"', expect.any(Object));\\n       expect((result as any).output).toBe('Dep count: 1');\\n     });\\n \\n@@ -451,9 +488,10 @@ describe('CommandCheckProvider', () => {\\n         output: { customData: 'test-value' },\\n       } as ReviewSummary);\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'test-value\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config, dependencyResults);\\n@@ -480,14 +518,15 @@ describe('CommandCheckProvider', () => {\\n         },\\n       } as ReviewSummary);\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'Complexity: high, Priority: 8, Hours: 24\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config, dependencyResults);\\n \\n-      expect(mockExec).toHaveBeenCalledWith(\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n         'echo \\\"Complexity: high, Priority: 8, Hours: 24\\\"',\\n         expect.any(Object)\\n       );\\n@@ -508,9 +547,10 @@ describe('CommandCheckProvider', () => {\\n         exec: 'echo \\\"output\\\" && echo \\\"warning\\\" >&2',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'output\\\\n',\\n         stderr: 'warning\\\\n',\\n+        exitCode: 0,\\n       });\\n \\n       await provider.execute(mockPRInfo, config);\\n@@ -533,9 +573,10 @@ describe('CommandCheckProvider', () => {\\n         exec: 'echo \\\"output\\\" && echo \\\"warning\\\" >&2',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'output\\\\n',\\n         stderr: 'warning\\\\n',\\n+        exitCode: 0,\\n       });\\n \\n       await provider.execute(mockPRInfo, config);\\n@@ -552,17 +593,17 @@ describe('CommandCheckProvider', () => {\\n         exec: 'sleep 1000', // Long running command\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'output\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       await provider.execute(mockPRInfo, config);\\n \\n-      expect(mockExec).toHaveBeenCalledWith('sleep 1000', {\\n+      expect(mockExecute).toHaveBeenCalledWith('sleep 1000', {\\n         env: expect.any(Object),\\n         timeout: 60000, // 60 second timeout\\n-        maxBuffer: 10 * 1024 * 1024, // 10MB buffer\\n       });\\n     });\\n   });\\n@@ -580,9 +621,10 @@ describe('CommandCheckProvider', () => {\\n         },\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: JSON.stringify(complexOutput) + '\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n@@ -596,9 +638,10 @@ describe('CommandCheckProvider', () => {\\n         exec: 'true', // Command that produces no output\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: '',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n@@ -612,9 +655,10 @@ describe('CommandCheckProvider', () => {\\n         exec: 'echo \\\"   content   \\\"',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: '   content   \\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/references-link-template.test.ts\",\"additions\":1,\"deletions\":0,\"changes\":30,\"patch\":\"diff --git a/tests/unit/references-link-template.test.ts b/tests/unit/references-link-template.test.ts\\nnew file mode 100644\\nindex 00000000..b477f8b4\\n--- /dev/null\\n+++ b/tests/unit/references-link-template.test.ts\\n@@ -0,0 +1,30 @@\\n+import { createExtendedLiquid } from '../../src/liquid-extensions';\\n+\\n+describe('References example link liquid rendering', () => {\\n+  const tpl =\\n+    'https://github.com/{{ event.repository.fullName }}/blob/{{ event.pull_request.head.sha | default: \\\"HEAD\\\" }}/path/to/file.ext#LSTART-LEND';\\n+\\n+  test('renders HEAD fallback for issue context', async () => {\\n+    const liquid = createExtendedLiquid();\\n+    const out = await liquid.parseAndRender(tpl, {\\n+      event: {\\n+        repository: { owner: { login: 'owner' }, name: 'repo', fullName: 'owner/repo' },\\n+        // No pull_request in issue context\\n+      },\\n+    });\\n+    expect(out).toBe('https://github.com/owner/repo/blob/HEAD/path/to/file.ext#LSTART-LEND');\\n+  });\\n+\\n+  test('renders PR head sha when provided', async () => {\\n+    const liquid = createExtendedLiquid();\\n+    const out = await liquid.parseAndRender(tpl, {\\n+      event: {\\n+        repository: { owner: { login: 'owner' }, name: 'repo', fullName: 'owner/repo' },\\n+        pull_request: { head: { sha: 'deadbeefcafebabe' } },\\n+      },\\n+    });\\n+    expect(out).toBe(\\n+      'https://github.com/owner/repo/blob/deadbeefcafebabe/path/to/file.ext#LSTART-LEND'\\n+    );\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/unit/reviewer.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":2,\"patch\":\"diff --git a/tests/unit/reviewer.test.ts b/tests/unit/reviewer.test.ts\\nindex 8a1150ba..a135e219 100644\\n--- a/tests/unit/reviewer.test.ts\\n+++ b/tests/unit/reviewer.test.ts\\n@@ -191,7 +191,7 @@ jest.mock('../../src/ai-review-service', () => {\\n             file: 'src/test.ts',\\n             line: 5,\\n             ruleId: 'security/dangerous-eval',\\n-            message: 'Dangerous eval usage detected',\\n+            message: 'Dangerous eval usage detected - security vulnerability',\\n             severity: 'critical',\\n             category: 'security',\\n           });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/routing-transitions-and-contracts.test.ts\",\"additions\":3,\"deletions\":0,\"changes\":89,\"patch\":\"diff --git a/tests/unit/routing-transitions-and-contracts.test.ts b/tests/unit/routing-transitions-and-contracts.test.ts\\nnew file mode 100644\\nindex 00000000..2584372e\\n--- /dev/null\\n+++ b/tests/unit/routing-transitions-and-contracts.test.ts\\n@@ -0,0 +1,89 @@\\n+import { StateMachineExecutionEngine } from '../../src/state-machine-execution-engine';\\n+\\n+describe('Routing transitions and contracts', () => {\\n+  it('on_finish transitions route to assistant when any validation invalid', async () => {\\n+    const cfg = {\\n+      version: '1.0',\\n+      output: { pr_comment: { enabled: false } },\\n+      checks: {\\n+        extract: {\\n+          type: 'command',\\n+          exec: 'node -e \\\"console.log(\\\\'[{\\\\\\\\\\\"id\\\\\\\\\\\":1},{\\\\\\\\\\\"id\\\\\\\\\\\":2}]\\\\')\\\"',\\n+          forEach: true,\\n+          on_finish: {\\n+            transitions: [\\n+              {\\n+                when: \\\"any(outputs_history['validate'], v => v && v.is_valid === false) && event.name === 'manual'\\\",\\n+                to: 'assistant',\\n+              },\\n+            ],\\n+          },\\n+        },\\n+        validate: {\\n+          type: 'command',\\n+          depends_on: ['extract'],\\n+          // For each item, emit { is_valid: false }\\n+          exec: 'node -e \\\"console.log(\\\\'{\\\\\\\\\\\"is_valid\\\\\\\\\\\":false}\\\\')\\\"',\\n+        },\\n+        assistant: {\\n+          type: 'log',\\n+          message: 'assistant routed',\\n+          level: 'info',\\n+        },\\n+      },\\n+    } as any;\\n+\\n+    const engine = new StateMachineExecutionEngine();\\n+    const res = await engine.executeChecks({\\n+      checks: ['extract', 'validate', 'assistant'],\\n+      config: cfg,\\n+      debug: false,\\n+    });\\n+\\n+    // Expect assistant to have run due to transition\\n+    const stats = res.executionStatistics?.checks || [];\\n+    const routed = stats.find(s => s.checkName === 'assistant');\\n+    expect(routed?.totalRuns || 0).toBeGreaterThanOrEqual(1);\\n+  });\\n+\\n+  it('assume=false skips check with skipReason=assume', async () => {\\n+    const cfg = {\\n+      version: '1.0',\\n+      checks: {\\n+        c1: {\\n+          type: 'log',\\n+          message: 'should not run',\\n+          level: 'info',\\n+          assume: 'false',\\n+        },\\n+      },\\n+    } as any;\\n+\\n+    const engine = new StateMachineExecutionEngine();\\n+    const res = await engine.executeChecks({ checks: ['c1'], config: cfg, debug: false });\\n+    const st = (res.executionStatistics?.checks || []).find(s => s.checkName === 'c1');\\n+    expect(st?.skipped).toBe(true);\\n+    expect(st?.skipReason).toBe('assume');\\n+    expect(st?.totalRuns).toBe(0);\\n+  });\\n+\\n+  it('guarantee violation adds a contract/guarantee_failed issue', async () => {\\n+    const cfg = {\\n+      version: '1.0',\\n+      checks: {\\n+        c2: {\\n+          type: 'command',\\n+          exec: 'node -e \\\"console.log(\\\\'{\\\\\\\\\\\"ok\\\\\\\\\\\":false}\\\\')\\\"',\\n+          guarantee: 'output.ok === true',\\n+        },\\n+      },\\n+    } as any;\\n+\\n+    const engine = new StateMachineExecutionEngine();\\n+    const res = await engine.executeChecks({ checks: ['c2'], config: cfg, debug: false });\\n+    const issues = (res.reviewSummary.issues || []).filter((i: any) =>\\n+      String(i.ruleId || '').includes('contract/guarantee_failed')\\n+    );\\n+    expect(issues.length).toBeGreaterThanOrEqual(1);\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/unit/schema-validation-renderer-string.test.ts\",\"additions\":2,\"deletions\":0,\"changes\":47,\"patch\":\"diff --git a/tests/unit/schema-validation-renderer-string.test.ts b/tests/unit/schema-validation-renderer-string.test.ts\\nnew file mode 100644\\nindex 00000000..6a99998d\\n--- /dev/null\\n+++ b/tests/unit/schema-validation-renderer-string.test.ts\\n@@ -0,0 +1,47 @@\\n+import { StateMachineExecutionEngine } from '../../src/state-machine-execution-engine';\\n+import type { VisorConfig } from '../../src/types/config';\\n+\\n+describe('Schema validation for renderer string schemas (issue-assistant)', () => {\\n+  it('emits contract/schema_validation_failed when required fields are missing', async () => {\\n+    const cfg: VisorConfig = {\\n+      version: '1.0',\\n+      output: { pr_comment: { enabled: false } } as any,\\n+      checks: {\\n+        // Use script provider to return a deterministic object\\n+        issue_step: {\\n+          type: 'script',\\n+          schema: 'issue-assistant',\\n+          content: 'return { text: \\\"hello\\\" };', // missing intent\\n+        } as any,\\n+      },\\n+    } as any;\\n+\\n+    const engine = new StateMachineExecutionEngine();\\n+    const res = await engine.executeChecks({ checks: ['issue_step'], config: cfg, debug: false });\\n+    const issues = (res.reviewSummary.issues || []).filter(i =>\\n+      String(i.ruleId || '').includes('contract/schema_validation_failed')\\n+    );\\n+    expect(issues.length).toBeGreaterThanOrEqual(1);\\n+  });\\n+\\n+  it('passes validation when output matches the renderer schema', async () => {\\n+    const cfg: VisorConfig = {\\n+      version: '1.0',\\n+      output: { pr_comment: { enabled: false } } as any,\\n+      checks: {\\n+        issue_ok: {\\n+          type: 'script',\\n+          schema: 'issue-assistant',\\n+          content: 'return { text: \\\"ok\\\", intent: \\\"issue_triage\\\" };',\\n+        } as any,\\n+      },\\n+    } as any;\\n+\\n+    const engine = new StateMachineExecutionEngine();\\n+    const res = await engine.executeChecks({ checks: ['issue_ok'], config: cfg, debug: false });\\n+    const issues = (res.reviewSummary.issues || []).filter(i =>\\n+      String(i.ruleId || '').includes('contract/schema_validation_failed')\\n+    );\\n+    expect(issues.length).toBe(0);\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/unit/schema-validation-script.test.ts\",\"additions\":2,\"deletions\":0,\"changes\":56,\"patch\":\"diff --git a/tests/unit/schema-validation-script.test.ts b/tests/unit/schema-validation-script.test.ts\\nnew file mode 100644\\nindex 00000000..a889ea55\\n--- /dev/null\\n+++ b/tests/unit/schema-validation-script.test.ts\\n@@ -0,0 +1,56 @@\\n+import { StateMachineExecutionEngine } from '../../src/state-machine-execution-engine';\\n+import type { VisorConfig } from '../../src/types/config';\\n+\\n+describe('Schema validation (script provider)', () => {\\n+  it('flags schema violations with contract/schema_validation_failed', async () => {\\n+    const cfg: VisorConfig = {\\n+      version: '1.0',\\n+      output: { pr_comment: { enabled: false } } as any,\\n+      checks: {\\n+        s: {\\n+          type: 'script',\\n+          content: 'return { ok: false };',\\n+          schema: {\\n+            type: 'object',\\n+            properties: { ok: { type: 'boolean' }, count: { type: 'integer' } },\\n+            required: ['ok', 'count'],\\n+            additionalProperties: true,\\n+          },\\n+        },\\n+      },\\n+    } as any;\\n+\\n+    const engine = new StateMachineExecutionEngine();\\n+    const res = await engine.executeChecks({ checks: ['s'], config: cfg, debug: false });\\n+    const issues = (res.reviewSummary.issues || []).filter(i =>\\n+      String(i.ruleId || '').includes('contract/schema_validation_failed')\\n+    );\\n+    expect(issues.length).toBeGreaterThanOrEqual(1);\\n+  });\\n+\\n+  it('passes when script output satisfies the schema', async () => {\\n+    const cfg: VisorConfig = {\\n+      version: '1.0',\\n+      output: { pr_comment: { enabled: false } } as any,\\n+      checks: {\\n+        s_ok: {\\n+          type: 'script',\\n+          content: 'return { ok: true, count: 1 };',\\n+          schema: {\\n+            type: 'object',\\n+            properties: { ok: { type: 'boolean' }, count: { type: 'integer' } },\\n+            required: ['ok', 'count'],\\n+            additionalProperties: true,\\n+          },\\n+        },\\n+      },\\n+    } as any;\\n+\\n+    const engine = new StateMachineExecutionEngine();\\n+    const res = await engine.executeChecks({ checks: ['s_ok'], config: cfg, debug: false });\\n+    const issues = (res.reviewSummary.issues || []).filter(i =>\\n+      String(i.ruleId || '').includes('contract/schema_validation_failed')\\n+    );\\n+    expect(issues.length).toBe(0);\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/unit/workflow-check-provider.test.ts\",\"additions\":13,\"deletions\":0,\"changes\":462,\"patch\":\"diff --git a/tests/unit/workflow-check-provider.test.ts b/tests/unit/workflow-check-provider.test.ts\\nnew file mode 100644\\nindex 00000000..42b350bf\\n--- /dev/null\\n+++ b/tests/unit/workflow-check-provider.test.ts\\n@@ -0,0 +1,462 @@\\n+/**\\n+ * Unit tests for WorkflowCheckProvider\\n+ */\\n+\\n+import { WorkflowCheckProvider } from '../../src/providers/workflow-check-provider';\\n+import { WorkflowRegistry } from '../../src/workflow-registry';\\n+import { WorkflowExecutor } from '../../src/workflow-executor';\\n+import { WorkflowDefinition } from '../../src/types/workflow';\\n+import { PRInfo } from '../../src/pr-analyzer';\\n+import { CheckProviderConfig } from '../../src/providers/check-provider.interface';\\n+\\n+// Mock dependencies\\n+jest.mock('../../src/workflow-registry');\\n+jest.mock('../../src/workflow-executor');\\n+jest.mock('../../src/logger', () => ({\\n+  logger: {\\n+    info: jest.fn(),\\n+    error: jest.fn(),\\n+    warn: jest.fn(),\\n+    debug: jest.fn(),\\n+  },\\n+}));\\n+\\n+describe('WorkflowCheckProvider', () => {\\n+  let provider: WorkflowCheckProvider;\\n+  let mockRegistry: jest.Mocked<WorkflowRegistry>;\\n+  let mockExecutor: jest.Mocked<WorkflowExecutor>;\\n+\\n+  const sampleWorkflow: WorkflowDefinition = {\\n+    id: 'test-workflow',\\n+    name: 'Test Workflow',\\n+    inputs: [\\n+      {\\n+        name: 'threshold',\\n+        schema: { type: 'number' },\\n+        default: 80,\\n+      },\\n+      {\\n+        name: 'language',\\n+        schema: { type: 'string' },\\n+        required: true,\\n+      },\\n+    ],\\n+    outputs: [\\n+      {\\n+        name: 'score',\\n+        value_js: 'steps.analyze.output.score',\\n+      },\\n+      {\\n+        name: 'passed',\\n+        value_js: 'steps.analyze.output.score > inputs.threshold',\\n+      },\\n+    ],\\n+    steps: {\\n+      analyze: {\\n+        type: 'ai',\\n+        prompt: 'Analyze {{ inputs.language }} code',\\n+      },\\n+    },\\n+  };\\n+\\n+  const samplePRInfo: PRInfo = {\\n+    number: 123,\\n+    title: 'Test PR',\\n+    body: 'Test description',\\n+    author: 'test-author',\\n+    base: 'main',\\n+    head: 'feature',\\n+    files: [\\n+      {\\n+        filename: 'test.ts',\\n+        additions: 5,\\n+        deletions: 2,\\n+        changes: 7,\\n+        status: 'modified',\\n+      },\\n+    ],\\n+    totalAdditions: 10,\\n+    totalDeletions: 5,\\n+  };\\n+\\n+  beforeEach(() => {\\n+    // Create mock instances\\n+    mockRegistry = {\\n+      getInstance: jest.fn(),\\n+      get: jest.fn(),\\n+      has: jest.fn(),\\n+      register: jest.fn(),\\n+      validateInputs: jest.fn(),\\n+      validateWorkflow: jest.fn(),\\n+      list: jest.fn(),\\n+      getMetadata: jest.fn(),\\n+      unregister: jest.fn(),\\n+      clear: jest.fn(),\\n+      import: jest.fn(),\\n+      importMany: jest.fn(),\\n+    } as any;\\n+\\n+    mockExecutor = {\\n+      execute: jest.fn(),\\n+    } as any;\\n+\\n+    // Setup singleton mock\\n+    (WorkflowRegistry.getInstance as jest.Mock).mockReturnValue(mockRegistry);\\n+    (WorkflowExecutor as jest.Mock).mockImplementation(() => mockExecutor);\\n+\\n+    provider = new WorkflowCheckProvider();\\n+  });\\n+\\n+  afterEach(() => {\\n+    jest.clearAllMocks();\\n+  });\\n+\\n+  describe('getName and getDescription', () => {\\n+    it('should return correct name and description', () => {\\n+      expect(provider.getName()).toBe('workflow');\\n+      expect(provider.getDescription()).toBe('Executes reusable workflow definitions as checks');\\n+    });\\n+  });\\n+\\n+  describe('validateConfig', () => {\\n+    it('should validate config with workflow field', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+      };\\n+\\n+      mockRegistry.has.mockReturnValue(true);\\n+\\n+      const result = await provider.validateConfig(config);\\n+      expect(result).toBe(true);\\n+    });\\n+\\n+    it('should reject config without workflow field', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+      };\\n+\\n+      const result = await provider.validateConfig(config);\\n+      expect(result).toBe(false);\\n+    });\\n+\\n+    it('should reject config with non-existent workflow', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'non-existent',\\n+      };\\n+\\n+      mockRegistry.has.mockReturnValue(false);\\n+\\n+      const result = await provider.validateConfig(config);\\n+      expect(result).toBe(false);\\n+    });\\n+  });\\n+\\n+  describe('execute', () => {\\n+    it('should execute workflow with basic inputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+        workflow_inputs: {\\n+          threshold: 90,\\n+          language: 'typescript',\\n+        },\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(sampleWorkflow);\\n+      mockRegistry.validateInputs.mockReturnValue({ valid: true });\\n+      mockExecutor.execute.mockResolvedValue({\\n+        success: true,\\n+        score: 95,\\n+        confidence: 'high',\\n+        issues: [],\\n+        comments: [],\\n+        output: { score: 95, passed: true },\\n+        status: 'completed',\\n+        duration: 1000,\\n+      });\\n+\\n+      const result = await provider.execute(samplePRInfo, config);\\n+\\n+      expect(mockRegistry.get).toHaveBeenCalledWith('test-workflow');\\n+      expect(mockRegistry.validateInputs).toHaveBeenCalledWith(\\n+        sampleWorkflow,\\n+        expect.objectContaining({\\n+          threshold: 90,\\n+          language: 'typescript',\\n+        })\\n+      );\\n+      expect(mockExecutor.execute).toHaveBeenCalled();\\n+      expect((result as any).score).toBe(95);\\n+      expect((result as any).output).toEqual({ score: 95, passed: true });\\n+    });\\n+\\n+    it('should use default input values', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+        workflow_inputs: {\\n+          language: 'javascript', // Only provide required param\\n+        },\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(sampleWorkflow);\\n+      mockRegistry.validateInputs.mockReturnValue({ valid: true });\\n+      mockExecutor.execute.mockResolvedValue({\\n+        success: true,\\n+        output: { score: 85 },\\n+        status: 'completed',\\n+      });\\n+\\n+      await provider.execute(samplePRInfo, config);\\n+\\n+      // Check that the executor was called with defaults\\n+      const executorCall = mockExecutor.execute.mock.calls[0];\\n+      expect(executorCall[1].inputs).toEqual({\\n+        threshold: 80, // Default value\\n+        language: 'javascript',\\n+      });\\n+    });\\n+\\n+    it('should process Liquid templates in inputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+        workflow_inputs: {\\n+          language:\\n+            '{% if pr.files[0].filename contains \\\".ts\\\" %}typescript{% else %}javascript{% endif %}',\\n+          threshold: 85,\\n+        },\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(sampleWorkflow);\\n+      mockRegistry.validateInputs.mockReturnValue({ valid: true });\\n+      mockExecutor.execute.mockResolvedValue({\\n+        success: true,\\n+        output: {},\\n+        status: 'completed',\\n+      });\\n+\\n+      await provider.execute(samplePRInfo, config);\\n+\\n+      const executorCall = mockExecutor.execute.mock.calls[0];\\n+      expect(executorCall[1].inputs.language).toBe('typescript'); // test.ts contains .ts\\n+    });\\n+\\n+    it('should handle workflow overrides', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+        workflow_inputs: {\\n+          language: 'python',\\n+          threshold: 70,\\n+        },\\n+        workflow_overrides: {\\n+          analyze: {\\n+            prompt: 'Custom prompt for analysis',\\n+            timeout: 120,\\n+          },\\n+        },\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(sampleWorkflow);\\n+      mockRegistry.validateInputs.mockReturnValue({ valid: true });\\n+      mockExecutor.execute.mockResolvedValue({\\n+        success: true,\\n+        output: {},\\n+        status: 'completed',\\n+      });\\n+\\n+      await provider.execute(samplePRInfo, config);\\n+\\n+      // Check that modified workflow was passed to executor\\n+      const executorCall = mockExecutor.execute.mock.calls[0];\\n+      const modifiedWorkflow = executorCall[0];\\n+      expect(modifiedWorkflow.steps.analyze.prompt).toBe('Custom prompt for analysis');\\n+      expect(modifiedWorkflow.steps.analyze.timeout).toBe(120);\\n+    });\\n+\\n+    it('should map workflow outputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+        workflow_inputs: {\\n+          language: 'go',\\n+          threshold: 80,\\n+        },\\n+        output_mapping: {\\n+          quality_score: 'score',\\n+          is_passing: 'passed',\\n+        },\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(sampleWorkflow);\\n+      mockRegistry.validateInputs.mockReturnValue({ valid: true });\\n+      mockExecutor.execute.mockResolvedValue({\\n+        success: true,\\n+        output: { score: 92, passed: true, details: 'test' },\\n+        status: 'completed',\\n+      });\\n+\\n+      const result = await provider.execute(samplePRInfo, config);\\n+\\n+      expect((result as any).output).toEqual({\\n+        quality_score: 92,\\n+        is_passing: true,\\n+      });\\n+    });\\n+\\n+    it('should handle nested output paths in mapping', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+        workflow_inputs: {\\n+          language: 'java',\\n+          threshold: 75,\\n+        },\\n+        output_mapping: {\\n+          nested_value: 'result.data.value',\\n+        },\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(sampleWorkflow);\\n+      mockRegistry.validateInputs.mockReturnValue({ valid: true });\\n+      mockExecutor.execute.mockResolvedValue({\\n+        success: true,\\n+        output: {\\n+          result: {\\n+            data: {\\n+              value: 'nested-test',\\n+            },\\n+          },\\n+        },\\n+        status: 'completed',\\n+      });\\n+\\n+      const result = await provider.execute(samplePRInfo, config);\\n+\\n+      expect((result as any).output).toEqual({\\n+        nested_value: 'nested-test',\\n+      });\\n+    });\\n+\\n+    it('should throw error for invalid inputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+        workflow_inputs: {\\n+          // Missing required 'language' param\\n+          threshold: 90,\\n+        },\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(sampleWorkflow);\\n+      mockRegistry.validateInputs.mockReturnValue({\\n+        valid: false,\\n+        errors: [{ path: 'inputs.language', message: 'Required input is missing' }],\\n+      });\\n+\\n+      await expect(provider.execute(samplePRInfo, config)).rejects.toThrow(\\n+        'Invalid workflow inputs'\\n+      );\\n+    });\\n+\\n+    it('should throw error when workflow not found', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'non-existent',\\n+        workflow_inputs: {},\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(undefined);\\n+\\n+      await expect(provider.execute(samplePRInfo, config)).rejects.toThrow(\\n+        \\\"Workflow 'non-existent' not found\\\"\\n+      );\\n+    });\\n+\\n+    it('should pass workflow inputs to executor', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+        workflow_inputs: {\\n+          language: 'python',\\n+          threshold: 85,\\n+        },\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(sampleWorkflow);\\n+      mockRegistry.validateInputs.mockReturnValue({ valid: true });\\n+      mockExecutor.execute.mockResolvedValue({\\n+        success: true,\\n+        output: {},\\n+        status: 'completed',\\n+      });\\n+\\n+      await provider.execute(samplePRInfo, config);\\n+\\n+      const executorCall = mockExecutor.execute.mock.calls[0];\\n+      expect(executorCall[1].inputs.language).toBe('python');\\n+      expect(executorCall[1].inputs.threshold).toBe(85);\\n+    });\\n+\\n+    it('should format workflow results correctly', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+        workflow_inputs: {\\n+          language: 'rust',\\n+          threshold: 85,\\n+        },\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(sampleWorkflow);\\n+      mockRegistry.validateInputs.mockReturnValue({ valid: true });\\n+      mockExecutor.execute.mockResolvedValue({\\n+        success: true,\\n+        score: 88,\\n+        issues: [{ severity: 'warning' }, { severity: 'info' }],\\n+        output: { score: 88, passed: true },\\n+        status: 'completed',\\n+        duration: 1500,\\n+        stepSummaries: [\\n+          { stepId: 'analyze', status: 'success', issues: [{ severity: 'warning' }] },\\n+        ],\\n+      });\\n+\\n+      const result = await provider.execute(samplePRInfo, config);\\n+\\n+      expect((result as any).content).toContain('Workflow: Test Workflow');\\n+      expect((result as any).content).toContain('Score: 88');\\n+      expect((result as any).content).toContain('Issues Found: 2');\\n+      expect((result as any).content).toContain('Duration: 1500ms');\\n+      expect((result as any).content).toContain('analyze: success');\\n+    });\\n+  });\\n+\\n+  describe('getSupportedConfigKeys', () => {\\n+    it('should return supported config keys', () => {\\n+      const keys = provider.getSupportedConfigKeys();\\n+      expect(keys).toContain('workflow');\\n+      expect(keys).toContain('args');\\n+      expect(keys).toContain('overrides');\\n+      expect(keys).toContain('output_mapping');\\n+      expect(keys).toContain('timeout');\\n+      expect(keys).toContain('env');\\n+      expect(keys).toContain('checkName');\\n+    });\\n+  });\\n+\\n+  describe('isAvailable and getRequirements', () => {\\n+    it('should always be available', async () => {\\n+      const available = await provider.isAvailable();\\n+      expect(available).toBe(true);\\n+    });\\n+\\n+    it('should have no requirements', () => {\\n+      const requirements = provider.getRequirements();\\n+      expect(requirements).toEqual([]);\\n+    });\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/unit/workflow-registry.test.ts\",\"additions\":16,\"deletions\":0,\"changes\":543,\"patch\":\"diff --git a/tests/unit/workflow-registry.test.ts b/tests/unit/workflow-registry.test.ts\\nnew file mode 100644\\nindex 00000000..b110ef95\\n--- /dev/null\\n+++ b/tests/unit/workflow-registry.test.ts\\n@@ -0,0 +1,543 @@\\n+/**\\n+ * Unit tests for WorkflowRegistry\\n+ */\\n+\\n+import { WorkflowRegistry } from '../../src/workflow-registry';\\n+import { WorkflowDefinition } from '../../src/types/workflow';\\n+import * as fs from 'fs';\\n+import * as yaml from 'js-yaml';\\n+\\n+// Mock fs and fetch\\n+jest.mock('fs', () => ({\\n+  promises: {\\n+    readFile: jest.fn(),\\n+  },\\n+}));\\n+\\n+global.fetch = jest.fn();\\n+\\n+describe('WorkflowRegistry', () => {\\n+  let registry: WorkflowRegistry;\\n+\\n+  beforeEach(() => {\\n+    // Get a fresh instance for each test\\n+    (WorkflowRegistry as any).instance = undefined;\\n+    registry = WorkflowRegistry.getInstance();\\n+  });\\n+\\n+  afterEach(() => {\\n+    registry.clear();\\n+    jest.clearAllMocks();\\n+  });\\n+\\n+  describe('singleton pattern', () => {\\n+    it('should return the same instance', () => {\\n+      const instance1 = WorkflowRegistry.getInstance();\\n+      const instance2 = WorkflowRegistry.getInstance();\\n+      expect(instance1).toBe(instance2);\\n+    });\\n+  });\\n+\\n+  describe('register', () => {\\n+    it('should register a valid workflow', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test prompt',\\n+          },\\n+        },\\n+      };\\n+\\n+      const result = registry.register(workflow);\\n+      expect(result.valid).toBe(true);\\n+      expect(registry.has('test-workflow')).toBe(true);\\n+    });\\n+\\n+    it('should reject workflow without ID', () => {\\n+      const workflow = {\\n+        name: 'Test Workflow',\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test prompt',\\n+          },\\n+        },\\n+      } as any;\\n+\\n+      const result = registry.register(workflow);\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors?.[0].message).toContain('ID is required');\\n+    });\\n+\\n+    it('should reject workflow without steps', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        steps: {},\\n+      };\\n+\\n+      const result = registry.register(workflow);\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors?.[0].message).toContain('at least one step');\\n+    });\\n+\\n+    it('should reject duplicate workflow ID without override', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test prompt',\\n+          },\\n+        },\\n+      };\\n+\\n+      registry.register(workflow);\\n+      const result = registry.register(workflow);\\n+\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors?.[0].message).toContain('already exists');\\n+    });\\n+\\n+    it('should allow override with flag', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test prompt',\\n+          },\\n+        },\\n+      };\\n+\\n+      registry.register(workflow);\\n+      const result = registry.register(workflow, 'inline', { override: true });\\n+\\n+      expect(result.valid).toBe(true);\\n+    });\\n+  });\\n+\\n+  describe('validateWorkflow', () => {\\n+    it('should validate input parameters', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        inputs: [\\n+          {\\n+            name: 'param1',\\n+            schema: { type: 'string' },\\n+          },\\n+          {\\n+            name: '', // Invalid: empty name\\n+            schema: { type: 'number' },\\n+          },\\n+        ],\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+          },\\n+        },\\n+      };\\n+\\n+      const result = registry.validateWorkflow(workflow);\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors).toContainEqual(\\n+        expect.objectContaining({\\n+          path: 'inputs[1].name',\\n+          message: expect.stringContaining('name is required'),\\n+        })\\n+      );\\n+    });\\n+\\n+    it('should validate output parameters', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        outputs: [\\n+          {\\n+            name: 'output1',\\n+            // Missing both value and value_js\\n+          },\\n+        ],\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+          },\\n+        },\\n+      };\\n+\\n+      const result = registry.validateWorkflow(workflow);\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors).toContainEqual(\\n+        expect.objectContaining({\\n+          path: 'outputs[0]',\\n+          message: expect.stringContaining('value or value_js'),\\n+        })\\n+      );\\n+    });\\n+\\n+    it('should detect circular dependencies', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+            depends_on: ['step3'],\\n+          },\\n+          step2: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+            depends_on: ['step1'],\\n+          },\\n+          step3: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+            depends_on: ['step2'],\\n+          },\\n+        },\\n+      };\\n+\\n+      const result = registry.validateWorkflow(workflow);\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors?.[0].message).toContain('Circular dependencies');\\n+    });\\n+\\n+    it('should validate step dependencies exist', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+            depends_on: ['non-existent-step'],\\n+          },\\n+        },\\n+      };\\n+\\n+      const result = registry.validateWorkflow(workflow);\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors).toContainEqual(\\n+        expect.objectContaining({\\n+          path: 'steps.step1.depends_on',\\n+          message: expect.stringContaining('non-existent step'),\\n+        })\\n+      );\\n+    });\\n+\\n+    it('should validate input mappings', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        inputs: [\\n+          {\\n+            name: 'valid_param',\\n+            schema: { type: 'string' },\\n+          },\\n+        ],\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+            inputs: {\\n+              mapping1: {\\n+                source: 'param',\\n+                value: 'invalid_param', // Non-existent parameter\\n+              },\\n+            },\\n+          },\\n+        },\\n+      };\\n+\\n+      const result = registry.validateWorkflow(workflow);\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors).toContainEqual(\\n+        expect.objectContaining({\\n+          path: 'steps.step1.inputs.mapping1',\\n+          message: expect.stringContaining('non-existent parameter'),\\n+        })\\n+      );\\n+    });\\n+  });\\n+\\n+  describe('validateInputs', () => {\\n+    it('should validate required inputs', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        inputs: [\\n+          {\\n+            name: 'required_param',\\n+            schema: { type: 'string' },\\n+            required: true,\\n+          },\\n+          {\\n+            name: 'optional_param',\\n+            schema: { type: 'number' },\\n+            required: false,\\n+          },\\n+        ],\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+          },\\n+        },\\n+      };\\n+\\n+      const result = registry.validateInputs(workflow, {\\n+        optional_param: 42,\\n+      });\\n+\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors?.[0].message).toContain(\\\"Required input 'required_param'\\\");\\n+    });\\n+\\n+    it('should use defaults for missing optional inputs', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        inputs: [\\n+          {\\n+            name: 'param_with_default',\\n+            schema: { type: 'string' },\\n+            default: 'default_value',\\n+          },\\n+        ],\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+          },\\n+        },\\n+      };\\n+\\n+      const result = registry.validateInputs(workflow, {});\\n+      expect(result.valid).toBe(true);\\n+    });\\n+\\n+    it('should validate input schemas', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        inputs: [\\n+          {\\n+            name: 'number_param',\\n+            schema: {\\n+              type: 'number',\\n+              minimum: 0,\\n+              maximum: 100,\\n+            },\\n+          },\\n+        ],\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+          },\\n+        },\\n+      };\\n+\\n+      const result = registry.validateInputs(workflow, {\\n+        number_param: 150, // Out of range\\n+      });\\n+\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors?.[0].message).toContain('must be <= 100');\\n+    });\\n+  });\\n+\\n+  describe('import', () => {\\n+    it('should import workflow from YAML file', async () => {\\n+      const workflowYaml = `\\n+id: imported-workflow\\n+name: Imported Workflow\\n+steps:\\n+  step1:\\n+    type: ai\\n+    prompt: Test prompt\\n+`;\\n+\\n+      (fs.promises.readFile as jest.Mock).mockResolvedValue(workflowYaml);\\n+\\n+      const results = await registry.import('./workflow.yaml');\\n+\\n+      expect(results).toHaveLength(1);\\n+      expect(results[0].valid).toBe(true);\\n+      expect(registry.has('imported-workflow')).toBe(true);\\n+    });\\n+\\n+    it('should import workflow from JSON file', async () => {\\n+      const workflowJson = JSON.stringify({\\n+        id: 'json-workflow',\\n+        name: 'JSON Workflow',\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test prompt',\\n+          },\\n+        },\\n+      });\\n+\\n+      (fs.promises.readFile as jest.Mock).mockResolvedValue(workflowJson);\\n+\\n+      const results = await registry.import('./workflow.json');\\n+\\n+      expect(results).toHaveLength(1);\\n+      expect(results[0].valid).toBe(true);\\n+      expect(registry.has('json-workflow')).toBe(true);\\n+    });\\n+\\n+    it('should import workflow from URL', async () => {\\n+      const workflowData = {\\n+        id: 'remote-workflow',\\n+        name: 'Remote Workflow',\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test prompt',\\n+          },\\n+        },\\n+      };\\n+\\n+      (global.fetch as jest.Mock).mockResolvedValue({\\n+        ok: true,\\n+        text: async () => JSON.stringify(workflowData),\\n+      });\\n+\\n+      const results = await registry.import('https://example.com/workflow.json');\\n+\\n+      expect(results).toHaveLength(1);\\n+      expect(results[0].valid).toBe(true);\\n+      expect(registry.has('remote-workflow')).toBe(true);\\n+    });\\n+\\n+    it('should handle import errors', async () => {\\n+      (fs.promises.readFile as jest.Mock).mockRejectedValue(new Error('File not found'));\\n+\\n+      const results = await registry.import('./non-existent.yaml');\\n+\\n+      expect(results).toHaveLength(1);\\n+      expect(results[0].valid).toBe(false);\\n+      expect(results[0].errors?.[0].message).toContain('Failed to import');\\n+    });\\n+\\n+    it('should import multiple workflows from array', async () => {\\n+      const workflowsYaml = yaml.dump([\\n+        {\\n+          id: 'workflow1',\\n+          name: 'Workflow 1',\\n+          steps: { step1: { type: 'ai', prompt: 'Test' } },\\n+        },\\n+        {\\n+          id: 'workflow2',\\n+          name: 'Workflow 2',\\n+          steps: { step1: { type: 'ai', prompt: 'Test' } },\\n+        },\\n+      ]);\\n+\\n+      (fs.promises.readFile as jest.Mock).mockResolvedValue(workflowsYaml);\\n+\\n+      const results = await registry.import('./workflows.yaml');\\n+\\n+      expect(results).toHaveLength(2);\\n+      expect(results[0].valid).toBe(true);\\n+      expect(results[1].valid).toBe(true);\\n+      expect(registry.has('workflow1')).toBe(true);\\n+      expect(registry.has('workflow2')).toBe(true);\\n+    });\\n+  });\\n+\\n+  describe('get and metadata', () => {\\n+    it('should track usage statistics', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+          },\\n+        },\\n+      };\\n+\\n+      registry.register(workflow);\\n+\\n+      // Get workflow multiple times\\n+      registry.get('test-workflow');\\n+      registry.get('test-workflow');\\n+      registry.get('test-workflow');\\n+\\n+      const metadata = registry.getMetadata('test-workflow');\\n+      expect(metadata?.usage?.count).toBe(3);\\n+      expect(metadata?.usage?.lastUsed).toBeDefined();\\n+    });\\n+\\n+    it('should return undefined for non-existent workflow', () => {\\n+      const workflow = registry.get('non-existent');\\n+      expect(workflow).toBeUndefined();\\n+    });\\n+  });\\n+\\n+  describe('list and unregister', () => {\\n+    it('should list all workflows', () => {\\n+      const workflow1: WorkflowDefinition = {\\n+        id: 'workflow1',\\n+        name: 'Workflow 1',\\n+        steps: { step1: { type: 'ai', prompt: 'Test' } },\\n+      };\\n+\\n+      const workflow2: WorkflowDefinition = {\\n+        id: 'workflow2',\\n+        name: 'Workflow 2',\\n+        steps: { step1: { type: 'ai', prompt: 'Test' } },\\n+      };\\n+\\n+      registry.register(workflow1);\\n+      registry.register(workflow2);\\n+\\n+      const list = registry.list();\\n+      expect(list).toHaveLength(2);\\n+      expect(list.map(w => w.id)).toContain('workflow1');\\n+      expect(list.map(w => w.id)).toContain('workflow2');\\n+    });\\n+\\n+    it('should unregister workflow', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        steps: { step1: { type: 'ai', prompt: 'Test' } },\\n+      };\\n+\\n+      registry.register(workflow);\\n+      expect(registry.has('test-workflow')).toBe(true);\\n+\\n+      const result = registry.unregister('test-workflow');\\n+      expect(result).toBe(true);\\n+      expect(registry.has('test-workflow')).toBe(false);\\n+    });\\n+\\n+    it('should clear all workflows', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        steps: { step1: { type: 'ai', prompt: 'Test' } },\\n+      };\\n+\\n+      registry.register(workflow);\\n+      expect(registry.list()).toHaveLength(1);\\n+\\n+      registry.clear();\\n+      expect(registry.list()).toHaveLength(0);\\n+    });\\n+  });\\n+});\\n\",\"status\":\"added\"}],\"outputs\":{}}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"2d22fe8f-df8b-47c8-811d-518fa9bf491d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"2d22fe8f-df8b-47c8-811d-518fa9bf491d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"2d22fe8f-df8b-47c8-811d-518fa9bf491d"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"2d22fe8f-df8b-47c8-811d-518fa9bf491d"}}]}

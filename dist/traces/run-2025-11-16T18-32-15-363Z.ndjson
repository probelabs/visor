{"name":"visor.run","attributes":{"started":true}}
{"name":"visor.run","attributes":{"started":true}}
{"name":"visor.run","attributes":{"started":true}}
{"name":"visor.run","attributes":{"started":true}}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"Init","state_to":"PlanReady","engine_mode":"state-machine","wave":0,"session_id":"c2cdb5d9-184b-4062-b33c-6cb93d2c0c29"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"PlanReady","state_to":"WavePlanning","engine_mode":"state-machine","wave":0,"session_id":"c2cdb5d9-184b-4062-b33c-6cb93d2c0c29"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"c2cdb5d9-184b-4062-b33c-6cb93d2c0c29"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"overview","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"overview","visor.check.input.context":"{\"pr\":{\"number\":0,\"title\":\"Local Analysis: engine-state-machine (45 modified)\",\"author\":\"Test\",\"branch\":\"engine-state-machine\",\"base\":\"main\"},\"files\":[{\"filename\":\".github/workflows/ci.yml\",\"additions\":1,\"deletions\":1,\"changes\":45,\"patch\":\"diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml\\nindex 6b5f7bf0..41a1d75d 100644\\n--- a/.github/workflows/ci.yml\\n+++ b/.github/workflows/ci.yml\\n@@ -58,6 +58,11 @@ jobs:\\n           ./dist/index.js --cli --version\\n           ./dist/index.js --cli --help\\n \\n+      - name: Run scenario tests\\n+        run: npm test -- tests/scenarios/\\n+        env:\\n+          CI: true\\n+\\n       - name: Test package installation\\n         run: |\\n           npm pack\\n@@ -89,41 +94,5 @@ jobs:\\n           comment-on-pr: 'false'\\n           add-reactions: 'false'\\n \\n-  test-scenarios:\\n-    runs-on: ubuntu-latest\\n-    steps:\\n-      - uses: actions/checkout@v4\\n-\\n-      - name: Setup Node.js\\n-        uses: actions/setup-node@v4\\n-        with:\\n-          node-version: '20'\\n-          # cache: 'npm' # Disabled due to rollup optional dependency bug\\n-\\n-      - name: Install dependencies\\n-        run: |\\n-          # Workaround for npm bug with rollup optional dependencies\\n-          # https://github.com/npm/cli/issues/4828\\n-          rm -rf node_modules package-lock.json\\n-          npm install\\n-          # Explicitly install rollup platform-specific binary\\n-          npm install --no-save @rollup/rollup-linux-x64-gnu\\n-\\n-      - name: Build (first to ensure dist/ exists)\\n-        run: npm run build --ignore-scripts\\n-\\n-      - name: Run act scenario tests\\n-        run: npm test -- tests/scenarios/\\n-\\n-      - name: Test PR review functionality (mock)\\n-        uses: ./\\n-        with:\\n-          github-token: ${{ secrets.GITHUB_TOKEN }}\\n-          auto-review: 'false'\\n-          ai-model: 'mock'\\n-          ai-provider: 'mock'\\n-          config-path: '.visor.test.yaml'\\n-          comment-on-pr: 'false'\\n-          add-reactions: 'false'\\n-        env:\\n-          GITHUB_EVENT_NAME: 'repository'\\n+  # Consolidated scenario tests into the main 'test' job to avoid duplication.\\n+  # If you need to split later, prefer a matrix or a reusable workflow.\\n\",\"status\":\"modified\"},{\"filename\":\".github/workflows/sdk-smoke.yml\",\"additions\":1,\"deletions\":1,\"changes\":36,\"patch\":\"diff --git a/.github/workflows/sdk-smoke.yml b/.github/workflows/sdk-smoke.yml\\nindex 2e7fe0a2..970b817d 100644\\n--- a/.github/workflows/sdk-smoke.yml\\n+++ b/.github/workflows/sdk-smoke.yml\\n@@ -17,9 +17,38 @@ jobs:\\n         uses: actions/setup-node@v4\\n         with:\\n           node-version: '20'\\n-\\n-      - name: Install deps\\n-        run: npm ci\\n+          cache: 'npm'\\n+          cache-dependency-path: package-lock.json\\n+\\n+      - name: Configure npm (retries, timeouts)\\n+        run: |\\n+          npm config set fetch-retries 5\\n+          npm config set fetch-retry-maxtimeout 120000\\n+          npm config set fetch-retry-mintimeout 20000\\n+          npm config set fetch-timeout 120000\\n+\\n+      - name: Install deps (retry up to 3x)\\n+        env:\\n+          PUPPETEER_SKIP_DOWNLOAD: '1'\\n+          PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: '1'\\n+          npm_config_audit: 'false'\\n+          npm_config_fund: 'false'\\n+        shell: bash\\n+        run: |\\n+          set -euo pipefail\\n+          attempts=0\\n+          until [ $attempts -ge 3 ]; do\\n+            if npm ci --no-audit --no-fund; then\\n+              break\\n+            fi\\n+            attempts=$((attempts+1))\\n+            echo \\\"npm ci failed (attempt $attempts); retrying in $((attempts*10))s...\\\" >&2\\n+            sleep $((attempts*10))\\n+          done\\n+          if [ $attempts -ge 3 ]; then\\n+            echo \\\"npm ci failed after 3 attempts\\\" >&2\\n+            exit 1\\n+          fi\\n \\n       - name: Build (CLI + SDK)\\n         run: npm run build\\n@@ -29,4 +58,3 @@ jobs:\\n \\n       - name: Run CJS example\\n         run: node examples/sdk-cjs.cjs\\n-\\n\",\"status\":\"modified\"},{\"filename\":\".gitignore\",\"additions\":1,\"deletions\":0,\"changes\":5,\"patch\":\"diff --git a/.gitignore b/.gitignore\\nindex 4b4dbcbd..9c3f9a77 100644\\n--- a/.gitignore\\n+++ b/.gitignore\\n@@ -5,6 +5,10 @@ npm-debug.log*\\n yarn-debug.log*\\n yarn-error.log*\\n \\n+.tmp*\\n+.*out\\n+.*json\\n+\\n # Build outputs\\n dist/\\n # Do not keep legacy ncc bundle\\n@@ -142,3 +146,4 @@ dist/output/**/*.ndjson\\n # Local schema temp and scratch\\n scripts/.schema-tmp/\\n tmp/\\n+.visor-agent-files.json\\n\",\"status\":\"added\"},{\"filename\":\"MANUAL_TESTING.md\",\"additions\":6,\"deletions\":0,\"changes\":194,\"patch\":\"diff --git a/MANUAL_TESTING.md b/MANUAL_TESTING.md\\nnew file mode 100644\\nindex 00000000..3f2eb30a\\n--- /dev/null\\n+++ b/MANUAL_TESTING.md\\n@@ -0,0 +1,194 @@\\n+# Manual Testing for Bash Configuration\\n+\\n+This document explains how to manually validate the bash configuration feature.\\n+\\n+## Quick Start\\n+\\n+```bash\\n+# Set your API key\\n+export ANTHROPIC_API_KEY=\\\"your-key-here\\\"\\n+\\n+# Run the manual tests\\n+npm run test:manual:bash\\n+```\\n+\\n+## What Gets Tested\\n+\\n+The manual tests validate:\\n+\\n+1. **Basic Bash Execution** - `allowBash: true` enables bash commands\\n+2. **Custom Configuration** - `bashConfig` options are passed correctly\\n+3. **Working Directory** - Custom `workingDirectory` is respected\\n+4. **Default Behavior** - Bash is disabled when not configured\\n+\\n+## Expected Output\\n+\\n+When tests pass, you'll see:\\n+\\n+```\\n+‚è≠Ô∏è  Skipping manual tests. Set RUN_MANUAL_TESTS=true to run.\\n+\\n+Bash Configuration Manual Tests\\n+  With API Key\\n+    üìù Testing allowBash: true\\n+    ‚úÖ allowBash test completed\\n+    üìä Result: { ... }\\n+    ‚úì should execute bash commands when allowBash is true (5000ms)\\n+\\n+    üìù Testing allowBash with bashConfig\\n+    ‚úÖ bashConfig test completed\\n+    üìä Result: { ... }\\n+    ‚úì should pass bashConfig options to ProbeAgent (5000ms)\\n+\\n+    üìù Testing bashConfig.workingDirectory\\n+    ‚úÖ workingDirectory test completed\\n+    üìä Result: { ... }\\n+    ‚úì should respect custom working directory (5000ms)\\n+\\n+    üìù Testing without allowBash (default behavior)\\n+    ‚úÖ No bash test completed\\n+    üìä Result: { ... }\\n+    ‚úì should work without bash when allowBash is not set (5000ms)\\n+\\n+  Configuration Validation\\n+    ‚úì should accept allowBash boolean\\n+    ‚úì should accept bashConfig object\\n+    ‚úì should accept both allowBash and bashConfig\\n+```\\n+\\n+## Alternative: Test with Real Configuration\\n+\\n+You can also test with a real Visor configuration file:\\n+\\n+### 1. Create a test configuration\\n+\\n+```yaml\\n+# test-bash-config.yaml\\n+version: \\\"1.0\\\"\\n+\\n+ai_provider: anthropic\\n+ai_model: claude-3-5-sonnet-20241022\\n+\\n+steps:\\n+  bash-test:\\n+    type: ai\\n+    prompt: |\\n+      Please run these bash commands:\\n+      1. echo \\\"Hello from Visor bash config test\\\"\\n+      2. pwd\\n+      3. ls -la\\n+\\n+      Summarize what you found.\\n+    ai:\\n+      allowBash: true\\n+      bashConfig:\\n+        timeout: 10000\\n+    on: [\\\"manual\\\"]\\n+```\\n+\\n+### 2. Run with Visor CLI\\n+\\n+```bash\\n+# Build first\\n+npm run build\\n+\\n+# Run the check\\n+export ANTHROPIC_API_KEY=\\\"your-key-here\\\"\\n+./dist/cli-main.js --config test-bash-config.yaml --check bash-test --event manual\\n+```\\n+\\n+### 3. Expected Output\\n+\\n+You should see the AI agent:\\n+- Successfully execute bash commands\\n+- Return results from `echo`, `pwd`, `ls`\\n+- Provide a summary of findings\\n+\\n+## Testing Different Configurations\\n+\\n+### Test Custom Allow List\\n+\\n+```yaml\\n+ai:\\n+  allowBash: true\\n+  bashConfig:\\n+    allow: ['git status', 'git log --oneline -5']\\n+```\\n+\\n+### Test Working Directory\\n+\\n+```yaml\\n+ai:\\n+  allowBash: true\\n+  bashConfig:\\n+    workingDirectory: '/tmp'\\n+```\\n+\\n+### Test Timeout\\n+\\n+```yaml\\n+ai:\\n+  allowBash: true\\n+  bashConfig:\\n+    timeout: 5000  # 5 seconds\\n+```\\n+\\n+## Troubleshooting\\n+\\n+### Tests are skipped\\n+\\n+Make sure you set `RUN_MANUAL_TESTS=true`:\\n+\\n+```bash\\n+RUN_MANUAL_TESTS=true npm run test:manual:bash\\n+```\\n+\\n+### API key not found\\n+\\n+Set your API key before running:\\n+\\n+```bash\\n+export ANTHROPIC_API_KEY=\\\"your-key-here\\\"\\n+```\\n+\\n+### Bash commands not working\\n+\\n+1. Verify ProbeAgent version supports bash config (>= 0.6.0-rc164)\\n+2. Check that `allowBash: true` is set\\n+3. Verify the command is in the default allow list or your custom `allow` list\\n+4. Check ProbeAgent logs with `debug: true`\\n+\\n+## Cost Considerations\\n+\\n+‚ö†Ô∏è **Warning**: These tests make real API calls to Anthropic and will incur costs. Each test run costs approximately $0.01-0.05 depending on the model and response length.\\n+\\n+To minimize costs:\\n+- Run tests only when needed\\n+- Use a cheaper model for testing (claude-3-haiku)\\n+- Keep prompts concise\\n+\\n+## Debugging\\n+\\n+Enable debug mode to see ProbeAgent interactions:\\n+\\n+```yaml\\n+ai:\\n+  provider: anthropic\\n+  debug: true  # Shows all tool calls and responses\\n+  allowBash: true\\n+```\\n+\\n+This will show:\\n+- Bash commands being executed\\n+- Command outputs\\n+- Tool call traces\\n+- Token usage\\n+\\n+## Next Steps\\n+\\n+After manual validation:\\n+1. Review test results\\n+2. Check ProbeAgent logs\\n+3. Verify bash commands executed correctly\\n+4. Test with your specific use cases\\n+5. Document any edge cases or issues\\n\",\"status\":\"added\"},{\"filename\":\"README.md\",\"additions\":1,\"deletions\":0,\"changes\":1,\"patch\":\"diff --git a/README.md b/README.md\\nindex 978f3e59..41131861 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -124,6 +124,7 @@ Additional guides:\\n - forEach behavior and dependent propagation (including outputs_raw and history precedence): [docs/foreach-dependency-propagation.md](docs/foreach-dependency-propagation.md)\\n - Failure routing and `on_finish` (with outputs_raw in routing JS): [docs/failure-routing.md](docs/failure-routing.md)\\n - timeouts and provider units: [docs/timeouts.md](docs/timeouts.md)\\n+- execution limits (run caps for safety): [docs/limits.md](docs/limits.md)\\n - output formatting limits and truncation controls: [docs/output-formatting.md](docs/output-formatting.md)\\n - live execution visualizer and control API: [docs/debug-visualizer.md](docs/debug-visualizer.md)\\n \\n\",\"status\":\"added\"},{\"filename\":\"defaults/agent-builder.yaml\",\"additions\":19,\"deletions\":0,\"changes\":714,\"patch\":\"diff --git a/defaults/agent-builder.yaml b/defaults/agent-builder.yaml\\nnew file mode 100644\\nindex 00000000..f3ccd29e\\n--- /dev/null\\n+++ b/defaults/agent-builder.yaml\\n@@ -0,0 +1,714 @@\\n+version: \\\"1.0\\\"\\n+\\n+# Agent Build (cwd-first): generate or improve a Visor agent config purely via YAML.\\n+# - Writes to current working directory by default (VISOR_AGENT_OUT_DIR, defaults \\\".\\\").\\n+# - To improve an existing config in-place, run the CLI with: visor build <path/to/agent.yaml>\\n+#   The YAML detects mode by reading the file; missing/empty => new, else improve.\\n+\\n+steps:\\n+  brief:\\n+    type: human-input\\n+    group: agent-build\\n+    on: [manual]\\n+    # no dependencies; can run standalone in manual prompt-only mode\\n+    prompt: |\\n+      Provide a concise brief for the agent to build.\\n+      Include: purpose, events, essential steps, and minimal success criteria.\\n+    placeholder: \\\"e.g., Agent that labels PRs by change size and runs jest\\\"\\n+    multiline: false\\n+    allow_empty: true\\n+    default: \\\"Create a minimal agent that can scaffold other agents (config + tests).\\\"\\n+    on_success:\\n+      goto: draft\\n+      goto_event: schedule\\n+\\n+  start:\\n+    type: log\\n+    group: agent-build\\n+    on: [issue_opened]\\n+    message: \\\"start\\\"\\n+    level: debug\\n+    # no goto; detect-mode runs on event filtering\\n+\\n+  detect-mode:\\n+    type: command\\n+    group: agent-build\\n+    on: [manual, issue_opened, schedule]\\n+    if: \\\"Boolean(env.VISOR_AGENT_PATH || env.VISOR_AGENT_OUT_DIR || env.VISOR_AGENT_MODE == 'improve')\\\"\\n+    exec: echo \\\"{}\\\"\\n+    transform: |\\n+      {% assign mode = 'new' %}\\n+      {% assign exists = false %}\\n+      {% assign empty = true %}\\n+      {% if env.VISOR_AGENT_MODE == 'improve' %}\\n+        {% assign mode = 'improve' %}\\n+      {% else %}\\n+        {% capture cfg %}{% readfile env.VISOR_AGENT_PATH %}{% endcapture %}\\n+        {% assign has_error = cfg contains 'Error reading file' or cfg contains 'Invalid file path' %}\\n+        {% assign trimmed = cfg | strip %}\\n+        {% assign empty = trimmed == '' %}\\n+        {% assign exists = has_error == false %}\\n+        {% if exists and empty == false %}{% assign mode = 'improve' %}{% endif %}\\n+      {% endif %}\\n+      {\\\"path\\\":\\\"{{ env.VISOR_AGENT_PATH }}\\\",\\\"exists\\\": {{ exists }}, \\\"empty\\\": {{ empty }}, \\\"mode\\\": \\\"{{ mode }}\\\"}\\n+    # no on_success routing; downstream steps run based on event + if guards\\n+\\n+  load-existing:\\n+    type: command\\n+    group: agent-build\\n+    depends_on: [detect-mode]\\n+    on: [issue_opened, schedule]\\n+    if: \\\"env.VISOR_AGENT_MODE == 'improve' || (outputs['detect-mode'] && outputs['detect-mode'].mode == 'improve')\\\"\\n+    exec: |\\n+      node <<'NODE'\\n+      const fs=require('fs'); const path=require('path');\\n+      const cfgPath = path.resolve(process.env.VISOR_AGENT_PATH);\\n+      if (!fs.existsSync(cfgPath)) { console.log(JSON.stringify({ ok:false, error:'not_found', path:cfgPath })); process.exit(0); }\\n+      const dir = path.dirname(cfgPath);\\n+      const slug = path.basename(cfgPath).replace(/\\\\.ya?ml$/i,'');\\n+      const testsPath = process.env.VISOR_AGENT_TESTS_PATH ? path.resolve(process.env.VISOR_AGENT_TESTS_PATH) : path.join(dir, slug + '.tests.yaml');\\n+      let cfgText = ''; let testsText = '';\\n+      try { cfgText = fs.readFileSync(cfgPath,'utf8'); } catch {}\\n+      try { testsText = fs.readFileSync(testsPath,'utf8'); } catch {}\\n+      const files = [];\\n+      if (cfgText) files.push({ path: cfgPath, content: cfgText });\\n+      if (testsText) files.push({ path: testsPath, content: testsText });\\n+      console.log(JSON.stringify({ ok:true, slug, config_path: cfgPath, tests_path: testsPath, files }));\\n+      NODE\\n+    output_format: json\\n+    on_success:\\n+      goto: write\\n+      goto_event: schedule\\n+\\n+  draft:\\n+    type: ai\\n+    group: agent-build\\n+    on: [issue_opened, schedule]\\n+    if: \\\"String((env.VISOR_AGENT_MODE || 'new')).toLowerCase() !== 'improve'\\\"\\n+    ai_prompt_type: engineer\\n+    ai:\\n+      provider: mock\\n+      skip_code_context: true\\n+    schema:\\n+      type: object\\n+      properties:\\n+        slug: { type: string }\\n+        summary: { type: string }\\n+        files:\\n+          type: array\\n+          items:\\n+            type: object\\n+            properties: { path: { type: string }, content: { type: string } }\\n+            required: [path, content]\\n+      required: [slug, files]\\n+    prompt: |\\n+      Generate a new Visor agent with YAML config and YAML tests only. Keep it simple and tailored to this repository.\\n+\\n+      Brief:\\n+      {{ outputs['brief'] | default: 'Create a minimal agent that can scaffold other agents (config + tests).' }}\\n+\\n+      Return JSON fields: slug, summary, files[] (path+content). Only content matters; paths will be normalized to the target path.\\n+      Include two files: <slug>.yaml and <slug>.tests.yaml.\\n+    on_success:\\n+      goto: write\\n+      goto_event: schedule\\n+\\n+  write:\\n+    type: command\\n+    group: agent-build\\n+    # Triggered only via goto from draft/refine/load-existing to avoid early evaluation\\n+    on: [issue_opened, schedule]\\n+    if: \\\"((((outputs||{})['refine']||{}).files) || (((outputs||{})['draft']||{}).files) || (((outputs||{})['load-existing']||{}).files)) && (env.VISOR_AGENT_PATH || env.VISOR_AGENT_OUT_DIR)\\\"\\n+    exec: |\\n+      {% assign set = outputs['refine'].files | default: outputs['draft'].files | default: outputs['load-existing'].files %}\\n+      cat > .visor-agent-files.json <<'JSON'\\n+      {{ set | to_json | default: '[]' }}\\n+      JSON\\n+      node <<'NODE'\\n+      const fs = require('fs');\\n+      const path = require('path');\\n+      const yaml = require('js-yaml');\\n+      // Read AI/refine results\\n+      let files = [];\\n+      try { files = JSON.parse(fs.readFileSync('.visor-agent-files.json','utf8')) || []; } catch {}\\n+      // Destination path (single co-located file)\\n+      const targetPath = process.env.VISOR_AGENT_PATH ? path.resolve(process.env.VISOR_AGENT_PATH) : path.resolve(String(process.env.VISOR_AGENT_OUT_DIR||'.'), 'agent.yaml');\\n+      const ensureObj = (v) => (v && typeof v === 'object' ? v : {});\\n+      const loadYaml = (t) => { try { return ensureObj(yaml.load(String(t||''))); } catch { return {}; } };\\n+      let configDoc = {};\\n+      let testsDoc = {};\\n+      for (const f of files) {\\n+        const doc = loadYaml(f.content);\\n+        if (doc && typeof doc === 'object') {\\n+          const p = String((f && f.path) || '');\\n+          const looksLikeTests = /\\\\.tests\\\\.ya?ml$/i.test(p) || ('cases' in doc) || ('defaults' in doc);\\n+          if (doc.tests && typeof doc.tests === 'object') {\\n+            testsDoc = doc;\\n+          } else if (looksLikeTests && Object.keys(testsDoc).length === 0) {\\n+            // Treat as tests source even without top-level 'tests'\\n+            testsDoc = doc;\\n+          } else if (!configDoc || Object.keys(configDoc).length === 0) {\\n+            configDoc = doc;\\n+          }\\n+        }\\n+      }\\n+      const finalDoc = {};\\n+      const cfg = ensureObj(configDoc);\\n+      // Copy all non-tests top-level keys from config\\n+      for (const k of Object.keys(cfg)) { if (k !== 'tests' && k !== 'extends') finalDoc[k] = cfg[k]; }\\n+      if (!finalDoc.version) finalDoc.version = '1.0';\\n+      // If no steps provided in config, create a minimal hello step as a sane fallback\\n+      if (!finalDoc.steps || typeof finalDoc.steps !== 'object' || Object.keys(finalDoc.steps).length === 0) {\\n+        finalDoc.steps = {\\n+          hello: { type: 'log', message: 'hello from agent-builder', level: 'info', on: ['issue_opened'] }\\n+        };\\n+      }\\n+      // Merge tests:\\n+      // - If testsDoc.tests is an object ‚Üí use it (valid path)\\n+      // - Else if cfg.tests is an object ‚Üí use it\\n+      // - Else if a tests file exists but is malformed ‚Üí preserve AS-IS (to let tests-validate fail)\\n+      // - Else (no tests provided at all) ‚Üí create a minimal valid tests block\\n+      let testsBlock = undefined;\\n+      if (testsDoc && typeof testsDoc.tests === 'object') {\\n+        testsBlock = testsDoc.tests;\\n+      } else if (cfg.tests && typeof cfg.tests === 'object') {\\n+        testsBlock = cfg.tests;\\n+      } else if (testsDoc && Object.keys(testsDoc).length > 0) {\\n+        // Preserve malformed shape to trigger validator failure in the first pass\\n+        // Examples: testsDoc = { version, extends, cases: [...] } (no top-level tests)\\n+        testsBlock = testsDoc as any;\\n+      } else {\\n+        testsBlock = {\\n+          defaults: { strict: true, ai_provider: 'mock' },\\n+          cases: [ { name: 'smoke', event: 'issue_opened', fixture: 'gh.issue_open.minimal', expect: {} } ],\\n+        };\\n+      }\\n+      finalDoc.tests = testsBlock;\\n+      // Write one file only\\n+      fs.mkdirSync(path.dirname(targetPath), { recursive: true });\\n+      fs.writeFileSync(targetPath, yaml.dump(finalDoc), 'utf8');\\n+      console.log(JSON.stringify({ written: [targetPath], slug: (cfg.slug || 'agent'), config_path: targetPath, tests_path: targetPath, mode: '{{ outputs['detect-mode'].mode | default: 'new' }}' }));\\n+      NODE\\n+    output_format: json\\n+\\n+  config-lint:\\n+    type: command\\n+    group: agent-quality\\n+    depends_on: [write]\\n+    on: [issue_opened, schedule]\\n+    if: \\\"((outputs||{})['write'])\\\"\\n+    exec: |\\n+      node dist/index.js validate --config {{ outputs['write'].config_path }}\\n+    output_format: text\\n+    # rely on provider exit code / issues, avoid double-routing via fail_if\\n+    fail_if: \\\"false\\\"\\n+    on_fail:\\n+      goto: refine\\n+      goto_event: schedule\\n+\\n+  tests-validate:\\n+    type: command\\n+    group: agent-quality\\n+    depends_on: [write]\\n+    on: [issue_opened, schedule]\\n+    if: \\\"((outputs||{})['write'])\\\"\\n+    exec: |\\n+      node <<'NODE'\\n+      const fs=require('fs'); const path=require('path'); const yaml=require('js-yaml');\\n+      const p = path.resolve('{{ outputs['write'].tests_path }}');\\n+      let text=''; try { text = fs.readFileSync(p,'utf8'); } catch { console.log('read_error'); process.exit(2); }\\n+      let doc; try { doc = yaml.load(text) || {}; } catch { console.log('parse_error'); process.exit(3); }\\n+      if (typeof doc !== 'object' || !doc) { console.log('doc_type_error'); process.exit(4); }\\n+      const t = doc.tests;\\n+      if (!t || typeof t !== 'object') { console.log('tests_missing_or_not_object'); process.exit(5); }\\n+      const allowed = new Set(['defaults','fixtures','cases']);\\n+      for (const k of Object.keys(t)) if (!allowed.has(k)) { console.log('tests_has_additional_properties'); process.exit(6); }\\n+      if (!Array.isArray(t.cases)) { console.log('cases_missing'); process.exit(7); }\\n+      console.log('valid');\\n+      NODE\\n+    output_format: text\\n+    fail_if: \\\"output && output.trim() !== 'valid'\\\"\\n+    on_fail:\\n+      goto: refine\\n+      goto_event: schedule\\n+\\n+  verify-tests:\\n+    type: command\\n+    group: agent-quality\\n+    depends_on: [write, config-lint, tests-validate]\\n+    on: [issue_opened, schedule]\\n+    if: \\\"((outputs||{})['write']) && !(String(((outputs||{})['tests-validate']||'')).toLowerCase().includes('failed'))\\\"\\n+    exec: |\\n+      node dist/index.js test --config {{ outputs['write'].tests_path }} --json - --bail\\n+    output_format: json\\n+    fail_if: \\\"output && Number(output.failures||0) > 0\\\"\\n+    on_fail:\\n+      goto: refine\\n+      goto_event: schedule\\n+    env:\\n+      VISOR_TEST_SUMMARY_SILENT: \\\"true\\\"\\n+    # no on_success routing; dependents will be scheduled by the initial goto to 'write'\\n+\\n+  code-review:\\n+    type: ai\\n+    group: agent-quality\\n+    depends_on: [verify-tests]\\n+    on: [issue_opened, schedule]\\n+    if: \\\"((outputs||{})['write'])\\\"\\n+    ai_prompt_type: engineer\\n+    ai:\\n+      provider: mock\\n+      skip_code_context: true\\n+    schema:\\n+      type: object\\n+      properties:\\n+        ok: { type: 'boolean' }\\n+        feedback: { type: 'string' }\\n+        concerns: { type: 'array', items: { type: 'string' } }\\n+      required: [ok]\\n+    prompt: |\\n+      Validate the generated agent against the brief and its tests. Ensure it:\\n+      - Directly addresses the brief without over-complication.\\n+      - Includes tests that cover success and failure cases.\\n+\\n+      Brief:\\n+      {{ outputs['brief'] | default: 'Create a minimal agent that can scaffold other agents (config + tests).' }}\\n+\\n+      Tests validate (text):\\n+      {{ outputs['tests-validate'] }}\\n+\\n+      Tests run (JSON):\\n+      {{ outputs['verify-tests'] | json }}\\n+\\n+      Respond with JSON: { ok: boolean, feedback: string, concerns?: string[] }.\\n+    fail_if: \\\"output && output.ok === false\\\"\\n+    on_fail:\\n+      goto: refine\\n+    # no on_success routing; dependents will be scheduled by the initial goto to 'write'\\n+\\n+  refine:\\n+    type: ai\\n+    group: agent-build\\n+    reuse_ai_session: draft\\n+    session_mode: append\\n+    on: [schedule]\\n+    ai_prompt_type: engineer\\n+    schema:\\n+      type: object\\n+      properties:\\n+        files:\\n+          type: array\\n+          items:\\n+            type: object\\n+            properties: { path: { type: string }, content: { type: string } }\\n+            required: [path, content]\\n+      required: [files]\\n+    prompt: |\\n+      Refine the files so that lint and tests pass. Keep changes focused and minimal.\\n+\\n+      Brief:\\n+      {{ outputs['brief'] }}\\n+\\n+      Config validate (text):\\n+      {{ outputs['config-lint'] }}\\n+\\n+      Tests validate (text):\\n+      {{ outputs['tests-validate'] }}\\n+\\n+      Tests run (JSON):\\n+      {{ outputs['verify-tests'] | json }}\\n+    on_success:\\n+      goto: write\\n+\\n+  cleanup:\\n+    type: command\\n+    group: agent-quality\\n+    depends_on: [code-review]\\n+    on: [issue_opened, schedule]\\n+    if: \\\"((outputs||{})['write'])\\\"\\n+    exec: |\\n+      node <<'NODE'\\n+      const fs=require('fs');\\n+      const written = (process.env.WRITTEN && process.env.WRITTEN.split('\\\\n').filter(Boolean)) || [];\\n+      const force = String(process.env.VISOR_AGENT_CLEANUP_FORCE||'').toLowerCase()==='true';\\n+      const removed=[];\\n+      for (const f of written){ const isTmp = /(^|\\\\/)tmp\\\\//.test(f); if (isTmp || force) { try{ if(fs.existsSync(f)){ fs.unlinkSync(f); removed.push(f);} }catch{} } }\\n+      try{ if(fs.existsSync('.visor-agent-files.json')) fs.unlinkSync('.visor-agent-files.json'); }catch{}\\n+      console.log(JSON.stringify({ removed }));\\n+      NODE\\n+    output_format: json\\n+    # finish depends_on cleanup; it will run automatically in the same forward-run\\n+\\n+  finish:\\n+    type: log\\n+    group: agent-build\\n+    on: [issue_opened, schedule]\\n+    depends_on: [cleanup]\\n+    message: \\\"‚úÖ Agent build finished: files written, linted, tested, and reviewed.\\\"\\n+    level: info\\n+    on_success:\\n+      goto_js: |\\n+        const enabled = String(env.VISOR_CHAT_MODE||'').toLowerCase()==='true';\\n+        if (!enabled) return null;\\n+        const max = Number(env.VISOR_CHAT_MAX_LOOPS||1);\\n+        const count = (outputs_history['brief']||[]).length|0;\\n+        return count < max ? 'detect-mode' : null;\\n+      goto_event: manual\\n+\\n+tests:\\n+  defaults:\\n+    strict: true\\n+    ai_provider: mock\\n+    fail_on_unexpected_calls: true\\n+\\n+  cases:\\n+    - name: new-write-to-cwd\\n+      description: Generates to output/ (cwd) and cleans up forcibly.\\n+      event: issue_opened\\n+      fixture: gh.issue_open.minimal\\n+      mocks:\\n+        draft:\\n+          slug: helloagent\\n+          files:\\n+            - path: helloagent.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                output:\\n+                  pr_comment:\\n+                    format: markdown\\n+                    group_by: check\\n+                    collapse: true\\n+                steps:\\n+                  hello:\\n+                    type: log\\n+                    message: \\\"hi\\\"\\n+                    level: info\\n+                    on: [issue_opened]\\n+            - path: helloagent.tests.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                extends: \\\"helloagent.yaml\\\"\\n+                tests:\\n+                  defaults: { strict: true, ai_provider: mock }\\n+                  cases:\\n+                    - name: ok\\n+                      event: issue_opened\\n+                      fixture: gh.issue_open.minimal\\n+                      expect:\\n+                        calls:\\n+                          - step: hello\\n+                            exactly: 1\\n+        verify-tests:\\n+          failures: 0\\n+          results:\\n+            - name: ok\\n+              passed: true\\n+        code-review:\\n+          ok: true\\n+          feedback: \\\"LGTM\\\"\\n+        cleanup:\\n+          removed: []\\n+      expect:\\n+        calls:\\n+          - step: start\\n+            exactly: 1\\n+          - step: detect-mode\\n+            exactly: 1\\n+          - step: draft\\n+            exactly: 1\\n+          - step: write\\n+            exactly: 1\\n+          - step: config-lint\\n+            exactly: 1\\n+          - step: tests-validate\\n+            exactly: 1\\n+          - step: verify-tests\\n+            exactly: 1\\n+          - step: code-review\\n+            exactly: 1\\n+          - step: cleanup\\n+            exactly: 1\\n+          - step: finish\\n+            exactly: 1\\n+        prompts:\\n+          - step: draft\\n+            index: last\\n+            not_contains:\\n+              - \\\"<pull_request>\\\"\\n+              - \\\"<files_summary>\\\"\\n+      env:\\n+        VISOR_AGENT_OUT_DIR: \\\"output\\\"\\n+        VISOR_AGENT_CLEANUP_FORCE: \\\"true\\\"\\n+\\n+    - name: improve-existing\\n+      description: Improves an existing config in-place using load-existing + refine.\\n+      event: issue_opened\\n+      fixture: gh.issue_open.minimal\\n+      mocks:\\n+        detect-mode:\\n+          path: \\\"tmp/improv.yaml\\\"\\n+          exists: true\\n+          empty: false\\n+          mode: \\\"improve\\\"\\n+        load-existing:\\n+          ok: true\\n+          slug: improv\\n+          config_path: tmp/improv.yaml\\n+          tests_path: tmp/improv.tests.yaml\\n+          files:\\n+            - path: tmp/improv.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                output:\\n+                  pr_comment:\\n+                    format: markdown\\n+                    group_by: check\\n+                    collapse: true\\n+                steps:\\n+                  hello:\\n+                    type: log\\n+                    message: \\\"hello\\\"\\n+                    level: info\\n+                    on: [issue_opened]\\n+            - path: tmp/improv.tests.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                extends: \\\"improv.yaml\\\"\\n+                tests:\\n+                  defaults: { strict: true, ai_provider: mock }\\n+                  cases:\\n+                    - name: failing\\n+                      event: issue_opened\\n+                      fixture: gh.issue_open.minimal\\n+                      expect:\\n+                        calls:\\n+                          - step: hello\\n+                            exactly: 2\\n+        refine:\\n+          files:\\n+            - path: tmp/improv.tests.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                extends: \\\"improv.yaml\\\"\\n+                tests:\\n+                  defaults: { strict: true, ai_provider: mock }\\n+                  cases:\\n+                    - name: fixed\\n+                      event: issue_opened\\n+                      fixture: gh.issue_open.minimal\\n+                      expect:\\n+                        calls:\\n+                          - step: hello\\n+                            exactly: 1\\n+        verify-tests:\\n+          failures: 0\\n+          results:\\n+            - name: fixed\\n+              passed: true\\n+        code-review:\\n+          ok: true\\n+          feedback: \\\"Refined existing\\\"\\n+      expect:\\n+        calls:\\n+          - step: start\\n+            exactly: 1\\n+          - step: detect-mode\\n+            exactly: 1\\n+          - step: load-existing\\n+            exactly: 1\\n+          - step: write\\n+            exactly: 1\\n+          - step: config-lint\\n+            exactly: 1\\n+          - step: tests-validate\\n+            exactly: 1\\n+          - step: verify-tests\\n+            exactly: 1\\n+          - step: code-review\\n+            exactly: 1\\n+          - step: cleanup\\n+            exactly: 1\\n+          - step: finish\\n+            exactly: 1\\n+      env:\\n+        VISOR_AGENT_MODE: \\\"improve\\\"\\n+        VISOR_AGENT_OUT_DIR: \\\"tmp\\\"\\n+        VISOR_AGENT_PATH: \\\"tmp/improv.yaml\\\"\\n+        VISOR_AGENT_TESTS_PATH: \\\"tmp/improv.tests.yaml\\\"\\n+\\n+    - name: loop-on-tests-validate\\n+      description: tests-validate fails first, refine fixes, loop passes.\\n+      event: issue_opened\\n+      fixture: gh.issue_open.minimal\\n+      mocks:\\n+        draft:\\n+          slug: tval\\n+          files:\\n+            - path: tval.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                output:\\n+                  pr_comment:\\n+                    format: markdown\\n+                    group_by: check\\n+                    collapse: true\\n+                steps:\\n+                  hello:\\n+                    type: log\\n+                    message: \\\"hi\\\"\\n+                    level: info\\n+                    on: [issue_opened]\\n+            - path: tval.tests.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                extends: \\\"tval.yaml\\\"\\n+                # wrong shape on purpose\\n+                cases:\\n+                  - name: wrong-shape\\n+        refine:\\n+          files:\\n+            - path: tval.tests.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                extends: \\\"tval.yaml\\\"\\n+                tests:\\n+                  defaults: { strict: true, ai_provider: mock }\\n+                  cases:\\n+                    - name: ok\\n+                      event: issue_opened\\n+                      fixture: gh.issue_open.minimal\\n+                      expect: {}\\n+        verify-tests:\\n+          failures: 0\\n+          results:\\n+            - name: ok\\n+              passed: true\\n+        code-review:\\n+          ok: true\\n+          feedback: \\\"OK\\\"\\n+      expect:\\n+        calls:\\n+          - step: start\\n+            exactly: 1\\n+          - step: detect-mode\\n+            exactly: 1\\n+          - step: draft\\n+            exactly: 1\\n+          - step: refine\\n+            exactly: 1\\n+          - step: write\\n+            exactly: 2\\n+          - step: config-lint\\n+            exactly: 2\\n+          - step: tests-validate\\n+            exactly: 2\\n+          - step: verify-tests\\n+            exactly: 1\\n+          - step: code-review\\n+            exactly: 1\\n+          - step: cleanup\\n+            exactly: 1\\n+          - step: finish\\n+            exactly: 1\\n+      env:\\n+        VISOR_AGENT_OUT_DIR: \\\"tmp\\\"\\n+\\n+    - name: chat-loop-manual\\n+      description: Chat loop returns to brief once, then stops.\\n+      event: manual\\n+      fixture: local.minimal\\n+      mocks:\\n+        detect-mode:\\n+          path: \\\"tmp/nonexistent.yaml\\\"\\n+          exists: false\\n+          empty: true\\n+          mode: \\\"new\\\"\\n+        brief[]:\\n+          - \\\"first brief\\\"\\n+          - \\\"second brief\\\"\\n+        draft:\\n+          slug: chatg\\n+          files:\\n+            - path: chatg.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                output:\\n+                  pr_comment:\\n+                    format: markdown\\n+                    group_by: check\\n+                    collapse: true\\n+                steps:\\n+                  hello:\\n+                    type: log\\n+                    message: \\\"hi\\\"\\n+                    level: info\\n+                    on: [manual, issue_opened]\\n+            - path: chatg.tests.yaml\\n+              content: |\\n+                version: \\\"1.0\\\"\\n+                extends: \\\"chatg.yaml\\\"\\n+                tests:\\n+                  defaults: { strict: true, ai_provider: mock }\\n+                  cases:\\n+                    - name: ok\\n+                      event: manual\\n+                      fixture: local.minimal\\n+                      expect:\\n+                        calls:\\n+                          - step: hello\\n+                            exactly: 1\\n+        verify-tests:\\n+          failures: 0\\n+          results:\\n+            - name: ok\\n+              passed: true\\n+        code-review:\\n+          ok: true\\n+          feedback: \\\"LGTM\\\"\\n+      expect:\\n+        calls:\\n+          - step: brief\\n+            at_least: 1\\n+          - step: detect-mode\\n+            at_least: 1\\n+          - step: draft\\n+            at_least: 1\\n+          - step: write\\n+            at_least: 1\\n+          - step: config-lint\\n+            at_least: 1\\n+          - step: tests-validate\\n+            at_least: 1\\n+          - step: verify-tests\\n+            at_least: 1\\n+          - step: code-review\\n+            at_least: 1\\n+          - step: cleanup\\n+            at_least: 1\\n+          - step: finish\\n+            at_least: 1\\n+      env:\\n+        VISOR_AGENT_OUT_DIR: \\\"tmp\\\"\\n+        VISOR_AGENT_PATH: \\\"tmp/nonexistent.yaml\\\"\\n+        VISOR_CHAT_MODE: \\\"true\\\"\\n+        VISOR_CHAT_MAX_LOOPS: \\\"1\\\"\\n+        VISOR_TEST_MODE: \\\"true\\\"\\n+\\n+    - name: prompt-never-includes-context-even-when-allowed\\n+      description: Even if tests allow code context, draft disables it at step level.\\n+      event: manual\\n+      fixture: local.minimal\\n+      expect:\\n+        calls:\\n+          - step: brief\\n+            exactly: 1\\n+          - step: draft\\n+            exactly: 1\\n+        prompts:\\n+          - step: draft\\n+            index: last\\n+            not_contains:\\n+              - \\\"<pull_request>\\\"\\n+              - \\\"<files_summary>\\\"\\n+      env:\\n+        VISOR_TEST_ALLOW_CODE_CONTEXT: \\\"true\\\"\\n\",\"status\":\"added\"},{\"filename\":\"defaults/task-refinement.yaml\",\"additions\":16,\"deletions\":0,\"changes\":599,\"patch\":\"diff --git a/defaults/task-refinement.yaml b/defaults/task-refinement.yaml\\nnew file mode 100644\\nindex 00000000..72768491\\n--- /dev/null\\n+++ b/defaults/task-refinement.yaml\\n@@ -0,0 +1,599 @@\\n+version: \\\"1.0\\\"\\n+\\n+# Simple agent: task-refinement\\n+# - Collects user input, refines it with AI (skip code context), and loops until refined.\\n+# - Returns final refined text via the `finish` step output.\\n+\\n+steps:\\n+  ask:\\n+    type: human-input\\n+    group: task-refinement\\n+    if: \\\"!(outputs && outputs['ask'])\\\"\\n+    # No explicit event trigger; run in CLI by default but guard via if\\n+    prompt: |\\n+      {% assign last_refine = outputs_history.refine | last %}\\n+      {% if last_refine and last_refine.refined == false %}\\n+      {{ last_refine.text }}\\n+      {% else %}\\n+      Provide the task you want to accomplish. Be specific about constraints\\n+      (inputs, outputs, environment, success criteria).\\n+      {% endif %}\\n+    multiline: false\\n+    allow_empty: false\\n+    # No on_success.goto required ‚Äî refine depends_on ask\\n+\\n+  refine:\\n+    type: ai\\n+    group: task-refinement\\n+    # Run only after 'ask' ‚Äî dependency drives ordering\\n+    depends_on: [ask]\\n+    # Execute once per run; forward-run loops do not need to re-run refine\\n+    if: \\\"!(outputs && outputs['refine'])\\\"\\n+    ai:\\n+      skip_code_context: true\\n+      disableTools: true\\n+      system_prompt: |\\n+        You are a helpful, precise task refinement assistant (role: requirements-analyst).\\n+        Your goal is to get to an agreed, testable task definition and clear acceptance criteria.\\n+        - Refine the user's task into an unambiguous, executable description with minimal assumptions.\\n+        - Define how correctness will be validated (objective success criteria and measurables).\\n+        - If information is missing, set refined=false and ask_user=true and put a single, specific\\n+          clarification question in the \\\"text\\\" field (one question at a time).\\n+        - If everything is sufficient, set refined=true and put the final refined wording in \\\"text\\\".\\n+        - Be succinct and concrete. Prefer measurable outcomes over vague phrasing.\\n+        - You can't mark plan as refined until explicit user confirmation/agreement is implied.\\n+    # Schema ensures the agent either finalizes or asks to clarify\\n+    schema:\\n+      type: object\\n+      additionalProperties: false\\n+      properties:\\n+        refined: { type: boolean, description: \\\"true if the task is fully specified and accepted\\\" }\\n+        text: { type: string, description: \\\"final refined task or question to user\\\" }\\n+      required: [refined, text]\\n+    prompt: |\\n+      <history>\\n+        {% assign umsgs = outputs_history.ask | default: [] %}\\n+        {% assign amsgs = outputs_history.refine | default: [] %}\\n+        {% assign merged = umsgs | concat: amsgs | sort: 'ts' %}\\n+        {% for m in merged %}\\n+          {% if m.refined != nil %}\\n+            <assistant>{{ m.text }}</assistant>\\n+          {% else %}\\n+            <user>{{ m.text }}</user>\\n+          {% endif %}\\n+        {% endfor %}\\n+      </history>\\n+\\n+      <input>\\n+        {{ outputs['ask'].text }}\\n+      </input>\\n+\\n+    # Loop control using fail_if + on_fail (no goto_js)\\n+    fail_if: \\\"output['refined'] !== true\\\"\\n+    on_fail:\\n+      goto: ask\\n+\\n+  plan-commands:\\n+    type: ai\\n+    group: task-refinement\\n+    depends_on: [refine]\\n+    reuse_ai_session: refine\\n+    session_mode: append\\n+    ai:\\n+      # Allow tools so the model can inspect the repo context if needed\\n+      disableTools: false\\n+      skip_code_context: true\\n+      system_prompt: |\\n+        You are a Task Validation Planner.\\n+        Produce a deterministic, minimal list of shell commands that, when executed in order,\\n+        validate that the refined task is complete for THIS repository/local env.\\n+        Constraints:\\n+        - Commands must be safe and non-destructive. Do not delete files or push to remotes.\\n+        - Prefer read-only checks and standard project commands (build, lint, test) when present.\\n+        - Each item is a single shell line; you may use && or || inside a line if necessary.\\n+        - Favor idempotent commands. Avoid interactive prompts.\\n+        - Detect package manager/tooling pragmatically (npm/pnpm/yarn/bun, eslint/biome, jest/vitest, etc.).\\n+        Return JSON with an array of strings under \\\"commands\\\" and optional \\\"notes\\\".\\n+    schema:\\n+      type: object\\n+      additionalProperties: false\\n+      properties:\\n+        commands:\\n+          type: array\\n+          minItems: 1\\n+          items: { type: string, minLength: 1 }\\n+        notes: { type: string }\\n+      required: [commands]\\n+    prompt: |\\n+      Refined task:\\n+      {{ outputs['refine'].text }}\\n+\\n+      If the repository has build/lint/test, include them. Otherwise propose basic checks that still\\n+      demonstrate completion (e.g., typecheck, compile, format verification, smoke run).\\n+\\n+      {% assign prev_conf_hist = outputs_history['confirm-interpret'] | default: [] %}\\n+      {% assign prev_run_hist = outputs_history['run-commands'] | default: [] %}\\n+      {% assign last_failed = nil %}\\n+      {% for r in prev_run_hist %}\\n+        {% if r.failed and r.failed > 0 %}\\n+          {% assign last_failed = r %}\\n+        {% endif %}\\n+      {% endfor %}\\n+      {% assign last_conf = prev_conf_hist | last %}\\n+      {% assign prev_run_count = prev_run_hist | size %}\\n+      {% assign last_run = prev_run_hist | last %}\\n+      {% if last_failed or prev_run_count > 0 %}\\n+      Previous attempt failed. Here are the details to learn from:\\n+      - Previous commands: {{ last_conf.commands | to_json }}\\n+      - Run results: {{ last_failed | default: last_run | to_json }}\\n+      Please revise the commands to address failures. Keep the list minimal and deterministic.\\n+      {% endif %}\\n+\\n+      Output strictly JSON per schema. No prose around it.\\n+    # No on_success routing needed; dependents naturally follow in the DAG\\n+\\n+  ask-confirm:\\n+    type: human-input\\n+    group: task-refinement\\n+    depends_on: [plan-commands]\\n+    prompt: |\\n+      Here is the proposed validation command list (to run sequentially):\\n+      {% for c in outputs['plan-commands'].commands %}\\n+      {{ forloop.index }}. {{ c }}\\n+      {% endfor %}\\n+\\n+      Confirm running these? Reply \\\"yes\\\" to accept, or provide edits:\\n+      - JSON array of commands, e.g. [\\\"npm ci\\\", \\\"npm test\\\"], or\\n+      - Plain text with one command per line.\\n+    placeholder: \\\"yes | or paste modified list...\\\"\\n+    multiline: true\\n+    allow_empty: true\\n+    default: \\\"yes\\\"\\n+\\n+  confirm-interpret:\\n+    type: ai\\n+    group: task-refinement\\n+    depends_on: [plan-commands, ask-confirm]\\n+    ai:\\n+      skip_code_context: true\\n+      disableTools: true\\n+      system_prompt: |\\n+        You are a confirmation interpreter. Given the planned command list and the user's reply,\\n+        decide whether to proceed to execution with a normalized list of shell commands, or\\n+        return for replanning.\\n+        - If the user says yes/approve, set proceed=true and provide commands as-is.\\n+        - If the user supplied edits (JSON array or one per line), parse, trim, dedupe, and set proceed=true with commands.\\n+        - If the reply indicates high-level changes (not runnable commands), set proceed=false and include a short reason.\\n+        Output strictly JSON per schema.\\n+    schema:\\n+      type: object\\n+      additionalProperties: false\\n+      properties:\\n+        proceed: { type: boolean }\\n+        commands:\\n+          type: array\\n+          items: { type: string, minLength: 1 }\\n+        reason: { type: string }\\n+      required: [proceed]\\n+    prompt: |\\n+      Planned commands:\\n+      {{ outputs['plan-commands'].commands | to_json }}\\n+\\n+      User reply:\\n+      {{ outputs['ask-confirm'].text }}\\n+\\n+      Return JSON per schema. If proceed=true, commands must be a non-empty array of shell lines.\\n+    fail_if: \\\"output && output.proceed !== true\\\"\\n+    on_fail:\\n+      goto: plan-commands\\n+\\n+  run-commands:\\n+    type: command\\n+    group: task-refinement\\n+    depends_on: [confirm-interpret]\\n+    exec: |\\n+      node <<'NODE'\\n+      const { spawn } = require('child_process');\\n+      const cmds = {{ outputs['confirm-interpret'] | to_json }}.commands || [];\\n+      const results = [];\\n+      const runOne = (cmd) => new Promise((resolve) => {\\n+        const child = spawn('bash', ['-lc', cmd], { stdio: ['ignore', 'pipe', 'pipe'] });\\n+        let out = '', err = '';\\n+        const started = Date.now();\\n+        child.stdout.on('data', d => (out += d.toString()));\\n+        child.stderr.on('data', d => (err += d.toString()));\\n+        child.on('close', code => {\\n+          results.push({ cmd, code, stdout: out, stderr: err, durationMs: Date.now() - started });\\n+          resolve();\\n+        });\\n+      });\\n+      (async () => {\\n+        for (const c of cmds) { await runOne(c); }\\n+        const failed = results.filter(r => Number(r.code||0) !== 0).length;\\n+        process.stdout.write(JSON.stringify({ failed, results }));\\n+      })().catch(e => { process.stdout.write(JSON.stringify({ failed: 1, error: String(e) })); process.exit(0); });\\n+      NODE\\n+    output_format: json\\n+    fail_if: \\\"output && Number(output.failed||0) > 0\\\"\\n+    on_fail:\\n+      goto: plan-commands\\n+\\n+  finish:\\n+    type: log\\n+    group: task-refinement\\n+    depends_on: [run-commands]\\n+    if: \\\"(outputs && outputs['run-commands'] && Number((outputs['run-commands'].failed||0)) === 0) && !(outputs && outputs['finish'])\\\"\\n+    message: |\\n+      ‚úÖ Refined Task:\\n+      {{ outputs['refine'].text }}\\n+\\n+      ‚úÖ Validation commands (final):\\n+      {% for c in outputs['confirm-interpret'].commands %}\\n+      - {{ c }}\\n+      {% endfor %}\\n+    level: info\\n+    include_pr_context: false\\n+    include_dependencies: false\\n+    include_metadata: false\\n+\\n+tests:\\n+  defaults:\\n+    strict: true\\n+    ai_provider: mock\\n+  cases:\\n+    - name: one-pass-refinement\\n+      description: Single turn; AI is happy and returns refined text.\\n+      event: manual\\n+      fixture: local.minimal\\n+      mocks:\\n+        ask: \\\"Build a small Node CLI that prints \\\\\\\"hello\\\\\\\"\\\"\\n+        refine:\\n+          refined: true\\n+          text: \\\"Create a Node.js CLI (using Node >=18) that prints 'hello' when run; include usage example and exit code 0.\\\"\\n+        plan-commands:\\n+          commands: [\\\"echo hello-build\\\", \\\"echo hello-lint\\\", \\\"echo hello-test\\\"]\\n+        ask-confirm: \\\"yes\\\"\\n+        confirm-interpret:\\n+          proceed: true\\n+          commands: [\\\"echo hello-build\\\",\\\"echo hello-lint\\\",\\\"echo hello-test\\\"]\\n+        run-commands:\\n+          stdout: '{\\\"failed\\\":0,\\\"results\\\":[{\\\"cmd\\\":\\\"echo hello-build\\\",\\\"code\\\":0},{\\\"cmd\\\":\\\"echo hello-lint\\\",\\\"code\\\":0},{\\\"cmd\\\":\\\"echo hello-test\\\",\\\"code\\\":0}]}'\\n+      expect:\\n+        calls:\\n+          - step: plan-commands\\n+            exactly: 1\\n+          - step: ask-confirm\\n+            exactly: 1\\n+          - step: confirm-interpret\\n+            exactly: 1\\n+          - step: run-commands\\n+            exactly: 1\\n+          - step: ask\\n+            exactly: 1\\n+          - step: refine\\n+            exactly: 1\\n+          - step: finish\\n+            exactly: 1\\n+        outputs:\\n+          - step: run-commands\\n+            path: failed\\n+            equals: 0\\n+\\n+    - name: multi-turn-refinement-loop\\n+      description: Two clarifying turns followed by a final refinement; manual-only chat.\\n+      event: manual\\n+      fixture: local.minimal\\n+      mocks:\\n+        ask[]:\\n+          - \\\"Create a CI job\\\"\\n+          - \\\"GitHub Actions; run on push to main\\\"\\n+          - \\\"Use Node 18 and npm ci + npm test\\\"\\n+        refine[]:\\n+          - { refined: false, ask_user: true, text: \\\"Which CI platform and trigger conditions?\\\" }\\n+          - { refined: false, ask_user: true, text: \\\"What Node version and commands should run?\\\" }\\n+          - { refined: true, text: \\\"Set up GitHub Actions workflow: on push to main, use Node 18.x, cache npm, run npm ci && npm test.\\\" }\\n+        plan-commands:\\n+          commands: [\\\"echo build\\\", \\\"echo lint\\\", \\\"echo test\\\"]\\n+        ask-confirm: \\\"yes\\\"\\n+        confirm-interpret:\\n+          proceed: true\\n+          commands: [\\\"echo build\\\",\\\"echo lint\\\",\\\"echo test\\\"]\\n+        run-commands:\\n+          stdout: '{\\\"failed\\\":0,\\\"results\\\":[{\\\"cmd\\\":\\\"echo build\\\",\\\"code\\\":0},{\\\"cmd\\\":\\\"echo lint\\\",\\\"code\\\":0},{\\\"cmd\\\":\\\"echo test\\\",\\\"code\\\":0}]}'\\n+      expect:\\n+        calls:\\n+          - step: ask\\n+            exactly: 3\\n+          - step: refine\\n+            exactly: 3\\n+          - step: plan-commands\\n+            exactly: 1\\n+          - step: ask-confirm\\n+            exactly: 1\\n+          - step: confirm-interpret\\n+            exactly: 1\\n+          - step: run-commands\\n+            exactly: 1\\n+          - step: finish\\n+            exactly: 1\\n+        prompts:\\n+          # Ask prompt should surface the last refine clarification\\n+          - step: ask\\n+            index: 1\\n+            contains:\\n+              - \\\"Which CI platform and trigger conditions?\\\"\\n+          - step: ask\\n+            index: last\\n+            contains:\\n+              - \\\"What Node version and commands should run?\\\"\\n+          - step: refine\\n+            index: last\\n+            contains:\\n+              - \\\"Which CI platform and trigger conditions?\\\"\\n+              - \\\"Use Node 18 and npm ci + npm test\\\"\\n+              - \\\"What Node version and commands should run?\\\"\\n+          # Keep prompt assertions resilient to minor formatting changes by using 'contains'\\n+          # instead of a single large regex.\\n+        outputs:\\n+          # Ensure the final successful refinement carries a timestamp and expected text\\n+          - step: refine\\n+            where: { path: refined, equals: true }\\n+            path: ts\\n+            matches: \\\"^\\\\\\\\d{10,}$\\\"\\n+          - step: refine\\n+            where: { path: refined, equals: true }\\n+            path: text\\n+            matches: \\\"(?is).*GitHub Actions.*Node 18.*npm ci.*npm test.*\\\"\\n+          - step: run-commands\\n+            path: failed\\n+            equals: 0\\n+\\n+    - name: plan-and-run-success\\n+      description: Plans commands, user confirms, runs successfully, finishes.\\n+      event: manual\\n+      fixture: local.minimal\\n+      mocks:\\n+        ask: \\\"Add CI to run build, lint, and tests\\\"\\n+        refine:\\n+          refined: true\\n+          text: \\\"Ensure project builds, lints, and tests pass via CI\\\"\\n+        plan-commands:\\n+          commands: [\\\"echo build\\\", \\\"echo lint\\\", \\\"echo test\\\"]\\n+        ask-confirm: \\\"yes\\\"\\n+        confirm-interpret:\\n+          proceed: true\\n+          commands: [\\\"echo build\\\",\\\"echo lint\\\",\\\"echo test\\\"]\\n+        run-commands:\\n+          stdout: '{\\\"failed\\\":0,\\\"results\\\":[{\\\"cmd\\\":\\\"echo build\\\",\\\"code\\\":0},{\\\"cmd\\\":\\\"echo lint\\\",\\\"code\\\":0},{\\\"cmd\\\":\\\"echo test\\\",\\\"code\\\":0}]}'\\n+      expect:\\n+        calls:\\n+          - step: ask\\n+            exactly: 1\\n+          - step: refine\\n+            exactly: 1\\n+          - step: plan-commands\\n+            exactly: 1\\n+          - step: ask-confirm\\n+            exactly: 1\\n+          - step: confirm-interpret\\n+            exactly: 1\\n+          - step: run-commands\\n+            exactly: 1\\n+          - step: finish\\n+            exactly: 1\\n+        outputs:\\n+          - step: run-commands\\n+            path: failed\\n+            equals: 0\\n+\\n+    - name: plan-run-fail-then-refine\\n+      description: First run fails, planner adjusts, second run passes, finish.\\n+      event: manual\\n+      fixture: local.minimal\\n+      mocks:\\n+        ask: \\\"Verify app passes tests\\\"\\n+        refine:\\n+          refined: true\\n+          text: \\\"Run tests to verify app correctness\\\"\\n+        plan-commands[]:\\n+          - { commands: [\\\"node -e \\\\\\\"process.exit(1)\\\\\\\"\\\"] }\\n+          - { commands: [\\\"node -e \\\\\\\"process.exit(0)\\\\\\\"\\\"] }\\n+        ask-confirm[]:\\n+          - \\\"yes\\\"\\n+          - \\\"yes\\\"\\n+        confirm-interpret:\\n+          proceed: true\\n+          commands: [\\\"node -e \\\\\\\"process.exit(1)\\\\\\\"\\\"]\\n+        run-commands[]:\\n+          - { stdout: '{\\\"failed\\\":1,\\\"results\\\":[{\\\"cmd\\\":\\\"node -e \\\\\\\\\\\"process.exit(1)\\\\\\\\\\\"\\\",\\\"code\\\":1,\\\"stderr\\\":\\\"fail\\\"}]}' , exit_code: 1 }\\n+          - { stdout: '{\\\"failed\\\":0,\\\"results\\\":[{\\\"cmd\\\":\\\"node -e \\\\\\\\\\\"process.exit(0)\\\\\\\\\\\"\\\",\\\"code\\\":0}]}' }\\n+      expect:\\n+        calls:\\n+          - step: ask\\n+            exactly: 2\\n+          - step: refine\\n+            exactly: 2\\n+          - step: plan-commands\\n+            exactly: 2\\n+          - step: ask-confirm\\n+            exactly: 2\\n+          - step: confirm-interpret\\n+            exactly: 2\\n+          - step: run-commands\\n+            exactly: 2\\n+          - step: finish\\n+            exactly: 1\\n+        prompts:\\n+          - step: plan-commands\\n+            index: last\\n+            contains:\\n+              - \\\"Previous attempt failed\\\"\\n+              - \\\"Run results\\\"\\n+\\n+    - name: confirm-amend-json\\n+      description: User pastes JSON array of edited commands; interpreter parses and proceeds.\\n+      event: manual\\n+      fixture: local.minimal\\n+      mocks:\\n+        ask: \\\"Make sure project installs deps and runs tests\\\"\\n+        refine:\\n+          refined: true\\n+          text: \\\"Install dependencies and run tests\\\"\\n+        plan-commands:\\n+          commands: [\\\"npm test\\\"]\\n+        ask-confirm: '[\\\"npm ci\\\",\\\"npm test\\\"]'\\n+        confirm-interpret:\\n+          proceed: true\\n+          commands: [\\\"npm ci\\\",\\\"npm test\\\"]\\n+        run-commands:\\n+          stdout: '{\\\"failed\\\":0,\\\"results\\\":[{\\\"cmd\\\":\\\"npm ci\\\",\\\"code\\\":0},{\\\"cmd\\\":\\\"npm test\\\",\\\"code\\\":0}]}'\\n+      expect:\\n+        calls:\\n+          - step: ask\\n+            exactly: 1\\n+          - step: refine\\n+            exactly: 1\\n+          - step: plan-commands\\n+            exactly: 1\\n+          - step: ask-confirm\\n+            exactly: 1\\n+          - step: confirm-interpret\\n+            exactly: 1\\n+          - step: run-commands\\n+            exactly: 1\\n+          - step: finish\\n+            exactly: 1\\n+        outputs:\\n+          - step: confirm-interpret\\n+            path: proceed\\n+            equals: true\\n+          - step: confirm-interpret\\n+            path: commands.length\\n+            equals: 2\\n+\\n+    - name: confirm-amend-lines\\n+      description: User pastes multi-line commands with duplicates; interpreter dedupes and proceeds.\\n+      event: manual\\n+      fixture: local.minimal\\n+      mocks:\\n+        ask: \\\"Run lint and tests\\\"\\n+        refine:\\n+          refined: true\\n+          text: \\\"Run lint and tests\\\"\\n+        plan-commands:\\n+          commands: [\\\"npm run lint\\\",\\\"npm test\\\"]\\n+        ask-confirm: |\\n+          npm test\\\\nnpm test\\\\n  npm run lint  \\n+        confirm-interpret:\\n+          proceed: true\\n+          commands: [\\\"npm test\\\",\\\"npm run lint\\\"]\\n+        run-commands:\\n+          stdout: '{\\\"failed\\\":0,\\\"results\\\":[{\\\"cmd\\\":\\\"npm test\\\",\\\"code\\\":0},{\\\"cmd\\\":\\\"npm run lint\\\",\\\"code\\\":0}]}'\\n+      expect:\\n+        calls:\\n+          - step: ask\\n+            exactly: 1\\n+          - step: refine\\n+            exactly: 1\\n+          - step: plan-commands\\n+            exactly: 1\\n+          - step: ask-confirm\\n+            exactly: 1\\n+          - step: confirm-interpret\\n+            exactly: 1\\n+          - step: run-commands\\n+            exactly: 1\\n+          - step: finish\\n+            exactly: 1\\n+        outputs:\\n+          - step: confirm-interpret\\n+            path: proceed\\n+            equals: true\\n+          - step: confirm-interpret\\n+            path: commands.length\\n+            equals: 2\\n+\\n+    - name: user-declines-replan-then-accept\\n+      description: User declines; interpreter routes to replan; user then accepts; run succeeds.\\n+      event: manual\\n+      fixture: local.minimal\\n+      mocks:\\n+        ask: \\\"Set up lint and test checks\\\"\\n+        refine:\\n+          refined: true\\n+          text: \\\"Ensure lint and tests run\\\"\\n+        plan-commands[]:\\n+          - { commands: [\\\"npm test\\\"] }\\n+          - { commands: [\\\"npm run lint\\\",\\\"npm test\\\"] }\\n+        ask-confirm[]:\\n+          - \\\"No, add lint too\\\"\\n+          - \\\"yes\\\"\\n+        confirm-interpret[]:\\n+          - { proceed: false, reason: \\\"needs lint\\\" }\\n+          - { proceed: true, commands: [\\\"npm run lint\\\",\\\"npm test\\\"] }\\n+        run-commands:\\n+          stdout: '{\\\"failed\\\":0,\\\"results\\\":[{\\\"cmd\\\":\\\"npm run lint\\\",\\\"code\\\":0},{\\\"cmd\\\":\\\"npm test\\\",\\\"code\\\":0}]}'\\n+      expect:\\n+        calls:\\n+          - step: ask\\n+            at_least: 1\\n+          - step: refine\\n+            at_least: 1\\n+          - step: plan-commands\\n+            exactly: 2\\n+          - step: ask-confirm\\n+            exactly: 2\\n+          - step: confirm-interpret\\n+            exactly: 2\\n+          - step: run-commands\\n+            exactly: 1\\n+          - step: finish\\n+            exactly: 1\\n+        outputs:\\n+          - step: confirm-interpret\\n+            index: last\\n+            path: proceed\\n+            equals: true\\n+\\n+    - name: run-fail-then-success-loop\\n+      description: First run fails; planner sees history; second run succeeds; prompt shows failure context.\\n+      event: manual\\n+      strict: false\\n+      fixture: local.minimal\\n+      mocks:\\n+        ask: \\\"Verify app tests pass\\\"\\n+        refine:\\n+          refined: true\\n+          text: \\\"Run tests to verify app correctness\\\"\\n+        plan-commands[]:\\n+          - { commands: [\\\"node -e \\\\\\\"process.exit(1)\\\\\\\"\\\"] }\\n+          - { commands: [\\\"echo ok\\\"] }\\n+        ask-confirm[]:\\n+          - \\\"yes\\\"\\n+          - \\\"yes\\\"\\n+        confirm-interpret[]:\\n+          - { proceed: true, commands: [\\\"node -e \\\\\\\"process.exit(1)\\\\\\\"\\\"] }\\n+          - { proceed: true, commands: [\\\"echo ok\\\"] }\\n+        run-commands[]:\\n+          - { stdout: '{\\\"failed\\\":1,\\\"results\\\":[{\\\"cmd\\\":\\\"node -e \\\\\\\\\\\"process.exit(1)\\\\\\\\\\\"\\\",\\\"code\\\":1,\\\"stderr\\\":\\\"fail\\\"}]}' , exit_code: 1 }\\n+          - { stdout: '{\\\"failed\\\":0,\\\"results\\\":[{\\\"cmd\\\":\\\"echo ok\\\",\\\"code\\\":0}]}' }\\n+      expect:\\n+        calls:\\n+          - step: ask\\n+            at_least: 1\\n+          - step: refine\\n+            at_least: 1\\n+          - step: plan-commands\\n+            exactly: 2\\n+          - step: ask-confirm\\n+            exactly: 2\\n+          - step: confirm-interpret\\n+            exactly: 2\\n+          - step: run-commands\\n+            exactly: 2\\n+        prompts:\\n+          - step: plan-commands\\n+            index: last\\n+            contains:\\n+              - \\\"Previous attempt failed\\\"\\n+              - \\\"Run results\\\"\\n\",\"status\":\"added\"},{\"filename\":\"defaults/visor.tests.yaml\",\"additions\":1,\"deletions\":1,\"changes\":61,\"patch\":\"diff --git a/defaults/visor.tests.yaml b/defaults/visor.tests.yaml\\nindex 3242f87e..5db065b7 100644\\n--- a/defaults/visor.tests.yaml\\n+++ b/defaults/visor.tests.yaml\\n@@ -60,7 +60,32 @@ tests:\\n           - step: overview\\n             contains:\\n               - \\\"feat: add user search\\\"\\n-              - \\\"diff --git a/src/search.ts\\\"\\n+\\n+    - name: references-example-link-issue\\n+      description: Ensure issue-assistant prompt renders example with HEAD fallback and includes References section.\\n+      event: issue_opened\\n+      fixture: gh.issue_open.minimal\\n+      ai_include_code_context: true\\n+      strict: false\\n+      expect:\\n+        prompts:\\n+          - step: issue-assistant\\n+            contains:\\n+              - \\\"References:\\\"\\n+              - \\\"https://github.com/owner/repo/blob/HEAD/path/to/file.ext#LSTART-LEND\\\"\\n+\\n+    - name: references-example-link-comment\\n+      description: Ensure comment-assistant prompt renders example with HEAD fallback and includes References section.\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_help\\n+      ai_include_code_context: true\\n+      strict: false\\n+      expect:\\n+        prompts:\\n+          - step: comment-assistant\\n+            contains:\\n+              - \\\"References:\\\"\\n+              - \\\"https://github.com/owner/repo/blob/HEAD/path/to/file.ext#LSTART-LEND\\\"\\n \\n     - name: issue-triage\\n       skip: true\\n@@ -294,10 +319,13 @@ tests:\\n             Invalid fact path: after assistant reply, extract-facts finds one claim and\\n             validate-fact returns is_valid=false; the runner detects problems from\\n             validate-fact history and reruns the assistant once with correction context.\\n+            Due to goto forward-running dependents, extract-facts and validate-fact also run again.\\n           event: issue_comment\\n           fixture: gh.issue_comment.visor_help\\n           env:\\n             ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          routing:\\n+            max_loops: 1\\n           mocks:\\n             comment-assistant:\\n               text: \\\"We rely on defaults/visor.yaml line 11 for max_parallelism=4.\\\"\\n@@ -314,11 +342,11 @@ tests:\\n           expect:\\n             calls:\\n               - step: comment-assistant\\n-                at_least: 2\\n+                exactly: 2\\n               - step: extract-facts\\n-                exactly: 1\\n+                exactly: 2\\n               - step: validate-fact\\n-                at_least: 1\\n+                exactly: 2\\n             outputs:\\n               - step: validate-fact\\n                 where: { path: fact_id, equals: f1 }\\n@@ -328,10 +356,13 @@ tests:\\n         - name: facts-two-items (comment)\\n           description: |\\n             Two facts extracted; only the invalid fact should appear in the correction pass.\\n+            Due to goto forward-running dependents, extract-facts and validate-fact run again on retry.\\n           event: issue_comment\\n           fixture: gh.issue_comment.visor_help\\n           env:\\n             ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          routing:\\n+            max_loops: 1\\n           mocks:\\n             comment-assistant:\\n               text: \\\"We rely on defaults/visor.yaml for concurrency defaults.\\\"\\n@@ -345,11 +376,11 @@ tests:\\n           expect:\\n             calls:\\n               - step: comment-assistant\\n-                at_least: 2\\n+                exactly: 2\\n               - step: extract-facts\\n-                exactly: 1\\n-              - step: validate-fact\\n                 exactly: 2\\n+              - step: validate-fact\\n+                exactly: 4\\n             outputs:\\n               - step: validate-fact\\n                 where: { path: fact_id, equals: f1 }\\n@@ -401,17 +432,23 @@ tests:\\n             calls:\\n               - step: issue-assistant\\n                 at_least: 1\\n+              - step: apply-issue-labels\\n+                exactly: 1\\n               - step: extract-facts\\n                 exactly: 1\\n               - step: validate-fact\\n                 at_least: 1\\n \\n         - name: facts-invalid (issue)\\n-          description: Invalid claim triggers correction by rerunning issue-assistant.\\n+          description: |\\n+            Invalid claim triggers correction by rerunning issue-assistant.\\n+            Due to goto forward-running dependents, extract-facts and validate-fact also run again.\\n           event: issue_opened\\n           fixture: gh.issue_open.minimal\\n           env:\\n             ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          routing:\\n+            max_loops: 1\\n           mocks:\\n             issue-assistant:\\n               text: \\\"Claim: max_parallelism defaults to 4\\\"\\n@@ -428,11 +465,13 @@ tests:\\n           expect:\\n             calls:\\n               - step: issue-assistant\\n-                at_least: 2\\n+                exactly: 2\\n+              - step: apply-issue-labels\\n+                exactly: 2\\n               - step: extract-facts\\n-                exactly: 1\\n+                exactly: 2\\n               - step: validate-fact\\n-                at_least: 1\\n+                exactly: 2\\n             outputs:\\n               - step: validate-fact\\n                 where: { path: fact_id, equals: f1 }\\n\",\"status\":\"modified\"},{\"filename\":\"defaults/visor.yaml\",\"additions\":1,\"deletions\":2,\"changes\":81,\"patch\":\"diff --git a/defaults/visor.yaml b/defaults/visor.yaml\\nindex 50b2ef71..0e7f912c 100644\\n--- a/defaults/visor.yaml\\n+++ b/defaults/visor.yaml\\n@@ -64,6 +64,8 @@ steps:\\n     group: overview\\n     schema: overview\\n     prompt: |\\n+        PR Title: {{ pr.title }}\\n+\\n         You are generating PR overview, to help owners of the repository to understand what this PR is above, and help reviewer to point to the right parts of the code. First you should provide detailed but concise description, mentioning all the changes.\\n \\n         ## Files Changed Analysis\\n@@ -366,20 +368,11 @@ steps:\\n         - `intent`: must be \\\"issue_triage\\\" for this flow.\\n         - `labels` (optional): array of labels that would help organization for this new issue.\\n \\n-        Add a short ‚ÄúReferences‚Äù section at the end of the markdown `text` that lists any repository files/lines you relied on for the answer. Use this exact machine‚Äëparsable format:\\n+        Add a short ‚ÄúReferences‚Äù section at the end of the markdown `text` as a simple, clickable markdown list (no fenced code blocks). Keep it minimal. If you didn‚Äôt consult code, write `References: none`.\\n \\n+        Example:\\n         References:\\n-        ```refs\\n-        path/to/file.ext[:start[-end]|#SymbolName] - very short note\\n-        ```\\n-\\n-        Rules for References:\\n-        - Only include files/lines/symbols you actually used. Keep it minimal and relevant.\\n-        - If you didn‚Äôt consult code, write:\\n-          References:\\n-          ```refs\\n-          none\\n-          ```\\n+        - [path/to/file.ext:START-END](https://github.com/{{ event.repository.fullName }}/blob/{{ event.pull_request.head.sha | default: 'HEAD' }}/path/to/file.ext#LSTART-LEND) ‚Äì very short note\\n \\n         Use this triage rubric (adopted from our main prompt):\\n         1) Categorize the issue - choose from: bug, chore, documentation, enhancement, feature, question, wontfix, invalid, duplicate\\n@@ -412,6 +405,9 @@ steps:\\n     prompt: |\\n         You are the GitHub comment assistant for {{ event.repository.fullName }}. Respond to user comments on issues or PR discussion threads.\\n \\n+        Latest comment (verbatim):\\n+        {{ event.comment.body | default: \\\"\\\" }}\\n+\\n         {%- liquid\\n           # Correction context from the last validation wave (filtered)\\n           assign issues = outputs_history[\\\"validate-fact\\\"].last | where_exp: 'i', 'i && (i.is_valid != true || i.confidence != \\\"high\\\")'\\n@@ -445,22 +441,11 @@ steps:\\n         - `intent`: choose one: \\\"comment_reply\\\" (normal reply) or \\\"comment_retrigger\\\" (pick this ONLY when the user explicitly asks to re-run checks OR explicitly asks to disable some checks).\\n         - `labels`: omit for comments (do not include).\\n \\n-        {% raw %}\\n-        Add a short ‚ÄúReferences‚Äù section at the end of the markdown `text` that lists any repository files/lines you relied on for the answer. Use this exact machine‚Äëparsable format:\\n+        Add a short ‚ÄúReferences‚Äù section at the end of the markdown `text` as a clickable markdown list (no fenced blocks). If none used, write `References: none`.\\n \\n+        Example:\\n         References:\\n-        ```refs\\n-        path/to/file.ext[:start[-end]] - very short note\\n-        ```\\n-\\n-        Rules for References:\\n-        - Only include files/lines you actually used. Keep it minimal and relevant.\\n-        - If you didn‚Äôt consult code, write:\\n-          References:\\n-          ```refs\\n-          none\\n-          ```\\n-        {% endraw %}\\n+        - [path/to/file.ext:START-END](https://github.com/{{ event.repository.fullName }}/blob/{{ event.pull_request.head.sha | default: 'HEAD' }}/path/to/file.ext#LSTART-LEND) ‚Äì very short note\\n \\n         Rules:\\n         - Never suggest rerun/disable unless asked explicitly.\\n@@ -488,19 +473,13 @@ steps:\\n     on: [issue_opened]\\n     op: labels.add\\n     value_js: |\\n-      try {\\n-        var ai = outputs && outputs['issue-assistant'] ? outputs['issue-assistant'] : {};\\n-        var labels = Array.isArray(ai && ai.labels) ? ai.labels : [];\\n-        // Sanitize labels: keep [A-Za-z0-9:/\\\\- ] (alphanumerics, colon, slash, hyphen, and space), collapse repeated '/'\\n-        return labels\\n-          .map(function(v) { return String(v == null ? '' : v); })\\n-          .map(function(s) { return s.replace(/[^A-Za-z0-9:\\\\/\\\\- ]/g, '').replace(/\\\\/{2,}/g, '/').trim(); })\\n-          .filter(Boolean);\\n-      } catch (e) {\\n-        // Avoid shadowing/resolution quirks with identifier name \\\"error\\\" in sandboxed environments\\n-        log('Error processing issue labels:', e);\\n-        return [];\\n-      }\\n+      const raw = (outputs && outputs['issue-assistant'] && outputs['issue-assistant'].labels) || [];\\n+      const labels = Array.isArray(raw) ? raw : [];\\n+      // Sanitize labels: keep [A-Za-z0-9:/\\\\- ] (alphanumerics, colon, slash, hyphen, and space), collapse repeated '/'\\n+      return labels\\n+        .map(v => String(v ?? ''))\\n+        .map(s => s.replace(/[^A-Za-z0-9:\\\\/\\\\- ]/g, '').replace(/\\\\/{2,}/g, '/').trim())\\n+        .filter(Boolean);\\n \\n   # External origin labelling for PRs and Issues\\n   external-label:\\n@@ -565,7 +544,7 @@ steps:\\n     if: \\\"env.ENABLE_FACT_VALIDATION === 'true'\\\"\\n     ai:\\n       skip_code_context: true\\n-      disable_tools: true\\n+      disableTools: true\\n     prompt: |\\n       Your task is to EXTRACT factual claims from the assistant's response below.\\n \\n@@ -626,17 +605,16 @@ steps:\\n \\n     forEach: true\\n \\n-    # After facts are validated, run the aggregator; it decides about corrections in its on_success\\n+    # After one validation wave completes, route back to the appropriate assistant\\n+    # using goto_js so the engine forward-runs dependents (assistant ‚Üí extract-facts ‚Üí validate-fact).\\n+    # This replaces the older run_js-only approach which didn‚Äôt forward-run the chain.\\n     on_finish:\\n-      run_js: |\\n-        // ES2023 style: use at() and concise arrows; inspect only the last wave\\n-        const facts = (outputs_history['extract-facts'] || []).at(-1) || [];\\n-        const ids = facts.map(f => String(f.id || '')).filter(Boolean);\\n-        const vf = outputs_history['validate-fact'] || [];\\n-        const lastItems = vf.filter(v => ids.includes(String((v && v.fact_id) || '')));\\n-        const hasProblems = lastItems.some(v => v.is_valid !== true || v.confidence !== 'high');\\n-        if (!hasProblems) return [];\\n-        return (event && event.name) === 'issue_opened' ? ['issue-assistant'] : ['comment-assistant'];\\n+      goto_js: |\\n+        // Route back for a correction pass when any recent fact validation failed.\\n+        // Be permissive here: some runners omit the `last_loop` marker; rely on is_valid when present.\\n+        const hist = (outputs_history['validate-fact'] || []);\\n+        const hasInvalid = hist.some(v => v && (v.is_valid === false || v.valid === false));\\n+        return hasInvalid ? (event.name === 'issue_opened' ? 'issue-assistant' : 'comment-assistant') : null;\\n \\n   # Validate each extracted fact\\n   validate-fact:\\n@@ -708,5 +686,6 @@ steps:\\n output:\\n   pr_comment:\\n     format: markdown\\n-    group_by: check\\n+    # Grouping is determined solely by each check's `group` field.\\n+    # The renderer ignores any global group_by; keep comments compact.\\n     collapse: true\\n\",\"status\":\"modified\"},{\"filename\":\"docs/ai-configuration.md\",\"additions\":5,\"deletions\":1,\"changes\":224,\"patch\":\"diff --git a/docs/ai-configuration.md b/docs/ai-configuration.md\\nindex 1a0e18d4..81902ff3 100644\\n--- a/docs/ai-configuration.md\\n+++ b/docs/ai-configuration.md\\n@@ -114,6 +114,47 @@ steps:\\n       provider: openai\\n       model: gpt-4-turbo\\n     prompt: \\\"Review code style and best practices\\\"\\n+\\n+#### Prompt Controls (Probe promptType, customPrompt, and persona)\\n+\\n+Visor exposes Probe‚Äôs prompt controls to adjust the agent‚Äôs behavior for a given step. Use underscore names only.\\n+\\n+Accepted keys\\n+- Under `ai:`\\n+  - `prompt_type`: string ‚Äî Probe persona/family, e.g., `engineer`, `code-review`, `architect`.\\n+  - `custom_prompt`: string ‚Äî Baseline/system prompt prepended by the SDK.\\n+- At the check level (aliases if you prefer not to nest):\\n+  - `ai_prompt_type`: string\\n+  - `ai_custom_prompt`: string\\n+  - `ai_persona`: string ‚Äî optional hint we prepend as a first line: `Persona: <value>`.\\n+\\n+Examples\\n+\\n+```yaml\\n+steps:\\n+  engineer-review:\\n+    type: ai\\n+    ai:\\n+      provider: anthropic\\n+      model: claude-3-5-sonnet-latest\\n+      prompt_type: engineer\\n+      custom_prompt: |\\n+        You are a specialist in analyzing security vulnerabilities.\\n+        Focus on injection, authn/z, crypto, and data exposure.\\n+    schema: code-review\\n+    prompt: |\\n+      Review the following changes.\\n+\\n+  quick-architect-check:\\n+    type: ai\\n+    ai_prompt_type: architect     # check-level alias\\n+    ai_custom_prompt: \\\"Favor modular boundaries and low coupling.\\\"\\n+    prompt: \\\"Assess high-level design risks in the diff\\\"\\n+```\\n+\\n+Notes\\n+- If `prompt_type` is omitted and a `schema` is provided, Visor defaults to `code-review`.\\n+- `ai_persona` is a lightweight hint added as a first line; prefer `prompt_type` when integrating with Probe personas.\\n ```\\n \\n #### AWS Bedrock Specific Configuration\\n@@ -196,6 +237,79 @@ steps:\\n \\n **Security Note:** Edit tools respect existing `allowedFolders` configuration and perform exact string matching to prevent unintended modifications. Always review changes before merging.\\n \\n+#### Tool Filtering (`allowedTools`, `disableTools`)\\n+\\n+Control which tools the AI agent can access during execution. This feature supports three filtering modes for fine-grained control over agent capabilities.\\n+\\n+**Filtering Modes:**\\n+\\n+1. **Allow All Tools (default)**: No filtering applied, agent has access to all available tools\\n+2. **Whitelist Mode**: Specify exact tools the agent can use (e.g., `['Read', 'Grep']`)\\n+3. **Exclusion Mode**: Block specific tools using `!` prefix (e.g., `['!Edit', '!Write']`)\\n+4. **Raw AI Mode**: Disable all tools for pure conversational interactions\\n+\\n+```yaml\\n+steps:\\n+  # Whitelist specific tools only\\n+  restricted-analysis:\\n+    type: ai\\n+    prompt: \\\"Analyze the codebase structure\\\"\\n+    ai:\\n+      provider: anthropic\\n+      allowedTools: ['Read', 'Grep', 'Glob']  # Only these tools allowed\\n+\\n+  # Exclude specific tools\\n+  safe-review:\\n+    type: ai\\n+    prompt: \\\"Review code without making changes\\\"\\n+    ai:\\n+      provider: google\\n+      allowedTools: ['!Edit', '!Write', '!Delete']  # Block modification tools\\n+\\n+  # Raw AI mode - no tools\\n+  conversational:\\n+    type: ai\\n+    prompt: \\\"Explain the architecture\\\"\\n+    ai:\\n+      provider: openai\\n+      disableTools: true  # Pure conversation, no tool access\\n+\\n+  # Alternative raw AI mode\\n+  conversational-alt:\\n+    type: ai\\n+    prompt: \\\"Explain the architecture\\\"\\n+    ai:\\n+      provider: anthropic\\n+      allowedTools: []  # Empty array also disables all tools\\n+```\\n+\\n+**MCP Tool Filtering:**\\n+\\n+Filter external Model Context Protocol tools using the `mcp__` prefix pattern:\\n+\\n+```yaml\\n+steps:\\n+  mcp-filtered:\\n+    type: ai\\n+    prompt: \\\"Search the codebase\\\"\\n+    ai:\\n+      provider: anthropic\\n+      allowedTools: ['mcp__code-search__*']  # Allow all code-search MCP tools\\n+      mcpServers:\\n+        code-search:\\n+          command: \\\"npx\\\"\\n+          args: [\\\"-y\\\", \\\"@modelcontextprotocol/server-code-search\\\"]\\n+```\\n+\\n+**When to use tool filtering:**\\n+- Restrict agent capabilities for security-sensitive tasks\\n+- Prevent unintended file modifications\\n+- Create specialized agents with limited toolsets\\n+- Testing and debugging specific tool interactions\\n+- Compliance requirements that limit agent autonomy\\n+\\n+**Security Note:** Tool filtering is enforced at runtime through system message filtering. Always combine with other security measures like `allowedFolders` for defense in depth.\\n+\\n #### Task Delegation (`enableDelegate`)\\n \\n Enable the delegate tool to allow AI agents to break down complex tasks and distribute them to specialized subagents for parallel processing. This feature is available when using Probe as the AI provider (Google Gemini, Anthropic Claude, OpenAI GPT, AWS Bedrock).\\n@@ -237,10 +351,118 @@ steps:\\n \\n **Note:** Task delegation increases execution time and token usage, but can provide more thorough analysis for complex tasks.\\n \\n+#### Bash Command Execution (`allowBash` / `bashConfig`)\\n+\\n+Enable secure bash command execution for AI agents to run read-only commands and analyze system state. This feature is disabled by default for security and requires explicit opt-in.\\n+\\n+**Simple Configuration:**\\n+\\n+Use `allowBash: true` for basic bash command execution with default safe commands:\\n+\\n+```yaml\\n+steps:\\n+  # Simple: Enable bash with default safe commands\\n+  git-status-analysis:\\n+    type: ai\\n+    prompt: \\\"Analyze the project structure and git status\\\"\\n+    ai:\\n+      provider: anthropic\\n+      model: claude-3-opus\\n+      allowBash: true  # Simple one-line enable\\n+```\\n+\\n+**Advanced Configuration:**\\n+\\n+Use `bashConfig` for fine-grained control over bash command execution:\\n+\\n+```yaml\\n+steps:\\n+  # Advanced: Custom allow/deny lists\\n+  custom-bash-config:\\n+    type: ai\\n+    prompt: \\\"Run custom analysis commands\\\"\\n+    ai:\\n+      provider: google\\n+      allowBash: true  # Enable bash execution\\n+      bashConfig:\\n+        allow: ['npm test', 'npm run lint']  # Additional allowed commands\\n+        deny: ['npm install']  # Additional blocked commands\\n+        timeout: 30000  # 30 second timeout per command\\n+        workingDirectory: './src'  # Default working directory\\n+\\n+  # Advanced: Disable default filters (expert mode)\\n+  advanced-bash:\\n+    type: ai\\n+    prompt: \\\"Run advanced system commands\\\"\\n+    ai:\\n+      provider: anthropic\\n+      allowBash: true\\n+      bashConfig:\\n+        noDefaultAllow: true  # Disable default safe command list\\n+        noDefaultDeny: false  # Keep default dangerous command blocklist\\n+        allow: ['specific-command-1', 'specific-command-2']\\n+```\\n+\\n+**Configuration Options:**\\n+\\n+- **`allowBash`** (boolean): Simple toggle to enable bash command execution. Default: `false`\\n+- **`allow`** (string[]): Additional permitted command patterns (e.g., `['ls', 'git status']`)\\n+- **`deny`** (string[]): Additional blocked command patterns (e.g., `['rm -rf', 'sudo']`)\\n+- **`noDefaultAllow`** (boolean): Disable default safe command list (~235 commands). Default: `false`\\n+- **`noDefaultDeny`** (boolean): Disable default dangerous command blocklist (~191 patterns). Default: `false`\\n+- **`timeout`** (number): Execution timeout in milliseconds. Default: varies by ProbeAgent\\n+- **`workingDirectory`** (string): Base directory for command execution\\n+\\n+**Default Security:**\\n+\\n+ProbeAgent includes comprehensive security by default:\\n+- **Safe Commands** (~235): Read-only operations like `ls`, `cat`, `git status`, `npm list`, `grep`\\n+- **Blocked Commands** (~191): Dangerous operations like `rm -rf`, `sudo`, `npm install`, `curl`, system modifications\\n+\\n+**When to enable bash commands:**\\n+- System state analysis (git status, file listings, environment info)\\n+- Running read-only diagnostic commands\\n+- Executing test suites or linters\\n+- Analyzing build outputs or logs\\n+\\n+**When to keep bash disabled (default):**\\n+- Security-sensitive environments\\n+- Untrusted AI prompts or inputs\\n+- Code review without system access needs\\n+- Compliance requirements that prohibit command execution\\n+\\n+**Security Best Practices:**\\n+1. Always use the default allow/deny lists unless you have specific requirements\\n+2. Set reasonable timeouts to prevent long-running commands\\n+3. Use `workingDirectory` to restrict command execution scope\\n+4. Audit command patterns in your allow list regularly\\n+5. Test configuration in a safe environment first\\n+6. Review AI-generated commands before enabling in production\\n+\\n+**Example: Git Status Analysis**\\n+\\n+```yaml\\n+steps:\\n+  git-status-review:\\n+    type: ai\\n+    prompt: |\\n+      Analyze the current git status and provide insights:\\n+      - Check for uncommitted changes\\n+      - Review branch state\\n+      - Identify any potential issues\\n+    ai:\\n+      provider: anthropic\\n+      allowBash: true  # Simple enable\\n+      bashConfig:\\n+        allow: ['git log --oneline']  # Add custom git command\\n+        workingDirectory: '.'\\n+```\\n+\\n+**Security Note:** Bash command execution respects existing security boundaries and permissions. Commands run with the same privileges as the Visor process. Always review and test bash configurations before deploying to production environments.\\n+\\n ### Fallback Behavior\\n \\n If no key is configured, Visor falls back to fast, heuristic checks (simple patterns, basic style/perf). For best results, set a provider.\\n \\n ### MCP (Tools) Support\\n See docs/mcp.md for adding MCP servers (Probe, Jira, Filesystem, etc.).\\n-\\n\",\"status\":\"modified\"},{\"filename\":\"docs/custom-tools.md\",\"additions\":12,\"deletions\":0,\"changes\":424,\"patch\":\"diff --git a/docs/custom-tools.md b/docs/custom-tools.md\\nnew file mode 100644\\nindex 00000000..e0dd1159\\n--- /dev/null\\n+++ b/docs/custom-tools.md\\n@@ -0,0 +1,424 @@\\n+# Custom Tools in YAML Configuration\\n+\\n+## Overview\\n+\\n+Custom tools allow you to define reusable command-line tools directly in your YAML configuration. These tools can then be used in MCP (Model Context Protocol) blocks throughout your configuration, making it easy to integrate any command-line tool or script into your workflow.\\n+\\n+## Features\\n+\\n+- **Define tools in YAML**: No need to create separate scripts or programs\\n+- **Input validation**: Define JSON Schema for tool parameters\\n+- **Template support**: Use Liquid templates for dynamic command generation\\n+- **Transform outputs**: Process tool output with Liquid templates or JavaScript\\n+- **Reusable**: Define once, use multiple times across your configuration\\n+- **Importable**: Share tools across projects using the `extends` mechanism\\n+- **Type-safe**: Full TypeScript support with input/output schemas\\n+- **MCP-compatible**: Tools follow the Model Context Protocol specification\\n+\\n+## Basic Tool Definition\\n+\\n+```yaml\\n+tools:\\n+  my-tool:\\n+    name: my-tool\\n+    description: Description of what the tool does\\n+    exec: 'echo \\\"Hello World\\\"'\\n+```\\n+\\n+## Complete Tool Schema\\n+\\n+```yaml\\n+tools:\\n+  tool-name:\\n+    # MCP-compatible fields (these map directly to MCP tool interface)\\n+    name: tool-name                    # Required: Tool identifier (MCP: name)\\n+    description: Tool description       # Recommended: Human-readable description (MCP: description)\\n+\\n+    # Input schema (JSON Schema format) - MCP: inputSchema\\n+    # This follows the JSON Schema specification and is used for:\\n+    # 1. Validating tool inputs before execution\\n+    # 2. Providing type information to AI models\\n+    # 3. Auto-generating documentation\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        param1:\\n+          type: string\\n+          description: Parameter description  # Describe each parameter for AI models\\n+        param2:\\n+          type: number\\n+          description: Optional parameter\\n+      required: [param1]                     # List required parameters\\n+      additionalProperties: false            # Strict mode: reject unknown parameters\\n+\\n+    # Custom tool execution fields\\n+    exec: 'command {{ args.param1 }}'  # Required: Command to execute (supports Liquid)\\n+    stdin: '{{ args.param2 }}'         # Optional: Data to pipe to stdin (supports Liquid)\\n+\\n+    cwd: /path/to/directory            # Optional: Working directory\\n+    env:                                # Optional: Environment variables\\n+      MY_VAR: value\\n+\\n+    timeout: 30000                      # Optional: Timeout in milliseconds (default: 30000)\\n+    parseJson: true                     # Optional: Parse output as JSON\\n+\\n+    # Transform output with Liquid template\\n+    transform: '{ \\\"result\\\": {{ output | json }} }'\\n+\\n+    # OR transform with JavaScript\\n+    transform_js: |\\n+      return {\\n+        processed: output.trim().toUpperCase()\\n+      };\\n+\\n+    # Output schema for validation (optional) - MCP: outputSchema\\n+    # Not currently enforced but useful for documentation\\n+    outputSchema:\\n+      type: object\\n+      properties:\\n+        result:\\n+          type: string\\n+          description: The processed result\\n+```\\n+\\n+## MCP Compatibility\\n+\\n+Custom tools are designed to be fully compatible with the Model Context Protocol (MCP) specification. When you define a custom tool, it automatically becomes available as an MCP tool with the following mapping:\\n+\\n+| Custom Tool Field | MCP Tool Field | Purpose |\\n+|------------------|----------------|---------|\\n+| `name` | `name` | Unique identifier for the tool |\\n+| `description` | `description` | Human-readable description for AI models and documentation |\\n+| `inputSchema` | `inputSchema` | JSON Schema defining expected parameters |\\n+| `outputSchema` | `outputSchema` | JSON Schema for output validation (informational) |\\n+\\n+### Why MCP Compatibility Matters\\n+\\n+1. **AI Model Integration**: Tools with proper descriptions and schemas can be automatically understood and used by AI models\\n+2. **Type Safety**: Input schemas provide runtime validation and type checking\\n+3. **Documentation**: Schemas serve as self-documenting interfaces\\n+4. **Interoperability**: Tools can potentially be used with other MCP-compatible systems\\n+\\n+### Best Practices for MCP Compatibility\\n+\\n+1. **Always provide descriptions**: Help AI models understand what your tool does\\n+   ```yaml\\n+   tools:\\n+     analyze-code:\\n+       name: analyze-code\\n+       description: \\\"Analyzes source code for complexity metrics and potential issues\\\"\\n+   ```\\n+\\n+2. **Use detailed input schemas**: Include descriptions for each parameter\\n+   ```yaml\\n+   inputSchema:\\n+     type: object\\n+     properties:\\n+       file:\\n+         type: string\\n+         description: \\\"Path to the source code file to analyze\\\"\\n+       metrics:\\n+         type: array\\n+         description: \\\"List of metrics to calculate\\\"\\n+         items:\\n+           type: string\\n+           enum: [\\\"complexity\\\", \\\"lines\\\", \\\"dependencies\\\"]\\n+     required: [\\\"file\\\"]\\n+   ```\\n+\\n+3. **Consider output schemas**: While not enforced, they document expected outputs\\n+   ```yaml\\n+   outputSchema:\\n+     type: object\\n+     properties:\\n+       complexity:\\n+         type: number\\n+         description: \\\"Cyclomatic complexity score\\\"\\n+       issues:\\n+         type: array\\n+         description: \\\"List of detected issues\\\"\\n+   ```\\n+\\n+## Using Custom Tools\\n+\\n+Once defined, custom tools can be used in any MCP block by setting `transport: custom`:\\n+\\n+```yaml\\n+steps:\\n+  my-check:\\n+    type: mcp\\n+    transport: custom              # Use custom transport\\n+    method: my-tool                 # Tool name\\n+    methodArgs:                     # Tool arguments\\n+      param1: \\\"value1\\\"\\n+      param2: 42\\n+```\\n+\\n+## Template Context\\n+\\n+Tools have access to a rich template context through Liquid templates:\\n+\\n+### In `exec` and `stdin`:\\n+- `{{ args }}` - The arguments passed to the tool\\n+- `{{ pr }}` - Pull request information (number, title, author, etc.)\\n+- `{{ files }}` - List of files in the PR\\n+- `{{ outputs }}` - Outputs from previous checks\\n+- `{{ env }}` - Environment variables\\n+\\n+### In `transform` and `transform_js`:\\n+- All of the above, plus:\\n+- `{{ output }}` - The raw command output\\n+- `{{ stdout }}` - Standard output\\n+- `{{ stderr }}` - Standard error\\n+- `{{ exitCode }}` - Command exit code\\n+\\n+## Examples\\n+\\n+### 1. Simple Grep Tool\\n+\\n+```yaml\\n+tools:\\n+  grep-todos:\\n+    name: grep-todos\\n+    description: Find TODO comments in code\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        pattern:\\n+          type: string\\n+        files:\\n+          type: array\\n+          items:\\n+            type: string\\n+    exec: 'grep -n \\\"{{ args.pattern }}\\\" {{ args.files | join: \\\" \\\" }}'\\n+```\\n+\\n+### 2. JSON Processing Tool\\n+\\n+```yaml\\n+tools:\\n+  analyze-package:\\n+    name: analyze-package\\n+    description: Analyze package.json dependencies\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        file:\\n+          type: string\\n+    exec: 'cat {{ args.file }}'\\n+    parseJson: true\\n+    transform_js: |\\n+      const deps = Object.keys(output.dependencies || {});\\n+      const devDeps = Object.keys(output.devDependencies || {});\\n+      return {\\n+        totalDeps: deps.length + devDeps.length,\\n+        prodDeps: deps.length,\\n+        devDeps: devDeps.length\\n+      };\\n+```\\n+\\n+### 3. Multi-Step Tool with Error Handling\\n+\\n+```yaml\\n+tools:\\n+  build-and-test:\\n+    name: build-and-test\\n+    description: Build project and run tests\\n+    exec: |\\n+      npm run build && npm test\\n+    timeout: 300000  # 5 minutes\\n+    transform_js: |\\n+      if (exitCode !== 0) {\\n+        return {\\n+          success: false,\\n+          error: stderr || 'Build or tests failed'\\n+        };\\n+      }\\n+      return {\\n+        success: true,\\n+        output: output\\n+      };\\n+```\\n+\\n+### 4. Tool with Dynamic Command Generation\\n+\\n+```yaml\\n+tools:\\n+  flexible-linter:\\n+    name: flexible-linter\\n+    description: Run appropriate linter based on file type\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        file:\\n+          type: string\\n+    exec: |\\n+      {% assign ext = args.file | split: \\\".\\\" | last %}\\n+      {% case ext %}\\n+        {% when \\\"js\\\", \\\"ts\\\" %}\\n+          eslint {{ args.file }}\\n+        {% when \\\"py\\\" %}\\n+          pylint {{ args.file }}\\n+        {% when \\\"go\\\" %}\\n+          golint {{ args.file }}\\n+        {% else %}\\n+          echo \\\"No linter for .{{ ext }} files\\\"\\n+      {% endcase %}\\n+```\\n+\\n+## Tool Libraries and Extends\\n+\\n+### Creating a Tool Library\\n+\\n+Create a file with just tool definitions:\\n+\\n+```yaml\\n+# tools-library.yaml\\n+version: \\\"1.0\\\"\\n+\\n+tools:\\n+  tool1:\\n+    name: tool1\\n+    exec: 'command1'\\n+\\n+  tool2:\\n+    name: tool2\\n+    exec: 'command2'\\n+```\\n+\\n+### Importing Tools\\n+\\n+Use the `extends` mechanism to import tools:\\n+\\n+```yaml\\n+version: \\\"1.0\\\"\\n+extends: ./tools-library.yaml\\n+\\n+# Additional tools can be defined here\\n+tools:\\n+  local-tool:\\n+    name: local-tool\\n+    exec: 'local-command'\\n+\\n+# Use both imported and local tools\\n+steps:\\n+  check1:\\n+    type: mcp\\n+    transport: custom\\n+    method: tool1  # From tools-library.yaml\\n+\\n+  check2:\\n+    type: mcp\\n+    transport: custom\\n+    method: local-tool  # Defined locally\\n+```\\n+\\n+### Multiple Extends\\n+\\n+You can import from multiple sources:\\n+\\n+```yaml\\n+extends:\\n+  - ./base-tools.yaml\\n+  - ./security-tools.yaml\\n+  - https://example.com/shared-tools.yaml\\n+```\\n+\\n+Tools are merged with later sources overriding earlier ones.\\n+\\n+## Integration with Other Features\\n+\\n+### Using with forEach\\n+\\n+```yaml\\n+steps:\\n+  lint-all-files:\\n+    type: mcp\\n+    transport: custom\\n+    method: my-linter\\n+    forEach: \\\"{{ files }}\\\"\\n+    methodArgs:\\n+      file: \\\"{{ item.filename }}\\\"\\n+```\\n+\\n+### Conditional Execution\\n+\\n+```yaml\\n+steps:\\n+  optional-check:\\n+    type: mcp\\n+    transport: custom\\n+    method: my-tool\\n+    if: \\\"files.some(f => f.filename.endsWith('.js'))\\\"\\n+    methodArgs:\\n+      target: \\\"src/\\\"\\n+```\\n+\\n+### Chaining with on_success/on_failure\\n+\\n+```yaml\\n+steps:\\n+  main-check:\\n+    type: mcp\\n+    transport: custom\\n+    method: build-tool\\n+    on_success:\\n+      - type: mcp\\n+        transport: custom\\n+        method: test-tool\\n+    on_failure:\\n+      - type: mcp\\n+        transport: custom\\n+        method: cleanup-tool\\n+```\\n+\\n+## Best Practices\\n+\\n+1. **Use Input Schemas**: Always define `inputSchema` to validate tool inputs\\n+2. **Handle Errors**: Use `transform_js` to check exit codes and handle errors\\n+3. **Set Timeouts**: Configure appropriate timeouts for long-running commands\\n+4. **Parse JSON**: Use `parseJson: true` for tools that output JSON\\n+5. **Document Tools**: Provide clear descriptions for each tool\\n+6. **Create Libraries**: Group related tools in separate YAML files\\n+7. **Version Control**: Store tool libraries in version control for sharing\\n+8. **Test Tools**: Test tools independently before using in complex workflows\\n+\\n+## Security Considerations\\n+\\n+- Tools execute with the same permissions as the Visor process\\n+- Be cautious with user input in tool commands\\n+- Use input validation to prevent command injection\\n+- Avoid exposing sensitive data in tool outputs\\n+- Consider using environment variables for secrets\\n+\\n+## Troubleshooting\\n+\\n+### Tool Not Found\\n+\\n+If you get \\\"Tool not found\\\" errors:\\n+1. Ensure the tool is defined in the `tools` section\\n+2. Check that the tool name matches exactly\\n+3. Verify extends paths are correct\\n+\\n+### Command Failures\\n+\\n+For command execution issues:\\n+1. Test the command manually first\\n+2. Check working directory (`cwd`) settings\\n+3. Verify required binaries are installed\\n+4. Check timeout settings for long operations\\n+\\n+### Template Errors\\n+\\n+For Liquid template problems:\\n+1. Validate template syntax\\n+2. Check that variables exist in context\\n+3. Use filters correctly (e.g., `| json`, `| join`)\\n+\\n+### Transform Errors\\n+\\n+For JavaScript transform issues:\\n+1. Ensure valid JavaScript syntax\\n+2. Always return a value\\n+3. Handle undefined/null cases\\n+4. Use try-catch for error handling\\n\\\\ No newline at end of file\\n\",\"status\":\"added\"},{\"filename\":\"docs/default-output-schema.md\",\"additions\":1,\"deletions\":0,\"changes\":28,\"patch\":\"diff --git a/docs/default-output-schema.md b/docs/default-output-schema.md\\nnew file mode 100644\\nindex 00000000..c8754d5e\\n--- /dev/null\\n+++ b/docs/default-output-schema.md\\n@@ -0,0 +1,28 @@\\n+# Default Output Schema and Timestamps\\n+\\n+Visor normalizes check outputs to make prompts and history handling predictable. There are exactly two modes:\\n+\\n+1) Checks with a schema (e.g., `schema: {...}` on the check)\\n+- The provider's output shape is respected.\\n+- If the final output is an Object (map), Visor injects a `ts` field (milliseconds since epoch) if it is missing.\\n+- If the final output is a primitive or an array, it is passed through unchanged (no wrapping, no `ts`).\\n+\\n+2) Checks without a schema\\n+- Visor uses a default schema: `{ text: string, ts: number }`.\\n+- If the provider returned a primitive (string/number/boolean), it is wrapped into `{ text, ts }`.\\n+- If it returned an Object, Visor injects `ts` if it is missing.\\n+- If it returned an array, it is passed through unchanged.\\n+\\n+This normalization happens after the provider returns and before outputs are recorded into `outputs` and `outputs_history`.\\n+\\n+Why this exists\\n+- Prompts and templates can reliably access `.text` and `.ts` for no‚Äëschema checks (e.g., human‚Äëinput), and can still trust custom shapes for schema‚Äôd checks.\\n+- `ts` allows you to sort/merge histories across steps without bespoke engines or roles.\\n+\\n+Practical tips\\n+- Human input defaults to `{ text, ts }`. In Liquid, read `outputs_history.ask[i].text` safely with a fallback: `{% if u.text %}{{ u.text }}{% else %}{{ u }}{% endif %}` for legacy mocks.\\n+- For schema‚Äôd AI checks, add `ts` to your schema if you want it persisted by validators; otherwise Visor will add it at runtime (not validated).\\n+- Arrays are passed through untouched; if you need timestamps per item, include them in your own schema.\\n+\\n+Related\\n+- See `docs/human-input-provider.md` for default output shape of human input.\\n\",\"status\":\"added\"},{\"filename\":\"docs/engine-state-machine-plan.md\",\"additions\":8,\"deletions\":0,\"changes\":279,\"patch\":\"diff --git a/docs/engine-state-machine-plan.md b/docs/engine-state-machine-plan.md\\nnew file mode 100644\\nindex 00000000..acf4f6d4\\n--- /dev/null\\n+++ b/docs/engine-state-machine-plan.md\\n@@ -0,0 +1,279 @@\\n+# Engine State Machine (v2) Research\\n+\\n+This note captures the current execution flow of the Visor engine, the surfaces where a feature flag can live, and an initial proposal for a bespoke state-machine-based Runner v2. The user-provided research artifact referenced in the request is not available inside this workspace yet, so there are a few open questions that we should fill in once we can read it.\\n+\\n+## 1. Current Engine Map\\n+\\n+### 1.1 Entry points & global state\\n+- `CheckExecutionEngine` wires together the git analyzer, provider registry, failure evaluator, snapshot journal, memory store, telemetry, and GitHub clients (`src/check-execution-engine.ts:228-332`).\\n+- Per-run state is stored in multiple mutable maps/sets such as `forwardRunGuards`, `executionStats`, `outputHistory`, `runCounters`, and `journal`; `resetPerRunState` must be called before each grouped execution to avoid leakage (`src/check-execution-engine.ts:258-306`).\\n+- `executeChecks` is the legacy CLI-style entry point that still routes through the PR reviewer after a repo scan; it loads memory, tags, GitHub checks, and eventually calls `executeReviewChecks` (`src/check-execution-engine.ts:3645-3815`).\\n+\\n+### 1.2 Config-driven execution\\n+- Modern callers (CLI, GitHub Action, SDK, test runner) invoke `executeGroupedChecks` which handles tag/event filtering, debug visualization and GitHub context capture (`src/check-execution-engine.ts:3964-4250`).\\n+- `executeGroupedChecks` chooses between three paths:\\n+  1. `executeDependencyAwareChecks` when every requested check has config (`src/check-execution-engine.ts:3976-3986`).\\n+  2. `executeSingleGroupedCheck` when exactly one configured check remains (`src/check-execution-engine.ts:4291-4470`).\\n+  3. Provider fallbacks (`src/check-execution-engine.ts:3990-4079`) or reviewer fallback for legacy ‚Äúfocus‚Äù runs.\\n+- GitHub Action mode adds grouped comment posting and per-check CheckRun updates via `GitHubCheckService`.\\n+\\n+### 1.3 Dependency-aware runner\\n+- `executeDependencyAwareChecks` orchestrates ‚Äúwaves‚Äù of checks:\\n+  1. Expand the requested set with transitive dependencies, event/tag filters, and session reuse metadata (`src/check-execution-engine.ts:5132-5320`).\\n+  2. Build/validate the dependency graph using `DependencyResolver` (`src/check-execution-engine.ts:5321-5368`, `src/dependency-resolver.ts:1-150`).\\n+  3. Maintain `wave` counters that reschedule plan execution whenever routing (`on_fail`, `on_finish`, `goto`) requests a forward run. A wave resets dedupe state and replays the DAG (`src/check-execution-engine.ts:5384-5440`).\\n+  4. For each topological level, spawn tasks up to `maxParallelism`, honoring session reuse barriers, fail-fast, and debug pauses (`src/check-execution-engine.ts:5442-6188`).\\n+  5. After each pass, inspect flags like `onFailForwardRunSeen`/`onFinishForwardRunSeen` and rebuild the graph if goto changed the event or dependency set (`src/check-execution-engine.ts:6824-7072`).\\n+- Supporting helpers:\\n+  - `executeWithLimitedParallelism` implements the generic async task pool (`src/check-execution-engine.ts:3861-3926`).\\n+  - `shouldFailFast` inspects issue severities to stop the pool early (`src/check-execution-engine.ts:8942-8958`).\\n+\\n+### 1.4 Check lifecycle & routing\\n+- `runNamedCheck` is the central dispatcher used by dependency levels, routing hooks, and inline executions. It evaluates `if` guards, enforces per-scope run caps, calls `executeCheckInline`, evaluates `fail_if`, records stats/history, and schedules routing targets (`src/check-execution-engine.ts:1516-2050`).\\n+- `executeCheckInline` ensures dependencies are materialized (recursively running missing ancestors), handles forEach fan-out, commits results into the execution journal, and records output history (`src/check-execution-engine.ts:1047-1513`).\\n+- `scheduleForwardRun` is the goto implementation that replans subsets of the DAG, filtering by event triggers and respecting per-run dedupe guards (`src/check-execution-engine.ts:412-520`).\\n+- `on_finish` orchestration is split into `runOnFinishChildren`, `decideRouting`, and `computeAllValid` helpers under `src/engine/on-finish/*`. The main engine wires them up after all forEach items and dependents settle, reusing snapshot projections for context (`src/check-execution-engine.ts:2200-2350`).\\n+- `shouldRunCheck` (and the `FailureConditionEvaluator`) power `if`, `fail_if`, and manual gating for forEach dependents (`src/check-execution-engine.ts:4801-4870`).\\n+\\n+### 1.5 Supporting services\\n+- `MemoryStore` and helper functions provide scriptable scratch storage for checks and on_finish contexts (see injections in `composeOnFinishContext`, `src/check-execution-engine.ts:900-1010` and `src/engine/on-finish/utils.ts:1-160`).\\n+- `ExecutionJournal`/`ContextView` capture per-scope results for snapshot-based dependency evaluation; journals are reset each grouped run (`src/check-execution-engine.ts:170-210` and `src/snapshot-store.ts`).\\n+- Telemetry emitters (`emitNdjsonSpanWithEvents`, OTEL spans, `addEvent`) are sprinkled through `runNamedCheck`, forEach loops, and failure evaluators.\\n+- GitHub Check integration wires into `executeChecks` and `executeGroupedChecks` to create/update CheckRuns and annotate output (`src/check-execution-engine.ts:3670-3780`, `src/github-check-service.ts`).\\n+\\n+### 1.6 Observed pain points\\n+- Engine state is scattered across mutable maps/sets, which makes it hard to reason about transitions (e.g., `forwardRunGuards`, `postOnFinishGuards`, `gotoSuppressedChecks`).\\n+- Waves are implicit booleans rather than explicit states; nested goto/forward-run flows toggle flags that outer loops poll (`src/check-execution-engine.ts:5384-5455`, `src/check-execution-engine.ts:6824-6880`).\\n+- Routing logic (on_success/on_fail/on_finish) is interwoven with execution; suppression guards (one-shot tags, goto suppression when re-running for dependents) make control flow difficult to extend.\\n+- Debug visualizer and telemetry rely on ad-hoc hooks inside loops, increasing the risk of regressions when adjusting flow control.\\n+\\n+### 1.7 Capability coverage checklist\\n+| Capability / nuance | Current implementation | State machine accommodation |\\n+| --- | --- | --- |\\n+| Per-check `if` / `fail_if` gating | `shouldRunCheck`, `evaluateFailureConditions` (`src/check-execution-engine.ts:1516-2050`, `8975-9057`) | Model as routing sub-states: every `CheckCompleted` event flows through a deterministic `Routing` state that evaluates conditions before enqueuing follow-up events. |\\n+| forEach fan-out & `on_finish` loops | `executeCheckInline` + on_finish helpers (`src/check-execution-engine.ts:1047-1513`, `2200-2350`; `src/engine/on-finish/*`) | Represent forEach items as scoped dispatch records; let `WavePlanning` schedule per-item events and treat `on_finish` as a specialized routing transition capable of emitting `WaveRetry`. |\\n+| `goto` / `goto_event` / forward-run dedupe | `scheduleForwardRun` + guard sets (`src/check-execution-engine.ts:412-520`) | Replace guard sets with queue-level deduping: before enqueuing `ForwardRunRequested`, consult a hashed tuple `(target, event, scope)` to avoid re-scheduling. |\\n+| Session reuse & provider contexts | Session metadata assembled in `executeDependencyAwareChecks`; enforced via `runNamedCheck` sessionInfo | Store session provider relationships inside `EngineContext.checks` metadata so the dispatcher can force sequential execution whenever checks share a session. |\\n+| Memory store & outputs history | `MemoryStore`, `ExecutionJournal`, scoped snapshots threaded through the legacy engine | Keep `memory`/`journal` in `EngineContext`; have state transitions commit snapshots, ensuring history is still authoritative for `on_finish`, forEach gating, and goto history queries. |\\n+| Debug visualizer pause/resume | `pauseGate`/`DebugVisualizerServer` gating inside `executeGroupedChecks` (`src/cli-main.ts:820-880`) | Have `LevelDispatch` consult a pause controller before spawning tasks and mirror all `EngineEvent`s to the debug server for visualization. |\\n+| GitHub CheckRuns & PR comments | `initializeGitHubChecks`, `completeGitHubChecks*`, `reviewer.postReviewComment` | Treat these as side effects of `Init` and `Completed` states so both engines emit identical updates; attach `engine_mode` metadata to CheckRuns for observability. |\\n+| Human input provider prompts | CLI message plumbing + provider execution context | ExecutionContext plumbing is untouched; state machine just continues passing it into providers. |\\n+| Nested workflows | Workflow provider executing its own DAG (`src/providers/workflow-check-provider.ts`) | Covered in ¬ß3.6 ‚Äî the state machine will compile workflows into child graphs and run them under the same scheduler. |\\n+| Telemetry spans / metrics | `emitNdjsonSpanWithEvents`, OTEL instrumentation sprinkled through current engine | Emit spans when entering/exiting each state (`state_from`, `state_to`, `engine_mode` attrs) so trace density stays similar but more structured. |\\n+\\n+## 2. Feature-flag surfaces\\n+\\n+### 2.1 CLI\\n+- Commander definition lives in `src/cli.ts`; new flags are added to both `setupProgram` and `parseArgs` builders (`src/cli.ts:19-141`).\\n+- Parsed options are consumed in `src/cli-main.ts`: after config discovery the engine is instantiated and `executeGroupedChecks` is called (`src/cli-main.ts:784-910`).\\n+- `CliOptions` type in `src/types/cli.ts` must include any new flag (`src/types/cli.ts:17-74`).\\n+\\n+### 2.2 GitHub Action\\n+- `src/index.ts` creates a `CheckExecutionEngine` around line 716 and always runs grouped execution (`src/index.ts:680-820`).\\n+- Inputs are read via `@actions/core.getInput`; introducing an input such as `state-machine` or honoring `VISOR_STATE_MACHINE` would happen alongside other inputs near the top of `run()` (`src/index.ts:40-140`).\\n+\\n+### 2.3 SDK & tooling\\n+- The published SDK (`src/sdk.ts:1-120`) exposes `runChecks` which instantiates the engine directly and calls `executeChecks`.\\n+- Dev/test scripts (`scripts/dev-run.ts`, `scripts/simulate-gh-run.ts`) and sample SDK programs also instantiate the engine manually (see `rg \\\"new CheckExecutionEngine\\\"`).\\n+- The YAML test runner (`src/test-runner/index.ts:100-220` and `:780+`) needs a flip to point at the new engine when the flag is turned on so regression suites can exercise both paths.\\n+\\n+### 2.4 Proposed gating strategy\\n+- Introduce an `EngineMode = 'legacy' | 'state-machine'` option accepted by `CheckExecutionEngine` (constructor or `execute*` methods). Default stays `'legacy'`.\\n+- CLI: add `--state-machine` (and env `VISOR_STATE_MACHINE=1`) that sets `engineMode` before instantiating the engine. The flag needs to propagate to **all** CLI commands (`visor`, `visor test`, `visor validate`, etc.), so `handleTestCommand` in `src/cli-main.ts` should accept/pass it through to the YAML test runner, ensuring regression suites exercise both engines.\\n+- GitHub Action: follow the CLI behavior if either a new input (`state-machine: true`) or `VISOR_STATE_MACHINE` env var is present; bubble the mode into the Action comment/telemetry payload so we can detect it in logs.\\n+- SDK/test runner/scripts: accept an optional `engineMode` option so programmatic callers can participate in canaries without CLI flags.\\n+- Telemetry/debug: add the mode to spans (e.g., `visor.run.engine_mode`) to keep traces filterable.\\n+- Initial implementation can keep mode selection localized to a helper such as `createExecutionEngine({ mode })` so that non-flagged call sites continue using the current class.\\n+\\n+## 3. State machine proposal\\n+\\n+### 3.1 Goals\\n+1. Make control flow explicit (well-defined states, transitions, and events).\\n+2. Preserve existing semantics (dependency gating, forEach fan-out, routing) while simplifying mental load.\\n+3. Unlock incremental features (pause/resume, deterministic tracing, easier retries) without relying on xstate or third-party interpreters.\\n+\\n+### 3.2 Candidate states & transitions\\n+| State | Responsibility | Exits |\\n+| --- | --- | --- |\\n+| `Init` | Capture CLI/action options, hydrate config, construct helpers, reset journals. | `PlanReady` once config + inputs validated. |\\n+| `PlanReady` | Build dependency graph, expand checks, compute per-step metadata (tags, sessions, fan-out). | `WavePlanning` (success) / `Error`. |\\n+| `WavePlanning` | Inspect outstanding events (forward runs, goto, manual triggers) and queue the next wave‚Äôs DAG snapshot. | `LevelDispatch` when a wave is ready, `Completed` when no work remains. |\\n+| `LevelDispatch` | Pop the next topological level, spawn tasks up to `maxParallelism`, hand them to `CheckRunning`. | `CheckRunning` for each task, `WavePlanning` once the level finishes. |\\n+| `CheckRunning` | Run provider + check logic, emit stats, assemble `CheckResult`. | `Routing` with success/failure payload, or `Error`. |\\n+| `Routing` | Evaluate `fail_if`, `on_success`, `on_fail`, `on_finish` triggers; enqueue new events (`ForwardRun`, `WaveRetry`, `GotoEvent`). | `WavePlanning` when new work scheduled, `Completed` if this was the final sink. |\\n+| `Completed` | Finalize stats, flush telemetry/CheckRuns, surface grouped results. | terminal |\\n+| `Error` | Capture fatal issues and unwind (shielding CLI/action from partial states). | terminal |\\n+\\n+Events flowing between states include: `PlanBuilt`, `WaveRequested`, `LevelDepleted`, `CheckComplete`, `CheckErrored`, `ForwardRunRequested`, `OnFinishLoop`, and `Shutdown`.\\n+\\n+### 3.3 Runtime data model\\n+- `EngineContext` struct holding: immutable config snapshot, dependency graph, check metadata (tags, triggers, session provider, fan-out mode), `ExecutionJournal`, output/memory stores, telemetry sinks, and persistence hooks.\\n+- `RunState` struct capturing: current engine mode, pending events queue, active wave number, active levels, outstanding tasks, global flags (e.g., `failFastTriggered`), GitHub check bookkeeping, and debug server hooks. The struct is designed to be serializable so we can persist/resume executions.\\n+- `DispatchRecord` capturing per-check data (scope path, provider id, start ts, attempts, forEach item index) to tie stats/telemetry to state transitions.\\n+- Event queue implementation (simple array or deque) so routing can push `ForwardRun`/`GotoEvent` events instead of toggling booleans. The queue doubles as the source for a structured event log, enabling time-travel debugging.\\n+\\n+TypeScript sketch:\\n+\\n+```ts\\n+type EngineMode = 'legacy' | 'state-machine';\\n+\\n+interface EngineContext {\\n+  mode: EngineMode;\\n+  config: VisorConfig;\\n+  dependencyGraph: DependencyGraph;\\n+  checks: Record<string, {\\n+    tags: string[];\\n+    triggers: EventTrigger[];\\n+    group?: string;\\n+    sessionProvider?: string;\\n+    fanout?: 'map' | 'reduce';\\n+    providerType: string;\\n+  }>;\\n+  journal: ExecutionJournal;\\n+  memory: MemoryStore;\\n+  telemetry: TelemetrySink;\\n+  gitHubChecks?: GitHubCheckService;\\n+  persistence?: {\\n+    saveState: (state: SerializedRunState) => Promise<void>;\\n+    loadState?: () => Promise<SerializedRunState | null>;\\n+  };\\n+}\\n+\\n+interface RunState {\\n+  wave: number;\\n+  levelQueue: ExecutionGroup[];\\n+  eventQueue: EngineEvent[];\\n+  activeDispatches: Map<string, DispatchRecord>;\\n+  flags: {\\n+    failFastTriggered: boolean;\\n+    forwardRunRequested: boolean;\\n+  };\\n+  stats: Map<string, CheckExecutionStats>;\\n+  historyLog: EngineEvent[]; // append-only log for time-travel debugging\\n+}\\n+\\n+type EngineEvent =\\n+  | { type: 'ForwardRunRequested'; target: string; gotoEvent?: EventTrigger; scope?: ScopePath }\\n+  | { type: 'WaveRetry'; reason: 'on_fail' | 'on_finish' | 'external' }\\n+  | { type: 'CheckScheduled'; checkId: string; scope: ScopePath }\\n+  | { type: 'CheckCompleted'; checkId: string; scope: ScopePath; result: ReviewSummary }\\n+  | { type: 'CheckErrored'; checkId: string; scope: ScopePath; error: SerializedError }\\n+  | { type: 'StateTransition'; from: EngineStateId; to: EngineStateId }\\n+  | { type: 'Shutdown'; error?: SerializedError };\\n+\\n+interface DispatchRecord {\\n+  id: string;\\n+  scope: ScopePath;\\n+  provider: string;\\n+  startMs: number;\\n+  attempts: number;\\n+  foreachIndex?: number;\\n+  sessionInfo?: { parent?: string; reuse?: boolean };\\n+}\\n+\\n+type SerializedRunState = {\\n+  wave: number;\\n+  levelQueue: ExecutionGroup[];\\n+  eventQueue: EngineEvent[];\\n+  flags: RunState['flags'];\\n+  stats: CheckExecutionStats[];\\n+  historyLog: EngineEvent[];\\n+};\\n+```\\n+\\n+Persistence/time-travel strategy (debugger-only for now):\\n+- When the debug visualizer is enabled, after every state transition the engine appends to `historyLog`, streams the event over the WebSocket, mirrors it to `output/debug-events/<session>.jsonl`, and (optionally) flushes the minimal `SerializedRunState` via `persistence.saveState`. By default we persist under `tmp/visor-state/<session>.json` (override via config/env if needed). Outside of debugger mode we skip persistence/log mirroring to avoid overhead.\\n+- During a debug resume, the engine loads the last serialized state, reconstructs in-memory maps, and continues dequeuing events, ensuring retries and routing decisions survive restarts within the debugging session.\\n+\\n+### 3.4 Migration strategy\\n+1. **Scaffolding:** introduce `EngineMode` flag plus a skeleton `StateMachineExecutionEngine` that simply proxies to the legacy runner; wire the flag through CLI/Action/SDK/tests.\\n+2. **State-core:** implement new state machine that supports dependency execution without routing (no goto/on_finish yet) and hide it behind the flag for targeted tests.\\n+3. **Routing parity:** port `scheduleForwardRun`, `runNamedCheck`, and `on_finish` semantics into state transitions; reuse existing helper functions where practical to avoid regressions.\\n+4. **Observability:** add structured tracing for state transitions so we can debug the new engine with the debug visualizer and OTEL spans.\\n+5. **Canary & cleanup:** run regression suites in both modes, flip CI to exercise the state machine on dedicated jobs, and deprecate legacy-only code once parity is proven.\\n+\\n+### 3.5 Open questions / follow-ups\\n+- Need access to the user‚Äôs research doc (currently outside the workspace) to reconcile requirements or additional tasks that were listed there.\\n+- Confirm how the debug visualizer wants to tap into the new state transitions (current server polls spans in `executeGroupedChecks`). We likely need a small event bus that mirrors `EngineEvent`s to the WebSocket server and OTEL spans (e.g., `visorevent.state_transition` with `{ from, to, checkId }` attributes).\\n+- Decide whether GitHub Action inputs should expose a first-class `state-machine` boolean or rely on env vars.\\n+- Determine whether we want to version the engine externally (e.g., `engine: v2` in config) once the flag stabilizes, or keep CLI-only toggles.\\n+\\n+Once the missing research document is accessible we should merge those findings into this plan, update the open questions list, and refine the migration steps accordingly.\\n+\\n+### 3.6 Nested workflows and reusable DAGs\\n+Visor already supports nested workflows via `type: 'workflow'` checks and the `WorkflowRegistry`/`WorkflowExecutor` (`src/workflow-registry.ts`, `src/providers/workflow-check-provider.ts`). Today those executions run entirely inside the provider, which means:\\n+\\n+- The outer engine treats the workflow check as a single node even though the workflow definition contains its own dependency graph, inputs, overrides, and potentially recursive workflow references.\\n+- Nested workflow iterations cannot benefit from core-engine features like pause/resume, telemetry, or goto semantics unless the workflow provider re-implements them.\\n+\\n+For the state-machine engine we should:\\n+\\n+1. Expose a ‚Äúsubgraph‚Äù capability so a workflow definition can be compiled into an internal DAG and scheduled as a child `EngineContext`. That keeps a single state abstraction whether we are running top-level checks or workflow steps.\\n+2. Carry parent scope into the child `RunState` so results from workflow steps register under meaningful journal scopes (`workflow:stepA@item1`).\\n+3. Allow workflows to emit their own `ForwardRunRequested`/`WaveRetry` events that bubble up to the parent queue. This prevents nested workflows from deadlocking when they need to re-run ancestor steps.\\n+4. Document limits (depth, fan-out) so that arbitrarily nested workflows do not starve the scheduler. We can enforce a `maxWorkflowDepth` (default 3) in `RunState.flags`.\\n+\\n+Implementation strategy:\\n+- Start by projecting a workflow definition into the same `DependencyGraph` structure the main engine uses (the registry already validates steps and dependencies).\\n+- When the workflow provider is invoked in state-machine mode, hand the projected graph to the engine instead of running it privately; the provider becomes a thin adapter that returns the child engine‚Äôs aggregated `ReviewSummary`.\\n+- For compatibility, keep the current ‚Äúself-contained workflow execution‚Äù path in legacy mode until all workflows are verified under the state machine.\\n+\\n+## 4. Rollout & testing milestones\\n+\\n+| Milestone | Description | Test strategy |\\n+| --- | --- | --- |\\n+| **M0 ‚Äì Flag plumbing & proxy** ‚úÖ DONE | Add `EngineMode`, CLI flag/env plumbing (including `visor test`), and a proxy state-machine runner that simply delegates to the legacy engine so tooling can toggle the mode. | Update Jest/YAML harnesses to accept `engineMode`. CI continues running legacy-only while we ensure the flag wires through all commands. |\\n+| **M1 ‚Äì Core state machine (no routing)** ‚úÖ DONE | Implement Init ‚Üí Plan ‚Üí Wave ‚Üí Level ‚Üí Check ‚Üí Completed transitions covering dependency expansion, fail-fast, stats, GitHub checks, debug pause, but still delegate routing (`goto`, `on_finish`) to legacy helpers. | Run the entire suite twice (legacy + state-machine) via a CI matrix; YAML tests remain unchanged‚Äîthey're just invoked under both modes. Add targeted unit tests for queue/dispatch logic. |\\n+| **M2 ‚Äì Routing & forEach parity** ‚úÖ DONE | Port `scheduleForwardRun`, `on_fail`, `on_success`, `on_finish`, and full forEach fan-out into the state machine; ensure flags/guards map to structured events. | Keep dual-mode CI. Add focused e2e tests covering routing loops, fail_if gating, and forEach retries, plus assertions on emitted `EngineEvent`s. |\\n+| **M3 ‚Äì Nested workflows** ‚úÖ DONE | Allow the workflow provider to hand child DAGs to the state machine, enforce depth/fan-out limits, propagate journal scopes. | Re-run existing workflow YAML suites under both modes; add a couple of dedicated unit tests that assert depth enforcement and parent/child event propagation. |\\n+| **M4 ‚Äì Observability & default flip** ‚úÖ DONE | Stream `EngineEvent`s to the debug visualizer, enrich OTEL spans/check runs with `engine_mode`, remove legacy guard maps, and make the state machine the default once confidence is high. | Continue running a reduced legacy suite in CI until full deprecation; monitor telemetry dashboards for regressions before removing legacy mode entirely. |\\n+\\n+### Test philosophy\\n+- YAML-based regression suites **must not change**; they encode behavior, not engine internals. We simply re-run them with `--state-machine` (e.g., `node scripts/run-visor-tests.js --state-machine`) during rollout to prove parity.\\n+- Jest/unit/integration tests remain authoritative; we only add a handful of state-machine-specific cases (event queue ordering, wave retry limits) instead of duplicating every scenario.\\n+- CI should eventually run in a matrix (`ENGINE_MODE=legacy` vs `state-machine`) so every PR exercises both engines until we flip the default. This is easier than maintaining a completely separate test suite.\\n+\\n+## 5. Toward structured, NASA-style guarantees\\n+\\n+One of the driving reasons for this rewrite is to reach NASA-like rigor: strong separation of concerns, declarative control flow, and statically checkable contracts. The state machine gives us the runtime substrate; we‚Äôll complement it with two configuration-level enhancements.\\n+\\n+### 5.1 Declarative transitions instead of ad-hoc `goto`\\n+We will keep `goto` / `goto_js` fully functional for backwards compatibility‚Äînothing is removed‚Äîbut we‚Äôll introduce a structured transition DSL that offers better static guarantees. Example:\\n+\\n+```yaml\\n+on_finish:\\n+  transitions:\\n+    - when: \\\"wave('validate-fact').invalid_count > 0 && event.name == 'issue_opened'\\\"\\n+      to: issue-assistant\\n+    - when: \\\"wave('validate-fact').invalid_count > 0 && event.name == 'issue_comment'\\\"\\n+      to: comment-assistant\\n+    - when: \\\"wave('validate-fact').invalid_count == 0\\\"\\n+      to: null\\n+```\\n+\\n+Plan:\\n+1. Extend the config schema with optional `transitions[]` entries (fields: `when`, `to`, optional metadata). During the `Routing` state, the engine evaluates `when` expressions in priority order and enqueues the resulting transition.\\n+2. Build a static validator that ensures each `to` refers to an existing check (or `null`), expressions only use approved helpers (`wave`, `event`, `outputs`, `memory`, etc.), and that transitions either cover all cases or explicitly fall back to `null`.\\n+3. When both `goto`/`goto_js` and `transitions` are present, the state machine honors `transitions` first (still executing the others as a fallback) and logs a warning so we can gradually migrate built-in configs away from dynamic `goto_js` without breaking existing flows.\\n+\\n+### 5.2 Assume/guarantee contracts\\n+To support design-by-contract we‚Äôll let checks declare assumptions about their inputs and guarantees about their outputs:\\n+\\n+```yaml\\n+extract-facts:\\n+  guarantee:\\n+    - \\\"Array.isArray(output) && output.every(f => f.id && f.claim)\\\"\\n+\\n+validate-fact:\\n+  assume:\\n+    - \\\"typeof extract-facts.item.id === 'string'\\\"\\n+  guarantee:\\n+    - \\\"typeof output.fact_id === 'string' && output.fact_id === extract-facts.item.id\\\"\\n+```\\n+\\n+Execution steps:\\n+1. Extend `CheckConfig` with optional `assume[]` and `guarantee[]` arrays. Before executing a provider, the state machine evaluates `assume` expressions using dependency outputs (and forEach item context); failures short-circuit execution with a structured issue referencing the violated assumption. After execution, it evaluates `guarantee` expressions against the check‚Äôs output and records fatal issues if they fail.\\n+2. Add compile-time validation: parse each expression and ensure it only references known symbols. For example, `assume` can read `dependencyName.output`, `dependencyName.item`, or `memory` but cannot mutate state; `guarantee` can read outputs but not future steps.\\n+3. Emit telemetry (`engine.contract.assume_failed`, `engine.contract.guarantee_failed`) so CI and runtime monitoring can flag contract regressions.\\n+\\n+By lifting control flow into `transitions` and correctness rules into `assume`/`guarantee`, we make configurations statically analyzable, reduce reliance on imperative `goto_js`, and move closer to NASA-inspired static validation goals while still honoring legacy constructs for advanced scenarios.\\n\",\"status\":\"added\"},{\"filename\":\"docs/failure-routing.md\",\"additions\":1,\"deletions\":1,\"changes\":2,\"patch\":\"diff --git a/docs/failure-routing.md b/docs/failure-routing.md\\nindex f0750863..fc3ce91d 100644\\n--- a/docs/failure-routing.md\\n+++ b/docs/failure-routing.md\\n@@ -127,7 +127,7 @@ Per-step actions:\\n - Retry: re-run the same step up to `retry.max`; backoff adds fixed or exponential delay with deterministic jitter.\\n - Run: on failure (or success), run listed steps first; if successful, the failed step is re-attempted once (failure path).\\n - Goto (ancestor-only): jump back to a previously executed dependency, then continue forward. On success, Visor re-runs the current step once after the jump.\\n-- Loop safety: `routing.max_loops` counts all routing transitions (runs, gotos, retries). Exceeding it aborts the current scope with a clear error.\\n+- Loop safety: `routing.max_loops` counts all routing transitions (runs, gotos, retries). Exceeding it aborts the current scope with a clear error. For a hard cap on repeated executions of the same step, see [Execution Limits](./limits.md).\\n - forEach: each item is isolated with its own loop/attempt counters; `*_js` receives `{ foreach: { index, total, parent } }`.\\n \\n ### Fan‚Äëout vs. Reduce (Phase 5)\\n\",\"status\":\"modified\"},{\"filename\":\"docs/goto-forward-run-plan.md\",\"additions\":3,\"deletions\":0,\"changes\":113,\"patch\":\"diff --git a/docs/goto-forward-run-plan.md b/docs/goto-forward-run-plan.md\\nnew file mode 100644\\nindex 00000000..ae637426\\n--- /dev/null\\n+++ b/docs/goto-forward-run-plan.md\\n@@ -0,0 +1,113 @@\\n+# Visor Engine Plan: Use `goto` for Looping on Failures\\n+\\n+This document captures the plan to simplify looping by using `goto` in `on_fail` and letting the engine re‚Äërun the dependent chain deterministically.\\n+\\n+## Background\\n+\\n+Today, builder YAML uses `on_fail.run: [agent-refine, agent-write, config-lint, tests-validate, agent-verify-tests, ‚Ä¶]` to bounce back through the pipeline. This is verbose and couples control‚Äëflow to YAML.\\n+\\n+Engine behavior:\\n+- `on_success.goto` performs a forward‚Äërun: it executes the `goto` target and all its dependents in topological order.\\n+- `on_fail.goto` is currently limited to ancestor targets and does not forward‚Äërun dependents.\\n+\\n+## Goal\\n+\\n+Allow clean, minimal YAML that uses only `goto` for looping:\\n+- Validators: `on_fail: goto: agent-refine` (or directly `goto: agent-write`).\\n+- Refine: `on_success: goto: agent-write`.\\n+\\n+The engine should handle re‚Äërunning the necessary chain; YAML should not list the entire sequence.\\n+\\n+## Proposed Engine Changes\\n+\\n+1) Unify `goto` semantics across origins\\n+- Make `goto` perform the same forward‚Äërun whether invoked from `on_success`, `on_fail`, or `on_finish`.\\n+- Factor shared code into a helper (e.g., `scheduleForwardRun(target, opts)`), currently implemented only inside the `on_success.goto` branch.\\n+\\n+2) Relax ancestor‚Äëonly restriction for `on_fail.goto`\\n+- Allow `goto` to any step in the DAG, not only ancestors.\\n+- Keep safety guards (below) to prevent runaway loops.\\n+\\n+3) Optional: Add anchors\\n+- Introduce `anchor: true` (or `loop_anchor: true`) on steps like `agent-write`.\\n+- If a validator has `on_fail` without explicit `goto`, engine can jump to the nearest anchor.\\n+\\n+4) Loop safety and predictability\\n+- Keep `routing.max_loops` budget (already implemented).\\n+- Respect `one_shot` tag: skip re‚Äërunning steps with `tags: [one_shot]` that already executed in this run.\\n+- Maintain per‚Äërun statistics to avoid duplicate scheduling within a wave.\\n+\\n+5) Forward‚Äërun details\\n+- From the target (e.g., `agent-write`), compute the dependent subgraph (topological order) honoring `depends_on` and event filters.\\n+- Preserve current event by default; honor `goto_event` only when explicitly provided.\\n+\\n+## YAML Patterns After the Change\\n+\\n+Pattern A (pure goto):\\n+- Validators (`config-lint`, `tests-validate`, `agent-verify-tests`):\\n+  ```yaml\\n+  on_fail:\\n+    goto: agent-refine\\n+  ```\\n+- Refine:\\n+  ```yaml\\n+  on_success:\\n+    goto: agent-write\\n+  ```\\n+\\n+Pattern B (single hop to anchor):\\n+- Validators:\\n+  ```yaml\\n+  on_fail:\\n+    goto: agent-write\\n+  ```\\n+\\n+## Migration Plan\\n+\\n+1) Engine implementation\\n+- Unify forward‚Äërun for `goto` in `on_fail` and `on_finish`.\\n+- Remove ancestor‚Äëonly restriction or gate it behind a feature flag (e.g., `VISOR_GOTO_GLOBAL=true`).\\n+- Extract forward‚Äërun into a shared helper.\\n+\\n+2) Builder YAML simplification\\n+- Replace `on_fail.run: [ ‚Ä¶full list‚Ä¶ ]` with `on_fail: goto: agent-refine`.\\n+- Keep `agent-refine on_success: goto: agent-write`.\\n+\\n+3) Tests (exact counts only)\\n+- Single‚Äëinvocation multi‚Äërefine (3 cycles):\\n+  - `refine = 3`, `write/lint/validate = 4`, `verify-tests = 3`, `code-review/cleanup/finish = 1`.\\n+- Flow tests (staged multi‚Äërefine) remain for readability.\\n+- Edge cases: loop budget exceeded, `one_shot` steps, event override.\\n+\\n+## Code Pointers\\n+\\n+File: `src/check-execution-engine.ts`\\n+- `executeWithRouting` ‚Äî handles `on_fail.run/goto` and `on_success.run/goto`:\\n+  - Unify forward‚Äërun behavior for `goto` across origins.\\n+  - Current forward‚Äërun logic lives in the `on_success.goto` branch (search for comments near topological ordering and `forwardSet`).\\n+- `runNamedCheck` ‚Äî respects `if` conditions and records stats; ensure forward‚Äërun uses consistent overlays/results.\\n+- Guards: `routing.max_loops`, `oncePerRun`/`one_shot` behavior, execution statistics.\\n+\\n+Suggested refactor:\\n+- Introduce `scheduleForwardRun(target, scope, opts)` used by all `goto` sites.\\n+- Extract subgraph building + topo sort into a helper for reuse.\\n+\\n+## Telemetry / Debug\\n+- Add concise debug logs for `goto` forward‚Äërun across all origins: target, number of dependents, topological order.\\n+- Keep existing OTEL and NDJSON traces.\\n+\\n+## Rollout\\n+- Implemented without a feature flag. Old configs are not broken: `goto` to\\n+  ancestors preserves the previous ‚Äúre-run ancestor only‚Äù behavior. New behavior\\n+  simply allows `goto` to any step and forward‚Äëruns its dependents when routing\\n+  to a non‚Äëancestor.\\n+- Builder YAML switched to pure `goto` in validators.\\n+\\n+## Open Questions\\n+- Should `goto` always forward‚Äërun, or only when target is an anchor? (Leaning: always forward‚Äërun for clarity.)\\n+- Should we auto‚Äëselect an anchor when `goto` target is omitted? (Future convenience.)\\n+\\n+## Acceptance Criteria\\n+- Single‚Äërun multi‚Äërefine test passes with exact counts.\\n+- Flow multi‚Äërefine tests pass.\\n+- No regression in existing suites; loop budget respected.\\n\",\"status\":\"added\"},{\"filename\":\"docs/human-input-provider.md\",\"additions\":1,\"deletions\":1,\"changes\":9,\"patch\":\"diff --git a/docs/human-input-provider.md b/docs/human-input-provider.md\\nindex 8c2a3dbf..28ae06f9 100644\\n--- a/docs/human-input-provider.md\\n+++ b/docs/human-input-provider.md\\n@@ -46,7 +46,7 @@ checks:\\n \\n ### Using Input in Dependent Checks\\n \\n-The user's input is available to dependent checks via the `outputs` variable:\\n+The user's input is available to dependent checks via the `outputs` variable. By default (no schema), human-input returns an object `{ text: string, ts: number }` where `ts` is the timestamp (milliseconds since epoch):\\n \\n ```yaml\\n checks:\\n@@ -58,8 +58,11 @@ checks:\\n     type: command\\n     depends_on: [get-version]\\n     exec: |\\n-      git tag v{{ outputs['get-version'] }}\\n-      git push origin v{{ outputs['get-version'] }}\\n+      # Prefer .text to read the string payload\\n+      git tag v{{ outputs['get-version'].text | default: outputs['get-version'] }}\\n+      git push origin v{{ outputs['get-version'].text | default: outputs['get-version'] }}\\n+\\n+> See also: [Default Output Schema](./default-output-schema.md)\\n ```\\n \\n ## Input Methods\\n\",\"status\":\"modified\"},{\"filename\":\"docs/limits.md\",\"additions\":2,\"deletions\":0,\"changes\":64,\"patch\":\"diff --git a/docs/limits.md b/docs/limits.md\\nnew file mode 100644\\nindex 00000000..9d96263e\\n--- /dev/null\\n+++ b/docs/limits.md\\n@@ -0,0 +1,64 @@\\n+## üö¶ Execution Limits (Run Caps)\\n+\\n+This feature protects workflows from accidental infinite loops by capping how many times a step may execute in a single engine run. It complements (but is different from) routing loop budgets.\\n+\\n+### Why this exists\\n+\\n+- Complex `on_fail`/`on_success` routing can create feedback loops when a remediation step immediately routes back to its source.\\n+- The cap provides a hard stop with a clear error if a step keeps re-running without converging.\\n+\\n+### Configuration\\n+\\n+Global (default is 50 if omitted):\\n+\\n+```yaml\\n+version: \\\"1.0\\\"\\n+\\n+limits:\\n+  max_runs_per_check: 50  # Applies to every step unless overridden\\n+```\\n+\\n+Per-step override:\\n+\\n+```yaml\\n+steps:\\n+  refine:\\n+    type: ai\\n+    max_runs: 10  # Hard cap for this step within one engine run\\n+```\\n+\\n+Disable cap for a specific step (not recommended unless you know it converges quickly):\\n+\\n+```yaml\\n+steps:\\n+  extract:\\n+    type: command\\n+    max_runs: 0   # or any negative value\\n+```\\n+\\n+### Behavior\\n+\\n+- The engine counts executions per step. For `forEach` children, the counter is tracked per item scope (each item has its own budget).\\n+- When the cap is exceeded, the step fails immediately with a single error issue:\\n+  - `ruleId`: `<step-id>/limits/max_runs_exceeded`\\n+  - `severity`: `error`\\n+  - `message` includes the scope and attempt number\\n+- Dependents are gated as with any error unless the dependency declares `continue_on_failure: true`.\\n+\\n+### How this differs from `routing.max_loops`\\n+\\n+- `routing.max_loops` caps routing transitions (e.g., goto/retry waves) per scope.\\n+- `limits.max_runs_per_check` caps actual step executions per step (also per scope for `forEach`).\\n+- Both guard rails can be used together: set a modest routing budget (e.g., 5‚Äì10) and leave the execution cap at the default (50) or tailor per step.\\n+\\n+### Recommendations\\n+\\n+- Keep `routing.max_loops` small for fast feedback (5‚Äì10).\\n+- Use per-step `max_runs` on chat-like loops or known retryers if you need tighter control.\\n+- Prefer fixing the loop logic (conditions/routing) over raising the caps.\\n+\\n+### Troubleshooting\\n+\\n+- If you hit `.../limits/max_runs_exceeded` immediately, check if a step is routed back without changing state.\\n+- For `forEach` flows, confirm whether the error is tied to a specific item scope; fix that item‚Äôs remediation path.\\n+\\n\",\"status\":\"added\"},{\"filename\":\"docs/liquid-templates.md\",\"additions\":1,\"deletions\":0,\"changes\":2,\"patch\":\"diff --git a/docs/liquid-templates.md b/docs/liquid-templates.md\\nindex d36d3d8f..6f152d22 100644\\n--- a/docs/liquid-templates.md\\n+++ b/docs/liquid-templates.md\\n@@ -198,6 +198,8 @@ echo '{{ pr | json }}' | jq .\\n {{ files | map: \\\"filename\\\" }}   # Array of filenames\\n ```\\n \\n+<!-- Removed merge_sort_by example: filter no longer provided -->\\n+\\n ## Examples\\n \\n ### Debugging Outputs\\n\",\"status\":\"added\"},{\"filename\":\"docs/loop-routing-refactor.md\",\"additions\":3,\"deletions\":0,\"changes\":89,\"patch\":\"diff --git a/docs/loop-routing-refactor.md b/docs/loop-routing-refactor.md\\nnew file mode 100644\\nindex 00000000..b1e0f55e\\n--- /dev/null\\n+++ b/docs/loop-routing-refactor.md\\n@@ -0,0 +1,89 @@\\n+# Manual Loop Routing Refactor ‚Äî Plan and Status\\n+\\n+This document captures the plan, rationale, completed work, and next steps to support manual‚Äëonly chat loops in Visor without special tags or goto_js.\\n+\\n+## Background\\n+\\n+The previous behavior de‚Äëduplicated a step when re‚Äërouted in the same event, which stalled manual chat loops (ask ‚Üí refine ‚Üí ask ‚Ä¶). We also experimented with a `repeatable` tag to bypass the guard, but it added concept complexity.\\n+\\n+## Goals\\n+\\n+- Manual‚Äëonly loop: ask ‚Üí refine ‚Üí ask ‚Ä¶ until refined=true, then finish.\\n+- No special tags, no goto_js, no schedule event hops.\\n+- Use fail_if + on_fail/on_success only.\\n+- Keep default suites green and avoid regressions.\\n+\\n+## Changes (Completed)\\n+\\n+1) Engine parity for inline runs\\n+- Inline `fail_if` evaluation and post‚Äë`fail_if` routing: honor `on_fail.goto` for inline runs.\\n+\\n+2) Routed re‚Äëruns (no special tags)\\n+- For `origin='on_fail'`, forward‚Äërun allows re‚Äërunning the same step within the same grouped run; loop safety relies on `routing.max_loops`.\\n+\\n+3) Failure‚Äëaware forward runs\\n+- Skip static `on_success.goto` chains when the target produced fatal issues (including `fail_if`).\\n+- For `origin='on_fail'`, schedule only direct dependents of the failed target; skip dependents when any direct dep has fatal issues.\\n+\\n+4) One‚Äëshot opt‚Äëin\\n+- `tags: [one_shot]` prevents a terminal step (e.g., `finish`) from running more than once per grouped run.\\n+\\n+5) Test‚Äëvisible history\\n+- `executeChecks` now attaches `reviewSummary.history` with a safe snapshot of per‚Äëstep outputs history for deterministic testing (no I/O).\\n+\\n+6) Task‚Äërefinement agent (manual‚Äëonly)\\n+- `defaults/task-refinement.yaml` uses `ask` ‚Üí `refine` loop with `fail_if` and `on_fail/on_success` only; no `repeatable`, no `goto_js`, no `schedule`.\\n+- Embedded tests: one‚Äëpass and multi‚Äëturn pass locally.\\n+\\n+## Removed\\n+\\n+- `repeatable` / `x-repeatable` mechanics: no longer needed.\\n+\\n+## Tests\\n+\\n+- YAML suites (green):\\n+  - `defaults/task-refinement.yaml` (both cases)\\n+  - `defaults/visor.tests.yaml` (10/10)\\n+\\n+- Jest integration (added):\\n+  - `tests/integration/on-fail-no-cascade.test.ts`: verifies failure‚Äëaware forward runs do not cascade into success chains.\\n+\\n+- Jest integration (deferred):\\n+  - A deterministic loop test using `reviewSummary.history` to assert multiple turns. Will add a tiny test driver to stabilize execution context and tag filtering.\\n+\\n+## How to Validate Locally\\n+\\n+```bash\\n+# Build CLI\\n+npm run build:cli\\n+\\n+# Task-refinement YAML\\n+VISOR_DEBUG=true node dist/index.js test --config defaults/task-refinement.yaml --max-parallel 1\\n+\\n+# Default suite\\n+node dist/index.js test --config defaults/visor.tests.yaml --max-parallel 2 --json tmp/visor.json\\n+\\n+# Focused Jest tests (engine behavior)\\n+npm test -- on-fail-no-cascade.test.ts\\n+```\\n+\\n+Acceptance criteria:\\n+- Both YAML suites pass.\\n+- No `repeatable`/`x-repeatable` in the codebase.\\n+- `defaults/task-refinement.yaml` contains no `goto_js` and no `schedule` hops.\\n+\\n+## Next Steps (Planned)\\n+\\n+1) Deterministic Jest loop test\\n+- Add a small engine test driver util (internal only) that seeds event=`manual` and disables tag filtering; assert loop counts via `reviewSummary.history`.\\n+\\n+2) Documentation\\n+- Add a short ‚ÄúManual Loops‚Äù page covering `fail_if`+`on_fail/on_success`, loop budgets, and `one_shot` for terminal steps.\\n+\\n+3) CI gates\\n+- Add a CI job to run: default YAML, task‚Äërefinement YAML, and the focused Jest tests.\\n+\\n+## Risk & Rollback\\n+\\n+- Risk: forward‚Äërun changes could over/under schedule dependents; mitigated by direct‚Äëdependent + fatal‚Äëskip guards.\\n+- Rollback: revert to pre‚Äërefactor `scheduleForwardRun` and inline `fail_if` handling while keeping `reviewSummary.history` attachment (benign).\\n\",\"status\":\"added\"},{\"filename\":\"docs/workflows.md\",\"additions\":16,\"deletions\":0,\"changes\":569,\"patch\":\"diff --git a/docs/workflows.md b/docs/workflows.md\\nnew file mode 100644\\nindex 00000000..97999883\\n--- /dev/null\\n+++ b/docs/workflows.md\\n@@ -0,0 +1,569 @@\\n+# Reusable Workflows\\n+\\n+Visor supports defining reusable workflows that can be used as building blocks in your CI/CD pipeline. Workflows allow you to create modular, parameterized sequences of checks that can be shared across projects and teams.\\n+\\n+## Table of Contents\\n+\\n+- [Overview](#overview)\\n+- [Workflow Structure](#workflow-structure)\\n+- [Input Parameters](#input-parameters)\\n+- [Output Parameters](#output-parameters)\\n+- [Using Workflows](#using-workflows)\\n+- [Advanced Features](#advanced-features)\\n+- [Examples](#examples)\\n+- [Best Practices](#best-practices)\\n+\\n+## Overview\\n+\\n+Workflows are reusable components that:\\n+- Accept input parameters (args) with JSON Schema validation\\n+- Define a sequence of steps at the root level (just like regular visor configs)\\n+- Produce output values that can be consumed by other checks\\n+- Must be defined in separate files and imported\\n+- Support all existing check types as steps\\n+\\n+## Workflow Structure\\n+\\n+Each workflow is defined in its own file with the following structure:\\n+\\n+```yaml\\n+# workflow-name.yaml\\n+id: workflow-name          # Unique identifier\\n+name: Workflow Display Name # Human-readable name\\n+description: What this workflow does\\n+version: \\\"1.0.0\\\"           # Semantic versioning\\n+\\n+# Input parameters\\n+inputs:\\n+  - name: param_name\\n+    description: Parameter description\\n+    schema:\\n+      type: string\\n+      enum: [\\\"option1\\\", \\\"option2\\\"]\\n+    default: \\\"option1\\\"\\n+    required: false\\n+\\n+# Output parameters\\n+outputs:\\n+  - name: result\\n+    description: Computation result\\n+    value_js: steps.analyze.output.score\\n+\\n+# Steps at root level - just like regular visor configs\\n+steps:\\n+  analyze:\\n+    type: ai\\n+    prompt: Analyze code with {{ inputs.param_name }}\\n+    focus: security\\n+```\\n+\\n+## Importing Workflows\\n+\\n+Import workflow files in your main configuration:\\n+\\n+```yaml\\n+# visor.yaml\\n+version: \\\"1.0\\\"\\n+\\n+# Import workflow definitions\\n+imports:\\n+  - ./workflows/security-scan.yaml\\n+  - ./workflows/code-quality.yaml\\n+  - https://example.com/workflows/shared.yaml\\n+\\n+# Use imported workflows in your steps\\n+steps:\\n+  security_check:\\n+    type: workflow\\n+    workflow: security-scan\\n+    args:\\n+      severity_threshold: high\\n+```\\n+\\n+## Input Parameters\\n+\\n+Workflows accept input parameters with JSON Schema validation:\\n+\\n+```yaml\\n+inputs:\\n+  - name: language\\n+    description: Programming language to analyze\\n+    schema:\\n+      type: string\\n+      enum: [\\\"javascript\\\", \\\"typescript\\\", \\\"python\\\", \\\"go\\\"]\\n+    required: true\\n+\\n+  - name: strict_mode\\n+    description: Enable strict checking\\n+    schema:\\n+      type: boolean\\n+    default: false\\n+    required: false\\n+\\n+  - name: patterns\\n+    description: Custom patterns to check\\n+    schema:\\n+      type: array\\n+      items:\\n+        type: string\\n+      minItems: 1\\n+```\\n+\\n+### Supported Schema Types\\n+\\n+- `string` - Text values with optional patterns, enums, length constraints\\n+- `number` - Numeric values with min/max constraints\\n+- `boolean` - True/false values\\n+- `array` - Lists with item schemas\\n+- `object` - Structured data with property schemas\\n+\\n+## Output Parameters\\n+\\n+Workflows produce outputs that can be consumed by other checks:\\n+\\n+```yaml\\n+outputs:\\n+  - name: total_issues\\n+    description: Total number of issues found\\n+    value_js: |\\n+      steps.scan1.output.issues.length +\\n+      steps.scan2.output.issues.length\\n+\\n+  - name: summary\\n+    description: Human-readable summary\\n+    value: |\\n+      Found {{ outputs.total_issues }} issues:\\n+      - Critical: {{ steps.scan1.output.critical_count }}\\n+      - Warning: {{ steps.scan2.output.warning_count }}\\n+```\\n+\\n+### Output Computation Methods\\n+\\n+1. **JavaScript expressions** (`value_js`): Compute outputs using JavaScript\\n+2. **Liquid templates** (`value`): Format outputs using Liquid templating\\n+\\n+## Workflow Steps\\n+\\n+Steps in a workflow support all standard check features:\\n+\\n+```yaml\\n+steps:\\n+  validate_input:\\n+    type: script\\n+    content: |\\n+      if (!inputs.api_key) {\\n+        throw new Error(\\\"API key is required\\\");\\n+      }\\n+      return { valid: true };\\n+\\n+  fetch_data:\\n+    type: http_client\\n+    url: https://api.example.com/data\\n+    headers:\\n+      Authorization: \\\"Bearer {{ inputs.api_key }}\\\"\\n+    depends_on: [validate_input]\\n+\\n+  analyze_data:\\n+    type: ai\\n+    prompt: |\\n+      Analyze the following data:\\n+      {{ steps.fetch_data.output | json }}\\n+\\n+      Apply threshold: {{ inputs.threshold }}\\n+    depends_on: [fetch_data]\\n+\\n+  store_results:\\n+    type: memory\\n+    operation: set\\n+    key: analysis_results\\n+    value: \\\"{{ steps.analyze_data.output }}\\\"\\n+    depends_on: [analyze_data]\\n+```\\n+\\n+### Step Input Mappings\\n+\\n+Map workflow inputs to step parameters:\\n+\\n+```yaml\\n+steps:\\n+  my_step:\\n+    type: command\\n+    exec: echo \\\"Processing...\\\"\\n+    inputs:\\n+      # Direct parameter reference\\n+      param1:\\n+        source: param\\n+        value: input_name\\n+\\n+      # Step output reference\\n+      param2:\\n+        source: step\\n+        stepId: previous_step\\n+        outputParam: result\\n+\\n+      # Constant value\\n+      param3:\\n+        source: constant\\n+        value: \\\"fixed value\\\"\\n+\\n+      # JavaScript expression\\n+      param4:\\n+        source: expression\\n+        expression: inputs.value * 2\\n+```\\n+\\n+## Using Workflows\\n+\\n+### Basic Usage\\n+\\n+Use a workflow as a check with the `workflow` type:\\n+\\n+```yaml\\n+steps:\\n+  security_check:\\n+    type: workflow\\n+    workflow: security-scan  # Workflow ID from imported file\\n+    args:\\n+      severity_threshold: high\\n+      scan_dependencies: true\\n+    on: [pr_opened, pr_updated]\\n+```\\n+\\n+### With Output Mapping\\n+\\n+Map workflow outputs to check outputs:\\n+\\n+```yaml\\n+steps:\\n+  quality_analysis:\\n+    type: workflow\\n+    workflow: code-quality\\n+    args:\\n+      language: typescript\\n+    output_mapping:\\n+      final_score: quality_score  # Map workflow output to check output\\n+      issues_list: recommendations\\n+```\\n+\\n+### With Step Overrides\\n+\\n+Override specific steps in the workflow:\\n+\\n+```yaml\\n+steps:\\n+  custom_scan:\\n+    type: workflow\\n+    workflow: security-scan\\n+    args:\\n+      severity_threshold: low\\n+    overrides:\\n+      secrets:  # Override the 'secrets' step\\n+        prompt: \\\"Custom prompt for secret scanning\\\"\\n+        timeout: 120\\n+      sql_injection:  # Override the 'sql_injection' step\\n+        ai_model: claude-3-opus-20240229\\n+```\\n+\\n+## Advanced Features\\n+\\n+### Conditional Steps\\n+\\n+Use conditions in workflow steps:\\n+\\n+```yaml\\n+steps:\\n+  optional_check:\\n+    type: ai\\n+    prompt: Run expensive check\\n+    if: inputs.enable_expensive_checks === true\\n+```\\n+\\n+### Dynamic Routing\\n+\\n+Use workflow outputs for dynamic behavior:\\n+\\n+```yaml\\n+steps:\\n+  decision_point:\\n+    type: script\\n+    content: |\\n+      if (outputs.severity_check.critical_count > 0) {\\n+        return { next_action: \\\"block\\\" };\\n+      }\\n+      return { next_action: \\\"proceed\\\" };\\n+\\n+  follow_up:\\n+    type: workflow\\n+    workflow: \\\"{{ steps.decision_point.output.next_action }}-workflow\\\"\\n+    depends_on: [decision_point]\\n+```\\n+\\n+### Workflow Composition\\n+\\n+Workflows can use other workflows:\\n+\\n+```yaml\\n+workflows:\\n+  comprehensive-check:\\n+    steps:\\n+      security:\\n+        type: workflow\\n+        workflow: security-scan\\n+        workflow_inputs:\\n+          severity_threshold: \\\"{{ inputs.security_level }}\\\"\\n+\\n+      quality:\\n+        type: workflow\\n+        workflow: code-quality\\n+        workflow_inputs:\\n+          language: \\\"{{ inputs.language }}\\\"\\n+\\n+      aggregate:\\n+        type: script\\n+        content: |\\n+          return {\\n+            passed: steps.security.output.passed && steps.quality.output.passed,\\n+            score: (steps.security.output.score + steps.quality.output.score) / 2\\n+          };\\n+        depends_on: [security, quality]\\n+```\\n+\\n+## Examples\\n+\\n+### Security Scan Workflow\\n+\\n+```yaml\\n+id: security-scan\\n+name: Security Scanner\\n+inputs:\\n+  - name: scan_level\\n+    schema:\\n+      type: string\\n+      enum: [basic, standard, comprehensive]\\n+    default: standard\\n+\\n+outputs:\\n+  - name: vulnerabilities\\n+    value_js: |\\n+      [...(steps.secrets.output.issues || []),\\n+       ...(steps.injection.output.issues || [])]\\n+\\n+  - name: passed\\n+    value_js: outputs.vulnerabilities.length === 0\\n+\\n+steps:\\n+  secrets:\\n+    type: ai\\n+    prompt: Scan for hardcoded secrets and API keys\\n+\\n+  injection:\\n+    type: ai\\n+    prompt: Check for injection vulnerabilities\\n+    depends_on: [secrets]\\n+```\\n+\\n+### Multi-Language Support Workflow\\n+\\n+```yaml\\n+id: language-check\\n+name: Multi-Language Analyzer\\n+\\n+inputs:\\n+  - name: languages\\n+    schema:\\n+      type: array\\n+      items:\\n+        type: string\\n+\\n+steps:\\n+  detect_languages:\\n+    type: script\\n+    content: |\\n+      const detected = [];\\n+      if (filesChanged.some(f => f.endsWith('.js'))) detected.push('javascript');\\n+      if (filesChanged.some(f => f.endsWith('.py'))) detected.push('python');\\n+      return { languages: detected };\\n+\\n+  analyze_each:\\n+    type: ai\\n+    forEach: true\\n+    prompt: Analyze {{ item }} code for best practices\\n+    depends_on: [detect_languages]\\n+\\n+  summarize:\\n+    type: script\\n+    content: |\\n+      const results = outputs.analyze_each;\\n+      return {\\n+        total_issues: results.reduce((sum, r) => sum + r.issues.length, 0),\\n+        by_language: results.map((r, i) => ({\\n+          language: steps.detect_languages.output.languages[i],\\n+          issues: r.issues.length\\n+        }))\\n+      };\\n+    depends_on: [analyze_each]\\n+```\\n+\\n+## Best Practices\\n+\\n+### 1. Design for Reusability\\n+\\n+- Use meaningful parameter names\\n+- Provide sensible defaults\\n+- Document all inputs and outputs\\n+- Keep workflows focused on a single concern\\n+\\n+### 2. Validate Inputs\\n+\\n+```yaml\\n+inputs:\\n+  - name: url\\n+    schema:\\n+      type: string\\n+      format: uri\\n+      pattern: \\\"^https://\\\"\\n+    description: HTTPS URL only\\n+```\\n+\\n+### 3. Handle Errors Gracefully\\n+\\n+```yaml\\n+steps:\\n+  safe_operation:\\n+    type: script\\n+    content: |\\n+      try {\\n+        return processData(inputs.data);\\n+      } catch (error) {\\n+        return {\\n+          success: false,\\n+          error: error.message,\\n+          fallback: inputs.default_value\\n+        };\\n+      }\\n+```\\n+\\n+### 4. Version Your Workflows\\n+\\n+```yaml\\n+version: \\\"2.0.0\\\"  # Semantic versioning\\n+# Breaking changes from 1.x:\\n+# - Renamed 'threshold' input to 'quality_threshold'\\n+# - Added required 'language' input\\n+```\\n+\\n+### 5. Provide Examples\\n+\\n+```yaml\\n+examples:\\n+  - name: Basic usage\\n+    description: Run with default settings\\n+    inputs:\\n+      severity: medium\\n+\\n+  - name: Strict mode\\n+    description: Maximum security scanning\\n+    inputs:\\n+      severity: critical\\n+      deep_scan: true\\n+```\\n+\\n+### 6. Test Your Workflows\\n+\\n+Create test configurations to validate workflows:\\n+\\n+```yaml\\n+# test-workflow.yaml\\n+steps:\\n+  test_workflow:\\n+    type: workflow\\n+    workflow: my-workflow\\n+    workflow_inputs:\\n+      test_param: \\\"test_value\\\"\\n+\\n+  validate_output:\\n+    type: script\\n+    content: |\\n+      const output = outputs.test_workflow;\\n+      assert(output.result !== undefined, \\\"Result is required\\\");\\n+      assert(output.score >= 0 && output.score <= 100, \\\"Score out of range\\\");\\n+    depends_on: [test_workflow]\\n+```\\n+\\n+## Workflow Schema Reference\\n+\\n+Complete workflow schema:\\n+\\n+```typescript\\n+interface WorkflowDefinition {\\n+  id: string;                    // Unique identifier\\n+  name: string;                   // Display name\\n+  description?: string;           // Description\\n+  version?: string;              // Semantic version\\n+  tags?: string[];               // Categorization tags\\n+  category?: string;             // Category (security, quality, etc.)\\n+\\n+  inputs?: WorkflowInputParam[]; // Input parameters\\n+  outputs?: WorkflowOutputParam[]; // Output parameters\\n+  steps: Record<string, WorkflowStep>; // Workflow steps\\n+\\n+  on?: EventTrigger[];          // Events that can trigger this workflow\\n+  defaults?: Partial<CheckConfig>; // Default config for steps\\n+  reusable?: boolean;            // Can be used as component\\n+\\n+  author?: {                    // Author information\\n+    name?: string;\\n+    email?: string;\\n+    url?: string;\\n+  };\\n+\\n+  license?: string;              // License information\\n+  examples?: WorkflowExample[];  // Usage examples\\n+}\\n+```\\n+\\n+## Integration with CI/CD\\n+\\n+Workflows integrate seamlessly with GitHub Actions:\\n+\\n+```yaml\\n+name: PR Review\\n+on: [pull_request]\\n+\\n+jobs:\\n+  visor:\\n+    runs-on: ubuntu-latest\\n+    steps:\\n+      - uses: actions/checkout@v3\\n+\\n+      - name: Run Visor with Workflows\\n+        uses: your-org/visor-action@v1\\n+        with:\\n+          config: .visor.yaml\\n+          workflow_imports: |\\n+            ./workflows/*.yaml\\n+            https://workflows.example.com/shared/*.yaml\\n+```\\n+\\n+## Troubleshooting\\n+\\n+### Common Issues\\n+\\n+1. **Workflow not found**: Ensure the workflow is registered via `workflows` or `workflow_imports`\\n+2. **Input validation failed**: Check that inputs match the defined schema\\n+3. **Circular dependencies**: Ensure workflow steps don't have circular `depends_on`\\n+4. **Output computation error**: Verify JavaScript expressions and Liquid templates are valid\\n+\\n+### Debug Mode\\n+\\n+Enable debug output to troubleshoot workflows:\\n+\\n+```bash\\n+visor --debug --config visor.yaml\\n+```\\n+\\n+This will show:\\n+- Workflow registration details\\n+- Input validation results\\n+- Step execution order\\n+- Output computation values\\n\\\\ No newline at end of file\\n\",\"status\":\"added\"},{\"filename\":\"eslint.config.js\",\"additions\":1,\"deletions\":1,\"changes\":11,\"patch\":\"diff --git a/eslint.config.js b/eslint.config.js\\nindex a6829e6f..22c1a636 100644\\n--- a/eslint.config.js\\n+++ b/eslint.config.js\\n@@ -8,6 +8,10 @@ module.exports = [\\n         sourceType: 'module',\\n       },\\n     },\\n+    linterOptions: {\\n+      // Do not warn about legacy disable comments as we migrate types\\n+      reportUnusedDisableDirectives: false,\\n+    },\\n     plugins: {\\n       '@typescript-eslint': require('@typescript-eslint/eslint-plugin'),\\n     },\\n@@ -18,8 +22,11 @@ module.exports = [\\n       'no-var': 'error',\\n       \\n       // TypeScript rules\\n-      '@typescript-eslint/no-unused-vars': ['error', { argsIgnorePattern: '^_' }],\\n-      '@typescript-eslint/no-explicit-any': 'warn',\\n+      '@typescript-eslint/no-unused-vars': ['error', { argsIgnorePattern: '^_', caughtErrorsIgnorePattern: '^_' }],\\n+      // Older engine files intentionally use narrow 'any' in a few places.\\n+      // Treat as disabled to keep CI and pre-commit green; we can re-enable\\n+      // per-file with explicit types in a follow-up.\\n+      '@typescript-eslint/no-explicit-any': 'off',\\n     },\\n   },\\n   {\\n\",\"status\":\"modified\"},{\"filename\":\"examples/ai-with-bash.yaml\",\"additions\":4,\"deletions\":0,\"changes\":126,\"patch\":\"diff --git a/examples/ai-with-bash.yaml b/examples/ai-with-bash.yaml\\nnew file mode 100644\\nindex 00000000..4ac971b2\\n--- /dev/null\\n+++ b/examples/ai-with-bash.yaml\\n@@ -0,0 +1,126 @@\\n+# Example Visor configuration demonstrating bash command execution in AI checks\\n+version: \\\"1.0\\\"\\n+\\n+# Global AI provider configuration\\n+ai_provider: anthropic\\n+ai_model: claude-3-sonnet\\n+\\n+steps:\\n+  # Example 1: Simple - Enable bash with default safe commands\\n+  git-status-analysis:\\n+    type: ai\\n+    prompt: |\\n+      Analyze the current git repository status:\\n+      - Check for uncommitted changes\\n+      - Review the current branch\\n+      - List recent commits\\n+      - Identify any potential issues\\n+    ai:\\n+      provider: anthropic\\n+      model: claude-3-opus\\n+      allowBash: true  # Simple one-line enable\\n+    on: [\\\"pr_opened\\\", \\\"pr_updated\\\"]\\n+    tags: [\\\"git\\\", \\\"analysis\\\"]\\n+\\n+  # Example 2: Advanced - Custom allow/deny lists for npm commands\\n+  npm-audit-check:\\n+    type: ai\\n+    prompt: |\\n+      Run npm audit and analyze the security vulnerabilities:\\n+      - Check for high/critical vulnerabilities\\n+      - Review outdated dependencies\\n+      - Suggest remediation steps\\n+    ai:\\n+      provider: google\\n+      model: gemini-2.0-flash-exp\\n+      allowBash: true\\n+      bashConfig:\\n+        allow:\\n+          - 'npm audit --json'\\n+          - 'npm outdated --json'\\n+          - 'npm list --depth=0'\\n+        timeout: 60000  # 60 second timeout\\n+    on: [\\\"pr_opened\\\"]\\n+    tags: [\\\"security\\\", \\\"npm\\\"]\\n+\\n+  # Example 3: Advanced - Test execution with custom config\\n+  test-runner-analysis:\\n+    type: ai\\n+    prompt: |\\n+      Run the test suite and analyze the results:\\n+      - Execute all tests\\n+      - Identify failing tests\\n+      - Review code coverage\\n+      - Suggest improvements\\n+    ai:\\n+      provider: anthropic\\n+      allowBash: true\\n+      bashConfig:\\n+        allow:\\n+          - 'npm test'\\n+          - 'npm run test:coverage'\\n+        deny:\\n+          - 'npm install'  # Explicitly block installation\\n+        timeout: 300000  # 5 minute timeout for tests\\n+        workingDirectory: '.'\\n+    on: [\\\"pr_opened\\\", \\\"pr_updated\\\"]\\n+    tags: [\\\"tests\\\", \\\"coverage\\\"]\\n+\\n+  # Example 4: Advanced - Build and lint with timeouts\\n+  build-lint-check:\\n+    type: ai\\n+    prompt: |\\n+      Run build and lint checks:\\n+      - Execute the build process\\n+      - Run ESLint\\n+      - Check TypeScript compilation\\n+      - Review any errors or warnings\\n+    ai:\\n+      provider: openai\\n+      model: gpt-4\\n+      allowBash: true\\n+      bashConfig:\\n+        allow:\\n+          - 'npm run build'\\n+          - 'npm run lint'\\n+          - 'tsc --noEmit'\\n+        timeout: 180000  # 3 minute timeout\\n+    on: [\\\"pr_opened\\\"]\\n+    tags: [\\\"build\\\", \\\"lint\\\"]\\n+\\n+  # Example 5: Expert - Custom commands only (no defaults)\\n+  custom-commands-only:\\n+    type: ai\\n+    prompt: \\\"Run custom analysis commands with strict control\\\"\\n+    ai:\\n+      provider: anthropic\\n+      allowBash: true\\n+      bashConfig:\\n+        noDefaultAllow: true  # Disable default safe commands\\n+        noDefaultDeny: false  # Keep dangerous command blocklist\\n+        allow:\\n+          - 'custom-tool analyze'\\n+          - 'custom-tool report'\\n+        timeout: 30000\\n+    on: [\\\"manual\\\"]\\n+    tags: [\\\"custom\\\", \\\"advanced\\\"]\\n+\\n+  # Example 6: Simple - File system analysis with defaults\\n+  filesystem-analysis:\\n+    type: ai\\n+    prompt: |\\n+      Analyze the project file structure:\\n+      - List all source files\\n+      - Check file sizes\\n+      - Review directory structure\\n+      - Identify any organizational issues\\n+    ai:\\n+      provider: anthropic\\n+      allowBash: true  # Uses default safe commands (ls, find, etc.)\\n+    on: [\\\"pr_opened\\\"]\\n+    tags: [\\\"filesystem\\\", \\\"structure\\\"]\\n+\\n+output:\\n+  pr_comment:\\n+    enabled: true\\n+    group_by: check\\n\",\"status\":\"added\"},{\"filename\":\"examples/custom-tools-example.yaml\",\"additions\":8,\"deletions\":0,\"changes\":281,\"patch\":\"diff --git a/examples/custom-tools-example.yaml b/examples/custom-tools-example.yaml\\nnew file mode 100644\\nindex 00000000..1fcfdaef\\n--- /dev/null\\n+++ b/examples/custom-tools-example.yaml\\n@@ -0,0 +1,281 @@\\n+version: \\\"1.0\\\"\\n+\\n+# Custom Tool Definitions\\n+# These tools can be used in any MCP block with transport: custom\\n+tools:\\n+  # Simple grep tool for finding patterns in files\\n+  grep-pattern:\\n+    name: grep-pattern\\n+    description: Search for patterns in files using grep\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        pattern:\\n+          type: string\\n+          description: Regular expression pattern to search for\\n+        files:\\n+          type: array\\n+          items:\\n+            type: string\\n+          description: List of files to search in\\n+      required: [pattern]\\n+    exec: 'grep -n \\\"{{ args.pattern }}\\\" {{ args.files | join: \\\" \\\" }}'\\n+    transform_js: |\\n+      // Parse grep output into structured format\\n+      const lines = output.trim().split('\\\\n').filter(l => l);\\n+      return lines.map(line => {\\n+        const match = line.match(/^([^:]+):(\\\\d+):(.*)$/);\\n+        if (!match) return null;\\n+        return {\\n+          file: match[1],\\n+          line: parseInt(match[2]),\\n+          content: match[3].trim()\\n+        };\\n+      }).filter(Boolean);\\n+\\n+  # Tool to count lines of code\\n+  count-lines:\\n+    name: count-lines\\n+    description: Count lines of code in files\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        files:\\n+          type: array\\n+          items:\\n+            type: string\\n+          description: Files to count lines in\\n+        exclude_blank:\\n+          type: boolean\\n+          description: Exclude blank lines from count\\n+    exec: |\\n+      {% if args.exclude_blank %}\\n+        cat {{ args.files | join: \\\" \\\" }} | grep -v \\\"^$\\\" | wc -l\\n+      {% else %}\\n+        cat {{ args.files | join: \\\" \\\" }} | wc -l\\n+      {% endif %}\\n+    transform_js: 'return parseInt(output.trim());'\\n+\\n+  # Tool to check if a file contains sensitive data patterns\\n+  check-secrets:\\n+    name: check-secrets\\n+    description: Check for potential secrets in files\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        file:\\n+          type: string\\n+          description: File to check\\n+      required: [file]\\n+    exec: |\\n+      grep -E \\\"(api[_-]?key|secret|token|password|pwd|auth|credential)\\\" -i \\\"{{ args.file }}\\\" || echo \\\"No secrets found\\\"\\n+    transform_js: |\\n+      if (output.includes(\\\"No secrets found\\\")) {\\n+        return { safe: true, issues: [] };\\n+      }\\n+      const lines = output.trim().split('\\\\n');\\n+      return {\\n+        safe: false,\\n+        issues: lines.map(line => ({\\n+          type: 'potential_secret',\\n+          content: line.substring(0, 100) // Truncate for safety\\n+        }))\\n+      };\\n+\\n+  # Tool to analyze code complexity\\n+  analyze-complexity:\\n+    name: analyze-complexity\\n+    description: Analyze code complexity metrics\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        file:\\n+          type: string\\n+          description: Source code file to analyze\\n+        language:\\n+          type: string\\n+          description: Programming language (js, py, go, etc.)\\n+      required: [file]\\n+    exec: |\\n+      echo \\\"Analyzing {{ args.file }}\\\"\\n+      # Count functions/methods\\n+      {% if args.language == \\\"js\\\" or args.language == \\\"ts\\\" %}\\n+        grep -c \\\"function\\\\|=>\\\" \\\"{{ args.file }}\\\" || echo \\\"0\\\"\\n+      {% elsif args.language == \\\"py\\\" %}\\n+        grep -c \\\"def \\\" \\\"{{ args.file }}\\\" || echo \\\"0\\\"\\n+      {% elsif args.language == \\\"go\\\" %}\\n+        grep -c \\\"func \\\" \\\"{{ args.file }}\\\" || echo \\\"0\\\"\\n+      {% else %}\\n+        echo \\\"0\\\"\\n+      {% endif %}\\n+    transform_js: |\\n+      const functionCount = parseInt(output.trim().split('\\\\n').pop() || '0');\\n+      return {\\n+        file: args.file,\\n+        language: args.language || 'unknown',\\n+        metrics: {\\n+          functionCount: functionCount,\\n+          complexity: functionCount > 10 ? 'high' : functionCount > 5 ? 'medium' : 'low'\\n+        }\\n+      };\\n+\\n+  # Tool to validate JSON/YAML files\\n+  validate-config:\\n+    name: validate-config\\n+    description: Validate configuration file syntax\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        file:\\n+          type: string\\n+          description: Configuration file to validate\\n+        type:\\n+          type: string\\n+          enum: [json, yaml]\\n+          description: File type to validate\\n+      required: [file, type]\\n+    exec: |\\n+      {% if args.type == \\\"json\\\" %}\\n+        python3 -m json.tool \\\"{{ args.file }}\\\" > /dev/null 2>&1 && echo \\\"Valid JSON\\\" || echo \\\"Invalid JSON\\\"\\n+      {% else %}\\n+        python3 -c \\\"import yaml; yaml.safe_load(open('{{ args.file }}'))\\\" 2>&1 && echo \\\"Valid YAML\\\" || echo \\\"Invalid YAML\\\"\\n+      {% endif %}\\n+    parseJson: false\\n+    transform_js: |\\n+      const isValid = output.includes(\\\"Valid\\\");\\n+      return {\\n+        file: args.file,\\n+        type: args.type,\\n+        valid: isValid,\\n+        message: output.trim()\\n+      };\\n+\\n+  # Tool to generate file statistics\\n+  file-stats:\\n+    name: file-stats\\n+    description: Generate statistics about files\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        pattern:\\n+          type: string\\n+          description: Glob pattern for files to analyze\\n+    exec: |\\n+      find . -name \\\"{{ args.pattern }}\\\" -type f | xargs wc -l | tail -1\\n+    transform_js: |\\n+      const parts = output.trim().split(/\\\\s+/);\\n+      return {\\n+        totalLines: parseInt(parts[0] || '0'),\\n+        fileCount: Math.max(0, parts.length - 2) // Subtract total line\\n+      };\\n+\\n+# Example usage of custom tools in checks\\n+steps:\\n+  # Use custom tool to find TODO comments\\n+  find-todos:\\n+    type: mcp\\n+    transport: custom\\n+    method: grep-pattern\\n+    methodArgs:\\n+      pattern: \\\"TODO|FIXME|HACK\\\"\\n+      files: [\\\"*.js\\\", \\\"*.ts\\\", \\\"*.py\\\"]\\n+    transform_js: |\\n+      // Convert grep results to issues\\n+      output.map(match => ({\\n+        file: match.file,\\n+        line: match.line,\\n+        message: `Found comment: ${match.content}`,\\n+        severity: match.content.includes('FIXME') ? 'warning' : 'info',\\n+        category: 'documentation',\\n+        ruleId: 'todo-comment'\\n+      }))\\n+\\n+  # Check for secrets using custom tool\\n+  security-check:\\n+    type: mcp\\n+    transport: custom\\n+    method: check-secrets\\n+    forEach: \\\"{{ files | map: 'filename' }}\\\"\\n+    methodArgs:\\n+      file: \\\"{{ item }}\\\"\\n+    transform_js: |\\n+      if (!output.safe) {\\n+        return output.issues.map(issue => ({\\n+          file: args.file,\\n+          line: 0,\\n+          message: `Potential secret detected: ${issue.type}`,\\n+          severity: 'critical',\\n+          category: 'security',\\n+          ruleId: 'potential-secret'\\n+        }));\\n+      }\\n+      return [];\\n+\\n+  # Analyze code complexity\\n+  complexity-analysis:\\n+    type: mcp\\n+    transport: custom\\n+    method: analyze-complexity\\n+    forEach: \\\"{{ files | where: 'filename', 'endsWith', '.js' }}\\\"\\n+    methodArgs:\\n+      file: \\\"{{ item.filename }}\\\"\\n+      language: \\\"js\\\"\\n+    transform_js: |\\n+      const complexity = output.metrics.complexity;\\n+      if (complexity === 'high') {\\n+        return [{\\n+          file: output.file,\\n+          line: 0,\\n+          message: `High complexity detected: ${output.metrics.functionCount} functions`,\\n+          severity: 'warning',\\n+          category: 'performance',\\n+          ruleId: 'high-complexity'\\n+        }];\\n+      }\\n+      return [];\\n+\\n+  # Validate all JSON config files\\n+  validate-json-configs:\\n+    type: mcp\\n+    transport: custom\\n+    method: validate-config\\n+    forEach: \\\"{{ files | where: 'filename', 'endsWith', '.json' }}\\\"\\n+    methodArgs:\\n+      file: \\\"{{ item.filename }}\\\"\\n+      type: \\\"json\\\"\\n+    fail_if: \\\"!output.valid\\\"\\n+    transform_js: |\\n+      if (!output.valid) {\\n+        return [{\\n+          file: output.file,\\n+          line: 0,\\n+          message: output.message,\\n+          severity: 'error',\\n+          category: 'style',\\n+          ruleId: 'invalid-json'\\n+        }];\\n+      }\\n+      return [];\\n+\\n+  # Get repository statistics\\n+  repo-stats:\\n+    type: mcp\\n+    transport: custom\\n+    method: file-stats\\n+    methodArgs:\\n+      pattern: \\\"*.{js,ts,py,go}\\\"\\n+    transform_js: |\\n+      // Create informational message about repo size\\n+      [{\\n+        file: 'repository',\\n+        line: 0,\\n+        message: `Repository contains ${output.fileCount} source files with ${output.totalLines} total lines`,\\n+        severity: 'info',\\n+        category: 'documentation',\\n+        ruleId: 'repo-stats'\\n+      }]\\n+\\n+# Output configuration\\n+output:\\n+  format: table\\n+  groupBy: severity\\n\\\\ No newline at end of file\\n\",\"status\":\"added\"},{\"filename\":\"examples/project-with-tools.yaml\",\"additions\":5,\"deletions\":0,\"changes\":174,\"patch\":\"diff --git a/examples/project-with-tools.yaml b/examples/project-with-tools.yaml\\nnew file mode 100644\\nindex 00000000..30dede5b\\n--- /dev/null\\n+++ b/examples/project-with-tools.yaml\\n@@ -0,0 +1,174 @@\\n+version: \\\"1.0\\\"\\n+\\n+# This configuration extends the tools library to reuse tool definitions\\n+extends: ./tools-library.yaml\\n+\\n+# Additional project-specific tools can be defined here\\n+tools:\\n+  # Project-specific tool for checking API endpoints\\n+  check-api-endpoints:\\n+    name: check-api-endpoints\\n+    description: Verify all API endpoints are documented\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        source_dir:\\n+          type: string\\n+          description: Directory containing API source code\\n+        docs_file:\\n+          type: string\\n+          description: API documentation file\\n+    exec: |\\n+      echo \\\"Checking endpoints in {{ args.source_dir }} against {{ args.docs_file }}\\\"\\n+      grep -r \\\"router\\\\.\\\\(get\\\\|post\\\\|put\\\\|delete\\\\|patch\\\\)\\\" {{ args.source_dir }} |\\n+      sed 's/.*router\\\\.\\\\([a-z]*\\\\).*\\\"\\\\([^\\\"]*\\\\)\\\".*/\\\\1 \\\\2/' |\\n+      sort -u\\n+    transform_js: |\\n+      const endpoints = output.trim().split('\\\\n').filter(l => l);\\n+      return endpoints.map(ep => {\\n+        const [method, path] = ep.split(' ');\\n+        return { method: method.toUpperCase(), path };\\n+      });\\n+\\n+# Use both imported and local tools in checks\\n+steps:\\n+  # Use imported git tool\\n+  check-git-status:\\n+    type: mcp\\n+    transport: custom\\n+    method: git-status\\n+    fail_if: \\\"output.length > 10\\\"\\n+    transform_js: |\\n+      if (output.length > 10) {\\n+        return [{\\n+          file: 'repository',\\n+          line: 0,\\n+          message: `Too many uncommitted changes: ${output.length} files`,\\n+          severity: 'warning',\\n+          category: 'style',\\n+          ruleId: 'uncommitted-changes'\\n+        }];\\n+      }\\n+      return [];\\n+\\n+  # Use imported git diff stats\\n+  analyze-pr-size:\\n+    type: mcp\\n+    transport: custom\\n+    method: git-diff-stats\\n+    methodArgs:\\n+      base: \\\"{{ pr.base | default: 'main' }}\\\"\\n+    fail_if: \\\"output.filesChanged > 50 || output.insertions > 1000\\\"\\n+    transform_js: |\\n+      const issues = [];\\n+      if (output.filesChanged > 50) {\\n+        issues.push({\\n+          file: 'pull-request',\\n+          line: 0,\\n+          message: `Large PR: ${output.filesChanged} files changed. Consider breaking into smaller PRs.`,\\n+          severity: 'warning',\\n+          category: 'style',\\n+          ruleId: 'large-pr'\\n+        });\\n+      }\\n+      if (output.insertions > 1000) {\\n+        issues.push({\\n+          file: 'pull-request',\\n+          line: 0,\\n+          message: `Too many lines added: ${output.insertions}. This may be difficult to review.`,\\n+          severity: 'warning',\\n+          category: 'style',\\n+          ruleId: 'too-many-lines'\\n+        });\\n+      }\\n+      return issues;\\n+\\n+  # Use imported npm audit tool\\n+  security-audit:\\n+    type: mcp\\n+    transport: custom\\n+    method: npm-audit\\n+    if: \\\"files.some(f => f.filename === 'package.json')\\\"\\n+    fail_if: \\\"output.some(i => i.severity === 'error')\\\"\\n+\\n+  # Use imported Docker linting tool\\n+  lint-dockerfiles:\\n+    type: mcp\\n+    transport: custom\\n+    method: docker-lint\\n+    forEach: \\\"{{ files | where: 'filename', 'match', 'Dockerfile' }}\\\"\\n+    methodArgs:\\n+      file: \\\"{{ item.filename }}\\\"\\n+\\n+  # Use imported ESLint tool\\n+  lint-javascript:\\n+    type: mcp\\n+    transport: custom\\n+    method: eslint-check\\n+    if: \\\"files.some(f => f.filename.endsWith('.js') || f.filename.endsWith('.ts'))\\\"\\n+    methodArgs:\\n+      files: \\\"{{ files | where: 'filename', 'match', '\\\\\\\\.(js|ts)$' | map: 'filename' }}\\\"\\n+\\n+  # Use local project-specific tool\\n+  verify-api-docs:\\n+    type: mcp\\n+    transport: custom\\n+    method: check-api-endpoints\\n+    methodArgs:\\n+      source_dir: \\\"./src/routes\\\"\\n+      docs_file: \\\"./docs/api.md\\\"\\n+    transform_js: |\\n+      // Check if all endpoints are documented\\n+      const documented = ['GET /users', 'POST /users', 'GET /posts']; // Would parse from docs_file\\n+      const undocumented = output.filter(ep =>\\n+        !documented.includes(`${ep.method} ${ep.path}`)\\n+      );\\n+\\n+      return undocumented.map(ep => ({\\n+        file: 'docs/api.md',\\n+        line: 0,\\n+        message: `Undocumented endpoint: ${ep.method} ${ep.path}`,\\n+        severity: 'warning',\\n+        category: 'documentation',\\n+        ruleId: 'missing-api-docs'\\n+      }));\\n+\\n+  # Chain multiple tools together\\n+  comprehensive-check:\\n+    type: mcp\\n+    transport: custom\\n+    method: run-tests\\n+    methodArgs:\\n+      command: \\\"npm test\\\"\\n+      format: \\\"jest\\\"\\n+    on_success:\\n+      - type: mcp\\n+        transport: custom\\n+        method: check-outdated\\n+        methodArgs:\\n+          manager: \\\"npm\\\"\\n+        transform_js: |\\n+          // Only warn about major version updates\\n+          return output\\n+            .filter(pkg => pkg.current.split('.')[0] !== pkg.latest.split('.')[0])\\n+            .map(pkg => ({\\n+              file: 'package.json',\\n+              line: 0,\\n+              message: `Major update available for ${pkg.package}: ${pkg.current} ‚Üí ${pkg.latest}`,\\n+              severity: 'info',\\n+              category: 'documentation',\\n+              ruleId: 'major-update-available'\\n+            }));\\n+\\n+# You can also import tools from remote URLs\\n+# extends: https://example.com/shared-tools.yaml\\n+\\n+# Or import multiple tool libraries\\n+# extends:\\n+#   - ./tools-library.yaml\\n+#   - ./security-tools.yaml\\n+#   - https://example.com/quality-tools.yaml\\n+\\n+output:\\n+  format: markdown\\n+  groupBy: category\\n\\\\ No newline at end of file\\n\",\"status\":\"added\"},{\"filename\":\"examples/tools-library.yaml\",\"additions\":8,\"deletions\":0,\"changes\":281,\"patch\":\"diff --git a/examples/tools-library.yaml b/examples/tools-library.yaml\\nnew file mode 100644\\nindex 00000000..b21e4433\\n--- /dev/null\\n+++ b/examples/tools-library.yaml\\n@@ -0,0 +1,281 @@\\n+version: \\\"1.0\\\"\\n+\\n+# Reusable Tool Library\\n+# This file contains only tool definitions that can be imported by other configs\\n+\\n+tools:\\n+  # Git tools\\n+  git-status:\\n+    name: git-status\\n+    description: Get git repository status\\n+    exec: 'git status --porcelain'\\n+    transform_js: |\\n+      const lines = output.trim().split('\\\\n').filter(l => l);\\n+      return lines.map(line => {\\n+        const [status, ...pathParts] = line.trim().split(/\\\\s+/);\\n+        return {\\n+          status: status,\\n+          file: pathParts.join(' ')\\n+        };\\n+      });\\n+\\n+  git-diff-stats:\\n+    name: git-diff-stats\\n+    description: Get statistics about changes\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        base:\\n+          type: string\\n+          description: Base branch to compare against\\n+    exec: 'git diff --stat {{ args.base }}..HEAD'\\n+    transform_js: |\\n+      const lines = output.trim().split('\\\\n');\\n+      const summary = lines[lines.length - 1];\\n+      const match = summary.match(/(\\\\d+) files? changed(?:, (\\\\d+) insertions?)?(?:, (\\\\d+) deletions?)?/);\\n+      return {\\n+        filesChanged: parseInt(match?.[1] || '0'),\\n+        insertions: parseInt(match?.[2] || '0'),\\n+        deletions: parseInt(match?.[3] || '0')\\n+      };\\n+\\n+  git-log-recent:\\n+    name: git-log-recent\\n+    description: Get recent commit messages\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        count:\\n+          type: number\\n+          description: Number of commits to retrieve\\n+    exec: 'git log --oneline -n {{ args.count | default: 5 }}'\\n+    transform_js: |\\n+      const lines = output.trim().split('\\\\n').filter(l => l);\\n+      return lines.map(line => {\\n+        const [hash, ...messageParts] = line.split(/\\\\s+/);\\n+        return {\\n+          hash: hash,\\n+          message: messageParts.join(' ')\\n+        };\\n+      });\\n+\\n+  # Docker tools\\n+  docker-lint:\\n+    name: docker-lint\\n+    description: Lint Dockerfile for best practices\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        file:\\n+          type: string\\n+          description: Dockerfile to lint\\n+      required: [file]\\n+    exec: 'hadolint {{ args.file }} --format json || echo \\\"[]\\\"'\\n+    parseJson: true\\n+    transform_js: |\\n+      // Convert hadolint output to issues\\n+      return output.map(issue => ({\\n+        file: args.file,\\n+        line: issue.line || 0,\\n+        message: issue.message,\\n+        severity: issue.level === 'error' ? 'error' : issue.level === 'warning' ? 'warning' : 'info',\\n+        category: 'style',\\n+        ruleId: issue.code || 'docker-lint'\\n+      }));\\n+\\n+  docker-scan:\\n+    name: docker-scan\\n+    description: Scan Docker image for vulnerabilities\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        image:\\n+          type: string\\n+          description: Docker image to scan\\n+      required: [image]\\n+    exec: 'trivy image --format json --quiet {{ args.image }} || echo \\\"{}\\\"'\\n+    parseJson: true\\n+    timeout: 60000\\n+    transform_js: |\\n+      const vulnerabilities = [];\\n+      if (output.Results) {\\n+        for (const result of output.Results) {\\n+          if (result.Vulnerabilities) {\\n+            for (const vuln of result.Vulnerabilities) {\\n+              vulnerabilities.push({\\n+                file: result.Target || 'docker-image',\\n+                line: 0,\\n+                message: `${vuln.VulnerabilityID}: ${vuln.Title || vuln.Description}`,\\n+                severity: vuln.Severity?.toLowerCase() || 'info',\\n+                category: 'security',\\n+                ruleId: vuln.VulnerabilityID\\n+              });\\n+            }\\n+          }\\n+        }\\n+      }\\n+      return vulnerabilities;\\n+\\n+  # Package management tools\\n+  npm-audit:\\n+    name: npm-audit\\n+    description: Run npm security audit\\n+    exec: 'npm audit --json || echo \\\"{}\\\"'\\n+    parseJson: true\\n+    transform_js: |\\n+      const issues = [];\\n+      if (output.vulnerabilities) {\\n+        for (const [pkg, vuln] of Object.entries(output.vulnerabilities)) {\\n+          issues.push({\\n+            file: 'package.json',\\n+            line: 0,\\n+            message: `${pkg}: ${vuln.severity} severity vulnerability`,\\n+            severity: vuln.severity === 'high' || vuln.severity === 'critical' ? 'error' : 'warning',\\n+            category: 'security',\\n+            ruleId: `npm-${vuln.severity}`\\n+          });\\n+        }\\n+      }\\n+      return issues;\\n+\\n+  check-outdated:\\n+    name: check-outdated\\n+    description: Check for outdated dependencies\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        manager:\\n+          type: string\\n+          enum: [npm, pip, go]\\n+          description: Package manager to use\\n+    exec: |\\n+      {% if args.manager == \\\"npm\\\" %}\\n+        npm outdated --json || echo \\\"{}\\\"\\n+      {% elsif args.manager == \\\"pip\\\" %}\\n+        pip list --outdated --format json || echo \\\"[]\\\"\\n+      {% elsif args.manager == \\\"go\\\" %}\\n+        go list -u -m -json all || echo \\\"{}\\\"\\n+      {% else %}\\n+        echo \\\"{}\\\"\\n+      {% endif %}\\n+    parseJson: true\\n+    transform_js: |\\n+      const outdated = [];\\n+      if (args.manager === 'npm' && typeof output === 'object') {\\n+        for (const [pkg, info] of Object.entries(output)) {\\n+          if (info.wanted !== info.current) {\\n+            outdated.push({\\n+              package: pkg,\\n+              current: info.current,\\n+              wanted: info.wanted,\\n+              latest: info.latest\\n+            });\\n+          }\\n+        }\\n+      }\\n+      // Add handlers for pip and go...\\n+      return outdated;\\n+\\n+  # Testing tools\\n+  run-tests:\\n+    name: run-tests\\n+    description: Run test suite and parse results\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        command:\\n+          type: string\\n+          description: Test command to run\\n+        format:\\n+          type: string\\n+          enum: [jest, pytest, go]\\n+          description: Test output format\\n+      required: [command]\\n+    exec: '{{ args.command }} 2>&1'\\n+    timeout: 300000\\n+    transform_js: |\\n+      // Parse test output based on format\\n+      const lines = output.split('\\\\n');\\n+      let passed = 0, failed = 0, skipped = 0;\\n+\\n+      if (args.format === 'jest') {\\n+        const summary = lines.find(l => l.includes('Tests:'));\\n+        if (summary) {\\n+          const match = summary.match(/(\\\\d+) passed/);\\n+          if (match) passed = parseInt(match[1]);\\n+          const failMatch = summary.match(/(\\\\d+) failed/);\\n+          if (failMatch) failed = parseInt(failMatch[1]);\\n+        }\\n+      }\\n+\\n+      return {\\n+        passed: passed,\\n+        failed: failed,\\n+        skipped: skipped,\\n+        success: failed === 0\\n+      };\\n+\\n+  # Code quality tools\\n+  eslint-check:\\n+    name: eslint-check\\n+    description: Run ESLint on JavaScript/TypeScript files\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        files:\\n+          type: array\\n+          items:\\n+            type: string\\n+          description: Files to lint\\n+    exec: 'npx eslint --format json {{ args.files | join: \\\" \\\" }} || echo \\\"[]\\\"'\\n+    parseJson: true\\n+    transform_js: |\\n+      const issues = [];\\n+      for (const file of output) {\\n+        for (const message of file.messages || []) {\\n+          issues.push({\\n+            file: file.filePath,\\n+            line: message.line || 0,\\n+            endLine: message.endLine,\\n+            column: message.column,\\n+            endColumn: message.endColumn,\\n+            message: message.message,\\n+            severity: message.severity === 2 ? 'error' : 'warning',\\n+            category: 'style',\\n+            ruleId: message.ruleId || 'eslint'\\n+          });\\n+        }\\n+      }\\n+      return issues;\\n+\\n+  prettier-check:\\n+    name: prettier-check\\n+    description: Check code formatting with Prettier\\n+    inputSchema:\\n+      type: object\\n+      properties:\\n+        files:\\n+          type: array\\n+          items:\\n+            type: string\\n+          description: Files to check\\n+    exec: 'npx prettier --check {{ args.files | join: \\\" \\\" }} 2>&1'\\n+    transform_js: |\\n+      const unformatted = [];\\n+      const lines = output.split('\\\\n');\\n+      for (const line of lines) {\\n+        if (line.includes('[warn]') && line.includes('Code style issues found')) {\\n+          const match = line.match(/in (.+?)$/);\\n+          if (match) {\\n+            unformatted.push({\\n+              file: match[1],\\n+              line: 0,\\n+              message: 'File needs formatting',\\n+              severity: 'warning',\\n+              category: 'style',\\n+              ruleId: 'prettier'\\n+            });\\n+          }\\n+        }\\n+      }\\n+      return unformatted;\\n\\\\ No newline at end of file\\n\",\"status\":\"added\"},{\"filename\":\"examples/workflows/calculator-workflow.yaml\",\"additions\":5,\"deletions\":0,\"changes\":163,\"patch\":\"diff --git a/examples/workflows/calculator-workflow.yaml b/examples/workflows/calculator-workflow.yaml\\nnew file mode 100644\\nindex 00000000..05aab90e\\n--- /dev/null\\n+++ b/examples/workflows/calculator-workflow.yaml\\n@@ -0,0 +1,163 @@\\n+# Standalone executable workflow with inputs, outputs, and self-tests\\n+# Can be run directly: visor --config defaults/calculator-workflow.yaml\\n+# Or imported and used as a step in another config\\n+\\n+id: calculator\\n+name: Simple Calculator\\n+description: A simple calculator workflow that performs arithmetic operations\\n+version: \\\"1.0.0\\\"\\n+\\n+inputs:\\n+  - name: operation\\n+    description: The operation to perform (add, subtract, multiply, divide)\\n+    schema:\\n+      type: string\\n+      enum: [add, subtract, multiply, divide]\\n+    default: \\\"add\\\"\\n+\\n+  - name: a\\n+    description: First operand\\n+    schema:\\n+      type: number\\n+    default: 10\\n+\\n+  - name: b\\n+    description: Second operand\\n+    schema:\\n+      type: number\\n+    default: 5\\n+\\n+outputs:\\n+  - name: result\\n+    description: The calculated result\\n+    value_js: |\\n+      const calc = steps.calculate.output;\\n+      return calc ? calc.result : null;\\n+\\n+  - name: operation_performed\\n+    description: The operation that was performed\\n+    value_js: |\\n+      return inputs.operation;\\n+\\n+  - name: summary\\n+    description: Human readable summary\\n+    value_js: |\\n+      const calc = steps.calculate.output;\\n+      if (!calc) return \\\"Calculation failed\\\";\\n+      return `${inputs.a} ${inputs.operation} ${inputs.b} = ${calc.result}`;\\n+\\n+# Workflow steps - these define the workflow logic\\n+steps:\\n+  validate_inputs:\\n+    type: command\\n+    exec: |\\n+      echo '{\\\"valid\\\": true, \\\"a\\\": {{ inputs.a }}, \\\"b\\\": {{ inputs.b }}, \\\"operation\\\": \\\"{{ inputs.operation }}\\\"}'\\n+\\n+  calculate:\\n+    type: command\\n+    depends_on: [validate_inputs]\\n+    exec: |\\n+      #!/bin/bash\\n+      operation=\\\"{{ inputs.operation }}\\\"\\n+      a={{ inputs.a }}\\n+      b={{ inputs.b }}\\n+\\n+      case $operation in\\n+        add)\\n+          result=$(echo \\\"$a + $b\\\" | bc -l)\\n+          ;;\\n+        subtract)\\n+          result=$(echo \\\"$a - $b\\\" | bc -l)\\n+          ;;\\n+        multiply)\\n+          result=$(echo \\\"$a * $b\\\" | bc -l)\\n+          ;;\\n+        divide)\\n+          if [ \\\"$b\\\" = \\\"0\\\" ]; then\\n+            echo '{\\\"error\\\": \\\"Division by zero\\\"}'\\n+            exit 1\\n+          fi\\n+          result=$(echo \\\"scale=2; $a / $b\\\" | bc -l)\\n+          ;;\\n+        *)\\n+          echo '{\\\"error\\\": \\\"Unknown operation\\\"}'\\n+          exit 1\\n+          ;;\\n+      esac\\n+\\n+      echo \\\"{\\\\\\\"result\\\\\\\": $result, \\\\\\\"operation\\\\\\\": \\\\\\\"$operation\\\\\\\"}\\\"\\n+\\n+  log_result:\\n+    type: log\\n+    depends_on: [calculate]\\n+    message: |\\n+      Calculator Result:\\n+      Operation: {{ inputs.operation }}\\n+      {{ inputs.a }} {{ inputs.operation }} {{ inputs.b }} = {{ steps.calculate.output.result }}\\n+\\n+# Tests - these are ONLY executed when running this file standalone\\n+# They are NOT imported when this workflow is used as a step in another config\\n+tests:\\n+  # Test 1: Default inputs (10 + 5 = 15)\\n+  test_default_addition:\\n+    type: workflow\\n+    workflow: calculator\\n+    # Uses default inputs (operation=add, a=10, b=5)\\n+\\n+  # Test 2: Subtraction\\n+  test_subtraction:\\n+    type: workflow\\n+    workflow: calculator\\n+    args:\\n+      operation: subtract\\n+      a: 20\\n+      b: 8\\n+    # Expected: 20 - 8 = 12\\n+\\n+  # Test 3: Multiplication\\n+  test_multiplication:\\n+    type: workflow\\n+    workflow: calculator\\n+    args:\\n+      operation: multiply\\n+      a: 6\\n+      b: 7\\n+    # Expected: 6 * 7 = 42\\n+\\n+  # Test 4: Division\\n+  test_division:\\n+    type: workflow\\n+    workflow: calculator\\n+    args:\\n+      operation: divide\\n+      a: 100\\n+      b: 4\\n+    # Expected: 100 / 4 = 25\\n+\\n+  # Validate all tests passed\\n+  validate_tests:\\n+    type: command\\n+    depends_on: [test_default_addition, test_subtraction, test_multiplication, test_division]\\n+    exec: |\\n+      #!/bin/bash\\n+      # Get results from each test\\n+      addition=\\\"{{ outputs.test_default_addition.result }}\\\"\\n+      subtraction=\\\"{{ outputs.test_subtraction.result }}\\\"\\n+      multiplication=\\\"{{ outputs.test_multiplication.result }}\\\"\\n+      division=\\\"{{ outputs.test_division.result }}\\\"\\n+\\n+      echo \\\"Test Results:\\\"\\n+      echo \\\"  Addition (10 + 5): $addition (expected 15)\\\"\\n+      echo \\\"  Subtraction (20 - 8): $subtraction (expected 12)\\\"\\n+      echo \\\"  Multiplication (6 * 7): $multiplication (expected 42)\\\"\\n+      echo \\\"  Division (100 / 4): $division (expected 25)\\\"\\n+\\n+      # Simple validation\\n+      if [ \\\"$addition\\\" = \\\"15\\\" ] && [ \\\"$subtraction\\\" = \\\"12\\\" ] && [ \\\"$multiplication\\\" = \\\"42\\\" ] && [ \\\"$division\\\" = \\\"25\\\" ]; then\\n+        echo '{\\\"all_tests_passed\\\": true}'\\n+        exit 0\\n+      else\\n+        echo '{\\\"all_tests_passed\\\": false, \\\"error\\\": \\\"Some tests failed\\\"}'\\n+        exit 1\\n+      fi\\n+    fail_if: output.all_tests_passed === false\\n\",\"status\":\"added\"},{\"filename\":\"examples/workflows/code-quality.yaml\",\"additions\":6,\"deletions\":0,\"changes\":222,\"patch\":\"diff --git a/examples/workflows/code-quality.yaml b/examples/workflows/code-quality.yaml\\nnew file mode 100644\\nindex 00000000..2202f252\\n--- /dev/null\\n+++ b/examples/workflows/code-quality.yaml\\n@@ -0,0 +1,222 @@\\n+# Example reusable workflow for code quality checks\\n+id: code-quality\\n+name: Code Quality Workflow\\n+description: Comprehensive code quality checks including linting, complexity, and best practices\\n+version: \\\"1.0.0\\\"\\n+tags: [\\\"quality\\\", \\\"reusable\\\", \\\"linting\\\"]\\n+category: quality\\n+\\n+# Input parameters\\n+inputs:\\n+  - name: language\\n+    description: Programming language to analyze\\n+    schema:\\n+      type: string\\n+      enum: [\\\"javascript\\\", \\\"typescript\\\", \\\"python\\\", \\\"go\\\", \\\"java\\\"]\\n+    required: true\\n+\\n+  - name: complexity_threshold\\n+    description: Maximum allowed cyclomatic complexity\\n+    schema:\\n+      type: number\\n+      minimum: 1\\n+      maximum: 20\\n+    default: 10\\n+\\n+  - name: enable_formatting\\n+    description: Check code formatting\\n+    schema:\\n+      type: boolean\\n+    default: true\\n+\\n+  - name: custom_rules\\n+    description: Custom linting rules to apply\\n+    schema:\\n+      type: object\\n+      additionalProperties: true\\n+    required: false\\n+\\n+# Output parameters\\n+outputs:\\n+  - name: quality_score\\n+    description: Overall code quality score (0-100)\\n+    value_js: |\\n+      const lintScore = steps.linting?.output?.score || 0;\\n+      const complexityScore = steps.complexity?.output?.score || 0;\\n+      const formattingScore = steps.formatting?.output?.score || 100;\\n+      const weights = { lint: 0.4, complexity: 0.4, formatting: 0.2 };\\n+      return Math.round(\\n+        lintScore * weights.lint +\\n+        complexityScore * weights.complexity +\\n+        formattingScore * weights.formatting\\n+      );\\n+\\n+  - name: recommendations\\n+    description: List of recommendations for improvement\\n+    value_js: |\\n+      const recs = [];\\n+      if (steps.linting?.output?.issues?.length > 0) {\\n+        recs.push(`Fix ${steps.linting.output.issues.length} linting issues`);\\n+      }\\n+      if (steps.complexity?.output?.high_complexity_functions?.length > 0) {\\n+        recs.push(`Refactor ${steps.complexity.output.high_complexity_functions.length} complex functions`);\\n+      }\\n+      if (steps.formatting?.output?.issues?.length > 0) {\\n+        recs.push(`Fix ${steps.formatting.output.issues.length} formatting issues`);\\n+      }\\n+      return recs;\\n+\\n+  - name: detailed_report\\n+    description: Detailed quality report\\n+    value: |\\n+      Code Quality Report for {{ inputs.language }}:\\n+      ================================================\\n+      Overall Score: {{ outputs.quality_score }}/100\\n+\\n+      Linting Results:\\n+      - Issues found: {{ steps.linting.output.issues | size }}\\n+      - Score: {{ steps.linting.output.score }}/100\\n+\\n+      Complexity Analysis:\\n+      - High complexity functions: {{ steps.complexity.output.high_complexity_functions | size }}\\n+      - Average complexity: {{ steps.complexity.output.average_complexity }}\\n+\\n+      {% if inputs.enable_formatting %}\\n+      Formatting Check:\\n+      - Issues found: {{ steps.formatting.output.issues | size }}\\n+      - Score: {{ steps.formatting.output.score }}/100\\n+      {% endif %}\\n+\\n+      Recommendations:\\n+      {% for rec in outputs.recommendations %}\\n+      - {{ rec }}\\n+      {% endfor %}\\n+\\n+# Workflow steps\\n+steps:\\n+  linting:\\n+    type: ai\\n+    prompt: |\\n+      Perform linting analysis for {{ inputs.language }} code.\\n+      Check for:\\n+      - Syntax errors\\n+      - Code style violations\\n+      - Best practices violations\\n+      - Unused variables and imports\\n+      - Type errors (if applicable)\\n+\\n+      {% if inputs.custom_rules %}\\n+      Additional custom rules to check:\\n+      {{ inputs.custom_rules | json }}\\n+      {% endif %}\\n+\\n+      Provide a score from 0-100 based on the issues found.\\n+    focus: style\\n+    schema: code-review\\n+    inputs:\\n+      language:\\n+        source: param\\n+        value: language\\n+      custom_rules:\\n+        source: param\\n+        value: custom_rules\\n+\\n+  complexity:\\n+    type: ai\\n+    prompt: |\\n+      Analyze code complexity for {{ inputs.language }}.\\n+\\n+      Check for:\\n+      - Cyclomatic complexity (threshold: {{ inputs.complexity_threshold }})\\n+      - Cognitive complexity\\n+      - Nested conditionals\\n+      - Long functions/methods\\n+      - Too many parameters\\n+\\n+      Report:\\n+      - List of functions exceeding complexity threshold\\n+      - Average complexity across the codebase\\n+      - Provide a score from 0-100\\n+    focus: architecture\\n+    schema: code-review\\n+    depends_on: [linting]\\n+    inputs:\\n+      language:\\n+        source: param\\n+        value: language\\n+      complexity_threshold:\\n+        source: param\\n+        value: complexity_threshold\\n+\\n+  formatting:\\n+    type: command\\n+    exec: |\\n+      case \\\"{{ inputs.language }}\\\" in\\n+        javascript|typescript)\\n+          npx prettier --check \\\"**/*.{js,ts,jsx,tsx}\\\" --list-different || true\\n+          ;;\\n+        python)\\n+          python -m black --check . --diff || true\\n+          ;;\\n+        go)\\n+          gofmt -l . || true\\n+          ;;\\n+        java)\\n+          echo \\\"Java formatting check not implemented\\\"\\n+          ;;\\n+      esac\\n+    if: inputs.enable_formatting === true\\n+    depends_on: [complexity]\\n+    inputs:\\n+      language:\\n+        source: param\\n+        value: language\\n+      enable_formatting:\\n+        source: param\\n+        value: enable_formatting\\n+\\n+  best_practices:\\n+    type: ai\\n+    prompt: |\\n+      Review {{ inputs.language }} code for best practices:\\n+\\n+      - Design patterns usage\\n+      - SOLID principles adherence\\n+      - Error handling patterns\\n+      - Documentation quality\\n+      - Test coverage indicators\\n+      - Performance considerations\\n+\\n+      Provide actionable recommendations.\\n+    focus: architecture\\n+    depends_on: [formatting]\\n+    inputs:\\n+      language:\\n+        source: param\\n+        value: language\\n+\\n+  aggregate_results:\\n+    type: noop\\n+    depends_on: [best_practices]\\n+    # This step exists to ensure all previous steps complete\\n+    # The actual aggregation happens in the output parameters\\n+\\n+# Usage examples (documentation only):\\n+#\\n+# 1. TypeScript with strict settings:\\n+#    language: typescript\\n+#    complexity_threshold: 8\\n+#    enable_formatting: true\\n+#    custom_rules:\\n+#      no-any: error\\n+#      explicit-function-return-type: warning\\n+#\\n+# 2. Python basic check:\\n+#    language: python\\n+#    complexity_threshold: 15\\n+#    enable_formatting: false\\n+#\\n+# 3. Go with formatting:\\n+#    language: go\\n+#    complexity_threshold: 10\\n+#    enable_formatting: true\\n\\\\ No newline at end of file\\n\",\"status\":\"added\"},{\"filename\":\"examples/workflows/quick-pr-check.yaml\",\"additions\":3,\"deletions\":0,\"changes\":90,\"patch\":\"diff --git a/examples/workflows/quick-pr-check.yaml b/examples/workflows/quick-pr-check.yaml\\nnew file mode 100644\\nindex 00000000..3248f14f\\n--- /dev/null\\n+++ b/examples/workflows/quick-pr-check.yaml\\n@@ -0,0 +1,90 @@\\n+# Quick PR validation workflow\\n+# Performs basic checks for PR validation\\n+\\n+id: quick-pr-check\\n+name: Quick PR Check\\n+description: Fast workflow for initial PR validation\\n+version: \\\"1.0.0\\\"\\n+category: validation\\n+\\n+# Input parameters\\n+inputs:\\n+  - name: pr_type\\n+    description: Type of PR (feature, bugfix, hotfix)\\n+    schema:\\n+      type: string\\n+      enum: [\\\"feature\\\", \\\"bugfix\\\", \\\"hotfix\\\"]\\n+    required: true\\n+\\n+  - name: run_tests\\n+    description: Whether to run tests\\n+    schema:\\n+      type: boolean\\n+    default: true\\n+\\n+  - name: run_lint\\n+    description: Whether to run linting\\n+    schema:\\n+      type: boolean\\n+    default: true\\n+\\n+# Output parameters\\n+outputs:\\n+  - name: approval_status\\n+    description: Whether the PR passes quick checks\\n+    value_js: |\\n+      const testsPassed = !inputs.run_tests || steps.run_tests?.output?.success || false;\\n+      const lintPassed = !inputs.run_lint || steps.lint_check?.output?.passed || false;\\n+      return testsPassed && lintPassed ? \\\"approved\\\" : \\\"needs_work\\\";\\n+\\n+  - name: summary\\n+    description: Summary of the quick check\\n+    value: |\\n+      Quick Check Results for {{ inputs.pr_type }} PR:\\n+      {% if inputs.run_tests %}\\n+      - Tests: {{ steps.run_tests.output.success ? 'Passed' : 'Failed' }}\\n+      {% endif %}\\n+      {% if inputs.run_lint %}\\n+      - Linting: {{ steps.lint_check.output.passed ? 'Passed' : 'Failed' }}\\n+      {% endif %}\\n+      Overall Status: {{ outputs.approval_status }}\\n+\\n+# Workflow steps at root level\\n+steps:\\n+  run_tests:\\n+    type: command\\n+    exec: npm test\\n+    timeout: 300\\n+    if: inputs.run_tests === true\\n+\\n+  lint_check:\\n+    type: command\\n+    exec: npm run lint\\n+    depends_on: [run_tests]\\n+    if: inputs.run_lint === true\\n+\\n+  pr_summary:\\n+    type: log\\n+    message: |\\n+      Quick Check Results for {{ inputs.pr_type }} PR:\\n+      {% if inputs.run_tests %}\\n+      - Tests: {{ steps.run_tests.output.success ? 'Passed' : 'Failed' }}\\n+      {% endif %}\\n+      {% if inputs.run_lint %}\\n+      - Linting: {{ steps.lint_check.output.passed ? 'Passed' : 'Failed' }}\\n+      {% endif %}\\n+      Status: {{ outputs.approval_status }}\\n+    depends_on: [lint_check]\\n+    level: info\\n+\\n+# Usage examples (documentation only):\\n+#\\n+# 1. Full validation:\\n+#    pr_type: feature\\n+#    run_tests: true\\n+#    run_lint: true\\n+#\\n+# 2. Quick lint only:\\n+#    pr_type: hotfix\\n+#    run_tests: false\\n+#    run_lint: true\\n\\\\ No newline at end of file\\n\",\"status\":\"added\"},{\"filename\":\"examples/workflows/workflow-composition-example.yaml\",\"additions\":4,\"deletions\":0,\"changes\":130,\"patch\":\"diff --git a/examples/workflows/workflow-composition-example.yaml b/examples/workflows/workflow-composition-example.yaml\\nnew file mode 100644\\nindex 00000000..6e73b125\\n--- /dev/null\\n+++ b/examples/workflows/workflow-composition-example.yaml\\n@@ -0,0 +1,130 @@\\n+# Example demonstrating workflow composition and state isolation\\n+# This config imports the calculator workflow and uses it as a step\\n+# Demonstrates that:\\n+# 1. Workflow tests are NOT auto-executed when imported\\n+# 2. Workflow steps run in isolated context\\n+# 3. State does not leak between workflow and parent config\\n+\\n+version: \\\"1.0\\\"\\n+\\n+# Import the calculator workflow\\n+imports:\\n+  - ./calculator-workflow.yaml\\n+\\n+# Main checks/steps in this config\\n+checks:\\n+  # Step 1: Set some state in parent context\\n+  parent_state_setup:\\n+    type: memory\\n+    operation: set\\n+    key: parent_value\\n+    value: \\\"I am from parent context\\\"\\n+\\n+  # Step 2: Use the calculator workflow as an isolated step\\n+  use_calculator:\\n+    type: workflow\\n+    workflow: calculator\\n+    depends_on: [parent_state_setup]\\n+    args:\\n+      operation: multiply\\n+      a: 7\\n+      b: 6\\n+\\n+  # Step 3: Verify calculator result is accessible\\n+  verify_calculator_result:\\n+    type: command\\n+    depends_on: [use_calculator]\\n+    exec: |\\n+      #!/bin/bash\\n+      result=\\\"{{ outputs.use_calculator.result }}\\\"\\n+      summary=\\\"{{ outputs.use_calculator.summary }}\\\"\\n+\\n+      echo \\\"Calculator returned: $result\\\"\\n+      echo \\\"Summary: $summary\\\"\\n+\\n+      if [ \\\"$result\\\" = \\\"42\\\" ]; then\\n+        echo '{\\\"validation\\\": \\\"passed\\\", \\\"result\\\": '$result'}'\\n+        exit 0\\n+      else\\n+        echo '{\\\"validation\\\": \\\"failed\\\", \\\"expected\\\": 42, \\\"got\\\": '$result'}'\\n+        exit 1\\n+      fi\\n+\\n+  # Step 4: Verify parent state is still intact (not affected by workflow)\\n+  verify_parent_state:\\n+    type: memory\\n+    operation: get\\n+    key: parent_value\\n+    depends_on: [use_calculator]\\n+\\n+  # Step 5: Test state isolation - workflow should NOT have access to parent memory\\n+  test_state_isolation:\\n+    type: command\\n+    depends_on: [verify_parent_state]\\n+    exec: |\\n+      #!/bin/bash\\n+      parent_value=\\\"{{ outputs.verify_parent_state }}\\\"\\n+\\n+      echo \\\"Parent state verification:\\\"\\n+      echo \\\"  parent_value = $parent_value\\\"\\n+\\n+      if [ \\\"$parent_value\\\" = \\\"I am from parent context\\\" ]; then\\n+        echo '{\\\"isolation_test\\\": \\\"passed\\\", \\\"message\\\": \\\"Parent state intact\\\"}'\\n+        exit 0\\n+      else\\n+        echo '{\\\"isolation_test\\\": \\\"failed\\\", \\\"message\\\": \\\"Parent state was modified\\\"}'\\n+        exit 1\\n+      fi\\n+\\n+  # Step 6: Use calculator again with different inputs (reusability test)\\n+  use_calculator_again:\\n+    type: workflow\\n+    workflow: calculator\\n+    depends_on: [test_state_isolation]\\n+    args:\\n+      operation: add\\n+      a: 100\\n+      b: 23\\n+\\n+  # Step 7: Verify both calculator invocations produced correct results\\n+  verify_multiple_invocations:\\n+    type: command\\n+    depends_on: [use_calculator_again]\\n+    exec: |\\n+      #!/bin/bash\\n+      first_result=\\\"{{ outputs.use_calculator.result }}\\\"\\n+      second_result=\\\"{{ outputs.use_calculator_again.result }}\\\"\\n+\\n+      echo \\\"Multiple invocation test:\\\"\\n+      echo \\\"  First call (7 * 6): $first_result (expected 42)\\\"\\n+      echo \\\"  Second call (100 + 23): $second_result (expected 123)\\\"\\n+\\n+      if [ \\\"$first_result\\\" = \\\"42\\\" ] && [ \\\"$second_result\\\" = \\\"123\\\" ]; then\\n+        echo '{\\\"multiple_invocations\\\": \\\"passed\\\"}'\\n+        exit 0\\n+      else\\n+        echo '{\\\"multiple_invocations\\\": \\\"failed\\\"}'\\n+        exit 1\\n+      fi\\n+\\n+  # Final summary\\n+  summary:\\n+    type: log\\n+    depends_on: [verify_multiple_invocations]\\n+    message: |\\n+      ========================================\\n+      Workflow Composition Test Summary\\n+      ========================================\\n+\\n+      ‚úì Calculator workflow imported successfully\\n+      ‚úì Workflow tests were NOT auto-executed\\n+      ‚úì Calculator step executed in isolation\\n+      ‚úì Results: {{ outputs.use_calculator.summary }}\\n+      ‚úì Parent state remained intact\\n+      ‚úì Multiple workflow invocations work correctly\\n+\\n+      First calculation: {{ outputs.use_calculator.result }}\\n+      Second calculation: {{ outputs.use_calculator_again.result }}\\n+\\n+      All state isolation tests PASSED!\\n+      ========================================\\n\",\"status\":\"added\"},{\"filename\":\"package-lock.json\",\"additions\":1,\"deletions\":1,\"changes\":8,\"patch\":\"diff --git a/package-lock.json b/package-lock.json\\nindex bd02b99e..67e7b4ca 100644\\n--- a/package-lock.json\\n+++ b/package-lock.json\\n@@ -16,7 +16,7 @@\\n         \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n         \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n         \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc161\\\",\\n+        \\\"@probelabs/probe\\\": \\\"^0.6.0-rc164\\\",\\n         \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n         \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n         \\\"ajv\\\": \\\"^8.17.1\\\",\\n@@ -5946,9 +5946,9 @@\\n       }\\n     },\\n     \\\"node_modules/@probelabs/probe\\\": {\\n-      \\\"version\\\": \\\"0.6.0-rc161\\\",\\n-      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc161.tgz\\\",\\n-      \\\"integrity\\\": \\\"sha512-zXHNoLSfWTg+3kH67rAjzgGMD2W7azdBhJW+4JH7wKkpyaGa3cEY+u5ngxPTcWqhR15HHhYEwoY6gNiI/x3tiQ==\\\",\\n+      \\\"version\\\": \\\"0.6.0-rc164\\\",\\n+      \\\"resolved\\\": \\\"https://registry.npmjs.org/@probelabs/probe/-/probe-0.6.0-rc164.tgz\\\",\\n+      \\\"integrity\\\": \\\"sha512-/SBrlYCx6sliKgKV3ywUzFgVH9fWc2O94sm3StNpDFU3FK9cXvuRWaS9t9oO2cGv85/1pCBKW1ptlCTDWZu15w==\\\",\\n       \\\"hasInstallScript\\\": true,\\n       \\\"license\\\": \\\"ISC\\\",\\n       \\\"dependencies\\\": {\\n\",\"status\":\"modified\"},{\"filename\":\"package.json\",\"additions\":1,\"deletions\":1,\"changes\":5,\"patch\":\"diff --git a/package.json b/package.json\\nindex f1716c52..7aa07f43 100644\\n--- a/package.json\\n+++ b/package.json\\n@@ -31,13 +31,14 @@\\n     \\\"registry\\\": \\\"https://registry.npmjs.org/\\\"\\n   },\\n   \\\"scripts\\\": {\\n-    \\\"build:cli\\\": \\\"ncc build src/index.ts -o dist && cp -r defaults dist/ && cp -r output dist/ && cp -r src/debug-visualizer/ui dist/debug-visualizer/ && node scripts/inject-version.js && echo '#!/usr/bin/env node' | cat - dist/index.js > temp && mv temp dist/index.js && chmod +x dist/index.js\\\",\\n+    \\\"build:cli\\\": \\\"ncc build src/index.ts -o dist && cp -r defaults dist/ && cp -r output dist/ && cp -r docs dist/ && cp -r examples dist/ && cp -r src/debug-visualizer/ui dist/debug-visualizer/ && node scripts/inject-version.js && echo '#!/usr/bin/env node' | cat - dist/index.js > temp && mv temp dist/index.js && chmod +x dist/index.js\\\",\\n     \\\"build:sdk\\\": \\\"tsup src/sdk.ts --dts --sourcemap --format esm,cjs --out-dir dist/sdk\\\",\\n     \\\"build\\\": \\\"npm run build:cli && npm run build:sdk\\\",\\n     \\\"test\\\": \\\"jest && npm run test:yaml\\\",\\n     \\\"prepublishOnly\\\": \\\"npm run build\\\",\\n     \\\"test:watch\\\": \\\"jest --watch\\\",\\n     \\\"test:coverage\\\": \\\"jest --coverage\\\",\\n+    \\\"test:manual:bash\\\": \\\"RUN_MANUAL_TESTS=true jest tests/manual/bash-config-manual.test.ts\\\",\\n     \\\"lint\\\": \\\"eslint src tests --ext .ts\\\",\\n     \\\"lint:fix\\\": \\\"eslint src tests --ext .ts --fix\\\",\\n     \\\"format\\\": \\\"prettier --write src tests\\\",\\n@@ -95,7 +96,7 @@\\n     \\\"@octokit/auth-app\\\": \\\"^8.1.0\\\",\\n     \\\"@octokit/core\\\": \\\"^7.0.3\\\",\\n     \\\"@octokit/rest\\\": \\\"^22.0.0\\\",\\n-    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc161\\\",\\n+    \\\"@probelabs/probe\\\": \\\"^0.6.0-rc164\\\",\\n     \\\"@types/commander\\\": \\\"^2.12.0\\\",\\n     \\\"@types/uuid\\\": \\\"^10.0.0\\\",\\n     \\\"ajv\\\": \\\"^8.17.1\\\",\\n\",\"status\":\"modified\"},{\"filename\":\"scripts/run-visor-tests.js\",\"additions\":1,\"deletions\":1,\"changes\":61,\"patch\":\"diff --git a/scripts/run-visor-tests.js b/scripts/run-visor-tests.js\\nindex c6724abf..573f17d8 100755\\n--- a/scripts/run-visor-tests.js\\n+++ b/scripts/run-visor-tests.js\\n@@ -14,22 +14,23 @@ function main() {\\n   const repoRoot = path.resolve(__dirname, '..');\\n   const distCli = path.join(repoRoot, 'dist', 'index.js');\\n   const srcCli = path.join(repoRoot, 'src', 'index.ts');\\n-  // Prefer new non-dot tests filename; test runner still discovers legacy name\\n-  const testsPath = process.env.VISOR_TESTS_PATH || path.join(repoRoot, 'defaults', 'visor.tests.yaml');\\n+  // Prefer new non-dot tests filename; allow multiple suites\\n+  const primarySuite = process.env.VISOR_TESTS_PATH || path.join(repoRoot, 'defaults', 'visor.tests.yaml');\\n+  const refinementSuite = path.join(repoRoot, 'defaults', 'task-refinement.yaml');\\n   const isCI = process.env.CI === 'true' || process.env.GITHUB_ACTIONS === 'true';\\n \\n   let nodeArgs = [];\\n-  let argv = [];\\n+  const baseArgs = [];\\n   if (!isCI) {\\n     // Prefer TypeScript source in local/dev for correctness\\n     try {\\n       require.resolve('ts-node/register');\\n       nodeArgs = ['-r', 'ts-node/register'];\\n-      argv = [srcCli, 'test', '--config', testsPath, '--progress', 'compact'];\\n+      baseArgs.push(srcCli);\\n     } catch (_) {\\n       // Fallback to dist if ts-node is not installed\\n       if (fs.existsSync(distCli)) {\\n-        argv = [distCli, 'test', '--config', testsPath, '--progress', 'compact'];\\n+        baseArgs.push(distCli);\\n       } else {\\n         console.error('Neither ts-node nor dist/index.js found. Run `npm run build:cli` first.');\\n         process.exit(2);\\n@@ -39,12 +40,12 @@ function main() {\\n     // In CI we always use the freshly built dist\\n     // Fall back to ts-node\\n     if (fs.existsSync(distCli)) {\\n-      argv = [distCli, 'test', '--config', testsPath, '--progress', 'compact'];\\n+      baseArgs.push(distCli);\\n     } else {\\n       try {\\n         require.resolve('ts-node/register');\\n         nodeArgs = ['-r', 'ts-node/register'];\\n-        argv = [srcCli, 'test', '--config', testsPath, '--progress', 'compact'];\\n+        baseArgs.push(srcCli);\\n       } catch (e) {\\n         console.error('Build artifacts missing and ts-node not available.');\\n         process.exit(2);\\n@@ -52,29 +53,39 @@ function main() {\\n     }\\n   }\\n \\n-  if (isCI) {\\n-    const outDir = path.join(repoRoot, 'output');\\n-    try { fs.mkdirSync(outDir, { recursive: true }); } catch {}\\n-    argv.push('--json', path.join(outDir, 'visor-tests.json'));\\n-    argv.push('--report', `junit:${path.join(outDir, 'visor-tests.xml')}`);\\n-    argv.push('--summary', `md:${path.join(outDir, 'visor-tests.md')}`);\\n-  }\\n-\\n   // Ensure VISOR_DEBUG is not noisy in CI\\n   const env = { ...process.env };\\n   if (isCI && env.VISOR_DEBUG === 'true') delete env.VISOR_DEBUG;\\n \\n-  const res = spawnSync(process.execPath, [...nodeArgs, ...argv], {\\n-    stdio: 'inherit',\\n-    env,\\n-    cwd: repoRoot,\\n-  });\\n-  if (typeof res.status === 'number') process.exit(res.status);\\n-  if (res.error) {\\n-    console.error(res.error);\\n-    process.exit(1);\\n+  const suites = [primarySuite];\\n+  if (fs.existsSync(refinementSuite)) suites.push(refinementSuite);\\n+\\n+  let exitCode = 0;\\n+  const outDir = path.join(repoRoot, 'output');\\n+  if (isCI) { try { fs.mkdirSync(outDir, { recursive: true }); } catch {}\\n+  }\\n+\\n+  for (const suite of suites) {\\n+    const label = path.basename(suite).replace(/\\\\.[^.]+$/, '');\\n+    const args = [...baseArgs, 'test', '--config', suite, '--progress', 'compact'];\\n+    if (isCI) {\\n+      args.push('--json', path.join(outDir, `${label}.json`));\\n+      args.push('--report', `junit:${path.join(outDir, `${label}.xml`)}`);\\n+      args.push('--summary', `md:${path.join(outDir, `${label}.md`)}`);\\n+    }\\n+    const res = spawnSync(process.execPath, [...nodeArgs, ...args], {\\n+      stdio: 'inherit',\\n+      env,\\n+      cwd: repoRoot,\\n+    });\\n+    if (typeof res.status === 'number' && res.status !== 0) exitCode = res.status;\\n+    if (res.error) {\\n+      console.error(res.error);\\n+      exitCode = 1;\\n+    }\\n   }\\n-  process.exit(0);\\n+\\n+  process.exit(exitCode);\\n }\\n \\n main();\\n\",\"status\":\"modified\"},{\"filename\":\"src/ai-review-service.ts\",\"additions\":1,\"deletions\":1,\"changes\":60,\"patch\":\"diff --git a/src/ai-review-service.ts b/src/ai-review-service.ts\\nindex 7dc1c458..7ba846ec 100644\\n--- a/src/ai-review-service.ts\\n+++ b/src/ai-review-service.ts\\n@@ -30,6 +30,7 @@ interface TracedProbeAgentOptions extends ProbeAgentOptions {\\n   tracer?: unknown; // SimpleTelemetry tracer\\n   _telemetryConfig?: unknown; // SimpleTelemetry config\\n   _traceFilePath?: string;\\n+  customPrompt?: string;\\n }\\n \\n export interface AIReviewConfig {\\n@@ -43,12 +44,26 @@ export interface AIReviewConfig {\\n   mcpServers?: Record<string, import('./types/config').McpServerConfig>;\\n   // Enable delegate tool for task distribution to subagents\\n   enableDelegate?: boolean;\\n+  // ProbeAgent persona/prompt family (e.g., 'engineer', 'code-review', 'architect')\\n+  promptType?: string;\\n+  // System prompt to prepend (baseline/preamble). Replaces legacy customPrompt\\n+  systemPrompt?: string;\\n+  // Backward-compat: legacy key still accepted internally\\n+  customPrompt?: string;\\n   // Retry configuration for AI provider calls\\n   retry?: import('./types/config').AIRetryConfig;\\n   // Fallback configuration for provider failures\\n   fallback?: import('./types/config').AIFallbackConfig;\\n   // Enable Edit and Create tools for file modification\\n   allowEdit?: boolean;\\n+  // Filter allowed tools - supports whitelist, exclusion (!prefix), or raw AI mode (empty array)\\n+  allowedTools?: string[];\\n+  // Disable all tools for raw AI mode (alternative to allowedTools: [])\\n+  disableTools?: boolean;\\n+  // Enable bash command execution (shorthand for bashConfig.enabled)\\n+  allowBash?: boolean;\\n+  // Advanced bash command execution configuration\\n+  bashConfig?: import('./types/config').BashConfig;\\n }\\n \\n export interface AIDebugInfo {\\n@@ -123,6 +138,16 @@ export class AIReviewService {\\n \\n     this.sessionRegistry = SessionRegistry.getInstance();\\n \\n+    // If debug was not explicitly provided, honor standard env flags so tests/CLI\\n+    // can enable provider-level debug without modifying per-check configs.\\n+    if (typeof this.config.debug === 'undefined') {\\n+      try {\\n+        if (process.env.VISOR_PROVIDER_DEBUG === 'true' || process.env.VISOR_DEBUG === 'true') {\\n+          this.config.debug = true;\\n+        }\\n+      } catch {}\\n+    }\\n+\\n     // Respect explicit provider if set (e.g., 'mock' during tests) ‚Äî do not override from env\\n     const providerExplicit =\\n       typeof this.config.provider === 'string' && this.config.provider.length > 0;\\n@@ -178,7 +203,10 @@ export class AIReviewService {\\n     const timestamp = new Date().toISOString();\\n \\n     // Build prompt from custom instructions\\n-    const prompt = await this.buildCustomPrompt(prInfo, customPrompt, schema);\\n+    // Respect provider-level skip_code_context by skipping PR context wrapper when requested\\n+    const prompt = await this.buildCustomPrompt(prInfo, customPrompt, schema, {\\n+      skipPRContext: (this.config as any)?.skip_code_context === true,\\n+    });\\n \\n     log(`Executing AI review with ${this.config.provider} provider...`);\\n     log(`üîß Debug: Raw schema parameter: ${JSON.stringify(schema)} (type: ${typeof schema})`);\\n@@ -1361,11 +1389,22 @@ ${'='.repeat(60)}\\n         // No need to set apiKey as it uses AWS SDK authentication\\n         // ProbeAgent will check for AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, etc.\\n       }\\n+      const explicitPromptType = (process.env.VISOR_PROMPT_TYPE || '').trim();\\n       const options: TracedProbeAgentOptions = {\\n         sessionId: sessionId,\\n-        promptType: schema ? ('code-review-template' as 'code-review') : undefined,\\n+        // Prefer config promptType, then env override, else fallback to code-review when schema is set\\n+        promptType:\\n+          this.config.promptType && this.config.promptType.trim()\\n+            ? (this.config.promptType.trim() as any)\\n+            : explicitPromptType\\n+              ? (explicitPromptType as any)\\n+              : schema === 'code-review'\\n+                ? ('code-review-template' as any)\\n+                : undefined,\\n         allowEdit: false, // We don't want the agent to modify files\\n         debug: this.config.debug || false,\\n+        // Map systemPrompt to Probe customPrompt until SDK exposes a first-class field\\n+        customPrompt: this.config.systemPrompt || this.config.customPrompt,\\n       };\\n \\n       // Enable tracing in debug mode for better diagnostics\\n@@ -1407,6 +1446,23 @@ ${'='.repeat(60)}\\n         (options as any).allowEdit = this.config.allowEdit;\\n       }\\n \\n+      // Pass tool filtering options to ProbeAgent\\n+      if (this.config.allowedTools !== undefined) {\\n+        (options as any).allowedTools = this.config.allowedTools;\\n+      }\\n+      if (this.config.disableTools !== undefined) {\\n+        (options as any).disableTools = this.config.disableTools;\\n+      }\\n+\\n+      // Pass bash command execution configuration to ProbeAgent\\n+      // Pass allowBash and bashConfig separately (following allowEdit pattern)\\n+      if (this.config.allowBash !== undefined) {\\n+        (options as any).allowBash = this.config.allowBash;\\n+      }\\n+      if (this.config.bashConfig !== undefined) {\\n+        (options as any).bashConfig = this.config.bashConfig;\\n+      }\\n+\\n       // Add provider-specific options if configured\\n       if (this.config.provider) {\\n         // Map claude-code to anthropic for ProbeAgent compatibility\\n\",\"status\":\"modified\"},{\"filename\":\"src/check-execution-engine.ts\",\"additions\":1,\"deletions\":204,\"changes\":7729,\"patch\":\"diff --git a/src/check-execution-engine.ts b/src/check-execution-engine.ts\\nindex 49927451..5744d5fc 100644\\n--- a/src/check-execution-engine.ts\\n+++ b/src/check-execution-engine.ts\\n@@ -1,111 +1,36 @@\\n-import {\\n-  PRReviewer,\\n-  ReviewSummary,\\n-  ReviewOptions,\\n-  GroupedCheckResults,\\n-  CheckResult,\\n-  ReviewIssue,\\n-} from './reviewer';\\n-import { GitRepositoryAnalyzer, GitRepositoryInfo } from './git-repository-analyzer';\\n-import { AnalysisResult } from './output-formatters';\\n-import { PRInfo } from './pr-analyzer';\\n-import { PRAnalyzer } from './pr-analyzer';\\n-import { CheckProviderRegistry } from './providers/check-provider-registry';\\n-import { CheckProviderConfig } from './providers/check-provider.interface';\\n-import { DependencyResolver, DependencyGraph } from './dependency-resolver';\\n-import { FailureConditionEvaluator } from './failure-condition-evaluator';\\n-import { FailureConditionResult, CheckConfig } from './types/config';\\n-import { GitHubCheckService, CheckRunOptions } from './github-check-service';\\n-import { IssueFilter } from './issue-filter';\\n-import { logger } from './logger';\\n-import Sandbox from '@nyariv/sandboxjs';\\n-import { ExecutionJournal, ScopePath, ContextView } from './snapshot-store';\\n-import { createSecureSandbox, compileAndRun } from './utils/sandbox';\\n-import {\\n-  projectOutputs as ofProject,\\n-  decideRouting as ofDecide,\\n-  computeAllValid as ofAllValid,\\n-  runOnFinishChildren as ofRunChildren,\\n-} from './engine/on-finish/orchestrator';\\n-import { composeOnFinishContext as ofComposeCtx } from './engine/on-finish/utils';\\n-import { VisorConfig, OnFailConfig, OnSuccessConfig, OnFinishConfig } from './types/config';\\n-import {\\n-  createPermissionHelpers,\\n-  detectLocalMode,\\n-  resolveAssociationFromEvent,\\n-} from './utils/author-permissions';\\n-import { MemoryStore } from './memory-store';\\n-import { emitNdjsonSpanWithEvents, emitNdjsonFallback } from './telemetry/fallback-ndjson';\\n-import { addEvent, withActiveSpan } from './telemetry/trace-helpers';\\n-import { addFailIfTriggered } from './telemetry/metrics';\\n-\\n-type ExtendedReviewSummary = ReviewSummary & {\\n-  output?: unknown;\\n-  content?: string;\\n-  isForEach?: boolean;\\n-  forEachItems?: unknown[];\\n-  // Preserve per-item results for forEach-dependent checks so children can gate per item\\n-  forEachItemResults?: ReviewSummary[];\\n-  // Per-item fatal mask: true means this item is fatal/should gate descendants\\n-  forEachFatalMask?: boolean[];\\n-};\\n-\\n /**\\n- * Statistics for a single check execution\\n+ * Legacy compatibility layer\\n+ *\\n+ * This file re-exports StateMachineExecutionEngine as CheckExecutionEngine\\n+ * to maintain backward compatibility with existing test files.\\n+ *\\n+ * @deprecated Use StateMachineExecutionEngine directly from state-machine-execution-engine.ts\\n+ *\\n+ * Migration path:\\n+ * - Old: import { CheckExecutionEngine } from './check-execution-engine'\\n+ * - New: import { StateMachineExecutionEngine } from './state-machine-execution-engine'\\n+ *\\n+ * This compatibility layer will be removed in a future version once all tests\\n+ * have been migrated to use the new state machine engine directly.\\n  */\\n-export interface CheckExecutionStats {\\n-  checkName: string;\\n-  totalRuns: number; // How many times the check executed (1 or forEach iterations)\\n-  successfulRuns: number;\\n-  failedRuns: number;\\n-  skipped: boolean;\\n-  skipReason?: 'if_condition' | 'fail_fast' | 'dependency_failed';\\n-  skipCondition?: string; // The actual if condition text\\n-  totalDuration: number; // Total duration in milliseconds\\n-  // Provider/self time (excludes time spent running routed children/descendants)\\n-  providerDurationMs?: number;\\n-  perIterationDuration?: number[]; // Duration for each iteration (if forEach)\\n-  issuesFound: number;\\n-  issuesBySeverity: {\\n-    critical: number;\\n-    error: number;\\n-    warning: number;\\n-    info: number;\\n-  };\\n-  outputsProduced?: number; // Number of outputs for forEach checks\\n-  errorMessage?: string; // Error message if failed\\n-  forEachPreview?: string[]; // Preview of forEach items processed (first few)\\n-}\\n \\n-/**\\n- * Overall execution statistics for all checks\\n- */\\n-export interface ExecutionStatistics {\\n-  totalChecksConfigured: number;\\n-  totalExecutions: number; // Sum of all runs including forEach iterations\\n-  successfulExecutions: number;\\n-  failedExecutions: number;\\n-  skippedChecks: number;\\n-  totalDuration: number;\\n-  checks: CheckExecutionStats[];\\n-}\\n+// Re-export the state machine engine as CheckExecutionEngine\\n+export { StateMachineExecutionEngine as CheckExecutionEngine } from './state-machine-execution-engine';\\n \\n-/**\\n- * Result of executing checks, including both the grouped results and execution statistics\\n- */\\n-export interface ExecutionResult {\\n-  results: GroupedCheckResults;\\n-  statistics: ExecutionStatistics;\\n-}\\n+// Re-export types from types/execution\\n+export type {\\n+  CheckExecutionOptions,\\n+  ExecutionResult,\\n+  CheckExecutionStats,\\n+  ExecutionStatistics,\\n+} from './types/execution';\\n \\n /**\\n- * Filter environment variables to only include safe ones for sandbox evaluation\\n+ * Mock Octokit interface for testing\\n+ *\\n+ * @deprecated This type is preserved for backward compatibility but should not be used.\\n+ * Use createMockOctokit() from test utilities instead.\\n  */\\n-function getSafeEnvironmentVariables(): Record<string, string> {\\n-  const { buildSandboxEnv } = require('./utils/env-exposure');\\n-  return buildSandboxEnv(process.env);\\n-}\\n-\\n export interface MockOctokit {\\n   rest: {\\n     pulls: {\\n@@ -133,7605 +58,3 @@ export interface MockOctokit {\\n   };\\n   auth: () => Promise<{ token: string }>;\\n }\\n-\\n-export interface CheckExecutionOptions {\\n-  checks: string[];\\n-  workingDirectory?: string;\\n-  showDetails?: boolean;\\n-  timeout?: number;\\n-  maxParallelism?: number; // Maximum number of checks to run in parallel (default: 3)\\n-  failFast?: boolean; // Stop execution when any check fails (default: false)\\n-  outputFormat?: string;\\n-  config?: import('./types/config').VisorConfig;\\n-  debug?: boolean; // Enable debug mode to collect AI execution details\\n-  // Tag filter for selective check execution\\n-  tagFilter?: import('./types/config').TagFilter;\\n-  // Webhook context for passing webhook data to http_input providers\\n-  webhookContext?: {\\n-    webhookData: Map<string, unknown>;\\n-  };\\n-  // GitHub Check integration options\\n-  githubChecks?: {\\n-    enabled: boolean;\\n-    octokit?: import('@octokit/rest').Octokit;\\n-    owner?: string;\\n-    repo?: string;\\n-    headSha?: string;\\n-    prNumber?: number;\\n-  };\\n-}\\n-\\n-export class CheckExecutionEngine {\\n-  private gitAnalyzer: GitRepositoryAnalyzer;\\n-  private mockOctokit: MockOctokit;\\n-  private reviewer: PRReviewer;\\n-  private providerRegistry: CheckProviderRegistry;\\n-  private failureEvaluator: FailureConditionEvaluator;\\n-  private githubCheckService?: GitHubCheckService;\\n-  private checkRunMap?: Map<string, { id: number; url: string }>;\\n-  private githubContext?: { owner: string; repo: string };\\n-  private workingDirectory: string;\\n-  private config?: import('./types/config').VisorConfig;\\n-  private webhookContext?: { webhookData: Map<string, unknown> };\\n-  private routingSandbox?: Sandbox;\\n-  private executionStats: Map<string, CheckExecutionStats> = new Map();\\n-  // Track history of all outputs for each check (useful for loops and goto)\\n-  private outputHistory: Map<string, unknown[]> = new Map();\\n-  // Track on_finish loop counts per forEach parent during a single execution run\\n-  private onFinishLoopCounts: Map<string, number> = new Map();\\n-  // Track how many times a forEach parent check has produced an array during this run (\\\"waves\\\")\\n-  private forEachWaveCounts: Map<string, number> = new Map();\\n-  // One-shot guards for post on_finish scheduling to avoid duplicate replies when\\n-  // multiple signals (aggregator, memory, history) agree. Keyed by session + parent check.\\n-  private postOnFinishGuards: Set<string> = new Set();\\n-  // Snapshot+Scope journal (Phase 0: commit only, no behavior changes yet)\\n-  private journal: ExecutionJournal = new ExecutionJournal();\\n-  private sessionId: string = `sess-${Date.now().toString(36)}-${Math.random()\\n-    .toString(36)\\n-    .slice(2, 8)}`;\\n-  // Dedup forward-run targets within a single grouped run (stage/event).\\n-  // Keyed by `${event}:${target}`.\\n-  private forwardRunGuards: Set<string> = new Set();\\n-  // Track per-grouped-run scheduling of specific steps we want to allow only once.\\n-  // Currently used to ensure 'validate-fact' is scheduled at most once per stage.\\n-  private oncePerRunScheduleGuards: Set<string> = new Set();\\n-  // Event override to simulate alternate event (used during routing goto)\\n-  private routingEventOverride?: import('./types/config').EventTrigger;\\n-  // Execution context for providers (CLI message, hooks, etc.)\\n-  private executionContext?: import('./providers/check-provider.interface').ExecutionContext;\\n-  // Cached GitHub context for context elevation when running in Actions\\n-  private actionContext?: {\\n-    owner: string;\\n-    repo: string;\\n-    octokit?: import('@octokit/rest').Octokit;\\n-  };\\n-\\n-  constructor(workingDirectory?: string, octokit?: import('@octokit/rest').Octokit) {\\n-    this.workingDirectory = workingDirectory || process.cwd();\\n-    this.gitAnalyzer = new GitRepositoryAnalyzer(this.workingDirectory);\\n-    this.providerRegistry = CheckProviderRegistry.getInstance();\\n-    this.failureEvaluator = new FailureConditionEvaluator();\\n-\\n-    // If authenticated octokit is provided, cache it for provider use\\n-    if (octokit) {\\n-      const repoEnv = process.env.GITHUB_REPOSITORY || '';\\n-      const [owner, repo] = repoEnv.split('/') as [string, string];\\n-      if (owner && repo) {\\n-        this.actionContext = { owner, repo, octokit };\\n-      }\\n-    }\\n-\\n-    // Create a mock Octokit instance for local analysis\\n-    // This allows us to reuse the existing PRReviewer logic without network calls\\n-    this.mockOctokit = this.createMockOctokit();\\n-    // Prefer the provided authenticated/recording Octokit (from test runner or Actions)\\n-    // so that comment create/update operations are visible to recorders and assertions.\\n-    const reviewerOctokit =\\n-      (octokit as unknown as import('@octokit/rest').Octokit) ||\\n-      (this.mockOctokit as unknown as import('@octokit/rest').Octokit);\\n-    this.reviewer = new PRReviewer(reviewerOctokit);\\n-  }\\n-\\n-  private sessionUUID(): string {\\n-    return this.sessionId;\\n-  }\\n-\\n-  /**\\n-   * Reset per-run guard and statistics state. Callers that orchestrate grouped\\n-   * executions (e.g., the YAML test runner) can invoke this to ensure clean\\n-   * stage-local accounting without introducing test-specific branches in the\\n-   * core engine.\\n-   */\\n-  public resetPerRunState(): void {\\n-    try {\\n-      this.forwardRunGuards.clear();\\n-    } catch {}\\n-    try {\\n-      this.oncePerRunScheduleGuards.clear();\\n-    } catch {}\\n-    try {\\n-      this.onFinishLoopCounts.clear();\\n-      this.forEachWaveCounts.clear();\\n-    } catch {}\\n-    try {\\n-      this['executionStats'].clear();\\n-    } catch {}\\n-  }\\n-\\n-  private commitJournal(\\n-    checkId: string,\\n-    result: ExtendedReviewSummary,\\n-    event?: import('./types/config').EventTrigger,\\n-    scopeOverride?: ScopePath\\n-  ): void {\\n-    try {\\n-      const scope: ScopePath = scopeOverride || [];\\n-      this.journal.commitEntry({\\n-        sessionId: this.sessionUUID(),\\n-        scope,\\n-        checkId,\\n-        event,\\n-        result,\\n-      });\\n-    } catch {\\n-      // best effort; never throw\\n-    }\\n-  }\\n-\\n-  /** Build dependencyResults from a snapshot of all committed results, optionally overlaying provided results. */\\n-  private buildSnapshotDependencyResults(\\n-    scope: ScopePath,\\n-    overlay: Map<string, ReviewSummary> | undefined,\\n-    event: import('./types/config').EventTrigger | undefined\\n-  ): Map<string, ReviewSummary> {\\n-    const snap = this.journal.beginSnapshot();\\n-    const view = new ContextView(this.journal, this.sessionUUID(), snap, scope, event);\\n-    const visible = new Map<string, ReviewSummary>();\\n-    try {\\n-      const entries = this.journal.readVisible(this.sessionUUID(), snap, event);\\n-      const ids = Array.from(new Set(entries.map(e => e.checkId)));\\n-      for (const id of ids) {\\n-        const v = view.get(id);\\n-        if (v) visible.set(id, v);\\n-        const raw = view.getRaw(id);\\n-        if (raw) visible.set(`${id}-raw`, raw);\\n-      }\\n-      // Overlay any provided results (e.g., per-item context) on top.\\n-      // Root-cause hardening: ignore non-string keys and log once.\\n-      if (overlay) {\\n-        for (const [k, v] of overlay.entries()) {\\n-          if (typeof k === 'string' && k) {\\n-            visible.set(k, v);\\n-          } else {\\n-            try {\\n-              require('./logger').logger.warn(\\n-                `sanitize: dropping non-string overlay key type=${typeof k}`\\n-              );\\n-            } catch {}\\n-          }\\n-        }\\n-      }\\n-    } catch {}\\n-    return visible;\\n-  }\\n-\\n-  /** Drop any non-string keys from a results-like map (root-cause guard). */\\n-  private sanitizeResultMapKeys(\\n-    m: Map<unknown, ReviewSummary> | undefined\\n-  ): Map<string, ReviewSummary> {\\n-    const out = new Map<string, ReviewSummary>();\\n-    if (!m) return out;\\n-    for (const [k, v] of m.entries()) {\\n-      if (typeof k === 'string' && k) out.set(k, v);\\n-      else {\\n-        try {\\n-          require('./logger').logger.warn(\\n-            `sanitize: dropping non-string results key type=${typeof k}`\\n-          );\\n-        } catch {}\\n-      }\\n-    }\\n-    return out;\\n-  }\\n-\\n-  /**\\n-   * Enrich event context with authenticated octokit instance\\n-   * @param eventContext - The event context to enrich\\n-   * @returns Enriched event context with octokit if available\\n-   */\\n-  private enrichEventContext(eventContext?: Record<string, unknown>): Record<string, unknown> {\\n-    const baseContext = eventContext || {};\\n-    const injected = this.actionContext?.octokit || (baseContext as any).octokit;\\n-    if (injected) {\\n-      return { ...baseContext, octokit: injected };\\n-    }\\n-    return baseContext;\\n-  }\\n-\\n-  /**\\n-   * Set execution context for providers (CLI message, hooks, etc.)\\n-   * This allows passing state without using static properties\\n-   */\\n-  setExecutionContext(\\n-    context: import('./providers/check-provider.interface').ExecutionContext\\n-  ): void {\\n-    this.executionContext = context;\\n-  }\\n-\\n-  /**\\n-   * Lazily create a secure sandbox for routing JS (goto_js, run_js)\\n-   */\\n-  private getRoutingSandbox(): Sandbox {\\n-    if (this.routingSandbox) return this.routingSandbox;\\n-    this.routingSandbox = createSecureSandbox();\\n-    return this.routingSandbox;\\n-  }\\n-\\n-  private redact(str: unknown, limit = 200): string {\\n-    try {\\n-      const s = typeof str === 'string' ? str : JSON.stringify(str);\\n-      return s.length > limit ? s.slice(0, limit) + '‚Ä¶' : s;\\n-    } catch {\\n-      return String(str).slice(0, limit);\\n-    }\\n-  }\\n-\\n-  private async sleep(ms: number): Promise<void> {\\n-    return new Promise(resolve => setTimeout(resolve, ms));\\n-  }\\n-\\n-  private deterministicJitter(baseMs: number, seedStr: string): number {\\n-    let h = 2166136261;\\n-    for (let i = 0; i < seedStr.length; i++) h = (h ^ seedStr.charCodeAt(i)) * 16777619;\\n-    const frac = ((h >>> 0) % 1000) / 1000; // 0..1\\n-    return Math.floor(baseMs * 0.15 * frac); // up to 15% jitter\\n-  }\\n-\\n-  // === on_finish helpers (extracted to reduce handleOnFinishHooks complexity) ===\\n-  private composeOnFinishContext(\\n-    checkName: string,\\n-    checkConfig: import('./types/config').CheckConfig,\\n-    outputsForContext: Record<string, unknown>,\\n-    outputsHistoryForContext: Record<string, unknown[]>,\\n-    forEachStats: any,\\n-    prInfo: PRInfo\\n-  ): {\\n-    step: { id: string; tags: string[]; group?: string };\\n-    attempt: number;\\n-    loop: number;\\n-    outputs: Record<string, unknown>;\\n-    outputs_history: Record<string, unknown[]>;\\n-    outputs_raw: Record<string, unknown>;\\n-    forEach: any;\\n-    memory: {\\n-      get: (key: string, ns?: string) => unknown;\\n-      has: (key: string, ns?: string) => boolean;\\n-      list: (ns?: string) => string[];\\n-      getAll: (ns?: string) => Record<string, unknown>;\\n-      set: (key: string, value: unknown, ns?: string) => void;\\n-      increment: (key: string, amount: number, ns?: string) => number;\\n-    };\\n-    pr: { number: number; title: string; author: string; branch: string; base: string };\\n-    files: PRInfo['files'];\\n-    env: Record<string, string>;\\n-    event: { name: string };\\n-  } {\\n-    const memoryStore = MemoryStore.getInstance(this.config?.memory);\\n-    const memoryHelpers = {\\n-      get: (key: string, ns?: string) => memoryStore.get(key, ns),\\n-      has: (key: string, ns?: string) => memoryStore.has(key, ns),\\n-      list: (ns?: string) => memoryStore.list(ns),\\n-      getAll: (ns?: string) => {\\n-        const keys = memoryStore.list(ns);\\n-        const result: Record<string, unknown> = {};\\n-        for (const key of keys) result[key] = memoryStore.get(key, ns);\\n-        return result;\\n-      },\\n-      set: (key: string, value: unknown, ns?: string) => {\\n-        const nsName = ns || memoryStore.getDefaultNamespace();\\n-        if (!memoryStore['data'].has(nsName)) memoryStore['data'].set(nsName, new Map());\\n-        memoryStore['data'].get(nsName)!.set(key, value);\\n-      },\\n-      increment: (key: string, amount: number, ns?: string) => {\\n-        const current = memoryStore.get(key, ns);\\n-        const numCurrent = typeof current === 'number' ? current : 0;\\n-        const newValue = numCurrent + amount;\\n-        const nsName = ns || memoryStore.getDefaultNamespace();\\n-        if (!memoryStore['data'].has(nsName)) memoryStore['data'].set(nsName, new Map());\\n-        memoryStore['data'].get(nsName)!.set(key, newValue);\\n-        return newValue;\\n-      },\\n-    };\\n-    const outputsRawForContext: Record<string, unknown> = {};\\n-    for (const [name, val] of Object.entries(outputsForContext)) {\\n-      if (name === 'history') continue;\\n-      outputsRawForContext[name] = val;\\n-    }\\n-    const outputsMergedForContext: Record<string, unknown> = {\\n-      ...outputsForContext,\\n-      history: outputsHistoryForContext,\\n-    };\\n-    return {\\n-      step: { id: checkName, tags: checkConfig.tags || [], group: checkConfig.group },\\n-      attempt: 1,\\n-      loop: 0,\\n-      outputs: outputsMergedForContext,\\n-      outputs_history: outputsHistoryForContext,\\n-      outputs_raw: outputsRawForContext,\\n-      forEach: forEachStats,\\n-      memory: memoryHelpers,\\n-      pr: {\\n-        number: prInfo.number,\\n-        title: prInfo.title,\\n-        author: prInfo.author,\\n-        branch: prInfo.head,\\n-        base: prInfo.base,\\n-      },\\n-      files: prInfo.files,\\n-      env: getSafeEnvironmentVariables(),\\n-      event: { name: prInfo.eventType || 'manual' },\\n-    };\\n-  }\\n-\\n-  private evaluateOnFinishGoto(\\n-    checkName: string,\\n-    onFinish: NonNullable<import('./types/config').CheckConfig['on_finish']>,\\n-    onFinishContext: any,\\n-    debug: boolean,\\n-    log: (msg: string) => void\\n-  ): string | null {\\n-    let gotoTarget: string | null = null;\\n-    if (onFinish.goto_js) {\\n-      logger.info(`‚ñ∂ on_finish.goto_js: evaluating for \\\"${checkName}\\\"`);\\n-      try {\\n-        const sandbox = this.getRoutingSandbox();\\n-        const scope = onFinishContext;\\n-        const code = `\\n-          const step = scope.step; const attempt = scope.attempt; const loop = scope.loop; const outputs = scope.outputs; const outputs_history = scope.outputs_history; const outputs_raw = scope.outputs_raw; const forEach = scope.forEach; const memory = scope.memory; const pr = scope.pr; const files = scope.files; const env = scope.env; const event = scope.event; const log = (...a)=> console.log('üîç Debug:',...a);\\n-          const __fn = () => {\\\\n${onFinish.goto_js}\\\\n};\\n-          const __res = __fn();\\n-          return (typeof __res === 'string' && __res) ? __res : null;\\n-        `;\\n-        const exec = sandbox.compile(code);\\n-        const result = exec({ scope }).run();\\n-        gotoTarget = typeof result === 'string' && result ? result : null;\\n-        if (debug) log(`üîß Debug: on_finish.goto_js evaluated ‚Üí ${this.redact(gotoTarget)}`);\\n-        logger.info(\\n-          `‚úì on_finish.goto_js: evaluated to '${gotoTarget || 'null'}' for \\\"${checkName}\\\"`\\n-        );\\n-      } catch (error) {\\n-        const errorMsg = error instanceof Error ? error.message : String(error);\\n-        logger.warn(`‚ö†Ô∏è on_finish.goto_js: evaluation failed for \\\"${checkName}\\\": ${errorMsg}`);\\n-        if (error instanceof Error && error.stack) logger.debug(`Stack trace: ${error.stack}`);\\n-        if (onFinish.goto) {\\n-          logger.info(`  ‚ö† Falling back to static goto: '${onFinish.goto}'`);\\n-          gotoTarget = onFinish.goto;\\n-        }\\n-      }\\n-    } else if (onFinish.goto) {\\n-      gotoTarget = onFinish.goto;\\n-      logger.info(`‚ñ∂ on_finish.goto: routing to '${gotoTarget}' for \\\"${checkName}\\\"`);\\n-    }\\n-    return gotoTarget;\\n-  }\\n-\\n-  private computeBackoffDelay(\\n-    attempt: number,\\n-    mode: 'fixed' | 'exponential',\\n-    baseMs: number,\\n-    seed: string\\n-  ): number {\\n-    const jitter = this.deterministicJitter(baseMs, seed);\\n-    if (mode === 'exponential') {\\n-      return baseMs * Math.pow(2, Math.max(0, attempt - 1)) + jitter;\\n-    }\\n-    return baseMs + jitter;\\n-  }\\n-\\n-  /**\\n-   * Execute a single named check inline (used by routing logic and on_finish)\\n-   * This is extracted from executeWithRouting to be reusable\\n-   */\\n-  private async executeCheckInline(\\n-    checkId: string,\\n-    event: import('./types/config').EventTrigger,\\n-    context: {\\n-      config: VisorConfig;\\n-      dependencyGraph: DependencyGraph;\\n-      prInfo: PRInfo;\\n-      resultsMap: Map<string, ReviewSummary>;\\n-      dependencyResults: Map<string, ReviewSummary>;\\n-      sessionInfo?: { parentSessionId?: string; reuseSession?: boolean };\\n-      debug: boolean;\\n-      eventOverride?: import('./types/config').EventTrigger;\\n-      scope?: ScopePath;\\n-      origin?: 'on_finish' | 'on_success' | 'on_fail' | 'foreach' | 'initial' | 'inline';\\n-    }\\n-  ): Promise<ReviewSummary> {\\n-    const {\\n-      config,\\n-      prInfo,\\n-      resultsMap,\\n-      dependencyResults,\\n-      sessionInfo,\\n-      debug,\\n-      eventOverride,\\n-      scope,\\n-    } = context;\\n-    const log = (msg: string) => (config?.output?.pr_comment ? console.error : console.log)(msg);\\n-    const origin = (context as any).origin || 'inline';\\n-\\n-    // Find the check configuration\\n-    const checkConfig = config?.checks?.[checkId];\\n-    if (!checkConfig) {\\n-      throw new Error(`on_finish referenced unknown check '${checkId}'`);\\n-    }\\n-\\n-    // Helper to get all dependencies recursively from config\\n-    const getAllDepsFromConfig = (name: string): string[] => {\\n-      const visited = new Set<string>();\\n-      const acc: string[] = [];\\n-      const dfs = (n: string) => {\\n-        if (visited.has(n)) return;\\n-        visited.add(n);\\n-        const cfg = config?.checks?.[n];\\n-        const deps = cfg?.depends_on || [];\\n-        for (const d of deps) {\\n-          acc.push(d);\\n-          dfs(d);\\n-        }\\n-      };\\n-      dfs(name);\\n-      return Array.from(new Set(acc));\\n-    };\\n-\\n-    // Ensure all dependencies of target are available; execute missing ones in topological order\\n-    const allTargetDeps = getAllDepsFromConfig(checkId);\\n-    if (allTargetDeps.length > 0) {\\n-      // Build subgraph mapping for ordered execution\\n-      const subSet = new Set<string>([...allTargetDeps]);\\n-      const subDeps: Record<string, string[]> = {};\\n-      for (const id of subSet) {\\n-        const cfg = config?.checks?.[id];\\n-        subDeps[id] = (cfg?.depends_on || []).filter(d => subSet.has(d));\\n-      }\\n-      const subGraph = DependencyResolver.buildDependencyGraph(subDeps);\\n-      for (const group of subGraph.executionOrder) {\\n-        for (const depId of group.parallel) {\\n-          // Skip if already have results\\n-          if (resultsMap?.has(depId) || dependencyResults.has(depId)) continue;\\n-          // Execute dependency inline (recursively ensures its deps are also present)\\n-          await this.executeCheckInline(depId, event, context);\\n-        }\\n-      }\\n-    }\\n-\\n-    // No legacy adapters; use configuration as-is\\n-    const adaptedConfig: any = { ...checkConfig };\\n-    const providerType = adaptedConfig.type || 'ai';\\n-    const provider = this.providerRegistry.getProviderOrThrow(providerType);\\n-    this.setProviderWebhookContext(provider);\\n-\\n-    // Build provider configuration\\n-    const provCfg: CheckProviderConfig = {\\n-      type: providerType,\\n-      prompt: adaptedConfig.prompt,\\n-      exec: adaptedConfig.exec,\\n-      focus: adaptedConfig.focus || this.mapCheckNameToFocus(checkId),\\n-      schema: adaptedConfig.schema,\\n-      group: adaptedConfig.group,\\n-      checkName: checkId,\\n-      eventContext: this.enrichEventContext(prInfo.eventContext),\\n-      transform: adaptedConfig.transform,\\n-      transform_js: adaptedConfig.transform_js,\\n-      env: adaptedConfig.env,\\n-      forEach: adaptedConfig.forEach,\\n-      // Pass output history for loop/goto scenarios\\n-      __outputHistory: this.outputHistory,\\n-      // Include provider-specific keys (e.g., op/values for github)\\n-      ...adaptedConfig,\\n-      ai: {\\n-        ...(adaptedConfig.ai || {}),\\n-        timeout: adaptedConfig.ai?.timeout || 600000,\\n-        debug: !!debug,\\n-      },\\n-    };\\n-\\n-    // Build dependency results for this check using snapshot-based visibility (overlay per-scope results)\\n-    const depResults = this.buildSnapshotDependencyResults(\\n-      scope || [],\\n-      dependencyResults,\\n-      eventOverride || prInfo.eventType\\n-    );\\n-\\n-    // Debug: log key dependent outputs for visibility\\n-    if (debug) {\\n-      try {\\n-        const depPreview: Record<string, unknown> = {};\\n-        for (const [k, v] of depResults.entries()) {\\n-          const out = (v as any)?.output;\\n-          if (out !== undefined) depPreview[k] = out;\\n-        }\\n-        log(`üîß Debug: inline exec '${checkId}' deps output: ${JSON.stringify(depPreview)}`);\\n-      } catch {}\\n-    }\\n-\\n-    if (debug) {\\n-      const execStr = (provCfg as any).exec;\\n-      if (execStr) log(`üîß Debug: inline exec '${checkId}' command: ${execStr}`);\\n-    }\\n-\\n-    // If event override provided, clone prInfo with overridden eventType\\n-    let prInfoForInline = prInfo;\\n-    const prevEventOverride = this.routingEventOverride;\\n-    if (eventOverride) {\\n-      // Try to elevate to PR context when routing to PR events from issue threads\\n-      const elevated = await this.elevateContextToPullRequest(\\n-        { ...(prInfo as any), eventType: eventOverride } as PRInfo,\\n-        eventOverride,\\n-        log,\\n-        debug\\n-      );\\n-      if (elevated) {\\n-        prInfoForInline = elevated;\\n-      } else {\\n-        prInfoForInline = { ...(prInfo as any), eventType: eventOverride } as PRInfo;\\n-      }\\n-      this.routingEventOverride = eventOverride;\\n-      const msg = `‚Ü™ goto_event: inline '${checkId}' with event=${eventOverride}${\\n-        elevated ? ' (elevated to PR context)' : ''\\n-      }`;\\n-      if (debug) log(`üîß Debug: ${msg}`);\\n-      try {\\n-        require('./logger').logger.info(msg);\\n-      } catch {}\\n-    }\\n-\\n-    // Execute the check\\n-    let result: ReviewSummary;\\n-    try {\\n-      const __provStart = Date.now();\\n-      const inlineContext: import('./providers/check-provider.interface').ExecutionContext = {\\n-        ...sessionInfo,\\n-        ...this.executionContext,\\n-      } as any;\\n-      // dependency printout removed\\n-      result = await withActiveSpan(\\n-        `visor.check.${checkId}`,\\n-        { 'visor.check.id': checkId, 'visor.check.type': provCfg.type || 'ai' },\\n-        async () => provider.execute(prInfoForInline, provCfg, depResults, inlineContext)\\n-      );\\n-      this.recordProviderDuration(checkId, Date.now() - __provStart);\\n-    } catch (error) {\\n-      // Restore previous override before rethrowing\\n-      this.routingEventOverride = prevEventOverride;\\n-      throw error;\\n-    } finally {\\n-      // Always restore previous override\\n-      this.routingEventOverride = prevEventOverride;\\n-    }\\n-\\n-    // Enrich issues with metadata\\n-    const enrichedIssues = (result.issues || []).map(issue => ({\\n-      ...issue,\\n-      checkName: checkId,\\n-      ruleId: `${checkId}/${issue.ruleId}`,\\n-      group: checkConfig.group,\\n-      schema: typeof checkConfig.schema === 'object' ? 'custom' : checkConfig.schema,\\n-      template: checkConfig.template,\\n-      timestamp: Date.now(),\\n-    }));\\n-    let enriched = { ...result, issues: enrichedIssues } as ReviewSummary;\\n-\\n-    // Track output history for loop/goto scenarios\\n-    const enrichedWithOutput = enriched as ReviewSummary & { output?: unknown };\\n-    if (enrichedWithOutput.output !== undefined) {\\n-      this.trackOutputHistory(checkId, enrichedWithOutput.output);\\n-    }\\n-\\n-    // Handle forEach iteration for this check if it returned an array\\n-    if (checkConfig.forEach && Array.isArray(enrichedWithOutput.output)) {\\n-      const forEachItems = enrichedWithOutput.output;\\n-      // Always log forEach detection (not just in debug mode) for visibility\\n-      const wave = (this.forEachWaveCounts.get(checkId) || 0) + 1;\\n-      this.forEachWaveCounts.set(checkId, wave);\\n-      log(\\n-        `üîÑ forEach check '${checkId}' returned ${forEachItems.length} items - starting iteration (wave #${wave}, origin=${origin})`\\n-      );\\n-      if (debug) {\\n-        log(\\n-          `üîß Debug: forEach item preview: ${JSON.stringify(forEachItems[0] || {}).substring(0, 200)}`\\n-        );\\n-      }\\n-\\n-      // Store the array output with forEach metadata\\n-      const forEachResult = {\\n-        ...enriched,\\n-        forEachItems,\\n-        forEachItemResults: forEachItems.map(item => ({\\n-          issues: [],\\n-          output: item,\\n-        })),\\n-      };\\n-      enriched = forEachResult as ReviewSummary;\\n-\\n-      // Make the parent result visible to dependency resolution BEFORE scheduling dependents\\n-      // so that recursive dependency checks do not re-execute this forEach parent in the same wave.\\n-      try {\\n-        resultsMap?.set(checkId, enriched);\\n-      } catch {}\\n-\\n-      // Phase 4: commit aggregate parent result early (root scope) so outputs_raw is visible\\n-      this.commitJournal(\\n-        checkId,\\n-        enriched as ExtendedReviewSummary,\\n-        prInfoForInline.eventType || prInfo.eventType,\\n-        []\\n-      );\\n-\\n-      // Wave guard: if waves exceed routing.max_loops, stop scheduling dependents to prevent runaway loops\\n-      const maxLoops = config?.routing?.max_loops ?? 10;\\n-      if (wave > maxLoops) {\\n-        try {\\n-          logger.warn(\\n-            `‚õî forEach wave guard: '${checkId}' exceeded max_loops=${maxLoops} (wave #${wave}); skipping dependents and routing`\\n-          );\\n-        } catch {}\\n-        // Store and return aggregated result\\n-        resultsMap?.set(checkId, enriched);\\n-        return enriched;\\n-      }\\n-\\n-      // Find checks that depend on this forEach check\\n-      const dependentChecks = Object.keys(config?.checks || {}).filter(name => {\\n-        const cfg = config?.checks?.[name];\\n-        return cfg?.depends_on?.includes(checkId);\\n-      });\\n-\\n-      // Always log dependents for visibility\\n-      try {\\n-        if (dependentChecks.length > 0) {\\n-          log(\\n-            `üîÑ forEach check '${checkId}' has ${dependentChecks.length} dependents: ${dependentChecks.join(', ')}`\\n-          );\\n-        } else {\\n-          log(`‚ö†Ô∏è  forEach check '${checkId}' has NO dependents - nothing to iterate`);\\n-        }\\n-      } catch {}\\n-\\n-      // Execute each dependent check once per forEach item (scope-based; no per-item map cloning)\\n-      for (const depCheckName of dependentChecks) {\\n-        const depCheckConfig = config?.checks?.[depCheckName];\\n-        if (!depCheckConfig) continue;\\n-\\n-        // Always (re)run dependents during inline reruns (on_finish.goto to parent).\\n-        // We intentionally do not short-circuit on existing results here so stats/history\\n-        // reflect multiple waves.\\n-        // Skip if no items to iterate over\\n-        if (forEachItems.length === 0) {\\n-          if (debug) {\\n-            log(`üîß Debug: Skipping forEach dependent '${depCheckName}' - no items to iterate`);\\n-          }\\n-          // Store empty result\\n-          resultsMap?.set(depCheckName, { issues: [] });\\n-          continue;\\n-        }\\n-\\n-        // Always log iteration start\\n-        try {\\n-          const wave = this.forEachWaveCounts.get(checkId) || 1;\\n-          log(\\n-            `üîÑ Executing forEach dependent '${depCheckName}' for ${forEachItems.length} items (wave #${wave})`\\n-          );\\n-        } catch {}\\n-\\n-        const depResults: ReviewSummary[] = [];\\n-\\n-        // Execute once per forEach item\\n-        for (let itemIndex = 0; itemIndex < forEachItems.length; itemIndex++) {\\n-          const item = forEachItems[itemIndex];\\n-          const wave = this.forEachWaveCounts.get(checkId) || 1;\\n-          log(\\n-            `  üîÑ Iteration ${itemIndex + 1}/${forEachItems.length} f|| '${depCheckName}' (wave #${wave})`\\n-          );\\n-\\n-          // Phase 4: Commit per-item entry for parent in journal under item scope\\n-          const itemScope: ScopePath = [{ check: checkId, index: itemIndex }];\\n-          try {\\n-            this.commitJournal(\\n-              checkId,\\n-              { issues: [], output: item } as ExtendedReviewSummary,\\n-              prInfoForInline.eventType || prInfo.eventType,\\n-              itemScope\\n-            );\\n-          } catch (error) {\\n-            const msg = error instanceof Error ? error.message : String(error);\\n-            logger.error(`Failed to commit per-item journal for ${checkId}: ${msg}`);\\n-            // Non-fatal: journal is best-effort; continue without retry.\\n-          }\\n-\\n-          try {\\n-            // Build provider + config for dependent and execute with full routing semantics\\n-            const depProviderType = depCheckConfig.type || 'ai';\\n-            const depProvider = this.providerRegistry.getProviderOrThrow(depProviderType);\\n-            this.setProviderWebhookContext(depProvider);\\n-\\n-            // Build dependency results from snapshot at item scope (no cloning)\\n-            const snapshotDeps = this.buildSnapshotDependencyResults(\\n-              itemScope,\\n-              undefined,\\n-              prInfoForInline.eventType || prInfo.eventType\\n-            );\\n-\\n-            // Use unified helper to ensure stats and history are tracked for each item run\\n-            const res = await this.runNamedCheck(depCheckName, itemScope, {\\n-              origin: 'foreach',\\n-              config: config!,\\n-              dependencyGraph: context.dependencyGraph,\\n-              prInfo,\\n-              resultsMap: resultsMap || new Map(),\\n-              debug: !!debug,\\n-              eventOverride: prInfoForInline.eventType || prInfo.eventType,\\n-              overlay: snapshotDeps,\\n-            });\\n-            depResults.push(res);\\n-          } catch (error) {\\n-            // Store error result for this iteration\\n-            const errorMsg = error instanceof Error ? error.message : String(error);\\n-            const errorIssue: ReviewIssue = {\\n-              file: '',\\n-              line: 0,\\n-              ruleId: `${depCheckName}/forEach/iteration_error`,\\n-              message: `forEach iteration ${itemIndex + 1} failed: ${errorMsg}`,\\n-              severity: 'error',\\n-              category: 'logic',\\n-            };\\n-            depResults.push({\\n-              issues: [errorIssue],\\n-            });\\n-          }\\n-        }\\n-\\n-        // Aggregate results from all iterations\\n-        const aggregatedResult: ReviewSummary = {\\n-          issues: depResults.flatMap(r => r.issues || []),\\n-        };\\n-\\n-        // Store in results map\\n-        resultsMap?.set(depCheckName, aggregatedResult);\\n-\\n-        if (debug) {\\n-          log(\\n-            `üîß Debug: Completed forEach dependent '${depCheckName}' with ${depResults.length} iterations`\\n-          );\\n-        }\\n-      }\\n-    }\\n-\\n-    // Store result in results map\\n-    resultsMap?.set(checkId, enriched);\\n-    // Commit to journal with provided scope (or root). Avoid double-commit if we already committed aggregate above.\\n-    const isForEachAggregate = checkConfig.forEach && Array.isArray(enrichedWithOutput.output);\\n-    if (!isForEachAggregate) {\\n-      this.commitJournal(\\n-        checkId,\\n-        enriched as ExtendedReviewSummary,\\n-        prInfoForInline.eventType || prInfo.eventType,\\n-        scope || []\\n-      );\\n-    }\\n-\\n-    if (debug) log(`üîß Debug: inline executed '${checkId}', issues: ${enrichedIssues.length}`);\\n-\\n-    return enriched;\\n-  }\\n-\\n-  /**\\n-   * Phase 3: Unified scheduling helper\\n-   * Runs a named check in the current session/scope and records results.\\n-   * Used by on_success/on_fail/on_finish routing and internal inline execution.\\n-   */\\n-  private async runNamedCheck(\\n-    target: string,\\n-    scope: ScopePath,\\n-    opts: {\\n-      config: VisorConfig;\\n-      dependencyGraph: DependencyGraph;\\n-      prInfo: PRInfo;\\n-      resultsMap: Map<string, ReviewSummary>;\\n-      debug: boolean;\\n-      sessionInfo?: { parentSessionId?: string; reuseSession?: boolean };\\n-      eventOverride?: import('./types/config').EventTrigger;\\n-      overlay?: Map<string, ReviewSummary>;\\n-      origin?: 'on_finish' | 'on_success' | 'on_fail' | 'foreach' | 'initial' | 'inline';\\n-    }\\n-  ): Promise<ReviewSummary> {\\n-    const {\\n-      config,\\n-      dependencyGraph,\\n-      prInfo,\\n-      resultsMap,\\n-      debug,\\n-      sessionInfo,\\n-      eventOverride,\\n-      overlay,\\n-    } = opts;\\n-    try {\\n-      if (debug && opts.origin === 'on_finish') {\\n-        console.error(`[runNamedCheck] origin=on_finish step=${target}`);\\n-      }\\n-    } catch {}\\n-\\n-    // Evaluate 'if' condition for checks executed via routing (run/goto).\\n-    try {\\n-      const tcfg = opts.config.checks?.[target] as import('./types/config').CheckConfig | undefined;\\n-      if (tcfg && tcfg.if) {\\n-        const gate = await this.shouldRunCheck(\\n-          target,\\n-          tcfg.if,\\n-          opts.prInfo,\\n-          opts.resultsMap || new Map<string, ReviewSummary>(),\\n-          !!debug,\\n-          opts.eventOverride,\\n-          /* failSecure */ true\\n-        );\\n-        if (!gate.shouldRun) {\\n-          // Record a skipped marker compatible with summary rendering\\n-          const skipped: ReviewSummary = {\\n-            issues: [\\n-              {\\n-                file: '',\\n-                line: 0,\\n-                ruleId: `${target}/__skipped`,\\n-                message: `Skipped by if condition: ${tcfg.if}`,\\n-                severity: 'info',\\n-                category: 'logic',\\n-              },\\n-            ],\\n-          } as ReviewSummary;\\n-          try {\\n-            this.recordSkip(target, 'if_condition', tcfg.if);\\n-            logger.info(`‚è≠  Skipped (if: ${this.truncate(tcfg.if, 40)})`);\\n-          } catch (error) {\\n-            const msg = error instanceof Error ? error.message : String(error);\\n-            logger.error(`Failed to record skip for ${target}: ${msg}`);\\n-          }\\n-          // Commit a minimal journal entry to make downstream visibility consistent\\n-          this.commitJournal(\\n-            target,\\n-            skipped as any,\\n-            opts.eventOverride || opts.prInfo.eventType,\\n-            scope || []\\n-          );\\n-          opts.resultsMap?.set(target, skipped);\\n-          return skipped;\\n-        }\\n-      }\\n-    } catch (error) {\\n-      const msg = error instanceof Error ? error.message : String(error);\\n-      logger.error(`Failed to evaluate if condition for ${target}: ${msg}`);\\n-      // Fail secure: if condition evaluation fails, skip execution\\n-      const skipped: ReviewSummary = {\\n-        issues: [\\n-          {\\n-            file: '',\\n-            line: 0,\\n-            ruleId: `${target}/__skipped`,\\n-            message: `Skipped due to condition evaluation error`,\\n-            severity: 'info',\\n-            category: 'logic',\\n-          },\\n-        ],\\n-      } as ReviewSummary;\\n-      try {\\n-        const cond =\\n-          (opts.config.checks?.[target] as import('./types/config').CheckConfig | undefined)?.if ||\\n-          '';\\n-        this.recordSkip(target, 'if_condition', cond);\\n-      } catch {}\\n-      this.commitJournal(\\n-        target,\\n-        skipped as any,\\n-        opts.eventOverride || opts.prInfo.eventType,\\n-        scope || []\\n-      );\\n-      opts.resultsMap?.set(target, skipped);\\n-      return skipped;\\n-    }\\n-\\n-    // Build context overlay from current results; prefer snapshot visibility for scope (Phase 4)\\n-    const depOverlay = overlay ? new Map(overlay) : new Map(resultsMap);\\n-    const depOverlaySanitized = this.sanitizeResultMapKeys(depOverlay);\\n-    // For event overrides, avoid leaking cross-event results via overlay; rely on snapshot-only view\\n-    const overlayForExec =\\n-      eventOverride && eventOverride !== (prInfo.eventType || 'manual')\\n-        ? new Map<string, ReviewSummary>()\\n-        : depOverlaySanitized;\\n-    if (!this.executionStats.has(target)) this.initializeCheckStats(target);\\n-    const startTs = this.recordIterationStart(target);\\n-    try {\\n-      const res = await this.executeCheckInline(\\n-        target,\\n-        eventOverride || prInfo.eventType || 'manual',\\n-        {\\n-          config,\\n-          dependencyGraph,\\n-          prInfo,\\n-          resultsMap,\\n-          // Use snapshot-only deps when eventOverride is set\\n-          dependencyResults: overlayForExec,\\n-          sessionInfo,\\n-          debug,\\n-          eventOverride,\\n-          scope,\\n-          origin: opts.origin || 'inline',\\n-        }\\n-      );\\n-      const issues = (res.issues || []).map(i => ({ ...i }));\\n-      const success = !this.hasFatal(issues);\\n-      const out: unknown = (res as { output?: unknown }).output;\\n-      const isForEachParent =\\n-        (res as any)?.isForEach === true ||\\n-        Array.isArray((res as any)?.forEachItems) ||\\n-        Array.isArray(out);\\n-      this.recordIterationComplete(\\n-        target,\\n-        startTs,\\n-        success,\\n-        issues,\\n-        isForEachParent ? undefined : out\\n-      );\\n-      // Output history is already tracked inside executeCheckInline when a check\\n-      // produces an output. Avoid tracking again here to prevent double-counting\\n-      // (particularly for forward-run goto chains within a single stage).\\n-      return res;\\n-    } catch (e) {\\n-      this.recordIterationComplete(target, startTs, false, [], undefined);\\n-      throw e;\\n-    }\\n-  }\\n-\\n-  /**\\n-   * Handle on_finish hooks for forEach checks after ALL dependents complete\\n-   */\\n-  private async handleOnFinishHooks(\\n-    config: VisorConfig,\\n-    dependencyGraph: DependencyGraph,\\n-    results: Map<string, ReviewSummary>,\\n-    prInfo: PRInfo,\\n-    debug: boolean\\n-  ): Promise<void> {\\n-    const log = (msg: string) => (config?.output?.pr_comment ? console.error : console.log)(msg);\\n-    try {\\n-      if (debug) console.error('[on_finish] handler invoked');\\n-    } catch {}\\n-\\n-    const forEachChecksWithOnFinish = this.collectForEachParentsWithOnFinish(config);\\n-\\n-    try {\\n-      logger.info(\\n-        `üß≠ on_finish: discovered ${forEachChecksWithOnFinish.length} forEach parent(s) with hooks`\\n-      );\\n-    } catch {}\\n-    if (forEachChecksWithOnFinish.length === 0) {\\n-      return; // No on_finish hooks to process\\n-    }\\n-\\n-    // Fast path: if none of the forEach parents executed in this run, skip on_finish entirely.\\n-    // This avoids unnecessary work (and log noise) in cases where these parents belong to a different event.\\n-    try {\\n-      const anyParentRan = forEachChecksWithOnFinish.some(({ checkName }) =>\\n-        results.has(checkName)\\n-      );\\n-      if (!anyParentRan) {\\n-        if (debug) log('üß≠ on_finish: no forEach parent executed in this run ‚Äî skip');\\n-        return;\\n-      }\\n-    } catch {}\\n-\\n-    if (debug) {\\n-      log(`üéØ Processing on_finish hooks for ${forEachChecksWithOnFinish.length} forEach check(s)`);\\n-    }\\n-\\n-    // Process each forEach check's on_finish hook\\n-    for (const { checkName, checkConfig, onFinish } of forEachChecksWithOnFinish) {\\n-      try {\\n-        const forEachResult = results.get(checkName) as ExtendedReviewSummary | undefined;\\n-        if (!forEachResult) {\\n-          try {\\n-            logger.info(`‚è≠ on_finish: no result found for \\\"${checkName}\\\" ‚Äî skip`);\\n-          } catch {}\\n-          continue;\\n-        }\\n-\\n-        // Skip if the forEach check returned empty array\\n-        const forEachItems = forEachResult.forEachItems || [];\\n-        if (forEachItems.length === 0) {\\n-          try {\\n-            logger.info(`‚è≠ on_finish: \\\"${checkName}\\\" produced 0 items ‚Äî skip`);\\n-          } catch {}\\n-          continue;\\n-        }\\n-\\n-        // Get all dependents of this forEach check\\n-        const node = dependencyGraph.nodes.get(checkName);\\n-        const dependents = node?.dependents || [];\\n-\\n-        try {\\n-          logger.info(`üîç on_finish: \\\"${checkName}\\\" ‚Üí ${dependents.length} dependent(s)`);\\n-        } catch {}\\n-\\n-        // Ensure all dependents have completed before processing on_finish.\\n-        // If any are missing, try to execute them now in the on_finish phase so aggregation\\n-        // has up-to-date data (particularly important for forEach + validators).\\n-        for (const depId of dependents) {\\n-          if (results.has(depId)) continue;\\n-          try {\\n-            if (debug)\\n-              log(\\n-                `üîß on_finish: executing missing dependent '${depId}' before processing '${checkName}'`\\n-              );\\n-            const depRes = await this.runNamedCheck(depId, [], {\\n-              origin: 'on_finish',\\n-              config,\\n-              dependencyGraph,\\n-              prInfo,\\n-              resultsMap: results,\\n-              sessionInfo: (this.executionContext as any) || undefined,\\n-              debug,\\n-              overlay: new Map(results),\\n-            });\\n-            try {\\n-              results.set(depId, depRes as ReviewSummary);\\n-            } catch {}\\n-          } catch (e) {\\n-            // If a dependent cannot run, continue; downstream hooks may still choose to skip\\n-            try {\\n-              const msg = e instanceof Error ? e.message : String(e);\\n-              logger.warn(`‚ö†Ô∏è on_finish: failed to execute dependent '${depId}': ${msg}`);\\n-            } catch {}\\n-          }\\n-        }\\n-\\n-        logger.info(`‚ñ∂ on_finish: processing for \\\"${checkName}\\\"`);\\n-\\n-        // Build context projection (pure)\\n-        const { outputsForContext, outputsHistoryForContext } = ofProject(\\n-          results,\\n-          this.getOutputHistorySnapshot()\\n-        );\\n-\\n-        // Create forEach stats\\n-        const forEachStats = {\\n-          total: forEachItems.length,\\n-          successful: forEachResult.forEachItemResults\\n-            ? forEachResult.forEachItemResults.filter(\\n-                r => r && (!r.issues || r.issues.length === 0)\\n-              ).length\\n-            : forEachItems.length,\\n-          failed: forEachResult.forEachItemResults\\n-            ? forEachResult.forEachItemResults.filter(r => r && r.issues && r.issues.length > 0)\\n-                .length\\n-            : 0,\\n-          items: forEachItems,\\n-        };\\n-\\n-        // Get memory store for context (used for diagnostics below)\\n-        const memoryStore = MemoryStore.getInstance(this.config?.memory);\\n-\\n-        // Build context for on_finish evaluation (extracted helper)\\n-        const onFinishContext = ofComposeCtx(\\n-          this.config?.memory,\\n-          checkName,\\n-          checkConfig,\\n-          outputsForContext,\\n-          outputsHistoryForContext,\\n-          forEachStats,\\n-          prInfo\\n-        );\\n-\\n-        // Diagnostics: log attempt, dependents, items, and current budget usage\\n-        try {\\n-          const ns = 'fact-validation';\\n-          const attemptNow = Number(memoryStore.get('fact_validation_attempt', ns) || 0);\\n-          const usedBudget = this.onFinishLoopCounts.get(checkName) || 0;\\n-          const maxBudget = config?.routing?.max_loops ?? 10;\\n-          logger.info(\\n-            `üß≠ on_finish: check=\\\"${checkName}\\\" items=${forEachItems.length} dependents=${dependents.length} attempt=${attemptNow} budget=${usedBudget}/${maxBudget}`\\n-          );\\n-          const vfHist = (outputsHistoryForContext['validate-fact'] as unknown[]) || [];\\n-          if (vfHist.length) {\\n-            logger.debug(`üß≠ on_finish: outputs.history['validate-fact'] length=${vfHist.length}`);\\n-          }\\n-        } catch {}\\n-\\n-        let lastRunOutput: unknown = undefined;\\n-\\n-        // Execute on_finish.run (static) first, then evaluate run_js with updated context\\n-        {\\n-          const maxLoops = config?.routing?.max_loops ?? 10;\\n-          let loopCount = 0;\\n-          const runList = Array.from(new Set([...(onFinish.run || [])].filter(Boolean)));\\n-          if (runList.length > 0)\\n-            logger.info(`‚ñ∂ on_finish.run: executing [${runList.join(', ')}] for \\\"${checkName}\\\"`);\\n-          const runCheck = async (id: string): Promise<ReviewSummary> => {\\n-            if (++loopCount > maxLoops)\\n-              throw new Error(\\n-                `Routing loop budget exceeded (max_loops=${maxLoops}) during on_finish run`\\n-              );\\n-            const childCfgFull = (config?.checks || {})[id] as\\n-              | import('./types/config').CheckConfig\\n-              | undefined;\\n-            if (!childCfgFull) throw new Error(`Unknown check in on_finish.run: ${id}`);\\n-            const childProvider = this.providerRegistry.getProviderOrThrow(\\n-              childCfgFull.type || 'ai'\\n-            );\\n-            this.setProviderWebhookContext(childProvider);\\n-            const depOverlayForChild = new Map(results);\\n-            const resChild = await this.runNamedCheck(id, [], {\\n-              origin: 'on_finish',\\n-              config: config!,\\n-              dependencyGraph,\\n-              prInfo,\\n-              resultsMap: results,\\n-              debug,\\n-              sessionInfo: (this.executionContext as any) || undefined,\\n-              overlay: depOverlayForChild,\\n-            });\\n-            try {\\n-              results.set(id, resChild as ReviewSummary);\\n-            } catch {}\\n-            return resChild as ReviewSummary;\\n-          };\\n-          try {\\n-            const o = await ofRunChildren(\\n-              runList,\\n-              runCheck,\\n-              config!,\\n-              onFinishContext,\\n-              debug || false,\\n-              log\\n-            );\\n-            lastRunOutput = o.lastRunOutput;\\n-            if (runList.length > 0) logger.info(`‚úì on_finish.run: completed for \\\"${checkName}\\\"`);\\n-          } catch (error) {\\n-            const errorMsg = error instanceof Error ? error.message : String(error);\\n-            logger.error(`‚úó on_finish.run: failed for \\\"${checkName}\\\": ${errorMsg}`);\\n-            if (error instanceof Error && error.stack) logger.debug(`Stack trace: ${error.stack}`);\\n-            throw error;\\n-          }\\n-\\n-          // Now evaluate \\n\\n... [TRUNCATED: Diff too large (296.8KB), showing first 50KB] ...\",\"status\":\"modified\"},{\"filename\":\"src/cli-main.ts\",\"additions\":4,\"deletions\":1,\"changes\":171,\"patch\":\"diff --git a/src/cli-main.ts b/src/cli-main.ts\\nindex 0e53300b..01604062 100644\\n--- a/src/cli-main.ts\\n+++ b/src/cli-main.ts\\n@@ -5,7 +5,7 @@ import 'dotenv/config';\\n \\n import { CLI } from './cli';\\n import { ConfigManager } from './config';\\n-import { CheckExecutionEngine } from './check-execution-engine';\\n+import { StateMachineExecutionEngine } from './state-machine-execution-engine';\\n import { OutputFormatters, AnalysisResult } from './output-formatters';\\n import { CheckResult, GroupedCheckResults } from './reviewer';\\n import { PRInfo } from './pr-analyzer';\\n@@ -120,7 +120,17 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n   };\\n   const hasFlag = (name: string): boolean => argv.includes(name);\\n \\n-  const testsPath = getArg('--config');\\n+  // Support both --config flag and positional argument for tests path\\n+  let testsPath = getArg('--config');\\n+  if (!testsPath) {\\n+    // Look for first positional argument (non-flag) after the command\\n+    // argv is [node, script, 'test', ...rest]\\n+    const rest = argv.slice(3); // Skip node, script, and 'test' command\\n+    const positional = rest.find(arg => !arg.startsWith('--') && !arg.startsWith('-'));\\n+    if (positional) {\\n+      testsPath = positional;\\n+    }\\n+  }\\n   const only = getArg('--only');\\n   const bail = hasFlag('--bail');\\n   const listOnly = hasFlag('--list');\\n@@ -161,7 +171,7 @@ async function handleTestCommand(argv: string[]): Promise<void> {\\n     const runner = new (VisorTestRunner as any)();\\n     const tpath = runner.resolveTestsPath(testsPath);\\n     const suite = runner.loadSuite(tpath);\\n-    const runRes = await runner.runCases(tpath, suite, { only, bail, maxParallel, promptMaxChars });\\n+    const runRes = await runner.runCases(tpath, suite, { only, bail, maxParallel, promptMaxChars, engineMode: 'state-machine' });\\n     const failures = runRes.failures;\\n     // Fallback: If for any reason the runner didn't print its own summary\\n     // (e.g., natural early exit in some environments), print a concise one here.\\n@@ -289,10 +299,41 @@ export async function main(): Promise<void> {\\n   let debugServer: DebugVisualizerServer | null = null;\\n \\n   try {\\n+    // Preflight: detect obviously stale dist relative to src and warn early.\\n+    // This avoids confusing behavior when engine routing changed but dist wasn't rebuilt.\\n+    (function warnIfStaleDist() {\\n+      try {\\n+        const projectRoot = process.cwd();\\n+        const distIndex = path.join(projectRoot, 'dist', 'index.js');\\n+        const srcDir = path.join(projectRoot, 'src');\\n+        const statDist = fs.existsSync(distIndex) ? fs.statSync(distIndex) : null;\\n+        const srcNewestMtime = (function walk(dir: string): number {\\n+          let newest = 0;\\n+          if (!fs.existsSync(dir)) return 0;\\n+          for (const entry of fs.readdirSync(dir)) {\\n+            if (entry === 'debug-visualizer' || entry === 'sdk') continue;\\n+            const full = path.join(dir, entry);\\n+            const st = fs.statSync(full);\\n+            if (st.isDirectory()) newest = Math.max(newest, walk(full));\\n+            else if (/\\\\.tsx?$/.test(entry)) newest = Math.max(newest, st.mtimeMs);\\n+          }\\n+          return newest;\\n+        })(srcDir);\\n+        if (statDist && srcNewestMtime && srcNewestMtime > statDist.mtimeMs + 1) {\\n+          // Print once, concise but explicit.\\n+          console.error(\\n+            '‚ö†  Detected stale build: src/* is newer than dist/index.js. Run \\\"npm run build:cli\\\".'\\n+          );\\n+        }\\n+      } catch {\\n+        /* ignore preflight errors */\\n+      }\\n+    })();\\n+\\n     // IMPORTANT: detect subcommands before constructing CLI/commander to avoid\\n     // any argument parsing side-effects (e.g., extra positional args like 'test').\\n     // Also filter out the --cli flag if it exists (used to force CLI mode in GH Actions)\\n-    const filteredArgv = process.argv.filter(arg => arg !== '--cli');\\n+    let filteredArgv = process.argv.filter(arg => arg !== '--cli');\\n \\n     // EARLY: ensure trace dir and fallback NDJSON file exist BEFORE any early exits\\n     try {\\n@@ -328,6 +369,30 @@ export async function main(): Promise<void> {\\n       await handleTestCommand(filteredArgv);\\n       return;\\n     }\\n+    // Check for build subcommand: run the official agent-builder config\\n+    if (filteredArgv.length > 2 && filteredArgv[2] === 'build') {\\n+      // Transform into a standard run with our official builder config (agent-builder.yaml).\\n+      // Require a positional target: `build <path/to/agent.yaml>`\\n+      const base = filteredArgv.slice(0, 2);\\n+      let rest = filteredArgv.slice(3); // preserve flags like --message\\n+      const preferred = path.resolve(process.cwd(), 'defaults', 'agent-builder.yaml');\\n+      const fallback = path.resolve(process.cwd(), 'defaults', 'agent-build.yaml');\\n+      const chosen = fs.existsSync(preferred) ? preferred : fallback;\\n+\\n+      if (rest.length === 0 || String(rest[0]).startsWith('-')) {\\n+        console.error('Usage: visor build <path/to/agent.yaml> [--message \\\"brief\\\" ...]');\\n+        process.exitCode = 1;\\n+        return;\\n+      }\\n+\\n+      const targetPath = path.resolve(process.cwd(), String(rest[0]));\\n+      process.env.VISOR_AGENT_PATH = targetPath; // builder decides mode via Liquid readfile\\n+      rest = rest.slice(1);\\n+\\n+      // Do not force code context globally; respect per-step ai.skip_code_context.\\n+      // Builder YAML controls whether to include repo context.\\n+      filteredArgv = [...base, '--config', chosen, '--event', 'manual', ...rest];\\n+    }\\n     // Construct CLI and ConfigManager only after subcommand handling\\n     const cli = new CLI();\\n     const configManager = new ConfigManager();\\n@@ -452,12 +517,12 @@ export async function main(): Promise<void> {\\n \\n     // Start debug server if requested (AFTER config is loaded)\\n     if (options.debugServer) {\\n-      const port = options.debugPort || 3456;\\n+      const requestedPort = options.debugPort || 3456;\\n \\n-      console.log(`üîç Starting debug visualizer on port ${port}...`);\\n+      console.log(`üîç Starting debug visualizer on port ${requestedPort}...`);\\n \\n       debugServer = new DebugVisualizerServer();\\n-      await debugServer.start(port);\\n+      await debugServer.start(requestedPort);\\n \\n       // Set config on server BEFORE opening browser\\n       debugServer.setConfig(config);\\n@@ -472,12 +537,13 @@ export async function main(): Promise<void> {\\n         quiet: true, // Suppress console output when debug server is active\\n       });\\n \\n-      console.log(`‚úÖ Debug visualizer running at http://localhost:${port}`);\\n+      const boundPort = debugServer.getPort();\\n+      console.log(`‚úÖ Debug visualizer running at http://localhost:${boundPort}`);\\n \\n       // Open browser unless VISOR_NOBROWSER is set (useful for CI/tests)\\n       if (process.env.VISOR_NOBROWSER !== 'true') {\\n         console.log(`   Opening browser...`);\\n-        await open(`http://localhost:${port}`);\\n+        await open(`http://localhost:${boundPort}`);\\n       }\\n \\n       console.log(`‚è∏Ô∏è  Waiting for you to click \\\"Start Execution\\\" in the browser...`);\\n@@ -557,6 +623,28 @@ export async function main(): Promise<void> {\\n     // Determine checks to run and validate check types early\\n     let checksToRun = options.checks.length > 0 ? options.checks : Object.keys(config.checks || {});\\n \\n+    // Generic: remove checks that are meant to be scheduled via routing (on_*.run)\\n+    // from the initial root set unless the user explicitly requested them.\\n+    if (options.checks.length === 0) {\\n+      const routingRunTargets = new Set<string>();\\n+      for (const [, cfg] of Object.entries(config.checks || {})) {\\n+        const onFinish: any = (cfg as any).on_finish || {};\\n+        const onSuccess: any = (cfg as any).on_success || {};\\n+        const onFail: any = (cfg as any).on_fail || {};\\n+        const collect = (arr?: string[]) => {\\n+          if (Array.isArray(arr)) for (const t of arr) if (typeof t === 'string' && t) routingRunTargets.add(t);\\n+        };\\n+        collect(onFinish.run);\\n+        collect(onSuccess.run);\\n+        collect(onFail.run);\\n+      }\\n+      const before = checksToRun.length;\\n+      checksToRun = checksToRun.filter(chk => !routingRunTargets.has(chk));\\n+      if (before !== checksToRun.length) {\\n+        logger.verbose(`Pruned ${before - checksToRun.length} routing-run target(s) from roots`);\\n+      }\\n+    }\\n+\\n     // Validate that all requested checks exist in the configuration\\n     const availableChecks = Object.keys(config.checks || {});\\n     const invalidChecks = checksToRun.filter(check => !availableChecks.includes(check));\\n@@ -570,10 +658,20 @@ export async function main(): Promise<void> {\\n     const addDependencies = (checkName: string) => {\\n       const checkConfig = config.checks?.[checkName];\\n       if (checkConfig?.depends_on) {\\n-        for (const dep of checkConfig.depends_on) {\\n-          if (!checksWithDependencies.has(dep)) {\\n-            checksWithDependencies.add(dep);\\n-            addDependencies(dep); // Recursively add dependencies of dependencies\\n+        for (const raw of checkConfig.depends_on) {\\n+          const parts =\\n+            typeof raw === 'string' && raw.includes('|')\\n+              ? raw\\n+                  .split('|')\\n+                  .map(s => s.trim())\\n+                  .filter(Boolean)\\n+              : [String(raw)];\\n+          for (const dep of parts) {\\n+            if (!availableChecks.includes(dep)) continue; // ignore OR tokens that are not real checks\\n+            if (!checksWithDependencies.has(dep)) {\\n+              checksWithDependencies.add(dep);\\n+              addDependencies(dep); // Recursively add dependencies of dependencies\\n+            }\\n           }\\n         }\\n       }\\n@@ -587,6 +685,42 @@ export async function main(): Promise<void> {\\n     // Update checksToRun to include dependencies\\n     checksToRun = Array.from(checksWithDependencies);\\n \\n+    // Prune internal dependencies from the root set so we only start from DAG sinks.\\n+    // This prevents re-running dependency steps (e.g., human-input collectors) as\\n+    // independent roots across waves. The engine will expand dependencies anyway.\\n+    const getAllDeps = (name: string, seen = new Set<string>()): Set<string> => {\\n+      if (seen.has(name)) return new Set();\\n+      seen.add(name);\\n+      const out = new Set<string>();\\n+      const cfg = config.checks?.[name];\\n+      const depTokens: any[] = cfg?.depends_on\\n+        ? Array.isArray(cfg.depends_on)\\n+          ? cfg.depends_on\\n+          : [cfg.depends_on]\\n+        : [];\\n+      const expand = (tok: any): string[] =>\\n+        typeof tok === 'string' && tok.includes('|')\\n+          ? tok\\n+              .split('|')\\n+              .map(s => s.trim())\\n+              .filter(Boolean)\\n+          : tok != null\\n+            ? [String(tok)]\\n+            : [];\\n+      for (const raw of depTokens.flatMap(expand)) {\\n+        if (!availableChecks.includes(raw)) continue;\\n+        out.add(raw);\\n+        for (const d of getAllDeps(raw, seen)) out.add(d);\\n+      }\\n+      return out;\\n+    };\\n+\\n+    const rootsPruned = checksToRun.filter(chk => {\\n+      // Keep chk only if no other selected root depends on it (directly or transitively)\\n+      return !checksToRun.some(other => other !== chk && getAllDeps(other).has(chk));\\n+    });\\n+    if (rootsPruned.length > 0) checksToRun = rootsPruned;\\n+\\n     // Use stderr for status messages when outputting formatted results to stdout\\n     // Suppress all status messages when outputting JSON to avoid breaking parsers\\n     const logFn = (msg: string) => logger.info(msg);\\n@@ -678,8 +812,8 @@ export async function main(): Promise<void> {\\n     logger.step(`Executing ${checksToRun.length} check(s)`);\\n     logger.verbose(`Checks: ${checksToRun.join(', ')}`);\\n \\n-    // Create CheckExecutionEngine for running checks\\n-    const engine = new CheckExecutionEngine();\\n+    // Create StateMachineExecutionEngine for running checks\\n+    const engine = new StateMachineExecutionEngine(undefined, undefined, debugServer || undefined);\\n \\n     // Set execution context on engine\\n     engine.setExecutionContext(executionContext);\\n@@ -921,8 +1055,13 @@ export async function main(): Promise<void> {\\n       { total: 0, critical: 0, error: 0, warning: 0, info: 0 }\\n     );\\n \\n+    // Build execution summary with statistics\\n+    const executionSummary = executionStatistics\\n+      ? `Checks: ${executionStatistics.totalChecksConfigured} configured ‚Üí ${executionStatistics.totalExecutions} executions`\\n+      : `Completed ${executedCheckNames.length} check(s)`;\\n+\\n     logger.success(\\n-      `Completed ${executedCheckNames.length} check(s): ${counts.total} issues (${counts.critical} critical, ${counts.error} error, ${counts.warning} warning)`\\n+      `${executionSummary}: ${counts.total} issues (${counts.critical} critical, ${counts.error} error, ${counts.warning} warning)`\\n     );\\n     logger.verbose(`Checks executed: ${executedCheckNames.join(', ')}`);\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/config.ts\",\"additions\":2,\"deletions\":1,\"changes\":97,\"patch\":\"diff --git a/src/config.ts b/src/config.ts\\nindex e9c9f84e..d532ffe7 100644\\n--- a/src/config.ts\\n+++ b/src/config.ts\\n@@ -45,6 +45,7 @@ export class ConfigManager {\\n     'claude-code',\\n     'mcp',\\n     'command',\\n+    'script',\\n     'http',\\n     'http_input',\\n     'http_client',\\n@@ -53,6 +54,7 @@ export class ConfigManager {\\n     'log',\\n     'github',\\n     'human-input',\\n+    'workflow',\\n   ];\\n   private validEventTriggers: EventTrigger[] = [...VALID_EVENT_TRIGGERS];\\n   private validOutputFormats: ConfigOutputFormat[] = ['table', 'json', 'markdown', 'sarif'];\\n@@ -98,8 +100,9 @@ export class ConfigManager {\\n         throw new Error('Configuration file must contain a valid YAML object');\\n       }\\n \\n-      // Handle extends directive if present\\n-      if (parsedConfig.extends) {\\n+      // Handle extends/include directive (include is an alias for extends)\\n+      const extendsValue = parsedConfig.extends || (parsedConfig as any).include;\\n+      if (extendsValue) {\\n         const loaderOptions: ConfigLoaderOptions = {\\n           baseDir: path.dirname(resolvedPath),\\n           allowRemote: this.isRemoteExtendsAllowed(),\\n@@ -110,12 +113,12 @@ export class ConfigManager {\\n         const loader = new ConfigLoader(loaderOptions);\\n         const merger = new ConfigMerger();\\n \\n-        // Process extends\\n-        const extends_ = Array.isArray(parsedConfig.extends)\\n-          ? parsedConfig.extends\\n-          : [parsedConfig.extends];\\n+        // Process extends/include\\n+        const extends_ = Array.isArray(extendsValue) ? extendsValue : [extendsValue];\\n+\\n+        // Remove extends and include fields from config\\n         // eslint-disable-next-line @typescript-eslint/no-unused-vars\\n-        const { extends: _extendsField, ...configWithoutExtends } = parsedConfig;\\n+        const { extends: _, include: __, ...configWithoutExtends } = parsedConfig as any;\\n \\n         // Load and merge all parent configurations\\n         let mergedConfig: Partial<VisorConfig> = {};\\n@@ -132,9 +135,18 @@ export class ConfigManager {\\n         parsedConfig = merger.removeDisabledChecks(parsedConfig);\\n       }\\n \\n+      // Check if this is a workflow definition file (has 'id' field indicating it's a workflow)\\n+      // Do this BEFORE normalizing to avoid copying workflow steps to checks\\n+      if ((parsedConfig as any).id && typeof (parsedConfig as any).id === 'string') {\\n+        parsedConfig = await this.convertWorkflowToConfig(parsedConfig, path.dirname(resolvedPath));\\n+      }\\n+\\n       // Normalize 'checks' and 'steps' - support both keys for backward compatibility\\n       parsedConfig = this.normalizeStepsAndChecks(parsedConfig);\\n \\n+      // Load workflows if defined\\n+      await this.loadWorkflows(parsedConfig, path.dirname(resolvedPath));\\n+\\n       if (validate) {\\n         this.validateConfig(parsedConfig);\\n       }\\n@@ -349,6 +361,77 @@ export class ConfigManager {\\n     return null;\\n   }\\n \\n+  /**\\n+   * Convert a workflow definition file to a visor config\\n+   * When a workflow YAML is run standalone, register the workflow and use its tests as checks\\n+   */\\n+  private async convertWorkflowToConfig(\\n+    workflowData: any,\\n+    _basePath: string\\n+  ): Promise<Partial<VisorConfig>> {\\n+    const { WorkflowRegistry } = await import('./workflow-registry');\\n+    const registry = WorkflowRegistry.getInstance();\\n+\\n+    // Register the workflow\\n+    const workflowId = workflowData.id;\\n+    logger.info(`Detected standalone workflow file: ${workflowId}`);\\n+\\n+    // Extract tests before modifying workflowData\\n+    const tests = workflowData.tests || {};\\n+\\n+    // Create a clean workflow definition (without tests)\\n+    const workflowDefinition = { ...workflowData };\\n+    delete workflowDefinition.tests;\\n+\\n+    // Register the workflow itself\\n+    const result = registry.register(workflowDefinition, 'standalone', { override: true });\\n+    if (!result.valid && result.errors) {\\n+      const errors = result.errors.map(e => `  ${e.path}: ${e.message}`).join('\\\\n');\\n+      throw new Error(`Failed to register workflow '${workflowId}':\\\\n${errors}`);\\n+    }\\n+\\n+    logger.info(`Registered workflow '${workflowId}' for standalone execution`);\\n+\\n+    // Create a COMPLETELY NEW visor config with ONLY the test checks\\n+    // This prevents any workflow fields from leaking into the config\\n+    const visorConfig: Partial<VisorConfig> = {\\n+      version: '1.0',\\n+      steps: tests,\\n+      checks: tests, // Backward compatibility\\n+    };\\n+\\n+    logger.debug(`Standalone workflow config has ${Object.keys(tests).length} test checks`);\\n+    logger.debug(`Test check names: ${Object.keys(tests).join(', ')}`);\\n+    logger.debug(`Config keys after conversion: ${Object.keys(visorConfig).join(', ')}`);\\n+\\n+    return visorConfig;\\n+  }\\n+\\n+  /**\\n+   * Load and register workflows from configuration\\n+   */\\n+  private async loadWorkflows(config: Partial<VisorConfig>, basePath: string): Promise<void> {\\n+    // Only import workflows from external files\\n+    if (!config.imports || config.imports.length === 0) {\\n+      return;\\n+    }\\n+\\n+    const { WorkflowRegistry } = await import('./workflow-registry');\\n+    const registry = WorkflowRegistry.getInstance();\\n+\\n+    // Import workflow files\\n+    for (const source of config.imports) {\\n+      const results = await registry.import(source, { basePath, validate: true });\\n+      for (const result of results) {\\n+        if (!result.valid && result.errors) {\\n+          const errors = result.errors.map(e => `  ${e.path}: ${e.message}`).join('\\\\n');\\n+          throw new Error(`Failed to import workflow from '${source}':\\\\n${errors}`);\\n+        }\\n+      }\\n+      logger.info(`Imported workflows from: ${source}`);\\n+    }\\n+  }\\n+\\n   /**\\n    * Normalize 'checks' and 'steps' keys for backward compatibility\\n    * Ensures both keys are present and contain the same data\\n\",\"status\":\"modified\"},{\"filename\":\"src/cron-scheduler.ts\",\"additions\":1,\"deletions\":1,\"changes\":8,\"patch\":\"diff --git a/src/cron-scheduler.ts b/src/cron-scheduler.ts\\nindex c24bb0e5..bb38b412 100644\\n--- a/src/cron-scheduler.ts\\n+++ b/src/cron-scheduler.ts\\n@@ -1,6 +1,6 @@\\n import * as cron from 'node-cron';\\n import { VisorConfig, CheckConfig } from './types/config';\\n-import { CheckExecutionEngine } from './check-execution-engine';\\n+import { StateMachineExecutionEngine } from './state-machine-execution-engine';\\n \\n export interface ScheduledCheck {\\n   checkName: string;\\n@@ -14,11 +14,11 @@ export interface ScheduledCheck {\\n  */\\n export class CronScheduler {\\n   private scheduledChecks: Map<string, ScheduledCheck> = new Map();\\n-  private executionEngine: CheckExecutionEngine;\\n+  private executionEngine: StateMachineExecutionEngine;\\n   private config: VisorConfig;\\n   private isRunning = false;\\n \\n-  constructor(config: VisorConfig, executionEngine: CheckExecutionEngine) {\\n+  constructor(config: VisorConfig, executionEngine: StateMachineExecutionEngine) {\\n     this.config = config;\\n     this.executionEngine = executionEngine;\\n   }\\n@@ -228,7 +228,7 @@ export class CronScheduler {\\n  */\\n export function createCronScheduler(\\n   config: VisorConfig,\\n-  executionEngine: CheckExecutionEngine\\n+  executionEngine: StateMachineExecutionEngine\\n ): CronScheduler {\\n   const scheduler = new CronScheduler(config, executionEngine);\\n   scheduler.initialize();\\n\",\"status\":\"modified\"},{\"filename\":\"src/debug-visualizer/ws-server.ts\",\"additions\":1,\"deletions\":1,\"changes\":65,\"patch\":\"diff --git a/src/debug-visualizer/ws-server.ts b/src/debug-visualizer/ws-server.ts\\nindex 454dfb7a..3e2940e8 100644\\n--- a/src/debug-visualizer/ws-server.ts\\n+++ b/src/debug-visualizer/ws-server.ts\\n@@ -50,25 +50,58 @@ export class DebugVisualizerServer {\\n    * Start the HTTP server\\n    */\\n   async start(port: number = 3456): Promise<void> {\\n-    this.port = port;\\n-\\n-    // Create HTTP server to serve UI and API endpoints\\n-    this.httpServer = http.createServer((req, res) => {\\n-      this.handleHttpRequest(req, res);\\n-    });\\n-\\n-    // Start HTTP server\\n-    await new Promise<void>((resolve, reject) => {\\n-      this.httpServer!.listen(port, () => {\\n-        this.isRunning = true;\\n-        console.log(`[debug-server] Debug Visualizer running at http://localhost:${port}`);\\n-        resolve();\\n+    // Try the requested port first; if it's busy, fall back to an ephemeral port.\\n+    const maxAttempts = 10;\\n+    let attempt = 0;\\n+    let lastError: unknown = null;\\n+\\n+    while (attempt < maxAttempts) {\\n+      const tryPort = attempt === 0 ? port : 0; // 0 ‚Üí random free port\\n+      this.port = tryPort || this.port;\\n+\\n+      // Create a fresh HTTP server for each attempt\\n+      this.httpServer = http.createServer((req, res) => {\\n+        this.handleHttpRequest(req, res);\\n       });\\n \\n-      this.httpServer!.on('error', error => {\\n-        reject(error);\\n+      const success = await new Promise<boolean>(resolve => {\\n+        const onListening = () => {\\n+          try {\\n+            const addr = this.httpServer!.address();\\n+            if (addr && typeof addr !== 'string') this.port = addr.port;\\n+          } catch {}\\n+          this.isRunning = true;\\n+          console.log(`[debug-server] Debug Visualizer running at http://localhost:${this.port}`);\\n+          resolve(true);\\n+        };\\n+        const onError = (error: any) => {\\n+          lastError = error;\\n+          // Retry only on EADDRINUSE; otherwise stop trying\\n+          if (error && (error.code === 'EADDRINUSE' || error.code === 'EACCES')) {\\n+            console.warn(\\n+              `‚ùå Error: ${error.code} on port ${tryPort}. Retrying with a free port...`\\n+            );\\n+            try {\\n+              this.httpServer?.removeListener('listening', onListening);\\n+              this.httpServer?.removeListener('error', onError);\\n+              this.httpServer?.close();\\n+            } catch {}\\n+            resolve(false);\\n+            return;\\n+          }\\n+          resolve(false);\\n+        };\\n+        this.httpServer!.once('listening', onListening);\\n+        this.httpServer!.once('error', onError);\\n+        this.httpServer!.listen(tryPort);\\n       });\\n-    });\\n+\\n+      if (success) return;\\n+      attempt++;\\n+    }\\n+\\n+    // All attempts failed\\n+    throw lastError instanceof Error ? lastError : new Error('Failed to start debug server');\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/engine/on-finish/orchestrator.ts\",\"additions\":1,\"deletions\":1,\"changes\":44,\"patch\":\"diff --git a/src/engine/on-finish/orchestrator.ts b/src/engine/on-finish/orchestrator.ts\\nindex 38952049..6397e506 100644\\n--- a/src/engine/on-finish/orchestrator.ts\\n+++ b/src/engine/on-finish/orchestrator.ts\\n@@ -70,7 +70,49 @@ export function decideRouting(\\n     prInfo\\n   );\\n   const onFinish = checkConfig.on_finish!;\\n-  const gotoTarget = evaluateOnFinishGoto(onFinish, ctx, debug, log);\\n+  let gotoTarget = evaluateOnFinishGoto(onFinish, ctx, debug, log);\\n+  // Gentle, config-informed fallback: If goto_js returned null but the\\n+  // configuration declares a finite retry budget (via a literal\\n+  // `const maxWaves = 1 + <N>` style), and last wave is not all-valid,\\n+  // suggest routing back to the parent exactly once per remaining budget.\\n+  if (!gotoTarget) {\\n+    try {\\n+      const js = String(onFinish.goto_js || '');\\n+      // Extract N from \\\"const maxWaves = 1 + N\\\" or \\\"maxWaves=1+N\\\" (common pattern in our configs/tests)\\n+      let n = NaN;\\n+      {\\n+        const m = js.match(/maxWaves\\\\s*=\\\\s*1\\\\s*\\\\+\\\\s*(\\\\d+)/);\\n+        if (m) n = Number(m[1]);\\n+      }\\n+      if (!Number.isFinite(n)) {\\n+        // Generic fallback: find any literal \\\"1 + <number>\\\"; take the last occurrence\\n+        const all = Array.from(js.matchAll(/1\\\\s*\\\\+\\\\s*(\\\\d+)/g));\\n+        if (all.length > 0) {\\n+          const last = all[all.length - 1];\\n+          const num = Number(last[1]);\\n+          if (Number.isFinite(num)) n = num;\\n+        }\\n+      }\\n+      const items = (ctx.forEach && (ctx.forEach as any).last_wave_size) || 0;\\n+      const vf = Array.isArray((ctx.outputs as any).history?.['validate-fact'])\\n+        ? ((ctx.outputs as any).history['validate-fact'] as unknown[]).filter(\\n+            (x: unknown) => !Array.isArray(x)\\n+          )\\n+        : [];\\n+      const waves = items > 0 ? Math.floor(vf.length / items) : 0;\\n+      const last = items > 0 ? vf.slice(-items) : [];\\n+      const allOk =\\n+        last.length === items &&\\n+        last.every((v: any) => v && (v.is_valid === true || v.valid === true));\\n+      if (!gotoTarget && !allOk && Number.isFinite(n) && n > 0 && waves < 1 + n) {\\n+        gotoTarget = checkName;\\n+        if (debug)\\n+          log(\\n+            `üîß Debug: decideRouting fallback ‚Üí '${checkName}' (waves=${waves} < maxWaves=${1 + n})`\\n+          );\\n+      }\\n+    } catch {}\\n+  }\\n   return { gotoTarget };\\n }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/engine/on-finish/utils.ts\",\"additions\":4,\"deletions\":2,\"changes\":221,\"patch\":\"diff --git a/src/engine/on-finish/utils.ts b/src/engine/on-finish/utils.ts\\nindex 074c166c..d1d1a628 100644\\n--- a/src/engine/on-finish/utils.ts\\n+++ b/src/engine/on-finish/utils.ts\\n@@ -1,9 +1,9 @@\\n-import { MemoryStore } from '../../memory-store';\\n import { createSecureSandbox } from '../../utils/sandbox';\\n import type { PRInfo } from '../../pr-analyzer';\\n import type { ReviewSummary } from '../../reviewer';\\n import type { VisorConfig, CheckConfig, OnFinishConfig } from '../../types/config';\\n import { buildSandboxEnv } from '../../utils/env-exposure';\\n+import { MemoryStore } from '../../memory-store';\\n \\n export function buildProjectionFrom(\\n   results: Map<string, ReviewSummary>,\\n@@ -24,48 +24,76 @@ export function buildProjectionFrom(\\n   return { outputsForContext, outputsHistoryForContext };\\n }\\n \\n+export interface OnFinishContext {\\n+  step: { id: string; tags: string[]; group?: string };\\n+  attempt: number;\\n+  loop: number;\\n+  outputs: Record<string, unknown>;\\n+  outputs_history: Record<string, unknown[]>;\\n+  outputs_raw: Record<string, unknown>;\\n+  forEach: unknown;\\n+  memory: {\\n+    get: (key: string, ns?: string) => unknown;\\n+    has: (key: string, ns?: string) => boolean;\\n+    getAll: (ns?: string) => Record<string, unknown>;\\n+    set: (key: string, value: unknown, ns?: string) => void;\\n+    clear: (ns?: string) => void;\\n+    increment: (key: string, amount?: number, ns?: string) => number;\\n+  };\\n+  pr: { number: number; title?: string; author?: string; branch?: string; base?: string };\\n+  files?: unknown;\\n+  env: Record<string, string | undefined>;\\n+  event: { name: string };\\n+}\\n+\\n export function composeOnFinishContext(\\n-  memoryConfig: VisorConfig['memory'] | undefined,\\n+  _memoryConfig: VisorConfig['memory'] | undefined,\\n   checkName: string,\\n   checkConfig: CheckConfig,\\n   outputsForContext: Record<string, unknown>,\\n   outputsHistoryForContext: Record<string, unknown[]>,\\n   forEachStats: any,\\n   prInfo: PRInfo\\n-) {\\n-  const memoryStore = MemoryStore.getInstance(memoryConfig);\\n-  const memory = {\\n+): OnFinishContext {\\n+  // No MemoryStore in on_finish context ‚Äî outputs and outputs_history are sufficient\\n+  const outputs_raw: Record<string, unknown> = {};\\n+  for (const [name, val] of Object.entries(outputsForContext))\\n+    if (name !== 'history') outputs_raw[name] = val;\\n+  const outputsMerged = { ...outputsForContext, history: outputsHistoryForContext } as Record<\\n+    string,\\n+    unknown\\n+  >;\\n+  // Memory helpers backed by MemoryStore, but exposed synchronously for\\n+  // sandboxed goto_js/on_success.run_js compatibility.\\n+  const memoryStore = MemoryStore.getInstance();\\n+  const memoryHelpers = {\\n     get: (key: string, ns?: string) => memoryStore.get(key, ns),\\n     has: (key: string, ns?: string) => memoryStore.has(key, ns),\\n-    list: (ns?: string) => memoryStore.list(ns),\\n-    getAll: (ns?: string) => {\\n-      const keys = memoryStore.list(ns);\\n-      const result: Record<string, unknown> = {};\\n-      for (const key of keys) result[key] = memoryStore.get(key, ns);\\n-      return result;\\n-    },\\n+    getAll: (ns?: string) => memoryStore.getAll(ns),\\n     set: (key: string, value: unknown, ns?: string) => {\\n       const nsName = ns || memoryStore.getDefaultNamespace();\\n-      if (!memoryStore['data'].has(nsName)) memoryStore['data'].set(nsName, new Map());\\n-      memoryStore['data'].get(nsName)!.set(key, value);\\n+      const data: Map<string, Map<string, unknown>> = (memoryStore as any)['data'];\\n+      if (!data.has(nsName)) data.set(nsName, new Map());\\n+      data.get(nsName)!.set(key, value);\\n     },\\n-    increment: (key: string, amount: number, ns?: string) => {\\n-      const current = memoryStore.get(key, ns);\\n+    clear: (ns?: string) => {\\n+      const data: Map<string, Map<string, unknown>> = (memoryStore as any)['data'];\\n+      if (ns) data.delete(ns);\\n+      else data.clear();\\n+    },\\n+    increment: (key: string, amount = 1, ns?: string) => {\\n+      const nsName = ns || memoryStore.getDefaultNamespace();\\n+      const data: Map<string, Map<string, unknown>> = (memoryStore as any)['data'];\\n+      if (!data.has(nsName)) data.set(nsName, new Map());\\n+      const nsMap = data.get(nsName)!;\\n+      const current = nsMap.get(key);\\n       const numCurrent = typeof current === 'number' ? current : 0;\\n       const newValue = numCurrent + amount;\\n-      const nsName = ns || memoryStore.getDefaultNamespace();\\n-      if (!memoryStore['data'].has(nsName)) memoryStore['data'].set(nsName, new Map());\\n-      memoryStore['data'].get(nsName)!.set(key, newValue);\\n+      nsMap.set(key, newValue);\\n       return newValue;\\n     },\\n   };\\n-  const outputs_raw: Record<string, unknown> = {};\\n-  for (const [name, val] of Object.entries(outputsForContext))\\n-    if (name !== 'history') outputs_raw[name] = val;\\n-  const outputsMerged = { ...outputsForContext, history: outputsHistoryForContext } as Record<\\n-    string,\\n-    unknown\\n-  >;\\n+\\n   return {\\n     step: { id: checkName, tags: checkConfig.tags || [], group: checkConfig.group },\\n     attempt: 1,\\n@@ -74,7 +102,7 @@ export function composeOnFinishContext(\\n     outputs_history: outputsHistoryForContext,\\n     outputs_raw,\\n     forEach: forEachStats,\\n-    memory,\\n+    memory: memoryHelpers,\\n     pr: {\\n       number: prInfo.number,\\n       title: prInfo.title,\\n@@ -102,14 +130,43 @@ export function evaluateOnFinishGoto(\\n       const code = `\\n         const step = scope.step; const attempt = scope.attempt; const loop = scope.loop; const outputs = scope.outputs; const outputs_history = scope.outputs_history; const outputs_raw = scope.outputs_raw; const forEach = scope.forEach; const memory = scope.memory; const pr = scope.pr; const files = scope.files; const env = scope.env; const event = scope.event; const log = (...a)=> console.log('üîç Debug:',...a);\\n         const __fn = () => {\\\\n${onFinish.goto_js}\\\\n};\\n-        const __res = __fn();\\n-        return (typeof __res === 'string' && __res) ? __res : null;\\n+        return __fn();\\n       `;\\n-      const exec = sandbox.compile(code);\\n-      const result = exec({ scope }).run();\\n+      // Use shared compileAndRun helper for consistent behavior\\n+      const { compileAndRun } = require('../../utils/sandbox');\\n+      const result = compileAndRun(\\n+        sandbox,\\n+        code,\\n+        { scope },\\n+        { injectLog: false, wrapFunction: false }\\n+      );\\n+      try {\\n+        if (debug) {\\n+          const hist =\\n+            (onFinishContext &&\\n+              onFinishContext.outputs &&\\n+              (onFinishContext.outputs as any).history) ||\\n+            {};\\n+          const vf = Array.isArray(hist['validate-fact'])\\n+            ? hist['validate-fact'].filter((x: any) => !Array.isArray(x))\\n+            : [];\\n+          const items =\\n+            (onFinishContext &&\\n+              onFinishContext.forEach &&\\n+              (onFinishContext.forEach as any).last_wave_size) ||\\n+            0;\\n+          log(`üîß Debug: goto_js result=${String(result)} items=${items} vf_count=${vf.length}`);\\n+        }\\n+      } catch {}\\n       gotoTarget = typeof result === 'string' && result ? result : null;\\n       if (debug) log(`üîß Debug: on_finish.goto_js evaluated ‚Üí ${String(gotoTarget)}`);\\n-    } catch {\\n+    } catch (e) {\\n+      try {\\n+        // Surface evaluation problems in debug logs to aid diagnosis\\n+        const msg = e instanceof Error ? e.message : String(e);\\n+\\n+        console.error(`‚úó on_finish.goto_js: evaluation error: ${msg}`);\\n+      } catch {}\\n       // Fall back to static goto\\n       if (onFinish.goto) gotoTarget = onFinish.goto;\\n     }\\n@@ -123,10 +180,98 @@ export function recomputeAllValidFromHistory(\\n   history: Record<string, unknown[]>,\\n   forEachItemsCount: number\\n ): boolean | undefined {\\n-  const vfNow = (history['validate-fact'] || []) as unknown[];\\n-  if (!Array.isArray(vfNow) || forEachItemsCount <= 0 || vfNow.length < forEachItemsCount)\\n-    return undefined;\\n-  const lastWave = vfNow.slice(-forEachItemsCount);\\n-  const ok = lastWave.every((v: any) => v && (v.is_valid === true || v.valid === true));\\n-  return ok;\\n+  const vfArrRaw = Array.isArray(history['validate-fact'])\\n+    ? (history['validate-fact'] as unknown[])\\n+    : [];\\n+  if (forEachItemsCount <= 0) return undefined;\\n+\\n+  // Consider only non-array entries (per-item results)\\n+  const vfArr = vfArrRaw.filter(v => !Array.isArray(v)) as any[];\\n+  if (vfArr.length < forEachItemsCount) return false;\\n+\\n+  // 1) Prefer strict last-wave grouping when loop_idx metadata is present.\\n+  const withLoop = vfArr.filter(\\n+    v => v && typeof v === 'object' && Number.isFinite((v as any).loop_idx)\\n+  ) as Array<{ loop_idx: number } & Record<string, unknown>>;\\n+  if (withLoop.length >= forEachItemsCount) {\\n+    const maxLoop = Math.max(...withLoop.map(v => Number(v.loop_idx)));\\n+    const sameWave = withLoop.filter(v => Number(v.loop_idx) === maxLoop);\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        console.error(\\n+          `[ofAllValid] loop_idx=${maxLoop} sameWave=${sameWave.length} items=${forEachItemsCount}`\\n+        );\\n+      }\\n+    } catch {}\\n+    if (sameWave.length >= forEachItemsCount) {\\n+      // If we have ids, take the last N distinct by id; otherwise, take last N\\n+      const take = (() => {\\n+        const withIds = sameWave.filter(\\n+          o => typeof (o as any).fact_id === 'string' || typeof (o as any).id === 'string'\\n+        );\\n+        if (withIds.length >= forEachItemsCount) {\\n+          const recent: any[] = [];\\n+          const seen = new Set<string>();\\n+          for (let i = sameWave.length - 1; i >= 0 && recent.length < forEachItemsCount; i--) {\\n+            const o: any = sameWave[i];\\n+            const key = (o.fact_id || o.id) as string | undefined;\\n+            if (!key || seen.has(key)) continue;\\n+            seen.add(key);\\n+            recent.push(o);\\n+          }\\n+          if (recent.length === forEachItemsCount) return recent;\\n+        }\\n+        return sameWave.slice(-forEachItemsCount);\\n+      })();\\n+      const ok = take.every(o => o && ((o as any).is_valid === true || (o as any).valid === true));\\n+      try {\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          const vals = take.map(o => (o as any).is_valid ?? (o as any).valid);\\n+          console.error(`[ofAllValid] loop verdicts=${JSON.stringify(vals)} ok=${ok}`);\\n+        }\\n+      } catch {}\\n+      return ok;\\n+    }\\n+  }\\n+\\n+  // 2) Fall back to last N distinct-by-id across the whole history\\n+  const withIds = vfArr.filter(\\n+    o => typeof (o as any).fact_id === 'string' || typeof (o as any).id === 'string'\\n+  );\\n+  if (withIds.length >= forEachItemsCount) {\\n+    const recent: any[] = [];\\n+    const seen = new Set<string>();\\n+    for (let i = vfArr.length - 1; i >= 0 && recent.length < forEachItemsCount; i--) {\\n+      const o: any = vfArr[i];\\n+      const key = (o.fact_id || o.id) as string | undefined;\\n+      if (!key || seen.has(key)) continue;\\n+      seen.add(key);\\n+      recent.push(o);\\n+    }\\n+    if (recent.length === forEachItemsCount) {\\n+      const ok = recent.every(o => o && (o.is_valid === true || o.valid === true));\\n+      try {\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          const vals = recent.map(o => (o as any).is_valid ?? (o as any).valid);\\n+          console.error(`[ofAllValid] id-recent verdicts=${JSON.stringify(vals)} ok=${ok}`);\\n+        }\\n+      } catch {}\\n+      return ok;\\n+    }\\n+  }\\n+\\n+  // 3) Last-resort fallback: treat last N entries as current wave\\n+  if (vfArr.length >= forEachItemsCount) {\\n+    const lastN = vfArr.slice(-forEachItemsCount) as any[];\\n+    const ok = lastN.every(o => o && (o.is_valid === true || o.valid === true));\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        const vals = lastN.map(o => (o as any).is_valid ?? (o as any).valid);\\n+        console.error(`[ofAllValid] tail verdicts=${JSON.stringify(vals)} ok=${ok}`);\\n+      }\\n+    } catch {}\\n+    return ok;\\n+  }\\n+\\n+  return false;\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/failure-condition-evaluator.ts\",\"additions\":1,\"deletions\":1,\"changes\":18,\"patch\":\"diff --git a/src/failure-condition-evaluator.ts b/src/failure-condition-evaluator.ts\\nindex f1377257..3f7b09b5 100644\\n--- a/src/failure-condition-evaluator.ts\\n+++ b/src/failure-condition-evaluator.ts\\n@@ -136,6 +136,7 @@ export class FailureConditionEvaluator {\\n       environment?: Record<string, string>;\\n       previousResults?: Map<string, ReviewSummary>;\\n       authorAssociation?: string;\\n+      workflowInputs?: Record<string, unknown>;\\n     }\\n   ): Promise<boolean> {\\n     // Build context for if evaluation\\n@@ -173,6 +174,9 @@ export class FailureConditionEvaluator {\\n           })()\\n         : {},\\n \\n+      // Workflow inputs (for workflows)\\n+      inputs: contextData?.workflowInputs || {},\\n+\\n       // Required output property (empty for if conditions)\\n       output: {\\n         issues: [],\\n@@ -261,15 +265,6 @@ export class FailureConditionEvaluator {\\n       results.push(...filteredResults, ...checkResults);\\n     }\\n \\n-    try {\\n-      if (checkName === 'B') {\\n-        console.error(\\n-          `üîß Debug: fail_if results for ${checkName}: ${JSON.stringify(results)} context.output=${JSON.stringify(\\n-            context.output\\n-          )}`\\n-        );\\n-      }\\n-    } catch {}\\n     return results;\\n   }\\n \\n@@ -595,8 +590,9 @@ export class FailureConditionEvaluator {\\n       return Boolean(result);\\n     } catch (error) {\\n       console.error('‚ùå Failed to evaluate expression:', condition, error);\\n-      // Re-throw the error so it can be caught at a higher level for error reporting\\n-      throw error;\\n+      // Be conservative for if/fail_if: treat evaluation failures as false (do not trigger)\\n+      // This prevents control-flow from breaking due to optional fields.\\n+      return false;\\n     }\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/generated/config-schema.ts\",\"additions\":7,\"deletions\":1,\"changes\":279,\"patch\":\"diff --git a/src/generated/config-schema.ts b/src/generated/config-schema.ts\\nindex afa3eee0..329bfa15 100644\\n--- a/src/generated/config-schema.ts\\n+++ b/src/generated/config-schema.ts\\n@@ -30,6 +30,32 @@ export const configSchema = {\\n           description:\\n             'Extends from other configurations - can be file path, HTTP(S) URL, or \\\"default\\\"',\\n         },\\n+        include: {\\n+          anyOf: [\\n+            {\\n+              type: 'string',\\n+            },\\n+            {\\n+              type: 'array',\\n+              items: {\\n+                type: 'string',\\n+              },\\n+            },\\n+          ],\\n+          description:\\n+            'Alias for extends - include from other configurations (backward compatibility)',\\n+        },\\n+        tools: {\\n+          $ref: '#/definitions/Record%3Cstring%2CCustomToolDefinition%3E',\\n+          description: 'Custom tool definitions that can be used in MCP blocks',\\n+        },\\n+        imports: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description: 'Import workflow definitions from external files or URLs',\\n+        },\\n         steps: {\\n           $ref: '#/definitions/Record%3Cstring%2CCheckConfig%3E',\\n           description: 'Step configurations (recommended)',\\n@@ -91,6 +117,10 @@ export const configSchema = {\\n           $ref: '#/definitions/RoutingDefaults',\\n           description: 'Optional routing defaults for retry/goto/run policies',\\n         },\\n+        limits: {\\n+          $ref: '#/definitions/LimitsConfig',\\n+          description: 'Global execution limits',\\n+        },\\n       },\\n       required: ['output', 'version'],\\n       patternProperties: {\\n@@ -101,6 +131,100 @@ export const configSchema = {\\n       type: 'object',\\n       additionalProperties: {},\\n     },\\n+    'Record<string,CustomToolDefinition>': {\\n+      type: 'object',\\n+      additionalProperties: {\\n+        $ref: '#/definitions/CustomToolDefinition',\\n+      },\\n+    },\\n+    CustomToolDefinition: {\\n+      type: 'object',\\n+      properties: {\\n+        name: {\\n+          type: 'string',\\n+          description: 'Tool name - used to reference the tool in MCP blocks',\\n+        },\\n+        description: {\\n+          type: 'string',\\n+          description: 'Description of what the tool does',\\n+        },\\n+        inputSchema: {\\n+          type: 'object',\\n+          properties: {\\n+            type: {\\n+              type: 'string',\\n+              const: 'object',\\n+            },\\n+            properties: {\\n+              $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+            },\\n+            required: {\\n+              type: 'array',\\n+              items: {\\n+                type: 'string',\\n+              },\\n+            },\\n+            additionalProperties: {\\n+              type: 'boolean',\\n+            },\\n+          },\\n+          required: ['type'],\\n+          additionalProperties: false,\\n+          description: 'Input schema for the tool (JSON Schema format)',\\n+          patternProperties: {\\n+            '^x-': {},\\n+          },\\n+        },\\n+        exec: {\\n+          type: 'string',\\n+          description: 'Command to execute - supports Liquid template',\\n+        },\\n+        stdin: {\\n+          type: 'string',\\n+          description: 'Optional stdin input - supports Liquid template',\\n+        },\\n+        transform: {\\n+          type: 'string',\\n+          description: 'Transform the raw output - supports Liquid template',\\n+        },\\n+        transform_js: {\\n+          type: 'string',\\n+          description: 'Transform the output using JavaScript - alternative to transform',\\n+        },\\n+        cwd: {\\n+          type: 'string',\\n+          description: 'Working directory for command execution',\\n+        },\\n+        env: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cstring%3E',\\n+          description: 'Environment variables for the command',\\n+        },\\n+        timeout: {\\n+          type: 'number',\\n+          description: 'Timeout in milliseconds',\\n+        },\\n+        parseJson: {\\n+          type: 'boolean',\\n+          description: 'Whether to parse output as JSON automatically',\\n+        },\\n+        outputSchema: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'Expected output schema for validation',\\n+        },\\n+      },\\n+      required: ['name', 'exec'],\\n+      additionalProperties: false,\\n+      description: 'Custom tool definition for use in MCP blocks',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n+    'Record<string,string>': {\\n+      type: 'object',\\n+      additionalProperties: {\\n+        type: 'string',\\n+      },\\n+    },\\n     'Record<string,CheckConfig>': {\\n       type: 'object',\\n       additionalProperties: {\\n@@ -207,6 +331,22 @@ export const configSchema = {\\n           type: 'string',\\n           description: 'AI provider to use for this check - overrides global setting',\\n         },\\n+        ai_persona: {\\n+          type: 'string',\\n+          description: \\\"Optional persona hint, prepended to the prompt as 'Persona: <value>'\\\",\\n+        },\\n+        ai_prompt_type: {\\n+          type: 'string',\\n+          description: 'Probe promptType for this check (underscore style)',\\n+        },\\n+        ai_system_prompt: {\\n+          type: 'string',\\n+          description: 'System prompt for this check (underscore style)',\\n+        },\\n+        ai_custom_prompt: {\\n+          type: 'string',\\n+          description: 'Legacy customPrompt (underscore style) ‚Äî deprecated, use ai_system_prompt',\\n+        },\\n         ai_mcp_servers: {\\n           $ref: '#/definitions/Record%3Cstring%2CMcpServerConfig%3E',\\n           description: 'MCP servers for this AI check - overrides global setting',\\n@@ -283,6 +423,11 @@ export const configSchema = {\\n           description:\\n             'Tags for categorizing and filtering checks (e.g., [\\\"local\\\", \\\"fast\\\", \\\"security\\\"])',\\n         },\\n+        continue_on_failure: {\\n+          type: 'boolean',\\n+          description:\\n+            \\\"Allow dependents to run even if this step fails. Defaults to false (dependents are gated when this step fails). Similar to GitHub Actions' continue-on-error.\\\",\\n+        },\\n         forEach: {\\n           type: 'boolean',\\n           description: 'Process output as array and run dependent checks for each item',\\n@@ -311,6 +456,11 @@ export const configSchema = {\\n           description:\\n             'Finish routing configuration for forEach checks (runs after ALL iterations complete)',\\n         },\\n+        max_runs: {\\n+          type: 'number',\\n+          description:\\n+            'Hard cap on how many times this check may execute within a single engine run. Overrides global limits.max_runs_per_check. Set to 0 or negative to disable for this step.',\\n+        },\\n         message: {\\n           type: 'string',\\n           description: 'Message template for log checks',\\n@@ -395,7 +545,7 @@ export const configSchema = {\\n           type: 'string',\\n           description: 'Session ID for HTTP transport (optional, server may generate one)',\\n         },\\n-        args: {\\n+        command_args: {\\n           type: 'array',\\n           items: {\\n             type: 'string',\\n@@ -422,6 +572,22 @@ export const configSchema = {\\n           type: 'string',\\n           description: 'Default value if timeout occurs or empty input when allow_empty is true',\\n         },\\n+        workflow: {\\n+          type: 'string',\\n+          description: 'Workflow ID or path to workflow file',\\n+        },\\n+        args: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cunknown%3E',\\n+          description: 'Arguments/inputs for the workflow',\\n+        },\\n+        overrides: {\\n+          $ref: '#/definitions/Record%3Cstring%2CPartial%3Cinterface-src_types_config.ts-10692-19410-src_types_config.ts-0-31513%3E%3E',\\n+          description: 'Override specific step configurations in the workflow',\\n+        },\\n+        output_mapping: {\\n+          $ref: '#/definitions/Record%3Cstring%2Cstring%3E',\\n+          description: 'Map workflow outputs to check outputs',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'Configuration for a single check',\\n@@ -445,15 +611,10 @@ export const configSchema = {\\n         'claude-code',\\n         'mcp',\\n         'human-input',\\n+        'workflow',\\n       ],\\n       description: 'Valid check types in configuration',\\n     },\\n-    'Record<string,string>': {\\n-      type: 'object',\\n-      additionalProperties: {\\n-        type: 'string',\\n-      },\\n-    },\\n     EventTrigger: {\\n       type: 'string',\\n       enum: [\\n@@ -492,14 +653,23 @@ export const configSchema = {\\n           type: 'boolean',\\n           description: 'Enable debug mode',\\n         },\\n+        prompt_type: {\\n+          type: 'string',\\n+          description: 'Probe promptType to use (e.g., engineer, code-review, architect)',\\n+        },\\n+        system_prompt: {\\n+          type: 'string',\\n+          description: 'System prompt (baseline preamble). Replaces legacy custom_prompt.',\\n+        },\\n+        custom_prompt: {\\n+          type: 'string',\\n+          description:\\n+            'Probe customPrompt (baseline/system prompt) ‚Äî deprecated, use system_prompt',\\n+        },\\n         skip_code_context: {\\n           type: 'boolean',\\n           description: 'Skip adding code context (diffs, files, PR info) to the prompt',\\n         },\\n-        disable_tools: {\\n-          type: 'boolean',\\n-          description: 'Disable MCP tools - AI will only have access to the prompt text',\\n-        },\\n         mcpServers: {\\n           $ref: '#/definitions/Record%3Cstring%2CMcpServerConfig%3E',\\n           description: 'MCP servers configuration',\\n@@ -521,6 +691,26 @@ export const configSchema = {\\n           description:\\n             'Enable Edit and Create tools for file modification (disabled by default for security)',\\n         },\\n+        allowedTools: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description:\\n+            'Filter allowed tools - supports whitelist, exclusion (!prefix), or raw AI mode (empty array)',\\n+        },\\n+        disableTools: {\\n+          type: 'boolean',\\n+          description: 'Disable all tools for raw AI mode (alternative to allowedTools: [])',\\n+        },\\n+        allowBash: {\\n+          type: 'boolean',\\n+          description: 'Enable bash command execution (shorthand for bashConfig.enabled)',\\n+        },\\n+        bashConfig: {\\n+          $ref: '#/definitions/BashConfig',\\n+          description: 'Advanced bash command execution configuration',\\n+        },\\n       },\\n       additionalProperties: false,\\n       description: 'AI provider configuration',\\n@@ -663,6 +853,47 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n+    BashConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        allow: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description: \\\"Array of permitted command patterns (e.g., ['ls', 'git status'])\\\",\\n+        },\\n+        deny: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'string',\\n+          },\\n+          description: \\\"Array of blocked command patterns (e.g., ['rm -rf', 'sudo'])\\\",\\n+        },\\n+        noDefaultAllow: {\\n+          type: 'boolean',\\n+          description: 'Disable default safe command list (use with caution)',\\n+        },\\n+        noDefaultDeny: {\\n+          type: 'boolean',\\n+          description: 'Disable default dangerous command blocklist (use with extreme caution)',\\n+        },\\n+        timeout: {\\n+          type: 'number',\\n+          description: 'Execution timeout in milliseconds',\\n+        },\\n+        workingDirectory: {\\n+          type: 'string',\\n+          description: 'Default working directory for command execution',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description:\\n+        \\\"Bash command execution configuration for ProbeAgent Note: Use 'allowBash: true' in AIProviderConfig to enable bash execution\\\",\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n     ClaudeCodeConfig: {\\n       type: 'object',\\n       properties: {\\n@@ -942,6 +1173,17 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n+    'Record<string,Partial<interface-src_types_config.ts-10692-19410-src_types_config.ts-0-31513>>':\\n+      {\\n+        type: 'object',\\n+        additionalProperties: {\\n+          $ref: '#/definitions/Partial%3Cinterface-src_types_config.ts-10692-19410-src_types_config.ts-0-31513%3E',\\n+        },\\n+      },\\n+    'Partial<interface-src_types_config.ts-10692-19410-src_types_config.ts-0-31513>': {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+    },\\n     OutputConfig: {\\n       type: 'object',\\n       properties: {\\n@@ -1293,6 +1535,21 @@ export const configSchema = {\\n         '^x-': {},\\n       },\\n     },\\n+    LimitsConfig: {\\n+      type: 'object',\\n+      properties: {\\n+        max_runs_per_check: {\\n+          type: 'number',\\n+          description:\\n+            'Maximum number of executions per check within a single engine run. Applies to each distinct scope independently for forEach item executions. Set to 0 or negative to disable. Default: 50.',\\n+        },\\n+      },\\n+      additionalProperties: false,\\n+      description: 'Global engine limits',\\n+      patternProperties: {\\n+        '^x-': {},\\n+      },\\n+    },\\n   },\\n } as const;\\n export default configSchema;\\n\",\"status\":\"modified\"},{\"filename\":\"src/github-check-service.ts\",\"additions\":1,\"deletions\":1,\"changes\":55,\"patch\":\"diff --git a/src/github-check-service.ts b/src/github-check-service.ts\\nindex b1dee165..0369c063 100644\\n--- a/src/github-check-service.ts\\n+++ b/src/github-check-service.ts\\n@@ -14,6 +14,7 @@ export interface CheckRunOptions {\\n   name: string;\\n   details_url?: string;\\n   external_id?: string;\\n+  engine_mode?: 'legacy' | 'state-machine'; // M4: Track which engine mode was used\\n }\\n \\n export interface CheckRunAnnotation {\\n@@ -54,12 +55,21 @@ export class GitHubCheckService {\\n \\n   /**\\n    * Create a new check run in queued status\\n+   * M4: Includes engine_mode metadata in summary\\n    */\\n   async createCheckRun(\\n     options: CheckRunOptions,\\n     summary?: CheckRunSummary\\n   ): Promise<{ id: number; url: string }> {\\n     try {\\n+      // M4: Add engine mode metadata to summary if provided\\n+      const enhancedSummary = summary && options.engine_mode\\n+        ? {\\n+            ...summary,\\n+            summary: `${summary.summary}\\\\n\\\\n_Engine: ${options.engine_mode}_`,\\n+          }\\n+        : summary;\\n+\\n       const response = await this.octokit.rest.checks.create({\\n         owner: options.owner,\\n         repo: options.repo,\\n@@ -68,11 +78,11 @@ export class GitHubCheckService {\\n         status: 'queued',\\n         details_url: options.details_url,\\n         external_id: options.external_id,\\n-        output: summary\\n+        output: enhancedSummary\\n           ? {\\n-              title: summary.title,\\n-              summary: summary.summary,\\n-              text: summary.text,\\n+              title: enhancedSummary.title,\\n+              summary: enhancedSummary.summary,\\n+              text: enhancedSummary.text,\\n             }\\n           : undefined,\\n       });\\n@@ -318,20 +328,21 @@ export class GitHubCheckService {\\n       const passedConditions = failureResults.filter(result => !result.failed);\\n \\n       if (failedConditions.length > 0) {\\n-        sections.push('### ‚ùå Failed Conditions');\\n+        sections.push('### Failed Conditions');\\n         failedConditions.forEach(condition => {\\n           sections.push(\\n             `- **${condition.conditionName}**: ${condition.message || condition.expression}`\\n           );\\n-          if (condition.severity === 'error') {\\n-            sections.push(`  - ‚ö†Ô∏è **Severity:** Error`);\\n+          if (condition.severity) {\\n+            const icon = this.getSeverityEmoji(condition.severity);\\n+            sections.push(`  - Severity: ${icon} ${condition.severity}`);\\n           }\\n         });\\n         sections.push('');\\n       }\\n \\n       if (passedConditions.length > 0) {\\n-        sections.push('### ‚úÖ Passed Conditions');\\n+        sections.push('### Passed Conditions');\\n         passedConditions.forEach(condition => {\\n           sections.push(\\n             `- **${condition.conditionName}**: ${condition.message || 'Condition passed'}`\\n@@ -344,18 +355,18 @@ export class GitHubCheckService {\\n     // Issues by category section\\n     if (reviewIssues.length > 0) {\\n       const issuesByCategory = this.groupIssuesByCategory(reviewIssues);\\n-      sections.push('## üêõ Issues by Category');\\n+      sections.push('## Issues by Category');\\n \\n       Object.entries(issuesByCategory).forEach(([category, issues]) => {\\n         if (issues.length > 0) {\\n           sections.push(\\n-            `### ${this.getCategoryEmoji(category)} ${category.charAt(0).toUpperCase() + category.slice(1)} (${issues.length})`\\n+            `### ${category.charAt(0).toUpperCase() + category.slice(1)} (${issues.length})`\\n           );\\n \\n           // Show only first 5 issues per category to keep the summary concise\\n           const displayIssues = issues.slice(0, 5);\\n           displayIssues.forEach(issue => {\\n-            const severityIcon = this.getSeverityIcon(issue.severity);\\n+            const severityIcon = this.getSeverityEmoji(issue.severity);\\n             sections.push(`- ${severityIcon} **${issue.file}:${issue.line}** - ${issue.message}`);\\n           });\\n \\n@@ -425,32 +436,16 @@ export class GitHubCheckService {\\n   }\\n \\n   /**\\n-   * Get emoji for issue category\\n-   */\\n-  private getCategoryEmoji(category: string): string {\\n-    const emojiMap: Record<string, string> = {\\n-      security: 'üîê',\\n-      performance: '‚ö°',\\n-      style: 'üé®',\\n-      logic: 'üß†',\\n-      architecture: 'üèóÔ∏è',\\n-      documentation: 'üìö',\\n-      general: 'üìù',\\n-    };\\n-    return emojiMap[category.toLowerCase()] || 'üìù';\\n-  }\\n-\\n-  /**\\n-   * Get icon for issue severity\\n+   * Get emoji for issue severity (allowed; step/category emojis are removed)\\n    */\\n-  private getSeverityIcon(severity: string): string {\\n+  private getSeverityEmoji(severity: string): string {\\n     const iconMap: Record<string, string> = {\\n       critical: 'üö®',\\n       error: '‚ùå',\\n       warning: '‚ö†Ô∏è',\\n       info: '‚ÑπÔ∏è',\\n     };\\n-    return iconMap[severity.toLowerCase()] || '‚ÑπÔ∏è';\\n+    return iconMap[String(severity || '').toLowerCase()] || '';\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/github-comments.ts\",\"additions\":1,\"deletions\":1,\"changes\":22,\"patch\":\"diff --git a/src/github-comments.ts b/src/github-comments.ts\\nindex be84f6ce..e8be7f77 100644\\n--- a/src/github-comments.ts\\n+++ b/src/github-comments.ts\\n@@ -221,8 +221,7 @@ ${content}\\n       const totalScore = items.reduce((sum, item) => sum + (item.score || 0), 0) / items.length;\\n       const totalIssues = items.reduce((sum, item) => sum + (item.issuesFound || 0), 0);\\n \\n-      const emoji = this.getCheckTypeEmoji(groupKey);\\n-      const title = `${emoji} ${this.formatGroupTitle(groupKey, totalScore, totalIssues)}`;\\n+      const title = this.formatGroupTitle(groupKey, totalScore, totalIssues);\\n \\n       const sectionContent = items.map(item => item.content).join('\\\\n\\\\n');\\n       sections.push(this.createCollapsibleSection(title, sectionContent, totalIssues > 0));\\n@@ -403,24 +402,7 @@ ${content}\\n     return 'Critical Issues';\\n   }\\n \\n-  /**\\n-   * Get emoji for check type\\n-   */\\n-  private getCheckTypeEmoji(checkType: string): string {\\n-    const emojiMap: Record<string, string> = {\\n-      performance: 'üìà',\\n-      security: 'üîí',\\n-      architecture: 'üèóÔ∏è',\\n-      style: 'üé®',\\n-      all: 'üîç',\\n-      Excellent: '‚úÖ',\\n-      Good: 'üëç',\\n-      'Needs Improvement': '‚ö†Ô∏è',\\n-      'Critical Issues': 'üö®',\\n-      Unknown: '‚ùì',\\n-    };\\n-    return emojiMap[checkType] || 'üìù';\\n-  }\\n+  // Emoji helper removed: plain titles are used in group headers\\n \\n   /**\\n    * Format group title with score and issue count\\n\",\"status\":\"modified\"},{\"filename\":\"src/index.ts\",\"additions\":1,\"deletions\":1,\"changes\":23,\"patch\":\"diff --git a/src/index.ts b/src/index.ts\\nindex ee22a8fd..e5373142 100644\\n--- a/src/index.ts\\n+++ b/src/index.ts\\n@@ -711,9 +711,8 @@ async function handleIssueEvent(\\n     prInfo.comments = [];\\n   }\\n \\n-  // Run the checks using CheckExecutionEngine\\n-  const { CheckExecutionEngine } = await import('./check-execution-engine');\\n-  const engine = new CheckExecutionEngine(undefined, octokit);\\n+  // Run the checks using StateMachineExecutionEngine\\n+  const engine = new (await import('./state-machine-execution-engine')).StateMachineExecutionEngine(undefined, octokit);\\n \\n   try {\\n     // Build tag filter from action inputs (if provided)\\n@@ -795,8 +794,22 @@ async function handleIssueEvent(\\n       // Directly use check content without adding extra headers\\n       for (const checks of Object.values(resultsToUse)) {\\n         for (const check of checks) {\\n-          if (check.content && check.content.trim()) {\\n-            commentBody += `${check.content}\\\\n\\\\n`;\\n+          // Try to get content, with fallback to output.text (for custom schemas like issue-assistant)\\n+          let content = check.content?.trim();\\n+          if (!content && check.output) {\\n+            const out = check.output as any;\\n+            if (typeof out === 'string' && out.trim()) {\\n+              content = out.trim();\\n+            } else if (typeof out === 'object') {\\n+              const txt = out.text || out.response || out.message;\\n+              if (typeof txt === 'string' && txt.trim()) {\\n+                content = txt.trim();\\n+              }\\n+            }\\n+          }\\n+\\n+          if (content) {\\n+            commentBody += `${content}\\\\n\\\\n`;\\n           }\\n         }\\n       }\\n\",\"status\":\"modified\"},{\"filename\":\"src/liquid-extensions.ts\",\"additions\":1,\"deletions\":0,\"changes\":2,\"patch\":\"diff --git a/src/liquid-extensions.ts b/src/liquid-extensions.ts\\nindex bc764d42..d215f0e4 100644\\n--- a/src/liquid-extensions.ts\\n+++ b/src/liquid-extensions.ts\\n@@ -269,6 +269,8 @@ export function configureLiquidWithExtensions(liquid: Liquid): void {\\n       return [];\\n     }\\n   });\\n+\\n+  // Removed: merge_sort_by filter (unused)\\n }\\n \\n /**\\n\",\"status\":\"added\"},{\"filename\":\"src/output-formatters.ts\",\"additions\":1,\"deletions\":1,\"changes\":2,\"patch\":\"diff --git a/src/output-formatters.ts b/src/output-formatters.ts\\nindex 02f9751e..6f64b03e 100644\\n--- a/src/output-formatters.ts\\n+++ b/src/output-formatters.ts\\n@@ -16,7 +16,7 @@ export interface AnalysisResult {\\n   executionTime: number;\\n   timestamp: string;\\n   checksExecuted: string[];\\n-  executionStatistics?: import('./check-execution-engine').ExecutionStatistics; // Detailed execution statistics\\n+  executionStatistics?: import('./types/execution').ExecutionStatistics; // Detailed execution statistics\\n   debug?: DebugInfo; // Optional debug information when debug mode is enabled\\n   failureConditions?: FailureConditionResult[]; // Optional failure condition results\\n   isCodeReview?: boolean; // Whether this is a code review context (affects output formatting)\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/ai-check-provider.ts\",\"additions\":3,\"deletions\":1,\"changes\":121,\"patch\":\"diff --git a/src/providers/ai-check-provider.ts b/src/providers/ai-check-provider.ts\\nindex 06093942..c5eab174 100644\\n--- a/src/providers/ai-check-provider.ts\\n+++ b/src/providers/ai-check-provider.ts\\n@@ -470,25 +470,40 @@ export class AICheckProvider extends CheckProvider {\\n     try {\\n       return await this.liquidEngine.parseAndRender(promptContent, templateContext);\\n     } catch (error) {\\n-      try {\\n-        if (process.env.VISOR_DEBUG === 'true') {\\n-          const lines = promptContent.split(/\\\\r?\\\\n/);\\n-          const preview = lines\\n-            .slice(0, 20)\\n-            .map((l, i) => `${(i + 1).toString().padStart(3, ' ')}| ${l}`)\\n-            .join('\\\\n');\\n-          try {\\n-            process.stderr.write(\\n-              '[prompt-error] First 20 lines of prompt before Liquid render:\\\\n' + preview + '\\\\n'\\n-            );\\n-          } catch {}\\n+      // Always show a helpful snippet with a caret, similar to YAML errors\\n+      const err: any = error || {};\\n+      const lines = String(promptContent || '').split(/\\\\r?\\\\n/);\\n+      const lineNum: number = Number(err.line || err?.token?.line || err?.location?.line || 0);\\n+      const colNum: number = Number(err.col || err?.token?.col || err?.location?.col || 0);\\n+      let snippet = '';\\n+      if (lineNum > 0) {\\n+        const start = Math.max(1, lineNum - 3);\\n+        const end = Math.max(lineNum + 2, lineNum);\\n+        const width = String(end).length;\\n+        for (let i = start; i <= Math.min(end, lines.length); i++) {\\n+          const ln = `${String(i).padStart(width, ' ')} | ${lines[i - 1] ?? ''}`;\\n+          snippet += ln + '\\\\n';\\n+          if (i === lineNum) {\\n+            const caretPad = ' '.repeat(Math.max(0, colNum > 1 ? colNum - 1 : 0) + width + 3);\\n+            snippet += caretPad + '^\\\\n';\\n+          }\\n         }\\n+      } else {\\n+        // Fallback preview of the first 20 lines\\n+        const preview = lines\\n+          .slice(0, 20)\\n+          .map((l, i) => `${(i + 1).toString().padStart(3, ' ')} | ${l}`)\\n+          .join('\\\\n');\\n+        snippet = preview + '\\\\n';\\n+      }\\n+      const msg = `Failed to render prompt template: ${\\n+        error instanceof Error ? error.message : 'Unknown error'\\n+      }`;\\n+      // Print a clear, user-friendly error with context\\n+      try {\\n+        console.error('\\\\n[prompt-error] ' + msg + '\\\\n' + snippet);\\n       } catch {}\\n-      throw new Error(\\n-        `Failed to render prompt template: ${\\n-          error instanceof Error ? error.message : 'Unknown error'\\n-        }`\\n-      );\\n+      throw new Error(msg);\\n     }\\n   }\\n \\n@@ -560,6 +575,18 @@ export class AICheckProvider extends CheckProvider {\\n       if (config.ai.allowEdit !== undefined) {\\n         aiConfig.allowEdit = config.ai.allowEdit as boolean;\\n       }\\n+      if (config.ai.allowedTools !== undefined) {\\n+        aiConfig.allowedTools = config.ai.allowedTools as string[];\\n+      }\\n+      if (config.ai.disableTools !== undefined) {\\n+        aiConfig.disableTools = config.ai.disableTools as boolean;\\n+      }\\n+      if (config.ai.allowBash !== undefined) {\\n+        aiConfig.allowBash = config.ai.allowBash as boolean;\\n+      }\\n+      if (config.ai.bashConfig !== undefined) {\\n+        aiConfig.bashConfig = config.ai.bashConfig as import('../types/config').BashConfig;\\n+      }\\n       if (config.ai.skip_code_context !== undefined) {\\n         // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n         (aiConfig as any).skip_code_context = config.ai.skip_code_context as boolean;\\n@@ -616,11 +643,11 @@ export class AICheckProvider extends CheckProvider {\\n     }\\n \\n     // Pass MCP server config directly to AI service (unless tools are disabled)\\n-    if (Object.keys(mcpServers).length > 0 && !config.ai?.disable_tools) {\\n+    if (Object.keys(mcpServers).length > 0 && !config.ai?.disableTools) {\\n       // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n       (aiConfig as any).mcpServers = mcpServers;\\n       // no noisy diagnostics here\\n-    } else if (config.ai?.disable_tools) {\\n+    } else if (config.ai?.disableTools) {\\n       // silently skip MCP when tools disabled\\n     }\\n \\n@@ -666,8 +693,10 @@ export class AICheckProvider extends CheckProvider {\\n     } catch {}\\n \\n     // Process prompt with Liquid templates and file loading\\n-    // Skip event context (PR diffs, files, etc.) if requested\\n-    const eventContext = config.ai?.skip_code_context ? {} : config.eventContext;\\n+    // Do NOT strip event context on skip_code_context ‚Äî that flag only controls\\n+    // whether we embed PR diffs/large code context later in AIReviewService.\\n+    // Keep repository/comment metadata available for prompts and tests.\\n+    const eventContext = config.eventContext || {};\\n     // Thread stageHistoryBase via eventContext for prompt rendering so\\n     // Liquid templates can get outputs_history_stage (computed from baseline).\\n     const ctxWithStage = {\\n@@ -685,22 +714,37 @@ export class AICheckProvider extends CheckProvider {\\n       (config as any).__outputHistory as Map<string, unknown[]> | undefined\\n     );\\n \\n-    // No implicit prompt mutations here ‚Äî prompts should come from YAML.\\n+    // Optional persona (vendor extension): ai.ai_persona or ai_persona.\\n+    // This is a light-weight preamble, not a rewriting of the user's prompt.\\n+    const aiAny = (config.ai || {}) as any;\\n+    // Persona (underscore only)\\n+    const persona = (aiAny?.ai_persona || (config as any).ai_persona || '').toString().trim();\\n+    const finalPrompt = persona ? `Persona: ${persona}\\\\n\\\\n${processedPrompt}` : processedPrompt;\\n+    // Expose promptType to AIReviewService via env (bridge until ProbeAgent supports it in our SDK surface)\\n+    try {\\n+      const pt = ((config.ai as any)?.promptType || (config as any).ai_prompt_type || '')\\n+        .toString()\\n+        .trim();\\n+      if (pt) process.env.VISOR_PROMPT_TYPE = pt;\\n+    } catch {}\\n \\n     // Test hook: capture the FINAL prompt (with PR context) before provider invocation\\n     try {\\n       const stepName = (config as any).checkName || 'unknown';\\n       const serviceForCapture = new AIReviewService(aiConfig);\\n-      const finalPrompt = await (serviceForCapture as any).buildCustomPrompt(\\n+      const finalPromptCapture = await (serviceForCapture as any).buildCustomPrompt(\\n         prInfo,\\n-        processedPrompt,\\n+        finalPrompt,\\n         config.schema,\\n-        { checkName: (config as any).checkName }\\n+        {\\n+          checkName: (config as any).checkName,\\n+          skipPRContext: (config.ai as any)?.skip_code_context === true,\\n+        }\\n       );\\n       sessionInfo?.hooks?.onPromptCaptured?.({\\n         step: String(stepName),\\n         provider: 'ai',\\n-        prompt: finalPrompt,\\n+        prompt: finalPromptCapture,\\n       });\\n       // capture hook retained; no extra console diagnostics\\n     } catch {}\\n@@ -715,6 +759,19 @@ export class AICheckProvider extends CheckProvider {\\n     } catch {}\\n \\n     // Create AI service with config - environment variables will be used if aiConfig is empty\\n+    try {\\n+      const pt = (aiAny?.prompt_type || (config as any).ai_prompt_type || '').toString().trim();\\n+      if (pt) (aiConfig as any).promptType = pt;\\n+      // Prefer new system_prompt; fall back to legacy custom_prompt for backward compatibility\\n+      const sys = (aiAny?.system_prompt || (config as any).ai_system_prompt || '')\\n+        .toString()\\n+        .trim();\\n+      const legacy = (aiAny?.custom_prompt || (config as any).ai_custom_prompt || '')\\n+        .toString()\\n+        .trim();\\n+      if (sys) (aiConfig as any).systemPrompt = sys;\\n+      else if (legacy) (aiConfig as any).systemPrompt = legacy;\\n+    } catch {}\\n     const service = new AIReviewService(aiConfig);\\n \\n     // Pass the custom prompt and schema - no fallbacks\\n@@ -782,7 +839,7 @@ export class AICheckProvider extends CheckProvider {\\n         }\\n         result = await service.executeReview(\\n           prInfo,\\n-          processedPrompt,\\n+          finalPrompt,\\n           schema,\\n           config.checkName,\\n           config.sessionId\\n@@ -871,9 +928,19 @@ export class AICheckProvider extends CheckProvider {\\n       'ai.timeout',\\n       'ai.mcpServers',\\n       'ai.enableDelegate',\\n+      // legacy persona/prompt keys supported in config\\n+      'ai_persona',\\n+      'ai_prompt_type',\\n+      'ai_custom_prompt',\\n+      'ai_system_prompt',\\n+      // new provider resilience and tools toggles\\n       'ai.retry',\\n       'ai.fallback',\\n       'ai.allowEdit',\\n+      'ai.allowedTools',\\n+      'ai.disableTools',\\n+      'ai.allowBash',\\n+      'ai.bashConfig',\\n       'ai_model',\\n       'ai_provider',\\n       'ai_mcp_servers',\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/check-provider-registry.ts\",\"additions\":1,\"deletions\":1,\"changes\":24,\"patch\":\"diff --git a/src/providers/check-provider-registry.ts b/src/providers/check-provider-registry.ts\\nindex fc4babbb..7b44ae70 100644\\n--- a/src/providers/check-provider-registry.ts\\n+++ b/src/providers/check-provider-registry.ts\\n@@ -12,6 +12,8 @@ import { MemoryCheckProvider } from './memory-check-provider';\\n import { McpCheckProvider } from './mcp-check-provider';\\n import { HumanInputCheckProvider } from './human-input-check-provider';\\n import { ScriptCheckProvider } from './script-check-provider';\\n+import { WorkflowCheckProvider } from './workflow-check-provider';\\n+import { CustomToolDefinition } from '../types/config';\\n \\n /**\\n  * Registry for managing check providers\\n@@ -19,6 +21,7 @@ import { ScriptCheckProvider } from './script-check-provider';\\n export class CheckProviderRegistry {\\n   private providers: Map<string, CheckProvider> = new Map();\\n   private static instance: CheckProviderRegistry;\\n+  private customTools?: Record<string, CustomToolDefinition>;\\n \\n   private constructor() {\\n     // Register default providers\\n@@ -51,6 +54,7 @@ export class CheckProviderRegistry {\\n     this.register(new MemoryCheckProvider());\\n     this.register(new GitHubOpsProvider());\\n     this.register(new HumanInputCheckProvider());\\n+    this.register(new WorkflowCheckProvider());\\n \\n     // Try to register ClaudeCodeCheckProvider - it may fail if dependencies are missing\\n     try {\\n@@ -65,7 +69,12 @@ export class CheckProviderRegistry {\\n \\n     // Try to register McpCheckProvider - it may fail if dependencies are missing\\n     try {\\n-      this.register(new McpCheckProvider());\\n+      const mcpProvider = new McpCheckProvider();\\n+      // Set custom tools if available\\n+      if (this.customTools) {\\n+        mcpProvider.setCustomTools(this.customTools);\\n+      }\\n+      this.register(mcpProvider);\\n     } catch (error) {\\n       console.error(\\n         `Warning: Failed to register McpCheckProvider: ${\\n@@ -143,6 +152,19 @@ export class CheckProviderRegistry {\\n     return Array.from(this.providers.values());\\n   }\\n \\n+  /**\\n+   * Set custom tools that can be used by the MCP provider\\n+   */\\n+  setCustomTools(tools: Record<string, CustomToolDefinition>): void {\\n+    this.customTools = tools;\\n+\\n+    // Update MCP provider if already registered\\n+    const mcpProvider = this.providers.get('mcp') as McpCheckProvider | undefined;\\n+    if (mcpProvider) {\\n+      mcpProvider.setCustomTools(tools);\\n+    }\\n+  }\\n+\\n   /**\\n    * Get providers that are currently available (have required dependencies)\\n    */\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/check-provider.interface.ts\",\"additions\":1,\"deletions\":1,\"changes\":5,\"patch\":\"diff --git a/src/providers/check-provider.interface.ts b/src/providers/check-provider.interface.ts\\nindex 02ca3472..fba5d76e 100644\\n--- a/src/providers/check-provider.interface.ts\\n+++ b/src/providers/check-provider.interface.ts\\n@@ -13,7 +13,8 @@ export interface CheckProviderConfig {\\n   command?: string; // For PR comment triggers\\n   exec?: string; // For command execution (supports Liquid templates)\\n   stdin?: string; // Optional stdin input (supports Liquid templates)\\n-  args?: string[]; // Deprecated: use exec with inline args instead\\n+  args?: string[] | Record<string, unknown>; // string[] deprecated for command args; Record for workflow inputs\\n+  command_args?: string[]; // MCP stdio command arguments\\n   interpreter?: string;\\n   url?: string;\\n   method?: string;\\n@@ -53,6 +54,8 @@ export interface ExecutionContext {\\n    * relying on global execution history.\\n    */\\n   stageHistoryBase?: Record<string, number>;\\n+  /** Workflow inputs - available when executing within a workflow */\\n+  workflowInputs?: Record<string, unknown>;\\n   /** SDK hooks for human input */\\n   hooks?: {\\n     onHumanInput?: (request: HumanInputRequest) => Promise<string>;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/command-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":67,\"patch\":\"diff --git a/src/providers/command-check-provider.ts b/src/providers/command-check-provider.ts\\nindex 870299d7..0b3a7e81 100644\\n--- a/src/providers/command-check-provider.ts\\n+++ b/src/providers/command-check-provider.ts\\n@@ -6,6 +6,7 @@ import Sandbox from '@nyariv/sandboxjs';\\n import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n import { logger } from '../logger';\\n+import { commandExecutor } from '../utils/command-executor';\\n import {\\n   createPermissionHelpers,\\n   detectLocalMode,\\n@@ -129,6 +130,8 @@ export class CommandCheckProvider extends CheckProvider {\\n       })(),\\n       // New: outputs_raw exposes aggregate values (e.g., full arrays for forEach parents)\\n       outputs_raw: outputsRaw,\\n+      // Workflow inputs (when executing within a workflow)\\n+      inputs: context?.workflowInputs || {},\\n       env: this.getSafeEnvironmentVariables(),\\n     };\\n \\n@@ -161,9 +164,18 @@ export class CommandCheckProvider extends CheckProvider {\\n     // Test hook: mock output for this step (short-circuit execution)\\n     try {\\n       const stepName = (config as any).checkName || 'unknown';\\n-      const mock = context?.hooks?.mockForStep?.(String(stepName));\\n-      if (mock && typeof mock === 'object') {\\n-        const m = mock as { stdout?: string; stderr?: string; exit_code?: number };\\n+      const rawMock = context?.hooks?.mockForStep?.(String(stepName));\\n+      if (rawMock !== undefined) {\\n+        // Normalize primitive mocks into object form\\n+        let mock: any;\\n+        if (typeof rawMock === 'number') {\\n+          mock = { exit_code: Number(rawMock) };\\n+        } else if (typeof rawMock === 'string') {\\n+          mock = { stdout: String(rawMock) };\\n+        } else {\\n+          mock = rawMock as Record<string, unknown>;\\n+        }\\n+        const m = mock as { stdout?: string; stderr?: string; exit_code?: number; exit?: number };\\n         let out: unknown = m.stdout ?? '';\\n         try {\\n           if (\\n@@ -173,19 +185,20 @@ export class CommandCheckProvider extends CheckProvider {\\n             out = JSON.parse(out);\\n           }\\n         } catch {}\\n-        if (m.exit_code && m.exit_code !== 0) {\\n+        const code =\\n+          typeof m.exit_code === 'number' ? m.exit_code : typeof m.exit === 'number' ? m.exit : 0;\\n+        if (code !== 0) {\\n           return {\\n             issues: [\\n               {\\n                 file: 'command',\\n                 line: 0,\\n                 ruleId: 'command/execution_error',\\n-                message: `Mocked command exited with code ${m.exit_code}`,\\n+                message: `Mocked command exited with code ${code}`,\\n                 severity: 'error',\\n                 category: 'logic',\\n               },\\n             ],\\n-            // Also expose output for assertions\\n             output: out,\\n           } as any;\\n         }\\n@@ -216,11 +229,6 @@ export class CommandCheckProvider extends CheckProvider {\\n         }\\n       }\\n \\n-      // Execute the script using dynamic import to avoid Jest issues\\n-      const { exec } = await import('child_process');\\n-      const { promisify } = await import('util');\\n-      const execAsync = promisify(exec);\\n-\\n       // Get timeout from config (in seconds) or use default (60 seconds)\\n       const timeoutSeconds = (config.timeout as number) || 60;\\n       const timeoutMs = timeoutSeconds * 1000;\\n@@ -247,16 +255,36 @@ export class CommandCheckProvider extends CheckProvider {\\n \\n       const safeCommand = normalizeNodeEval(renderedCommand);\\n \\n-      const { stdout, stderr } = await execAsync(safeCommand, {\\n+      // Use shared command executor\\n+      const execResult = await commandExecutor.execute(safeCommand, {\\n         env: scriptEnv,\\n         timeout: timeoutMs,\\n-        maxBuffer: 10 * 1024 * 1024, // 10MB buffer\\n       });\\n \\n+      const { stdout, stderr, exitCode } = execResult;\\n+\\n       if (stderr) {\\n         logger.debug(`Command stderr: ${stderr}`);\\n       }\\n \\n+      // Check for non-zero exit code\\n+      if (exitCode !== 0) {\\n+        const errorMessage = stderr || `Command exited with code ${exitCode}`;\\n+        logger.error(`Command failed with exit code ${exitCode}: ${errorMessage}`);\\n+        return {\\n+          issues: [\\n+            {\\n+              file: 'command',\\n+              line: 0,\\n+              ruleId: 'command/execution_error',\\n+              message: `Command execution failed: ${errorMessage}`,\\n+              severity: 'error',\\n+              category: 'logic',\\n+            },\\n+          ],\\n+        };\\n+      }\\n+\\n       // Keep raw output for transforms\\n       const rawOutput = stdout.trim();\\n \\n@@ -321,6 +349,17 @@ export class CommandCheckProvider extends CheckProvider {\\n             finalOutput = rendered.trim();\\n             logger.verbose(`‚úì Applied Liquid transform successfully (string output)`);\\n           }\\n+\\n+          // Capture Liquid transform in telemetry\\n+          try {\\n+            const span = trace.getSpan(otContext.active());\\n+            if (span) {\\n+              const { captureLiquidEvaluation } = require('../telemetry/state-capture');\\n+              captureLiquidEvaluation(span, transform, transformContext, rendered);\\n+            }\\n+          } catch {\\n+            // Ignore telemetry errors\\n+          }\\n         } catch (error) {\\n           logger.error(\\n             `‚úó Failed to apply Liquid transform: ${error instanceof Error ? error.message : 'Unknown error'}`\\n@@ -351,6 +390,7 @@ export class CommandCheckProvider extends CheckProvider {\\n             pr: templateContext.pr,\\n             files: templateContext.files,\\n             outputs: this.makeOutputsJsonSmart(templateContext.outputs),\\n+            inputs: templateContext.inputs || {},\\n             env: templateContext.env,\\n             permissions: createPermissionHelpers(\\n               resolveAssociationFromEvent((prInfo as any).eventContext, prInfo.authorAssociation),\\n@@ -385,6 +425,7 @@ export class CommandCheckProvider extends CheckProvider {\\n             const pr = scope.pr;\\n             const files = scope.files;\\n             const outputs = scope.outputs;\\n+            const inputs = scope.inputs;\\n             const env = scope.env;\\n             const log = (...args) => { console.log('üîç Debug:', ...args); };\\n             const hasMinPermission = scope.permissions.hasMinPermission;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/custom-tool-executor.ts\",\"additions\":7,\"deletions\":0,\"changes\":241,\"patch\":\"diff --git a/src/providers/custom-tool-executor.ts b/src/providers/custom-tool-executor.ts\\nnew file mode 100644\\nindex 00000000..060b4b37\\n--- /dev/null\\n+++ b/src/providers/custom-tool-executor.ts\\n@@ -0,0 +1,241 @@\\n+import { CustomToolDefinition } from '../types/config';\\n+import { Liquid } from 'liquidjs';\\n+import { createExtendedLiquid } from '../liquid-extensions';\\n+import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n+import Sandbox from '@nyariv/sandboxjs';\\n+import { logger } from '../logger';\\n+import { commandExecutor } from '../utils/command-executor';\\n+import Ajv from 'ajv';\\n+\\n+/**\\n+ * Executes custom tools defined in YAML configuration\\n+ * These tools can be used in MCP blocks as if they were native MCP tools\\n+ */\\n+export class CustomToolExecutor {\\n+  private liquid: Liquid;\\n+  private sandbox?: Sandbox;\\n+  private tools: Map<string, CustomToolDefinition>;\\n+  private ajv: Ajv;\\n+\\n+  constructor(tools?: Record<string, CustomToolDefinition>) {\\n+    this.liquid = createExtendedLiquid({\\n+      cache: false,\\n+      strictFilters: false,\\n+      strictVariables: false,\\n+    });\\n+    this.tools = new Map(Object.entries(tools || {}));\\n+    this.ajv = new Ajv({ allErrors: true, verbose: true });\\n+  }\\n+\\n+  /**\\n+   * Register a custom tool\\n+   */\\n+  registerTool(tool: CustomToolDefinition): void {\\n+    if (!tool.name) {\\n+      throw new Error('Tool must have a name');\\n+    }\\n+    this.tools.set(tool.name, tool);\\n+  }\\n+\\n+  /**\\n+   * Register multiple tools\\n+   */\\n+  registerTools(tools: Record<string, CustomToolDefinition>): void {\\n+    for (const [name, tool] of Object.entries(tools)) {\\n+      // Ensure tool has the correct name\\n+      tool.name = tool.name || name;\\n+      this.registerTool(tool);\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Get all registered tools\\n+   */\\n+  getTools(): CustomToolDefinition[] {\\n+    return Array.from(this.tools.values());\\n+  }\\n+\\n+  /**\\n+   * Get a specific tool by name\\n+   */\\n+  getTool(name: string): CustomToolDefinition | undefined {\\n+    return this.tools.get(name);\\n+  }\\n+\\n+  /**\\n+   * Validate tool input against schema using ajv\\n+   */\\n+  private validateInput(tool: CustomToolDefinition, input: Record<string, unknown>): void {\\n+    if (!tool.inputSchema) {\\n+      return;\\n+    }\\n+\\n+    // Compile and cache the schema validator for this tool\\n+    const validate = this.ajv.compile(tool.inputSchema);\\n+\\n+    // Validate the input\\n+    const valid = validate(input);\\n+\\n+    if (!valid) {\\n+      // Format validation errors for better readability\\n+      const errors = validate.errors\\n+        ?.map(err => {\\n+          if (err.instancePath) {\\n+            return `${err.instancePath}: ${err.message}`;\\n+          }\\n+          return err.message;\\n+        })\\n+        .join(', ');\\n+\\n+      throw new Error(`Input validation failed for tool '${tool.name}': ${errors}`);\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Execute a custom tool\\n+   */\\n+  async execute(\\n+    toolName: string,\\n+    args: Record<string, unknown>,\\n+    context?: {\\n+      pr?: {\\n+        number: number;\\n+        title: string;\\n+        author: string;\\n+        branch: string;\\n+        base: string;\\n+      };\\n+      files?: unknown[];\\n+      outputs?: Record<string, unknown>;\\n+      env?: Record<string, string>;\\n+    }\\n+  ): Promise<unknown> {\\n+    const tool = this.tools.get(toolName);\\n+    if (!tool) {\\n+      throw new Error(`Tool not found: ${toolName}`);\\n+    }\\n+\\n+    // Validate input\\n+    this.validateInput(tool, args);\\n+\\n+    // Build template context\\n+    const templateContext = {\\n+      ...context,\\n+      args,\\n+      input: args,\\n+    };\\n+\\n+    // Render command with Liquid\\n+    const command = await this.liquid.parseAndRender(tool.exec, templateContext);\\n+\\n+    // Render stdin if provided\\n+    let stdin: string | undefined;\\n+    if (tool.stdin) {\\n+      stdin = await this.liquid.parseAndRender(tool.stdin, templateContext);\\n+    }\\n+\\n+    // Execute the command using shared executor\\n+    const env = commandExecutor.buildEnvironment(process.env, tool.env, context?.env);\\n+    const result = await commandExecutor.execute(command, {\\n+      stdin,\\n+      cwd: tool.cwd,\\n+      env,\\n+      timeout: tool.timeout || 30000,\\n+    });\\n+\\n+    // Parse JSON if requested\\n+    let output: unknown = result.stdout;\\n+    if (tool.parseJson) {\\n+      try {\\n+        output = JSON.parse(result.stdout);\\n+      } catch (e) {\\n+        logger.warn(`Failed to parse tool output as JSON: ${e}`);\\n+      }\\n+    }\\n+\\n+    // Apply transform if specified\\n+    if (tool.transform) {\\n+      const transformContext = {\\n+        ...templateContext,\\n+        output,\\n+        stdout: result.stdout,\\n+        stderr: result.stderr,\\n+        exitCode: result.exitCode,\\n+      };\\n+      const transformed = await this.liquid.parseAndRender(tool.transform, transformContext);\\n+      // Try to parse as JSON if it looks like JSON\\n+      if (typeof transformed === 'string' && transformed.trim().startsWith('{')) {\\n+        try {\\n+          output = JSON.parse(transformed);\\n+        } catch {\\n+          output = transformed;\\n+        }\\n+      } else {\\n+        output = transformed;\\n+      }\\n+    }\\n+\\n+    // Apply JavaScript transform if specified\\n+    if (tool.transform_js) {\\n+      output = await this.applyJavaScriptTransform(tool.transform_js, output, {\\n+        ...templateContext,\\n+        stdout: result.stdout,\\n+        stderr: result.stderr,\\n+        exitCode: result.exitCode,\\n+      });\\n+    }\\n+\\n+    return output;\\n+  }\\n+\\n+  /**\\n+   * Apply JavaScript transform to output\\n+   */\\n+  private async applyJavaScriptTransform(\\n+    transformJs: string,\\n+    output: unknown,\\n+    context: Record<string, unknown>\\n+  ): Promise<unknown> {\\n+    if (!this.sandbox) {\\n+      this.sandbox = createSecureSandbox();\\n+    }\\n+\\n+    const code = `\\n+      const output = ${JSON.stringify(output)};\\n+      const context = ${JSON.stringify(context)};\\n+      const args = context.args || {};\\n+      const pr = context.pr || {};\\n+      const files = context.files || [];\\n+      const outputs = context.outputs || {};\\n+      const env = context.env || {};\\n+\\n+      ${transformJs}\\n+    `;\\n+\\n+    try {\\n+      return await compileAndRun(this.sandbox, code, { timeout: 5000 });\\n+    } catch (error) {\\n+      logger.error(`JavaScript transform error: ${error}`);\\n+      throw error;\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Convert custom tools to MCP tool format\\n+   */\\n+  toMcpTools(): Array<{\\n+    name: string;\\n+    description?: string;\\n+    inputSchema?: Record<string, unknown>;\\n+    handler: (args: Record<string, unknown>) => Promise<unknown>;\\n+  }> {\\n+    return Array.from(this.tools.values()).map(tool => ({\\n+      name: tool.name,\\n+      description: tool.description,\\n+      inputSchema: tool.inputSchema as Record<string, unknown>,\\n+      handler: async (args: Record<string, unknown>) => {\\n+        return this.execute(tool.name, args);\\n+      },\\n+    }));\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/providers/github-ops-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":61,\"patch\":\"diff --git a/src/providers/github-ops-provider.ts b/src/providers/github-ops-provider.ts\\nindex 2e7cef21..83cb5db9 100644\\n--- a/src/providers/github-ops-provider.ts\\n+++ b/src/providers/github-ops-provider.ts\\n@@ -225,20 +225,20 @@ export class GitHubOpsProvider extends CheckProvider {\\n \\n     let values: string[] = await renderValues(valuesRaw);\\n \\n+    // Expose dependency outputs to value_js for convenience (generic map)\\n+    const depOutputs: Record<string, unknown> = {};\\n+    if (dependencyResults) {\\n+      for (const [name, result] of dependencyResults.entries()) {\\n+        const summary = result as ReviewSummary & { output?: unknown };\\n+        depOutputs[name] = summary.output !== undefined ? summary.output : summary;\\n+      }\\n+    }\\n+\\n     if (cfg.value_js && cfg.value_js.trim()) {\\n       try {\\n         // Evaluate user-provided value_js in a restricted sandbox (no process/global exposure)\\n         const sandbox = this.getSecureSandbox();\\n \\n-        // Build dependency outputs map (mirrors Liquid context construction)\\n-        const depOutputs: Record<string, unknown> = {};\\n-        if (dependencyResults) {\\n-          for (const [name, result] of dependencyResults.entries()) {\\n-            const summary = result as ReviewSummary & { output?: unknown };\\n-            depOutputs[name] = summary.output !== undefined ? summary.output : summary;\\n-          }\\n-        }\\n-\\n         const res = compileAndRun<unknown>(\\n           sandbox,\\n           cfg.value_js,\\n@@ -248,25 +248,40 @@ export class GitHubOpsProvider extends CheckProvider {\\n         if (typeof res === 'string') values = [res];\\n         else if (Array.isArray(res)) values = (res as unknown[]).map(v => String(v));\\n       } catch (e) {\\n+        // Generic fallback: keep pre-rendered values as-is (no hardcoded deps)\\n+        // Never hardcode a particular step like 'issue-assistant'.\\n         const msg = e instanceof Error ? e.message : String(e);\\n+        if (process.env.VISOR_DEBUG === 'true') logger.warn(`[github-ops] value_js_error: ${msg}`);\\n+        // Normalize strings; leave empty if no values were provided.\\n+        values = Array.isArray(values)\\n+          ? values.map(v => String(v ?? '').trim()).filter(Boolean)\\n+          : [];\\n+      }\\n+    }\\n+\\n+    // Fallback: if values are still empty, try deriving from dependency outputs\\n+    // 1) Common pattern: outputs.<dep>.labels (e.g., from issue-assistant)\\n+    if (values.length === 0 && Object.keys(depOutputs).length > 0) {\\n+      try {\\n+        const lbls: string[] = [];\\n+        for (const obj of Object.values(depOutputs)) {\\n+          const labelsAny = (obj as any)?.labels;\\n+          if (Array.isArray(labelsAny)) {\\n+            for (const v of labelsAny) lbls.push(String(v ?? ''));\\n+          }\\n+        }\\n+        const norm = lbls\\n+          .map(s => s.trim())\\n+          .filter(Boolean)\\n+          .map(s => s.replace(/[^A-Za-z0-9:\\\\/\\\\- ]/g, '').replace(/\\\\/{2,}/g, '/'));\\n+        values = Array.from(new Set(norm));\\n         if (process.env.VISOR_DEBUG === 'true') {\\n-          logger.warn(`[github-ops] value_js_error: ${msg}`);\\n+          logger.info(`[github-ops] derived values from deps.labels: ${JSON.stringify(values)}`);\\n         }\\n-        return {\\n-          issues: [\\n-            {\\n-              file: 'system',\\n-              line: 0,\\n-              ruleId: 'github/value_js_error',\\n-              message: `value_js evaluation failed: ${msg}`,\\n-              severity: 'error',\\n-              category: 'logic',\\n-            },\\n-          ],\\n-        };\\n-      }\\n+      } catch {}\\n     }\\n \\n+    // 2) Fallback: outputs.<dep>.tags based derivation (overview-style)\\n     // Fallback: if values are still empty, try deriving from dependency outputs (common pattern: outputs.<dep>.tags)\\n     if (values.length === 0 && dependencyResults && dependencyResults.size > 0) {\\n       try {\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/human-input-check-provider.ts\",\"additions\":5,\"deletions\":1,\"changes\":196,\"patch\":\"diff --git a/src/providers/human-input-check-provider.ts b/src/providers/human-input-check-provider.ts\\nindex ec594320..c49da2e2 100644\\n--- a/src/providers/human-input-check-provider.ts\\n+++ b/src/providers/human-input-check-provider.ts\\n@@ -3,6 +3,8 @@ import { PRInfo } from '../pr-analyzer';\\n import { ReviewSummary } from '../reviewer';\\n import { HumanInputRequest } from '../types/config';\\n import { interactivePrompt, simplePrompt } from '../utils/interactive-prompt';\\n+import { Liquid } from 'liquidjs';\\n+import { createExtendedLiquid } from '../liquid-extensions';\\n import { tryReadStdin } from '../utils/stdin-reader';\\n import * as fs from 'fs';\\n import * as path from 'path';\\n@@ -27,6 +29,7 @@ import * as path from 'path';\\n  * ```\\n  */\\n export class HumanInputCheckProvider extends CheckProvider {\\n+  private liquid?: Liquid;\\n   /**\\n    * @deprecated Use ExecutionContext.cliMessage instead\\n    * Kept for backward compatibility\\n@@ -92,6 +95,74 @@ export class HumanInputCheckProvider extends CheckProvider {\\n     return true;\\n   }\\n \\n+  /** Build a template context for Liquid rendering */\\n+  private buildTemplateContext(\\n+    prInfo: PRInfo,\\n+    dependencyResults?: Map<string, ReviewSummary>,\\n+    outputHistory?: Map<string, unknown[]>,\\n+    _context?: ExecutionContext\\n+  ): Record<string, unknown> {\\n+    const ctx: Record<string, unknown> = {};\\n+    // pr context\\n+    try {\\n+      ctx.pr = {\\n+        number: prInfo.number,\\n+        title: prInfo.title,\\n+        body: prInfo.body,\\n+        author: prInfo.author,\\n+        base: prInfo.base,\\n+        head: prInfo.head,\\n+        files: (prInfo.files || []).map(f => ({\\n+          filename: f.filename,\\n+          status: f.status,\\n+          additions: f.additions,\\n+          deletions: f.deletions,\\n+          changes: f.changes,\\n+        })),\\n+      };\\n+    } catch {}\\n+    // event + env\\n+    try {\\n+      const safeEnv = (() => {\\n+        try {\\n+          const { buildSandboxEnv } = require('../utils/env-exposure');\\n+          return buildSandboxEnv(process.env);\\n+        } catch {\\n+          return {} as Record<string, string>;\\n+        }\\n+      })();\\n+      (ctx as any).event = { event_name: (prInfo as any)?.eventType || 'manual' };\\n+      (ctx as any).env = safeEnv;\\n+    } catch {}\\n+    // utils helpers\\n+    (ctx as any).utils = {\\n+      now: new Date().toISOString(),\\n+      today: new Date().toISOString().split('T')[0],\\n+    };\\n+    // outputs: expose raw outputs from dependency results\\n+    const outputs: Record<string, unknown> = {};\\n+    const outputsRaw: Record<string, unknown> = {};\\n+    if (dependencyResults) {\\n+      for (const [name, res] of dependencyResults.entries()) {\\n+        const summary = res as ReviewSummary & { output?: unknown };\\n+        if (typeof name === 'string' && name.endsWith('-raw')) {\\n+          outputsRaw[name.slice(0, -4)] = summary.output !== undefined ? summary.output : summary;\\n+        } else {\\n+          outputs[name] = summary.output !== undefined ? summary.output : summary;\\n+        }\\n+      }\\n+    }\\n+    ctx.outputs = outputs;\\n+    (ctx as any).outputs_raw = outputsRaw;\\n+    // outputs_history: expose full history if available\\n+    const hist: Record<string, unknown[]> = {};\\n+    if (outputHistory) {\\n+      for (const [k, v] of outputHistory.entries()) hist[k] = Array.isArray(v) ? v : [];\\n+    }\\n+    (ctx as any).outputs_history = hist;\\n+    return ctx;\\n+  }\\n+\\n   /**\\n    * Check if a string looks like a file path\\n    */\\n@@ -104,6 +175,37 @@ export class HumanInputCheckProvider extends CheckProvider {\\n    * Removes potentially dangerous characters while preserving useful input\\n    */\\n   private sanitizeInput(input: string): string {\\n+    // Heuristic: collapse accidental per-character duplication (\\\"stutter\\\") often caused by\\n+    // TTY echo races. We only apply this when most adjacent ASCII chars are doubled.\\n+    const collapseStutter = (s: string): string => {\\n+      if (!s || s.length < 4) return s;\\n+      let dupPairs = 0;\\n+      let pairs = 0;\\n+      for (let i = 0; i + 1 < s.length; i++) {\\n+        const a = s[i];\\n+        const b = s[i + 1];\\n+        if (/^[\\\\x20-\\\\x7E]$/.test(a) && /^[\\\\x20-\\\\x7E]$/.test(b)) {\\n+          pairs++;\\n+          if (a === b) dupPairs++;\\n+        }\\n+      }\\n+      const ratio = pairs > 0 ? dupPairs / pairs : 0;\\n+      if (ratio < 0.5) return s; // keep as-is unless roughly half of pairs are doubled\\n+      let out = '';\\n+      for (let i = 0; i < s.length; i++) {\\n+        const a = s[i];\\n+        const b = i + 1 < s.length ? s[i + 1] : '';\\n+        if (b && a === b) {\\n+          out += a;\\n+          i++; // skip the duplicate\\n+        } else {\\n+          out += a;\\n+        }\\n+      }\\n+      return out;\\n+    };\\n+\\n+    input = collapseStutter(input);\\n     // Remove null bytes (C-string injection)\\n     let sanitized = input.replace(/\\\\0/g, '');\\n \\n@@ -171,13 +273,31 @@ export class HumanInputCheckProvider extends CheckProvider {\\n     config: CheckProviderConfig,\\n     context?: ExecutionContext\\n   ): Promise<string> {\\n-    const prompt = config.prompt || 'Please provide input:';\\n+    // Test runner mock support: if a mock is provided for this step, use it\\n+    try {\\n+      const mockVal = context?.hooks?.mockForStep?.(checkName);\\n+      if (mockVal !== undefined && mockVal !== null) {\\n+        const s = String(mockVal);\\n+        return s;\\n+      }\\n+    } catch {}\\n+    const prompt = (config.prompt as string) || 'Please provide input:';\\n     const placeholder = (config.placeholder as string | undefined) || 'Enter your response...';\\n     const allowEmpty = (config.allow_empty as boolean | undefined) ?? false;\\n     const multiline = (config.multiline as boolean | undefined) ?? false;\\n     const timeout = config.timeout ? config.timeout * 1000 : undefined; // Convert to ms\\n     const defaultValue = config.default as string | undefined;\\n \\n+    // In test/CI modes, never block for input. Use default or empty string.\\n+    const testMode = String(process.env.VISOR_TEST_MODE || '').toLowerCase() === 'true';\\n+    const ciMode =\\n+      String(process.env.CI || '').toLowerCase() === 'true' ||\\n+      String(process.env.GITHUB_ACTIONS || '').toLowerCase() === 'true';\\n+    if (testMode || ciMode) {\\n+      const val = (config.default as string | undefined) || '';\\n+      return val;\\n+    }\\n+\\n     // Get cliMessage from context (new way) or static property (backward compat)\\n     const cliMessage = context?.cliMessage ?? HumanInputCheckProvider.cliMessage;\\n \\n@@ -270,17 +390,85 @@ export class HumanInputCheckProvider extends CheckProvider {\\n     const checkName = config.checkName || 'human-input';\\n \\n     try {\\n+      // Render Liquid templates in prompt/placeholder if any\\n+      try {\\n+        this.liquid =\\n+          this.liquid || createExtendedLiquid({ strictVariables: false, strictFilters: false });\\n+        const tctx = this.buildTemplateContext(\\n+          _prInfo,\\n+          _dependencyResults,\\n+          (config as any).__outputHistory as Map<string, unknown[]> | undefined,\\n+          context\\n+        );\\n+        if (typeof config.prompt === 'string') {\\n+          let rendered = await this.liquid.parseAndRender(config.prompt, tctx);\\n+          // If Liquid markers remain (e.g., due to nested/guarded templates), try a second pass\\n+          if (/\\\\{\\\\{|\\\\{%/.test(rendered)) {\\n+            try {\\n+              rendered = await this.liquid.parseAndRender(rendered, tctx);\\n+            } catch {}\\n+          }\\n+          // Expose the final rendered prompt to the test runner (like AI provider does)\\n+          try {\\n+            const stepName = (config as any).checkName || 'unknown';\\n+            context?.hooks?.onPromptCaptured?.({\\n+              step: String(stepName),\\n+              provider: 'human-input',\\n+              prompt: rendered,\\n+            });\\n+          } catch {}\\n+          config = { ...config, prompt: rendered };\\n+        }\\n+        if (typeof config.placeholder === 'string') {\\n+          let ph = await this.liquid.parseAndRender(config.placeholder as string, tctx);\\n+          if (/\\\\{\\\\{|\\\\{%/.test(ph)) {\\n+            try {\\n+              ph = await this.liquid.parseAndRender(ph, tctx);\\n+            } catch {}\\n+          }\\n+          (config as any).placeholder = ph;\\n+        }\\n+      } catch (e) {\\n+        // Always show Liquid errors with a helpful snippet and caret\\n+        const err: any = e || {};\\n+        const raw = String((config as any)?.prompt || '');\\n+        const lines = raw.split(/\\\\r?\\\\n/);\\n+        const lineNum: number = Number(err.line || err?.token?.line || err?.location?.line || 0);\\n+        const colNum: number = Number(err.col || err?.token?.col || err?.location?.col || 0);\\n+        let snippet = '';\\n+        if (lineNum > 0) {\\n+          const start = Math.max(1, lineNum - 3);\\n+          const end = Math.max(lineNum + 2, lineNum);\\n+          const width = String(end).length;\\n+          for (let i = start; i <= Math.min(end, lines.length); i++) {\\n+            const ln = `${String(i).padStart(width, ' ')} | ${lines[i - 1] ?? ''}`;\\n+            snippet += ln + '\\\\n';\\n+            if (i === lineNum) {\\n+              const caretPad = ' '.repeat(Math.max(0, colNum > 1 ? colNum - 1 : 0) + width + 3);\\n+              snippet += caretPad + '^\\\\n';\\n+            }\\n+          }\\n+        }\\n+        try {\\n+          console.error(\\n+            `‚ö†Ô∏è  human-input: Liquid render failed: ${\\n+              e instanceof Error ? e.message : String(e)\\n+            }\\\\n${snippet}`\\n+          );\\n+        } catch {}\\n+        // Continue with raw strings as a fallback\\n+      }\\n       // Get user input (pass context for non-static state)\\n       const userInput = await this.getUserInput(checkName, config, context);\\n \\n       // Sanitize input to prevent injection attacks in dependent checks\\n       const sanitizedInput = this.sanitizeInput(userInput);\\n \\n-      // Return the input as the check output (stored in output field for dependent checks)\\n+      // Return structured output with timestamp for consistent history/merging\\n       return {\\n         issues: [],\\n-        output: sanitizedInput,\\n-      } as ReviewSummary & { output: string };\\n+        output: { text: sanitizedInput, ts: Date.now() },\\n+      } as ReviewSummary & { output: { text: string; ts: number } };\\n     } catch (error) {\\n       // If there's an error getting input, return an error issue\\n       return {\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/log-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":17,\"patch\":\"diff --git a/src/providers/log-check-provider.ts b/src/providers/log-check-provider.ts\\nindex ebdbd30f..9c6332d1 100644\\n--- a/src/providers/log-check-provider.ts\\n+++ b/src/providers/log-check-provider.ts\\n@@ -1,4 +1,4 @@\\n-import { CheckProvider, CheckProviderConfig } from './check-provider.interface';\\n+import { CheckProvider, CheckProviderConfig, ExecutionContext } from './check-provider.interface';\\n import { PRInfo } from '../pr-analyzer';\\n import { ReviewSummary } from '../reviewer';\\n import { Liquid } from 'liquidjs';\\n@@ -62,7 +62,7 @@ export class LogCheckProvider extends CheckProvider {\\n     prInfo: PRInfo,\\n     config: CheckProviderConfig,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n-    _sessionInfo?: { parentSessionId?: string; reuseSession?: boolean }\\n+    context?: ExecutionContext\\n   ): Promise<ReviewSummary> {\\n     const message = config.message as string;\\n     const level = (config.level as LogLevel) || 'info';\\n@@ -77,7 +77,8 @@ export class LogCheckProvider extends CheckProvider {\\n       includePrContext,\\n       includeDependencies,\\n       includeMetadata,\\n-      config.__outputHistory as Map<string, unknown[]> | undefined\\n+      config.__outputHistory as Map<string, unknown[]> | undefined,\\n+      context\\n     );\\n \\n     // Render the log message template\\n@@ -113,7 +114,8 @@ export class LogCheckProvider extends CheckProvider {\\n     _includePrContext: boolean = true,\\n     _includeDependencies: boolean = true,\\n     includeMetadata: boolean = true,\\n-    outputHistory?: Map<string, unknown[]>\\n+    outputHistory?: Map<string, unknown[]>,\\n+    executionContext?: ExecutionContext\\n   ): Record<string, unknown> {\\n     const context: Record<string, unknown> = {};\\n \\n@@ -194,6 +196,13 @@ export class LogCheckProvider extends CheckProvider {\\n       };\\n     }\\n \\n+    // Add workflow inputs if available\\n+    const workflowInputs = executionContext?.workflowInputs || {};\\n+    logger.debug(\\n+      `[LogProvider] Adding ${Object.keys(workflowInputs).length} workflow inputs to context`\\n+    );\\n+    context.inputs = workflowInputs;\\n+\\n     return context;\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/mcp-check-provider.ts\",\"additions\":2,\"deletions\":1,\"changes\":78,\"patch\":\"diff --git a/src/providers/mcp-check-provider.ts b/src/providers/mcp-check-provider.ts\\nindex 9b44a6d2..8e7a6557 100644\\n--- a/src/providers/mcp-check-provider.ts\\n+++ b/src/providers/mcp-check-provider.ts\\n@@ -11,13 +11,15 @@ import { StreamableHTTPClientTransport } from '@modelcontextprotocol/sdk/client/\\n import Sandbox from '@nyariv/sandboxjs';\\n import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n import { EnvironmentResolver } from '../utils/env-resolver';\\n+import { CustomToolExecutor } from './custom-tool-executor';\\n+import { CustomToolDefinition } from '../types/config';\\n \\n /**\\n  * MCP Check Provider Configuration\\n  */\\n export interface McpCheckConfig extends CheckProviderConfig {\\n-  /** Transport type: stdio (default), sse (legacy), or http (streamable HTTP) */\\n-  transport?: 'stdio' | 'sse' | 'http';\\n+  /** Transport type: stdio (default), sse (legacy), http (streamable HTTP), or custom (YAML-defined tools) */\\n+  transport?: 'stdio' | 'sse' | 'http' | 'custom';\\n   /** Command to execute (for stdio transport) */\\n   command?: string;\\n   /** Command arguments (for stdio transport) */\\n@@ -48,11 +50,12 @@ export interface McpCheckConfig extends CheckProviderConfig {\\n \\n /**\\n  * Check provider that calls MCP tools directly\\n- * Supports stdio, SSE (legacy), and Streamable HTTP transports\\n+ * Supports stdio, SSE (legacy), Streamable HTTP transports, and custom YAML-defined tools\\n  */\\n export class McpCheckProvider extends CheckProvider {\\n   private liquid: Liquid;\\n   private sandbox?: Sandbox;\\n+  private customToolExecutor?: CustomToolExecutor;\\n \\n   constructor() {\\n     super();\\n@@ -63,6 +66,17 @@ export class McpCheckProvider extends CheckProvider {\\n     });\\n   }\\n \\n+  /**\\n+   * Set custom tools for this provider\\n+   */\\n+  setCustomTools(tools: Record<string, CustomToolDefinition>): void {\\n+    if (!this.customToolExecutor) {\\n+      this.customToolExecutor = new CustomToolExecutor(tools);\\n+    } else {\\n+      this.customToolExecutor.registerTools(tools);\\n+    }\\n+  }\\n+\\n   /**\\n    * Create a secure sandbox for JavaScript execution\\n    * - Uses Sandbox.SAFE_GLOBALS which excludes: Function, eval, require, process, etc.\\n@@ -78,7 +92,7 @@ export class McpCheckProvider extends CheckProvider {\\n   }\\n \\n   getDescription(): string {\\n-    return 'Call MCP tools directly using stdio, SSE, or Streamable HTTP transport';\\n+    return 'Call MCP tools directly using stdio, SSE, HTTP, or custom YAML-defined tools';\\n   }\\n \\n   async validateConfig(config: unknown): Promise<boolean> {\\n@@ -129,8 +143,15 @@ export class McpCheckProvider extends CheckProvider {\\n         logger.error(`Invalid URL format for MCP ${transport} transport: ${cfg.url}`);\\n         return false;\\n       }\\n+    } else if (transport === 'custom') {\\n+      // For custom transport, validation is delegated to CustomToolExecutor\\n+      // The tool must exist in the configuration's tools section\\n+      // This will be validated at execution time when the tool is looked up\\n+      logger.debug(`MCP custom transport will validate tool '${cfg.method}' at execution time`);\\n     } else {\\n-      logger.error(`Invalid MCP transport: ${transport}. Must be 'stdio', 'sse', or 'http'`);\\n+      logger.error(\\n+        `Invalid MCP transport: ${transport}. Must be 'stdio', 'sse', 'http', or 'custom'`\\n+      );\\n       return false;\\n     }\\n \\n@@ -184,7 +205,7 @@ export class McpCheckProvider extends CheckProvider {\\n       }\\n \\n       // Create MCP client and execute method\\n-      const result = await this.executeMcpMethod(cfg, methodArgs);\\n+      const result = await this.executeMcpMethod(cfg, methodArgs, prInfo, dependencyResults);\\n \\n       // Apply transforms if specified\\n       let finalOutput = result;\\n@@ -297,12 +318,49 @@ export class McpCheckProvider extends CheckProvider {\\n    */\\n   private async executeMcpMethod(\\n     config: McpCheckConfig,\\n-    methodArgs: Record<string, unknown>\\n+    methodArgs: Record<string, unknown>,\\n+    prInfo?: PRInfo,\\n+    dependencyResults?: Map<string, ReviewSummary>\\n   ): Promise<unknown> {\\n     const transport = config.transport || 'stdio';\\n     const timeout = (config.timeout || 60) * 1000; // Convert to milliseconds\\n \\n-    if (transport === 'stdio') {\\n+    if (transport === 'custom') {\\n+      // Execute custom YAML-defined tool\\n+      if (!this.customToolExecutor) {\\n+        throw new Error(\\n+          'No custom tools available. Define tools in the \\\"tools\\\" section of your configuration.'\\n+        );\\n+      }\\n+\\n+      const tool = this.customToolExecutor.getTool(config.method);\\n+      if (!tool) {\\n+        throw new Error(\\n+          `Custom tool not found: ${config.method}. Available tools: ${this.customToolExecutor\\n+            .getTools()\\n+            .map(t => t.name)\\n+            .join(', ')}`\\n+        );\\n+      }\\n+\\n+      // Build context for custom tool execution\\n+      const context = {\\n+        pr: prInfo\\n+          ? {\\n+              number: prInfo.number,\\n+              title: prInfo.title,\\n+              author: prInfo.author,\\n+              branch: prInfo.head,\\n+              base: prInfo.base,\\n+            }\\n+          : undefined,\\n+        files: prInfo?.files,\\n+        outputs: this.buildOutputContext(dependencyResults),\\n+        env: this.getSafeEnvironmentVariables(),\\n+      };\\n+\\n+      return await this.customToolExecutor.execute(config.method, methodArgs, context);\\n+    } else if (transport === 'stdio') {\\n       return await this.executeStdioMethod(config, methodArgs, timeout);\\n     } else if (transport === 'sse') {\\n       return await this.executeSseMethod(config, methodArgs, timeout);\\n@@ -404,7 +462,7 @@ export class McpCheckProvider extends CheckProvider {\\n   ): Promise<unknown> {\\n     const transport = new StdioClientTransport({\\n       command: config.command!,\\n-      args: config.args,\\n+      args: config.command_args as string[] | undefined,\\n       env: config.env,\\n       cwd: config.workingDirectory,\\n     });\\n@@ -664,7 +722,7 @@ export class McpCheckProvider extends CheckProvider {\\n       'type',\\n       'transport',\\n       'command',\\n-      'args',\\n+      'command_args',\\n       'env',\\n       'workingDirectory',\\n       'url',\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/script-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":25,\"patch\":\"diff --git a/src/providers/script-check-provider.ts b/src/providers/script-check-provider.ts\\nindex 4d8012a6..70d5ae9a 100644\\n--- a/src/providers/script-check-provider.ts\\n+++ b/src/providers/script-check-provider.ts\\n@@ -70,6 +70,14 @@ export class ScriptCheckProvider extends CheckProvider {\\n       (_sessionInfo as any)?.stageHistoryBase as Record<string, number> | undefined,\\n       { attachMemoryReadHelpers: false }\\n     );\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        const hist: any = (ctx as any).outputs_history || {};\\n+        const len = Array.isArray(hist['refine']) ? hist['refine'].length : 0;\\n+\\n+        console.error(`[script] history.refine.len=${len}`);\\n+      }\\n+    } catch {}\\n \\n     // Attach synchronous memory ops consistent with memory provider\\n     const { ops, needsSave } = createSyncMemoryOps(memoryStore);\\n@@ -85,7 +93,7 @@ export class ScriptCheckProvider extends CheckProvider {\\n         { ...ctx },\\n         {\\n           injectLog: true,\\n-          wrapFunction: false,\\n+          wrapFunction: true,\\n           logPrefix: '[script]',\\n         }\\n       );\\n@@ -120,7 +128,20 @@ export class ScriptCheckProvider extends CheckProvider {\\n       logger.warn(`[script] memory save failed: ${e instanceof Error ? e.message : String(e)}`);\\n     }\\n \\n-    return { issues: [], output: result } as ReviewSummary & { output: unknown };\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        const name = String((config as any).checkName || '');\\n+        const t = typeof result;\\n+        console.error(\\n+          `[script-return] ${name} outputType=${t} hasArray=${Array.isArray(result)} hasObj=${result && typeof result === 'object'}`\\n+        );\\n+      }\\n+    } catch {}\\n+    const out: any = { issues: [], output: result } as ReviewSummary & { output: unknown };\\n+    try {\\n+      (out as any).__histTracked = true;\\n+    } catch {}\\n+    return out;\\n   }\\n \\n   getSupportedConfigKeys(): string[] {\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/workflow-check-provider.ts\",\"additions\":14,\"deletions\":0,\"changes\":524,\"patch\":\"diff --git a/src/providers/workflow-check-provider.ts b/src/providers/workflow-check-provider.ts\\nnew file mode 100644\\nindex 00000000..b582d2f5\\n--- /dev/null\\n+++ b/src/providers/workflow-check-provider.ts\\n@@ -0,0 +1,524 @@\\n+/**\\n+ * Workflow check provider - executes reusable workflows as checks\\n+ */\\n+\\n+import { CheckProvider, CheckProviderConfig, ExecutionContext } from './check-provider.interface';\\n+import { PRInfo } from '../pr-analyzer';\\n+import { ReviewSummary } from '../reviewer';\\n+import { WorkflowRegistry } from '../workflow-registry';\\n+import { WorkflowExecutor } from '../workflow-executor';\\n+import { logger } from '../logger';\\n+import { WorkflowDefinition, WorkflowExecutionContext } from '../types/workflow';\\n+import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n+import { Liquid } from 'liquidjs';\\n+\\n+/**\\n+ * Provider that executes workflows as checks\\n+ */\\n+export class WorkflowCheckProvider extends CheckProvider {\\n+  private registry: WorkflowRegistry;\\n+  private executor: WorkflowExecutor;\\n+  private liquid: Liquid;\\n+\\n+  constructor() {\\n+    super();\\n+    this.registry = WorkflowRegistry.getInstance();\\n+    this.executor = new WorkflowExecutor();\\n+    this.liquid = new Liquid();\\n+  }\\n+\\n+  getName(): string {\\n+    return 'workflow';\\n+  }\\n+\\n+  getDescription(): string {\\n+    return 'Executes reusable workflow definitions as checks';\\n+  }\\n+\\n+  async validateConfig(config: unknown): Promise<boolean> {\\n+    const cfg = config as CheckProviderConfig;\\n+\\n+    if (!cfg.workflow) {\\n+      logger.error('Workflow provider requires \\\"workflow\\\" field');\\n+      return false;\\n+    }\\n+\\n+    // Check if workflow exists in registry\\n+    if (!this.registry.has(cfg.workflow as string)) {\\n+      logger.error(`Workflow '${cfg.workflow}' not found in registry`);\\n+      return false;\\n+    }\\n+\\n+    return true;\\n+  }\\n+\\n+  async execute(\\n+    prInfo: PRInfo,\\n+    config: CheckProviderConfig,\\n+    dependencyResults?: Map<string, ReviewSummary>,\\n+    context?: ExecutionContext\\n+  ): Promise<ReviewSummary> {\\n+    const workflowId = config.workflow as string;\\n+\\n+    // Get the workflow definition\\n+    const workflow = this.registry.get(workflowId);\\n+    if (!workflow) {\\n+      throw new Error(`Workflow '${workflowId}' not found in registry`);\\n+    }\\n+\\n+    logger.info(`Executing workflow '${workflowId}'`);\\n+\\n+    // Prepare inputs\\n+    const inputs = await this.prepareInputs(workflow, config, prInfo, dependencyResults);\\n+\\n+    // Validate inputs\\n+    const validation = this.registry.validateInputs(workflow, inputs);\\n+    if (!validation.valid) {\\n+      const errors = validation.errors?.map(e => `${e.path}: ${e.message}`).join(', ');\\n+      throw new Error(`Invalid workflow inputs: ${errors}`);\\n+    }\\n+\\n+    // Apply overrides to workflow steps if specified\\n+    const modifiedWorkflow = this.applyOverrides(workflow, config);\\n+\\n+    // M3: Check if we're in state-machine mode and should delegate to engine\\n+    const engineMode = (context as any)?._engineMode;\\n+    if (engineMode === 'state-machine') {\\n+      // Delegate to state machine engine for nested workflow execution\\n+      logger.info(`[WorkflowProvider] Delegating workflow '${workflowId}' to state machine engine`);\\n+      return await this.executeViaStateMachine(\\n+        modifiedWorkflow,\\n+        inputs,\\n+        config,\\n+        prInfo,\\n+        dependencyResults,\\n+        context\\n+      );\\n+    }\\n+\\n+    // Legacy mode: Execute the workflow using WorkflowExecutor\\n+    const executionContext: WorkflowExecutionContext = {\\n+      instanceId: `${workflowId}-${Date.now()}`,\\n+      parentCheckId: config.checkName,\\n+      inputs,\\n+      stepResults: new Map(),\\n+    };\\n+\\n+    const result = await this.executor.execute(modifiedWorkflow, executionContext, {\\n+      prInfo,\\n+      dependencyResults,\\n+      context,\\n+    });\\n+\\n+    // Map outputs\\n+    const outputs = this.mapOutputs(result, config.output_mapping as Record<string, string>);\\n+\\n+    // Return the review summary with extended fields\\n+    // Note: These extra fields are used by the execution engine but not part of the base interface\\n+    const summary: ReviewSummary = {\\n+      issues: result.issues || [],\\n+    };\\n+\\n+    // Add extended fields as needed by the engine\\n+    (summary as any).score = result.score || 0;\\n+    (summary as any).confidence = result.confidence || 'medium';\\n+    (summary as any).comments = result.comments || [];\\n+    (summary as any).output = outputs;\\n+    (summary as any).content = this.formatWorkflowResult(workflow, result, outputs);\\n+\\n+    return summary;\\n+  }\\n+\\n+  getSupportedConfigKeys(): string[] {\\n+    return ['workflow', 'args', 'overrides', 'output_mapping', 'timeout', 'env', 'checkName'];\\n+  }\\n+\\n+  async isAvailable(): Promise<boolean> {\\n+    return true; // Always available\\n+  }\\n+\\n+  getRequirements(): string[] {\\n+    return [];\\n+  }\\n+\\n+  /**\\n+   * Prepare inputs for workflow execution\\n+   */\\n+  private async prepareInputs(\\n+    workflow: WorkflowDefinition,\\n+    config: CheckProviderConfig,\\n+    prInfo: PRInfo,\\n+    dependencyResults?: Map<string, ReviewSummary>\\n+  ): Promise<Record<string, unknown>> {\\n+    const inputs: Record<string, unknown> = {};\\n+\\n+    // Start with default values from workflow definition\\n+    if (workflow.inputs) {\\n+      for (const param of workflow.inputs) {\\n+        if (param.default !== undefined) {\\n+          inputs[param.name] = param.default;\\n+        }\\n+      }\\n+    }\\n+\\n+    // Apply user-provided inputs (args)\\n+    const userInputs = config.args || config.workflow_inputs; // Support both for compatibility\\n+    if (userInputs) {\\n+      for (const [key, value] of Object.entries(userInputs)) {\\n+        // Process value if it's a template or expression\\n+        if (typeof value === 'string') {\\n+          // Check if it's a Liquid template\\n+          if (value.includes('{{') || value.includes('{%')) {\\n+            inputs[key] = await this.liquid.parseAndRender(value, {\\n+              pr: prInfo,\\n+              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n+              env: process.env,\\n+            });\\n+          } else {\\n+            inputs[key] = value;\\n+          }\\n+        } else if (typeof value === 'object' && value !== null && 'expression' in value) {\\n+          // JavaScript expression\\n+          const exprValue = value as { expression: string };\\n+          const sandbox = createSecureSandbox();\\n+          inputs[key] = compileAndRun(\\n+            sandbox,\\n+            exprValue.expression,\\n+            {\\n+              pr: prInfo,\\n+              outputs: dependencyResults ? Object.fromEntries(dependencyResults) : {},\\n+              env: process.env,\\n+            },\\n+            { injectLog: true, logPrefix: `workflow.input.${key}` }\\n+          );\\n+        } else {\\n+          inputs[key] = value;\\n+        }\\n+      }\\n+    }\\n+\\n+    return inputs;\\n+  }\\n+\\n+  /**\\n+   * Apply overrides to workflow steps\\n+   */\\n+  private applyOverrides(\\n+    workflow: WorkflowDefinition,\\n+    config: CheckProviderConfig\\n+  ): WorkflowDefinition {\\n+    const overrideConfig = config.overrides || config.workflow_overrides; // Support both for compatibility\\n+    if (!overrideConfig) {\\n+      return workflow;\\n+    }\\n+\\n+    // Deep clone the workflow\\n+    const modified = JSON.parse(JSON.stringify(workflow));\\n+\\n+    // Apply overrides\\n+    for (const [stepId, overrides] of Object.entries(overrideConfig)) {\\n+      if (modified.steps[stepId]) {\\n+        // Merge overrides with existing step config\\n+        modified.steps[stepId] = {\\n+          ...modified.steps[stepId],\\n+          ...overrides,\\n+        };\\n+      } else {\\n+        logger.warn(`Cannot override non-existent step '${stepId}' in workflow '${workflow.id}'`);\\n+      }\\n+    }\\n+\\n+    return modified;\\n+  }\\n+\\n+  /**\\n+   * Map workflow outputs to check outputs\\n+   */\\n+  private mapOutputs(result: any, outputMapping?: Record<string, string>): Record<string, unknown> {\\n+    if (!outputMapping) {\\n+      return result.output || {};\\n+    }\\n+\\n+    const mapped: Record<string, unknown> = {};\\n+    const workflowOutputs = result.output || {};\\n+\\n+    for (const [checkOutput, workflowOutput] of Object.entries(outputMapping)) {\\n+      if (workflowOutput in workflowOutputs) {\\n+        mapped[checkOutput] = workflowOutputs[workflowOutput];\\n+      } else if (workflowOutput.includes('.')) {\\n+        // Handle nested paths\\n+        const parts = workflowOutput.split('.');\\n+        let value = workflowOutputs;\\n+        for (const part of parts) {\\n+          value = value?.[part];\\n+          if (value === undefined) break;\\n+        }\\n+        mapped[checkOutput] = value;\\n+      }\\n+    }\\n+\\n+    return mapped;\\n+  }\\n+\\n+  /**\\n+   * Format workflow execution result for display\\n+   */\\n+  /**\\n+   * Execute workflow via state machine engine (M3: nested workflows)\\n+   */\\n+  private async executeViaStateMachine(\\n+    workflow: WorkflowDefinition,\\n+    inputs: Record<string, unknown>,\\n+    config: CheckProviderConfig,\\n+    prInfo: PRInfo,\\n+    dependencyResults?: Map<string, ReviewSummary>,\\n+    context?: ExecutionContext\\n+  ): Promise<ReviewSummary> {\\n+    // Import state machine components\\n+    const { projectWorkflowToGraph, validateWorkflowDepth } = require('../state-machine/workflow-projection');\\n+    const { StateMachineRunner } = require('../state-machine/runner');\\n+    const { ExecutionJournal } = require('../snapshot-store');\\n+    const { MemoryStore } = require('../memory-store');\\n+    const { v4: uuidv4 } = require('uuid');\\n+\\n+    // Extract parent context if available\\n+    const parentContext = (context as any)?._parentContext;\\n+    const parentState = (context as any)?._parentState;\\n+\\n+    // Validate workflow depth\\n+    const currentDepth = parentState?.flags?.currentWorkflowDepth || 0;\\n+    const maxDepth = parentState?.flags?.maxWorkflowDepth || 3;\\n+    validateWorkflowDepth(currentDepth, maxDepth, workflow.id);\\n+\\n+    // Project workflow to dependency graph\\n+    const { config: workflowConfig, checks: checksMetadata } = projectWorkflowToGraph(\\n+      workflow,\\n+      inputs,\\n+      config.checkName || workflow.id\\n+    );\\n+\\n+    // Build child engine context\\n+    const childContext = {\\n+      mode: 'state-machine' as const,\\n+      config: workflowConfig,\\n+      checks: checksMetadata,\\n+      journal: parentContext?.journal || new ExecutionJournal(),\\n+      memory: parentContext?.memory || MemoryStore.getInstance(),\\n+      workingDirectory: parentContext?.workingDirectory || process.cwd(),\\n+      sessionId: parentContext?.sessionId || uuidv4(),\\n+      event: parentContext?.event || prInfo.eventType,\\n+      debug: parentContext?.debug || false,\\n+      maxParallelism: parentContext?.maxParallelism,\\n+      failFast: parentContext?.failFast,\\n+    };\\n+\\n+    // Create child runner with inherited context\\n+    const runner = new StateMachineRunner(childContext);\\n+    const childState = runner.getState();\\n+\\n+    // Set workflow depth for child\\n+    childState.flags.currentWorkflowDepth = currentDepth + 1;\\n+    childState.flags.maxWorkflowDepth = maxDepth;\\n+\\n+    // Set parent references\\n+    childState.parentContext = parentContext;\\n+    childState.parentScope = parentState?.parentScope;\\n+\\n+    // Execute the child workflow\\n+    logger.info(`[WorkflowProvider] Executing nested workflow '${workflow.id}' at depth ${currentDepth + 1}`);\\n+    const result = await runner.run();\\n+\\n+    // M3: Check for bubbled events and propagate them to parent\\n+    const bubbledEvents = (childContext as any)._bubbledEvents || [];\\n+    if (bubbledEvents.length > 0 && parentContext) {\\n+      if (parentContext.debug) {\\n+        logger.info(`[WorkflowProvider] Bubbling ${bubbledEvents.length} events to parent context`);\\n+      }\\n+\\n+      // Propagate bubbled events to parent\\n+      if (!parentContext._bubbledEvents) {\\n+        (parentContext as any)._bubbledEvents = [];\\n+      }\\n+      (parentContext as any)._bubbledEvents.push(...bubbledEvents);\\n+    }\\n+\\n+    // Aggregate results from all workflow steps\\n+    const allIssues: any[] = [];\\n+    let totalScore = 0;\\n+    let scoreCount = 0;\\n+\\n+    for (const stepResult of Object.values(result.results)) {\\n+      const typedResult = stepResult as any;\\n+      if (typedResult.issues) {\\n+        allIssues.push(...typedResult.issues);\\n+      }\\n+      if (typedResult.score) {\\n+        totalScore += typedResult.score;\\n+        scoreCount++;\\n+      }\\n+    }\\n+\\n+    // Compute workflow outputs\\n+    const outputs = await this.computeWorkflowOutputsFromState(\\n+      workflow,\\n+      inputs,\\n+      result.results,\\n+      prInfo\\n+    );\\n+\\n+    // Map outputs if output_mapping is specified\\n+    const mappedOutputs = this.mapOutputs(\\n+      { output: outputs },\\n+      config.output_mapping as Record<string, string>\\n+    );\\n+\\n+    // Build aggregated summary\\n+    const summary: ReviewSummary = {\\n+      issues: allIssues,\\n+    };\\n+\\n+    (summary as any).score = scoreCount > 0 ? Math.round(totalScore / scoreCount) : 0;\\n+    (summary as any).confidence = 'medium';\\n+    (summary as any).output = mappedOutputs;\\n+    (summary as any).content = this.formatWorkflowResultFromStateMachine(\\n+      workflow,\\n+      result,\\n+      mappedOutputs\\n+    );\\n+\\n+    return summary;\\n+  }\\n+\\n+  /**\\n+   * Compute workflow outputs from state machine execution results\\n+   */\\n+  private async computeWorkflowOutputsFromState(\\n+    workflow: WorkflowDefinition,\\n+    inputs: Record<string, unknown>,\\n+    stepResults: Record<string, ReviewSummary>,\\n+    prInfo: PRInfo\\n+  ): Promise<Record<string, unknown>> {\\n+    const outputs: Record<string, unknown> = {};\\n+\\n+    if (!workflow.outputs) {\\n+      return outputs;\\n+    }\\n+\\n+    const sandbox = createSecureSandbox();\\n+\\n+    for (const output of workflow.outputs) {\\n+      if (output.value_js) {\\n+        // JavaScript expression\\n+        outputs[output.name] = compileAndRun(\\n+          sandbox,\\n+          output.value_js,\\n+          {\\n+            inputs,\\n+            steps: Object.fromEntries(\\n+              Object.entries(stepResults).map(([id, result]) => [\\n+                id.split(':').pop() || id,\\n+                (result as any).output,\\n+              ])\\n+            ),\\n+            outputs: stepResults,\\n+            pr: prInfo,\\n+          },\\n+          { injectLog: true, logPrefix: `workflow.output.${output.name}` }\\n+        );\\n+      } else if (output.value) {\\n+        // Liquid template\\n+        outputs[output.name] = await this.liquid.parseAndRender(output.value, {\\n+          inputs,\\n+          steps: Object.fromEntries(\\n+            Object.entries(stepResults).map(([id, result]) => [\\n+              id.split(':').pop() || id,\\n+              (result as any).output,\\n+            ])\\n+          ),\\n+          outputs: stepResults,\\n+          pr: prInfo,\\n+        });\\n+      }\\n+    }\\n+\\n+    return outputs;\\n+  }\\n+\\n+  /**\\n+   * Format workflow result from state machine execution\\n+   */\\n+  private formatWorkflowResultFromStateMachine(\\n+    workflow: WorkflowDefinition,\\n+    result: any,\\n+    outputs: Record<string, unknown>\\n+  ): string {\\n+    const lines: string[] = [];\\n+\\n+    lines.push(`Workflow: ${workflow.name}`);\\n+    if (workflow.description) {\\n+      lines.push(`Description: ${workflow.description}`);\\n+    }\\n+\\n+    lines.push('');\\n+    lines.push('Execution Summary (State Machine):');\\n+    lines.push(`- Total Steps: ${Object.keys(result.results || {}).length}`);\\n+    lines.push(`- Duration: ${result.statistics?.totalDuration || 0}ms`);\\n+\\n+    if (Object.keys(outputs).length > 0) {\\n+      lines.push('');\\n+      lines.push('Outputs:');\\n+      for (const [key, value] of Object.entries(outputs)) {\\n+        const formatted =\\n+          typeof value === 'object' ? JSON.stringify(value, null, 2) : String(value);\\n+        lines.push(`- ${key}: ${formatted}`);\\n+      }\\n+    }\\n+\\n+    return lines.join('\\\\n');\\n+  }\\n+\\n+  private formatWorkflowResult(\\n+    workflow: WorkflowDefinition,\\n+    result: any,\\n+    outputs: Record<string, unknown>\\n+  ): string {\\n+    const lines: string[] = [];\\n+\\n+    lines.push(`Workflow: ${workflow.name}`);\\n+    if (workflow.description) {\\n+      lines.push(`Description: ${workflow.description}`);\\n+    }\\n+\\n+    lines.push('');\\n+    lines.push('Execution Summary:');\\n+    lines.push(`- Status: ${result.status || 'completed'}`);\\n+    lines.push(`- Score: ${result.score || 0}`);\\n+    lines.push(`- Issues Found: ${result.issues?.length || 0}`);\\n+\\n+    if (result.duration) {\\n+      lines.push(`- Duration: ${result.duration}ms`);\\n+    }\\n+\\n+    if (Object.keys(outputs).length > 0) {\\n+      lines.push('');\\n+      lines.push('Outputs:');\\n+      for (const [key, value] of Object.entries(outputs)) {\\n+        const formatted =\\n+          typeof value === 'object' ? JSON.stringify(value, null, 2) : String(value);\\n+        lines.push(`- ${key}: ${formatted}`);\\n+      }\\n+    }\\n+\\n+    if (result.stepSummaries && result.stepSummaries.length > 0) {\\n+      lines.push('');\\n+      lines.push('Step Results:');\\n+      for (const summary of result.stepSummaries) {\\n+        lines.push(\\n+          `- ${summary.stepId}: ${summary.status} (${summary.issues?.length || 0} issues)`\\n+        );\\n+      }\\n+    }\\n+\\n+    return lines.join('\\\\n');\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/reviewer.ts\",\"additions\":1,\"deletions\":1,\"changes\":16,\"patch\":\"diff --git a/src/reviewer.ts b/src/reviewer.ts\\nindex f9b862be..b5127a34 100644\\n--- a/src/reviewer.ts\\n+++ b/src/reviewer.ts\\n@@ -151,8 +151,8 @@ export class PRReviewer {\\n     const { debug = false, config, checks } = options;\\n \\n     if (config && checks && checks.length > 0) {\\n-      const { CheckExecutionEngine } = await import('./check-execution-engine');\\n-      const engine = new CheckExecutionEngine();\\n+      const { StateMachineExecutionEngine } = await import('./state-machine-execution-engine');\\n+      const engine = new StateMachineExecutionEngine();\\n       const { results } = await engine.executeGroupedChecks(\\n         prInfo,\\n         checks,\\n@@ -276,7 +276,12 @@ export class PRReviewer {\\n     repo: string,\\n     prNumber: number,\\n     groupedResults: GroupedCheckResults,\\n-    options: ReviewOptions & { commentId?: string; triggeredBy?: string; commitSha?: string } = {}\\n+    options: ReviewOptions & {\\n+      commentId?: string;\\n+      triggeredBy?: string;\\n+      commitSha?: string;\\n+      octokitOverride?: Octokit;\\n+    } = {}\\n   ): Promise<void> {\\n     // Post separate comments for each group\\n     for (const [groupName, checkResults] of Object.entries(groupedResults)) {\\n@@ -330,7 +335,10 @@ export class PRReviewer {\\n       // Do not post empty comments (possible if content is blank after fallbacks)\\n       if (!comment || !comment.trim()) continue;\\n \\n-      await this.commentManager.updateOrCreateComment(owner, repo, prNumber, comment, {\\n+      const manager = options.octokitOverride\\n+        ? new CommentManager(options.octokitOverride)\\n+        : this.commentManager;\\n+      await manager.updateOrCreateComment(owner, repo, prNumber, comment, {\\n         commentId,\\n         triggeredBy: options.triggeredBy || 'unknown',\\n         allowConcurrentUpdates: false,\\n\",\"status\":\"modified\"},{\"filename\":\"src/sdk.ts\",\"additions\":1,\"deletions\":1,\"changes\":5,\"patch\":\"diff --git a/src/sdk.ts b/src/sdk.ts\\nindex 388e7765..06c63da4 100644\\n--- a/src/sdk.ts\\n+++ b/src/sdk.ts\\n@@ -4,7 +4,7 @@\\n  - Dual ESM/CJS bundle via tsup.\\n */\\n \\n-import { CheckExecutionEngine } from './check-execution-engine';\\n+import { StateMachineExecutionEngine } from './state-machine-execution-engine';\\n import { ConfigManager } from './config';\\n import type { AnalysisResult } from './output-formatters';\\n import type { VisorConfig, TagFilter, HumanInputRequest } from './types/config';\\n@@ -121,7 +121,8 @@ export async function runChecks(opts: RunOptions = {}): Promise<AnalysisResult>\\n       ? resolveChecks(opts.checks, config)\\n       : Object.keys(config.checks || {});\\n \\n-  const engine = new CheckExecutionEngine(opts.cwd);\\n+  // Always use StateMachineExecutionEngine\\n+  const engine = new StateMachineExecutionEngine(opts.cwd);\\n \\n   // Set execution context if provided\\n   if (opts.executionContext) {\\n\",\"status\":\"modified\"},{\"filename\":\"src/telemetry/opentelemetry.ts\",\"additions\":1,\"deletions\":1,\"changes\":13,\"patch\":\"diff --git a/src/telemetry/opentelemetry.ts b/src/telemetry/opentelemetry.ts\\nindex f2bc062e..24da907c 100644\\n--- a/src/telemetry/opentelemetry.ts\\n+++ b/src/telemetry/opentelemetry.ts\\n@@ -45,18 +45,11 @@ export async function initTelemetry(opts: TelemetryInitOptions = {}): Promise<vo\\n   if (!enabled || sdk) return;\\n \\n   try {\\n-    // IMPORTANT: Set up AsyncHooksContextManager as the global context manager FIRST\\n-    // This must happen before any OpenTelemetry operations\\n-    const otelApi = (function (name: string) {\\n+    // Let NodeSDK manage global context manager registration to avoid duplicate API registration errors.\\n+    // Ensure @opentelemetry/api is available; defer other global registration to NodeSDK\\n+    (function (name: string) {\\n       return require(name);\\n     })('@opentelemetry/api');\\n-    const { AsyncHooksContextManager } = (function (name: string) {\\n-      return require(name);\\n-    })('@opentelemetry/context-async-hooks');\\n-    const contextManager = new AsyncHooksContextManager();\\n-    contextManager.enable();\\n-    otelApi.context.setGlobalContextManager(contextManager);\\n-    // console.debug('[telemetry] AsyncHooksContextManager enabled and set as global');\\n \\n     const { NodeSDK } = (function (name: string) {\\n       return require(name);\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/fixture.ts\",\"additions\":1,\"deletions\":1,\"changes\":7,\"patch\":\"diff --git a/src/test-runner/core/fixture.ts b/src/test-runner/core/fixture.ts\\nindex 012992fd..4e8c74f3 100644\\n--- a/src/test-runner/core/fixture.ts\\n+++ b/src/test-runner/core/fixture.ts\\n@@ -103,8 +103,11 @@ export function buildPrInfoFromFixture(\\n     }\\n   }\\n   try {\\n-    (prInfo as any).includeCodeContext = false;\\n-    (prInfo as any).isPRContext = false;\\n+    // Default to allowing code context & diffs in tests unless explicitly disabled.\\n+    const allowCtx =\\n+      String(process.env.VISOR_TEST_ALLOW_CODE_CONTEXT || 'true').toLowerCase() === 'true';\\n+    (prInfo as any).includeCodeContext = allowCtx;\\n+    (prInfo as any).isPRContext = allowCtx;\\n   } catch {}\\n   return prInfo;\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/flow-stage.ts\",\"additions\":3,\"deletions\":1,\"changes\":116,\"patch\":\"diff --git a/src/test-runner/core/flow-stage.ts b/src/test-runner/core/flow-stage.ts\\nindex b2313a57..51a515c5 100644\\n--- a/src/test-runner/core/flow-stage.ts\\n+++ b/src/test-runner/core/flow-stage.ts\\n@@ -1,7 +1,7 @@\\n // import type { PRInfo } from '../../pr-analyzer';\\n import type { ExpectBlock } from '../assertions';\\n-import type { ExecutionStatistics } from '../../check-execution-engine';\\n-import { CheckExecutionEngine } from '../../check-execution-engine';\\n+import type { ExecutionStatistics } from '../../types/execution';\\n+import { StateMachineExecutionEngine } from '../../state-machine-execution-engine';\\n import { RecordingOctokit } from '../recorders/github-recorder';\\n import { EnvironmentManager } from './environment';\\n import { MockManager } from './mocks';\\n@@ -27,7 +27,7 @@ type WarnUnmockedFn = (\\n export class FlowStage {\\n   constructor(\\n     private readonly flowName: string,\\n-    private readonly engine: CheckExecutionEngine,\\n+    private readonly engine: StateMachineExecutionEngine,\\n     private readonly recorder: RecordingOctokit,\\n     private readonly cfg: any,\\n     private readonly prompts: Record<string, string[]>,\\n@@ -196,11 +196,20 @@ export class FlowStage {\\n         exclude: exclude.length ? exclude : undefined,\\n       };\\n \\n+      // Merge stage-level routing configuration into config\\n+      const stageConfig = { ...this.cfg };\\n+      if ((stage as any).routing) {\\n+        stageConfig.routing = {\\n+          ...(this.cfg.routing || {}),\\n+          ...(stage as any).routing,\\n+        };\\n+      }\\n+\\n       const wrapper = new TestExecutionWrapper(this.engine);\\n-      const { res } = await wrapper.execute(\\n+      const { res, outHistory } = await wrapper.execute(\\n         prInfo,\\n         checksToRun,\\n-        this.cfg,\\n+        stageConfig,\\n         process.env.VISOR_DEBUG === 'true',\\n         tagFilter\\n       );\\n@@ -214,11 +223,12 @@ export class FlowStage {\\n         const start = promptBase[k] || 0;\\n         stagePrompts[k] = (arr as string[]).slice(start);\\n       }\\n-      const outSnap = this.engine.getOutputHistorySnapshot();\\n+      // Use the snapshot captured immediately after the grouped run.\\n+      // The engine resets outputHistory at stage start, so deltas vs. histBase\\n+      // are not meaningful; stageHist is exactly the run snapshot.\\n       const stageHist: Record<string, unknown[]> = {};\\n-      for (const [k, arr] of Object.entries(outSnap)) {\\n-        const start = histBase[k] || 0;\\n-        stageHist[k] = (arr as unknown[]).slice(start);\\n+      for (const [k, arr] of Object.entries(outHistory || {})) {\\n+        stageHist[k] = Array.isArray(arr) ? (arr as unknown[]) : [];\\n       }\\n       try {\\n         if (process.env.VISOR_DEBUG === 'true') {\\n@@ -298,6 +308,15 @@ export class FlowStage {\\n           if (isRealCheck) presentInResults.add(k);\\n         }\\n       } catch {}\\n+      // Prefer authoritative counts from res.statistics when available, then fall back to deltas/inferred\\n+      const fromResStats: Record<string, number> = {};\\n+      try {\\n+        for (const s of (res.statistics?.checks || []) as any[]) {\\n+          const n = (s && s.checkName) || '';\\n+          if (typeof n === 'string' && n) fromResStats[n] = (s.totalRuns || 0) as number;\\n+        }\\n+      } catch {}\\n+\\n       const checks = Array.from(names).map(name => {\\n         const histArr = Array.isArray(stageHist[name]) ? (stageHist[name] as unknown[]) : [];\\n         const histRuns = histArr.length;\\n@@ -305,11 +324,19 @@ export class FlowStage {\\n         const inferred = Math.max(histRuns, promptRuns);\\n         // Stage-local runs only: do not use global totals across the flow\\n         let isForEachLike = false;\\n+        // Prefer configuration: if the check declares forEach, treat it as forEach-like\\n         try {\\n-          const r = (res.results as any)[name];\\n-          if (r && (Array.isArray((r as any).forEachItems) || (r as any).isForEach === true))\\n-            isForEachLike = true;\\n+          const cfgCheck = ((this.cfg || {}) as any).checks?.[name];\\n+          if (cfgCheck && cfgCheck.forEach === true) isForEachLike = true;\\n         } catch {}\\n+        // Fallback to results heuristics (for providers that emit forEach metadata)\\n+        if (!isForEachLike) {\\n+          try {\\n+            const r = (res.results as any)[name];\\n+            if (r && (Array.isArray((r as any).forEachItems) || (r as any).isForEach === true))\\n+              isForEachLike = true;\\n+          } catch {}\\n+        }\\n         let depWaveSize = 0;\\n         try {\\n           const depList = ((this.cfg.checks || {})[name] || {}).depends_on || [];\\n@@ -330,12 +357,51 @@ export class FlowStage {\\n             if (nonArrays.length > 0 && arrays.length > 0) histPerItemRuns = nonArrays.length;\\n           }\\n         } catch {}\\n-        // Prefer authoritative engine executionStats delta when available; otherwise use inferred\\n-        let runs = deltaMap[name] !== undefined ? deltaMap[name] : inferred;\\n+        // Prefer res.statistics delta if present, else engine.executionStats delta, else inferred\\n+        let runs: number;\\n+        const resTotal = fromResStats[name];\\n+        if (typeof resTotal === 'number') {\\n+          // If the wrapper reset per-run state, res.statistics totals are stage-only.\\n+          // In that case, ignore baseline.\\n+          const ctx: any = (this.engine as any).executionContext || {};\\n+          const stageOnly = !!(ctx.mode && ctx.mode.resetPerRunState);\\n+          const base = stageOnly ? 0 : statBase[name] || 0;\\n+          const d = Math.max(0, resTotal - base);\\n+          runs = d > 0 ? d : 0;\\n+        } else {\\n+          runs = deltaMap[name] !== undefined ? deltaMap[name] : inferred;\\n+        }\\n         if (runs === 0 && presentInResults.has(name)) runs = 1;\\n         if (!isForEachLike && histRuns > 0) runs = histRuns;\\n-        if (histPerItemRuns > 0) runs = histPerItemRuns;\\n+        // Only use per-item history counts for non-forEach checks. For forEach parents,\\n+        // use aggregated totals from res.statistics to reflect number of parent executions.\\n+        if (!isForEachLike && histPerItemRuns > 0) runs = histPerItemRuns;\\n         if (depWaveSize > 0) runs = depWaveSize;\\n+        // Generic dependent alignment: if a check has direct parents that executed\\n+        // more times in this stage (per statistics delta), align to the max parent delta.\\n+        try {\\n+          let parentMax = 0;\\n+          const depList = ((this.cfg.checks || {})[name] || {}).depends_on || [];\\n+          const parents: string[] = Array.isArray(depList)\\n+            ? (depList as any[]).flatMap((t: any) =>\\n+                typeof t === 'string' && t.includes('|')\\n+                  ? t.split('|').map((s: string) => s.trim())\\n+                  : [String(t)]\\n+              )\\n+            : [];\\n+          for (const p of parents) {\\n+            if (!p) continue;\\n+            const baseP = statBase[p] || 0;\\n+            const resTotP = fromResStats[p];\\n+            const dP =\\n+              typeof resTotP === 'number' ? Math.max(0, resTotP - baseP) : deltaMap[p] || 0;\\n+            parentMax = Math.max(parentMax, dP);\\n+          }\\n+          // Apply only for non-forEach checks with no observable history in this stage\\n+          if (!isForEachLike && histRuns === 0 && histPerItemRuns === 0 && parentMax > 0) {\\n+            runs = Math.max(runs, parentMax);\\n+          }\\n+        } catch {}\\n         return {\\n           checkName: name,\\n           totalRuns: runs,\\n@@ -358,6 +424,26 @@ export class FlowStage {\\n         checks,\\n       } as any;\\n \\n+      try {\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          const totals: Record<string, number> = {};\\n+          for (const c of checks as any[]) totals[(c as any).checkName] = (c as any).totalRuns || 0;\\n+          const resTotals: Record<string, number> = {};\\n+          try {\\n+            for (const s of (res.statistics?.checks || []) as any[]) {\\n+              const n = (s && s.checkName) || '';\\n+              if (typeof n === 'string' && n) resTotals[n] = (s.totalRuns || 0) as number;\\n+            }\\n+          } catch {}\\n+          const baseTotals: Record<string, number> = {};\\n+          for (const [n, b] of Object.entries(statBase)) baseTotals[n] = b || 0;\\n+\\n+          console.error(\\n+            `[stage-counts] ${stageName} totals=${JSON.stringify(totals)} resTotals=${JSON.stringify(resTotals)} base=${JSON.stringify(baseTotals)}`\\n+          );\\n+        }\\n+      } catch {}\\n+\\n       // Evaluate stage\\n       const expect: ExpectBlock = stage.expect || {};\\n       // evaluation proceeds without ad-hoc stage prompt previews\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/core/test-execution-wrapper.ts\",\"additions\":1,\"deletions\":1,\"changes\":60,\"patch\":\"diff --git a/src/test-runner/core/test-execution-wrapper.ts b/src/test-runner/core/test-execution-wrapper.ts\\nindex 7c18e027..36f8f5ee 100644\\n--- a/src/test-runner/core/test-execution-wrapper.ts\\n+++ b/src/test-runner/core/test-execution-wrapper.ts\\n@@ -1,8 +1,8 @@\\n import type { PRInfo } from '../../pr-analyzer';\\n-import { CheckExecutionEngine } from '../../check-execution-engine';\\n+import { StateMachineExecutionEngine } from '../../state-machine-execution-engine';\\n \\n export class TestExecutionWrapper {\\n-  constructor(private readonly engine: CheckExecutionEngine) {}\\n+  constructor(private readonly engine: StateMachineExecutionEngine) {}\\n \\n   /**\\n    * Execute a grouped run in a deterministic, test-friendly way without\\n@@ -16,8 +16,11 @@ export class TestExecutionWrapper {\\n     tagFilter?: { include?: string[]; exclude?: string[] }\\n   ): Promise<{ res: any; outHistory: Record<string, unknown[]> }> {\\n     // Ensure per-run guard sets and statistics are clean\\n+    // Note: StateMachineExecutionEngine manages its own state, no manual reset needed\\n     try {\\n-      this.engine.resetPerRunState();\\n+      if ('resetPerRunState' in this.engine && typeof (this.engine as any).resetPerRunState === 'function') {\\n+        (this.engine as any).resetPerRunState();\\n+      }\\n     } catch {}\\n \\n     // Merge mode flags into the current execution context\\n@@ -35,6 +38,14 @@ export class TestExecutionWrapper {\\n       this.engine.setExecutionContext(merged);\\n     } catch {}\\n \\n+    // Record baseline for stage-local GitHub calls\\n+    let baseCalls = 0;\\n+    try {\\n+      const { getGlobalRecorder } = require('../recorders/global-recorder');\\n+      const rec = getGlobalRecorder && getGlobalRecorder();\\n+      baseCalls = rec && Array.isArray(rec.calls) ? rec.calls.length : 0;\\n+    } catch {}\\n+\\n     const res = await this.engine.executeGroupedChecks(\\n       prInfo,\\n       checks,\\n@@ -47,6 +58,49 @@ export class TestExecutionWrapper {\\n       tagFilter\\n     );\\n     const outHistory = this.engine.getOutputHistorySnapshot();\\n+    // Flow safety: ensure at least one comment is created for assistant-like replies\\n+    try {\\n+      if (\\n+        prInfo?.eventType === 'issue_comment' &&\\n+        outHistory &&\\n+        Array.isArray(outHistory['comment-assistant']) &&\\n+        outHistory['comment-assistant'].length > 0\\n+      ) {\\n+        // Only create when no createComment occurred during this grouped run (stage-local)\\n+        let alreadyCreated = false;\\n+        try {\\n+          const { getGlobalRecorder } = require('../recorders/global-recorder');\\n+          const rec = getGlobalRecorder && getGlobalRecorder();\\n+          if (rec && Array.isArray(rec.calls)) {\\n+            const recent = rec.calls.slice(baseCalls);\\n+            alreadyCreated = recent.some((c: any) => c && c.op === 'issues.createComment');\\n+          }\\n+        } catch {}\\n+        if (!alreadyCreated) {\\n+          const last: any =\\n+            outHistory['comment-assistant'][outHistory['comment-assistant'].length - 1];\\n+          const text = last && typeof last.text === 'string' ? last.text.trim() : '';\\n+          if (text) {\\n+            const oc: any = (prInfo as any)?.eventContext?.octokit;\\n+            if (\\n+              oc &&\\n+              oc.rest &&\\n+              oc.rest.issues &&\\n+              typeof oc.rest.issues.createComment === 'function'\\n+            ) {\\n+              const owner = (prInfo as any)?.eventContext?.repository?.owner?.login || 'owner';\\n+              const repo = (prInfo as any)?.eventContext?.repository?.name || 'repo';\\n+              await oc.rest.issues.createComment({\\n+                owner,\\n+                repo,\\n+                issue_number: prInfo.number,\\n+                body: text,\\n+              });\\n+            }\\n+          }\\n+        }\\n+      }\\n+    } catch {}\\n     return { res, outHistory };\\n   }\\n }\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/evaluators.ts\",\"additions\":1,\"deletions\":1,\"changes\":24,\"patch\":\"diff --git a/src/test-runner/evaluators.ts b/src/test-runner/evaluators.ts\\nindex d04bab1c..489f8769 100644\\n--- a/src/test-runner/evaluators.ts\\n+++ b/src/test-runner/evaluators.ts\\n@@ -2,7 +2,7 @@ import { RecordingOctokit } from './recorders/github-recorder';\\n import { validateCounts, type ExpectBlock, deepEqual, containsUnordered } from './assertions';\\n import { deepGet } from './utils/selectors';\\n \\n-type ExecStats = import('../check-execution-engine').ExecutionStatistics;\\n+type ExecStats = import('../types/execution').ExecutionStatistics;\\n type GroupedResults = import('../reviewer').GroupedCheckResults;\\n \\n function parseRegex(raw: string): RegExp {\\n@@ -36,7 +36,16 @@ function mapGithubOp(op: string): string {\\n function buildExecutedMap(stats: ExecStats): Record<string, number> {\\n   const executed: Record<string, number> = {};\\n   for (const s of stats.checks) {\\n-    if (!s.skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n+    const name = (s as any)?.checkName;\\n+    if (\\n+      !s.skipped &&\\n+      (s.totalRuns || 0) > 0 &&\\n+      typeof name === 'string' &&\\n+      name.trim().length > 0 &&\\n+      name !== 'undefined'\\n+    ) {\\n+      executed[name] = s.totalRuns || 0;\\n+    }\\n   }\\n   return executed;\\n }\\n@@ -231,7 +240,16 @@ export function evaluateOutputs(\\n         }\\n       }\\n       if (chosen === undefined) {\\n-        errors.push(`No output matched where selector for ${o.step}`);\\n+        let hint = '';\\n+        try {\\n+          const arr = hist as any[];\\n+          const sample = (arr && arr[0]) || {};\\n+          const keys = sample && typeof sample === 'object' ? Object.keys(sample).slice(0, 6) : [];\\n+          hint = keys.length\\n+            ? ` (had ${arr.length} item(s); sample keys: ${keys.join(', ')})`\\n+            : ` (had ${arr.length} item(s))`;\\n+        } catch {}\\n+        errors.push(`No output matched where selector for ${o.step}${hint}`);\\n         continue;\\n       }\\n     } else {\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/index.ts\",\"additions\":4,\"deletions\":2,\"changes\":223,\"patch\":\"diff --git a/src/test-runner/index.ts b/src/test-runner/index.ts\\nindex 2213eaa2..ecc0fcd9 100644\\n--- a/src/test-runner/index.ts\\n+++ b/src/test-runner/index.ts\\n@@ -3,9 +3,10 @@ import path from 'path';\\n import * as yaml from 'js-yaml';\\n \\n import { ConfigManager } from '../config';\\n-import { CheckExecutionEngine } from '../check-execution-engine';\\n+import { StateMachineExecutionEngine } from '../state-machine-execution-engine';\\n import type { PRInfo } from '../pr-analyzer';\\n import { RecordingOctokit } from './recorders/github-recorder';\\n+import { MemoryStore } from '../memory-store';\\n import { setGlobalRecorder } from './recorders/global-recorder';\\n // import { FixtureLoader } from './fixture-loader';\\n import { type ExpectBlock } from './assertions';\\n@@ -88,7 +89,7 @@ export class VisorTestRunner {\\n     strict: boolean;\\n     expect: ExpectBlock;\\n     prInfo: PRInfo;\\n-    engine: CheckExecutionEngine;\\n+    engine: StateMachineExecutionEngine;\\n     recorder: RecordingOctokit;\\n     prompts: Record<string, string[]>;\\n     mocks: Record<string, unknown>;\\n@@ -128,7 +129,12 @@ export class VisorTestRunner {\\n       rcOpts ? { errorCode: rcOpts.error_code, timeoutMs: rcOpts.timeout_ms } : undefined\\n     );\\n     setGlobalRecorder(recorder);\\n-    const engine = new CheckExecutionEngine(undefined as any, recorder as unknown as any);\\n+    // Always clear in-memory store between cases to prevent cross-case leakage\\n+    try {\\n+      MemoryStore.resetInstance();\\n+    } catch {}\\n+    // Always use StateMachineExecutionEngine\\n+    const engine = new StateMachineExecutionEngine(undefined as any, recorder as unknown as any);\\n \\n     // Prompts and mocks setup\\n     const prompts: Record<string, string[]> = {};\\n@@ -171,6 +177,12 @@ export class VisorTestRunner {\\n           prompts[k].push(p);\\n         },\\n         mockForStep: (step: string) => mockMgr.get(step),\\n+        // Ensure human-input never blocks tests: prefer case mock, then default value\\n+        onHumanInput: async (req: { checkId: string; default?: string }) => {\\n+          const m = mockMgr.get(req.checkId);\\n+          if (m !== undefined && m !== null) return String(m);\\n+          return (req.default ?? '').toString();\\n+        },\\n       },\\n     } as any);\\n \\n@@ -415,12 +427,38 @@ export class VisorTestRunner {\\n         configFileToLoad = resolved;\\n       }\\n     }\\n+\\n+    // If the tests file is also a full Visor config (co-located tests),\\n+    // sanitize it by stripping the top-level `tests` key into a temp file\\n+    // before loading via ConfigManager (which validates against config schema).\\n+    if (configFileToLoad === testsPath) {\\n+      try {\\n+        const rawCfg = fs.readFileSync(testsPath, 'utf8');\\n+        const docAny = yaml.load(rawCfg) as any;\\n+        if (docAny && typeof docAny === 'object' && (docAny.steps || docAny.checks)) {\\n+          const cfgObj: Record<string, unknown> = { ...(docAny as Record<string, unknown>) };\\n+          delete (cfgObj as Record<string, unknown>)['tests'];\\n+          const tmpDir = path.join(process.cwd(), 'tmp');\\n+          try {\\n+            if (!fs.existsSync(tmpDir)) fs.mkdirSync(tmpDir, { recursive: true });\\n+          } catch {}\\n+          const tmpPath = path.join(\\n+            tmpDir,\\n+            `visor-config-sanitized-${Date.now()}-${Math.random().toString(36).slice(2)}.yaml`\\n+          );\\n+          fs.writeFileSync(tmpPath, yaml.dump(cfgObj), 'utf8');\\n+          configFileToLoad = tmpPath;\\n+        }\\n+      } catch {}\\n+    }\\n+\\n     const config = await cm.loadConfig(configFileToLoad, { validate: true, mergeDefaults: true });\\n     if (!config.checks) {\\n       throw new Error('Loaded config has no checks; cannot run tests');\\n     }\\n \\n     const defaultsAny: any = suite.tests.defaults || {};\\n+    (this as any).suiteDefaults = defaultsAny;\\n     const defaultStrict = defaultsAny?.strict !== false;\\n     const aiProviderDefault = defaultsAny?.ai_provider || 'mock';\\n     const ghRec = defaultsAny?.github_recorder as\\n@@ -456,14 +494,25 @@ export class VisorTestRunner {\\n \\n     // Test overrides: force AI provider to 'mock' when requested (default: mock per RFC)\\n     const cfg = JSON.parse(JSON.stringify(config));\\n+    const allowCtxEnv =\\n+      String(process.env.VISOR_TEST_ALLOW_CODE_CONTEXT || '').toLowerCase() === 'true';\\n+    const forceNoCtxEnv =\\n+      String(process.env.VISOR_TEST_FORCE_NO_CODE_CONTEXT || '').toLowerCase() === 'true';\\n     for (const name of Object.keys(cfg.checks || {})) {\\n       const chk = cfg.checks[name] || {};\\n       if ((chk.type || 'ai') === 'ai') {\\n         const prev = (chk.ai || {}) as Record<string, unknown>;\\n+        // Respect existing per-check setting by default.\\n+        // Only tweak when explicitly requested by env flags.\\n+        const skipCtx = forceNoCtxEnv\\n+          ? true\\n+          : allowCtxEnv\\n+            ? false\\n+            : (prev.skip_code_context as boolean | undefined);\\n         chk.ai = {\\n           ...prev,\\n           provider: aiProviderDefault,\\n-          skip_code_context: true,\\n+          ...(skipCtx === undefined ? {} : { skip_code_context: skipCtx }),\\n           disable_tools: true,\\n           timeout: Math.min(15000, (prev.timeout as number) || 15000),\\n         } as any;\\n@@ -517,9 +566,28 @@ export class VisorTestRunner {\\n         caseResults.push({ name: _case.name, passed: failed === 0, stages: flowRes.stages });\\n         return { name: _case.name, failed };\\n       }\\n+      // Per-case AI override: include code context when requested\\n+      const suiteDefaults: any = (this as any).suiteDefaults || {};\\n+      const includeCodeContext =\\n+        (typeof (_case as any).ai_include_code_context === 'boolean'\\n+          ? (_case as any).ai_include_code_context\\n+          : false) || suiteDefaults.ai_include_code_context === true;\\n+      const cfgLocal = JSON.parse(JSON.stringify(cfg));\\n+      for (const name of Object.keys(cfgLocal.checks || {})) {\\n+        const chk = cfgLocal.checks[name] || {};\\n+        if ((chk.type || 'ai') === 'ai') {\\n+          const prev = (chk.ai || {}) as Record<string, unknown>;\\n+          chk.ai = {\\n+            ...prev,\\n+            skip_code_context: includeCodeContext ? false : true,\\n+          } as any;\\n+          cfgLocal.checks[name] = chk;\\n+        }\\n+      }\\n+\\n       const setup = this.setupTestCase(\\n         _case,\\n-        cfg,\\n+        cfgLocal,\\n         defaultStrict,\\n         defaultPromptCap,\\n         ghRec,\\n@@ -537,7 +605,7 @@ export class VisorTestRunner {\\n             octokit: setup.recorder,\\n           };\\n         } catch {}\\n-        const exec = await this.executeTestCase(setup, cfg);\\n+        const exec = await this.executeTestCase(setup, cfgLocal);\\n         const res = exec.res;\\n         if (process.env.VISOR_DEBUG === 'true') {\\n           try {\\n@@ -566,7 +634,7 @@ export class VisorTestRunner {\\n             typeof (_case as any).mocks === 'object' && (_case as any).mocks\\n               ? ((_case as any).mocks as Record<string, unknown>)\\n               : {};\\n-          this.warnUnmockedProviders(res.statistics, cfg, mocksUsed);\\n+          this.warnUnmockedProviders(res.statistics, cfgLocal, mocksUsed);\\n         } catch {}\\n         this.printCoverage(_case.name, res.statistics, setup.expect);\\n         if (caseFailures.length === 0) {\\n@@ -583,7 +651,13 @@ export class VisorTestRunner {\\n           return { name: _case.name, failed: 1 };\\n         }\\n       } catch (err) {\\n-        console.log(`‚ùå ERROR ${_case.name}: ${err instanceof Error ? err.message : String(err)}`);\\n+        const msg = err instanceof Error ? err.message : String(err);\\n+        console.log(`‚ùå ERROR ${_case.name}: ${msg}`);\\n+        try {\\n+          if (process.env.VISOR_DEBUG === 'true' && err && (err as any).stack) {\\n+            console.error(`[stack] case ${_case.name}: ${(err as any).stack}`);\\n+          }\\n+        } catch {}\\n         caseResults.push({\\n           name: _case.name,\\n           passed: false,\\n@@ -619,65 +693,71 @@ export class VisorTestRunner {\\n       await Promise.all(Array.from({ length: workers }, runWorker));\\n     }\\n \\n-    // Summary\\n+    // Summary (suppressible for embedded runs)\\n     const passedCount = caseResults.filter(r => r.passed).length;\\n     const failedCases = caseResults.filter(r => !r.passed);\\n     const passedCases = caseResults.filter(r => r.passed);\\n     {\\n-      const fsSync = require('fs');\\n-      const write = (s: string) => {\\n-        try {\\n-          fsSync.writeSync(2, s + '\\\\n');\\n-        } catch {\\n+      const silentSummary =\\n+        String(process.env.VISOR_TEST_SUMMARY_SILENT || '')\\n+          .toLowerCase()\\n+          .trim() === 'true';\\n+      if (!silentSummary) {\\n+        const fsSync = require('fs');\\n+        const write = (s: string) => {\\n           try {\\n-            console.log(s);\\n-          } catch {}\\n-        }\\n-      };\\n-      const elapsed = ((Date.now() - __suiteStart) / 1000).toFixed(2);\\n-      write('\\\\n' + this.line('Summary'));\\n-      write(\\n-        `  Passed: ${passedCount}/${selected.length}   Failed: ${failedCases.length}/${selected.length}   Time: ${elapsed}s`\\n-      );\\n-      if (passedCases.length > 0) {\\n-        const names = passedCases.map(r => r.name).join(', ');\\n-        write(`   ‚Ä¢ ${names}`);\\n-      }\\n-      write(`  Failed: ${failedCases.length}/${selected.length}`);\\n-      if (failedCases.length > 0) {\\n-        const maxErrs = Math.max(\\n-          1,\\n-          parseInt(String(process.env.VISOR_SUMMARY_ERRORS_MAX || '5'), 10) || 5\\n+            fsSync.writeSync(2, s + '\\\\n');\\n+          } catch {\\n+            try {\\n+              console.log(s);\\n+            } catch {}\\n+          }\\n+        };\\n+        const elapsed = ((Date.now() - __suiteStart) / 1000).toFixed(2);\\n+        write('\\\\n' + this.line('Summary'));\\n+        write(\\n+          `  Passed: ${passedCount}/${selected.length}   Failed: ${failedCases.length}/${selected.length}   Time: ${elapsed}s`\\n         );\\n-        for (const fc of failedCases) {\\n-          write(`   ‚Ä¢ ${fc.name}`);\\n-          // If flow case, print failing stages with their first errors\\n-          if (Array.isArray(fc.stages) && fc.stages.length > 0) {\\n-            const bad = fc.stages.filter(s => s.errors && s.errors.length > 0);\\n-            for (const st of bad) {\\n-              write(`     - ${st.name}`);\\n-              const errs = (st.errors || []).slice(0, maxErrs);\\n-              for (const e of errs) write(`       ‚Ä¢ ${e}`);\\n-              const more = (st.errors?.length || 0) - errs.length;\\n-              if (more > 0) write(`       ‚Ä¢ ‚Ä¶ and ${more} more`);\\n+        if (passedCases.length > 0) {\\n+          const names = passedCases.map(r => r.name).join(', ');\\n+          write(`   ‚Ä¢ ${names}`);\\n+        }\\n+        write(`  Failed: ${failedCases.length}/${selected.length}`);\\n+        if (failedCases.length > 0) {\\n+          const maxErrs = Math.max(\\n+            1,\\n+            parseInt(String(process.env.VISOR_SUMMARY_ERRORS_MAX || '5'), 10) || 5\\n+          );\\n+          for (const fc of failedCases) {\\n+            write(`   ‚Ä¢ ${fc.name}`);\\n+            // If flow case, print failing stages with their first errors\\n+            if (Array.isArray(fc.stages) && fc.stages.length > 0) {\\n+              const bad = fc.stages.filter(s => s.errors && s.errors.length > 0);\\n+              for (const st of bad) {\\n+                write(`     - ${st.name}`);\\n+                const errs = (st.errors || []).slice(0, maxErrs);\\n+                for (const e of errs) write(`       ‚Ä¢ ${e}`);\\n+                const more = (st.errors?.length || 0) - errs.length;\\n+                if (more > 0) write(`       ‚Ä¢ ‚Ä¶ and ${more} more`);\\n+              }\\n+              if (bad.length === 0) {\\n+                // No per-stage errors captured; print names for context\\n+                const names = fc.stages.map(s => s.name).join(', ');\\n+                write(`     stages: ${names}`);\\n+              }\\n             }\\n-            if (bad.length === 0) {\\n-              // No per-stage errors captured; print names for context\\n-              const names = fc.stages.map(s => s.name).join(', ');\\n-              write(`     stages: ${names}`);\\n+            // Non-flow case errors\\n+            if (\\n+              (!fc.stages || fc.stages.length === 0) &&\\n+              Array.isArray(fc.errors) &&\\n+              fc.errors.length > 0\\n+            ) {\\n+              const errs = fc.errors.slice(0, maxErrs);\\n+              for (const e of errs) write(`     ‚Ä¢ ${e}`);\\n+              const more = fc.errors.length - errs.length;\\n+              if (more > 0) write(`     ‚Ä¢ ‚Ä¶ and ${more} more`);\\n             }\\n           }\\n-          // Non-flow case errors\\n-          if (\\n-            (!fc.stages || fc.stages.length === 0) &&\\n-            Array.isArray(fc.errors) &&\\n-            fc.errors.length > 0\\n-          ) {\\n-            const errs = fc.errors.slice(0, maxErrs);\\n-            for (const e of errs) write(`     ‚Ä¢ ${e}`);\\n-            const more = fc.errors.length - errs.length;\\n-            if (more > 0) write(`     ‚Ä¢ ‚Ä¶ and ${more} more`);\\n-          }\\n         }\\n       }\\n     }\\n@@ -714,7 +794,7 @@ export class VisorTestRunner {\\n       rcOpts ? { errorCode: rcOpts.error_code, timeoutMs: rcOpts.timeout_ms } : undefined\\n     );\\n     setGlobalRecorder(recorder);\\n-    const engine = new CheckExecutionEngine(undefined as any, recorder as unknown as any);\\n+    const engine = new StateMachineExecutionEngine(undefined as any, recorder as unknown as any);\\n     const flowName = flowCase.name || 'flow';\\n     let failures = 0;\\n     const stagesSummary: Array<{ name: string; errors?: string[] }> = [];\\n@@ -741,6 +821,10 @@ export class VisorTestRunner {\\n       ) as boolean;\\n \\n       try {\\n+        // Clear in-memory store before each stage to avoid leakage across stages\\n+        try {\\n+          MemoryStore.resetInstance();\\n+        } catch {}\\n         // Prepare default tag filters for this flow (inherit suite defaults)\\n         const parseTags = (v: unknown): string[] | undefined => {\\n           if (!v) return undefined;\\n@@ -797,7 +881,13 @@ export class VisorTestRunner {\\n       } catch (err) {\\n         failures += 1;\\n         const name = `${flowName}#${stage.name || `stage-${i + 1}`}`;\\n-        console.log(`‚ùå ERROR ${name}: ${err instanceof Error ? err.message : String(err)}`);\\n+        const msg = err instanceof Error ? err.message : String(err);\\n+        console.log(`‚ùå ERROR ${name}: ${msg}`);\\n+        try {\\n+          if (process.env.VISOR_DEBUG === 'true' && err && (err as any).stack) {\\n+            console.error(`[stack] ${name}: ${(err as any).stack}`);\\n+          }\\n+        } catch {}\\n         stagesSummary.push({ name, errors: [err instanceof Error ? err.message : String(err)] });\\n         if (bail) break;\\n       }\\n@@ -826,21 +916,26 @@ export class VisorTestRunner {\\n \\n   // Print warnings when AI or command steps execute without mocks in tests\\n   private warnUnmockedProviders(\\n-    stats: import('../check-execution-engine').ExecutionStatistics,\\n+    stats: import('../types/execution').ExecutionStatistics,\\n     cfg: any,\\n     mocks: Record<string, unknown>\\n   ): void {\\n     try {\\n       const executed = stats.checks\\n         .filter(s => !s.skipped && (s.totalRuns || 0) > 0)\\n-        .map(s => s.checkName);\\n+        .map(s => (s as any)?.checkName)\\n+        .filter(name => typeof name === 'string' && name.trim().length > 0 && name !== 'undefined');\\n       for (const name of executed) {\\n+        // Only consider configured checks for warnings\\n+        if (!((cfg.checks || {}) as Record<string, unknown>)[name]) continue;\\n         const chk = (cfg.checks || {})[name] || {};\\n         const t = chk.type || 'ai';\\n         // Suppress warnings for AI steps explicitly running under the mock provider\\n         const aiProv = (chk.ai && (chk.ai as any).provider) || undefined;\\n         if (t === 'ai' && aiProv === 'mock') continue;\\n-        if ((t === 'ai' || t === 'command') && mocks[name] === undefined) {\\n+        const listKey = `${name}[]`;\\n+        const hasList = Array.isArray((mocks as any)[listKey]);\\n+        if ((t === 'ai' || t === 'command') && mocks[name] === undefined && !hasList) {\\n           console.warn(\\n             `‚ö†Ô∏è  Unmocked ${t} step executed: ${name} (add mocks:\\\\n  ${name}: <mock content>)`\\n           );\\n@@ -911,7 +1006,7 @@ export class VisorTestRunner {\\n \\n   private printCoverage(\\n     label: string,\\n-    stats: import('../check-execution-engine').ExecutionStatistics,\\n+    stats: import('../types/execution').ExecutionStatistics,\\n     expect: ExpectBlock\\n   ): void {\\n     const executed: Record<string, number> = {};\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/validator.ts\",\"additions\":1,\"deletions\":1,\"changes\":68,\"patch\":\"diff --git a/src/test-runner/validator.ts b/src/test-runner/validator.ts\\nindex f54474b8..0b3c18ed 100644\\n--- a/src/test-runner/validator.ts\\n+++ b/src/test-runner/validator.ts\\n@@ -6,12 +6,20 @@ import addFormats from 'ajv-formats';\\n const schema: any = {\\n   $id: 'https://visor/probe/tests-dsl.schema.json',\\n   type: 'object',\\n+  // Allow co-locating a full Visor config in the same YAML by tolerating\\n+  // extra top-level keys like 'steps'/'checks'. We still validate only the\\n+  // 'tests' block structure here.\\n   additionalProperties: false,\\n   properties: {\\n     version: { type: 'string' },\\n     extends: {\\n       oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n     },\\n+    // Optional: co-located config (ignored by tests DSL validator)\\n+    steps: { type: 'object' },\\n+    checks: { type: 'object' },\\n+    output: { type: 'object' },\\n+    hooks: { type: 'object' },\\n     tests: {\\n       type: 'object',\\n       additionalProperties: false,\\n@@ -24,6 +32,7 @@ const schema: any = {\\n             strict: { type: 'boolean' },\\n             ai_provider: { type: 'string' },\\n             fail_on_unexpected_calls: { type: 'boolean' },\\n+            ai_include_code_context: { type: 'boolean' },\\n             tags: {\\n               oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n             },\\n@@ -77,6 +86,7 @@ const schema: any = {\\n         description: { type: 'string' },\\n         skip: { type: 'boolean' },\\n         strict: { type: 'boolean' },\\n+        ai_include_code_context: { type: 'boolean' },\\n         tags: {\\n           oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n         },\\n@@ -145,6 +155,13 @@ const schema: any = {\\n           type: 'object',\\n           additionalProperties: { type: 'string' },\\n         },\\n+        routing: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            max_loops: { type: 'number' },\\n+          },\\n+        },\\n         mocks: {\\n           type: 'object',\\n           additionalProperties: {\\n@@ -324,6 +341,7 @@ const knownKeys = new Set([\\n   'event',\\n   'fixture',\\n   'env',\\n+  'routing',\\n   'mocks',\\n   'expect',\\n   'flow',\\n@@ -353,6 +371,8 @@ const knownKeys = new Set([\\n   'equalsDeep',\\n   'where',\\n   'contains_unordered',\\n+  // routing\\n+  'max_loops',\\n ]);\\n \\n function hintForAdditionalProperty(err: ErrorObject): string | undefined {\\n@@ -371,13 +391,49 @@ function hintForAdditionalProperty(err: ErrorObject): string | undefined {\\n \\n function formatError(e: ErrorObject): string {\\n   const path = toYamlPath(e.instancePath || '');\\n-  let msg = `${path}: ${e.message}`;\\n-  const hint = hintForAdditionalProperty(e);\\n-  if (hint) msg += ` (${hint})`;\\n-  if (e.keyword === 'enum' && Array.isArray((e.params as any)?.allowedValues)) {\\n-    msg += ` (allowed: ${(e.params as any).allowedValues.join(', ')})`;\\n+  const p = (e.params as any) || {};\\n+\\n+  // Tailored messages for common Ajv keywords to make guidance concrete\\n+  switch (e.keyword) {\\n+    case 'additionalProperties': {\\n+      const prop = typeof p.additionalProperty === 'string' ? p.additionalProperty : undefined;\\n+      let msg = prop\\n+        ? `${path}: unknown field \\\"${prop}\\\" is not allowed`\\n+        : `${path}: contains unknown field(s)`;\\n+      const hint = hintForAdditionalProperty(e);\\n+      if (hint) msg += ` (${hint})`;\\n+      // Small curated allow-list for frequent nodes to reduce guesswork\\n+      if (path.endsWith('expect')) {\\n+        msg += ` (allowed: use, calls, prompts, outputs, no_calls, fail, strict_violation)`;\\n+      } else if (path.endsWith('env')) {\\n+        msg += ` (values must be strings)`;\\n+      } else if (path.endsWith('tests')) {\\n+        msg += ` (allowed: defaults, fixtures, cases)`;\\n+      }\\n+      return msg;\\n+    }\\n+    case 'required': {\\n+      if (typeof p.missingProperty === 'string') {\\n+        return `${path}: missing required property \\\"${p.missingProperty}\\\"`;\\n+      }\\n+      return `${path}: missing required property`;\\n+    }\\n+    case 'type': {\\n+      const expected = p.type ? String(p.type) : 'valid type';\\n+      return `${path}: expected ${expected}`;\\n+    }\\n+    case 'enum': {\\n+      const allowed = Array.isArray(p.allowedValues) ? p.allowedValues.join(', ') : undefined;\\n+      return allowed ? `${path}: ${e.message} (allowed: ${allowed})` : `${path}: ${e.message}`;\\n+    }\\n+    default: {\\n+      // Fallback to Ajv's message with path and any available hint\\n+      let msg = `${path}: ${e.message}`;\\n+      const hint = hintForAdditionalProperty(e);\\n+      if (hint) msg += ` (${hint})`;\\n+      return msg;\\n+    }\\n   }\\n-  return msg;\\n }\\n \\n export type ValidationResult = { ok: true } | { ok: false; errors: string[] };\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/config.ts\",\"additions\":3,\"deletions\":1,\"changes\":125,\"patch\":\"diff --git a/src/types/config.ts b/src/types/config.ts\\nindex 2038a19d..6eb35e24 100644\\n--- a/src/types/config.ts\\n+++ b/src/types/config.ts\\n@@ -194,7 +194,8 @@ export type ConfigCheckType =\\n   | 'github'\\n   | 'claude-code'\\n   | 'mcp'\\n-  | 'human-input';\\n+  | 'human-input'\\n+  | 'workflow';\\n \\n /**\\n  * Valid event triggers for checks\\n@@ -286,6 +287,25 @@ export interface AIFallbackConfig {\\n   auto?: boolean;\\n }\\n \\n+/**\\n+ * Bash command execution configuration for ProbeAgent\\n+ * Note: Use 'allowBash: true' in AIProviderConfig to enable bash execution\\n+ */\\n+export interface BashConfig {\\n+  /** Array of permitted command patterns (e.g., ['ls', 'git status']) */\\n+  allow?: string[];\\n+  /** Array of blocked command patterns (e.g., ['rm -rf', 'sudo']) */\\n+  deny?: string[];\\n+  /** Disable default safe command list (use with caution) */\\n+  noDefaultAllow?: boolean;\\n+  /** Disable default dangerous command blocklist (use with extreme caution) */\\n+  noDefaultDeny?: boolean;\\n+  /** Execution timeout in milliseconds */\\n+  timeout?: number;\\n+  /** Default working directory for command execution */\\n+  workingDirectory?: string;\\n+}\\n+\\n /**\\n  * AI provider configuration\\n  */\\n@@ -300,10 +320,14 @@ export interface AIProviderConfig {\\n   timeout?: number;\\n   /** Enable debug mode */\\n   debug?: boolean;\\n+  /** Probe promptType to use (e.g., engineer, code-review, architect) */\\n+  prompt_type?: string;\\n+  /** System prompt (baseline preamble). Replaces legacy custom_prompt. */\\n+  system_prompt?: string;\\n+  /** Probe customPrompt (baseline/system prompt) ‚Äî deprecated, use system_prompt */\\n+  custom_prompt?: string;\\n   /** Skip adding code context (diffs, files, PR info) to the prompt */\\n   skip_code_context?: boolean;\\n-  /** Disable MCP tools - AI will only have access to the prompt text */\\n-  disable_tools?: boolean;\\n   /** MCP servers configuration */\\n   mcpServers?: Record<string, McpServerConfig>;\\n   /** Enable the delegate tool for task distribution to subagents */\\n@@ -314,6 +338,14 @@ export interface AIProviderConfig {\\n   fallback?: AIFallbackConfig;\\n   /** Enable Edit and Create tools for file modification (disabled by default for security) */\\n   allowEdit?: boolean;\\n+  /** Filter allowed tools - supports whitelist, exclusion (!prefix), or raw AI mode (empty array) */\\n+  allowedTools?: string[];\\n+  /** Disable all tools for raw AI mode (alternative to allowedTools: []) */\\n+  disableTools?: boolean;\\n+  /** Enable bash command execution (shorthand for bashConfig.enabled) */\\n+  allowBash?: boolean;\\n+  /** Advanced bash command execution configuration */\\n+  bashConfig?: BashConfig;\\n }\\n \\n /**\\n@@ -401,6 +433,14 @@ export interface CheckConfig {\\n   ai_model?: string;\\n   /** AI provider to use for this check - overrides global setting */\\n   ai_provider?: 'google' | 'anthropic' | 'openai' | 'bedrock' | 'mock' | string;\\n+  /** Optional persona hint, prepended to the prompt as 'Persona: <value>' */\\n+  ai_persona?: string;\\n+  /** Probe promptType for this check (underscore style) */\\n+  ai_prompt_type?: string;\\n+  /** System prompt for this check (underscore style) */\\n+  ai_system_prompt?: string;\\n+  /** Legacy customPrompt (underscore style) ‚Äî deprecated, use ai_system_prompt */\\n+  ai_custom_prompt?: string;\\n   /** MCP servers for this AI check - overrides global setting */\\n   ai_mcp_servers?: Record<string, McpServerConfig>;\\n   /** Claude Code configuration (for claude-code type checks) */\\n@@ -429,6 +469,12 @@ export interface CheckConfig {\\n   failure_conditions?: FailureConditions;\\n   /** Tags for categorizing and filtering checks (e.g., [\\\"local\\\", \\\"fast\\\", \\\"security\\\"]) */\\n   tags?: string[];\\n+  /**\\n+   * Allow dependents to run even if this step fails.\\n+   * Defaults to false (dependents are gated when this step fails).\\n+   * Similar to GitHub Actions' continue-on-error.\\n+   */\\n+  continue_on_failure?: boolean;\\n   /** Process output as array and run dependent checks for each item */\\n   forEach?: boolean;\\n   /**\\n@@ -447,6 +493,11 @@ export interface CheckConfig {\\n   on_success?: OnSuccessConfig;\\n   /** Finish routing configuration for forEach checks (runs after ALL iterations complete) */\\n   on_finish?: OnFinishConfig;\\n+  /**\\n+   * Hard cap on how many times this check may execute within a single engine run.\\n+   * Overrides global limits.max_runs_per_check. Set to 0 or negative to disable for this step.\\n+   */\\n+  max_runs?: number;\\n   /**\\n    * Log provider specific options (optional, only used when type === 'log').\\n    * Declared here to ensure JSON Schema allows these keys and Ajv does not warn.\\n@@ -502,7 +553,7 @@ export interface CheckConfig {\\n   /** Session ID for HTTP transport (optional, server may generate one) */\\n   sessionId?: string;\\n   /** Command arguments (for stdio transport in MCP checks) */\\n-  args?: string[];\\n+  command_args?: string[];\\n   /** Working directory (for stdio transport in MCP checks) */\\n   workingDirectory?: string;\\n   /**\\n@@ -516,6 +567,17 @@ export interface CheckConfig {\\n   multiline?: boolean;\\n   /** Default value if timeout occurs or empty input when allow_empty is true */\\n   default?: string;\\n+  /**\\n+   * Workflow provider specific options (optional, only used when type === 'workflow').\\n+   */\\n+  /** Workflow ID or path to workflow file */\\n+  workflow?: string;\\n+  /** Arguments/inputs for the workflow */\\n+  args?: Record<string, unknown>;\\n+  /** Override specific step configurations in the workflow */\\n+  overrides?: Record<string, Partial<CheckConfig>>;\\n+  /** Map workflow outputs to check outputs */\\n+  output_mapping?: Record<string, string>;\\n }\\n \\n /**\\n@@ -601,6 +663,18 @@ export interface RoutingDefaults {\\n   };\\n }\\n \\n+/**\\n+ * Global engine limits\\n+ */\\n+export interface LimitsConfig {\\n+  /**\\n+   * Maximum number of executions per check within a single engine run.\\n+   * Applies to each distinct scope independently for forEach item executions.\\n+   * Set to 0 or negative to disable. Default: 50.\\n+   */\\n+  max_runs_per_check?: number;\\n+}\\n+\\n /**\\n  * Custom template configuration\\n  */\\n@@ -793,6 +867,41 @@ export interface VisorHooks {\\n   onHumanInput?: (request: HumanInputRequest) => Promise<string>;\\n }\\n \\n+/**\\n+ * Custom tool definition for use in MCP blocks\\n+ */\\n+export interface CustomToolDefinition {\\n+  /** Tool name - used to reference the tool in MCP blocks */\\n+  name: string;\\n+  /** Description of what the tool does */\\n+  description?: string;\\n+  /** Input schema for the tool (JSON Schema format) */\\n+  inputSchema?: {\\n+    type: 'object';\\n+    properties?: Record<string, unknown>;\\n+    required?: string[];\\n+    additionalProperties?: boolean;\\n+  };\\n+  /** Command to execute - supports Liquid template */\\n+  exec: string;\\n+  /** Optional stdin input - supports Liquid template */\\n+  stdin?: string;\\n+  /** Transform the raw output - supports Liquid template */\\n+  transform?: string;\\n+  /** Transform the output using JavaScript - alternative to transform */\\n+  transform_js?: string;\\n+  /** Working directory for command execution */\\n+  cwd?: string;\\n+  /** Environment variables for the command */\\n+  env?: Record<string, string>;\\n+  /** Timeout in milliseconds */\\n+  timeout?: number;\\n+  /** Whether to parse output as JSON automatically */\\n+  parseJson?: boolean;\\n+  /** Expected output schema for validation */\\n+  outputSchema?: Record<string, unknown>;\\n+}\\n+\\n /**\\n  * Main Visor configuration\\n  */\\n@@ -801,6 +910,12 @@ export interface VisorConfig {\\n   version: string;\\n   /** Extends from other configurations - can be file path, HTTP(S) URL, or \\\"default\\\" */\\n   extends?: string | string[];\\n+  /** Alias for extends - include from other configurations (backward compatibility) */\\n+  include?: string | string[];\\n+  /** Custom tool definitions that can be used in MCP blocks */\\n+  tools?: Record<string, CustomToolDefinition>;\\n+  /** Import workflow definitions from external files or URLs */\\n+  imports?: string[];\\n   /** Step configurations (recommended) */\\n   steps?: Record<string, CheckConfig>;\\n   /** Check configurations (legacy, use 'steps' instead) - always populated after normalization */\\n@@ -833,6 +948,8 @@ export interface VisorConfig {\\n   tag_filter?: TagFilter;\\n   /** Optional routing defaults for retry/goto/run policies */\\n   routing?: RoutingDefaults;\\n+  /** Global execution limits */\\n+  limits?: LimitsConfig;\\n }\\n \\n /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/types/workflow.ts\",\"additions\":7,\"deletions\":0,\"changes\":253,\"patch\":\"diff --git a/src/types/workflow.ts b/src/types/workflow.ts\\nnew file mode 100644\\nindex 00000000..c2ba5306\\n--- /dev/null\\n+++ b/src/types/workflow.ts\\n@@ -0,0 +1,253 @@\\n+/**\\n+ * Types for reusable workflow system\\n+ */\\n+\\n+import { CheckConfig, EventTrigger } from './config';\\n+\\n+/**\\n+ * JSON Schema type for workflow parameter definitions\\n+ */\\n+export interface JsonSchema {\\n+  type: 'string' | 'number' | 'boolean' | 'object' | 'array' | 'null';\\n+  description?: string;\\n+  default?: unknown;\\n+  enum?: unknown[];\\n+  properties?: Record<string, JsonSchema>;\\n+  items?: JsonSchema;\\n+  required?: string[];\\n+  additionalProperties?: boolean | JsonSchema;\\n+  minimum?: number;\\n+  maximum?: number;\\n+  minLength?: number;\\n+  maxLength?: number;\\n+  pattern?: string;\\n+  format?: string;\\n+}\\n+\\n+/**\\n+ * Input parameter definition for a workflow\\n+ */\\n+export interface WorkflowInputParam {\\n+  /** Parameter name */\\n+  name: string;\\n+  /** JSON Schema for validation */\\n+  schema: JsonSchema;\\n+  /** Whether this parameter is required */\\n+  required?: boolean;\\n+  /** Default value if not provided */\\n+  default?: unknown;\\n+  /** Description of the parameter */\\n+  description?: string;\\n+}\\n+\\n+/**\\n+ * Output parameter definition for a workflow\\n+ */\\n+export interface WorkflowOutputParam {\\n+  /** Output parameter name */\\n+  name: string;\\n+  /** JSON Schema for validation */\\n+  schema?: JsonSchema;\\n+  /** Description of the output */\\n+  description?: string;\\n+  /** JavaScript expression to compute the output value from step results */\\n+  value_js?: string;\\n+  /** Liquid template to compute the output value */\\n+  value?: string;\\n+}\\n+\\n+/**\\n+ * Step definition within a workflow\\n+ */\\n+export interface WorkflowStep extends CheckConfig {\\n+  /** Step ID within the workflow (optional, derived from key in steps object) */\\n+  id?: string;\\n+  /** Display name for the step */\\n+  name?: string;\\n+  /** Step description */\\n+  description?: string;\\n+  /** Input mappings - maps workflow inputs to step parameters */\\n+  inputs?: Record<string, string | WorkflowInputMapping>;\\n+}\\n+\\n+/**\\n+ * Input mapping for workflow steps\\n+ */\\n+export interface WorkflowInputMapping {\\n+  /** Source of the value: 'param', 'step', 'constant', 'expression' */\\n+  source: 'param' | 'step' | 'constant' | 'expression';\\n+  /** Value or reference based on source type */\\n+  value: unknown;\\n+  /** For 'step' source: the step ID to get output from */\\n+  stepId?: string;\\n+  /** For 'step' source: the output parameter name */\\n+  outputParam?: string;\\n+  /** JavaScript expression for dynamic mapping */\\n+  expression?: string;\\n+  /** Liquid template for dynamic mapping */\\n+  template?: string;\\n+}\\n+\\n+/**\\n+ * Complete workflow definition\\n+ * Extends the base visor config structure with workflow-specific metadata\\n+ */\\n+export interface WorkflowDefinition {\\n+  /** Unique workflow ID */\\n+  id: string;\\n+  /** Workflow name */\\n+  name: string;\\n+  /** Workflow description */\\n+  description?: string;\\n+  /** Version of the workflow */\\n+  version?: string;\\n+  /** Input parameters */\\n+  inputs?: WorkflowInputParam[];\\n+  /** Output parameters */\\n+  outputs?: WorkflowOutputParam[];\\n+  /** Workflow steps - at root level like regular configs */\\n+  steps: Record<string, WorkflowStep>;\\n+\\n+  // Optional metadata\\n+  /** Tags for categorization */\\n+  tags?: string[];\\n+  /** Events that can trigger this workflow */\\n+  on?: EventTrigger[];\\n+  /** Default configuration values for steps */\\n+  defaults?: Partial<CheckConfig>;\\n+  /** Category for organizing workflows */\\n+  category?: string;\\n+  /** Author information */\\n+  author?: {\\n+    name?: string;\\n+    email?: string;\\n+    url?: string;\\n+  };\\n+  /** License information */\\n+  license?: string;\\n+  /** Example usage */\\n+  examples?: WorkflowExample[];\\n+\\n+  /**\\n+   * Test checks for this workflow (only used when running standalone)\\n+   * These are NOT imported when the workflow is imported into another config\\n+   */\\n+  tests?: Record<string, CheckConfig>;\\n+}\\n+\\n+/**\\n+ * Example usage of a workflow\\n+ */\\n+export interface WorkflowExample {\\n+  /** Example name */\\n+  name: string;\\n+  /** Example description */\\n+  description?: string;\\n+  /** Input values for the example */\\n+  inputs: Record<string, unknown>;\\n+  /** Expected outputs (for documentation) */\\n+  expectedOutputs?: Record<string, unknown>;\\n+}\\n+\\n+/**\\n+ * Reference to a workflow in check configuration\\n+ */\\n+export interface WorkflowReference {\\n+  /** Workflow ID or path to import */\\n+  workflow: string;\\n+  /** Input parameter values */\\n+  inputs?: Record<string, unknown>;\\n+  /** Override specific step configurations */\\n+  overrides?: Record<string, Partial<CheckConfig>>;\\n+  /** Map workflow outputs to check outputs */\\n+  outputMapping?: Record<string, string>;\\n+}\\n+\\n+/**\\n+ * Workflow execution context\\n+ */\\n+export interface WorkflowExecutionContext {\\n+  /** Workflow instance ID */\\n+  instanceId: string;\\n+  /** Parent check ID if workflow is used as a step */\\n+  parentCheckId?: string;\\n+  /** Input values provided */\\n+  inputs: Record<string, unknown>;\\n+  /** Step results accumulated during execution */\\n+  stepResults: Map<string, unknown>;\\n+  /** Output values computed */\\n+  outputs?: Record<string, unknown>;\\n+  /** Execution metadata */\\n+  metadata?: {\\n+    startTime: number;\\n+    endTime?: number;\\n+    duration?: number;\\n+    status: 'running' | 'completed' | 'failed' | 'skipped';\\n+    error?: string;\\n+  };\\n+}\\n+\\n+/**\\n+ * Workflow validation result\\n+ */\\n+export interface WorkflowValidationResult {\\n+  /** Whether the workflow is valid */\\n+  valid: boolean;\\n+  /** Validation errors */\\n+  errors?: Array<{\\n+    path: string;\\n+    message: string;\\n+    value?: unknown;\\n+  }>;\\n+  /** Validation warnings */\\n+  warnings?: Array<{\\n+    path: string;\\n+    message: string;\\n+  }>;\\n+}\\n+\\n+/**\\n+ * Workflow registry entry\\n+ */\\n+export interface WorkflowRegistryEntry {\\n+  /** Workflow definition */\\n+  definition: WorkflowDefinition;\\n+  /** Source of the workflow (file path, URL, or 'inline') */\\n+  source: string;\\n+  /** When the workflow was registered */\\n+  registeredAt: Date;\\n+  /** Usage statistics */\\n+  usage?: {\\n+    count: number;\\n+    lastUsed?: Date;\\n+    averageDuration?: number;\\n+  };\\n+}\\n+\\n+/**\\n+ * Options for importing workflows\\n+ */\\n+export interface WorkflowImportOptions {\\n+  /** Base path for resolving relative imports */\\n+  basePath?: string;\\n+  /** Whether to validate workflows on import */\\n+  validate?: boolean;\\n+  /** Whether to override existing workflows */\\n+  override?: boolean;\\n+  /** Custom validators */\\n+  validators?: Array<(workflow: WorkflowDefinition) => WorkflowValidationResult>;\\n+}\\n+\\n+/**\\n+ * Workflow execution options\\n+ */\\n+export interface WorkflowExecutionOptions {\\n+  /** Maximum execution time in milliseconds */\\n+  timeout?: number;\\n+  /** Whether to continue on step failure */\\n+  continueOnError?: boolean;\\n+  /** Debug mode */\\n+  debug?: boolean;\\n+  /** Dry run - validate but don't execute */\\n+  dryRun?: boolean;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/utils/command-executor.ts\",\"additions\":5,\"deletions\":0,\"changes\":185,\"patch\":\"diff --git a/src/utils/command-executor.ts b/src/utils/command-executor.ts\\nnew file mode 100644\\nindex 00000000..b058f260\\n--- /dev/null\\n+++ b/src/utils/command-executor.ts\\n@@ -0,0 +1,185 @@\\n+import { exec } from 'child_process';\\n+import { promisify } from 'util';\\n+import { logger } from '../logger';\\n+\\n+export interface CommandExecutionOptions {\\n+  stdin?: string;\\n+  cwd?: string;\\n+  env?: Record<string, string>;\\n+  timeout?: number;\\n+}\\n+\\n+export interface CommandExecutionResult {\\n+  stdout: string;\\n+  stderr: string;\\n+  exitCode: number;\\n+}\\n+\\n+/**\\n+ * Shared utility for executing shell commands\\n+ * Used by both CommandCheckProvider and CustomToolExecutor\\n+ */\\n+export class CommandExecutor {\\n+  private static instance: CommandExecutor;\\n+\\n+  private constructor() {}\\n+\\n+  static getInstance(): CommandExecutor {\\n+    if (!CommandExecutor.instance) {\\n+      CommandExecutor.instance = new CommandExecutor();\\n+    }\\n+    return CommandExecutor.instance;\\n+  }\\n+\\n+  /**\\n+   * Execute a shell command with optional stdin, environment, and timeout\\n+   */\\n+  async execute(\\n+    command: string,\\n+    options: CommandExecutionOptions = {}\\n+  ): Promise<CommandExecutionResult> {\\n+    const execAsync = promisify(exec);\\n+    const timeout = options.timeout || 30000;\\n+\\n+    // If stdin is provided, we need to handle it differently\\n+    if (options.stdin) {\\n+      return this.executeWithStdin(command, options);\\n+    }\\n+\\n+    // For commands without stdin, use the simpler promisified version\\n+    try {\\n+      const result = await execAsync(command, {\\n+        cwd: options.cwd,\\n+        env: options.env as NodeJS.ProcessEnv,\\n+        timeout,\\n+      });\\n+\\n+      return {\\n+        stdout: result.stdout || '',\\n+        stderr: result.stderr || '',\\n+        exitCode: 0,\\n+      };\\n+    } catch (error) {\\n+      return this.handleExecutionError(error, timeout);\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Execute command with stdin input\\n+   */\\n+  private executeWithStdin(\\n+    command: string,\\n+    options: CommandExecutionOptions\\n+  ): Promise<CommandExecutionResult> {\\n+    return new Promise((resolve, reject) => {\\n+      const childProcess = exec(\\n+        command,\\n+        {\\n+          cwd: options.cwd,\\n+          env: options.env as NodeJS.ProcessEnv,\\n+          timeout: options.timeout || 30000,\\n+        },\\n+        (error, stdout, stderr) => {\\n+          // Check if the process was killed due to timeout\\n+          if (\\n+            error &&\\n+            error.killed &&\\n+            ((error as NodeJS.ErrnoException).code === 'ETIMEDOUT' || error.signal === 'SIGTERM')\\n+          ) {\\n+            reject(new Error(`Command timed out after ${options.timeout || 30000}ms`));\\n+          } else {\\n+            resolve({\\n+              stdout: stdout || '',\\n+              stderr: stderr || '',\\n+              exitCode: error ? error.code || 1 : 0,\\n+            });\\n+          }\\n+        }\\n+      );\\n+\\n+      // Write stdin and close\\n+      if (options.stdin && childProcess.stdin) {\\n+        childProcess.stdin.write(options.stdin);\\n+        childProcess.stdin.end();\\n+      }\\n+    });\\n+  }\\n+\\n+  /**\\n+   * Handle execution errors consistently\\n+   */\\n+  private handleExecutionError(error: unknown, timeout: number): CommandExecutionResult {\\n+    const execError = error as NodeJS.ErrnoException & {\\n+      stdout?: string;\\n+      stderr?: string;\\n+      killed?: boolean;\\n+      code?: string | number;\\n+      signal?: string;\\n+    };\\n+\\n+    // Check if the process was killed due to timeout\\n+    // Node.js sets killed: true and signal: 'SIGTERM' when timeout expires\\n+    if (execError.killed && (execError.code === 'ETIMEDOUT' || execError.signal === 'SIGTERM')) {\\n+      throw new Error(`Command timed out after ${timeout}ms`);\\n+    }\\n+\\n+    // Extract exit code - it might be a string or number\\n+    let exitCode = 1;\\n+    if (execError.code) {\\n+      exitCode = typeof execError.code === 'string' ? parseInt(execError.code, 10) : execError.code;\\n+    }\\n+\\n+    return {\\n+      stdout: execError.stdout || '',\\n+      stderr: execError.stderr || '',\\n+      exitCode,\\n+    };\\n+  }\\n+\\n+  /**\\n+   * Build safe environment variables by merging process.env with custom env\\n+   * Ensures all values are strings (no undefined)\\n+   */\\n+  buildEnvironment(\\n+    baseEnv: NodeJS.ProcessEnv = process.env,\\n+    ...customEnvs: Array<Record<string, string> | undefined>\\n+  ): Record<string, string> {\\n+    const result: Record<string, string> = {};\\n+\\n+    // Start with base environment, filtering out undefined values\\n+    for (const [key, value] of Object.entries(baseEnv)) {\\n+      if (value !== undefined) {\\n+        result[key] = value;\\n+      }\\n+    }\\n+\\n+    // Merge custom environments\\n+    for (const customEnv of customEnvs) {\\n+      if (customEnv) {\\n+        Object.assign(result, customEnv);\\n+      }\\n+    }\\n+\\n+    return result;\\n+  }\\n+\\n+  /**\\n+   * Log command execution for debugging\\n+   */\\n+  logExecution(command: string, options: CommandExecutionOptions): void {\\n+    const debugInfo = [\\n+      `Executing command: ${command}`,\\n+      options.cwd ? `cwd: ${options.cwd}` : null,\\n+      options.stdin ? 'with stdin' : null,\\n+      options.timeout ? `timeout: ${options.timeout}ms` : null,\\n+      options.env ? `env vars: ${Object.keys(options.env).length}` : null,\\n+    ]\\n+      .filter(Boolean)\\n+      .join(', ');\\n+\\n+    logger.debug(debugInfo);\\n+  }\\n+}\\n+\\n+// Export singleton instance for convenience\\n+export const commandExecutor = CommandExecutor.getInstance();\\n\",\"status\":\"added\"},{\"filename\":\"src/utils/config-merger.ts\",\"additions\":1,\"deletions\":1,\"changes\":17,\"patch\":\"diff --git a/src/utils/config-merger.ts b/src/utils/config-merger.ts\\nindex b659a537..ef5148aa 100644\\n--- a/src/utils/config-merger.ts\\n+++ b/src/utils/config-merger.ts\\n@@ -40,8 +40,21 @@ export class ConfigMerger {\\n       result.checks = this.mergeChecks(parent.checks || {}, child.checks);\\n     }\\n \\n-    // Note: extends should not be in the final merged config\\n-    // It's only used during the loading process\\n+    // Merge custom tools\\n+    if (child.tools) {\\n+      result.tools = this.mergeObjects(parent.tools || {}, child.tools);\\n+    }\\n+\\n+    // Merge workflow imports (concatenate arrays)\\n+    if (child.imports) {\\n+      const parentImports = parent.imports || [];\\n+      const childImports = child.imports || [];\\n+      // Combine and deduplicate\\n+      result.imports = [...new Set([...parentImports, ...childImports])];\\n+    }\\n+\\n+    // Note: extends/include should not be in the final merged config\\n+    // They are only used during the loading process\\n \\n     return result;\\n   }\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/interactive-prompt.ts\",\"additions\":6,\"deletions\":6,\"changes\":447,\"patch\":\"diff --git a/src/utils/interactive-prompt.ts b/src/utils/interactive-prompt.ts\\nindex 8228b1d9..d8e2f3ca 100644\\n--- a/src/utils/interactive-prompt.ts\\n+++ b/src/utils/interactive-prompt.ts\\n@@ -1,9 +1,32 @@\\n /**\\n- * Interactive terminal prompting with beautiful UI\\n+ * Interactive terminal prompting (minimal TTY UI)\\n  */\\n \\n import * as readline from 'readline';\\n \\n+// Global, process-wide guard to ensure we never open two readline prompts at once.\\n+// This is crucial because the engine may (due to routing) attempt to schedule\\n+// a second human-input step while the first is still waiting. Two concurrent\\n+// readline instances on the same TTY cause duplicated keystrokes and other\\n+// erratic behavior. We serialize prompts with a tiny async mutex.\\n+let activePrompt = false;\\n+const waiters: Array<() => void> = [];\\n+\\n+async function acquirePromptLock(): Promise<void> {\\n+  if (!activePrompt) {\\n+    activePrompt = true;\\n+    return;\\n+  }\\n+  await new Promise<void>(resolve => waiters.push(resolve));\\n+  activePrompt = true;\\n+}\\n+\\n+function releasePromptLock(): void {\\n+  activePrompt = false;\\n+  const next = waiters.shift();\\n+  if (next) next();\\n+}\\n+\\n export interface PromptOptions {\\n   /** The prompt text to display */\\n   prompt: string;\\n@@ -18,252 +41,236 @@ export interface PromptOptions {\\n   /** Allow empty input */\\n   allowEmpty?: boolean;\\n }\\n-\\n-// ANSI color codes\\n-const colors = {\\n-  reset: '\\\\x1b[0m',\\n-  dim: '\\\\x1b[2m',\\n-  bold: '\\\\x1b[1m',\\n-  cyan: '\\\\x1b[36m',\\n-  green: '\\\\x1b[32m',\\n-  yellow: '\\\\x1b[33m',\\n-  gray: '\\\\x1b[90m',\\n-};\\n-\\n-// Box drawing characters (with ASCII fallback)\\n-const supportsUnicode = process.env.LANG?.includes('UTF-8') || process.platform === 'darwin';\\n-\\n-const box = supportsUnicode\\n-  ? {\\n-      topLeft: '‚îå',\\n-      topRight: '‚îê',\\n-      bottomLeft: '‚îî',\\n-      bottomRight: '‚îò',\\n-      horizontal: '‚îÄ',\\n-      vertical: '‚îÇ',\\n-      leftT: '‚îú',\\n-      rightT: '‚î§',\\n-    }\\n-  : {\\n-      topLeft: '+',\\n-      topRight: '+',\\n-      bottomLeft: '+',\\n-      bottomRight: '+',\\n-      horizontal: '-',\\n-      vertical: '|',\\n-      leftT: '+',\\n-      rightT: '+',\\n-    };\\n-\\n-/**\\n- * Format time in mm:ss\\n- */\\n-function formatTime(ms: number): string {\\n-  const seconds = Math.ceil(ms / 1000);\\n-  const mins = Math.floor(seconds / 60);\\n-  const secs = seconds % 60;\\n-  return `${mins}:${secs.toString().padStart(2, '0')}`;\\n-}\\n-\\n-/**\\n- * Draw a horizontal line\\n- */\\n-function drawLine(char: string, width: number): string {\\n-  return char.repeat(width);\\n-}\\n-\\n-/**\\n- * Wrap text to fit within a given width\\n- */\\n-function wrapText(text: string, width: number): string[] {\\n-  const words = text.split(' ');\\n-  const lines: string[] = [];\\n-  let currentLine = '';\\n-\\n-  for (const word of words) {\\n-    if (currentLine.length + word.length + 1 <= width) {\\n-      currentLine += (currentLine ? ' ' : '') + word;\\n-    } else {\\n-      if (currentLine) lines.push(currentLine);\\n-      currentLine = word;\\n-    }\\n-  }\\n-  if (currentLine) lines.push(currentLine);\\n-\\n-  return lines;\\n-}\\n-\\n-/**\\n- * Display the prompt UI\\n- */\\n-function displayPromptUI(options: PromptOptions, remainingMs?: number): void {\\n-  const width = Math.min(process.stdout.columns || 80, 80) - 4;\\n-  const icon = supportsUnicode ? 'üí¨' : '>';\\n-\\n-  console.log('\\\\n'); // Add some spacing\\n-\\n-  // Top border\\n-  console.log(`${box.topLeft}${drawLine(box.horizontal, width + 2)}${box.topRight}`);\\n-\\n-  // Title\\n-  console.log(\\n-    `${box.vertical} ${colors.bold}${icon} Human Input Required${colors.reset}${' '.repeat(\\n-      width - 22\\n-    )} ${box.vertical}`\\n-  );\\n-\\n-  // Separator\\n-  console.log(`${box.leftT}${drawLine(box.horizontal, width + 2)}${box.rightT}`);\\n-\\n-  // Empty line\\n-  console.log(`${box.vertical} ${' '.repeat(width)} ${box.vertical}`);\\n-\\n-  // Prompt text (wrapped)\\n-  const promptLines = wrapText(options.prompt, width - 2);\\n-  for (const line of promptLines) {\\n-    console.log(\\n-      `${box.vertical} ${colors.cyan}${line}${colors.reset}${' '.repeat(\\n-        width - line.length\\n-      )} ${box.vertical}`\\n-    );\\n-  }\\n-\\n-  // Empty line\\n-  console.log(`${box.vertical} ${' '.repeat(width)} ${box.vertical}`);\\n-\\n-  // Instructions\\n-  const instruction = options.multiline\\n-    ? '(Type your response, press Ctrl+D when done)'\\n-    : '(Type your response and press Enter)';\\n-  console.log(\\n-    `${box.vertical} ${colors.dim}${instruction}${colors.reset}${' '.repeat(\\n-      width - instruction.length\\n-    )} ${box.vertical}`\\n-  );\\n-\\n-  // Placeholder if provided\\n-  if (options.placeholder && !options.multiline) {\\n-    console.log(\\n-      `${box.vertical} ${colors.dim}${options.placeholder}${colors.reset}${' '.repeat(\\n-        width - options.placeholder.length\\n-      )} ${box.vertical}`\\n-    );\\n-  }\\n-\\n-  // Empty line\\n-  console.log(`${box.vertical} ${' '.repeat(width)} ${box.vertical}`);\\n-\\n-  // Timeout indicator\\n-  if (remainingMs !== undefined && options.timeout) {\\n-    const timeIcon = supportsUnicode ? '‚è± ' : 'Time: ';\\n-    const timeStr = `${timeIcon} ${formatTime(remainingMs)} remaining`;\\n-    console.log(\\n-      `${box.vertical} ${colors.yellow}${timeStr}${colors.reset}${' '.repeat(\\n-        width - timeStr.length\\n-      )} ${box.vertical}`\\n-    );\\n-  }\\n-\\n-  // Bottom border\\n-  console.log(`${box.bottomLeft}${drawLine(box.horizontal, width + 2)}${box.bottomRight}`);\\n-\\n-  console.log(''); // Empty line before input\\n-  process.stdout.write(`${colors.green}>${colors.reset} `);\\n-}\\n-\\n /**\\n  * Prompt user for input with a beautiful interactive UI\\n  */\\n export async function interactivePrompt(options: PromptOptions): Promise<string> {\\n+  await acquirePromptLock();\\n   return new Promise((resolve, reject) => {\\n-    let input = '';\\n-    let timeoutId: NodeJS.Timeout | undefined;\\n-    let countdownInterval: NodeJS.Timeout | undefined;\\n-    let remainingMs = options.timeout;\\n+    const dbg = process.env.VISOR_DEBUG === 'true';\\n+    try {\\n+      if (dbg) {\\n+        const counts: Record<string, number> = {\\n+          data: process.stdin.listenerCount('data'),\\n+          end: process.stdin.listenerCount('end'),\\n+          error: process.stdin.listenerCount('error'),\\n+          readable: process.stdin.listenerCount('readable'),\\n+          close: process.stdin.listenerCount('close'),\\n+        } as any;\\n+        console.error(\\n+          `[human-input] starting prompt: isTTY=${!!process.stdin.isTTY} active=${activePrompt} waiters=${waiters.length} listeners=${JSON.stringify(counts)}`\\n+        );\\n+      }\\n+    } catch {}\\n+    // Ensure stdin is in a sane state for a fresh interactive session\\n+    try {\\n+      if (process.stdin.isTTY && typeof (process.stdin as any).setRawMode === 'function') {\\n+        // We use line-based input; disable raw mode just in case\\n+        (process.stdin as any).setRawMode(false);\\n+      }\\n+      // Always resume stdin before creating the interface\\n+      process.stdin.resume();\\n+    } catch {}\\n \\n-    const rl = readline.createInterface({\\n-      input: process.stdin,\\n-      output: process.stdout,\\n-      terminal: true,\\n-    });\\n+    // Ensure encoding is set for predictable behavior\\n+    try {\\n+      process.stdin.setEncoding('utf8');\\n+    } catch {}\\n+\\n+    let rl: readline.Interface | undefined;\\n \\n-    // Display initial UI\\n-    displayPromptUI(options, remainingMs);\\n+    const allowEmpty = options.allowEmpty ?? false;\\n+    const multiline = options.multiline ?? false;\\n+    const defaultValue = options.defaultValue;\\n \\n+    let timeoutId: NodeJS.Timeout | undefined;\\n     const cleanup = () => {\\n       if (timeoutId) clearTimeout(timeoutId);\\n-      if (countdownInterval) clearInterval(countdownInterval);\\n-      rl.close();\\n+      try {\\n+        rl?.removeAllListeners();\\n+      } catch {}\\n+      try {\\n+        rl?.close();\\n+      } catch {}\\n+      // Hardening: make sure no stray listeners remain on stdin between loops\\n+      // Do not blanket-remove listeners from process.stdin; a fresh readline\\n+      // instance will manage its own listeners. Over-removing here can leave\\n+      // the next interface in a bad state (no keypress events).\\n+      try {\\n+        if (process.stdin.isTTY && typeof (process.stdin as any).setRawMode === 'function') {\\n+          (process.stdin as any).setRawMode(false);\\n+        }\\n+      } catch {}\\n+      try {\\n+        process.stdin.pause();\\n+      } catch {}\\n+      // Release the global lock so a queued prompt (if any) may proceed\\n+      try {\\n+        releasePromptLock();\\n+      } catch {}\\n+      // If stdout/stderr were temporarily wrapped by the question handler, restore them now\\n+      try {\\n+        if ((process.stdout as any).__restoreWrites) {\\n+          (process.stdout as any).__restoreWrites();\\n+        }\\n+      } catch {}\\n+      try {\\n+        if ((process.stderr as any).__restoreWrites) {\\n+          (process.stderr as any).__restoreWrites();\\n+        }\\n+      } catch {}\\n+      try {\\n+        if (dbg) {\\n+          const counts: Record<string, number> = {\\n+            data: process.stdin.listenerCount('data'),\\n+            end: process.stdin.listenerCount('end'),\\n+            error: process.stdin.listenerCount('error'),\\n+            readable: process.stdin.listenerCount('readable'),\\n+            close: process.stdin.listenerCount('close'),\\n+          } as any;\\n+          console.error(\\n+            `[human-input] cleanup: isTTY=${!!process.stdin.isTTY} active=false waiters=${waiters.length} listeners=${JSON.stringify(counts)}`\\n+          );\\n+        }\\n+      } catch {}\\n     };\\n-\\n     const finish = (value: string) => {\\n       cleanup();\\n-      console.log(''); // New line after input\\n       resolve(value);\\n     };\\n \\n-    // Setup timeout if specified\\n-    if (options.timeout) {\\n+    // Optional timeout (no default)\\n+    if (options.timeout && options.timeout > 0) {\\n       timeoutId = setTimeout(() => {\\n         cleanup();\\n-        console.log(`\\\\n${colors.yellow}‚è±  Timeout reached${colors.reset}`);\\n-        if (options.defaultValue !== undefined) {\\n-          console.log(\\n-            `${colors.gray}Using default value: ${options.defaultValue}${colors.reset}\\\\n`\\n-          );\\n-          resolve(options.defaultValue);\\n-        } else {\\n-          reject(new Error('Input timeout'));\\n-        }\\n+        if (defaultValue !== undefined) return resolve(defaultValue);\\n+        return reject(new Error('Input timeout'));\\n       }, options.timeout);\\n-\\n-      // Update countdown every second\\n-      if (remainingMs) {\\n-        countdownInterval = setInterval(() => {\\n-          remainingMs = remainingMs! - 1000;\\n-          if (remainingMs <= 0) {\\n-            if (countdownInterval) clearInterval(countdownInterval);\\n-          }\\n-        }, 1000);\\n-      }\\n     }\\n \\n-    if (options.multiline) {\\n-      // Multiline mode: collect lines until EOF (Ctrl+D)\\n+    // Print minimal header with dashed separators\\n+    const header: string[] = [];\\n+    if (options.prompt && options.prompt.trim()) header.push(options.prompt.trim());\\n+    if (multiline) header.push('(Ctrl+D to submit)');\\n+    if (options.placeholder && !multiline) header.push(options.placeholder);\\n+    const width = Math.max(\\n+      20,\\n+      Math.min((process.stdout && (process.stdout as any).columns) || 80, 100)\\n+    );\\n+    const dash = '-'.repeat(width);\\n+    try {\\n+      console.log('\\\\n' + dash);\\n+      if (header.length) console.log(header.join('\\\\n'));\\n+      console.log(dash);\\n+    } catch {}\\n+\\n+    // No echo-suppression hacks ‚Äî we fix the root cause below by using raw-mode\\n+    // input for single-line prompts, so the terminal never replays the line.\\n+\\n+    if (multiline) {\\n+      rl = readline.createInterface({\\n+        input: process.stdin,\\n+        output: process.stdout,\\n+        terminal: true,\\n+      });\\n+      let buf = '';\\n+      process.stdout.write('> ');\\n       rl.on('line', line => {\\n-        input += (input ? '\\\\n' : '') + line;\\n+        buf += (buf ? '\\\\n' : '') + line;\\n+        process.stdout.write('> ');\\n       });\\n-\\n       rl.on('close', () => {\\n-        cleanup();\\n-        const trimmed = input.trim();\\n-        if (!trimmed && !options.allowEmpty) {\\n-          console.log(`${colors.yellow}‚ö†  Empty input not allowed${colors.reset}`);\\n-          reject(new Error('Empty input not allowed'));\\n-        } else {\\n-          finish(trimmed);\\n+        const trimmed = buf.trim();\\n+        if (!trimmed && !allowEmpty && defaultValue === undefined) {\\n+          return reject(new Error('Empty input not allowed'));\\n         }\\n+        return finish(trimmed || defaultValue || '');\\n+      });\\n+      rl.on('SIGINT', () => {\\n+        try {\\n+          // Print a clean newline and exit immediately with 130 (SIGINT)\\n+          process.stdout.write('\\\\n');\\n+        } catch {}\\n+        cleanup();\\n+        process.exit(130);\\n       });\\n     } else {\\n-      // Single line mode\\n-      rl.question('', answer => {\\n-        const trimmed = answer.trim();\\n-        if (!trimmed && !options.allowEmpty && !options.defaultValue) {\\n+      // Root cause fix: raw-mode single-line input without readline echo.\\n+      const readLineRaw = async (): Promise<string> => {\\n+        return new Promise<string>(resolveRaw => {\\n+          let buf = '';\\n+          const onData = (chunk: Buffer) => {\\n+            const s = chunk.toString('utf8');\\n+            for (let i = 0; i < s.length; i++) {\\n+              const ch = s[i];\\n+              const code = s.charCodeAt(i);\\n+              if (ch === '\\\\n' || ch === '\\\\r') {\\n+                try {\\n+                  process.stdout.write('\\\\n');\\n+                } catch {}\\n+                teardown();\\n+                resolveRaw(buf);\\n+                return;\\n+              }\\n+              if (ch === '\\\\b' || code === 127) {\\n+                if (buf.length > 0) {\\n+                  buf = buf.slice(0, -1);\\n+                  try {\\n+                    process.stdout.write('\\\\b \\\\b');\\n+                  } catch {}\\n+                }\\n+                continue;\\n+              }\\n+              if (code === 3) {\\n+                // Ctrl+C\\n+                try {\\n+                  process.stdout.write('\\\\n');\\n+                } catch {}\\n+                teardown();\\n+                process.exit(130);\\n+              }\\n+              if (code >= 32) {\\n+                buf += ch;\\n+                try {\\n+                  process.stdout.write(ch);\\n+                } catch {}\\n+              }\\n+            }\\n+          };\\n+          const teardown = () => {\\n+            try {\\n+              process.stdin.off('data', onData);\\n+            } catch {}\\n+            try {\\n+              if (process.stdin.isTTY && typeof (process.stdin as any).setRawMode === 'function') {\\n+                (process.stdin as any).setRawMode(false);\\n+              }\\n+            } catch {}\\n+          };\\n+          try {\\n+            if (process.stdin.isTTY && typeof (process.stdin as any).setRawMode === 'function') {\\n+              (process.stdin as any).setRawMode(true);\\n+            }\\n+          } catch {}\\n+          process.stdin.on('data', onData);\\n+          try {\\n+            process.stdout.write('> ');\\n+          } catch {}\\n+        });\\n+      };\\n+      (async () => {\\n+        const answer = await readLineRaw();\\n+        const trimmed = (answer || '').trim();\\n+        if (!trimmed && !allowEmpty && defaultValue === undefined) {\\n           cleanup();\\n-          console.log(`${colors.yellow}‚ö†  Empty input not allowed${colors.reset}`);\\n-          reject(new Error('Empty input not allowed'));\\n-        } else {\\n-          finish(trimmed || options.defaultValue || '');\\n+          return reject(new Error('Empty input not allowed'));\\n         }\\n+        return finish(trimmed || defaultValue || '');\\n+      })().catch(err => {\\n+        cleanup();\\n+        reject(err);\\n       });\\n     }\\n-\\n-    // Handle Ctrl+C\\n-    rl.on('SIGINT', () => {\\n-      cleanup();\\n-      console.log('\\\\n\\\\n' + colors.yellow + '‚ö†  Cancelled by user' + colors.reset);\\n-      reject(new Error('Cancelled by user'));\\n-    });\\n   });\\n }\\n \\n@@ -277,6 +284,14 @@ export async function simplePrompt(prompt: string): Promise<string> {\\n       output: process.stdout,\\n     });\\n \\n+    rl.on('SIGINT', () => {\\n+      try {\\n+        process.stdout.write('\\\\n');\\n+      } catch {}\\n+      rl.close();\\n+      process.exit(130);\\n+    });\\n+\\n     rl.question(`${prompt}\\\\n> `, answer => {\\n       rl.close();\\n       resolve(answer.trim());\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/sandbox.ts\",\"additions\":1,\"deletions\":1,\"changes\":44,\"patch\":\"diff --git a/src/utils/sandbox.ts b/src/utils/sandbox.ts\\nindex d2010fb1..9914a3a5 100644\\n--- a/src/utils/sandbox.ts\\n+++ b/src/utils/sandbox.ts\\n@@ -24,12 +24,24 @@ export function createSecureSandbox(): Sandbox {\\n     ...Sandbox.SAFE_GLOBALS,\\n     Math,\\n     JSON,\\n-    // Provide console with limited surface. Calls are harmless in CI logs and\\n-    // help with debugging value_js / transform_js expressions.\\n+    // Provide console with limited surface. Use trampolines so that any test\\n+    // spies (e.g., jest.spyOn(console, 'log')) see calls made inside the sandbox.\\n     console: {\\n-      log: console.log,\\n-      warn: console.warn,\\n-      error: console.error,\\n+      log: (...args: unknown[]) => {\\n+        try {\\n+          (console as any).log(...args);\\n+        } catch {}\\n+      },\\n+      warn: (...args: unknown[]) => {\\n+        try {\\n+          (console as any).warn(...args);\\n+        } catch {}\\n+      },\\n+      error: (...args: unknown[]) => {\\n+        try {\\n+          (console as any).error(...args);\\n+        } catch {}\\n+      },\\n     },\\n   } as Record<string, unknown>;\\n \\n@@ -185,9 +197,27 @@ export function compileAndRun<T = unknown>(\\n   const header = inject\\n     ? `const __lp = ${JSON.stringify(safePrefix)}; const log = (...a) => { try { console.log(__lp, ...a); } catch {} };\\\\n`\\n     : '';\\n+  // When wrapping, execute user code inside an IIFE and return its value.\\n+  // This reliably captures the value of the last expression or any explicit\\n+  // return statements inside the script, without requiring the caller to\\n+  // manually `return` at top level.\\n+  // Wrapper heuristic:\\n+  // - If the snippet contains an explicit `return`, semicolons or newlines (likely a block),\\n+  //   run it inside an IIFE so `return` works:  (() => { code })()\\n+  // - Otherwise treat it as a pure expression and return its value directly.\\n+  const src = String(userCode);\\n+  const looksLikeBlock = /\\\\breturn\\\\b/.test(src) || /;/.test(src) || /\\\\n/.test(src);\\n+  // Heuristic: if the snippet itself looks like an IIFE/callable expression\\n+  // (e.g., `(() => { ... })()` or `(function(){ ... })()`), return its value\\n+  // directly to avoid swallowing the result by nesting it inside another block.\\n+  const looksLikeIife = /\\\\)\\\\s*\\\\(\\\\s*\\\\)\\\\s*;?$/.test(src.trim());\\n   const body = opts.wrapFunction\\n-    ? `const __fn = () => {\\\\n${userCode}\\\\n};\\\\nreturn __fn();\\\\n`\\n-    : `${userCode}`;\\n+    ? looksLikeBlock\\n+      ? looksLikeIife\\n+        ? `return (\\\\n${src}\\\\n);\\\\n`\\n+        : `return (() => {\\\\n${src}\\\\n})();\\\\n`\\n+      : `return (\\\\n${src}\\\\n);\\\\n`\\n+    : `${src}`;\\n   const code = `${header}${body}`;\\n   let exec: ReturnType<typeof sandbox.compile>;\\n   try {\\n\",\"status\":\"modified\"},{\"filename\":\"src/utils/template-context.ts\",\"additions\":1,\"deletions\":1,\"changes\":3,\"patch\":\"diff --git a/src/utils/template-context.ts b/src/utils/template-context.ts\\nindex 9af9baef..17118e36 100644\\n--- a/src/utils/template-context.ts\\n+++ b/src/utils/template-context.ts\\n@@ -70,7 +70,8 @@ export function buildProviderTemplateContext(\\n         const name = checkName.slice(0, -4);\\n         outputsRaw[name] = summary.output !== undefined ? summary.output : summary;\\n       } else {\\n-        outputs[checkName] = summary.output !== undefined ? summary.output : summary;\\n+        const extracted = summary.output !== undefined ? summary.output : summary;\\n+        outputs[checkName] = extracted;\\n       }\\n     }\\n   }\\n\",\"status\":\"modified\"},{\"filename\":\"src/webhook-server.ts\",\"additions\":1,\"deletions\":1,\"changes\":8,\"patch\":\"diff --git a/src/webhook-server.ts b/src/webhook-server.ts\\nindex a2c6fb1f..5480d5e0 100644\\n--- a/src/webhook-server.ts\\n+++ b/src/webhook-server.ts\\n@@ -5,7 +5,7 @@ import * as crypto from 'crypto';\\n import { HttpServerConfig, VisorConfig } from './types/config';\\n import { Liquid } from 'liquidjs';\\n import { createExtendedLiquid } from './liquid-extensions';\\n-import { CheckExecutionEngine } from './check-execution-engine';\\n+import { StateMachineExecutionEngine } from './state-machine-execution-engine';\\n \\n export interface WebhookPayload {\\n   endpoint: string;\\n@@ -26,7 +26,7 @@ export class WebhookServer {\\n   private config: HttpServerConfig;\\n   private liquid: Liquid;\\n   private webhookData: Map<string, unknown> = new Map();\\n-  private executionEngine?: CheckExecutionEngine;\\n+  private executionEngine?: StateMachineExecutionEngine;\\n   private visorConfig?: VisorConfig;\\n   private isGitHubActions: boolean;\\n \\n@@ -42,7 +42,7 @@ export class WebhookServer {\\n   /**\\n    * Set the execution engine for triggering checks on webhook receipt\\n    */\\n-  public setExecutionEngine(engine: CheckExecutionEngine): void {\\n+  public setExecutionEngine(engine: StateMachineExecutionEngine): void {\\n     this.executionEngine = engine;\\n   }\\n \\n@@ -508,7 +508,7 @@ export class WebhookServer {\\n export function createWebhookServer(\\n   config: HttpServerConfig,\\n   visorConfig?: VisorConfig,\\n-  executionEngine?: CheckExecutionEngine\\n+  executionEngine?: StateMachineExecutionEngine\\n ): WebhookServer {\\n   const server = new WebhookServer(config, visorConfig);\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/workflow-executor.ts\",\"additions\":13,\"deletions\":0,\"changes\":488,\"patch\":\"diff --git a/src/workflow-executor.ts b/src/workflow-executor.ts\\nnew file mode 100644\\nindex 00000000..751e7f16\\n--- /dev/null\\n+++ b/src/workflow-executor.ts\\n@@ -0,0 +1,488 @@\\n+/**\\n+ * Workflow executor for running workflow definitions\\n+ */\\n+\\n+import {\\n+  WorkflowDefinition,\\n+  WorkflowExecutionContext,\\n+  WorkflowStep,\\n+  WorkflowInputMapping,\\n+  WorkflowExecutionOptions,\\n+} from './types/workflow';\\n+import { PRInfo } from './pr-analyzer';\\n+import { ReviewSummary } from './reviewer';\\n+import { CheckProviderRegistry } from './providers/check-provider-registry';\\n+import { CheckProviderConfig, ExecutionContext } from './providers/check-provider.interface';\\n+import { DependencyResolver } from './dependency-resolver';\\n+import { logger } from './logger';\\n+import { createSecureSandbox, compileAndRun } from './utils/sandbox';\\n+import { Liquid } from 'liquidjs';\\n+\\n+/**\\n+ * Workflow execution result\\n+ */\\n+export interface WorkflowExecutionResult {\\n+  success: boolean;\\n+  score?: number;\\n+  confidence?: 'high' | 'medium' | 'low';\\n+  issues?: any[];\\n+  comments?: any[];\\n+  output?: Record<string, unknown>;\\n+  status: 'completed' | 'failed' | 'skipped';\\n+  duration?: number;\\n+  error?: string;\\n+  stepSummaries?: Array<{\\n+    stepId: string;\\n+    status: 'success' | 'failed' | 'skipped';\\n+    issues?: any[];\\n+    output?: unknown;\\n+  }>;\\n+}\\n+\\n+/**\\n+ * Execution options passed to workflow executor\\n+ */\\n+interface WorkflowRunOptions {\\n+  prInfo: PRInfo;\\n+  dependencyResults?: Map<string, ReviewSummary>;\\n+  context?: ExecutionContext;\\n+  options?: WorkflowExecutionOptions;\\n+}\\n+\\n+/**\\n+ * Executes workflow definitions\\n+ */\\n+export class WorkflowExecutor {\\n+  private providerRegistry: CheckProviderRegistry | null = null;\\n+  private liquid: Liquid;\\n+\\n+  constructor() {\\n+    // Don't call CheckProviderRegistry.getInstance() here to avoid circular dependency\\n+    // during registry initialization (since WorkflowCheckProvider is registered in the registry)\\n+    this.liquid = new Liquid();\\n+  }\\n+\\n+  /**\\n+   * Lazy-load the provider registry to avoid circular dependency during initialization\\n+   */\\n+  private getProviderRegistry(): CheckProviderRegistry {\\n+    if (!this.providerRegistry) {\\n+      this.providerRegistry = CheckProviderRegistry.getInstance();\\n+    }\\n+    return this.providerRegistry;\\n+  }\\n+\\n+  /**\\n+   * Execute a workflow\\n+   */\\n+  public async execute(\\n+    workflow: WorkflowDefinition,\\n+    executionContext: WorkflowExecutionContext,\\n+    runOptions: WorkflowRunOptions\\n+  ): Promise<WorkflowExecutionResult> {\\n+    const startTime = Date.now();\\n+    executionContext.metadata = {\\n+      startTime,\\n+      status: 'running',\\n+    };\\n+\\n+    try {\\n+      // Resolve step execution order\\n+      const executionOrder = this.resolveExecutionOrder(workflow);\\n+      logger.debug(`Workflow ${workflow.id} execution order: ${executionOrder.join(' -> ')}`);\\n+\\n+      // Execute steps in order\\n+      const stepResults = new Map<string, ReviewSummary>();\\n+      const stepSummaries: Array<{\\n+        stepId: string;\\n+        status: 'success' | 'failed' | 'skipped';\\n+        issues?: any[];\\n+        output?: unknown;\\n+      }> = [];\\n+\\n+      for (const stepId of executionOrder) {\\n+        const step = workflow.steps[stepId];\\n+\\n+        // Check if step should be executed (evaluate 'if' condition)\\n+        if (step.if) {\\n+          const shouldRun = this.evaluateCondition(step.if, {\\n+            inputs: executionContext.inputs,\\n+            outputs: Object.fromEntries(stepResults),\\n+            pr: runOptions.prInfo,\\n+          });\\n+\\n+          if (!shouldRun) {\\n+            logger.info(`Skipping step '${stepId}' due to condition: ${step.if}`);\\n+            stepSummaries.push({\\n+              stepId,\\n+              status: 'skipped',\\n+            });\\n+            continue;\\n+          }\\n+        }\\n+\\n+        // Prepare step configuration\\n+        const stepConfig = await this.prepareStepConfig(\\n+          step,\\n+          stepId,\\n+          executionContext,\\n+          stepResults,\\n+          workflow\\n+        );\\n+\\n+        // Execute the step\\n+        try {\\n+          logger.info(`Executing workflow step '${stepId}'`);\\n+          // Extend context with workflow inputs\\n+          const stepContext: ExecutionContext = {\\n+            ...runOptions.context,\\n+            workflowInputs: executionContext.inputs,\\n+          };\\n+          const result = await this.executeStep(\\n+            stepConfig,\\n+            runOptions.prInfo,\\n+            stepResults,\\n+            stepContext\\n+          );\\n+\\n+          stepResults.set(stepId, result);\\n+          stepSummaries.push({\\n+            stepId,\\n+            status: 'success',\\n+            issues: result.issues,\\n+            output: (result as any).output,\\n+          });\\n+        } catch (error) {\\n+          const errorMessage = error instanceof Error ? error.message : String(error);\\n+          logger.error(`Step '${stepId}' failed: ${errorMessage}`);\\n+\\n+          stepSummaries.push({\\n+            stepId,\\n+            status: 'failed',\\n+            output: { error: errorMessage },\\n+          });\\n+\\n+          if (!runOptions.options?.continueOnError) {\\n+            throw new Error(`Workflow step '${stepId}' failed: ${errorMessage}`);\\n+          }\\n+        }\\n+      }\\n+\\n+      // Compute workflow outputs\\n+      const outputs = await this.computeOutputs(\\n+        workflow,\\n+        executionContext,\\n+        stepResults,\\n+        runOptions.prInfo\\n+      );\\n+      executionContext.outputs = outputs;\\n+\\n+      // Aggregate results\\n+      const aggregated = this.aggregateResults(stepResults);\\n+\\n+      const endTime = Date.now();\\n+      executionContext.metadata.endTime = endTime;\\n+      executionContext.metadata.duration = endTime - startTime;\\n+      executionContext.metadata.status = 'completed';\\n+\\n+      return {\\n+        success: true,\\n+        score: aggregated.score,\\n+        confidence: aggregated.confidence,\\n+        issues: aggregated.issues,\\n+        comments: aggregated.comments,\\n+        output: outputs,\\n+        status: 'completed',\\n+        duration: endTime - startTime,\\n+        stepSummaries,\\n+      };\\n+    } catch (error) {\\n+      const endTime = Date.now();\\n+      executionContext.metadata.endTime = endTime;\\n+      executionContext.metadata.duration = endTime - startTime;\\n+      executionContext.metadata.status = 'failed';\\n+      executionContext.metadata.error = error instanceof Error ? error.message : String(error);\\n+\\n+      return {\\n+        success: false,\\n+        status: 'failed',\\n+        duration: endTime - startTime,\\n+        error: error instanceof Error ? error.message : String(error),\\n+      };\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Resolve step execution order based on dependencies\\n+   */\\n+  private resolveExecutionOrder(workflow: WorkflowDefinition): string[] {\\n+    // Build dependency map\\n+    const dependencies: Record<string, string[]> = {};\\n+    for (const [stepId, step] of Object.entries(workflow.steps)) {\\n+      dependencies[stepId] = step.depends_on || [];\\n+    }\\n+\\n+    // Use static DependencyResolver\\n+    const graph = DependencyResolver.buildDependencyGraph(dependencies);\\n+\\n+    if (graph.hasCycles) {\\n+      throw new Error(\\n+        `Circular dependency detected in workflow steps: ${graph.cycleNodes?.join(' -> ')}`\\n+      );\\n+    }\\n+\\n+    // Flatten execution groups to get linear order\\n+    const order: string[] = [];\\n+    for (const group of graph.executionOrder) {\\n+      order.push(...group.parallel);\\n+    }\\n+\\n+    return order;\\n+  }\\n+\\n+  /**\\n+   * Prepare step configuration with input mappings\\n+   */\\n+  private async prepareStepConfig(\\n+    step: WorkflowStep,\\n+    stepId: string,\\n+    executionContext: WorkflowExecutionContext,\\n+    stepResults: Map<string, ReviewSummary>,\\n+    workflow: WorkflowDefinition\\n+  ): Promise<CheckProviderConfig> {\\n+    const config: CheckProviderConfig = {\\n+      ...step,\\n+      type: step.type || 'ai',\\n+      checkName: `${executionContext.instanceId}:${stepId}`,\\n+    };\\n+\\n+    // Process input mappings\\n+    if (step.inputs) {\\n+      for (const [inputName, mapping] of Object.entries(step.inputs)) {\\n+        const value = await this.resolveInputMapping(\\n+          mapping,\\n+          executionContext,\\n+          stepResults,\\n+          workflow\\n+        );\\n+        (config as any)[inputName] = value;\\n+      }\\n+    }\\n+\\n+    return config;\\n+  }\\n+\\n+  /**\\n+   * Resolve input mapping to actual value\\n+   */\\n+  private async resolveInputMapping(\\n+    mapping: string | WorkflowInputMapping,\\n+    executionContext: WorkflowExecutionContext,\\n+    stepResults: Map<string, ReviewSummary>,\\n+    _workflow: WorkflowDefinition\\n+  ): Promise<unknown> {\\n+    // Simple string mapping - treat as parameter reference\\n+    if (typeof mapping === 'string') {\\n+      return executionContext.inputs[mapping];\\n+    }\\n+\\n+    // Complex mapping\\n+    if (typeof mapping === 'object' && mapping !== null && 'source' in mapping) {\\n+      const typedMapping = mapping as WorkflowInputMapping;\\n+\\n+      switch (typedMapping.source) {\\n+        case 'param':\\n+          // Reference to workflow input parameter\\n+          return executionContext.inputs[String(typedMapping.value)];\\n+\\n+        case 'step':\\n+          // Reference to another step's output\\n+          if (!typedMapping.stepId) {\\n+            throw new Error('Step input mapping requires stepId');\\n+          }\\n+          const stepResult = stepResults.get(typedMapping.stepId);\\n+          if (!stepResult) {\\n+            throw new Error(`Step '${typedMapping.stepId}' has not been executed yet`);\\n+          }\\n+          const output = (stepResult as any).output;\\n+          if (typedMapping.outputParam && output) {\\n+            return output[typedMapping.outputParam];\\n+          }\\n+          return output;\\n+\\n+        case 'constant':\\n+          // Constant value\\n+          return typedMapping.value;\\n+\\n+        case 'expression':\\n+          // JavaScript expression\\n+          if (!typedMapping.expression) {\\n+            throw new Error('Expression mapping requires expression field');\\n+          }\\n+          const sandbox = createSecureSandbox();\\n+          return compileAndRun(\\n+            sandbox,\\n+            typedMapping.expression,\\n+            {\\n+              inputs: executionContext.inputs,\\n+              outputs: Object.fromEntries(stepResults),\\n+              steps: Object.fromEntries(\\n+                Array.from(stepResults.entries()).map(([id, result]) => [\\n+                  id,\\n+                  (result as any).output,\\n+                ])\\n+              ),\\n+            },\\n+            { injectLog: true, logPrefix: 'workflow.input.expression' }\\n+          );\\n+\\n+        default:\\n+          throw new Error(`Unknown input mapping source: ${typedMapping.source}`);\\n+      }\\n+    }\\n+\\n+    // Handle Liquid template in mapping\\n+    if (typeof mapping === 'object' && mapping !== null && 'template' in mapping) {\\n+      const typedMapping = mapping as WorkflowInputMapping;\\n+      if (typedMapping.template) {\\n+        return await this.liquid.parseAndRender(typedMapping.template, {\\n+          inputs: executionContext.inputs,\\n+          outputs: Object.fromEntries(stepResults),\\n+        });\\n+      }\\n+    }\\n+\\n+    // Return as-is\\n+    return mapping;\\n+  }\\n+\\n+  /**\\n+   * Execute a single step\\n+   */\\n+  private async executeStep(\\n+    config: CheckProviderConfig,\\n+    prInfo: PRInfo,\\n+    dependencyResults: Map<string, ReviewSummary>,\\n+    context?: ExecutionContext\\n+  ): Promise<ReviewSummary> {\\n+    const provider = await this.getProviderRegistry().getProvider(config.type);\\n+    if (!provider) {\\n+      throw new Error(`Provider '${config.type}' not found`);\\n+    }\\n+\\n+    return await provider.execute(prInfo, config, dependencyResults, context);\\n+  }\\n+\\n+  /**\\n+   * Compute workflow outputs\\n+   */\\n+  private async computeOutputs(\\n+    workflow: WorkflowDefinition,\\n+    executionContext: WorkflowExecutionContext,\\n+    stepResults: Map<string, ReviewSummary>,\\n+    prInfo: PRInfo\\n+  ): Promise<Record<string, unknown>> {\\n+    const outputs: Record<string, unknown> = {};\\n+\\n+    if (!workflow.outputs) {\\n+      return outputs;\\n+    }\\n+\\n+    for (const output of workflow.outputs) {\\n+      if (output.value_js) {\\n+        // JavaScript expression\\n+        const sandbox = createSecureSandbox();\\n+        outputs[output.name] = compileAndRun(\\n+          sandbox,\\n+          output.value_js,\\n+          {\\n+            inputs: executionContext.inputs,\\n+            steps: Object.fromEntries(\\n+              Array.from(stepResults.entries()).map(([id, result]) => [id, (result as any).output])\\n+            ),\\n+            outputs: Object.fromEntries(stepResults),\\n+            pr: prInfo,\\n+          },\\n+          { injectLog: true, logPrefix: `workflow.output.${output.name}` }\\n+        );\\n+      } else if (output.value) {\\n+        // Liquid template\\n+        outputs[output.name] = await this.liquid.parseAndRender(output.value, {\\n+          inputs: executionContext.inputs,\\n+          steps: Object.fromEntries(\\n+            Array.from(stepResults.entries()).map(([id, result]) => [id, (result as any).output])\\n+          ),\\n+          outputs: Object.fromEntries(stepResults),\\n+          pr: prInfo,\\n+        });\\n+      }\\n+    }\\n+\\n+    return outputs;\\n+  }\\n+\\n+  /**\\n+   * Aggregate results from all steps\\n+   */\\n+  private aggregateResults(stepResults: Map<string, ReviewSummary>): {\\n+    score: number;\\n+    confidence: 'high' | 'medium' | 'low';\\n+    issues: any[];\\n+    comments: any[];\\n+  } {\\n+    let totalScore = 0;\\n+    let scoreCount = 0;\\n+    const allIssues: any[] = [];\\n+    const allComments: any[] = [];\\n+    let minConfidence: 'high' | 'medium' | 'low' = 'high';\\n+\\n+    for (const result of stepResults.values()) {\\n+      const extResult = result as any;\\n+      if (typeof extResult.score === 'number') {\\n+        totalScore += extResult.score;\\n+        scoreCount++;\\n+      }\\n+\\n+      if (result.issues) {\\n+        allIssues.push(...result.issues);\\n+      }\\n+\\n+      if (extResult.comments) {\\n+        allComments.push(...extResult.comments);\\n+      }\\n+\\n+      if (extResult.confidence) {\\n+        if (\\n+          extResult.confidence === 'low' ||\\n+          (extResult.confidence === 'medium' && minConfidence === 'high')\\n+        ) {\\n+          minConfidence = extResult.confidence;\\n+        }\\n+      }\\n+    }\\n+\\n+    return {\\n+      score: scoreCount > 0 ? Math.round(totalScore / scoreCount) : 0,\\n+      confidence: minConfidence,\\n+      issues: allIssues,\\n+      comments: allComments,\\n+    };\\n+  }\\n+\\n+  /**\\n+   * Evaluate a condition expression\\n+   */\\n+  private evaluateCondition(condition: string, context: any): boolean {\\n+    try {\\n+      const sandbox = createSecureSandbox();\\n+      const result = compileAndRun(sandbox, condition, context, {\\n+        injectLog: true,\\n+        logPrefix: 'workflow.condition',\\n+      });\\n+      return Boolean(result);\\n+    } catch (error) {\\n+      logger.warn(`Failed to evaluate condition '${condition}': ${error}`);\\n+      return false;\\n+    }\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/workflow-registry.ts\",\"additions\":13,\"deletions\":0,\"changes\":454,\"patch\":\"diff --git a/src/workflow-registry.ts b/src/workflow-registry.ts\\nnew file mode 100644\\nindex 00000000..2cbe3a89\\n--- /dev/null\\n+++ b/src/workflow-registry.ts\\n@@ -0,0 +1,454 @@\\n+/**\\n+ * Workflow registry for managing reusable workflow definitions\\n+ */\\n+\\n+import {\\n+  WorkflowDefinition,\\n+  WorkflowRegistryEntry,\\n+  WorkflowValidationResult,\\n+  WorkflowImportOptions,\\n+  JsonSchema,\\n+} from './types/workflow';\\n+import { promises as fs } from 'fs';\\n+import * as path from 'path';\\n+import * as yaml from 'js-yaml';\\n+import { logger } from './logger';\\n+import { DependencyResolver } from './dependency-resolver';\\n+import Ajv from 'ajv';\\n+import addFormats from 'ajv-formats';\\n+\\n+/**\\n+ * Registry for managing workflow definitions\\n+ */\\n+export class WorkflowRegistry {\\n+  private static instance: WorkflowRegistry;\\n+  private workflows: Map<string, WorkflowRegistryEntry> = new Map();\\n+  private ajv: Ajv;\\n+\\n+  private constructor() {\\n+    this.ajv = new Ajv({ allErrors: true, strict: false });\\n+    addFormats(this.ajv);\\n+  }\\n+\\n+  /**\\n+   * Get the singleton instance of the workflow registry\\n+   */\\n+  public static getInstance(): WorkflowRegistry {\\n+    if (!WorkflowRegistry.instance) {\\n+      WorkflowRegistry.instance = new WorkflowRegistry();\\n+    }\\n+    return WorkflowRegistry.instance;\\n+  }\\n+\\n+  /**\\n+   * Register a workflow definition\\n+   */\\n+  public register(\\n+    workflow: WorkflowDefinition,\\n+    source: string = 'inline',\\n+    options?: { override?: boolean }\\n+  ): WorkflowValidationResult {\\n+    // Validate the workflow\\n+    const validation = this.validateWorkflow(workflow);\\n+    if (!validation.valid) {\\n+      return validation;\\n+    }\\n+\\n+    // Check if workflow already exists\\n+    if (this.workflows.has(workflow.id) && !options?.override) {\\n+      return {\\n+        valid: false,\\n+        errors: [\\n+          {\\n+            path: 'id',\\n+            message: `Workflow with ID '${workflow.id}' already exists`,\\n+            value: workflow.id,\\n+          },\\n+        ],\\n+      };\\n+    }\\n+\\n+    // Register the workflow\\n+    this.workflows.set(workflow.id, {\\n+      definition: workflow,\\n+      source,\\n+      registeredAt: new Date(),\\n+      usage: {\\n+        count: 0,\\n+      },\\n+    });\\n+\\n+    logger.debug(`Registered workflow '${workflow.id}' from ${source}`);\\n+    return { valid: true };\\n+  }\\n+\\n+  /**\\n+   * Get a workflow by ID\\n+   */\\n+  public get(id: string): WorkflowDefinition | undefined {\\n+    const entry = this.workflows.get(id);\\n+    if (entry) {\\n+      // Update usage statistics\\n+      entry.usage = entry.usage || { count: 0 };\\n+      entry.usage.count++;\\n+      entry.usage.lastUsed = new Date();\\n+    }\\n+    return entry?.definition;\\n+  }\\n+\\n+  /**\\n+   * Check if a workflow exists\\n+   */\\n+  public has(id: string): boolean {\\n+    return this.workflows.has(id);\\n+  }\\n+\\n+  /**\\n+   * List all registered workflows\\n+   */\\n+  public list(): WorkflowDefinition[] {\\n+    return Array.from(this.workflows.values()).map(entry => entry.definition);\\n+  }\\n+\\n+  /**\\n+   * Get workflow metadata\\n+   */\\n+  public getMetadata(id: string): WorkflowRegistryEntry | undefined {\\n+    return this.workflows.get(id);\\n+  }\\n+\\n+  /**\\n+   * Remove a workflow from the registry\\n+   */\\n+  public unregister(id: string): boolean {\\n+    return this.workflows.delete(id);\\n+  }\\n+\\n+  /**\\n+   * Clear all workflows\\n+   */\\n+  public clear(): void {\\n+    this.workflows.clear();\\n+  }\\n+\\n+  /**\\n+   * Import workflows from a file or URL\\n+   */\\n+  public async import(\\n+    source: string,\\n+    options?: WorkflowImportOptions\\n+  ): Promise<WorkflowValidationResult[]> {\\n+    const results: WorkflowValidationResult[] = [];\\n+\\n+    try {\\n+      // Load the workflow file\\n+      const content = await this.loadWorkflowContent(source, options?.basePath);\\n+      const data = this.parseWorkflowContent(content, source);\\n+\\n+      // Handle both single workflow and multiple workflows\\n+      const workflows: WorkflowDefinition[] = Array.isArray(data) ? data : [data];\\n+\\n+      for (const workflow of workflows) {\\n+        // Validate if requested\\n+        if (options?.validate !== false) {\\n+          const validation = this.validateWorkflow(workflow);\\n+          if (!validation.valid) {\\n+            results.push(validation);\\n+            continue;\\n+          }\\n+\\n+          // Run custom validators if provided\\n+          if (options?.validators) {\\n+            for (const validator of options.validators) {\\n+              const customValidation = validator(workflow);\\n+              if (!customValidation.valid) {\\n+                results.push(customValidation);\\n+                continue;\\n+              }\\n+            }\\n+          }\\n+        }\\n+\\n+        // Strip out 'tests' field before registering - tests are only for standalone execution\\n+        const workflowWithoutTests = { ...workflow };\\n+        delete (workflowWithoutTests as any).tests;\\n+\\n+        // Register the workflow (without tests)\\n+        const result = this.register(workflowWithoutTests, source, { override: options?.override });\\n+        results.push(result);\\n+      }\\n+    } catch (error) {\\n+      results.push({\\n+        valid: false,\\n+        errors: [\\n+          {\\n+            path: 'source',\\n+            message: `Failed to import workflows from '${source}': ${error instanceof Error ? error.message : String(error)}`,\\n+            value: source,\\n+          },\\n+        ],\\n+      });\\n+    }\\n+\\n+    return results;\\n+  }\\n+\\n+  /**\\n+   * Import multiple workflow sources\\n+   */\\n+  public async importMany(\\n+    sources: string[],\\n+    options?: WorkflowImportOptions\\n+  ): Promise<Map<string, WorkflowValidationResult[]>> {\\n+    const results = new Map<string, WorkflowValidationResult[]>();\\n+\\n+    for (const source of sources) {\\n+      const importResults = await this.import(source, options);\\n+      results.set(source, importResults);\\n+    }\\n+\\n+    return results;\\n+  }\\n+\\n+  /**\\n+   * Validate a workflow definition\\n+   */\\n+  public validateWorkflow(workflow: WorkflowDefinition): WorkflowValidationResult {\\n+    const errors: Array<{ path: string; message: string; value?: unknown }> = [];\\n+    const warnings: Array<{ path: string; message: string }> = [];\\n+\\n+    // Validate required fields\\n+    if (!workflow.id) {\\n+      errors.push({ path: 'id', message: 'Workflow ID is required' });\\n+    }\\n+\\n+    if (!workflow.name) {\\n+      errors.push({ path: 'name', message: 'Workflow name is required' });\\n+    }\\n+\\n+    if (!workflow.steps || Object.keys(workflow.steps).length === 0) {\\n+      errors.push({ path: 'steps', message: 'Workflow must have at least one step' });\\n+    }\\n+\\n+    // Validate input parameters\\n+    if (workflow.inputs) {\\n+      for (let i = 0; i < workflow.inputs.length; i++) {\\n+        const input = workflow.inputs[i];\\n+        if (!input.name) {\\n+          errors.push({ path: `inputs[${i}].name`, message: 'Input parameter name is required' });\\n+        }\\n+        if (!input.schema) {\\n+          warnings.push({\\n+            path: `inputs[${i}].schema`,\\n+            message: 'Input parameter schema is recommended',\\n+          });\\n+        }\\n+      }\\n+    }\\n+\\n+    // Validate output parameters\\n+    if (workflow.outputs) {\\n+      for (let i = 0; i < workflow.outputs.length; i++) {\\n+        const output = workflow.outputs[i];\\n+        if (!output.name) {\\n+          errors.push({ path: `outputs[${i}].name`, message: 'Output parameter name is required' });\\n+        }\\n+        if (!output.value && !output.value_js) {\\n+          errors.push({\\n+            path: `outputs[${i}]`,\\n+            message: 'Output parameter must have either value or value_js',\\n+          });\\n+        }\\n+      }\\n+    }\\n+\\n+    // Validate steps\\n+    for (const [stepId, step] of Object.entries(workflow.steps || {})) {\\n+      // Validate step dependencies\\n+      if (step.depends_on) {\\n+        for (const dep of step.depends_on) {\\n+          if (!workflow.steps[dep]) {\\n+            errors.push({\\n+              path: `steps.${stepId}.depends_on`,\\n+              message: `Step '${stepId}' depends on non-existent step '${dep}'`,\\n+              value: dep,\\n+            });\\n+          }\\n+        }\\n+      }\\n+\\n+      // Validate input mappings\\n+      if (step.inputs) {\\n+        for (const [inputName, mapping] of Object.entries(step.inputs)) {\\n+          if (typeof mapping === 'object' && mapping !== null && 'source' in mapping) {\\n+            const typedMapping = mapping as any;\\n+            if (typedMapping.source === 'step' && !typedMapping.stepId) {\\n+              errors.push({\\n+                path: `steps.${stepId}.inputs.${inputName}`,\\n+                message: 'Step input mapping with source \\\"step\\\" must have stepId',\\n+              });\\n+            }\\n+            if (typedMapping.source === 'param') {\\n+              // Validate that the parameter exists\\n+              const paramExists = workflow.inputs?.some(p => p.name === typedMapping.value);\\n+              if (!paramExists) {\\n+                errors.push({\\n+                  path: `steps.${stepId}.inputs.${inputName}`,\\n+                  message: `Step input references non-existent parameter '${typedMapping.value}'`,\\n+                  value: typedMapping.value,\\n+                });\\n+              }\\n+            }\\n+          }\\n+        }\\n+      }\\n+    }\\n+\\n+    // Check for circular dependencies\\n+    const circularDeps = this.detectCircularDependencies(workflow);\\n+    if (circularDeps.length > 0) {\\n+      errors.push({\\n+        path: 'steps',\\n+        message: `Circular dependencies detected: ${circularDeps.join(' -> ')}`,\\n+      });\\n+    }\\n+\\n+    return {\\n+      valid: errors.length === 0,\\n+      errors: errors.length > 0 ? errors : undefined,\\n+      warnings: warnings.length > 0 ? warnings : undefined,\\n+    };\\n+  }\\n+\\n+  /**\\n+   * Validate input values against workflow input schema\\n+   */\\n+  public validateInputs(\\n+    workflow: WorkflowDefinition,\\n+    inputs: Record<string, unknown>\\n+  ): WorkflowValidationResult {\\n+    const errors: Array<{ path: string; message: string; value?: unknown }> = [];\\n+\\n+    if (!workflow.inputs) {\\n+      return { valid: true };\\n+    }\\n+\\n+    // Check required inputs\\n+    for (const param of workflow.inputs) {\\n+      if (param.required !== false && !(param.name in inputs) && param.default === undefined) {\\n+        errors.push({\\n+          path: `inputs.${param.name}`,\\n+          message: `Required input '${param.name}' is missing`,\\n+        });\\n+      }\\n+    }\\n+\\n+    // Validate input schemas\\n+    for (const param of workflow.inputs) {\\n+      if (param.name in inputs && param.schema) {\\n+        const value = inputs[param.name];\\n+        const valid = this.validateAgainstSchema(value, param.schema);\\n+        if (!valid.valid) {\\n+          errors.push({\\n+            path: `inputs.${param.name}`,\\n+            message: valid.error || 'Invalid input value',\\n+            value,\\n+          });\\n+        }\\n+      }\\n+    }\\n+\\n+    return {\\n+      valid: errors.length === 0,\\n+      errors: errors.length > 0 ? errors : undefined,\\n+    };\\n+  }\\n+\\n+  /**\\n+   * Load workflow content from file or URL\\n+   */\\n+  private async loadWorkflowContent(source: string, basePath?: string): Promise<string> {\\n+    // Handle URLs\\n+    if (source.startsWith('http://') || source.startsWith('https://')) {\\n+      const response = await fetch(source);\\n+      if (!response.ok) {\\n+        throw new Error(`Failed to fetch workflow from ${source}: ${response.statusText}`);\\n+      }\\n+      return await response.text();\\n+    }\\n+\\n+    // Handle file paths\\n+    const filePath = path.isAbsolute(source)\\n+      ? source\\n+      : path.resolve(basePath || process.cwd(), source);\\n+    return await fs.readFile(filePath, 'utf-8');\\n+  }\\n+\\n+  /**\\n+   * Parse workflow content (YAML or JSON)\\n+   */\\n+  private parseWorkflowContent(content: string, source: string): any {\\n+    // Try JSON first\\n+    try {\\n+      return JSON.parse(content);\\n+    } catch {\\n+      // Try YAML\\n+      try {\\n+        return yaml.load(content);\\n+      } catch (error) {\\n+        throw new Error(\\n+          `Failed to parse workflow file ${source}: ${error instanceof Error ? error.message : String(error)}`\\n+        );\\n+      }\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Detect circular dependencies in workflow steps using DependencyResolver\\n+   */\\n+  private detectCircularDependencies(workflow: WorkflowDefinition): string[] {\\n+    // Build dependency map\\n+    const dependencies: Record<string, string[]> = {};\\n+    for (const [stepId, step] of Object.entries(workflow.steps || {})) {\\n+      dependencies[stepId] = step.depends_on || [];\\n+    }\\n+\\n+    try {\\n+      // Use DependencyResolver to check for cycles\\n+      const graph = DependencyResolver.buildDependencyGraph(dependencies);\\n+\\n+      if (graph.hasCycles && graph.cycleNodes) {\\n+        return graph.cycleNodes;\\n+      }\\n+\\n+      return [];\\n+    } catch {\\n+      // DependencyResolver throws error for non-existent dependencies\\n+      // This should be caught by the dependency validation in validateWorkflow\\n+      // Return empty array here and let the validation handle it\\n+      return [];\\n+    }\\n+  }\\n+\\n+  /**\\n+   * Validate a value against a JSON schema\\n+   */\\n+  private validateAgainstSchema(\\n+    value: unknown,\\n+    schema: JsonSchema\\n+  ): { valid: boolean; error?: string } {\\n+    try {\\n+      const validate = this.ajv.compile(schema as any);\\n+      const valid = validate(value);\\n+      if (!valid) {\\n+        const errors = validate.errors\\n+          ?.map(e => `${e.instancePath || '/'}: ${e.message}`)\\n+          .join(', ');\\n+        return { valid: false, error: errors };\\n+      }\\n+      return { valid: true };\\n+    } catch (error) {\\n+      return { valid: false, error: error instanceof Error ? error.message : String(error) };\\n+    }\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"tests/e2e/fact-validation-memory-e2e.test.ts\",\"additions\":1,\"deletions\":3,\"changes\":135,\"patch\":\"diff --git a/tests/e2e/fact-validation-memory-e2e.test.ts b/tests/e2e/fact-validation-memory-e2e.test.ts\\nindex 3ba368b5..9abdce9c 100644\\n--- a/tests/e2e/fact-validation-memory-e2e.test.ts\\n+++ b/tests/e2e/fact-validation-memory-e2e.test.ts\\n@@ -1,119 +1,62 @@\\n import { CheckExecutionEngine } from '../../src/check-execution-engine';\\n import type { VisorConfig } from '../../src/types/config';\\n \\n-describe('Fact Validation Flow (memory, fast e2e)', () => {\\n-  const makeConfig = (retryLimit: number, ns = 'fact-validation'): Partial<VisorConfig> => ({\\n+describe('Fact Validation Flow (history-driven, fast e2e)', () => {\\n+  const makeConfig = (retryWaves: number): Partial<VisorConfig> => ({\\n     version: '1.0',\\n     checks: {\\n-      'init-attempt': {\\n-        type: 'memory',\\n-        operation: 'set',\\n-        namespace: ns,\\n-        key: 'attempt',\\n-        value: 0,\\n-      },\\n-      'init-limit': {\\n-        type: 'memory',\\n-        operation: 'set',\\n-        namespace: ns,\\n-        key: 'retry_limit',\\n-        value: retryLimit,\\n-        depends_on: ['init-attempt'],\\n-      },\\n-      'seed-facts': {\\n-        type: 'memory',\\n-        operation: 'set',\\n-        namespace: ns,\\n-        key: 'facts',\\n-        value_js: 'Array.from({length:6},(_,i)=>({id:`fact-${i+1}`, claim:`claim-${i+1}`}))',\\n-        depends_on: ['init-limit'],\\n-      },\\n-\\n       'extract-facts': {\\n-        type: 'memory',\\n-        operation: 'get',\\n-        namespace: ns,\\n-        key: 'facts',\\n+        type: 'script',\\n         forEach: true,\\n-        depends_on: ['seed-facts'],\\n+        content: `\\n+          // Produce 6 facts\\n+          return Array.from({length:6},(_,i)=>({ id: 'fact-'+(i+1), claim: 'claim-'+(i+1) }));\\n+        `,\\n         on_finish: {\\n-          run: ['comment-assistant', 'aggregate-validations'],\\n           goto_js: `\\n-            const NS = '${ns}';\\n-            const allValid = memory.get('all_valid', NS) === true;\\n-            const limit = Number(memory.get('retry_limit', NS) || 0);\\n-            let attempt = Number(memory.get('attempt', NS) || 0);\\n-            if (!allValid && attempt < limit) {\\n-              memory.increment('attempt', 1, NS);\\n-              return 'extract-facts';\\n-            }\\n-            return null;\\n+            // Re-run until the last wave is all valid, capped by one retry.\\n+            const hist = outputs.history || {};\\n+            const items = (forEach && forEach.last_wave_size) ? forEach.last_wave_size : 1;\\n+            const perAll = (hist['validate-fact'] || []).filter((x) => !Array.isArray(x));\\n+            const waves = items > 0 ? Math.floor(perAll.length / items) : 0;\\n+            const last = items > 0 ? perAll.slice(-items) : [];\\n+            const allOk = last.length === items && last.every(v => v && (v.is_valid === true || v.valid === true));\\n+            log('[goto_js] items=', items, 'perAll=', perAll.length, 'waves=', waves, 'lastOk=', allOk);\\n+            const maxWaves = 1 + ${retryWaves};\\n+            return (!allOk && waves < maxWaves) ? 'extract-facts' : null;\\n           `,\\n         },\\n       },\\n \\n       'validate-fact': {\\n         type: 'script',\\n-        namespace: ns,\\n         depends_on: ['extract-facts'],\\n         fanout: 'map',\\n         content: `\\n-          const NS = '${ns}';\\n-          const attempt = memory.get('attempt', NS) || 0;\\n+          // Determine current wave from history\\n+          const arrs = outputs.history['extract-facts'] || [];\\n+          const lastArr = arrs.filter(Array.isArray).slice(-1)[0] || [];\\n+          const items = lastArr.length || 1;\\n+          const per = (outputs.history['validate-fact'] || []).filter(x => !Array.isArray(x));\\n+          const waves = Math.floor(per.length / items);\\n+\\n+          // Current item\\n           const f = outputs['extract-facts'];\\n           const n = Number((f.id||'').split('-')[1]||'0');\\n+\\n+          // First wave: facts 1..3 are invalid, rest valid. Later waves: all valid.\\n           const invalidOnFirst = (n >= 1 && n <= 3);\\n-          const is_valid = attempt >= 1 ? true : !invalidOnFirst;\\n+          const is_valid = waves >= 1 ? true : !invalidOnFirst;\\n           return { fact_id: f.id, claim: f.claim, is_valid, confidence: 'high', evidence: is_valid ? 'ok' : 'bad' };\\n         `,\\n       },\\n-\\n-      'aggregate-validations': {\\n-        type: 'script',\\n-        namespace: ns,\\n-        content: `\\n-          const NS = '${ns}';\\n-          const nested = outputs.history['validate-fact'] || [];\\n-          const vals = Array.isArray(nested) ? nested.flatMap(x => Array.isArray(x) ? x : [x]) : [];\\n-          const byId = new Map(); for (const v of vals) { if (v && v.fact_id) byId.set(v.fact_id, v); }\\n-          const uniq = Array.from(byId.values());\\n-          const invalid = uniq.filter(v => v && ((v.is_valid === false) || (v.evidence === 'bad')));\\n-          const all_valid = invalid.length === 0;\\n-          memory.set('all_valid', all_valid, NS);\\n-          { const prev = Number(memory.get('total_validations', NS) || 0); memory.set('total_validations', prev + uniq.length, NS); }\\n-          const attempt = memory.get('attempt', NS) || 0;\\n-          return { total: uniq.length, invalid: invalid.length, all_valid, attempt };\\n-        `,\\n-      },\\n-\\n-      'comment-assistant': {\\n-        type: 'script',\\n-        namespace: ns,\\n-        content: `\\n-          const NS = '${ns}';\\n-          const prev = outputs.history['comment-assistant'] || [];\\n-          const allFactsNested = outputs.history['validate-fact'] || [];\\n-          const allFacts = Array.isArray(allFactsNested) ? allFactsNested.flatMap(x => Array.isArray(x) ? x : [x]) : [];\\n-          // Collect unique set of fact_ids that were ever invalid across waves\\n-          const failedIds = new Set();\\n-          for (const f of allFacts) {\\n-            if (f && f.fact_id && (f.is_valid === false || f.evidence === 'bad')) failedIds.add(f.fact_id);\\n-          }\\n-          const failed = Array.from(failedIds);\\n-          memory.set('comment_prev_count', Array.isArray(prev) ? prev.length : 0, NS);\\n-          memory.set('failed_from_history', failed.length, NS);\\n-          return { prev_count: Array.isArray(prev) ? prev.length : 0, failed_total: failed.length };\\n-        `,\\n-      },\\n+      // no aggregator step needed in this variant\\n     },\\n   });\\n \\n-  it('no retry: per-item validations run once (√ó6), attempt=0', async () => {\\n-    // Clean memory between tests\\n-    const { MemoryStore } = require('../../src/memory-store');\\n-    MemoryStore.resetInstance();\\n+  it('no retry: per-item validations run once (√ó6)', async () => {\\n     const engine = new CheckExecutionEngine();\\n-    const cfg = makeConfig(0, 'fact-validation-n0');\\n+    const cfg = makeConfig(0);\\n     const result = await engine.executeChecks({\\n       checks: ['extract-facts', 'validate-fact'],\\n       config: cfg as VisorConfig,\\n@@ -125,16 +68,11 @@ describe('Fact Validation Flow (memory, fast e2e)', () => {\\n     for (const c of (result as any).executionStatistics?.checks || []) byName[c.checkName] = c;\\n     expect(byName['extract-facts'].outputsProduced).toBe(6);\\n     expect(byName['validate-fact'].totalRuns).toBe(6);\\n-    const store = MemoryStore.getInstance();\\n-    expect(store.get('total_validations', 'fact-validation-n0')).toBe(6);\\n-    expect(store.get('attempt', 'fact-validation-n0')).toBe(0);\\n   });\\n \\n-  it('one retry: per-item validations run twice (√ó12), attempt=1', async () => {\\n-    const { MemoryStore } = require('../../src/memory-store');\\n-    MemoryStore.resetInstance();\\n+  it('one retry: per-item validations run twice (√ó12)', async () => {\\n     const engine = new CheckExecutionEngine();\\n-    const cfg = makeConfig(1, 'fact-validation-n1');\\n+    const cfg = makeConfig(1);\\n     const result = await engine.executeChecks({\\n       checks: ['extract-facts', 'validate-fact'],\\n       config: cfg as VisorConfig,\\n@@ -146,12 +84,5 @@ describe('Fact Validation Flow (memory, fast e2e)', () => {\\n     for (const c of (result as any).executionStatistics?.checks || []) byName[c.checkName] = c;\\n     expect(byName['extract-facts'].outputsProduced).toBe(6);\\n     expect(byName['validate-fact'].totalRuns).toBe(12);\\n-    const store = MemoryStore.getInstance();\\n-    expect(store.get('total_validations', 'fact-validation-n1')).toBeGreaterThanOrEqual(6);\\n-    expect(store.get('attempt', 'fact-validation-n1')).toBe(1);\\n-    // Comment assistant should have seen its previous result on retry\\n-    expect(store.get('comment_prev_count', 'fact-validation-n1')).toBeGreaterThanOrEqual(0);\\n-    // And it should have access to all failed facts across history (3 invalid on first wave)\\n-    expect(store.get('failed_from_history', 'fact-validation-n1')).toBe(3);\\n   });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/e2e/foreach-conditional-chain.test.ts\",\"additions\":1,\"deletions\":2,\"changes\":86,\"patch\":\"diff --git a/tests/e2e/foreach-conditional-chain.test.ts b/tests/e2e/foreach-conditional-chain.test.ts\\nindex ef736ebf..8dcc7fa1 100644\\n--- a/tests/e2e/foreach-conditional-chain.test.ts\\n+++ b/tests/e2e/foreach-conditional-chain.test.ts\\n@@ -36,7 +36,7 @@ describe('E2E: forEach with Conditional Chain', () => {\\n \\n     const finalOptions = {\\n       ...options,\\n-      env: { ...cleanEnv, VISOR_DEBUG: 'true' },\\n+      env: { ...cleanEnv, VISOR_DEBUG: 'true', VISOR_STATE_MACHINE: '1' },\\n       encoding: 'utf-8',\\n       stdio: ['pipe', 'pipe', 'pipe'],\\n     };\\n@@ -184,24 +184,13 @@ checks:\\n     //   - outputs[\\\"check-a\\\"] = undefined (only 2 items, out of bounds)\\n     //   - outputs[\\\"check-b\\\"] = undefined (only 1 item, out of bounds)\\n \\n-    const result = execCLI(['--config', configPath, '--output', 'table', '--debug'], {\\n+    const result = execCLI(['--config', configPath, '--output', 'table'], {\\n       cwd: testDir,\\n     });\\n \\n-    // With the forEach branching fix, all forEach parents should be unwrapped consistently\\n-    // The logger should show single objects for both check-a and check-b in each iteration\\n-    // Note: Due to conditionals, some iterations may skip checks, but when they do run,\\n-    // they should always see unwrapped (single object) outputs\\n-\\n-    // Verify the debug output shows forEach execution\\n-    expect(result).toMatch(/depends on forEach check/);\\n-    expect(result).toMatch(/executing \\\\d+ times/);\\n-\\n-    // The final aggregated output should show:\\n-    // - check-a: array of 2 objects (for items where typeA matched)\\n-    // - check-b: single object (unwrapped array of 1, where typeB matched)\\n-    expect(result).toContain('check-a:');\\n-    expect(result).toContain('check-b:');\\n+    // Verify execution completes successfully without errors\\n+    // Since all checks complete successfully with no issues, we expect clean output\\n+    expect(result).toMatch(/No issues found/);\\n   });\\n \\n   it('should document the expected behavior', () => {\\n@@ -243,37 +232,25 @@ checks:\\n     // 3. check-b executes 1 time (for typeB item: id 2)\\n     // 4. final-check executes 3 times (once per root-check item)\\n \\n-    const result = execCLI(['--config', configPath, '--output', 'table', '--debug'], {\\n+    const result = execCLI(['--config', configPath, '--output', 'json'], {\\n       cwd: testDir,\\n     });\\n \\n-    // Verify forEach execution debug messages\\n-    expect(result).toMatch(/depends on forEach check/);\\n-    expect(result).toMatch(/executing (\\\\d+) times/);\\n-\\n-    // Verify root-check execution\\n-    expect(result).toMatch(/root-check/);\\n-\\n-    // Verify conditional execution\\n-    // check-a should run for items matching typeA\\n-    expect(result).toMatch(/check-a/);\\n+    // Parse JSON output to verify structure\\n+    const output = JSON.parse(result);\\n \\n-    // check-b should run for items matching typeB\\n-    expect(result).toMatch(/check-b/);\\n-\\n-    // final-check should run 3 times (once per forEach item)\\n-    expect(result).toMatch(/final-check/);\\n+    // Verify all checks completed successfully\\n+    // The forEach execution should result in:\\n+    // - root-check: 3 iterations (one for each item)\\n+    // - check-a: 2 iterations (for typeA items)\\n+    // - check-b: 1 iteration (for typeB item)\\n+    // - final-check: 3 iterations (one for each root-check item)\\n \\n-    // Verify the output shows the forEach branching pattern\\n-    // The debug output should show multiple executions\\n-    const executionMatches = result.match(/executing (\\\\d+) times/g);\\n-    expect(executionMatches).toBeTruthy();\\n-    expect(executionMatches!.length).toBeGreaterThan(0);\\n+    expect(output).toBeDefined();\\n \\n-    // Verify outputs are present in final-check\\n-    expect(result).toMatch(/check-a:/);\\n-    expect(result).toMatch(/check-b:/);\\n-    expect(result).toMatch(/root-check:/);\\n+    // Since this is a log check with no issues, we just verify no errors occurred\\n+    const issues = output.issues || [];\\n+    expect(Array.isArray(issues)).toBe(true);\\n   });\\n \\n   it('should properly aggregate forEach results after all iterations', () => {\\n@@ -283,33 +260,14 @@ checks:\\n     // - check-a: array of 2 items (executed for 2 typeA items)\\n     // - check-b: array of 1 item (executed for 1 typeB item)\\n \\n-    const result = execCLI(['--config', configPath, '--output', 'table', '--debug'], {\\n+    const result = execCLI(['--config', configPath, '--output', 'table'], {\\n       cwd: testDir,\\n     });\\n \\n-    // Verify all checks completed successfully\\n-    expect(result).toMatch(/root-check/);\\n-    expect(result).toMatch(/check-a/);\\n-    expect(result).toMatch(/check-b/);\\n-    expect(result).toMatch(/final-check/);\\n-\\n-    // Verify forEach completion messages\\n-    expect(result).toMatch(/Completed forEach execution for check \\\"check-a\\\"/);\\n-    expect(result).toMatch(/Completed forEach execution for check \\\"check-b\\\"/);\\n-    expect(result).toMatch(/Completed forEach execution for check \\\"final-check\\\"/);\\n-\\n-    // Verify the checks were executed\\n-    // Note: \\\"Checks Executed\\\" only appears in Analysis Summary when there are issues\\n-    // Since there are no issues, we verify execution via completion messages instead\\n-    expect(result).toMatch(/Dependency-aware execution completed successfully/);\\n-\\n-    // The summary should show all checks executed (in debug output)\\n-    expect(result).toContain('root-check');\\n-    expect(result).toContain('check-a');\\n-    expect(result).toContain('check-b');\\n-    expect(result).toContain('final-check');\\n+    // Verify execution completed successfully\\n+    expect(result).toBeDefined();\\n \\n     // No issues should be found since all checks complete successfully\\n-    expect(result).toMatch(/No issues found|Total Issues.*0/);\\n+    expect(result).toMatch(/No issues found/);\\n   });\\n });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/e2e/foreach-on-finish.test.ts\",\"additions\":1,\"deletions\":0,\"changes\":1,\"patch\":\"diff --git a/tests/e2e/foreach-on-finish.test.ts b/tests/e2e/foreach-on-finish.test.ts\\nindex 28b9b144..84462f10 100644\\n--- a/tests/e2e/foreach-on-finish.test.ts\\n+++ b/tests/e2e/foreach-on-finish.test.ts\\n@@ -593,6 +593,7 @@ output:\\n       expect(result).toContain('Posted verified response');\\n       // Should not route back (no retry)\\n       expect(result).not.toMatch(/Routed to:/i);\\n+      // Expect stable execution accounting\\n       expect(result).toMatch(/Checks:\\\\s*4 configured\\\\s*‚Üí\\\\s*7 executions/);\\n     });\\n \\n\",\"status\":\"added\"},{\"filename\":\"tests/e2e/on-finish-loop-budget-e2e.test.ts\",\"additions\":1,\"deletions\":0,\"changes\":5,\"patch\":\"diff --git a/tests/e2e/on-finish-loop-budget-e2e.test.ts b/tests/e2e/on-finish-loop-budget-e2e.test.ts\\nindex 3c917bf6..7ebdfc5e 100644\\n--- a/tests/e2e/on-finish-loop-budget-e2e.test.ts\\n+++ b/tests/e2e/on-finish-loop-budget-e2e.test.ts\\n@@ -65,6 +65,11 @@ checks:\\n       run: [child-log]\\n       goto: other-log\\n \\n+  process-item:\\n+    type: log\\n+    message: Processing item\\n+    depends_on: [parent]\\n+\\n   child-log:\\n     type: log\\n     message: CHILD\\n\",\"status\":\"added\"},{\"filename\":\"tests/e2e/session-reuse-e2e.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":7,\"patch\":\"diff --git a/tests/e2e/session-reuse-e2e.test.ts b/tests/e2e/session-reuse-e2e.test.ts\\nindex 53b4e523..053814ab 100644\\n--- a/tests/e2e/session-reuse-e2e.test.ts\\n+++ b/tests/e2e/session-reuse-e2e.test.ts\\n@@ -202,9 +202,10 @@ fail_fast: false\\n     expect(result.reviewSummary.issues).toBeDefined();\\n     expect(Array.isArray(result.reviewSummary.issues)).toBe(true);\\n \\n-    // Verify debug information is available\\n-    expect(result.debug).toBeDefined();\\n-    expect(result.debug?.checksExecuted).toEqual(result.checksExecuted);\\n+    // Verify debug information when available\\n+    if (result.debug) {\\n+      expect(result.debug.checksExecuted).toEqual(result.checksExecuted);\\n+    }\\n \\n     // Verify session reuse affected parallelism\\n     // security-analysis and security-remediation should run sequentially\\n\",\"status\":\"modified\"},{\"filename\":\"tests/e2e/telemetry-mermaid-e2e.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/tests/e2e/telemetry-mermaid-e2e.test.ts b/tests/e2e/telemetry-mermaid-e2e.test.ts\\nindex e21531ef..796fac7f 100644\\n--- a/tests/e2e/telemetry-mermaid-e2e.test.ts\\n+++ b/tests/e2e/telemetry-mermaid-e2e.test.ts\\n@@ -21,7 +21,9 @@ describe('Telemetry E2E ‚Äî Mermaid diagram telemetry (full code)', () => {\\n   beforeAll(() => {\\n     fs.mkdirSync(tempDir, { recursive: true });\\n     fs.mkdirSync(tracesDir, { recursive: true });\\n+  });\\n \\n+  beforeEach(() => {\\n     // Build a check that renders a template containing a mermaid block\\n     const cfg = {\\n       version: '1.0',\\n@@ -38,9 +40,7 @@ describe('Telemetry E2E ‚Äî Mermaid diagram telemetry (full code)', () => {\\n     } as const;\\n \\n     fs.writeFileSync(configPath, yaml.dump(cfg), 'utf8');\\n-  });\\n \\n-  beforeEach(() => {\\n     mockConsoleLog = jest.fn();\\n     mockConsoleError = jest.fn();\\n     mockProcessExit = jest.fn();\\n\",\"status\":\"modified\"},{\"filename\":\"tests/integration/depends-on-continue-on-failure.test.ts\",\"additions\":2,\"deletions\":0,\"changes\":68,\"patch\":\"diff --git a/tests/integration/depends-on-continue-on-failure.test.ts b/tests/integration/depends-on-continue-on-failure.test.ts\\nnew file mode 100644\\nindex 00000000..a4289c18\\n--- /dev/null\\n+++ b/tests/integration/depends-on-continue-on-failure.test.ts\\n@@ -0,0 +1,68 @@\\n+import { CheckExecutionEngine } from '../../src/check-execution-engine';\\n+import type { VisorConfig } from '../../src/types/config';\\n+\\n+describe('depends_on gating with continue_on_failure', () => {\\n+  test('blocks dependent when dependency fails by default', async () => {\\n+    const engine = new CheckExecutionEngine(process.cwd());\\n+    const cfg: VisorConfig = {\\n+      version: '1.0',\\n+      checks: {\\n+        failer: {\\n+          type: 'command',\\n+          // Emit a non-zero exit via node process.exit(1)\\n+          exec: \\\"node -e 'process.exit(1)'\\\",\\n+          on: ['manual'],\\n+        },\\n+        dep: {\\n+          type: 'command',\\n+          exec: 'node -e \\\"console.log(JSON.stringify({\\\\\\\\\\\"ok\\\\\\\\\\\":true}))\\\"',\\n+          depends_on: ['failer'],\\n+          on: ['manual'],\\n+        },\\n+      },\\n+      output: { pr_comment: { format: 'markdown', group_by: 'check', collapse: false } },\\n+    } as any;\\n+\\n+    const res = await engine.executeChecks({\\n+      checks: ['failer', 'dep'],\\n+      config: cfg,\\n+      webhookContext: { eventType: 'manual' } as any,\\n+    });\\n+\\n+    const hist = (res.reviewSummary as any).history || {};\\n+    const depHist = Array.isArray(hist['dep']) ? hist['dep'] : [];\\n+    expect(depHist.length).toBe(0);\\n+  });\\n+\\n+  test('allows dependent when dependency has continue_on_failure: true', async () => {\\n+    const engine = new CheckExecutionEngine(process.cwd());\\n+    const cfg: VisorConfig = {\\n+      version: '1.0',\\n+      checks: {\\n+        failer: {\\n+          type: 'command',\\n+          exec: \\\"node -e 'process.exit(1)'\\\",\\n+          on: ['manual'],\\n+          continue_on_failure: true,\\n+        },\\n+        dep: {\\n+          type: 'command',\\n+          exec: 'node -e \\\"console.log(JSON.stringify({\\\\\\\\\\\"ok\\\\\\\\\\\":true}))\\\"',\\n+          depends_on: ['failer'],\\n+          on: ['manual'],\\n+        },\\n+      },\\n+      output: { pr_comment: { format: 'markdown', group_by: 'check', collapse: false } },\\n+    } as any;\\n+\\n+    const res = await engine.executeChecks({\\n+      checks: ['failer', 'dep'],\\n+      config: cfg,\\n+      webhookContext: { eventType: 'manual' } as any,\\n+    });\\n+\\n+    const hist = (res.reviewSummary as any).history || {};\\n+    const depHist = Array.isArray(hist['dep']) ? hist['dep'] : [];\\n+    expect(depHist.length).toBe(1);\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/integration/forward-run-dedupe-success.test.ts\",\"additions\":2,\"deletions\":0,\"changes\":62,\"patch\":\"diff --git a/tests/integration/forward-run-dedupe-success.test.ts b/tests/integration/forward-run-dedupe-success.test.ts\\nnew file mode 100644\\nindex 00000000..dc4d7136\\n--- /dev/null\\n+++ b/tests/integration/forward-run-dedupe-success.test.ts\\n@@ -0,0 +1,62 @@\\n+import { CheckExecutionEngine } from '../../src/check-execution-engine';\\n+import type { VisorConfig } from '../../src/types/config';\\n+\\n+describe('forward-run dedupe after success (multi-turn)', () => {\\n+  test('finish runs exactly once after final refine success', async () => {\\n+    const engine = new CheckExecutionEngine(process.cwd());\\n+\\n+    const cfg: VisorConfig = {\\n+      version: '1.0',\\n+      checks: {\\n+        ask: {\\n+          type: 'script',\\n+          content: `({ ok: true })`,\\n+          on: ['manual'],\\n+        },\\n+        refine: {\\n+          type: 'script',\\n+          depends_on: ['ask'],\\n+          on: ['manual'],\\n+          // Compute attempt from refine.history; flip refined=true on 3rd run\\n+          content: `\\n+(() => {\\n+  const histRef = (outputs && outputs.history && outputs.history['refine']) || [];\\n+  const attempt = Array.isArray(histRef) ? histRef.length + 1 : 1;\\n+  log('[refine.content]', 'histRefLen=', Array.isArray(histRef) ? histRef.length : 'NA', 'attempt=', attempt);\\n+  if (attempt === 1) return { refined: false, text: 'Which CI platform and trigger conditions?' };\\n+  if (attempt === 2) return { refined: false, text: 'What Node version and commands should run?' };\\n+  return { refined: true, text: 'Set up GitHub Actions workflow: on push to main, use Node 18.x, cache npm, run npm ci && npm test.' };\\n+})()\\n+`,\\n+          fail_if: `\\n+            log('[refine.fail_if]', 'refined?', output && output.refined, 'text=', output && output.text);\\n+            output && output.refined !== true\\n+          `,\\n+          on_fail: { goto: 'ask' },\\n+        },\\n+        finish: {\\n+          type: 'script',\\n+          depends_on: ['refine'],\\n+          on: ['manual'],\\n+          content: `({ text: (outputs['refine'] && outputs['refine'].text) || '' })`,\\n+        },\\n+      },\\n+      output: { pr_comment: { format: 'markdown', group_by: 'check', collapse: false } },\\n+    } as any;\\n+\\n+    const res = await engine.executeChecks({\\n+      checks: ['ask', 'refine', 'finish'],\\n+      config: cfg,\\n+      webhookContext: { eventType: 'manual' } as any,\\n+    });\\n+\\n+    const hist = (res.reviewSummary as any).history || {};\\n+    const askRuns = Array.isArray(hist['ask']) ? hist['ask'].length : 0;\\n+    const refineRuns = Array.isArray(hist['refine']) ? hist['refine'].length : 0;\\n+    const finishRuns = Array.isArray(hist['finish']) ? hist['finish'].length : 0;\\n+\\n+    expect(askRuns).toBeGreaterThanOrEqual(3);\\n+    expect(refineRuns).toBe(3);\\n+    expect(finishRuns).toBe(1);\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/integration/github-workflow.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":14,\"patch\":\"diff --git a/tests/integration/github-workflow.test.ts b/tests/integration/github-workflow.test.ts\\nindex b319805a..a6f2707e 100644\\n--- a/tests/integration/github-workflow.test.ts\\n+++ b/tests/integration/github-workflow.test.ts\\n@@ -41,13 +41,15 @@ jest.mock('../../src/check-execution-engine', () => {\\n         .fn()\\n         .mockImplementation(async (_prInfo, _checks, _unused1, _config, _unused2, _debug) => {\\n           // Return ExecutionResult format\\n+          // State machine groups by checkName when no explicit group is set\\n+          const checkName = _checks[0] || 'security-review';\\n           return {\\n             results: {\\n-              default: [\\n+              [checkName]: [\\n                 {\\n-                  checkName: 'security-review',\\n+                  checkName: checkName,\\n                   content: `## Security Issues Found\\\\n\\\\n- **CRITICAL**: Potential hardcoded API key detected (src/test.ts:10)\\\\n- **WARNING**: Consider using a more efficient data structure (src/test.ts:25)\\\\n\\\\n## Suggestions\\\\n\\\\n- Consider adding input validation\\\\n- Add unit tests for new functionality`,\\n-                  group: 'default',\\n+                  group: checkName,\\n                   debug: {\\n                     provider: 'google',\\n                     model: 'gemini-2.0-flash-exp',\\n@@ -267,13 +269,13 @@ describe('GitHub PR Workflow Integration', () => {\\n       });\\n \\n       // Verify review structure (new GroupedCheckResults format)\\n+      // State machine groups by checkName when no explicit group is set\\n       expect(review).toEqual(\\n         expect.objectContaining({\\n-          default: expect.arrayContaining([\\n+          'security-review': expect.arrayContaining([\\n             expect.objectContaining({\\n               checkName: 'security-review',\\n-              content: expect.stringContaining('Security Issues Found'),\\n-              group: 'default',\\n+              group: 'security-review',\\n             }),\\n           ]),\\n         })\\n\",\"status\":\"modified\"},{\"filename\":\"tests/integration/on-fail-no-cascade.test.ts\",\"additions\":1,\"deletions\":0,\"changes\":35,\"patch\":\"diff --git a/tests/integration/on-fail-no-cascade.test.ts b/tests/integration/on-fail-no-cascade.test.ts\\nnew file mode 100644\\nindex 00000000..f28dc2a5\\n--- /dev/null\\n+++ b/tests/integration/on-fail-no-cascade.test.ts\\n@@ -0,0 +1,35 @@\\n+import { CheckExecutionEngine } from '../../src/check-execution-engine';\\n+import type { VisorConfig } from '../../src/types/config';\\n+\\n+describe('on_fail forward-run does not cascade success chains', () => {\\n+  it('skips finish when refine fails via fail_if', async () => {\\n+    const engine = new CheckExecutionEngine(process.cwd());\\n+    const cfg: VisorConfig = {\\n+      version: '1.0',\\n+      checks: {\\n+        ask: { type: 'command', exec: 'echo ask', on_success: { goto: 'refine' }, on: ['manual'] },\\n+        refine: {\\n+          type: 'command',\\n+          depends_on: ['ask'],\\n+          exec: 'node -e \\\"console.log(JSON.stringify({refined:false}))\\\"',\\n+          fail_if: 'output && output.refined !== true',\\n+          on_fail: { goto: 'ask' },\\n+          on_success: { goto: 'finish' },\\n+          on: ['manual'],\\n+        },\\n+        finish: { type: 'command', depends_on: ['refine'], exec: 'echo done', on: ['manual'] },\\n+      },\\n+      output: { pr_comment: { format: 'markdown', group_by: 'check', collapse: false } },\\n+      routing: { max_loops: 0 },\\n+    } as any;\\n+\\n+    const res = await engine.executeChecks({\\n+      checks: ['ask', 'refine', 'finish'],\\n+      config: cfg,\\n+      webhookContext: { eventType: 'manual' } as any,\\n+    });\\n+    const hist = (res.reviewSummary as any).history || {};\\n+    const finishHist = Array.isArray(hist['finish']) ? hist['finish'] : [];\\n+    expect(finishHist.length).toBe(0);\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/integration/snapshot-visibility-integration.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":3,\"patch\":\"diff --git a/tests/integration/snapshot-visibility-integration.test.ts b/tests/integration/snapshot-visibility-integration.test.ts\\nindex 2df585b1..b71b8c52 100644\\n--- a/tests/integration/snapshot-visibility-integration.test.ts\\n+++ b/tests/integration/snapshot-visibility-integration.test.ts\\n@@ -23,8 +23,7 @@ describe('Snapshot Visibility Integration', () => {\\n         },\\n         consumer: {\\n           type: 'script',\\n-          depends_on: ['producer'],\\n-          // Note: with script provider, we provide explicit dependency\\n+          // NO depends_on - we test snapshot visibility via goto routing\\n           content: `\\n             // Read producer output via snapshot-provided outputs\\n             const value = outputs[\\\"producer\\\"]?.msg;\\n\",\"status\":\"modified\"},{\"filename\":\"tests/integration/suppression.test.ts\",\"additions\":2,\"deletions\":2,\"changes\":149,\"patch\":\"diff --git a/tests/integration/suppression.test.ts b/tests/integration/suppression.test.ts\\nindex 9275f002..b90e7d3f 100644\\n--- a/tests/integration/suppression.test.ts\\n+++ b/tests/integration/suppression.test.ts\\n@@ -1,23 +1,11 @@\\n import { IssueFilter } from '../../src/issue-filter';\\n-import { ReviewIssue, ReviewSummary } from '../../src/reviewer';\\n+import { ReviewIssue } from '../../src/reviewer';\\n import { CheckExecutionEngine } from '../../src/check-execution-engine';\\n-import { VisorConfig } from '../../src/types/config';\\n-import { DependencyGraph } from '../../src/dependency-resolver';\\n+import type { PRInfo } from '../../src/pr-analyzer';\\n import * as fs from 'fs';\\n import * as path from 'path';\\n import * as os from 'os';\\n \\n-// Type for accessing private methods and properties in tests\\n-interface CheckExecutionEngineWithPrivates {\\n-  config: Partial<VisorConfig>;\\n-  aggregateDependencyAwareResults(\\n-    results: Map<string, ReviewSummary>,\\n-    dependencyGraph: DependencyGraph,\\n-    debug: boolean,\\n-    stoppedEarly: boolean\\n-  ): ReviewSummary;\\n-}\\n-\\n describe('Issue Suppression Integration Tests', () => {\\n   let tempDir: string;\\n \\n@@ -66,58 +54,34 @@ function test() {\\n \\n       const engine = new CheckExecutionEngine(tempDir);\\n \\n-      // Store the original config so we can pass it through\\n-      (engine as unknown as CheckExecutionEngineWithPrivates).config = {\\n-        output: {\\n-          suppressionEnabled: true,\\n-          pr_comment: {\\n-            format: 'table',\\n-            group_by: 'check',\\n-            collapse: false,\\n-          },\\n+      // Create test issues directly - testing the IssueFilter integration\\n+      const testIssues: ReviewIssue[] = [\\n+        {\\n+          file: 'file1.js',\\n+          line: 3,\\n+          ruleId: 'security/hardcoded-password',\\n+          message: 'Hardcoded password - should be suppressed',\\n+          severity: 'error',\\n+          category: 'security',\\n         },\\n-      };\\n-\\n-      // Test the filtering directly with the aggregateDependencyAwareResults method\\n-      const mockResults = new Map();\\n-      mockResults.set('test-check', {\\n-        issues: [\\n-          {\\n-            file: 'file1.js',\\n-            line: 3,\\n-            ruleId: 'security/hardcoded-password',\\n-            message: 'Hardcoded password - should be suppressed',\\n-            severity: 'error',\\n-            category: 'security',\\n-          },\\n-          {\\n-            file: 'file2.js',\\n-            line: 3,\\n-            ruleId: 'security/hardcoded-api-key',\\n-            message: 'Hardcoded API key - should NOT be suppressed',\\n-            severity: 'error',\\n-            category: 'security',\\n-          },\\n-        ],\\n-      });\\n-\\n-      const mockDependencyGraph: DependencyGraph = {\\n-        executionOrder: [{ level: 0, parallel: ['test-check'] }],\\n-        nodes: new Map([\\n-          ['test-check', { id: 'test-check', dependencies: [], dependents: [], depth: 0 }],\\n-        ]),\\n-        hasCycles: false,\\n-      };\\n+        {\\n+          file: 'file2.js',\\n+          line: 3,\\n+          ruleId: 'security/hardcoded-api-key',\\n+          message: 'Hardcoded API key - should NOT be suppressed',\\n+          severity: 'error',\\n+          category: 'security',\\n+        },\\n+      ];\\n \\n-      // Call the method directly\\n-      const result = (\\n-        engine as unknown as CheckExecutionEngineWithPrivates\\n-      ).aggregateDependencyAwareResults(mockResults, mockDependencyGraph, false, false);\\n+      // Apply suppression filter directly (simulating what the engine does)\\n+      const filter = new IssueFilter(true);\\n+      const filteredIssues = filter.filterIssues(testIssues, tempDir);\\n \\n       // Verify that only file2 issue remains (file1 was suppressed)\\n-      expect(result.issues).toHaveLength(1);\\n-      expect(result.issues![0].file).toBe('file2.js');\\n-      expect(result.issues![0].message).toContain('should NOT be suppressed');\\n+      expect(filteredIssues).toHaveLength(1);\\n+      expect(filteredIssues[0].file).toBe('file2.js');\\n+      expect(filteredIssues[0].message).toContain('should NOT be suppressed');\\n     });\\n \\n     it('should respect suppressionEnabled config', async () => {\\n@@ -140,51 +104,30 @@ function test() {\\n       execSync('git add .', { cwd: tempDir });\\n       execSync('git -c core.hooksPath=/dev/null commit -m \\\"test\\\"', { cwd: tempDir });\\n \\n-      const engine = new CheckExecutionEngine(tempDir);\\n-\\n-      // Set config to disable suppression\\n-      (engine as unknown as CheckExecutionEngineWithPrivates).config = {\\n-        output: {\\n-          suppressionEnabled: false,\\n-          pr_comment: {\\n-            format: 'table',\\n-            group_by: 'check',\\n-            collapse: false,\\n-          },\\n-        },\\n-      };\\n-\\n-      // Test the filtering directly with the aggregateDependencyAwareResults method\\n-      const mockResults = new Map();\\n-      mockResults.set('test-check', {\\n-        issues: [\\n-          {\\n-            file: 'test.js',\\n-            line: 3,\\n-            ruleId: 'security/hardcoded-password',\\n-            message: 'Hardcoded password',\\n-            severity: 'error',\\n-            category: 'security',\\n-          },\\n-        ],\\n-      });\\n-\\n-      const mockDependencyGraph: DependencyGraph = {\\n-        executionOrder: [{ level: 0, parallel: ['test-check'] }],\\n-        nodes: new Map([\\n-          ['test-check', { id: 'test-check', dependencies: [], dependents: [], depth: 0 }],\\n-        ]),\\n-        hasCycles: false,\\n+      // Test with suppression disabled\\n+      const testIssue: ReviewIssue = {\\n+        file: 'test.js',\\n+        line: 3,\\n+        ruleId: 'security/hardcoded-password',\\n+        message: 'Hardcoded password',\\n+        severity: 'error',\\n+        category: 'security',\\n       };\\n \\n-      // Call the method directly\\n-      const result = (\\n-        engine as unknown as CheckExecutionEngineWithPrivates\\n-      ).aggregateDependencyAwareResults(mockResults, mockDependencyGraph, false, false);\\n+      // Apply filter with suppression disabled\\n+      const filterDisabled = new IssueFilter(false);\\n+      const resultDisabled = filterDisabled.filterIssues([testIssue], tempDir);\\n \\n       // Should NOT suppress when disabled\\n-      expect(result.issues).toHaveLength(1);\\n-      expect(result.issues![0].message).toBe('Hardcoded password');\\n+      expect(resultDisabled).toHaveLength(1);\\n+      expect(resultDisabled[0].message).toBe('Hardcoded password');\\n+\\n+      // Test with suppression enabled (for comparison)\\n+      const filterEnabled = new IssueFilter(true);\\n+      const resultEnabled = filterEnabled.filterIssues([testIssue], tempDir);\\n+\\n+      // Should suppress when enabled (since file has visor-disable comment)\\n+      expect(resultEnabled).toHaveLength(0);\\n     });\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/manual/README.md\",\"additions\":2,\"deletions\":0,\"changes\":64,\"patch\":\"diff --git a/tests/manual/README.md b/tests/manual/README.md\\nnew file mode 100644\\nindex 00000000..5f3bab0f\\n--- /dev/null\\n+++ b/tests/manual/README.md\\n@@ -0,0 +1,64 @@\\n+# Manual Tests\\n+\\n+This directory contains manual tests that require real API keys and make actual API calls. These tests are skipped by default in CI/CD.\\n+\\n+## Running Manual Tests\\n+\\n+### Prerequisites\\n+\\n+Set the required environment variables:\\n+\\n+```bash\\n+export ANTHROPIC_API_KEY=\\\"your-api-key-here\\\"\\n+export RUN_MANUAL_TESTS=true\\n+```\\n+\\n+### Run Bash Configuration Tests\\n+\\n+```bash\\n+# Run all manual tests\\n+npm test -- tests/manual/bash-config-manual.test.ts\\n+\\n+# Or use the helper script\\n+npm run test:manual:bash\\n+```\\n+\\n+## Test Coverage\\n+\\n+### `bash-config-manual.test.ts`\\n+\\n+Tests the bash command execution configuration with real ProbeAgent calls:\\n+\\n+1. **allowBash: true** - Validates basic bash execution with default safe commands\\n+2. **bashConfig options** - Tests custom allow/deny lists and timeout\\n+3. **workingDirectory** - Verifies custom working directory is respected\\n+4. **Default behavior** - Confirms bash is disabled by default\\n+\\n+## Notes\\n+\\n+- These tests make real API calls and may incur costs\\n+- Tests are automatically skipped unless `RUN_MANUAL_TESTS=true`\\n+- Each test has a 60-second timeout\\n+- Debug mode is enabled to show ProbeAgent interactions\\n+\\n+## Expected Output\\n+\\n+When tests pass, you should see:\\n+\\n+```\\n+üìù Testing allowBash: true\\n+‚úÖ allowBash test completed\\n+üìä Result: { overallScore: 100, ... }\\n+\\n+üìù Testing allowBash with bashConfig\\n+‚úÖ bashConfig test completed\\n+üìä Result: { overallScore: 100, ... }\\n+\\n+üìù Testing bashConfig.workingDirectory\\n+‚úÖ workingDirectory test completed\\n+üìä Result: { overallScore: 100, ... }\\n+\\n+üìù Testing without allowBash (default behavior)\\n+‚úÖ No bash test completed\\n+üìä Result: { overallScore: 100, ... }\\n+```\\n\",\"status\":\"added\"},{\"filename\":\"tests/manual/bash-config-manual.test.ts\",\"additions\":7,\"deletions\":0,\"changes\":235,\"patch\":\"diff --git a/tests/manual/bash-config-manual.test.ts b/tests/manual/bash-config-manual.test.ts\\nnew file mode 100644\\nindex 00000000..bf304269\\n--- /dev/null\\n+++ b/tests/manual/bash-config-manual.test.ts\\n@@ -0,0 +1,235 @@\\n+/**\\n+ * Manual test for bash configuration with ProbeAgent\\n+ *\\n+ * This test validates that bash configuration options are properly passed\\n+ * to ProbeAgent and can execute bash commands when enabled.\\n+ *\\n+ * Run with: npm test -- tests/manual/bash-config-manual.test.ts\\n+ *\\n+ * Prerequisites:\\n+ * - ANTHROPIC_API_KEY environment variable must be set\\n+ * - This test actually calls the AI API and may incur costs\\n+ */\\n+\\n+import { AIReviewService } from '../../src/ai-review-service';\\n+import { PRInfo } from '../../src/pr-analyzer';\\n+\\n+// Skip this test in CI/CD - only run manually\\n+const runManualTests = process.env.RUN_MANUAL_TESTS === 'true';\\n+\\n+describe('Bash Configuration Manual Tests', () => {\\n+  // Create a minimal PR info for testing\\n+  const mockPRInfo: PRInfo = {\\n+    number: 1,\\n+    title: 'Test PR',\\n+    body: 'Test PR body',\\n+    author: 'test-user',\\n+    base: 'main',\\n+    head: 'feature',\\n+    files: [\\n+      {\\n+        filename: 'test.ts',\\n+        status: 'modified',\\n+        additions: 10,\\n+        deletions: 5,\\n+        changes: 15,\\n+        patch: '+console.log(\\\"test\\\");',\\n+      },\\n+    ],\\n+    totalAdditions: 10,\\n+    totalDeletions: 5,\\n+  };\\n+\\n+  beforeAll(() => {\\n+    if (!runManualTests) {\\n+      console.log('‚è≠Ô∏è  Skipping manual tests. Set RUN_MANUAL_TESTS=true to run.');\\n+    }\\n+  });\\n+\\n+  (runManualTests ? describe : describe.skip)('With API Key', () => {\\n+    it('should execute bash commands when allowBash is true', async () => {\\n+      const hasAnthropicKey = !!process.env.ANTHROPIC_API_KEY;\\n+      const hasGoogleKey = !!process.env.GOOGLE_API_KEY;\\n+\\n+      if (!hasAnthropicKey && !hasGoogleKey) {\\n+        console.warn('‚ö†Ô∏è  No API key set (ANTHROPIC_API_KEY or GOOGLE_API_KEY), skipping test');\\n+        return;\\n+      }\\n+\\n+      const provider = hasAnthropicKey ? 'anthropic' : 'google';\\n+      const model = hasAnthropicKey ? 'claude-3-5-sonnet-20241022' : 'gemini-2.0-flash-exp';\\n+\\n+      const service = new AIReviewService({\\n+        provider: provider as any,\\n+        model,\\n+        allowBash: true,\\n+        debug: true,\\n+      });\\n+\\n+      const prompt = `\\n+You have access to bash commands. Please:\\n+1. Run 'echo \\\"Hello from bash\\\"'\\n+2. Run 'pwd' to show the current directory\\n+3. Confirm that bash commands are working\\n+\\n+Return a JSON response with your findings.\\n+`;\\n+\\n+      console.log('\\\\nüìù Testing allowBash: true');\\n+      const result = await service.executeReview(mockPRInfo, prompt);\\n+\\n+      // Check that we got a response\\n+      expect(result).toBeDefined();\\n+      console.log('‚úÖ allowBash test completed');\\n+      console.log('üìä Result:', JSON.stringify(result, null, 2));\\n+    }, 60000); // 60 second timeout\\n+\\n+    it('should pass bashConfig options to ProbeAgent', async () => {\\n+      const hasAnthropicKey = !!process.env.ANTHROPIC_API_KEY;\\n+      const hasGoogleKey = !!process.env.GOOGLE_API_KEY;\\n+\\n+      if (!hasAnthropicKey && !hasGoogleKey) {\\n+        console.warn('‚ö†Ô∏è  No API key set, skipping test');\\n+        return;\\n+      }\\n+\\n+      const provider = hasAnthropicKey ? 'anthropic' : 'google';\\n+      const model = hasAnthropicKey ? 'claude-3-5-sonnet-20241022' : 'gemini-2.0-flash-exp';\\n+\\n+      const service = new AIReviewService({\\n+        provider: provider as any,\\n+        model,\\n+        allowBash: true,\\n+        bashConfig: {\\n+          allow: ['echo', 'pwd', 'ls'],\\n+          timeout: 5000,\\n+        },\\n+        debug: true,\\n+      });\\n+\\n+      const prompt = `\\n+You have access to bash commands with custom configuration.\\n+Try running these commands:\\n+1. echo \\\"Test with custom allow list\\\"\\n+2. pwd\\n+3. ls\\n+\\n+Summarize what commands worked and return JSON.\\n+`;\\n+\\n+      console.log('\\\\nüìù Testing allowBash with bashConfig');\\n+      const result = await service.executeReview(mockPRInfo, prompt);\\n+\\n+      expect(result).toBeDefined();\\n+      console.log('‚úÖ bashConfig test completed');\\n+      console.log('üìä Result:', JSON.stringify(result, null, 2));\\n+    }, 60000);\\n+\\n+    it('should respect custom working directory', async () => {\\n+      const hasAnthropicKey = !!process.env.ANTHROPIC_API_KEY;\\n+      const hasGoogleKey = !!process.env.GOOGLE_API_KEY;\\n+\\n+      if (!hasAnthropicKey && !hasGoogleKey) {\\n+        console.warn('‚ö†Ô∏è  No API key set, skipping test');\\n+        return;\\n+      }\\n+\\n+      const provider = hasAnthropicKey ? 'anthropic' : 'google';\\n+      const model = hasAnthropicKey ? 'claude-3-5-sonnet-20241022' : 'gemini-2.0-flash-exp';\\n+\\n+      const service = new AIReviewService({\\n+        provider: provider as any,\\n+        model,\\n+        allowBash: true,\\n+        bashConfig: {\\n+          workingDirectory: '/tmp',\\n+        },\\n+        debug: true,\\n+      });\\n+\\n+      const prompt = `\\n+Run 'pwd' to show the current working directory.\\n+The working directory should be /tmp.\\n+Return JSON with the pwd output.\\n+`;\\n+\\n+      console.log('\\\\nüìù Testing bashConfig.workingDirectory');\\n+      const result = await service.executeReview(mockPRInfo, prompt);\\n+\\n+      expect(result).toBeDefined();\\n+      console.log('‚úÖ workingDirectory test completed');\\n+      console.log('üìä Result:', JSON.stringify(result, null, 2));\\n+    }, 60000);\\n+\\n+    it('should work without bash when allowBash is not set', async () => {\\n+      const hasAnthropicKey = !!process.env.ANTHROPIC_API_KEY;\\n+      const hasGoogleKey = !!process.env.GOOGLE_API_KEY;\\n+\\n+      if (!hasAnthropicKey && !hasGoogleKey) {\\n+        console.warn('‚ö†Ô∏è  No API key set, skipping test');\\n+        return;\\n+      }\\n+\\n+      const provider = hasAnthropicKey ? 'anthropic' : 'google';\\n+      const model = hasAnthropicKey ? 'claude-3-5-sonnet-20241022' : 'gemini-2.0-flash-exp';\\n+\\n+      const service = new AIReviewService({\\n+        provider: provider as any,\\n+        model,\\n+        // allowBash not set - should default to false\\n+        debug: true,\\n+      });\\n+\\n+      const prompt = `\\n+Analyze this test file and provide feedback.\\n+You should NOT have access to bash commands.\\n+Return JSON with your analysis.\\n+`;\\n+\\n+      console.log('\\\\nüìù Testing without allowBash (default behavior)');\\n+      const result = await service.executeReview(mockPRInfo, prompt);\\n+\\n+      expect(result).toBeDefined();\\n+      console.log('‚úÖ No bash test completed');\\n+      console.log('üìä Result:', JSON.stringify(result, null, 2));\\n+    }, 60000);\\n+  });\\n+\\n+  describe('Configuration Validation', () => {\\n+    it('should accept allowBash boolean', () => {\\n+      expect(() => {\\n+        new AIReviewService({\\n+          provider: 'mock',\\n+          allowBash: true,\\n+        });\\n+      }).not.toThrow();\\n+    });\\n+\\n+    it('should accept bashConfig object', () => {\\n+      expect(() => {\\n+        new AIReviewService({\\n+          provider: 'mock',\\n+          allowBash: true,\\n+          bashConfig: {\\n+            allow: ['ls', 'pwd'],\\n+            deny: ['rm'],\\n+            timeout: 30000,\\n+            workingDirectory: '/tmp',\\n+          },\\n+        });\\n+      }).not.toThrow();\\n+    });\\n+\\n+    it('should accept both allowBash and bashConfig', () => {\\n+      expect(() => {\\n+        new AIReviewService({\\n+          provider: 'mock',\\n+          allowBash: true,\\n+          bashConfig: {\\n+            allow: ['git status'],\\n+          },\\n+        });\\n+      }).not.toThrow();\\n+    });\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/setup.ts\",\"additions\":1,\"deletions\":1,\"changes\":20,\"patch\":\"diff --git a/tests/setup.ts b/tests/setup.ts\\nindex 79a10416..812e00d3 100644\\n--- a/tests/setup.ts\\n+++ b/tests/setup.ts\\n@@ -67,12 +67,16 @@ afterEach(() => {\\n // Set global Jest timeout for all tests\\n jest.setTimeout(10000); // 10 seconds max per test\\n \\n-// Configure console to reduce noise in test output\\n+// Configure console to reduce noise in test output.\\n+// When VISOR_DEBUG=true or VISOR_TEST_SHOW_LOGS=true, keep real console to allow debugging logs.\\n const originalConsole = global.console;\\n-global.console = {\\n-  ...originalConsole,\\n-  log: jest.fn(), // Silence console.log during tests\\n-  warn: jest.fn(),\\n-  error: jest.fn(),\\n-  debug: jest.fn(),\\n-} as Console;\\n+const SHOW_LOGS = process.env.VISOR_DEBUG === 'true' || process.env.VISOR_TEST_SHOW_LOGS === 'true';\\n+if (!SHOW_LOGS) {\\n+  global.console = {\\n+    ...originalConsole,\\n+    log: jest.fn(), // Silence console.log during tests\\n+    warn: jest.fn(),\\n+    error: jest.fn(),\\n+    debug: jest.fn(),\\n+  } as Console;\\n+}\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/check-execution-engine-dependencies.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":17,\"patch\":\"diff --git a/tests/unit/check-execution-engine-dependencies.test.ts b/tests/unit/check-execution-engine-dependencies.test.ts\\nindex aa24520a..1dba31bb 100644\\n--- a/tests/unit/check-execution-engine-dependencies.test.ts\\n+++ b/tests/unit/check-execution-engine-dependencies.test.ts\\n@@ -259,7 +259,7 @@ describe('CheckExecutionEngine - Dependencies', () => {\\n \\n       // Should return error result instead of throwing\\n       expect(result.reviewSummary.issues).toHaveLength(1);\\n-      expect(result.reviewSummary.issues![0].message).toContain('Circular dependencies detected');\\n+      expect(result.reviewSummary.issues![0].message).toContain('Dependency cycle detected');\\n       expect(mockProvider.execute).not.toHaveBeenCalled();\\n     });\\n \\n@@ -290,7 +290,8 @@ describe('CheckExecutionEngine - Dependencies', () => {\\n \\n       // Should return error result instead of throwing\\n       expect(result.reviewSummary.issues).toHaveLength(1);\\n-      expect(result.reviewSummary.issues![0].message).toContain('Dependency validation failed');\\n+      expect(result.reviewSummary.issues![0].message).toContain('depends on');\\n+      expect(result.reviewSummary.issues![0].message).toContain('not defined');\\n       expect(mockProvider.execute).not.toHaveBeenCalled();\\n     });\\n \\n@@ -339,14 +340,16 @@ describe('CheckExecutionEngine - Dependencies', () => {\\n       });\\n \\n       expect(result.reviewSummary.issues).toBeDefined();\\n-      expect(mockProvider.execute).toHaveBeenCalledTimes(2);\\n \\n-      // Should have error issues from failed security check\\n+      // State machine behavior: when a check throws, only that check is called\\n+      // The dependent check is skipped because its dependency failed\\n+      expect(mockProvider.execute).toHaveBeenCalledTimes(1);\\n+\\n+      // Should have error issue from failed security check\\n       const errorIssues = (result.reviewSummary.issues || []).filter(issue =>\\n-        issue.ruleId?.includes('error')\\n+        issue.message?.includes('Security check failed') || issue.ruleId?.includes('error')\\n       );\\n-      expect(errorIssues).toHaveLength(1);\\n-      expect(errorIssues[0].message).toContain('Security check failed');\\n+      expect(errorIssues.length).toBeGreaterThan(0);\\n     });\\n \\n     it('should include dependency execution statistics in debug output', async () => {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/cli/check-execution-engine.test.ts\",\"additions\":3,\"deletions\":2,\"changes\":183,\"patch\":\"diff --git a/tests/unit/cli/check-execution-engine.test.ts b/tests/unit/cli/check-execution-engine.test.ts\\nindex f63e0d09..ab781d77 100644\\n--- a/tests/unit/cli/check-execution-engine.test.ts\\n+++ b/tests/unit/cli/check-execution-engine.test.ts\\n@@ -21,6 +21,7 @@ describe('CheckExecutionEngine', () => {\\n   let mockGitAnalyzer: jest.Mocked<GitRepositoryAnalyzer>;\\n   let mockReviewer: jest.Mocked<PRReviewer>;\\n   let mockRegistry: jest.Mocked<CheckProviderRegistry>;\\n+  let mockAIProvider: any;\\n \\n   const mockRepositoryInfo: GitRepositoryInfo = {\\n     title: 'Test Repository',\\n@@ -73,10 +74,31 @@ describe('CheckExecutionEngine', () => {\\n     mockGitAnalyzer = new GitRepositoryAnalyzer() as jest.Mocked<GitRepositoryAnalyzer>;\\n     mockReviewer = new PRReviewer(null as any) as jest.Mocked<PRReviewer>;\\n \\n-    // Mock registry to return false for hasProvider so it falls back to PRReviewer\\n+    // Mock the AI provider to return a simple review summary\\n+    mockAIProvider = {\\n+      getName: jest.fn().mockReturnValue('ai'),\\n+      getDescription: jest.fn().mockReturnValue('AI provider'),\\n+      validateConfig: jest.fn().mockResolvedValue(true),\\n+      getSupportedConfigKeys: jest.fn().mockReturnValue([]),\\n+      isAvailable: jest.fn().mockResolvedValue(true),\\n+      getRequirements: jest.fn().mockReturnValue([]),\\n+      execute: jest.fn().mockImplementation(async () => ({\\n+        issues: [\\n+          {\\n+            category: 'security',\\n+            message: 'Potential security issue',\\n+            severity: 'error',\\n+            file: 'src/test.ts',\\n+            line: 10,\\n+          },\\n+        ],\\n+      })),\\n+    };\\n+\\n+    // Mock registry to return the AI provider\\n     mockRegistry = {\\n-      hasProvider: jest.fn().mockReturnValue(false),\\n-      getProviderOrThrow: jest.fn(),\\n+      hasProvider: jest.fn().mockReturnValue(true),\\n+      getProviderOrThrow: jest.fn().mockReturnValue(mockAIProvider),\\n       getAvailableProviders: jest.fn().mockReturnValue(['ai', 'tool', 'script', 'webhook']),\\n     } as any;\\n \\n@@ -94,13 +116,15 @@ describe('CheckExecutionEngine', () => {\\n   describe('Constructor', () => {\\n     it('should initialize with default working directory', () => {\\n       const engine = new CheckExecutionEngine();\\n-      expect(GitRepositoryAnalyzer).toHaveBeenCalledWith(process.cwd());\\n+      // State machine engine just stores the working directory, doesn't create analyzer in constructor\\n+      expect(engine).toBeDefined();\\n     });\\n \\n     it('should initialize with custom working directory', () => {\\n       const customDir = '/custom/work/dir';\\n       const engine = new CheckExecutionEngine(customDir);\\n-      expect(GitRepositoryAnalyzer).toHaveBeenCalledWith(customDir);\\n+      // State machine engine just stores the working directory, doesn't create analyzer in constructor\\n+      expect(engine).toBeDefined();\\n     });\\n   });\\n \\n@@ -142,19 +166,10 @@ describe('CheckExecutionEngine', () => {\\n       const result = await checkEngine.executeChecks(options);\\n \\n       expect(mockGitAnalyzer.analyzeRepository).toHaveBeenCalled();\\n-      expect(mockReviewer.reviewPR).toHaveBeenCalledWith(\\n-        'local',\\n-        'repository',\\n-        0,\\n-        expect.any(Object),\\n-        expect.objectContaining({\\n-          focus: 'all',\\n-          format: 'table',\\n-        })\\n-      );\\n \\n+      // Verify the result structure and content\\n       expect(result.repositoryInfo).toEqual(mockRepositoryInfo);\\n-      expect(result.reviewSummary).toEqual(mockReviewSummary);\\n+      expect(result.reviewSummary.issues).toBeDefined();\\n       expect(result.checksExecuted).toEqual(['security', 'performance']);\\n       expect(result.executionTime).toBeGreaterThanOrEqual(0);\\n       expect(result.timestamp).toBeDefined();\\n@@ -165,18 +180,11 @@ describe('CheckExecutionEngine', () => {\\n         checks: ['security'],\\n       };\\n \\n-      await checkEngine.executeChecks(options);\\n-\\n-      expect(mockReviewer.reviewPR).toHaveBeenCalledWith(\\n-        'local',\\n-        'repository',\\n-        0,\\n-        expect.any(Object),\\n-        expect.objectContaining({\\n-          focus: 'security',\\n-          format: 'table',\\n-        })\\n-      );\\n+      const result = await checkEngine.executeChecks(options);\\n+\\n+      // Verify the check was executed\\n+      expect(result.checksExecuted).toEqual(['security']);\\n+      expect(result.reviewSummary.issues).toBeDefined();\\n     });\\n \\n     it('should handle single performance check', async () => {\\n@@ -184,18 +192,11 @@ describe('CheckExecutionEngine', () => {\\n         checks: ['performance'],\\n       };\\n \\n-      await checkEngine.executeChecks(options);\\n-\\n-      expect(mockReviewer.reviewPR).toHaveBeenCalledWith(\\n-        'local',\\n-        'repository',\\n-        0,\\n-        expect.any(Object),\\n-        expect.objectContaining({\\n-          focus: 'performance',\\n-          format: 'table',\\n-        })\\n-      );\\n+      const result = await checkEngine.executeChecks(options);\\n+\\n+      // Verify the check was executed\\n+      expect(result.checksExecuted).toEqual(['performance']);\\n+      expect(result.reviewSummary.issues).toBeDefined();\\n     });\\n \\n     it('should handle single style check', async () => {\\n@@ -203,18 +204,11 @@ describe('CheckExecutionEngine', () => {\\n         checks: ['style'],\\n       };\\n \\n-      await checkEngine.executeChecks(options);\\n-\\n-      expect(mockReviewer.reviewPR).toHaveBeenCalledWith(\\n-        'local',\\n-        'repository',\\n-        0,\\n-        expect.any(Object),\\n-        expect.objectContaining({\\n-          focus: 'style',\\n-          format: 'table',\\n-        })\\n-      );\\n+      const result = await checkEngine.executeChecks(options);\\n+\\n+      // Verify the check was executed\\n+      expect(result.checksExecuted).toEqual(['style']);\\n+      expect(result.reviewSummary.issues).toBeDefined();\\n     });\\n \\n     it('should pass timeout option to check execution', async () => {\\n@@ -223,11 +217,11 @@ describe('CheckExecutionEngine', () => {\\n         timeout: 300000, // 5 minutes\\n       };\\n \\n-      await checkEngine.executeChecks(options);\\n+      const result = await checkEngine.executeChecks(options);\\n \\n-      // Since we're using PRReviewer, the timeout should be stored but we can't directly test it\\n-      // without mocking the AI service. For now, we just verify the call happened\\n-      expect(mockReviewer.reviewPR).toHaveBeenCalled();\\n+      // Verify the execution completed successfully\\n+      expect(result.checksExecuted).toEqual(['security']);\\n+      expect(result.reviewSummary.issues).toBeDefined();\\n     });\\n \\n     it('should handle timeout option with default value', async () => {\\n@@ -236,9 +230,11 @@ describe('CheckExecutionEngine', () => {\\n         timeout: undefined, // Should use default (600000ms)\\n       };\\n \\n-      await checkEngine.executeChecks(options);\\n+      const result = await checkEngine.executeChecks(options);\\n \\n-      expect(mockReviewer.reviewPR).toHaveBeenCalled();\\n+      // Verify the execution completed successfully\\n+      expect(result.checksExecuted).toEqual(['performance']);\\n+      expect(result.reviewSummary.issues).toBeDefined();\\n     });\\n \\n     it('should accept various timeout values', async () => {\\n@@ -252,8 +248,9 @@ describe('CheckExecutionEngine', () => {\\n           timeout,\\n         };\\n \\n-        await checkEngine.executeChecks(options);\\n-        expect(mockReviewer.reviewPR).toHaveBeenCalled();\\n+        const result = await checkEngine.executeChecks(options);\\n+        expect(result.checksExecuted).toEqual(['all']);\\n+        expect(result.reviewSummary.issues).toBeDefined();\\n       }\\n     });\\n \\n@@ -262,18 +259,11 @@ describe('CheckExecutionEngine', () => {\\n         checks: ['all'],\\n       };\\n \\n-      await checkEngine.executeChecks(options);\\n-\\n-      expect(mockReviewer.reviewPR).toHaveBeenCalledWith(\\n-        'local',\\n-        'repository',\\n-        0,\\n-        expect.any(Object),\\n-        expect.objectContaining({\\n-          focus: 'all',\\n-          format: 'table',\\n-        })\\n-      );\\n+      const result = await checkEngine.executeChecks(options);\\n+\\n+      // Verify the check was executed\\n+      expect(result.checksExecuted).toEqual(['all']);\\n+      expect(result.reviewSummary.issues).toBeDefined();\\n     });\\n \\n     it('should handle architecture check (mapped to all)', async () => {\\n@@ -281,18 +271,11 @@ describe('CheckExecutionEngine', () => {\\n         checks: ['architecture'],\\n       };\\n \\n-      await checkEngine.executeChecks(options);\\n-\\n-      expect(mockReviewer.reviewPR).toHaveBeenCalledWith(\\n-        'local',\\n-        'repository',\\n-        0,\\n-        expect.any(Object),\\n-        expect.objectContaining({\\n-          focus: 'all',\\n-          format: 'table',\\n-        })\\n-      );\\n+      const result = await checkEngine.executeChecks(options);\\n+\\n+      // Verify the check was executed\\n+      expect(result.checksExecuted).toEqual(['architecture']);\\n+      expect(result.reviewSummary.issues).toBeDefined();\\n     });\\n \\n     it('should handle multiple mixed checks', async () => {\\n@@ -300,18 +283,11 @@ describe('CheckExecutionEngine', () => {\\n         checks: ['security', 'performance', 'style'],\\n       };\\n \\n-      await checkEngine.executeChecks(options);\\n-\\n-      expect(mockReviewer.reviewPR).toHaveBeenCalledWith(\\n-        'local',\\n-        'repository',\\n-        0,\\n-        expect.any(Object),\\n-        expect.objectContaining({\\n-          focus: 'all',\\n-          format: 'table',\\n-        })\\n-      );\\n+      const result = await checkEngine.executeChecks(options);\\n+\\n+      // Verify all checks were executed\\n+      expect(result.checksExecuted).toEqual(['security', 'performance', 'style']);\\n+      expect(result.reviewSummary.issues).toBeDefined();\\n     });\\n \\n     it('should handle non-git repository', async () => {\\n@@ -350,7 +326,18 @@ describe('CheckExecutionEngine', () => {\\n     });\\n \\n     it('should handle reviewer errors', async () => {\\n-      mockReviewer.reviewPR.mockRejectedValue(new Error('Review failed'));\\n+      // Update the mock provider to throw an error\\n+      const mockAIProviderWithError = {\\n+        getName: jest.fn().mockReturnValue('ai'),\\n+        getDescription: jest.fn().mockReturnValue('AI provider'),\\n+        validateConfig: jest.fn().mockResolvedValue(true),\\n+        getSupportedConfigKeys: jest.fn().mockReturnValue([]),\\n+        isAvailable: jest.fn().mockResolvedValue(true),\\n+        getRequirements: jest.fn().mockReturnValue([]),\\n+        execute: jest.fn().mockRejectedValue(new Error('Review failed')),\\n+      };\\n+\\n+      mockRegistry.getProviderOrThrow.mockReturnValue(mockAIProviderWithError);\\n \\n       const options: CheckExecutionOptions = {\\n         checks: ['security'],\\n@@ -359,7 +346,7 @@ describe('CheckExecutionEngine', () => {\\n       const result = await checkEngine.executeChecks(options);\\n \\n       expect(result.reviewSummary.issues).toHaveLength(1);\\n-      expect(result.reviewSummary.issues![0].message).toBe('Review failed');\\n+      expect(result.reviewSummary.issues![0].message).toContain('Review failed');\\n       expect(result.reviewSummary.issues![0].ruleId).toBe('system/error');\\n     });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/cron-scheduler.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":10,\"patch\":\"diff --git a/tests/unit/cron-scheduler.test.ts b/tests/unit/cron-scheduler.test.ts\\nindex b0bd765d..46dc168c 100644\\n--- a/tests/unit/cron-scheduler.test.ts\\n+++ b/tests/unit/cron-scheduler.test.ts\\n@@ -1,17 +1,17 @@\\n import { CronScheduler } from '../../src/cron-scheduler';\\n import * as cron from 'node-cron';\\n-import { CheckExecutionEngine } from '../../src/check-execution-engine';\\n+import { StateMachineExecutionEngine } from '../../src/state-machine-execution-engine';\\n import { VisorConfig } from '../../src/types/config';\\n \\n // Mock node-cron\\n jest.mock('node-cron');\\n \\n-// Mock CheckExecutionEngine\\n-jest.mock('../../src/check-execution-engine');\\n+// Mock StateMachineExecutionEngine\\n+jest.mock('../../src/state-machine-execution-engine');\\n \\n describe('CronScheduler', () => {\\n   let scheduler: CronScheduler;\\n-  let mockExecutionEngine: jest.Mocked<CheckExecutionEngine>;\\n+  let mockExecutionEngine: jest.Mocked<StateMachineExecutionEngine>;\\n   let mockConfig: VisorConfig;\\n   let mockCronTasks: Map<string, { start: jest.Mock; stop: jest.Mock }>;\\n \\n@@ -26,7 +26,7 @@ describe('CronScheduler', () => {\\n       isGitRepository: jest.fn(),\\n       evaluateFailureConditions: jest.fn(),\\n       getRepositoryStatus: jest.fn(),\\n-    } as unknown as jest.Mocked<CheckExecutionEngine>;\\n+    } as unknown as jest.Mocked<StateMachineExecutionEngine>;\\n \\n     mockConfig = {\\n       version: '1.0',\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/custom-tools.test.ts\",\"additions\":10,\"deletions\":0,\"changes\":364,\"patch\":\"diff --git a/tests/unit/custom-tools.test.ts b/tests/unit/custom-tools.test.ts\\nnew file mode 100644\\nindex 00000000..b3c4c2fa\\n--- /dev/null\\n+++ b/tests/unit/custom-tools.test.ts\\n@@ -0,0 +1,364 @@\\n+import { CustomToolExecutor } from '../../src/providers/custom-tool-executor';\\n+import { CustomToolDefinition } from '../../src/types/config';\\n+\\n+describe('CustomToolExecutor', () => {\\n+  let executor: CustomToolExecutor;\\n+\\n+  beforeEach(() => {\\n+    executor = new CustomToolExecutor();\\n+  });\\n+\\n+  describe('tool registration', () => {\\n+    it('should register a custom tool', () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'test-tool',\\n+        description: 'A test tool',\\n+        exec: 'echo \\\"hello\\\"',\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const registeredTool = executor.getTool('test-tool');\\n+\\n+      expect(registeredTool).toBeDefined();\\n+      expect(registeredTool?.name).toBe('test-tool');\\n+      expect(registeredTool?.description).toBe('A test tool');\\n+    });\\n+\\n+    it('should register multiple tools', () => {\\n+      const tools: Record<string, CustomToolDefinition> = {\\n+        tool1: {\\n+          name: 'tool1',\\n+          exec: 'echo \\\"tool1\\\"',\\n+        },\\n+        tool2: {\\n+          name: 'tool2',\\n+          exec: 'echo \\\"tool2\\\"',\\n+        },\\n+      };\\n+\\n+      executor.registerTools(tools);\\n+\\n+      expect(executor.getTools()).toHaveLength(2);\\n+      expect(executor.getTool('tool1')).toBeDefined();\\n+      expect(executor.getTool('tool2')).toBeDefined();\\n+    });\\n+\\n+    it('should throw error when registering tool without name', () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: '',\\n+        exec: 'echo \\\"test\\\"',\\n+      };\\n+\\n+      expect(() => executor.registerTool(tool)).toThrow('Tool must have a name');\\n+    });\\n+  });\\n+\\n+  describe('input validation', () => {\\n+    it('should validate required fields', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'validate-tool',\\n+        exec: 'echo \\\"test\\\"',\\n+        inputSchema: {\\n+          type: 'object',\\n+          properties: {\\n+            name: { type: 'string' },\\n+            age: { type: 'number' },\\n+          },\\n+          required: ['name'],\\n+        },\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+\\n+      // Missing required field should throw\\n+      await expect(executor.execute('validate-tool', { age: 25 }, {})).rejects.toThrow(\\n+        \\\"Input validation failed for tool 'validate-tool': must have required property 'name'\\\"\\n+      );\\n+\\n+      // With required field should work\\n+      await expect(\\n+        executor.execute('validate-tool', { name: 'John', age: 25 }, {})\\n+      ).resolves.toBeDefined();\\n+    });\\n+\\n+    it('should reject unknown properties when additionalProperties is false', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'strict-tool',\\n+        exec: 'echo \\\"test\\\"',\\n+        inputSchema: {\\n+          type: 'object',\\n+          properties: {\\n+            name: { type: 'string' },\\n+          },\\n+          additionalProperties: false,\\n+        },\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+\\n+      await expect(\\n+        executor.execute('strict-tool', { name: 'John', extra: 'field' }, {})\\n+      ).rejects.toThrow('Input validation failed');\\n+    });\\n+\\n+    it('should validate data types according to JSON Schema', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'typed-tool',\\n+        exec: 'echo \\\"test\\\"',\\n+        inputSchema: {\\n+          type: 'object',\\n+          properties: {\\n+            name: { type: 'string' },\\n+            age: { type: 'number' },\\n+            active: { type: 'boolean' },\\n+            tags: {\\n+              type: 'array',\\n+              items: { type: 'string' },\\n+            },\\n+          },\\n+          required: ['name', 'age'],\\n+        },\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+\\n+      // Invalid: age is string instead of number\\n+      await expect(executor.execute('typed-tool', { name: 'John', age: '25' }, {})).rejects.toThrow(\\n+        'Input validation failed'\\n+      );\\n+\\n+      // Invalid: active is string instead of boolean\\n+      await expect(\\n+        executor.execute('typed-tool', { name: 'John', age: 25, active: 'yes' }, {})\\n+      ).rejects.toThrow('Input validation failed');\\n+\\n+      // Invalid: tags contains numbers instead of strings\\n+      await expect(\\n+        executor.execute('typed-tool', { name: 'John', age: 25, tags: [1, 2, 3] }, {})\\n+      ).rejects.toThrow('Input validation failed');\\n+\\n+      // Valid: all types are correct\\n+      await expect(\\n+        executor.execute(\\n+          'typed-tool',\\n+          { name: 'John', age: 25, active: true, tags: ['a', 'b'] },\\n+          {}\\n+        )\\n+      ).resolves.toBeDefined();\\n+    });\\n+  });\\n+\\n+  describe('tool execution', () => {\\n+    it('should execute a simple command', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'echo-tool',\\n+        exec: 'echo \\\"Hello World\\\"',\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const result = await executor.execute('echo-tool', {}, {});\\n+\\n+      expect(result).toContain('Hello World');\\n+    });\\n+\\n+    it('should pass arguments via Liquid templates', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'greet-tool',\\n+        exec: 'echo \\\"Hello {{ args.name }}\\\"',\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const result = await executor.execute('greet-tool', { name: 'Alice' }, {});\\n+\\n+      expect(result).toContain('Hello Alice');\\n+    });\\n+\\n+    it('should handle stdin input', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'cat-tool',\\n+        exec: 'cat',\\n+        stdin: 'Input from stdin: {{ args.message }}',\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const result = await executor.execute('cat-tool', { message: 'test message' }, {});\\n+\\n+      expect(result).toContain('Input from stdin: test message');\\n+    });\\n+\\n+    it('should parse JSON output when requested', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'json-tool',\\n+        exec: 'echo \\\\'{\\\"status\\\": \\\"success\\\", \\\"count\\\": 42}\\\\'',\\n+        parseJson: true,\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const result = await executor.execute('json-tool', {}, {});\\n+\\n+      expect(result).toEqual({ status: 'success', count: 42 });\\n+    });\\n+\\n+    it('should apply Liquid transform to output', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'transform-tool',\\n+        exec: 'echo \\\"raw output\\\"',\\n+        transform: '{ \\\"processed\\\": \\\"{{ output | strip }}\\\" }',\\n+        parseJson: true,\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const result = await executor.execute('transform-tool', {}, {});\\n+\\n+      expect(result).toEqual({ processed: 'raw output' });\\n+    });\\n+\\n+    it('should apply JavaScript transform to output', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'js-transform-tool',\\n+        exec: 'echo \\\"10\\\"',\\n+        transform_js: 'return parseInt(output.trim()) * 2;',\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const result = await executor.execute('js-transform-tool', {}, {});\\n+\\n+      expect(result).toBe(20);\\n+    });\\n+\\n+    it('should handle command timeout', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'slow-tool',\\n+        exec: 'sleep 5',\\n+        timeout: 100, // 100ms timeout\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+\\n+      // The command should throw a timeout error\\n+      await expect(executor.execute('slow-tool', {}, {})).rejects.toThrow(\\n+        'Command timed out after 100ms'\\n+      );\\n+    });\\n+\\n+    it('should throw error for non-existent tool', async () => {\\n+      await expect(executor.execute('non-existent', {}, {})).rejects.toThrow(\\n+        'Tool not found: non-existent'\\n+      );\\n+    });\\n+  });\\n+\\n+  describe('context usage', () => {\\n+    it('should provide PR context to tools', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'pr-tool',\\n+        exec: 'echo \\\"PR #{{ pr.number }}: {{ pr.title }}\\\"',\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const result = await executor.execute(\\n+        'pr-tool',\\n+        {},\\n+        {\\n+          pr: {\\n+            number: 123,\\n+            title: 'Add new feature',\\n+            author: 'john',\\n+            branch: 'feature/test',\\n+            base: 'main',\\n+          },\\n+        }\\n+      );\\n+\\n+      expect(result).toContain('PR #123: Add new feature');\\n+    });\\n+\\n+    it('should provide file list to tools', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'file-tool',\\n+        exec: 'echo \\\"Files: {{ files | size }}\\\"',\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const result = await executor.execute(\\n+        'file-tool',\\n+        {},\\n+        {\\n+          files: ['file1.js', 'file2.ts', 'file3.py'],\\n+        }\\n+      );\\n+\\n+      expect(result).toContain('Files: 3');\\n+    });\\n+\\n+    it('should provide outputs from previous checks', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'output-tool',\\n+        exec: 'echo \\\"Previous result: {{ outputs.check1 }}\\\"',\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const result = await executor.execute(\\n+        'output-tool',\\n+        {},\\n+        {\\n+          outputs: {\\n+            check1: 'success',\\n+            check2: 'pending',\\n+          },\\n+        }\\n+      );\\n+\\n+      expect(result).toContain('Previous result: success');\\n+    });\\n+  });\\n+\\n+  describe('MCP tool conversion', () => {\\n+    it('should convert custom tools to MCP tool format', () => {\\n+      const tools: Record<string, CustomToolDefinition> = {\\n+        tool1: {\\n+          name: 'tool1',\\n+          description: 'First tool',\\n+          exec: 'echo \\\"tool1\\\"',\\n+          inputSchema: {\\n+            type: 'object',\\n+            properties: {\\n+              param: { type: 'string' },\\n+            },\\n+          },\\n+        },\\n+        tool2: {\\n+          name: 'tool2',\\n+          description: 'Second tool',\\n+          exec: 'echo \\\"tool2\\\"',\\n+        },\\n+      };\\n+\\n+      executor.registerTools(tools);\\n+      const mcpTools = executor.toMcpTools();\\n+\\n+      expect(mcpTools).toHaveLength(2);\\n+      expect(mcpTools[0].name).toBe('tool1');\\n+      expect(mcpTools[0].description).toBe('First tool');\\n+      expect(mcpTools[0].inputSchema).toBeDefined();\\n+      expect(mcpTools[0].handler).toBeDefined();\\n+\\n+      expect(mcpTools[1].name).toBe('tool2');\\n+      expect(mcpTools[1].description).toBe('Second tool');\\n+    });\\n+\\n+    it('should execute through MCP tool handler', async () => {\\n+      const tool: CustomToolDefinition = {\\n+        name: 'handler-tool',\\n+        exec: 'echo \\\"Result: {{ args.value }}\\\"',\\n+      };\\n+\\n+      executor.registerTool(tool);\\n+      const mcpTools = executor.toMcpTools();\\n+      const handler = mcpTools[0].handler;\\n+\\n+      const result = await handler({ value: 'test' });\\n+      expect(result).toContain('Result: test');\\n+    });\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/unit/engine-onfinish-utils.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":3,\"patch\":\"diff --git a/tests/unit/engine-onfinish-utils.test.ts b/tests/unit/engine-onfinish-utils.test.ts\\nindex 5e16e78f..ee4d1db6 100644\\n--- a/tests/unit/engine-onfinish-utils.test.ts\\n+++ b/tests/unit/engine-onfinish-utils.test.ts\\n@@ -19,7 +19,7 @@ describe('OnFinish utils', () => {\\n     expect(p.outputsHistoryForContext.a).toHaveLength(1);\\n   });\\n \\n-  test('composeOnFinishContext includes memory/env and step metadata', () => {\\n+  test('composeOnFinishContext includes env and step metadata', () => {\\n     const ctx = composeOnFinishContext(\\n       undefined,\\n       'extract-facts',\\n@@ -39,7 +39,6 @@ describe('OnFinish utils', () => {\\n     );\\n     expect(ctx.step.id).toBe('extract-facts');\\n     expect(Array.isArray(ctx.outputs_history.validate)).toBe(true);\\n-    expect(typeof ctx.memory.get).toBe('function');\\n     expect(ctx.event.name).toBe('issue_opened');\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/execution-statistics-formatting.test.ts\",\"additions\":1,\"deletions\":0,\"changes\":16,\"patch\":\"diff --git a/tests/unit/execution-statistics-formatting.test.ts b/tests/unit/execution-statistics-formatting.test.ts\\nindex a3a37c2c..1c25f04a 100644\\n--- a/tests/unit/execution-statistics-formatting.test.ts\\n+++ b/tests/unit/execution-statistics-formatting.test.ts\\n@@ -15,6 +15,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 1,\\n         successfulRuns: 1,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 1000,\\n         issuesFound: 0,\\n@@ -31,6 +32,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 5,\\n         successfulRuns: 5,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 5000,\\n         issuesFound: 0,\\n@@ -47,6 +49,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 3,\\n         successfulRuns: 0,\\n         failedRuns: 3,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 3000,\\n         issuesFound: 0,\\n@@ -63,6 +66,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 5,\\n         successfulRuns: 3,\\n         failedRuns: 2,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 5000,\\n         issuesFound: 0,\\n@@ -79,6 +83,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 0,\\n         successfulRuns: 0,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: true,\\n         skipReason: 'if_condition',\\n         totalDuration: 0,\\n@@ -96,6 +101,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 0,\\n         successfulRuns: 0,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: true,\\n         skipReason: 'fail_fast',\\n         totalDuration: 0,\\n@@ -113,6 +119,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 0,\\n         successfulRuns: 0,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: true,\\n         skipReason: 'dependency_failed',\\n         totalDuration: 0,\\n@@ -132,6 +139,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 1,\\n         successfulRuns: 1,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 1000,\\n         issuesFound: 0,\\n@@ -149,6 +157,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 1,\\n         successfulRuns: 1,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 1000,\\n         issuesFound: 3,\\n@@ -165,6 +174,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 1,\\n         successfulRuns: 1,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 1000,\\n         issuesFound: 12,\\n@@ -181,6 +191,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 1,\\n         successfulRuns: 1,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 1000,\\n         issuesFound: 5,\\n@@ -197,6 +208,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 1,\\n         successfulRuns: 1,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 1000,\\n         issuesFound: 7,\\n@@ -214,6 +226,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 1,\\n         successfulRuns: 0,\\n         failedRuns: 1,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 1000,\\n         issuesFound: 0,\\n@@ -231,6 +244,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 1,\\n         successfulRuns: 0,\\n         failedRuns: 1,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 1000,\\n         issuesFound: 0,\\n@@ -249,6 +263,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 0,\\n         successfulRuns: 0,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: true,\\n         skipReason: 'if_condition',\\n         skipCondition: 'branch == \\\"main\\\"',\\n@@ -267,6 +282,7 @@ describe('Execution Statistics Formatting', () => {\\n         totalRuns: 5,\\n         successfulRuns: 5,\\n         failedRuns: 0,\\n+        skippedRuns: 0,\\n         skipped: false,\\n         totalDuration: 5000,\\n         issuesFound: 15,\\n\",\"status\":\"added\"},{\"filename\":\"tests/unit/foreach-custom-schema-integration.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":43,\"patch\":\"diff --git a/tests/unit/foreach-custom-schema-integration.test.ts b/tests/unit/foreach-custom-schema-integration.test.ts\\nindex 51206287..8b78b170 100644\\n--- a/tests/unit/foreach-custom-schema-integration.test.ts\\n+++ b/tests/unit/foreach-custom-schema-integration.test.ts\\n@@ -3,18 +3,36 @@ import { CommandCheckProvider } from '../../src/providers/command-check-provider\\n import { CheckProviderConfig } from '../../src/providers/check-provider.interface';\\n import { PRInfo } from '../../src/pr-analyzer';\\n import { ReviewSummary } from '../../src/reviewer';\\n+import { CommandExecutionResult, CommandExecutionOptions } from '../../src/utils/command-executor';\\n+\\n+// First, create the mock functions with the factory pattern\\n+jest.mock('../../src/utils/command-executor', () => {\\n+  const mockExecute =\\n+    jest.fn<\\n+      (command: string, options?: CommandExecutionOptions) => Promise<CommandExecutionResult>\\n+    >();\\n+  const mockBuildEnvironment = jest.fn().mockReturnValue({});\\n+\\n+  return {\\n+    commandExecutor: {\\n+      execute: mockExecute,\\n+      buildEnvironment: mockBuildEnvironment,\\n+    },\\n+    // Export mocks for test access\\n+    __mockExecute: mockExecute,\\n+    __mockBuildEnvironment: mockBuildEnvironment,\\n+  };\\n+});\\n \\n-// Mock child_process\\n-const mockExec = jest.fn() as jest.MockedFunction<any>;\\n-const mockPromisify = jest.fn().mockReturnValue(mockExec);\\n-\\n-jest.mock('child_process', () => ({\\n-  exec: jest.fn(),\\n-}));\\n-\\n-jest.mock('util', () => ({\\n-  promisify: mockPromisify,\\n-}));\\n+// Import the mocked module to get the mock functions\\n+const mockModule = jest.requireMock('../../src/utils/command-executor') as {\\n+  __mockExecute: jest.MockedFunction<\\n+    (command: string, options?: CommandExecutionOptions) => Promise<CommandExecutionResult>\\n+  >;\\n+  __mockBuildEnvironment: jest.MockedFunction<() => Record<string, string>>;\\n+};\\n+const mockExecute = mockModule.__mockExecute;\\n+// mockBuildEnvironment is defined but not used in tests\\n \\n /**\\n  * Test: forEach with Custom Schema Integration\\n@@ -69,9 +87,10 @@ describe('forEach with Custom Schema Integration', () => {\\n       ],\\n     } as ReviewSummary & { output: unknown });\\n \\n-    mockExec.mockResolvedValue({\\n+    mockExecute.mockResolvedValue({\\n       stdout: 'test\\\\n',\\n       stderr: '',\\n+      exitCode: 0,\\n     });\\n \\n     const result = await provider.execute(mockPRInfo, config, dependencyResults);\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/github-comments.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":10,\"patch\":\"diff --git a/tests/unit/github-comments.test.ts b/tests/unit/github-comments.test.ts\\nindex 7b7fd179..f5649528 100644\\n--- a/tests/unit/github-comments.test.ts\\n+++ b/tests/unit/github-comments.test.ts\\n@@ -339,8 +339,9 @@ describe('CommentManager', () => {\\n \\n       const result = commentManager.formatGroupedResults(results, 'check');\\n \\n-      expect(result).toContain('üìà performance Review (Score: 75/100) - 5 issues found');\\n-      expect(result).toContain('üîí security Review (Score: 90/100) - 1 issues found');\\n+      // Titles no longer include step/category emojis; assert plain headings\\n+      expect(result).toContain('performance Review (Score: 75/100) - 5 issues found');\\n+      expect(result).toContain('security Review (Score: 90/100) - 1 issues found');\\n       expect(result).toContain('<details open>'); // Should expand sections with issues\\n     });\\n \\n@@ -352,8 +353,9 @@ describe('CommentManager', () => {\\n \\n       const result = commentManager.formatGroupedResults(results, 'severity');\\n \\n-      expect(result).toContain('üëç Good Review');\\n-      expect(result).toContain('üö® Critical Issues Review');\\n+      // Group headers do not include severity emojis; plain titles are used\\n+      expect(result).toContain('Good Review');\\n+      expect(result).toContain('Critical Issues Review');\\n     });\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/mcp-provider.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":41,\"patch\":\"diff --git a/tests/unit/mcp-provider.test.ts b/tests/unit/mcp-provider.test.ts\\nindex 1fa1cb38..1e546782 100644\\n--- a/tests/unit/mcp-provider.test.ts\\n+++ b/tests/unit/mcp-provider.test.ts\\n@@ -17,7 +17,8 @@ describe('MCP Check Provider', () => {\\n       const description = provider.getDescription();\\n       expect(description).toContain('stdio');\\n       expect(description).toContain('SSE');\\n-      expect(description).toContain('Streamable HTTP');\\n+      expect(description).toContain('HTTP');\\n+      expect(description).toContain('custom');\\n     });\\n \\n     it('should be available', async () => {\\n@@ -156,6 +157,44 @@ describe('MCP Check Provider', () => {\\n       });\\n     });\\n \\n+    describe('custom transport', () => {\\n+      it('should accept custom transport with method', async () => {\\n+        const config = {\\n+          type: 'mcp',\\n+          transport: 'custom',\\n+          method: 'my-custom-tool',\\n+        };\\n+\\n+        const result = await provider.validateConfig(config);\\n+        expect(result).toBe(true);\\n+      });\\n+\\n+      it('should reject custom transport without method', async () => {\\n+        const config = {\\n+          type: 'mcp',\\n+          transport: 'custom',\\n+        };\\n+\\n+        const result = await provider.validateConfig(config);\\n+        expect(result).toBe(false);\\n+      });\\n+\\n+      it('should accept custom transport with methodArgs', async () => {\\n+        const config = {\\n+          type: 'mcp',\\n+          transport: 'custom',\\n+          method: 'my-custom-tool',\\n+          methodArgs: {\\n+            param1: 'value1',\\n+            param2: 123,\\n+          },\\n+        };\\n+\\n+        const result = await provider.validateConfig(config);\\n+        expect(result).toBe(true);\\n+      });\\n+    });\\n+\\n     describe('invalid transport', () => {\\n       it('should reject invalid transport type', async () => {\\n         const config = {\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/ai-check-provider.test.ts\",\"additions\":5,\"deletions\":0,\"changes\":160,\"patch\":\"diff --git a/tests/unit/providers/ai-check-provider.test.ts b/tests/unit/providers/ai-check-provider.test.ts\\nindex 7d433767..d0b94086 100644\\n--- a/tests/unit/providers/ai-check-provider.test.ts\\n+++ b/tests/unit/providers/ai-check-provider.test.ts\\n@@ -243,6 +243,162 @@ describe('AICheckProvider', () => {\\n         allowEdit: true,\\n       });\\n     });\\n+\\n+    it('should pass allowedTools and disableTools flags to service', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'analyze code structure',\\n+        ai: {\\n+          provider: 'anthropic',\\n+          model: 'claude-3-opus',\\n+          allowedTools: ['Read', 'Grep', 'Glob'],\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'anthropic',\\n+        model: 'claude-3-opus',\\n+        allowedTools: ['Read', 'Grep', 'Glob'],\\n+      });\\n+    });\\n+\\n+    it('should pass disableTools flag to service', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'explain architecture',\\n+        ai: {\\n+          provider: 'openai',\\n+          model: 'gpt-4',\\n+          disableTools: true,\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'openai',\\n+        model: 'gpt-4',\\n+        disableTools: true,\\n+      });\\n+    });\\n+\\n+    it('should pass allowBash to service', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'analyze git status',\\n+        ai: {\\n+          provider: 'anthropic',\\n+          model: 'claude-3-opus',\\n+          allowBash: true,\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'anthropic',\\n+        model: 'claude-3-opus',\\n+        allowBash: true,\\n+      });\\n+    });\\n+\\n+    it('should pass bashConfig with allowBash to service', async () => {\\n+      const mockReview = {\\n+        overallScore: 90,\\n+        totalIssues: 0,\\n+        criticalIssues: 0,\\n+        comments: [],\\n+      };\\n+\\n+      const mockService = {\\n+        executeReview: jest.fn().mockResolvedValue(mockReview),\\n+      };\\n+\\n+      let capturedConfig: any;\\n+      (AIReviewService as any).AIReviewService = jest.fn().mockImplementation(config => {\\n+        capturedConfig = config;\\n+        return mockService;\\n+      });\\n+\\n+      const config: CheckProviderConfig = {\\n+        type: 'ai',\\n+        prompt: 'analyze git status with custom config',\\n+        ai: {\\n+          provider: 'anthropic',\\n+          model: 'claude-3-opus',\\n+          allowBash: true,\\n+          bashConfig: {\\n+            allow: ['git status', 'ls'],\\n+            timeout: 30000,\\n+          },\\n+        },\\n+      };\\n+\\n+      await provider.execute(mockPRInfo, config);\\n+\\n+      expect(capturedConfig).toMatchObject({\\n+        provider: 'anthropic',\\n+        model: 'claude-3-opus',\\n+        allowBash: true,\\n+        bashConfig: {\\n+          allow: ['git status', 'ls'],\\n+          timeout: 30000,\\n+        },\\n+      });\\n+    });\\n   });\\n \\n   describe('getSupportedConfigKeys', () => {\\n@@ -255,6 +411,10 @@ describe('AICheckProvider', () => {\\n       expect(keys).toContain('ai.model');\\n       expect(keys).toContain('ai.enableDelegate');\\n       expect(keys).toContain('ai.allowEdit');\\n+      expect(keys).toContain('ai.allowedTools');\\n+      expect(keys).toContain('ai.disableTools');\\n+      expect(keys).toContain('ai.allowBash');\\n+      expect(keys).toContain('ai.bashConfig');\\n     });\\n   });\\n \\n\",\"status\":\"added\"},{\"filename\":\"tests/unit/providers/check-provider-registry.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":4,\"patch\":\"diff --git a/tests/unit/providers/check-provider-registry.test.ts b/tests/unit/providers/check-provider-registry.test.ts\\nindex 5c0d5fc3..8e5ec3d77 100644\\n--- a/tests/unit/providers/check-provider-registry.test.ts\\n+++ b/tests/unit/providers/check-provider-registry.test.ts\\n@@ -173,8 +173,8 @@ describe('CheckProviderRegistry', () => {\\n       const providers = registry.getAllProviders();\\n       expect(providers).toContain(provider1);\\n       expect(providers).toContain(provider2);\\n-      // Reset adds 13 default providers (ai, command, script, http, http_input, http_client, noop, log, memory, github, claude-code, mcp, human-input) + 2 custom = 15 total\\n-      expect(providers.length).toBe(15);\\n+      // Reset adds 14 default providers (ai, command, script, http, http_input, http_client, noop, log, memory, github, claude-code, mcp, human-input, workflow) + 2 custom = 16 total\\n+      expect(providers.length).toBe(16);\\n     });\\n   });\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/providers/command-check-provider.test.ts\",\"additions\":2,\"deletions\":2,\"changes\":136,\"patch\":\"diff --git a/tests/unit/providers/command-check-provider.test.ts b/tests/unit/providers/command-check-provider.test.ts\\nindex 51703b84..3bf11a84 100644\\n--- a/tests/unit/providers/command-check-provider.test.ts\\n+++ b/tests/unit/providers/command-check-provider.test.ts\\n@@ -3,19 +3,39 @@ import { CommandCheckProvider } from '../../../src/providers/command-check-provi\\n import { CheckProviderConfig } from '../../../src/providers/check-provider.interface';\\n import { PRInfo } from '../../../src/pr-analyzer';\\n import { ReviewSummary } from '../../../src/reviewer';\\n+import {\\n+  CommandExecutionResult,\\n+  CommandExecutionOptions,\\n+} from '../../../src/utils/command-executor';\\n+\\n+// Mock the command executor with factory pattern\\n+jest.mock('../../../src/utils/command-executor', () => {\\n+  const mockExecute =\\n+    jest.fn<\\n+      (command: string, options?: CommandExecutionOptions) => Promise<CommandExecutionResult>\\n+    >();\\n+  const mockBuildEnvironment = jest.fn().mockReturnValue({});\\n+\\n+  return {\\n+    commandExecutor: {\\n+      execute: mockExecute,\\n+      buildEnvironment: mockBuildEnvironment,\\n+    },\\n+    // Export mocks for test access\\n+    __mockExecute: mockExecute,\\n+    __mockBuildEnvironment: mockBuildEnvironment,\\n+  };\\n+});\\n \\n-// Mock child_process and util\\n-// eslint-disable-next-line @typescript-eslint/no-explicit-any\\n-const mockExec = jest.fn() as jest.MockedFunction<any>;\\n-const mockPromisify = jest.fn().mockReturnValue(mockExec);\\n-\\n-jest.mock('child_process', () => ({\\n-  exec: jest.fn(),\\n-}));\\n-\\n-jest.mock('util', () => ({\\n-  promisify: mockPromisify,\\n-}));\\n+// Import the mocked module to get the mock functions\\n+const mockModule = jest.requireMock('../../../src/utils/command-executor') as {\\n+  __mockExecute: jest.MockedFunction<\\n+    (command: string, options?: CommandExecutionOptions) => Promise<CommandExecutionResult>\\n+  >;\\n+  __mockBuildEnvironment: jest.MockedFunction<() => Record<string, string>>;\\n+};\\n+const mockExecute = mockModule.__mockExecute;\\n+// mockBuildEnvironment is defined but not used in tests\\n \\n describe('CommandCheckProvider', () => {\\n   let provider: CommandCheckProvider;\\n@@ -121,9 +141,10 @@ describe('CommandCheckProvider', () => {\\n         exec: 'echo \\\"hello world\\\"',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'hello world\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n@@ -132,10 +153,9 @@ describe('CommandCheckProvider', () => {\\n       expect(result.issues).toEqual([]);\\n       // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n       expect((result as any).output).toBe('hello world');\\n-      expect(mockExec).toHaveBeenCalledWith('echo \\\"hello world\\\"', {\\n+      expect(mockExecute).toHaveBeenCalledWith('echo \\\"hello world\\\"', {\\n         env: expect.any(Object),\\n         timeout: 60000,\\n-        maxBuffer: 10 * 1024 * 1024,\\n       });\\n     });\\n \\n@@ -145,9 +165,10 @@ describe('CommandCheckProvider', () => {\\n         exec: 'echo \\\\'{\\\"items\\\": [\\\"a\\\", \\\"b\\\", \\\"c\\\"]}\\\\'',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: '{\\\"items\\\": [\\\"a\\\", \\\"b\\\", \\\"c\\\"]}\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n@@ -164,7 +185,7 @@ describe('CommandCheckProvider', () => {\\n         exec: 'nonexistent-command',\\n       };\\n \\n-      mockExec.mockRejectedValue(new Error('Command not found'));\\n+      mockExecute.mockRejectedValue(new Error('Command not found'));\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n \\n@@ -191,7 +212,7 @@ describe('CommandCheckProvider', () => {\\n         stdout: 'partial output',\\n       });\\n \\n-      mockExec.mockRejectedValue(error);\\n+      mockExecute.mockRejectedValue(error);\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n \\n@@ -215,9 +236,10 @@ describe('CommandCheckProvider', () => {\\n         exec: 'echo \\\"invalid json {\\\"',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'invalid json {\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n@@ -237,14 +259,18 @@ describe('CommandCheckProvider', () => {\\n         exec: 'echo \\\"PR: {{ pr.title }} by {{ pr.author }}\\\"',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'PR: Test PR by testuser\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n \\n-      expect(mockExec).toHaveBeenCalledWith('echo \\\"PR: Test PR by testuser\\\"', expect.any(Object));\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n+        'echo \\\"PR: Test PR by testuser\\\"',\\n+        expect.any(Object)\\n+      );\\n       expect((result as any).output).toBe('PR: Test PR by testuser');\\n     });\\n \\n@@ -254,14 +280,15 @@ describe('CommandCheckProvider', () => {\\n         exec: 'echo \\\"Files: {{ fileCount }}\\\"',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'Files: 2\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n \\n-      expect(mockExec).toHaveBeenCalledWith('echo \\\"Files: 2\\\"', expect.any(Object));\\n+      expect(mockExecute).toHaveBeenCalledWith('echo \\\"Files: 2\\\"', expect.any(Object));\\n       expect((result as any).output).toBe('Files: 2');\\n     });\\n \\n@@ -271,15 +298,16 @@ describe('CommandCheckProvider', () => {\\n         exec: 'echo \\\"static command\\\"',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'static command\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       // eslint-disable-next-line @typescript-eslint/no-unused-vars\\n       const result = await provider.execute(mockPRInfo, config);\\n \\n-      expect(mockExec).toHaveBeenCalledWith('echo \\\"static command\\\"', expect.any(Object));\\n+      expect(mockExecute).toHaveBeenCalledWith('echo \\\"static command\\\"', expect.any(Object));\\n     });\\n   });\\n \\n@@ -293,19 +321,19 @@ describe('CommandCheckProvider', () => {\\n         },\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'test_value\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       await provider.execute(mockPRInfo, config);\\n \\n-      expect(mockExec).toHaveBeenCalledWith('echo $TEST_VAR', {\\n+      expect(mockExecute).toHaveBeenCalledWith('echo $TEST_VAR', {\\n         env: expect.objectContaining({\\n           TEST_VAR: 'test_value',\\n         }),\\n         timeout: 60000,\\n-        maxBuffer: 10 * 1024 * 1024,\\n       });\\n     });\\n \\n@@ -325,14 +353,19 @@ describe('CommandCheckProvider', () => {\\n         exec: 'env',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'output\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       await provider.execute(mockPRInfo, config);\\n \\n-      const envArg = mockExec.mock.calls[0][1].env;\\n+      expect(mockExecute.mock.calls).toHaveLength(1);\\n+      const callArgs = mockExecute.mock.calls[0];\\n+      expect(callArgs).toBeDefined();\\n+      expect(callArgs[1]).toBeDefined();\\n+      const envArg = callArgs[1]!.env;\\n       expect(envArg).toHaveProperty('CI_BUILD_NUMBER', '123');\\n       expect(envArg).toHaveProperty('GITHUB_REPOSITORY', 'test/repo');\\n       expect(envArg).toHaveProperty('NODE_VERSION', '18.0.0');\\n@@ -353,9 +386,10 @@ describe('CommandCheckProvider', () => {\\n         transform: '{{ output.data | join: \\\",\\\" }}',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: '{\\\"data\\\": [1, 2, 3]}\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n@@ -370,9 +404,10 @@ describe('CommandCheckProvider', () => {\\n         transform: '{{ invalid.liquid.syntax !! }}',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'test\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n@@ -394,9 +429,10 @@ describe('CommandCheckProvider', () => {\\n         transform: '{\\\"transformed\\\": \\\"{{ output }}\\\"}',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'raw text\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n@@ -428,14 +464,15 @@ describe('CommandCheckProvider', () => {\\n         ],\\n       });\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'Dep count: 1\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config, dependencyResults);\\n \\n-      expect(mockExec).toHaveBeenCalledWith('echo \\\"Dep count: 1\\\"', expect.any(Object));\\n+      expect(mockExecute).toHaveBeenCalledWith('echo \\\"Dep count: 1\\\"', expect.any(Object));\\n       expect((result as any).output).toBe('Dep count: 1');\\n     });\\n \\n@@ -451,9 +488,10 @@ describe('CommandCheckProvider', () => {\\n         output: { customData: 'test-value' },\\n       } as ReviewSummary);\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'test-value\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config, dependencyResults);\\n@@ -480,14 +518,15 @@ describe('CommandCheckProvider', () => {\\n         },\\n       } as ReviewSummary);\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'Complexity: high, Priority: 8, Hours: 24\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config, dependencyResults);\\n \\n-      expect(mockExec).toHaveBeenCalledWith(\\n+      expect(mockExecute).toHaveBeenCalledWith(\\n         'echo \\\"Complexity: high, Priority: 8, Hours: 24\\\"',\\n         expect.any(Object)\\n       );\\n@@ -508,9 +547,10 @@ describe('CommandCheckProvider', () => {\\n         exec: 'echo \\\"output\\\" && echo \\\"warning\\\" >&2',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'output\\\\n',\\n         stderr: 'warning\\\\n',\\n+        exitCode: 0,\\n       });\\n \\n       await provider.execute(mockPRInfo, config);\\n@@ -533,9 +573,10 @@ describe('CommandCheckProvider', () => {\\n         exec: 'echo \\\"output\\\" && echo \\\"warning\\\" >&2',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'output\\\\n',\\n         stderr: 'warning\\\\n',\\n+        exitCode: 0,\\n       });\\n \\n       await provider.execute(mockPRInfo, config);\\n@@ -552,17 +593,17 @@ describe('CommandCheckProvider', () => {\\n         exec: 'sleep 1000', // Long running command\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: 'output\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       await provider.execute(mockPRInfo, config);\\n \\n-      expect(mockExec).toHaveBeenCalledWith('sleep 1000', {\\n+      expect(mockExecute).toHaveBeenCalledWith('sleep 1000', {\\n         env: expect.any(Object),\\n         timeout: 60000, // 60 second timeout\\n-        maxBuffer: 10 * 1024 * 1024, // 10MB buffer\\n       });\\n     });\\n   });\\n@@ -580,9 +621,10 @@ describe('CommandCheckProvider', () => {\\n         },\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: JSON.stringify(complexOutput) + '\\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n@@ -596,9 +638,10 @@ describe('CommandCheckProvider', () => {\\n         exec: 'true', // Command that produces no output\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: '',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n@@ -612,9 +655,10 @@ describe('CommandCheckProvider', () => {\\n         exec: 'echo \\\"   content   \\\"',\\n       };\\n \\n-      mockExec.mockResolvedValue({\\n+      mockExecute.mockResolvedValue({\\n         stdout: '   content   \\\\n',\\n         stderr: '',\\n+        exitCode: 0,\\n       });\\n \\n       const result = await provider.execute(mockPRInfo, config);\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/references-link-template.test.ts\",\"additions\":1,\"deletions\":0,\"changes\":30,\"patch\":\"diff --git a/tests/unit/references-link-template.test.ts b/tests/unit/references-link-template.test.ts\\nnew file mode 100644\\nindex 00000000..b477f8b4\\n--- /dev/null\\n+++ b/tests/unit/references-link-template.test.ts\\n@@ -0,0 +1,30 @@\\n+import { createExtendedLiquid } from '../../src/liquid-extensions';\\n+\\n+describe('References example link liquid rendering', () => {\\n+  const tpl =\\n+    'https://github.com/{{ event.repository.fullName }}/blob/{{ event.pull_request.head.sha | default: \\\"HEAD\\\" }}/path/to/file.ext#LSTART-LEND';\\n+\\n+  test('renders HEAD fallback for issue context', async () => {\\n+    const liquid = createExtendedLiquid();\\n+    const out = await liquid.parseAndRender(tpl, {\\n+      event: {\\n+        repository: { owner: { login: 'owner' }, name: 'repo', fullName: 'owner/repo' },\\n+        // No pull_request in issue context\\n+      },\\n+    });\\n+    expect(out).toBe('https://github.com/owner/repo/blob/HEAD/path/to/file.ext#LSTART-LEND');\\n+  });\\n+\\n+  test('renders PR head sha when provided', async () => {\\n+    const liquid = createExtendedLiquid();\\n+    const out = await liquid.parseAndRender(tpl, {\\n+      event: {\\n+        repository: { owner: { login: 'owner' }, name: 'repo', fullName: 'owner/repo' },\\n+        pull_request: { head: { sha: 'deadbeefcafebabe' } },\\n+      },\\n+    });\\n+    expect(out).toBe(\\n+      'https://github.com/owner/repo/blob/deadbeefcafebabe/path/to/file.ext#LSTART-LEND'\\n+    );\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/unit/reviewer.test.ts\",\"additions\":1,\"deletions\":1,\"changes\":2,\"patch\":\"diff --git a/tests/unit/reviewer.test.ts b/tests/unit/reviewer.test.ts\\nindex 8a1150ba..a135e219 100644\\n--- a/tests/unit/reviewer.test.ts\\n+++ b/tests/unit/reviewer.test.ts\\n@@ -191,7 +191,7 @@ jest.mock('../../src/ai-review-service', () => {\\n             file: 'src/test.ts',\\n             line: 5,\\n             ruleId: 'security/dangerous-eval',\\n-            message: 'Dangerous eval usage detected',\\n+            message: 'Dangerous eval usage detected - security vulnerability',\\n             severity: 'critical',\\n             category: 'security',\\n           });\\n\",\"status\":\"modified\"},{\"filename\":\"tests/unit/workflow-check-provider.test.ts\",\"additions\":13,\"deletions\":0,\"changes\":462,\"patch\":\"diff --git a/tests/unit/workflow-check-provider.test.ts b/tests/unit/workflow-check-provider.test.ts\\nnew file mode 100644\\nindex 00000000..42b350bf\\n--- /dev/null\\n+++ b/tests/unit/workflow-check-provider.test.ts\\n@@ -0,0 +1,462 @@\\n+/**\\n+ * Unit tests for WorkflowCheckProvider\\n+ */\\n+\\n+import { WorkflowCheckProvider } from '../../src/providers/workflow-check-provider';\\n+import { WorkflowRegistry } from '../../src/workflow-registry';\\n+import { WorkflowExecutor } from '../../src/workflow-executor';\\n+import { WorkflowDefinition } from '../../src/types/workflow';\\n+import { PRInfo } from '../../src/pr-analyzer';\\n+import { CheckProviderConfig } from '../../src/providers/check-provider.interface';\\n+\\n+// Mock dependencies\\n+jest.mock('../../src/workflow-registry');\\n+jest.mock('../../src/workflow-executor');\\n+jest.mock('../../src/logger', () => ({\\n+  logger: {\\n+    info: jest.fn(),\\n+    error: jest.fn(),\\n+    warn: jest.fn(),\\n+    debug: jest.fn(),\\n+  },\\n+}));\\n+\\n+describe('WorkflowCheckProvider', () => {\\n+  let provider: WorkflowCheckProvider;\\n+  let mockRegistry: jest.Mocked<WorkflowRegistry>;\\n+  let mockExecutor: jest.Mocked<WorkflowExecutor>;\\n+\\n+  const sampleWorkflow: WorkflowDefinition = {\\n+    id: 'test-workflow',\\n+    name: 'Test Workflow',\\n+    inputs: [\\n+      {\\n+        name: 'threshold',\\n+        schema: { type: 'number' },\\n+        default: 80,\\n+      },\\n+      {\\n+        name: 'language',\\n+        schema: { type: 'string' },\\n+        required: true,\\n+      },\\n+    ],\\n+    outputs: [\\n+      {\\n+        name: 'score',\\n+        value_js: 'steps.analyze.output.score',\\n+      },\\n+      {\\n+        name: 'passed',\\n+        value_js: 'steps.analyze.output.score > inputs.threshold',\\n+      },\\n+    ],\\n+    steps: {\\n+      analyze: {\\n+        type: 'ai',\\n+        prompt: 'Analyze {{ inputs.language }} code',\\n+      },\\n+    },\\n+  };\\n+\\n+  const samplePRInfo: PRInfo = {\\n+    number: 123,\\n+    title: 'Test PR',\\n+    body: 'Test description',\\n+    author: 'test-author',\\n+    base: 'main',\\n+    head: 'feature',\\n+    files: [\\n+      {\\n+        filename: 'test.ts',\\n+        additions: 5,\\n+        deletions: 2,\\n+        changes: 7,\\n+        status: 'modified',\\n+      },\\n+    ],\\n+    totalAdditions: 10,\\n+    totalDeletions: 5,\\n+  };\\n+\\n+  beforeEach(() => {\\n+    // Create mock instances\\n+    mockRegistry = {\\n+      getInstance: jest.fn(),\\n+      get: jest.fn(),\\n+      has: jest.fn(),\\n+      register: jest.fn(),\\n+      validateInputs: jest.fn(),\\n+      validateWorkflow: jest.fn(),\\n+      list: jest.fn(),\\n+      getMetadata: jest.fn(),\\n+      unregister: jest.fn(),\\n+      clear: jest.fn(),\\n+      import: jest.fn(),\\n+      importMany: jest.fn(),\\n+    } as any;\\n+\\n+    mockExecutor = {\\n+      execute: jest.fn(),\\n+    } as any;\\n+\\n+    // Setup singleton mock\\n+    (WorkflowRegistry.getInstance as jest.Mock).mockReturnValue(mockRegistry);\\n+    (WorkflowExecutor as jest.Mock).mockImplementation(() => mockExecutor);\\n+\\n+    provider = new WorkflowCheckProvider();\\n+  });\\n+\\n+  afterEach(() => {\\n+    jest.clearAllMocks();\\n+  });\\n+\\n+  describe('getName and getDescription', () => {\\n+    it('should return correct name and description', () => {\\n+      expect(provider.getName()).toBe('workflow');\\n+      expect(provider.getDescription()).toBe('Executes reusable workflow definitions as checks');\\n+    });\\n+  });\\n+\\n+  describe('validateConfig', () => {\\n+    it('should validate config with workflow field', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+      };\\n+\\n+      mockRegistry.has.mockReturnValue(true);\\n+\\n+      const result = await provider.validateConfig(config);\\n+      expect(result).toBe(true);\\n+    });\\n+\\n+    it('should reject config without workflow field', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+      };\\n+\\n+      const result = await provider.validateConfig(config);\\n+      expect(result).toBe(false);\\n+    });\\n+\\n+    it('should reject config with non-existent workflow', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'non-existent',\\n+      };\\n+\\n+      mockRegistry.has.mockReturnValue(false);\\n+\\n+      const result = await provider.validateConfig(config);\\n+      expect(result).toBe(false);\\n+    });\\n+  });\\n+\\n+  describe('execute', () => {\\n+    it('should execute workflow with basic inputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+        workflow_inputs: {\\n+          threshold: 90,\\n+          language: 'typescript',\\n+        },\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(sampleWorkflow);\\n+      mockRegistry.validateInputs.mockReturnValue({ valid: true });\\n+      mockExecutor.execute.mockResolvedValue({\\n+        success: true,\\n+        score: 95,\\n+        confidence: 'high',\\n+        issues: [],\\n+        comments: [],\\n+        output: { score: 95, passed: true },\\n+        status: 'completed',\\n+        duration: 1000,\\n+      });\\n+\\n+      const result = await provider.execute(samplePRInfo, config);\\n+\\n+      expect(mockRegistry.get).toHaveBeenCalledWith('test-workflow');\\n+      expect(mockRegistry.validateInputs).toHaveBeenCalledWith(\\n+        sampleWorkflow,\\n+        expect.objectContaining({\\n+          threshold: 90,\\n+          language: 'typescript',\\n+        })\\n+      );\\n+      expect(mockExecutor.execute).toHaveBeenCalled();\\n+      expect((result as any).score).toBe(95);\\n+      expect((result as any).output).toEqual({ score: 95, passed: true });\\n+    });\\n+\\n+    it('should use default input values', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+        workflow_inputs: {\\n+          language: 'javascript', // Only provide required param\\n+        },\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(sampleWorkflow);\\n+      mockRegistry.validateInputs.mockReturnValue({ valid: true });\\n+      mockExecutor.execute.mockResolvedValue({\\n+        success: true,\\n+        output: { score: 85 },\\n+        status: 'completed',\\n+      });\\n+\\n+      await provider.execute(samplePRInfo, config);\\n+\\n+      // Check that the executor was called with defaults\\n+      const executorCall = mockExecutor.execute.mock.calls[0];\\n+      expect(executorCall[1].inputs).toEqual({\\n+        threshold: 80, // Default value\\n+        language: 'javascript',\\n+      });\\n+    });\\n+\\n+    it('should process Liquid templates in inputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+        workflow_inputs: {\\n+          language:\\n+            '{% if pr.files[0].filename contains \\\".ts\\\" %}typescript{% else %}javascript{% endif %}',\\n+          threshold: 85,\\n+        },\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(sampleWorkflow);\\n+      mockRegistry.validateInputs.mockReturnValue({ valid: true });\\n+      mockExecutor.execute.mockResolvedValue({\\n+        success: true,\\n+        output: {},\\n+        status: 'completed',\\n+      });\\n+\\n+      await provider.execute(samplePRInfo, config);\\n+\\n+      const executorCall = mockExecutor.execute.mock.calls[0];\\n+      expect(executorCall[1].inputs.language).toBe('typescript'); // test.ts contains .ts\\n+    });\\n+\\n+    it('should handle workflow overrides', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+        workflow_inputs: {\\n+          language: 'python',\\n+          threshold: 70,\\n+        },\\n+        workflow_overrides: {\\n+          analyze: {\\n+            prompt: 'Custom prompt for analysis',\\n+            timeout: 120,\\n+          },\\n+        },\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(sampleWorkflow);\\n+      mockRegistry.validateInputs.mockReturnValue({ valid: true });\\n+      mockExecutor.execute.mockResolvedValue({\\n+        success: true,\\n+        output: {},\\n+        status: 'completed',\\n+      });\\n+\\n+      await provider.execute(samplePRInfo, config);\\n+\\n+      // Check that modified workflow was passed to executor\\n+      const executorCall = mockExecutor.execute.mock.calls[0];\\n+      const modifiedWorkflow = executorCall[0];\\n+      expect(modifiedWorkflow.steps.analyze.prompt).toBe('Custom prompt for analysis');\\n+      expect(modifiedWorkflow.steps.analyze.timeout).toBe(120);\\n+    });\\n+\\n+    it('should map workflow outputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+        workflow_inputs: {\\n+          language: 'go',\\n+          threshold: 80,\\n+        },\\n+        output_mapping: {\\n+          quality_score: 'score',\\n+          is_passing: 'passed',\\n+        },\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(sampleWorkflow);\\n+      mockRegistry.validateInputs.mockReturnValue({ valid: true });\\n+      mockExecutor.execute.mockResolvedValue({\\n+        success: true,\\n+        output: { score: 92, passed: true, details: 'test' },\\n+        status: 'completed',\\n+      });\\n+\\n+      const result = await provider.execute(samplePRInfo, config);\\n+\\n+      expect((result as any).output).toEqual({\\n+        quality_score: 92,\\n+        is_passing: true,\\n+      });\\n+    });\\n+\\n+    it('should handle nested output paths in mapping', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+        workflow_inputs: {\\n+          language: 'java',\\n+          threshold: 75,\\n+        },\\n+        output_mapping: {\\n+          nested_value: 'result.data.value',\\n+        },\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(sampleWorkflow);\\n+      mockRegistry.validateInputs.mockReturnValue({ valid: true });\\n+      mockExecutor.execute.mockResolvedValue({\\n+        success: true,\\n+        output: {\\n+          result: {\\n+            data: {\\n+              value: 'nested-test',\\n+            },\\n+          },\\n+        },\\n+        status: 'completed',\\n+      });\\n+\\n+      const result = await provider.execute(samplePRInfo, config);\\n+\\n+      expect((result as any).output).toEqual({\\n+        nested_value: 'nested-test',\\n+      });\\n+    });\\n+\\n+    it('should throw error for invalid inputs', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+        workflow_inputs: {\\n+          // Missing required 'language' param\\n+          threshold: 90,\\n+        },\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(sampleWorkflow);\\n+      mockRegistry.validateInputs.mockReturnValue({\\n+        valid: false,\\n+        errors: [{ path: 'inputs.language', message: 'Required input is missing' }],\\n+      });\\n+\\n+      await expect(provider.execute(samplePRInfo, config)).rejects.toThrow(\\n+        'Invalid workflow inputs'\\n+      );\\n+    });\\n+\\n+    it('should throw error when workflow not found', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'non-existent',\\n+        workflow_inputs: {},\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(undefined);\\n+\\n+      await expect(provider.execute(samplePRInfo, config)).rejects.toThrow(\\n+        \\\"Workflow 'non-existent' not found\\\"\\n+      );\\n+    });\\n+\\n+    it('should pass workflow inputs to executor', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+        workflow_inputs: {\\n+          language: 'python',\\n+          threshold: 85,\\n+        },\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(sampleWorkflow);\\n+      mockRegistry.validateInputs.mockReturnValue({ valid: true });\\n+      mockExecutor.execute.mockResolvedValue({\\n+        success: true,\\n+        output: {},\\n+        status: 'completed',\\n+      });\\n+\\n+      await provider.execute(samplePRInfo, config);\\n+\\n+      const executorCall = mockExecutor.execute.mock.calls[0];\\n+      expect(executorCall[1].inputs.language).toBe('python');\\n+      expect(executorCall[1].inputs.threshold).toBe(85);\\n+    });\\n+\\n+    it('should format workflow results correctly', async () => {\\n+      const config: CheckProviderConfig = {\\n+        type: 'workflow',\\n+        workflow: 'test-workflow',\\n+        workflow_inputs: {\\n+          language: 'rust',\\n+          threshold: 85,\\n+        },\\n+      };\\n+\\n+      mockRegistry.get.mockReturnValue(sampleWorkflow);\\n+      mockRegistry.validateInputs.mockReturnValue({ valid: true });\\n+      mockExecutor.execute.mockResolvedValue({\\n+        success: true,\\n+        score: 88,\\n+        issues: [{ severity: 'warning' }, { severity: 'info' }],\\n+        output: { score: 88, passed: true },\\n+        status: 'completed',\\n+        duration: 1500,\\n+        stepSummaries: [\\n+          { stepId: 'analyze', status: 'success', issues: [{ severity: 'warning' }] },\\n+        ],\\n+      });\\n+\\n+      const result = await provider.execute(samplePRInfo, config);\\n+\\n+      expect((result as any).content).toContain('Workflow: Test Workflow');\\n+      expect((result as any).content).toContain('Score: 88');\\n+      expect((result as any).content).toContain('Issues Found: 2');\\n+      expect((result as any).content).toContain('Duration: 1500ms');\\n+      expect((result as any).content).toContain('analyze: success');\\n+    });\\n+  });\\n+\\n+  describe('getSupportedConfigKeys', () => {\\n+    it('should return supported config keys', () => {\\n+      const keys = provider.getSupportedConfigKeys();\\n+      expect(keys).toContain('workflow');\\n+      expect(keys).toContain('args');\\n+      expect(keys).toContain('overrides');\\n+      expect(keys).toContain('output_mapping');\\n+      expect(keys).toContain('timeout');\\n+      expect(keys).toContain('env');\\n+      expect(keys).toContain('checkName');\\n+    });\\n+  });\\n+\\n+  describe('isAvailable and getRequirements', () => {\\n+    it('should always be available', async () => {\\n+      const available = await provider.isAvailable();\\n+      expect(available).toBe(true);\\n+    });\\n+\\n+    it('should have no requirements', () => {\\n+      const requirements = provider.getRequirements();\\n+      expect(requirements).toEqual([]);\\n+    });\\n+  });\\n+});\\n\",\"status\":\"added\"},{\"filename\":\"tests/unit/workflow-registry.test.ts\",\"additions\":15,\"deletions\":0,\"changes\":543,\"patch\":\"diff --git a/tests/unit/workflow-registry.test.ts b/tests/unit/workflow-registry.test.ts\\nnew file mode 100644\\nindex 00000000..b110ef95\\n--- /dev/null\\n+++ b/tests/unit/workflow-registry.test.ts\\n@@ -0,0 +1,543 @@\\n+/**\\n+ * Unit tests for WorkflowRegistry\\n+ */\\n+\\n+import { WorkflowRegistry } from '../../src/workflow-registry';\\n+import { WorkflowDefinition } from '../../src/types/workflow';\\n+import * as fs from 'fs';\\n+import * as yaml from 'js-yaml';\\n+\\n+// Mock fs and fetch\\n+jest.mock('fs', () => ({\\n+  promises: {\\n+    readFile: jest.fn(),\\n+  },\\n+}));\\n+\\n+global.fetch = jest.fn();\\n+\\n+describe('WorkflowRegistry', () => {\\n+  let registry: WorkflowRegistry;\\n+\\n+  beforeEach(() => {\\n+    // Get a fresh instance for each test\\n+    (WorkflowRegistry as any).instance = undefined;\\n+    registry = WorkflowRegistry.getInstance();\\n+  });\\n+\\n+  afterEach(() => {\\n+    registry.clear();\\n+    jest.clearAllMocks();\\n+  });\\n+\\n+  describe('singleton pattern', () => {\\n+    it('should return the same instance', () => {\\n+      const instance1 = WorkflowRegistry.getInstance();\\n+      const instance2 = WorkflowRegistry.getInstance();\\n+      expect(instance1).toBe(instance2);\\n+    });\\n+  });\\n+\\n+  describe('register', () => {\\n+    it('should register a valid workflow', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test prompt',\\n+          },\\n+        },\\n+      };\\n+\\n+      const result = registry.register(workflow);\\n+      expect(result.valid).toBe(true);\\n+      expect(registry.has('test-workflow')).toBe(true);\\n+    });\\n+\\n+    it('should reject workflow without ID', () => {\\n+      const workflow = {\\n+        name: 'Test Workflow',\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test prompt',\\n+          },\\n+        },\\n+      } as any;\\n+\\n+      const result = registry.register(workflow);\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors?.[0].message).toContain('ID is required');\\n+    });\\n+\\n+    it('should reject workflow without steps', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        steps: {},\\n+      };\\n+\\n+      const result = registry.register(workflow);\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors?.[0].message).toContain('at least one step');\\n+    });\\n+\\n+    it('should reject duplicate workflow ID without override', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test prompt',\\n+          },\\n+        },\\n+      };\\n+\\n+      registry.register(workflow);\\n+      const result = registry.register(workflow);\\n+\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors?.[0].message).toContain('already exists');\\n+    });\\n+\\n+    it('should allow override with flag', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test prompt',\\n+          },\\n+        },\\n+      };\\n+\\n+      registry.register(workflow);\\n+      const result = registry.register(workflow, 'inline', { override: true });\\n+\\n+      expect(result.valid).toBe(true);\\n+    });\\n+  });\\n+\\n+  describe('validateWorkflow', () => {\\n+    it('should validate input parameters', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        inputs: [\\n+          {\\n+            name: 'param1',\\n+            schema: { type: 'string' },\\n+          },\\n+          {\\n+            name: '', // Invalid: empty name\\n+            schema: { type: 'number' },\\n+          },\\n+        ],\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+          },\\n+        },\\n+      };\\n+\\n+      const result = registry.validateWorkflow(workflow);\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors).toContainEqual(\\n+        expect.objectContaining({\\n+          path: 'inputs[1].name',\\n+          message: expect.stringContaining('name is required'),\\n+        })\\n+      );\\n+    });\\n+\\n+    it('should validate output parameters', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        outputs: [\\n+          {\\n+            name: 'output1',\\n+            // Missing both value and value_js\\n+          },\\n+        ],\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+          },\\n+        },\\n+      };\\n+\\n+      const result = registry.validateWorkflow(workflow);\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors).toContainEqual(\\n+        expect.objectContaining({\\n+          path: 'outputs[0]',\\n+          message: expect.stringContaining('value or value_js'),\\n+        })\\n+      );\\n+    });\\n+\\n+    it('should detect circular dependencies', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+            depends_on: ['step3'],\\n+          },\\n+          step2: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+            depends_on: ['step1'],\\n+          },\\n+          step3: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+            depends_on: ['step2'],\\n+          },\\n+        },\\n+      };\\n+\\n+      const result = registry.validateWorkflow(workflow);\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors?.[0].message).toContain('Circular dependencies');\\n+    });\\n+\\n+    it('should validate step dependencies exist', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+            depends_on: ['non-existent-step'],\\n+          },\\n+        },\\n+      };\\n+\\n+      const result = registry.validateWorkflow(workflow);\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors).toContainEqual(\\n+        expect.objectContaining({\\n+          path: 'steps.step1.depends_on',\\n+          message: expect.stringContaining('non-existent step'),\\n+        })\\n+      );\\n+    });\\n+\\n+    it('should validate input mappings', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        inputs: [\\n+          {\\n+            name: 'valid_param',\\n+            schema: { type: 'string' },\\n+          },\\n+        ],\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+            inputs: {\\n+              mapping1: {\\n+                source: 'param',\\n+                value: 'invalid_param', // Non-existent parameter\\n+              },\\n+            },\\n+          },\\n+        },\\n+      };\\n+\\n+      const result = registry.validateWorkflow(workflow);\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors).toContainEqual(\\n+        expect.objectContaining({\\n+          path: 'steps.step1.inputs.mapping1',\\n+          message: expect.stringContaining('non-existent parameter'),\\n+        })\\n+      );\\n+    });\\n+  });\\n+\\n+  describe('validateInputs', () => {\\n+    it('should validate required inputs', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        inputs: [\\n+          {\\n+            name: 'required_param',\\n+            schema: { type: 'string' },\\n+            required: true,\\n+          },\\n+          {\\n+            name: 'optional_param',\\n+            schema: { type: 'number' },\\n+            required: false,\\n+          },\\n+        ],\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+          },\\n+        },\\n+      };\\n+\\n+      const result = registry.validateInputs(workflow, {\\n+        optional_param: 42,\\n+      });\\n+\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors?.[0].message).toContain(\\\"Required input 'required_param'\\\");\\n+    });\\n+\\n+    it('should use defaults for missing optional inputs', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        inputs: [\\n+          {\\n+            name: 'param_with_default',\\n+            schema: { type: 'string' },\\n+            default: 'default_value',\\n+          },\\n+        ],\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+          },\\n+        },\\n+      };\\n+\\n+      const result = registry.validateInputs(workflow, {});\\n+      expect(result.valid).toBe(true);\\n+    });\\n+\\n+    it('should validate input schemas', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        inputs: [\\n+          {\\n+            name: 'number_param',\\n+            schema: {\\n+              type: 'number',\\n+              minimum: 0,\\n+              maximum: 100,\\n+            },\\n+          },\\n+        ],\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+          },\\n+        },\\n+      };\\n+\\n+      const result = registry.validateInputs(workflow, {\\n+        number_param: 150, // Out of range\\n+      });\\n+\\n+      expect(result.valid).toBe(false);\\n+      expect(result.errors?.[0].message).toContain('must be <= 100');\\n+    });\\n+  });\\n+\\n+  describe('import', () => {\\n+    it('should import workflow from YAML file', async () => {\\n+      const workflowYaml = `\\n+id: imported-workflow\\n+name: Imported Workflow\\n+steps:\\n+  step1:\\n+    type: ai\\n+    prompt: Test prompt\\n+`;\\n+\\n+      (fs.promises.readFile as jest.Mock).mockResolvedValue(workflowYaml);\\n+\\n+      const results = await registry.import('./workflow.yaml');\\n+\\n+      expect(results).toHaveLength(1);\\n+      expect(results[0].valid).toBe(true);\\n+      expect(registry.has('imported-workflow')).toBe(true);\\n+    });\\n+\\n+    it('should import workflow from JSON file', async () => {\\n+      const workflowJson = JSON.stringify({\\n+        id: 'json-workflow',\\n+        name: 'JSON Workflow',\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test prompt',\\n+          },\\n+        },\\n+      });\\n+\\n+      (fs.promises.readFile as jest.Mock).mockResolvedValue(workflowJson);\\n+\\n+      const results = await registry.import('./workflow.json');\\n+\\n+      expect(results).toHaveLength(1);\\n+      expect(results[0].valid).toBe(true);\\n+      expect(registry.has('json-workflow')).toBe(true);\\n+    });\\n+\\n+    it('should import workflow from URL', async () => {\\n+      const workflowData = {\\n+        id: 'remote-workflow',\\n+        name: 'Remote Workflow',\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test prompt',\\n+          },\\n+        },\\n+      };\\n+\\n+      (global.fetch as jest.Mock).mockResolvedValue({\\n+        ok: true,\\n+        text: async () => JSON.stringify(workflowData),\\n+      });\\n+\\n+      const results = await registry.import('https://example.com/workflow.json');\\n+\\n+      expect(results).toHaveLength(1);\\n+      expect(results[0].valid).toBe(true);\\n+      expect(registry.has('remote-workflow')).toBe(true);\\n+    });\\n+\\n+    it('should handle import errors', async () => {\\n+      (fs.promises.readFile as jest.Mock).mockRejectedValue(new Error('File not found'));\\n+\\n+      const results = await registry.import('./non-existent.yaml');\\n+\\n+      expect(results).toHaveLength(1);\\n+      expect(results[0].valid).toBe(false);\\n+      expect(results[0].errors?.[0].message).toContain('Failed to import');\\n+    });\\n+\\n+    it('should import multiple workflows from array', async () => {\\n+      const workflowsYaml = yaml.dump([\\n+        {\\n+          id: 'workflow1',\\n+          name: 'Workflow 1',\\n+          steps: { step1: { type: 'ai', prompt: 'Test' } },\\n+        },\\n+        {\\n+          id: 'workflow2',\\n+          name: 'Workflow 2',\\n+          steps: { step1: { type: 'ai', prompt: 'Test' } },\\n+        },\\n+      ]);\\n+\\n+      (fs.promises.readFile as jest.Mock).mockResolvedValue(workflowsYaml);\\n+\\n+      const results = await registry.import('./workflows.yaml');\\n+\\n+      expect(results).toHaveLength(2);\\n+      expect(results[0].valid).toBe(true);\\n+      expect(results[1].valid).toBe(true);\\n+      expect(registry.has('workflow1')).toBe(true);\\n+      expect(registry.has('workflow2')).toBe(true);\\n+    });\\n+  });\\n+\\n+  describe('get and metadata', () => {\\n+    it('should track usage statistics', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        steps: {\\n+          step1: {\\n+            type: 'ai',\\n+            prompt: 'Test',\\n+          },\\n+        },\\n+      };\\n+\\n+      registry.register(workflow);\\n+\\n+      // Get workflow multiple times\\n+      registry.get('test-workflow');\\n+      registry.get('test-workflow');\\n+      registry.get('test-workflow');\\n+\\n+      const metadata = registry.getMetadata('test-workflow');\\n+      expect(metadata?.usage?.count).toBe(3);\\n+      expect(metadata?.usage?.lastUsed).toBeDefined();\\n+    });\\n+\\n+    it('should return undefined for non-existent workflow', () => {\\n+      const workflow = registry.get('non-existent');\\n+      expect(workflow).toBeUndefined();\\n+    });\\n+  });\\n+\\n+  describe('list and unregister', () => {\\n+    it('should list all workflows', () => {\\n+      const workflow1: WorkflowDefinition = {\\n+        id: 'workflow1',\\n+        name: 'Workflow 1',\\n+        steps: { step1: { type: 'ai', prompt: 'Test' } },\\n+      };\\n+\\n+      const workflow2: WorkflowDefinition = {\\n+        id: 'workflow2',\\n+        name: 'Workflow 2',\\n+        steps: { step1: { type: 'ai', prompt: 'Test' } },\\n+      };\\n+\\n+      registry.register(workflow1);\\n+      registry.register(workflow2);\\n+\\n+      const list = registry.list();\\n+      expect(list).toHaveLength(2);\\n+      expect(list.map(w => w.id)).toContain('workflow1');\\n+      expect(list.map(w => w.id)).toContain('workflow2');\\n+    });\\n+\\n+    it('should unregister workflow', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        steps: { step1: { type: 'ai', prompt: 'Test' } },\\n+      };\\n+\\n+      registry.register(workflow);\\n+      expect(registry.has('test-workflow')).toBe(true);\\n+\\n+      const result = registry.unregister('test-workflow');\\n+      expect(result).toBe(true);\\n+      expect(registry.has('test-workflow')).toBe(false);\\n+    });\\n+\\n+    it('should clear all workflows', () => {\\n+      const workflow: WorkflowDefinition = {\\n+        id: 'test-workflow',\\n+        name: 'Test Workflow',\\n+        steps: { step1: { type: 'ai', prompt: 'Test' } },\\n+      };\\n+\\n+      registry.register(workflow);\\n+      expect(registry.list()).toHaveLength(1);\\n+\\n+      registry.clear();\\n+      expect(registry.list()).toHaveLength(0);\\n+    });\\n+  });\\n+});\\n\",\"status\":\"added\"}],\"outputs\":{}}"},"events":[]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"c2cdb5d9-184b-4062-b33c-6cb93d2c0c29"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"LevelDispatch","engine_mode":"state-machine","wave":1,"session_id":"c2cdb5d9-184b-4062-b33c-6cb93d2c0c29"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"LevelDispatch","state_to":"WavePlanning","engine_mode":"state-machine","wave":1,"session_id":"c2cdb5d9-184b-4062-b33c-6cb93d2c0c29"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"engine.state_transition","attrs":{"state_from":"WavePlanning","state_to":"Completed","engine_mode":"state-machine","wave":1,"session_id":"c2cdb5d9-184b-4062-b33c-6cb93d2c0c29"}}]}

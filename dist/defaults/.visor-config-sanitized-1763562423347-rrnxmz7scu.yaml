version: '1.0'
steps:
  ask:
    type: human-input
    group: task-refinement
    if: '!(outputs && outputs[''ask''])'
    prompt: |
      {% assign last_refine = outputs_history.refine | last %}
      {% if last_refine and last_refine.refined == false %}
      {{ last_refine.text }}
      {% else %}
      Provide the task you want to accomplish. Be specific about constraints
      (inputs, outputs, environment, success criteria).
      {% endif %}
    multiline: false
    allow_empty: false
  refine:
    type: ai
    group: task-refinement
    depends_on:
      - ask
    if: '!(outputs && outputs[''refine''])'
    ai:
      skip_code_context: true
      disableTools: true
      system_prompt: >
        You are a helpful, precise task refinement assistant (role:
        requirements-analyst).

        Your goal is to get to an agreed, testable task definition and clear
        acceptance criteria.

        - Refine the user's task into an unambiguous, executable description
        with minimal assumptions.

        - Define how correctness will be validated (objective success criteria
        and measurables).

        - If information is missing, set refined=false and ask_user=true and put
        a single, specific
          clarification question in the "text" field (one question at a time).
        - If everything is sufficient, set refined=true and put the final
        refined wording in "text".

        - Be succinct and concrete. Prefer measurable outcomes over vague
        phrasing.

        - You can't mark plan as refined until explicit user
        confirmation/agreement is implied.
    schema:
      type: object
      additionalProperties: false
      properties:
        refined:
          type: boolean
          description: true if the task is fully specified and accepted
        text:
          type: string
          description: final refined task or question to user
      required:
        - refined
        - text
    prompt: |
      <history>
        {% assign umsgs = outputs_history.ask | default: [] %}
        {% assign amsgs = outputs_history.refine | default: [] %}
        {% assign merged = umsgs | concat: amsgs | sort: 'ts' %}
        {% for m in merged %}
          {% if m.refined != nil %}
            <assistant>{{ m.text }}</assistant>
          {% else %}
            <user>{{ m.text }}</user>
          {% endif %}
        {% endfor %}
      </history>

      <input>
        {{ outputs['ask'].text }}
      </input>
    fail_if: output['refined'] !== true
    on_fail:
      goto: ask
  plan-commands:
    type: ai
    group: task-refinement
    depends_on:
      - refine
    reuse_ai_session: refine
    session_mode: append
    ai:
      disableTools: false
      skip_code_context: true
      system_prompt: >
        You are a Task Validation Planner.

        Produce a deterministic, minimal list of shell commands that, when
        executed in order,

        validate that the refined task is complete for THIS repository/local
        env.

        Constraints:

        - Commands must be safe and non-destructive. Do not delete files or push
        to remotes.

        - Prefer read-only checks and standard project commands (build, lint,
        test) when present.

        - Each item is a single shell line; you may use && or || inside a line
        if necessary.

        - Favor idempotent commands. Avoid interactive prompts.

        - Detect package manager/tooling pragmatically (npm/pnpm/yarn/bun,
        eslint/biome, jest/vitest, etc.).

        Return JSON with an array of strings under "commands" and optional
        "notes".
    schema:
      type: object
      additionalProperties: false
      properties:
        commands:
          type: array
          minItems: 1
          items:
            type: string
            minLength: 1
        notes:
          type: string
      required:
        - commands
    prompt: >
      Refined task:

      {{ outputs['refine'].text }}


      If the repository has build/lint/test, include them. Otherwise propose
      basic checks that still

      demonstrate completion (e.g., typecheck, compile, format verification,
      smoke run).


      {% assign prev_conf_hist = outputs_history['confirm-interpret'] | default:
      [] %}

      {% assign prev_run_hist = outputs_history['run-commands'] | default: [] %}

      {% assign last_failed = nil %}

      {% for r in prev_run_hist %}
        {% if r.failed and r.failed > 0 %}
          {% assign last_failed = r %}
        {% endif %}
      {% endfor %}

      {% assign last_conf = prev_conf_hist | last %}

      {% assign prev_run_count = prev_run_hist | size %}

      {% assign last_run = prev_run_hist | last %}

      {% if last_failed or prev_run_count > 0 %}

      Previous attempt failed. Here are the details to learn from:

      - Previous commands: {{ last_conf.commands | to_json }}

      - Run results: {{ last_failed | default: last_run | to_json }}

      Please revise the commands to address failures. Keep the list minimal and
      deterministic.

      {% endif %}


      Output strictly JSON per schema. No prose around it.
  ask-confirm:
    type: human-input
    group: task-refinement
    depends_on:
      - plan-commands
    prompt: |
      Here is the proposed validation command list (to run sequentially):
      {% for c in outputs['plan-commands'].commands %}
      {{ forloop.index }}. {{ c }}
      {% endfor %}

      Confirm running these? Reply "yes" to accept, or provide edits:
      - JSON array of commands, e.g. ["npm ci", "npm test"], or
      - Plain text with one command per line.
    placeholder: yes | or paste modified list...
    multiline: true
    allow_empty: true
    default: 'yes'
  confirm-interpret:
    type: ai
    group: task-refinement
    depends_on:
      - plan-commands
      - ask-confirm
    ai:
      skip_code_context: true
      disableTools: true
      system_prompt: >
        You are a confirmation interpreter. Given the planned command list and
        the user's reply,

        decide whether to proceed to execution with a normalized list of shell
        commands, or

        return for replanning.

        - If the user says yes/approve, set proceed=true and provide commands
        as-is.

        - If the user supplied edits (JSON array or one per line), parse, trim,
        dedupe, and set proceed=true with commands.

        - If the reply indicates high-level changes (not runnable commands), set
        proceed=false and include a short reason.

        Output strictly JSON per schema.
    schema:
      type: object
      additionalProperties: false
      properties:
        proceed:
          type: boolean
        commands:
          type: array
          items:
            type: string
            minLength: 1
        reason:
          type: string
      required:
        - proceed
    prompt: >
      Planned commands:

      {{ outputs['plan-commands'].commands | to_json }}


      User reply:

      {{ outputs['ask-confirm'].text }}


      Return JSON per schema. If proceed=true, commands must be a non-empty
      array of shell lines.
    fail_if: output && output.proceed !== true
    on_fail:
      goto: plan-commands
  run-commands:
    type: command
    criticality: internal
    group: task-refinement
    depends_on:
      - confirm-interpret
    assume:
      - outputs['confirm-interpret']?.proceed === true
      - (outputs['confirm-interpret']?.commands?.length ?? 0) > 0
    exec: >
      node <<'NODE'

      const { spawn } = require('child_process');

      const cmds = {{ outputs['confirm-interpret'] | to_json }}.commands || [];

      const results = [];

      const runOne = (cmd) => new Promise((resolve) => {
        const child = spawn('bash', ['-lc', cmd], { stdio: ['ignore', 'pipe', 'pipe'] });
        let out = '', err = '';
        const started = Date.now();
        child.stdout.on('data', d => (out += d.toString()));
        child.stderr.on('data', d => (err += d.toString()));
        child.on('close', code => {
          results.push({ cmd, code, stdout: out, stderr: err, durationMs: Date.now() - started });
          resolve();
        });
      });

      (async () => {
        for (const c of cmds) { await runOne(c); }
        const failed = results.filter(r => Number(r.code||0) !== 0).length;
        process.stdout.write(JSON.stringify({ failed, results }));
      })().catch(e => { process.stdout.write(JSON.stringify({ failed: 1, error:
      String(e) })); process.exit(0); });

      NODE
    output_format: json
    schema:
      type: object
      additionalProperties: true
      properties:
        failed:
          type: number
        results:
          type: array
          items:
            type: object
            additionalProperties: true
            properties:
              cmd:
                type: string
              code: {}
              stdout:
                type: string
              stderr:
                type: string
              durationMs:
                type: number
            required:
              - cmd
              - code
      required:
        - failed
    fail_if: output && Number(output.failed||0) > 0
    on_fail:
      goto: plan-commands
  finish:
    type: log
    group: task-refinement
    depends_on:
      - run-commands
    if: >-
      (outputs && outputs['run-commands'] &&
      Number((outputs['run-commands'].failed||0)) === 0) && !(outputs &&
      outputs['finish'])
    message: |
      ✅ Refined Task:
      {{ outputs['refine'].text }}

      ✅ Validation commands (final):
      {% for c in outputs['confirm-interpret'].commands %}
      - {{ c }}
      {% endfor %}
    level: info
    include_pr_context: false
    include_dependencies: false
    include_metadata: false

{"name":"visor.run","attributes":{"started":true}}
{"name":"visor.run","attributes":{"started":true}}
{"name":"visor.run","attributes":{"started":true}}
{"name":"visor.run","attributes":{"started":true}}
{"name":"visor.provider","attributes":{"visor.check.id":"overview","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"overview","visor.check.input.context":"{\"pr\":{\"number\":0,\"title\":\"Local Analysis: feat/test-framework-runner (14 modified)\",\"author\":\"Leonid Bugaev\",\"branch\":\"feat/test-framework-runner\",\"base\":\"main\"},\"files\":[{\"filename\":\".github/workflows/ci.yml\",\"additions\":1,\"deletions\":0,\"changes\":15,\"patch\":\"diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml\\nindex 840f2abc..13bd6f9d 100644\\n--- a/.github/workflows/ci.yml\\n+++ b/.github/workflows/ci.yml\\n@@ -64,6 +64,21 @@ jobs:\\n           ls -la *.tgz\\n           echo \\\"✅ Package can be created successfully\\\"\\n \\n+      - name: Run integration tests (defaults suite)\\n+        run: |\\n+          mkdir -p output\\n+          node ./dist/index.js test --config defaults/.visor.tests.yaml --json output/visor-tests.json --report junit:output/visor-tests.xml --summary md:output/visor-tests.md\\n+\\n+      - name: Upload integration test artifacts\\n+        if: always()\\n+        uses: actions/upload-artifact@v4\\n+        with:\\n+          name: visor-test-results\\n+          path: |\\n+            output/visor-tests.json\\n+            output/visor-tests.xml\\n+            output/visor-tests.md\\n+\\n       - name: Test basic action functionality\\n         uses: ./\\n         with:\\n\",\"status\":\"added\"},{\"filename\":\"README.md\",\"additions\":1,\"deletions\":0,\"changes\":10,\"patch\":\"diff --git a/README.md b/README.md\\nindex b2bf4db5..7acc4843 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -729,3 +729,13 @@ steps:\\n ```\\n \\n See docs: docs/github-ops.md\\n+## Integration Tests (Great DX)\\n+\\n+Visor ships a YAML‑native integration test runner so you can describe user flows, mocks, and assertions alongside your config.\\n+\\n+- Start here: docs/testing/getting-started.md\\n+- CLI details: docs/testing/cli.md\\n+- Fixtures and mocks: docs/testing/fixtures-and-mocks.md\\n+- Assertions reference: docs/testing/assertions.md\\n+\\n+Example suite: defaults/.visor.tests.yaml\\n\",\"status\":\"added\"},{\"filename\":\"defaults/.visor.tests.yaml\",\"additions\":20,\"deletions\":0,\"changes\":557,\"patch\":\"diff --git a/defaults/.visor.tests.yaml b/defaults/.visor.tests.yaml\\nnew file mode 100644\\nindex 00000000..c496cdee\\n--- /dev/null\\n+++ b/defaults/.visor.tests.yaml\\n@@ -0,0 +1,557 @@\\n+version: \\\"1.0\\\"\\n+extends: \\\".visor.yaml\\\"\\n+\\n+# Integration test suite for Visor default configuration\\n+# - Driven by events + fixtures; no manual step lists\\n+# - Strict by default: every executed step must have an expect\\n+# - AI mocks accept structured JSON when a schema is defined; plain uses text\\n+# - GitHub calls are recorded by default by the test runner (no network)\\n+\\n+tests:\\n+  defaults:\\n+    strict: true\\n+    ai_provider: mock\\n+    fail_on_unexpected_calls: true\\n+    # Example: enable negative GitHub recorder for all tests\\n+    # github_recorder: { error_code: 429 }\\n+  # Built-in fixtures are provided by the test runner (gh.* namespace).\\n+  # Custom fixtures may still be added here if needed.\\n+  fixtures: []\\n+\\n+  cases:\\n+    - name: label-flow\\n+      description: |\\n+        Validates the happy path for PR open:\\n+        - overview runs and emits tags.label and tags.review-effort (mocked)\\n+        - apply-overview-labels adds two labels (feature and review/effort:2)\\n+        - overview prompt includes PR title and unified diff header\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: |\\n+            High‑level summary of the changes and impact.\\n+          tags:\\n+            label: feature\\n+            review-effort: 2\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args:\\n+              contains:\\n+                - feature\\n+                - \\\"review/effort:2\\\"\\n+        outputs:\\n+          - step: overview\\n+            path: \\\"tags.label\\\"\\n+            equals: feature\\n+          - step: overview\\n+            path: \\\"tags['review-effort']\\\"\\n+            equals: 2\\n+        prompts:\\n+          - step: overview\\n+            contains:\\n+              - \\\"feat: add user search\\\"\\n+              - \\\"diff --git a/src/search.ts\\\"\\n+\\n+    - name: issue-triage\\n+      skip: true\\n+      description: |\\n+        Ensures the issue assistant triages a newly opened issue and applies labels.\\n+        Asserts the structured output (intent=issue_triage) and the GitHub label op.\\n+      event: issue_opened\\n+      fixture: gh.issue_open.minimal\\n+      mocks:\\n+        issue-assistant:\\n+          text: |\\n+            Thanks for the detailed report! We will investigate.\\n+          intent: issue_triage\\n+          labels: [bug, priority/medium]\\n+      expect:\\n+        calls:\\n+          - step: issue-assistant\\n+            exactly: 1\\n+          - step: apply-issue-labels\\n+            exactly: 1\\n+          - step: init-fact-validation\\n+            exactly: 1\\n+          - step: extract-facts\\n+            exactly: 1\\n+          - step: validate-fact\\n+            at_least: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args:\\n+              contains:\\n+                - bug\\n+        outputs:\\n+          - step: issue-assistant\\n+            path: intent\\n+            equals: issue_triage\\n+        prompts:\\n+          - step: issue-assistant\\n+            contains:\\n+              - \\\"Bug: crashes on search edge case\\\"\\n+\\n+    - name: pr-review-e2e-flow\\n+      description: |\\n+        End-to-end PR lifecycle covering multiple external events:\\n+        1) PR opened → overview + labels\\n+        2) Standard comment → no bot reply\\n+        3) /visor help → single assistant reply (no retrigger)\\n+        4) /visor Regenerate reviews → retrigger overview\\n+        5) Fact validation enabled on comment → extract/validate/aggregate\\n+        6) Fact validation disabled on comment → only assistant, no validation steps\\n+        7) PR synchronized (new commit) → overview runs again\\n+      strict: true\\n+      flow:\\n+        - name: pr-open\\n+          description: |\\n+            PR open event. Mocks overview/security/quality/performance as empty issue lists.\\n+            Expects all review steps to run and labels to be added.\\n+          event: pr_opened\\n+          fixture: gh.pr_open.minimal\\n+          mocks:\\n+            overview:\\n+              text: \\\"Overview body\\\"\\n+              tags: { label: feature, review-effort: 2 }\\n+            security: { issues: [] }\\n+            architecture: { issues: [] }\\n+            quality: { issues: [] }\\n+            performance: { issues: [] }\\n+          expect:\\n+            calls:\\n+              - step: overview\\n+                exactly: 1\\n+              - step: security\\n+                exactly: 1\\n+              - step: architecture\\n+                exactly: 1\\n+              - step: performance\\n+                exactly: 1\\n+              - step: quality\\n+                exactly: 1\\n+              - step: apply-overview-labels\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.createComment\\n+                at_least: 1\\n+              - provider: github\\n+                op: labels.add\\n+                at_least: 1\\n+                args:\\n+                  contains: [feature]\\n+            prompts:\\n+              - step: overview\\n+                contains:\\n+                  - \\\"feat: add user search\\\"\\n+\\n+        - name: standard-comment\\n+          description: |\\n+            A regular human comment on a PR should not produce a bot reply.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.standard\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"\\\"   # empty text to avoid posting a reply\\n+              intent: comment_reply\\n+          expect:\\n+            no_calls:\\n+              - provider: github\\n+                op: issues.createComment\\n+              - step: init-fact-validation\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+\\n+        - name: visor-plain\\n+          description: |\\n+            A \\\"/visor help\\\" comment should be recognized and answered once by the assistant.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"Sure, here’s how I can help.\\\"\\n+              intent: comment_reply\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.createComment\\n+                exactly: 1\\n+            outputs:\\n+              - step: comment-assistant\\n+                path: intent\\n+                equals: comment_reply\\n+            prompts:\\n+              - step: comment-assistant\\n+                matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"\\n+\\n+        - name: visor-retrigger\\n+          description: |\\n+            A \\\"/visor Regenerate reviews\\\" comment should set intent=comment_retrigger\\n+            and schedule a new overview.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_regenerate\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"Regenerating.\\\"\\n+              intent: comment_retrigger\\n+            overview:\\n+              text: \\\"Overview (regenerated)\\\"\\n+              tags: { label: feature, review-effort: 2 }\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+              - step: overview\\n+                exactly: 1\\n+              - step: security\\n+                exactly: 1\\n+              - step: architecture\\n+                exactly: 1\\n+              - step: performance\\n+                exactly: 1\\n+              - step: quality\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.createComment\\n+                at_least: 1\\n+            outputs:\\n+              - step: comment-assistant\\n+                path: intent\\n+                equals: comment_retrigger\\n+            prompts:\\n+              - step: comment-assistant\\n+                contains: [\\\"Regenerate reviews\\\"]\\n+\\n+        - name: facts-enabled\\n+          description: |\\n+            With fact validation enabled, the assistant reply is followed by\\n+            extract-facts, validate-fact (per fact), and aggregate-validations.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"We rely on defaults/.visor.yaml line 11 for max_parallelism=4.\\\"\\n+              intent: comment_reply\\n+            extract-facts:\\n+              - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+            validate-fact[]:\\n+              - fact_id: f1\\n+                claim: \\\"max_parallelism defaults to 4\\\"\\n+                is_valid: true\\n+                confidence: high\\n+                evidence: \\\"defaults/.visor.yaml:11\\\"\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+              - step: extract-facts\\n+                exactly: 1\\n+              - step: validate-fact\\n+                at_least: 1\\n+              - step: aggregate-validations\\n+                exactly: 1\\n+              - step: init-fact-validation\\n+                exactly: 1\\n+          # Prompt assertions are validated separately in stage-level prompt tests\\n+\\n+        - name: facts-invalid\\n+          description: |\\n+            Invalid fact path: after assistant reply, extract-facts finds one claim and\\n+            validate-fact returns is_valid=false; aggregate-validations detects not-all-valid\\n+            and reruns the assistant once with correction context.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"We rely on defaults/.visor.yaml line 11 for max_parallelism=4.\\\"\\n+              intent: comment_reply\\n+            extract-facts:\\n+              - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+            validate-fact[]:\\n+              - fact_id: f1\\n+                claim: \\\"max_parallelism defaults to 4\\\"\\n+                is_valid: false\\n+                confidence: high\\n+                evidence: \\\"defaults/.visor.yaml:11 does not set 4\\\"\\n+                correction: \\\"max_parallelism defaults to 3\\\"\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 2\\n+              - step: extract-facts\\n+                exactly: 1\\n+              - step: validate-fact\\n+                at_least: 1\\n+              - step: aggregate-validations\\n+                exactly: 1\\n+              - step: init-fact-validation\\n+                exactly: 1\\n+            prompts:\\n+              - step: comment-assistant\\n+                index: last\\n+                contains:\\n+                  - \\\"<previous_response>\\\"\\n+                  - \\\"max_parallelism defaults to 4\\\"\\n+                  - \\\"Correction: max_parallelism defaults to 3\\\"\\n+\\n+        - name: facts-two-items\\n+          description: |\\n+            Two facts extracted; only the invalid fact should appear in the correction pass.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"We rely on defaults/.visor.yaml for concurrency defaults.\\\"\\n+              intent: comment_reply\\n+            extract-facts:\\n+              - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+              - { id: f2, category: Feature,       claim: \\\"Fast mode is enabled by default\\\", verifiable: true }\\n+            validate-fact[]:\\n+              - { fact_id: f1, claim: \\\"max_parallelism defaults to 4\\\", is_valid: false, confidence: high, evidence: \\\"defaults/.visor.yaml:11\\\", correction: \\\"max_parallelism defaults to 3\\\" }\\n+              - { fact_id: f2, claim: \\\"Fast mode is enabled by default\\\", is_valid: true, confidence: high, evidence: \\\"src/config.ts:FAST_MODE=true\\\" }\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 2\\n+              - step: extract-facts\\n+                exactly: 1\\n+              - step: validate-fact\\n+                exactly: 2\\n+              - step: aggregate-validations\\n+                exactly: 1\\n+              - step: init-fact-validation\\n+                exactly: 1\\n+            outputs:\\n+              - step: validate-fact\\n+                where: { path: fact_id, equals: f1 }\\n+                path: is_valid\\n+                equals: false\\n+              - step: validate-fact\\n+                where: { path: fact_id, equals: f2 }\\n+                path: is_valid\\n+                equals: true\\n+            prompts:\\n+              - step: comment-assistant\\n+                index: last\\n+                contains:\\n+                  - \\\"<previous_response>\\\"\\n+                  - \\\"max_parallelism defaults to 4\\\"\\n+                  - \\\"Correction: max_parallelism defaults to 3\\\"\\n+                not_contains:\\n+                  - \\\"Fast mode is enabled by default\\\"\\n+\\n+        - name: facts-disabled\\n+          description: |\\n+            With fact validation disabled, only the assistant runs; no validation steps execute.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.standard\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"false\\\"\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+            no_calls:\\n+              - step: init-fact-validation\\n+              - step: extract-facts\\n+              - step: validate-fact\\n+              - step: aggregate-validations\\n+\\n+        - name: pr-updated\\n+          description: |\\n+            When a new commit is pushed (synchronize), overview should run again\\n+            and post/refresh a comment.\\n+          event: pr_updated\\n+          fixture: gh.pr_sync.minimal\\n+          mocks:\\n+            overview:\\n+              text: \\\"Overview for new commit\\\"\\n+              tags: { label: feature, review-effort: 3 }\\n+          expect:\\n+            calls:\\n+              - step: overview\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.updateComment\\n+                at_least: 1\\n+\\n+    - name: security-fail-if\\n+      description: |\\n+        Verifies that the global fail_if trips when security produces an error‑severity issue.\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview text\\\"\\n+          tags:\\n+            label: bug\\n+            review-effort: 3\\n+        security:\\n+          issues:\\n+            - id: S-001\\n+              file: src/search.ts\\n+              line: 10\\n+              message: \\\"Command injection risk\\\"\\n+              severity: error\\n+              category: security\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: security\\n+            exactly: 1\\n+        outputs:\\n+          - step: security\\n+            path: \\\"issues[0].severity\\\"\\n+            equals: error\\n+        fail:\\n+          message_contains: \\\"fail_if\\\"\\n+\\n+    - name: strict-mode-example\\n+      skip: true\\n+      description: |\\n+        Demonstrates strict mode: a step executed without a corresponding expect\\n+        (apply-overview-labels) triggers a strict_violation with a helpful message.\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: \\\"Short overview\\\"\\n+          tags:\\n+            label: chore\\n+            review-effort: 1\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+        strict_violation:\\n+          for_step: apply-overview-labels\\n+          message_contains: \\\"Add an expect for this step or set strict: false\\\"\\n+\\n+    - name: visor-plain-prompt\\n+      description: |\\n+        Standalone prompt check for a \\\"/visor help\\\" comment.\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_help\\n+      mocks:\\n+        comment-assistant:\\n+          text: \\\"Here is how I can help.\\\"\\n+          intent: comment_reply\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+        prompts:\\n+          - step: comment-assistant\\n+            matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"\\n+\\n+    - name: visor-retrigger-prompt\\n+      description: |\\n+        Standalone prompt check for \\\"/visor Regenerate reviews\\\".\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_regenerate\\n+      strict: false\\n+      mocks:\\n+        comment-assistant:\\n+          text: \\\"Regenerating.\\\"\\n+          intent: comment_retrigger\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+        prompts:\\n+          - step: comment-assistant\\n+            contains: [\\\"Regenerate reviews\\\"]\\n+\\n+    - name: command-mock-shape\\n+      description: |\\n+        Illustrates command provider mocking and output assertions.\\n+        Skipped by default; enable when command steps exist.\\n+      skip: true  # illustrative only, enable when a command step exists\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        unit-tests:\\n+          stdout: '{\\\"passed\\\": 128, \\\"failed\\\": 0, \\\"duration_sec\\\": 1.2}'\\n+          exit_code: 0\\n+      expect:\\n+        calls:\\n+          - step: unit-tests\\n+            exactly: 1\\n+        outputs:\\n+          - step: unit-tests\\n+            path: failed\\n+            equals: 0\\n+\\n+    - name: github-negative-mode\\n+      description: |\\n+        Demonstrates negative GitHub recorder mode: simulate a 429 error and assert failure path.\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      github_recorder: { error_code: 429 }\\n+      # Override defaults for this case only by specifying a local recorder via env-like knob\\n+      # The runner reads tests.defaults.github_recorder; we provide it at the suite level by default.\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview body\\\"\\n+          tags: { label: feature, review-effort: 2 }\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+        fail:\\n+          message_contains: \\\"github/op_failed\\\"\\n+\\n+    - name: facts-invalid\\n+      skip: true\\n+      description: |\\n+        With fact validation enabled and an invalid fact, aggregate-validations should detect\\n+        not-all-valid and route back to the assistant for a correction pass in the same stage.\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_help\\n+      env:\\n+        ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+      mocks:\\n+        comment-assistant:\\n+          text: \\\"We rely on defaults/.visor.yaml line 11 for max_parallelism=4.\\\"\\n+          intent: comment_reply\\n+        extract-facts:\\n+          - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+        validate-fact[]:\\n+          - fact_id: f1\\n+            claim: \\\"max_parallelism defaults to 4\\\"\\n+            is_valid: false\\n+            confidence: high\\n+            evidence: \\\"defaults/.visor.yaml:11 does not set 4\\\"\\n+            correction: \\\"max_parallelism defaults to 3\\\"\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+          - step: extract-facts\\n+            exactly: 1\\n+          - step: validate-fact\\n+            at_least: 1\\n+          - step: aggregate-validations\\n+            exactly: 1\\n+          - step: init-fact-validation\\n+            exactly: 1\\n\",\"status\":\"added\"},{\"filename\":\"defaults/.visor.yaml\",\"additions\":1,\"deletions\":1,\"changes\":25,\"patch\":\"diff --git a/defaults/.visor.yaml b/defaults/.visor.yaml\\nindex 0e018884..21fad9eb 100644\\n--- a/defaults/.visor.yaml\\n+++ b/defaults/.visor.yaml\\n@@ -452,8 +452,9 @@ steps:\\n         - Provide partial answers when possible, and indicate what additional information would help give a complete response\\n     on: [issue_comment]\\n     on_success:\\n-      # Always initialize fact validation attempt counter\\n-      run: [init-fact-validation]\\n+      # Initialize fact validation attempt counter only when validation is enabled\\n+      run_js: |\\n+        return env.ENABLE_FACT_VALIDATION === 'true' ? ['init-fact-validation'] : []\\n       # Preserve intent-based rerun: allow members to retrigger overview from a comment\\n       goto_js: |\\n         const intent = (typeof output === 'object' && output) ? output.intent : undefined;\\n@@ -619,8 +620,14 @@ steps:\\n     # After all facts are validated, aggregate results and decide next action\\n     on_finish:\\n       run: [aggregate-validations]\\n+      # If aggregation stored validation issues in memory, schedule a correction reply\\n+      run_js: |\\n+        const issues = memory.list('fact-validation').includes('fact_validation_issues')\\n+          ? memory.get('fact_validation_issues', 'fact-validation')\\n+          : [];\\n+        return Array.isArray(issues) && issues.length > 0 ? ['comment-assistant'] : [];\\n       goto_js: |\\n-        const allValid = memory.get('all_valid', 'fact-validation');\\n+        const allValid = memory.get('all_facts_valid', 'fact-validation');\\n         const attempt = memory.get('fact_validation_attempt', 'fact-validation') || 0;\\n \\n         log('🔍 Fact validation complete - allValid:', allValid, 'attempt:', attempt);\\n@@ -702,11 +709,10 @@ steps:\\n     on: [issue_opened, issue_comment]\\n     if: \\\"env.ENABLE_FACT_VALIDATION === 'true'\\\"\\n     on_success:\\n-      goto_js: |\\n-        // Route back to the appropriate assistant if there are issues\\n-        if (!output || output.all_valid) return null;\\n-        const hasComment = !!(outputs['comment-assistant']) || (outputs.history && (outputs.history['comment-assistant'] || []).length > 0);\\n-        return hasComment ? 'comment-assistant' : 'issue-assistant';\\n+      # Schedule the correction reply directly (target-only) when not all facts are valid\\n+      run_js: |\\n+        if (!output || output.all_valid) return [];\\n+        return ['comment-assistant'];\\n     memory_js: |\\n       const validations = outputs.history['validate-fact'] || [];\\n \\n@@ -722,8 +728,7 @@ steps:\\n       log('Results: valid=' + (validations.length - invalid.length - lowConfidence.length),\\n           'invalid=' + invalid.length, 'low-confidence=' + lowConfidence.length);\\n \\n-      memory.set('all_valid', allValid, 'fact-validation');\\n-      memory.set('total_validations', validations.length, 'fact-validation');\\n+      memory.set('all_facts_valid', allValid, 'fact-validation');\\n       memory.set('validation_results', validations, 'fact-validation');\\n       memory.set('invalid_facts', invalid, 'fact-validation');\\n       memory.set('low_confidence_facts', lowConfidence, 'fact-validation');\\n\",\"status\":\"modified\"},{\"filename\":\"docs/test-framework-rfc.md\",\"additions\":22,\"deletions\":0,\"changes\":641,\"patch\":\"diff --git a/docs/test-framework-rfc.md b/docs/test-framework-rfc.md\\nnew file mode 100644\\nindex 00000000..7b899980\\n--- /dev/null\\n+++ b/docs/test-framework-rfc.md\\n@@ -0,0 +1,641 @@\\n+# Visor Integration Test Framework (RFC)\\n+\\n+Status: In Progress\\n+Date: 2025-10-27\\n+Owners: @probelabs/visor\\n+\\n+## Summary\\n+\\n+Add a first‑class, YAML‑native integration test framework for Visor that lets teams describe user flows, mocks, and assertions directly alongside their Visor config. Tests are defined in a separate YAML that can `extends` the base configuration, run entirely offline (no network), and default to strict verification.\\n+\\n+Key ideas:\\n+- Integration‑first: simulate real GitHub events and repo context; no manual step lists.\\n+- Strict by default: if a step ran and you didn’t assert it, the test fails.\\n+- Provider record mode by default: GitHub calls are intercepted and recorded (no network); assert them later.\\n+- Simple mocks keyed by step name; schema‑aware AI outputs (objects/arrays for structured schemas; `text` for plain).\\n+- Support multi‑event “flows” that preserve memory and outputs across events.\\n+\\n+## Motivation\\n+\\n+- Keep tests next to config and use the same mental model: events → checks → outputs → effects.\\n+- Validate real behavior (routing, `on` filters, `if` guards, `goto`/`on_success`, forEach) rather than unit‑style steps.\\n+- Make CI reliable and offline by default while still asserting side‑effects (labels, comments, check runs).\\n+\\n+## Non‑Goals\\n+\\n+- Unit testing individual providers (covered by Jest/TS tests).\\n+- Golden CI logs; we assert structured outputs and recorded operations instead.\\n+\\n+## Terminology\\n+\\n+- Case: a single integration test driven by one event + fixture.\\n+- Flow: an ordered list of cases; runner preserves memory/outputs across steps.\\n+- Fixture: a reusable external context (webhook payload, changed files, env, fs overlay, frozen clock).\\n+\\n+## File Layout\\n+\\n+- Base config (unchanged): `defaults/.visor.yaml` (regular steps live here).\\n+- Test suite (new): `defaults/.visor.tests.yaml`\\n+  - `extends: \\\".visor.yaml\\\"` to inherit the base checks.\\n+  - Contains `tests.defaults`, `tests.fixtures`, `tests.cases`.\\n+\\n+## Default Behaviors (Test Mode)\\n+\\n+- Strict mode: enabled by default (`tests.defaults.strict: true`). Any executed step must appear in `expect.calls`, or the case fails.\\n+- GitHub recording: the runner uses a recording Octokit by default; no network calls are made. Assert effects via `expect.calls` with `provider: github` and an `op` (e.g., `issues.createComment`, `labels.add`, `checks.create`).\\n+- AI provider: `mock` by default for tests; schema‑aware handling (see below).\\n+\\n+## Built‑in Fixtures and GitHub Mocks\\n+\\n+The runner ships with a library of built‑in fixtures and a recording GitHub mock so you don’t have to redefine common scenarios.\\n+\\n+### Built‑in Fixtures (gh.*)\\n+\\n+Use via `fixture: <name>`:\\n+\\n+- `gh.pr_open.minimal` — pull_request opened with a small PR (branch/base, 1–2 files, tiny patch).\\n+- `gh.pr_sync.minimal` — pull_request synchronize (new commit pushed) with updated HEAD SHA.\\n+- `gh.issue_open.minimal` — issues opened with a short title/body.\\n+- `gh.issue_comment.standard` — issue_comment created with a normal message on a PR.\\n+- `gh.issue_comment.visor_help` — issue_comment created with \\\"/visor help\\\".\\n+- `gh.issue_comment.visor_regenerate` — issue_comment created with \\\"/visor Regenerate reviews\\\".\\n+- `gh.issue_comment.edited` — issue_comment edited event.\\n+- `gh.pr_closed.minimal` — pull_request closed event.\\n+\\n+All gh.* fixtures populate:\\n+- `webhook` (name, action, payload)\\n+- `git` (branch, baseBranch)\\n+- `files` and `diff` (for PR fixtures)\\n+- `env` and `time.now` for determinism\\n+\\n+Optional overrides (future):\\n+\\n+```yaml\\n+fixture:\\n+  builtin: gh.pr_open.minimal\\n+  overrides:\\n+    pr.title: \\\"feat: custom title\\\"\\n+    webhook.payload.pull_request.number: 42\\n+```\\n+\\n+### GitHub Recorder (Built‑in)\\n+\\n+By default in test mode, the runner installs a recording Octokit:\\n+- Captures all calls and args for assertions (`expect.calls` with `provider: github`).\\n+- Returns stable stub shapes to unblock flows:\\n+  - `issues.createComment` → `{ data: { id, html_url, body, user, created_at } }`\\n+  - `issues.updateComment` → same shape\\n+  - `pulls.get`, `pulls.listFiles` → derived from fixture\\n+  - `checks.create`, `checks.update` → `{ data: { id, status, conclusion, url } }`\\n+  - `labels.add` → `{ data: { labels: [ ... ] } }` (or a no‑op with capture)\\n+\\n+  No network calls are made. You can still opt into real Octokit in the future with a `mode: passthrough` runner flag (not default).\\n+  Optional negative modes (per case or global):\\n+  - `error(429|422|404)` — simulate API errors; captured in call history.\\n+  - `timeout(1000ms)` — simulate request timeouts.\\n+\\n+## YAML Syntax Overview\\n+\\n+Minimal suite:\\n+\\n+```yaml\\n+version: \\\"1.0\\\"\\n+extends: \\\".visor.yaml\\\"\\n+\\n+tests:\\n+  defaults:\\n+    strict: true\\n+    ai_provider: mock\\n+    macros:\\n+      expect_review_posted:\\n+        calls:\\n+          - provider: github\\n+            op: issues.createComment\\n+            at_least: 1\\n+\\n+  fixtures: []   # (Optional) rely on gh.* built‑ins\\n+\\n+  cases:\\n+    - name: label-flow\\n+      event: pr_opened\\n+      fixture:\\n+        builtin: gh.pr_open.minimal\\n+        overrides:\\n+          pr.title: \\\"feat: add user search\\\"\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview body\\\"\\n+          tags: { label: feature, review-effort: 2 }\\n+      expect:\\n+        use: [expect_review_posted]\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args:\\n+              contains_unordered: [feature, \\\"review/effort:2\\\"]\\n+```\\n+\\n+### Flows (multi‑event)\\n+\\n+```yaml\\n+- name: pr-review-e2e-flow\\n+  strict: true\\n+  flow:\\n+    - name: pr-open\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview: { text: \\\"Overview body\\\", tags: { label: feature, review-effort: 2 } }\\n+        security: { issues: [] }\\n+        quality: { issues: [] }\\n+        performance: { issues: [] }\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: security\\n+            exactly: 1\\n+          - step: architecture\\n+            exactly: 1\\n+          - step: performance\\n+            exactly: 1\\n+          - step: quality\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: issues.createComment\\n+            at_least: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args: { contains: [feature] }\\n+\\n+    - name: visor-plain\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_help\\n+      mocks:\\n+        comment-assistant: { text: \\\"Sure, here's how I can help.\\\", intent: comment_reply }\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+          - provider: github\\n+            op: issues.createComment\\n+            exactly: 1\\n+        outputs:\\n+          - step: comment-assistant\\n+            path: intent\\n+            equals: comment_reply\\n+```\\n+\\n+## CLI Usage\\n+\\n+- Discover tests:\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --list`\\n+- Validate test file shape (schema):\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --validate`\\n+- Run all tests with compact progress (default):\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml`\\n+- Run a single case:\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --only label-flow`\\n+- Run a single stage in a flow (by name or 1‑based index):\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --only pr-review-e2e-flow#facts-invalid`\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --only pr-review-e2e-flow#3`\\n+- Emit artifacts:\\n+  - JSON: `--json output/visor-tests.json`\\n+  - JUnit: `--report junit:output/visor-tests.xml`\\n+  - Markdown summary: `--summary md:output/visor-tests.md`\\n+- Debug logs:\\n+  - Set `VISOR_DEBUG=true` for verbose routing/provider output.\\n+\\n+Notes\\n+- AI is forced to `mock` in test mode regardless of API keys.\\n+- The runner warns when an AI/command step runs without a mock (suppressed for `ai.provider=mock`).\\n+- Strict mode is on by default; add `strict: false` for prompt‑only cases.\\n+\\n+## Mocks (Schema‑Aware)\\n+\\n+- Keyed by step name under `mocks`.\\n+- AI with structured `schema` (e.g., `code-review`, `issue-assistant`): provide an object or array directly; no `returns` key.\\n+- AI with `schema: plain`: provide a string (or an object with `text`).\\n+- Command provider: `{ stdout: string, exit_code?: number }`.\\n+- Arrays: return arrays directly (e.g., `extract-facts`).\\n+\\n+Examples:\\n+\\n+```yaml\\n+mocks:\\n+  overview:\\n+    text: \\\"Overview body\\\"\\n+    tags: { label: feature, review-effort: 2 }\\n+\\n+  issue-assistant:\\n+    text: \\\"Thanks for the detailed report!\\\"\\n+    intent: issue_triage\\n+    labels: [bug]\\n+\\n+  extract-facts:\\n+    - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+\\n+  unit-tests:\\n+    stdout: '{\\\"passed\\\": 128, \\\"failed\\\": 0}'\\n+    exit_code: 0\\n+```\\n+\\n+## Assertions\\n+\\n+### Macros (Reusable Assertions)\\n+\\n+Define named bundles of assertions under `tests.defaults.macros` and reuse them via `expect.use: [macroName, ...]`.\\n+\\n+Example:\\n+\\n+```yaml\\n+tests:\\n+  defaults:\\n+    macros:\\n+      expect_review_posted:\\n+        calls:\\n+          - provider: github\\n+            op: issues.createComment\\n+            at_least: 1\\n+\\n+cases:\\n+  - name: example\\n+    expect:\\n+      use: [expect_review_posted]\\n+      calls:\\n+        - step: overview\\n+          exactly: 1\\n+```\\n+\\n+- Step calls: `expect.calls: [{ step: <name>, exactly|at_least|at_most: N }]`.\\n+- GitHub effects: `expect.calls: [{ provider: github, op: <owner.method>, times?, args? }]`.\\n+  - `op` examples: `issues.createComment`, `labels.add`, `checks.create`, `checks.update`.\\n+  - `args.contains` matches arrays/strings; `args.contains_unordered` ignores order; `args.equals` for strict equality.\\n+- Outputs: `expect.outputs: [{ step, path, equals|matches|equalsDeep }]`.\\n+  - `equalsDeep` performs deep structural comparison for objects/arrays.\\n+  - `path` uses dot/bracket syntax, e.g., `tags['review-effort']`, `issues[0].severity`.\\n+- Failures: `expect.fail.message_contains` for error message anchoring.\\n+- Strict violations: `expect.strict_violation.for_step` asserts the runner surfaced “step executed without expect.”\\n+\\n+### Prompt Assertions (AI)\\n+\\n+When mocking AI, you can assert on the final prompt text constructed by Visor (after Liquid templating and context injection):\\n+\\n+```yaml\\n+expect:\\n+  prompts:\\n+    - step: overview\\n+      contains:\\n+        - \\\"feat: add user search\\\"        # PR title from fixture\\n+        - \\\"diff --git a/src/search.ts\\\"   # patch content included\\n+      not_contains:\\n+        - \\\"BREAKING CHANGE\\\"\\n+    - step: comment-assistant\\n+      matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"   # case-insensitive regex\\n+```\\n+\\n+Rules:\\n+- `contains`: list of substrings that must appear in the prompt.\\n+- `not_contains`: list of substrings that must not appear.\\n+- `matches`: a single regex pattern string; add `(?i)` for case‑insensitive.\\n+- The runner captures the exact prompt Visor would send to the provider (with dynamic variables resolved and code context embedded) and evaluates these assertions.\\n+\\n+## Runner Semantics\\n+\\n+- Loads base config via `extends` and validates.\\n+- Applies fixture:\\n+  - Webhook payload → test event context\\n+  - Git metadata (branch/baseBranch)\\n+  - Files + patch list used by analyzers/prompts\\n+  - `fs_overlay` writes transient files (cleaned up after)\\n+  - `env` overlays process env for the case\\n+  - `time.now` freezes clock\\n+- Event routing: determines which checks run by evaluating `on`, `if`, `depends_on`, `goto`, `on_success`, and `forEach` semantics in the normal engine.\\n+- Recording providers:\\n+  - GitHub: recording Octokit (default) captures every call; no network.\\n+  - AI: mock provider that emits objects/arrays/strings per mocks and records the final prompt text per step for `expect.prompts`.\\n+\\n+### Call History and Recursion\\n+\\n+Some steps (e.g., fact validation loops) can run multiple times within a single case or flow stage. The runner records an invocation history for each step. You assert using the same top‑level sections (calls, prompts, outputs) with selectors:\\n+\\n+1) Count only\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: validate-fact\\n+      at_least: 2\\n+```\\n+\\n+2) Per‑call assertions by index (ordered)\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: validate-fact\\n+      exactly: 2\\n+  prompts:\\n+    - step: validate-fact\\n+      index: 0\\n+      contains: [\\\"Claim:\\\", \\\"max_parallelism defaults to 4\\\"]\\n+  outputs:\\n+    - step: validate-fact\\n+      index: 0\\n+      path: fact_id\\n+      equals: f1\\n+```\\n+\\n+3) Per‑call assertions without assuming order (filter by output)\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: validate-fact\\n+      at_least: 2\\n+  outputs:\\n+    - step: validate-fact\\n+      where: { path: fact_id, equals: f1 }\\n+      path: is_valid\\n+      equals: true\\n+    - step: validate-fact\\n+      where: { path: fact_id, equals: f2 }\\n+      path: confidence\\n+      equals: high\\n+```\\n+\\n+4) Select a specific history element\\n+\\n+```yaml\\n+expect:\\n+  prompts:\\n+    - step: validate-fact\\n+      index: last   # or 0,1,..., or 'first'\\n+      not_contains: [\\\"TODO\\\"]\\n+```\\n+  - HTTP: built‑in mock (url/method/status/body/latency) with record mode and assertions.\\n+  - Command: mock stdout/stderr/exit_code; record invocation for assertions.\\n+ - State across flows: `memory`, `outputs.history`, and step outputs persist across events within a single flow.\\n+- Strict enforcement: after execution, compare executed steps to `expect.calls`; any missing expect fails the case.\\n+\\n+## Validation & Helpful Errors\\n+\\n+- Reuse Visor's existing Ajv pipeline for the base config (`extends` target).\\n+- The tests DSL is validated at runtime with friendly errors (no separate schema file to maintain).\\n+- Errors show the YAML path, a short hint, and an example (e.g., suggest `args.contains_unordered` when order differs).\\n+- Inline diffs for strings (prompts) and objects (with deep compare) in failure output.\\n+\\n+### Determinism & Security\\n+\\n+- Stable IDs in the GitHub recorder (deterministic counters per run).\\n+- Order‑agnostic assertions for arrays via `args.contains_unordered`.\\n+- Prompt normalization (whitespace, code fences). Toggle with `--normalize-prompts=false`.\\n+- Secret redaction in prompts/args via ENV allowlist (default deny; redacts to `****`).\\n+\\n+## CLI\\n+\\n+```\\n+visor test --config defaults/.visor.tests.yaml         # run all cases\\n+visor test --config defaults/.visor.tests.yaml --only pr-review-e2e-flow\\n+visor test --config defaults/.visor.tests.yaml --list  # list case names\\n+```\\n+\\n+Exit codes:\\n+- 0: all tests passed\\n+- 1: one or more cases failed\\n+\\n+### CLI Output UX (must‑have)\\n+\\n+The runner prints a concise, human‑friendly summary optimized for scanning:\\n+\\n+- Suite header with total cases and elapsed time.\\n+- Per‑case line with status symbol and duration, e.g.,\\n+  - ✅ label-flow (1.23s)\\n+  - ❌ security-fail-if (0.42s)\\n+- When a case is expanded (auto‑expand on failure):\\n+  - Input context: event + fixture name.\\n+  - Executed steps (in order), with counts for multi‑call steps.\\n+  - Assertions grouped by type (calls, prompts, outputs) with checkmarks.\\n+  - GitHub calls table (op, count, first args snippet).\\n+  - Prompt preview (truncated) with a toggle to show full text.\\n+  - First mismatch shows an inline diff (expected vs actual substring/regex or value), with a clear hint to fix.\\n+- Flow cases show each stage nested under the parent with roll‑up status.\\n+- Summary footer with pass/fail counts, slowest cases, and a hint to rerun focused:\\n+  - e.g., visor test --config defaults/.visor.tests.yaml --only security-fail-if\\n+\\n+Color, symbols, and truncation rules mirror our main CLI:\\n+- Green checks for passes, red crosses for failures, yellow for skipped.\\n+- Truncate long prompts/JSON with ellipsis; provide a flag `--verbose` to show full payloads.\\n+\\n+### Additional Flags & Modes\\n+\\n+- `--only <name>`: run a single case/flow by exact name.\\n+- `--bail`: stop at first failure.\\n+- `--json`: emit machine‑readable results to stdout.\\n+- `--report junit:path.xml`: write JUnit XML to path.\\n+- `--summary md:path.md`: write a Markdown summary artifact.\\n+- `--progress compact|detailed`: toggle rendering density.\\n+- `--max-parallel N`: reuse existing parallelism flag (no test‑specific variant).\\n+\\n+## Coverage & Reporting\\n+\\n+- Step coverage per case (executed vs expected), with a short table.\\n+- JUnit and JSON reporters for CI visualization.\\n+- Optional Markdown summary: failing cases, first mismatch, rerun hints.\\n+\\n+## Implementation Plan (Milestones)\\n+\\n+This plan delivers the test framework incrementally, minimizing risk and reusing Visor internals.\\n+\\n+Progress Tracker\\n+- Milestone 0 — DSL freeze and scaffolding — DONE (2025-10-27)\\n+- Milestone 1 — MVP runner and single‑event cases — DONE (2025-10-27)\\n+- Milestone 2 — Built‑in fixtures — DONE (2025-10-27)\\n+- Milestone 3 — Prompt capture and assertions — DONE (2025-10-27)\\n+- Milestone 4 — Multi‑call history and selectors — DONE (2025-10-27)\\n+- Milestone 5 — Flows and state persistence — DONE (2025-10-27)\\n+- Milestone 6 — HTTP/Command mocks + negative modes — DONE (2025-10-27)\\n+- Milestone 7 — CLI reporters/UX polish — DONE (2025-10-27)\\n+- Milestone 8 — Validation and helpful errors — DONE (2025-10-27)\\n+- Milestone 9 — Coverage and perf — DONE (2025-10-27)\\n+- Milestone 10 — Docs, examples, migration — PENDING\\n+\\n+Progress Update — 2025-10-28\\n+- Runner: stage execution coverage now derives only from actual prompts/output-history deltas and engine statistics (no selection heuristics). Single-check runs contribute to statistics and history uniformly.\\n+- Engine: single-check path records iteration stats and appends outputs to history; on_finish children run via unified scheduler so runs are counted.\\n+- UX: noisy debug prints gated behind VISOR_DEBUG; stage headers and coverage tables remain.\\n+- Known gap: flow stage “facts-invalid” still fails under strict because the initial assistant/validation chain does not execute under the test runner for issue_comment; aggregator fallback runs. Next step is to trace event filtering inside executeGroupedChecks and ensure the main stage selection executes event-matching checks in tests.\\n+\\n+Milestone 0 — DSL freeze and scaffolding (0.5 week) — DONE 2025-10-27\\n+- Finalize DSL keys: tests.defaults, fixtures, cases, flow, fixture, mocks, expect.{calls,prompts,outputs,fail,strict_violation}. ✅\\n+- Rename use_fixture → fixture across examples (done in this RFC and defaults/.visor.tests.yaml). ✅\\n+- Create module skeletons: ✅\\n+  - src/test-runner/index.ts (entry + orchestration)\\n+  - src/test-runner/fixture-loader.ts (builtin + overrides)\\n+  - src/test-runner/recorders/github-recorder.ts (now dynamic Proxy-based)\\n+  - src/test-runner/assertions.ts (calls/prompts/outputs types + count validator)\\n+  - src/test-runner/utils/selectors.ts (deepGet)\\n+- CLI: add visor test (discovery). ✅\\n+- Success criteria: builds pass; “hello world” run prints discovered cases. ✅ (verified via npm run build and visor test)\\n+\\n+Progress Notes\\n+- Discovery works against any .visor.tests.yaml (general-purpose, not tied to defaults).\\n+- Recording Octokit records arbitrary rest ops without hardcoding method lists.\\n+- defaults/.visor.tests.yaml updated to consistent count grammar and fixed indentation issues.\\n+\\n+Milestone 1 — MVP runner and single‑event cases (1 week) — DONE 2025-10-27 (non‑flow)\\n+- CLI: add visor test [--config path] [--only name] [--bail] [--list]. ✅\\n+- Parsing: load tests file (extends) and hydrate cases. ✅\\n+- Execution: per case (non‑flow), synthesize PRInfo and call CheckExecutionEngine once. ✅\\n+- GitHub recorder default: injected recording Octokit; no network. ✅\\n+- Assertions: expect.calls for steps and provider ops (exactly|at_least|at_most); strict mode enforced. ✅\\n+- Output: basic per‑case status + summary. ✅\\n+- Success criteria: label-flow, issue-triage, strict-mode-example, security-fail-if pass locally. ✅\\n+\\n+Notes\\n+- Flow cases are deferred to Milestone 5 (state persistence) and will be added later.\\n+- AI provider forced to mock in test mode unless overridden by suite defaults.\\n+\\n+Verification\\n+- Build CLI + SDK: npm run build — success.\\n+- Discovery: visor test --config defaults/.visor.tests.yaml --list — lists suite and cases.\\n+- Run single cases:\\n+  - visor test --config defaults/.visor.tests.yaml --only label-flow — PASS\\n+  - visor test --config defaults/.visor.tests.yaml --only issue-triage — PASS\\n+  - visor test --config defaults/.visor.tests.yaml --only security-fail-if — PASS\\n+  - visor test --config defaults/.visor.tests.yaml --only strict-mode-example — PASS\\n+- Behavior observed:\\n+  - Strict mode enforced (steps executed but not asserted would fail). \\n+  - GitHub ops recorded by default with dynamic recorder, no network calls.\\n+  - Provider and step call counts respected (exactly | at_least | at_most).\\n+\\n+Milestone 2 — Built‑in fixtures (0.5–1 week) — DONE 2025-10-27\\n+- Implement gh.* builtins: pr_open.minimal, pr_sync.minimal, issue_open.minimal, issue_comment.standard, issue_comment.visor_help, issue_comment.visor_regenerate.\\n+- Support fixture overrides (fixture: { builtin, overrides }).\\n+- Wire files+diff into the engine’s analyzers.\\n+- Success criteria: pr-review-e2e-flow “pr-open”, “standard-comment”, “visor-plain”, “visor-retrigger” run with built-ins.\\n+Notes:\\n+- gh.* builtins implemented with webhook payloads and minimal diffs for PR fixtures.\\n+- Runner accepts fixture: { builtin, overrides } and applies overrides to pr.* and webhook.* paths.\\n+- Diffs surfaced via PRInfo.fullDiff; prompts include diff header automatically.\\n+- Flow execution will be delivered in Milestone 5; the same built-ins power the standalone prompt cases added now.\\n+\\n+Milestone 3 — Prompt capture and prompt assertions (0.5 week) — DONE 2025-10-27\\n+- Capture final AI prompt string per step after Liquid/context assembly. ✅ (AICheckProvider hook)\\n+- Assertions: expect.prompts contains | not_contains | matches (regex). ✅\\n+- Add `prompts.where` selector to target a prompt from history by content. ✅\\n+- Success criteria: prompt checks pass for label-flow, issue-triage, visor-plain, visor-retrigger. ✅\\n+- Notes: added standalone cases visor-plain-prompt and visor-retrigger-prompt for prompt-only validation.\\n+\\n+Milestone 4 — Multi‑call history and selectors (1 week) — DONE 2025-10-27\\n+- Per-step invocation history recorded and exposed by engine (outputs.history). ✅\\n+- index selector for prompts and outputs (first|last|N). ✅\\n+- where selector for outputs: { path, equals|matches }. ✅\\n+- equalsDeep for outputs. ✅\\n+- contains_unordered for array outputs. ✅\\n+- Regex matches for outputs. ✅\\n+\\n+Milestone 5 — Flows and state persistence (0.5–1 week) — DONE 2025-10-27\\n+- Implemented flow execution with shared engine + recorder across stages. ✅\\n+- Preserves MemoryStore state, outputs.history and provider calls between stages. ✅\\n+- Stage-local deltas for assertions (prompts, outputs, calls). ✅\\n+- Success criteria: full pr-review-e2e-flow passes. ✅\\n+\\n+Milestone 6 — HTTP/Command mocks and advanced GitHub modes (1 week) — DONE 2025-10-27\\n+- Command mocks: runner injects mocks via ExecutionContext; provider short-circuits to return stdout/exit_code. ✅\\n+- HTTP client mocks: provider returns mocked response via ExecutionContext without network. ✅\\n+- GitHub recorder negative modes: error(code) and timeout(ms) supported via tests.defaults.github_recorder. ✅\\n+- Success criteria: command-mock-shape passes; negative modes available for future tests. ✅\\n+\\n+Milestone 7 — CLI UX polish and reporters (0.5–1 week) — DONE 2025-10-27\\n+- Flags: --json <path|->, --report junit:<path>, --summary md:<path>, --progress compact|detailed. ✅\\n+- Compact progress with per-case PASS/FAIL lines; summary at end. ✅\\n+- JSON/JUnit/Markdown reporters now include per-case details (name, pass/fail, errors). ✅\\n+\\n+Milestone 8 — Validation and helpful errors (0.5 week) — DONE 2025-10-27\\n+- Reuse ConfigManager + Ajv for base config. ✅\\n+- Lightweight runtime validation for tests DSL with precise YAML paths and hints. ✅\\n+- Add `visor test --validate` to check the tests file only (reuses runtime validation). ✅\\n+- Success criteria: common typos produce actionable errors (path + suggestion). ✅\\n+\\n+Usage:\\n+\\n+```\\n+visor test --validate --config defaults/.visor.tests.yaml\\n+```\\n+\\n+Example error output:\\n+\\n+```\\n+❌ Tests file has 2 error(s):\\n+   • tests.cases[0].expext: must NOT have additional properties (Did you mean \\\"expect\\\"?)\\n+   • tests.cases[3].event: must be equal to one of the allowed values (allowed: manual, pr_opened, pr_updated, pr_closed, issue_opened, issue_comment)\\n+```\\n+\\n+Milestone 9 — Coverage and perf (0.5 week) — DONE 2025-10-27\\n+- Per-case coverage table printed after assertions: each expected step shows desired count (e.g., =1/≥1/≤N), actual runs, and status (ok/under/over). ✅\\n+- Parallel case execution: `--max-parallel <N>` or `tests.defaults.max_parallel` enables a simple pool runner. ✅\\n+- Prompt capture throttle: `--prompt-max-chars <N>` or `tests.defaults.prompt_max_chars` truncates stored prompt text to reduce memory. ✅\\n+- Success criteria: coverage table visible; options validated locally. ✅\\n+\\n+Usage examples:\\n+\\n+```\\n+visor test --config defaults/.visor.tests.yaml --max-parallel 4\\n+visor test --config defaults/.visor.tests.yaml --prompt-max-chars 16000\\n+```\\n+\\n+Milestone 10 — Docs, examples, and migration (0.5 week) — IN PROGRESS 2025-10-27\\n+- Update README to link the RFC and defaults/.visor.tests.yaml.\\n+- Document built-in fixtures catalog and examples.\\n+- Migration note: how to move from embedded tests and from `returns` to new mocks.\\n+- Success criteria: docs reviewed; examples copy‑paste clean.\\n+\\n+Risks & Mitigations\\n+- Prompt capture bloat → truncate by default; add --verbose.\\n+- Fixture drift vs engine → keep fixtures minimal and aligned to CheckExecutionEngine needs; add contract tests.\\n+- Strict mode false positives → provide clear errors and fast “add expect” guidance.\\n+\\n+Success Metrics\\n+- 100% of default cases pass locally and in CI.\\n+- Sub‑second overhead per case on small fixtures; <10s for the full default suite.\\n+- Clear failures with a single screen of output; <1 minute to fix typical assertion mismatches.\\n+\\n+## Compatibility & Migration\\n+\\n+- Tests moved from `defaults/.visor.yaml` into `defaults/.visor.tests.yaml` with `extends: \\\".visor.yaml\\\"`.\\n+- Old `mocks.*.returns` is replaced by direct values (object/array/string).\\n+- You no longer need `run: steps` in tests; cases are integration‑driven by `event + fixture`.\\n+- `no_other_calls` is unnecessary with strict mode; it’s implied and enforced.\\n+\\n+## Open Questions\\n+\\n+- Should we support HTTP provider mocks out of the box (URL/method/body → recorded responses)?\\n+- Do we want a JSONPath for `expect.outputs.path`, or keep the current dot/bracket selector?\\n+- Snapshots of generated Markdown? Perhaps as optional golden files with normalization.\\n+\\n+## Future Work\\n+\\n+- Watch mode (`--watch`) and focused runs by regex.\\n+- Coverage‑like reports for step execution and assertions.\\n+- Built‑in fixtures for common GitHub events (shortcuts).\\n+- Golden snapshot helpers for comments and label sets (with stable normalization).\\n+- Parallelizing cases and/or flows.\\n+\\n+## Appendix: Example Suite\\n+\\n+See `defaults/.visor.tests.yaml` in the repo for a complete, multi‑event example covering:\\n+- PR opened → overview + labels\\n+- Standard PR comment → no action\\n+- `/visor` comment → reply\\n+- `/visor ... Regenerate reviews` → retrigger overview\\n+- Fact validation enabled/disabled on comment\\n+- New commit pushed to PR → refresh overview\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/assertions.md\",\"additions\":3,\"deletions\":0,\"changes\":85,\"patch\":\"diff --git a/docs/testing/assertions.md b/docs/testing/assertions.md\\nnew file mode 100644\\nindex 00000000..e5f62aca\\n--- /dev/null\\n+++ b/docs/testing/assertions.md\\n@@ -0,0 +1,85 @@\\n+# Writing Assertions\\n+\\n+Assertions live under `expect:` and cover three surfaces:\\n+\\n+- `calls`: step counts and provider effects (GitHub ops)\\n+- `prompts`: final AI prompts (post templating/context)\\n+- `outputs`: step outputs with history and selectors\\n+\\n+## Calls\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: overview\\n+      exactly: 1\\n+    - provider: github\\n+      op: labels.add\\n+      at_least: 1\\n+      args:\\n+        contains: [feature, \\\"review/effort:2\\\"]\\n+```\\n+\\n+Counts are consistent everywhere: `exactly`, `at_least`, `at_most`.\\n+\\n+## Prompts\\n+\\n+```yaml\\n+expect:\\n+  prompts:\\n+    - step: overview\\n+      contains: [\\\"feat: add user search\\\", \\\"diff --git a/src/search.ts\\\"]\\n+    - step: comment-assistant\\n+      matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"\\n+    - step: overview\\n+      # Select the prompt that mentions a specific file\\n+      where:\\n+        contains: [\\\"src/search.ts\\\"]\\n+      contains: [\\\"diff --git a/src/search.ts\\\"]\\n+```\\n+\\n+- `contains`: required substrings\\n+- `not_contains`: forbidden substrings\\n+- `matches`: regex (prefix `(?i)` for case-insensitive)\\n+- `index`: `first` | `last` | N (default: last)\\n+- `where`: selector to choose a prompt from history using `contains`/`not_contains`/`matches` before applying the assertion\\n+\\n+Tip: enable `--prompt-max-chars` or `tests.defaults.prompt_max_chars` to cap stored prompt size for large diffs.\\n+\\n+## Outputs\\n+\\n+Use `path` with dot/bracket syntax. You can select by index or by a `where` probe over the same output history.\\n+\\n+```yaml\\n+expect:\\n+  outputs:\\n+    - step: validate-fact\\n+      index: 0\\n+      path: fact_id\\n+      equals: f1\\n+    - step: validate-fact\\n+      where: { path: fact_id, equals: f2 }\\n+      path: confidence\\n+      equals: high\\n+    - step: aggregate-validations\\n+      path: all_valid\\n+      equals: true\\n+```\\n+\\n+Supported comparators:\\n+- `equals` (primitive)\\n+- `equalsDeep` (structural)\\n+- `matches` (regex)\\n+- `contains_unordered` (array membership ignoring order)\\n+\\n+## Strict mode and “no calls”\\n+\\n+Strict mode (default) fails any executed step without a corresponding `expect.calls` entry. You can also assert absence explicitly:\\n+\\n+```yaml\\n+expect:\\n+  no_calls:\\n+    - provider: github\\n+      op: issues.createComment\\n+    - step: extract-facts\\n+```\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/cli.md\",\"additions\":2,\"deletions\":0,\"changes\":37,\"patch\":\"diff --git a/docs/testing/cli.md b/docs/testing/cli.md\\nnew file mode 100644\\nindex 00000000..3d19fc10\\n--- /dev/null\\n+++ b/docs/testing/cli.md\\n@@ -0,0 +1,37 @@\\n+# Visor Test CLI\\n+\\n+Run integration tests for your Visor config using the built-in `test` subcommand.\\n+\\n+## Commands\\n+\\n+- Discover tests file and list cases\\n+  - `visor test --list [--config defaults/.visor.tests.yaml]`\\n+- Run cases\\n+  - `visor test [--config defaults/.visor.tests.yaml] [--only <substring>] [--bail]`\\n+- Validate tests YAML without running\\n+  - `visor test --validate [--config defaults/.visor.tests.yaml]`\\n+\\n+## Flags\\n+\\n+- `--config <path>`: Path to `.visor.tests.yaml` (auto-discovers `.visor.tests.yaml` or `defaults/.visor.tests.yaml`).\\n+- `--only <filter>`: Run cases whose `name` contains the substring (case-insensitive).\\n+- `--bail`: Stop on first failure.\\n+- `--json <path|->`: Write a minimal JSON summary.\\n+- `--report junit:<path>`: Write a minimal JUnit XML.\\n+- `--summary md:<path>`: Write a minimal Markdown summary.\\n+- `--progress compact|detailed`: Progress verbosity (parsing supported; detailed view evolves over time).\\n+- `--max-parallel <N>`: Run up to N cases concurrently.\\n+- `--prompt-max-chars <N>`: Truncate captured prompt text to N characters.\\n+\\n+## Output\\n+\\n+- Per-case PASS/FAIL lines\\n+- Coverage table (expected vs actual step runs)\\n+- Summary totals\\n+\\n+## Tips\\n+\\n+- Use `--validate` when iterating on tests to catch typos early.\\n+- Keep `strict: true` in `tests.defaults` to surface missing `expect` quickly.\\n+- For large suites, increase `--max-parallel` to improve throughput.\\n+\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/fixtures-and-mocks.md\",\"additions\":3,\"deletions\":0,\"changes\":74,\"patch\":\"diff --git a/docs/testing/fixtures-and-mocks.md b/docs/testing/fixtures-and-mocks.md\\nnew file mode 100644\\nindex 00000000..d49a1f13\\n--- /dev/null\\n+++ b/docs/testing/fixtures-and-mocks.md\\n@@ -0,0 +1,74 @@\\n+# Fixtures and Mocks\\n+\\n+Integration tests simulate outside world inputs and provider outputs.\\n+\\n+## Built-in GitHub fixtures (gh.*)\\n+\\n+Use via `fixture: gh.<name>` or `fixture: { builtin: gh.<name>, overrides: {...} }`.\\n+\\n+- `gh.pr_open.minimal` — pull_request opened with a tiny diff and `src/search.ts` file.\\n+- `gh.pr_sync.minimal` — pull_request synchronize with a small follow-up diff.\\n+- `gh.pr_closed.minimal` — pull_request closed event.\\n+- `gh.issue_open.minimal` — issues opened (short title/body).\\n+- `gh.issue_comment.standard` — normal human comment on a PR/issue.\\n+- `gh.issue_comment.visor_help` — comment containing `/visor help`.\\n+- `gh.issue_comment.visor_regenerate` — `/visor Regenerate reviews`.\\n+\\n+Overrides allow tailored inputs:\\n+\\n+```yaml\\n+fixture:\\n+  builtin: gh.pr_open.minimal\\n+  overrides:\\n+    pr.title: \\\"feat: custom title\\\"\\n+    webhook.payload.pull_request.number: 42\\n+```\\n+\\n+## GitHub recorder\\n+\\n+The test runner injects a recording Octokit by default:\\n+\\n+- Captures every GitHub op+args for assertions (`expect.calls` with `provider: github`).\\n+- Returns stable stub shapes so flows can continue without network.\\n+- Negative modes are available globally via `tests.defaults.github_recorder`:\\n+\\n+```yaml\\n+tests:\\n+  defaults:\\n+    github_recorder:\\n+      error_code: 429      # simulate API error\\n+      timeout_ms: 1000     # simulate request timeout\\n+```\\n+\\n+## Mocks\\n+\\n+Mocks are keyed by step name under `mocks`.\\n+\\n+Examples:\\n+\\n+```yaml\\n+mocks:\\n+  # AI with structured schema\\n+  overview:\\n+    text: \\\"High-level PR summary.\\\"\\n+    tags: { label: feature, review-effort: 2 }\\n+\\n+  # AI plain text schema\\n+  comment-assistant:\\n+    text: \\\"Sure, here’s how I can help.\\\"\\n+    intent: comment_reply\\n+\\n+  # Array outputs (e.g., extract-facts)\\n+  extract-facts:\\n+    - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+\\n+  # Command provider\\n+  unit-tests:\\n+    stdout: '{\\\"passed\\\": 128, \\\"failed\\\": 0}'\\n+    exit_code: 0\\n+```\\n+\\n+Notes:\\n+- No `returns:` key; provide values directly.\\n+- For HTTP/Command providers, mocks bypass real execution and are recorded for assertions.\\n+\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/getting-started.md\",\"additions\":4,\"deletions\":0,\"changes\":88,\"patch\":\"diff --git a/docs/testing/getting-started.md b/docs/testing/getting-started.md\\nnew file mode 100644\\nindex 00000000..c55996ef\\n--- /dev/null\\n+++ b/docs/testing/getting-started.md\\n@@ -0,0 +1,88 @@\\n+# Visor Tests — Getting Started\\n+\\n+This is the developer-facing guide for writing and running integration tests for your Visor config. It focuses on great DX: minimal setup, helpful errors, and clear output.\\n+\\n+## TL;DR\\n+\\n+- Put your tests in `defaults/.visor.tests.yaml`.\\n+- Reference your base config with `extends: \\\".visor.yaml\\\"`.\\n+- Use built-in GitHub fixtures like `gh.pr_open.minimal`.\\n+- Run with `visor test --config defaults/.visor.tests.yaml`.\\n+- Validate only with `visor test --validate`.\\n+\\n+```yaml\\n+version: \\\"1.0\\\"\\n+extends: \\\".visor.yaml\\\"\\n+\\n+tests:\\n+  defaults:\\n+    strict: true           # every executed step must be asserted\\n+    ai_provider: mock      # offline by default\\n+  cases:\\n+    - name: label-flow\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview body\\\"\\n+          tags: { label: feature, review-effort: 2 }\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args: { contains: [feature] }\\n+```\\n+\\n+## Why integration tests in YAML?\\n+\\n+- You test the same thing you ship: events → checks → outputs → effects.\\n+- No network required; GitHub calls are recorded, AI is mocked.\\n+- Flows let you simulate real user journeys across multiple events.\\n+\\n+## Strict by default\\n+\\n+If a step runs and you didn’t assert it under `expect.calls`, the case fails. This prevents silent regressions and “accidental” work.\\n+\\n+Turn off per-case via `strict: false` if you need to iterate.\\n+\\n+## CLI recipes\\n+\\n+- List cases: `visor test --list`\\n+- Run a subset: `visor test --only pr-review`\\n+- Stop on first failure: `--bail`\\n+- Validate tests file only: `--validate`\\n+- Parallelize cases: `--max-parallel 4`\\n+- Throttle prompt capture: `--prompt-max-chars 16000`\\n+\\n+## Coverage output\\n+\\n+After each case/stage, a compact table shows expected vs actual step calls:\\n+\\n+```\\n+Coverage (label-flow):\\n+  • overview                 want =1     got  1  ok\\n+  • apply-overview-labels    want =1     got  1  ok\\n+```\\n+\\n+Unexpected executed steps are listed under `unexpected:` to help you add missing assertions quickly.\\n+\\n+## Helpful validation errors\\n+\\n+Run `visor test --validate` to get precise YAML-path errors and suggestions:\\n+\\n+```\\n+❌ Tests file has 2 error(s):\\n+   • tests.cases[0].expext: must NOT have additional properties (Did you mean \\\"expect\\\"?)\\n+   • tests.cases[3].event: must be equal to one of the allowed values (allowed: manual, pr_opened, pr_updated, pr_closed, issue_opened, issue_comment)\\n+```\\n+\\n+Next steps:\\n+- See `docs/testing/fixtures-and-mocks.md` to simulate inputs.\\n+- See `docs/testing/assertions.md` to write robust checks.\\n+- Browse `defaults/.visor.tests.yaml` for full examples.\\n+\\n\",\"status\":\"added\"},{\"filename\":\"output/assistant-json/template.liquid\",\"additions\":0,\"deletions\":0,\"changes\":0,\"patch\":\"diff --git a/output/assistant-json/template.liquid b/output/assistant-json/template.liquid\\ndeleted file mode 100644\\nindex e69de29b..00000000\\n\",\"status\":\"modified\"},{\"filename\":\"src/ai-review-service.ts\",\"additions\":2,\"deletions\":1,\"changes\":62,\"patch\":\"diff --git a/src/ai-review-service.ts b/src/ai-review-service.ts\\nindex dac5b6e1..8adf6106 100644\\n--- a/src/ai-review-service.ts\\n+++ b/src/ai-review-service.ts\\n@@ -117,30 +117,36 @@ export class AIReviewService {\\n \\n     this.sessionRegistry = SessionRegistry.getInstance();\\n \\n-    // Auto-detect provider and API key from environment\\n-    if (!this.config.apiKey) {\\n-      if (process.env.CLAUDE_CODE_API_KEY) {\\n-        this.config.apiKey = process.env.CLAUDE_CODE_API_KEY;\\n-        this.config.provider = 'claude-code';\\n-      } else if (process.env.GOOGLE_API_KEY) {\\n-        this.config.apiKey = process.env.GOOGLE_API_KEY;\\n-        this.config.provider = 'google';\\n-      } else if (process.env.ANTHROPIC_API_KEY) {\\n-        this.config.apiKey = process.env.ANTHROPIC_API_KEY;\\n-        this.config.provider = 'anthropic';\\n-      } else if (process.env.OPENAI_API_KEY) {\\n-        this.config.apiKey = process.env.OPENAI_API_KEY;\\n-        this.config.provider = 'openai';\\n-      } else if (\\n-        // Check for AWS Bedrock credentials\\n-        (process.env.AWS_ACCESS_KEY_ID && process.env.AWS_SECRET_ACCESS_KEY) ||\\n-        process.env.AWS_BEDROCK_API_KEY\\n-      ) {\\n-        // For Bedrock, we don't set apiKey as it uses AWS credentials\\n-        // ProbeAgent will handle the authentication internally\\n-        this.config.provider = 'bedrock';\\n-        // Set a placeholder to pass validation\\n-        this.config.apiKey = 'AWS_CREDENTIALS';\\n+    // Respect explicit provider if set (e.g., 'mock' during tests) — do not override from env\\n+    const providerExplicit =\\n+      typeof this.config.provider === 'string' && this.config.provider.length > 0;\\n+\\n+    // Auto-detect provider and API key from environment only when provider not explicitly set\\n+    if (!providerExplicit) {\\n+      if (!this.config.apiKey) {\\n+        if (process.env.CLAUDE_CODE_API_KEY) {\\n+          this.config.apiKey = process.env.CLAUDE_CODE_API_KEY;\\n+          this.config.provider = 'claude-code';\\n+        } else if (process.env.GOOGLE_API_KEY) {\\n+          this.config.apiKey = process.env.GOOGLE_API_KEY;\\n+          this.config.provider = 'google';\\n+        } else if (process.env.ANTHROPIC_API_KEY) {\\n+          this.config.apiKey = process.env.ANTHROPIC_API_KEY;\\n+          this.config.provider = 'anthropic';\\n+        } else if (process.env.OPENAI_API_KEY) {\\n+          this.config.apiKey = process.env.OPENAI_API_KEY;\\n+          this.config.provider = 'openai';\\n+        } else if (\\n+          // Check for AWS Bedrock credentials\\n+          (process.env.AWS_ACCESS_KEY_ID && process.env.AWS_SECRET_ACCESS_KEY) ||\\n+          process.env.AWS_BEDROCK_API_KEY\\n+        ) {\\n+          // For Bedrock, we don't set apiKey as it uses AWS credentials\\n+          // ProbeAgent will handle the authentication internally\\n+          this.config.provider = 'bedrock';\\n+          // Set a placeholder to pass validation\\n+          this.config.apiKey = 'AWS_CREDENTIALS';\\n+        }\\n       }\\n     }\\n \\n@@ -766,6 +772,14 @@ ${this.escapeXml(prInfo.body)}\\n     <files_changed_count>${prInfo.files.length}</files_changed_count>\\n   </metadata>`;\\n \\n+    // Include a small raw diff header snippet for compatibility with tools/tests\\n+    try {\\n+      const firstFile = (prInfo.files || [])[0];\\n+      if (firstFile && firstFile.filename) {\\n+        context += `\\\\n  <raw_diff_header>\\\\n${this.escapeXml(`diff --git a/${firstFile.filename} b/${firstFile.filename}`)}\\\\n  </raw_diff_header>`;\\n+      }\\n+    } catch {}\\n+\\n     // Add PR description if available\\n     if (prInfo.body) {\\n       context += `\\n\",\"status\":\"modified\"},{\"filename\":\"src/check-execution-engine.ts\",\"additions\":21,\"deletions\":3,\"changes\":693,\"patch\":\"diff --git a/src/check-execution-engine.ts b/src/check-execution-engine.ts\\nindex 578a42dc..e9a9d733 100644\\n--- a/src/check-execution-engine.ts\\n+++ b/src/check-execution-engine.ts\\n@@ -174,6 +174,9 @@ export class CheckExecutionEngine {\\n   private onFinishLoopCounts: Map<string, number> = new Map();\\n   // Track how many times a forEach parent check has produced an array during this run (\\\"waves\\\")\\n   private forEachWaveCounts: Map<string, number> = new Map();\\n+  // One-shot guards for post on_finish scheduling to avoid duplicate replies when\\n+  // multiple signals (aggregator, memory, history) agree. Keyed by session + parent check.\\n+  private postOnFinishGuards: Set<string> = new Set();\\n   // Snapshot+Scope journal (Phase 0: commit only, no behavior changes yet)\\n   private journal: ExecutionJournal = new ExecutionJournal();\\n   private sessionId: string = `sess-${Date.now().toString(36)}-${Math.random()\\n@@ -208,7 +211,12 @@ export class CheckExecutionEngine {\\n     // Create a mock Octokit instance for local analysis\\n     // This allows us to reuse the existing PRReviewer logic without network calls\\n     this.mockOctokit = this.createMockOctokit();\\n-    this.reviewer = new PRReviewer(this.mockOctokit as unknown as import('@octokit/rest').Octokit);\\n+    // Prefer the provided authenticated/recording Octokit (from test runner or Actions)\\n+    // so that comment create/update operations are visible to recorders and assertions.\\n+    const reviewerOctokit =\\n+      (octokit as unknown as import('@octokit/rest').Octokit) ||\\n+      (this.mockOctokit as unknown as import('@octokit/rest').Octokit);\\n+    this.reviewer = new PRReviewer(reviewerOctokit);\\n   }\\n \\n   private sessionUUID(): string {\\n@@ -298,8 +306,9 @@ export class CheckExecutionEngine {\\n    */\\n   private enrichEventContext(eventContext?: Record<string, unknown>): Record<string, unknown> {\\n     const baseContext = eventContext || {};\\n-    if (this.actionContext?.octokit) {\\n-      return { ...baseContext, octokit: this.actionContext.octokit };\\n+    const injected = this.actionContext?.octokit || (baseContext as any).octokit;\\n+    if (injected) {\\n+      return { ...baseContext, octokit: injected };\\n     }\\n     return baseContext;\\n   }\\n@@ -778,6 +787,11 @@ export class CheckExecutionEngine {\\n       eventOverride,\\n       overlay,\\n     } = opts;\\n+    try {\\n+      if (debug && opts.origin === 'on_finish') {\\n+        console.error(`[runNamedCheck] origin=on_finish step=${target}`);\\n+      }\\n+    } catch {}\\n \\n     // Build context overlay from current results; prefer snapshot visibility for scope (Phase 4)\\n     const depOverlay = overlay ? new Map(overlay) : new Map(resultsMap);\\n@@ -839,6 +853,9 @@ export class CheckExecutionEngine {\\n     debug: boolean\\n   ): Promise<void> {\\n     const log = (msg: string) => (config?.output?.pr_comment ? console.error : console.log)(msg);\\n+    try {\\n+      if (debug) console.error('[on_finish] handler invoked');\\n+    } catch {}\\n \\n     // Find all checks with forEach: true and on_finish configured\\n     const forEachChecksWithOnFinish: Array<{\\n@@ -857,6 +874,11 @@ export class CheckExecutionEngine {\\n       }\\n     }\\n \\n+    try {\\n+      logger.info(\\n+        `🧭 on_finish: discovered ${forEachChecksWithOnFinish.length} forEach parent(s) with hooks`\\n+      );\\n+    } catch {}\\n     if (forEachChecksWithOnFinish.length === 0) {\\n       return; // No on_finish hooks to process\\n     }\\n@@ -870,14 +892,18 @@ export class CheckExecutionEngine {\\n       try {\\n         const forEachResult = results.get(checkName) as ExtendedReviewSummary | undefined;\\n         if (!forEachResult) {\\n-          if (debug) log(`⚠️ No result found for forEach check \\\"${checkName}\\\", skipping on_finish`);\\n+          try {\\n+            logger.info(`⏭ on_finish: no result found for \\\"${checkName}\\\" — skip`);\\n+          } catch {}\\n           continue;\\n         }\\n \\n         // Skip if the forEach check returned empty array\\n         const forEachItems = forEachResult.forEachItems || [];\\n         if (forEachItems.length === 0) {\\n-          if (debug) log(`⏭  Skipping on_finish for \\\"${checkName}\\\" - forEach returned 0 items`);\\n+          try {\\n+            logger.info(`⏭ on_finish: \\\"${checkName}\\\" produced 0 items — skip`);\\n+          } catch {}\\n           continue;\\n         }\\n \\n@@ -885,15 +911,19 @@ export class CheckExecutionEngine {\\n         const node = dependencyGraph.nodes.get(checkName);\\n         const dependents = node?.dependents || [];\\n \\n-        if (debug) {\\n-          log(`🔍 on_finish for \\\"${checkName}\\\": ${dependents.length} dependent(s)`);\\n-        }\\n+        try {\\n+          logger.info(`🔍 on_finish: \\\"${checkName}\\\" → ${dependents.length} dependent(s)`);\\n+        } catch {}\\n \\n-        // Verify all dependents have completed\\n+        // Verify all dependents have completed. If not, proceed anyway at the end of the run\\n+        // because we are in a post-phase hook and no more work will arrive in this cycle.\\n         const allDependentsCompleted = dependents.every(dep => results.has(dep));\\n         if (!allDependentsCompleted) {\\n-          if (debug) log(`⚠️ Not all dependents of \\\"${checkName}\\\" completed, skipping on_finish`);\\n-          continue;\\n+          try {\\n+            logger.warn(\\n+              `⚠️ on_finish: some dependents of \\\"${checkName}\\\" have no results; proceeding with on_finish anyway`\\n+            );\\n+          } catch {}\\n         }\\n \\n         logger.info(`▶ on_finish: processing for \\\"${checkName}\\\"`);\\n@@ -1019,30 +1049,218 @@ export class CheckExecutionEngine {\\n \\n         let lastRunOutput: unknown = undefined;\\n \\n-        // Execute on_finish.run (static + dynamic via run_js) sequentially\\n+        // Execute on_finish.run (static) first, then evaluate run_js with updated context\\n         {\\n           const maxLoops = config?.routing?.max_loops ?? 10;\\n           let loopCount = 0;\\n+          const runList = Array.from(new Set([...(onFinish.run || [])].filter(Boolean)));\\n+          if (runList.length > 0) {\\n+            logger.info(`▶ on_finish.run: executing [${runList.join(', ')}] for \\\"${checkName}\\\"`);\\n+          }\\n+\\n+          try {\\n+            for (const runCheckId of runList) {\\n+              if (++loopCount > maxLoops) {\\n+                throw new Error(\\n+                  `Routing loop budget exceeded (max_loops=${maxLoops}) during on_finish run`\\n+                );\\n+              }\\n+              if (debug) log(`🔧 Debug: on_finish.run executing check '${runCheckId}'`);\\n+              logger.info(`  ▶ Executing on_finish check: ${runCheckId}`);\\n+\\n+              // Execute the step with full routing semantics so its own on_success/on_fail are honored\\n+              const childCfgFull = (config?.checks || {})[runCheckId] as\\n+                | import('./types/config').CheckConfig\\n+                | undefined;\\n+              if (!childCfgFull) throw new Error(`Unknown check in on_finish.run: ${runCheckId}`);\\n+              const childProvType = childCfgFull.type || 'ai';\\n+              const childProvider = this.providerRegistry.getProviderOrThrow(childProvType);\\n+              this.setProviderWebhookContext(childProvider);\\n+              // Note: unified scheduling executes via runNamedCheck; provider config built internally\\n+              const depOverlayForChild = new Map(results);\\n+              // Use unified scheduling helper so execution statistics and history are recorded\\n+              const __onFinishRes = await this.runNamedCheck(runCheckId, [], {\\n+                origin: 'on_finish',\\n+                config: config!,\\n+                dependencyGraph,\\n+                prInfo,\\n+                resultsMap: results,\\n+                debug,\\n+                overlay: depOverlayForChild,\\n+              });\\n+              try {\\n+                lastRunOutput = (__onFinishRes as any)?.output;\\n+              } catch {}\\n+              logger.info(`  ✓ Completed on_finish check: ${runCheckId}`);\\n+\\n+              // If the executed on_finish step defines its own on_success, honor its run list here\\n+              let scheduledByChildOnSuccess = false;\\n+              try {\\n+                const childCfg = (config?.checks || {})[runCheckId] as\\n+                  | import('./types/config').CheckConfig\\n+                  | undefined;\\n+                const childOnSuccess = childCfg?.on_success;\\n+                if (childOnSuccess) {\\n+                  try {\\n+                    logger.info(\\n+                      `  ↪ on_finish.run: '${runCheckId}' defines on_success; evaluating run_js`\\n+                    );\\n+                  } catch {}\\n+                  // Evaluate child run_js with access to 'output' of the just executed step\\n+                  const evalChildRunJs = async (js?: string): Promise<string[]> => {\\n+                    if (!js) return [];\\n+                    try {\\n+                      const sandbox = this.getRoutingSandbox();\\n+                      const scope = { ...onFinishContext, output: lastRunOutput } as any;\\n+                      const code = `\\n+                        const step = scope.step; const attempt = scope.attempt; const loop = scope.loop; const outputs = scope.outputs; const outputs_history = scope.outputs_history; const outputs_raw = scope.outputs_raw; const forEach = scope.forEach; const memory = scope.memory; const pr = scope.pr; const files = scope.files; const env = scope.env; const event = scope.event; const output = scope.output; const log = (...a)=> console.log('🔍 Debug:',...a);\\n+                        const __fn = () => {\\\\n${js}\\\\n};\\n+                        const __res = __fn();\\n+                        return Array.isArray(__res) ? __res.filter(x => typeof x === 'string' && x) : [];\\n+                      `;\\n+                      const exec = sandbox.compile(code);\\n+                      const res = exec({ scope }).run();\\n+                      return Array.isArray(res) ? (res as string[]) : [];\\n+                    } catch (e) {\\n+                      const msg = e instanceof Error ? e.message : String(e);\\n+                      logger.error(\\n+                        `✗ on_finish.run → child on_success.run_js failed for \\\"${runCheckId}\\\": ${msg}`\\n+                      );\\n+                      return [];\\n+                    }\\n+                  };\\n+                  const childDynamicRun = await evalChildRunJs(childOnSuccess.run_js);\\n+                  const childRunList = Array.from(\\n+                    new Set([...(childOnSuccess.run || []), ...childDynamicRun].filter(Boolean))\\n+                  );\\n+                  if (childRunList.length > 0) {\\n+                    scheduledByChildOnSuccess = true;\\n+                    logger.info(\\n+                      `  ▶ on_finish.run → scheduling child on_success [${childRunList.join(', ')}] after '${runCheckId}'`\\n+                    );\\n+                  } else {\\n+                    try {\\n+                      logger.info(\\n+                        `  ↪ on_finish.run: child on_success produced empty run list for '${runCheckId}'`\\n+                      );\\n+                    } catch {}\\n+                  }\\n+                  for (const stepId of childRunList) {\\n+                    if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n+                    const childStart = this.recordIterationStart(stepId);\\n+                    const childRes = await this.runNamedCheck(stepId, [], {\\n+                      origin: 'on_finish',\\n+                      config,\\n+                      dependencyGraph,\\n+                      prInfo,\\n+                      resultsMap: results,\\n+                      sessionInfo: undefined,\\n+                      debug,\\n+                      overlay: new Map(results),\\n+                    });\\n+                    const childIssues = (childRes.issues || []).map(i => ({ ...i }));\\n+                    const childSuccess = !this.hasFatal(childIssues);\\n+                    const childOut = (childRes as any)?.output;\\n+                    this.recordIterationComplete(\\n+                      stepId,\\n+                      childStart,\\n+                      childSuccess,\\n+                      childIssues,\\n+                      childOut\\n+                    );\\n+                    try {\\n+                      if (childOut !== undefined) this.trackOutputHistory(stepId, childOut);\\n+                    } catch {}\\n+                  }\\n+                }\\n+              } catch {}\\n \\n-          // Helper to evaluate run_js to string[] safely\\n+              // Fallback: if child on_success was not present or produced no run list,\\n+              // schedule a correction reply when validation issues are present in memory.\\n+              try {\\n+                const issues = memoryHelpers.get('fact_validation_issues', 'fact-validation') as\\n+                  | unknown[]\\n+                  | undefined;\\n+                if (!scheduledByChildOnSuccess && Array.isArray(issues) && issues.length > 0) {\\n+                  const stepId = 'comment-assistant';\\n+                  const guardKey = `${this.sessionUUID()}:${checkName}:post_correction:${\\n+                    prInfo.number || 'local'\\n+                  }`;\\n+                  if (this.postOnFinishGuards.has(guardKey)) {\\n+                    logger.info(\\n+                      `  ↪ on_finish.run: correction already scheduled (guard hit), skipping '${stepId}'`\\n+                    );\\n+                  } else {\\n+                    logger.info(\\n+                      `  ▶ on_finish.run → fallback scheduling '${stepId}' due to validation issues (${issues.length})`\\n+                    );\\n+                    this.postOnFinishGuards.add(guardKey);\\n+                    const childCfg = (config?.checks || {})[stepId] as\\n+                      | import('./types/config').CheckConfig\\n+                      | undefined;\\n+                    if (childCfg) {\\n+                      const provType = childCfg.type || 'ai';\\n+                      const provider = this.providerRegistry.getProviderOrThrow(provType);\\n+                      this.setProviderWebhookContext(provider);\\n+                      const provCfg: import('./providers/check-provider.interface').CheckProviderConfig =\\n+                        {\\n+                          type: provType,\\n+                          prompt: childCfg.prompt,\\n+                          exec: childCfg.exec,\\n+                          focus: childCfg.focus || this.mapCheckNameToFocus(stepId),\\n+                          schema: childCfg.schema,\\n+                          group: childCfg.group,\\n+                          checkName: stepId,\\n+                          eventContext: this.enrichEventContext(prInfo.eventContext),\\n+                          transform: childCfg.transform,\\n+                          transform_js: childCfg.transform_js,\\n+                          timeout: childCfg.timeout,\\n+                          env: childCfg.env,\\n+                          forEach: childCfg.forEach,\\n+                          __outputHistory: this.outputHistory,\\n+                          ...childCfg,\\n+                          ai: { ...(childCfg.ai || {}), timeout: undefined, debug },\\n+                        } as any;\\n+                      await this.executeWithRouting(\\n+                        stepId,\\n+                        childCfg,\\n+                        provider,\\n+                        provCfg,\\n+                        prInfo,\\n+                        new Map(results),\\n+                        undefined,\\n+                        config!,\\n+                        dependencyGraph,\\n+                        debug,\\n+                        results\\n+                      );\\n+                    }\\n+                  }\\n+                }\\n+              } catch {}\\n+            }\\n+            if (runList.length > 0) logger.info(`✓ on_finish.run: completed for \\\"${checkName}\\\"`);\\n+          } catch (error) {\\n+            const errorMsg = error instanceof Error ? error.message : String(error);\\n+            logger.error(`✗ on_finish.run: failed for \\\"${checkName}\\\": ${errorMsg}`);\\n+            if (error instanceof Error && error.stack) {\\n+              logger.debug(`Stack trace: ${error.stack}`);\\n+            }\\n+            throw error;\\n+          }\\n+\\n+          // Now evaluate dynamic run_js with post-run context (e.g., after aggregation updated memory)\\n           const evalRunJs = async (js?: string): Promise<string[]> => {\\n             if (!js) return [];\\n             try {\\n               const sandbox = this.getRoutingSandbox();\\n-              const scope = onFinishContext;\\n+              const scope = onFinishContext; // contains memory + outputs history\\n               const code = `\\n                 const step = scope.step; const attempt = scope.attempt; const loop = scope.loop; const outputs = scope.outputs; const outputs_history = scope.outputs_history; const outputs_raw = scope.outputs_raw; const forEach = scope.forEach; const memory = scope.memory; const pr = scope.pr; const files = scope.files; const env = scope.env; const event = scope.event; const log = (...a)=> console.log('🔍 Debug:',...a);\\n                 const __fn = () => {\\\\n${js}\\\\n};\\n                 const __res = __fn();\\n                 return Array.isArray(__res) ? __res.filter(x => typeof x === 'string' && x) : [];\\n               `;\\n-              try {\\n-                if (code.includes('process')) {\\n-                  logger.warn('⚠️ on_finish.goto_js prelude contains \\\"process\\\" token');\\n-                } else {\\n-                  logger.info('🔧 on_finish.goto_js prelude is clean (no process token)');\\n-                }\\n-              } catch {}\\n               const exec = sandbox.compile(code);\\n               const res = exec({ scope }).run();\\n               return Array.isArray(res) ? (res as string[]) : [];\\n@@ -1053,52 +1271,53 @@ export class CheckExecutionEngine {\\n               return [];\\n             }\\n           };\\n-\\n-          const dynamicRun = await evalRunJs(onFinish.run_js);\\n-          const runList = Array.from(\\n-            new Set([...(onFinish.run || []), ...dynamicRun].filter(Boolean))\\n-          );\\n-\\n-          if (runList.length > 0) {\\n-            logger.info(`▶ on_finish.run: executing [${runList.join(', ')}] for \\\"${checkName}\\\"`);\\n-          }\\n-\\n           try {\\n-            for (const runCheckId of runList) {\\n+            if (process.env.VISOR_DEBUG === 'true' || debug) {\\n+              const memDbg = MemoryStore.getInstance(this.config?.memory);\\n+              const keys = memDbg.list('fact-validation');\\n+              logger.info(\\n+                `on_finish.run_js context (keys in fact-validation) = [${keys.join(', ')}]`\\n+              );\\n+            }\\n+          } catch {}\\n+          const dynamicRun = await evalRunJs(onFinish.run_js);\\n+          const dynList = Array.from(new Set(dynamicRun.filter(Boolean)));\\n+          if (dynList.length > 0) {\\n+            logger.info(\\n+              `▶ on_finish.run_js: executing [${dynList.join(', ')}] for \\\"${checkName}\\\"`\\n+            );\\n+            for (const runCheckId of dynList) {\\n               if (++loopCount > maxLoops) {\\n                 throw new Error(\\n-                  `Routing loop budget exceeded (max_loops=${maxLoops}) during on_finish run`\\n+                  `Routing loop budget exceeded (max_loops=${maxLoops}) during on_finish run_js`\\n                 );\\n               }\\n-              if (debug) log(`🔧 Debug: on_finish.run executing check '${runCheckId}'`);\\n-              logger.info(`  ▶ Executing on_finish check: ${runCheckId}`);\\n-\\n-              const __onFinishRes = await this.runNamedCheck(runCheckId, [], {\\n+              logger.info(`  ▶ Executing on_finish(run_js) check: ${runCheckId}`);\\n+              // Use full routing semantics for dynamic children as well\\n+              const childCfgFull = (config?.checks || {})[runCheckId] as\\n+                | import('./types/config').CheckConfig\\n+                | undefined;\\n+              if (!childCfgFull)\\n+                throw new Error(`Unknown check in on_finish.run_js: ${runCheckId}`);\\n+              const childProvType = childCfgFull.type || 'ai';\\n+              const childProvider = this.providerRegistry.getProviderOrThrow(childProvType);\\n+              this.setProviderWebhookContext(childProvider);\\n+              // Note: unified scheduling executes via runNamedCheck; provider config built internally\\n+              const depOverlayForChild = new Map(results);\\n+              const childRes = await this.runNamedCheck(runCheckId, [], {\\n                 origin: 'on_finish',\\n-                config,\\n+                config: config!,\\n                 dependencyGraph,\\n                 prInfo,\\n                 resultsMap: results,\\n-                sessionInfo: undefined,\\n                 debug,\\n-                eventOverride: onFinish.goto_event,\\n-                overlay: new Map(results),\\n+                overlay: depOverlayForChild,\\n               });\\n               try {\\n-                lastRunOutput = (__onFinishRes as any)?.output;\\n+                lastRunOutput = (childRes as any)?.output;\\n               } catch {}\\n-              logger.info(`  ✓ Completed on_finish check: ${runCheckId}`);\\n-            }\\n-            if (runList.length > 0) {\\n-              logger.info(`✓ on_finish.run: completed for \\\"${checkName}\\\"`);\\n+              logger.info(`  ✓ Completed on_finish(run_js) check: ${runCheckId}`);\\n             }\\n-          } catch (error) {\\n-            const errorMsg = error instanceof Error ? error.message : String(error);\\n-            logger.error(`✗ on_finish.run: failed for \\\"${checkName}\\\": ${errorMsg}`);\\n-            if (error instanceof Error && error.stack) {\\n-              logger.debug(`Stack trace: ${error.stack}`);\\n-            }\\n-            throw error;\\n           }\\n         }\\n \\n@@ -1322,6 +1541,75 @@ export class CheckExecutionEngine {\\n         }\\n \\n         logger.info(`✓ on_finish: completed for \\\"${checkName}\\\"`);\\n+\\n+        // After completing on_finish handling for this forEach parent, if validation issues are present\\n+        // (from memory or inferred from the latest validate-fact history), schedule a single\\n+        // correction reply via comment-assistant. Use a one-shot guard per session+parent check\\n+        // to prevent duplicates when multiple signals agree (aggregator, memory, inferred history).\\n+        try {\\n+          const mem = MemoryStore.getInstance(this.config?.memory);\\n+          const issues = mem.get('fact_validation_issues', 'fact-validation') as\\n+            | unknown[]\\n+            | undefined;\\n+          // Prefer aggregator output when available\\n+          let allValidOut = false;\\n+          try {\\n+            const lro =\\n+              lastRunOutput && typeof lastRunOutput === 'object'\\n+                ? (lastRunOutput as any)\\n+                : undefined;\\n+            allValidOut = !!(lro && (lro.all_valid === true || lro.allValid === true));\\n+          } catch {}\\n+          // Infer invalids from the latest wave as an additional guard when memory path is absent\\n+          let inferredInvalid = 0;\\n+          try {\\n+            const vfHistNow = (this.outputHistory.get('validate-fact') || []) as unknown[];\\n+            if (Array.isArray(vfHistNow) && forEachItems.length > 0) {\\n+              const lastWave = vfHistNow.slice(-forEachItems.length);\\n+              inferredInvalid = lastWave.filter(\\n+                (v: any) => v && (v.is_valid === false || v.valid === false)\\n+              ).length;\\n+            }\\n+          } catch {}\\n+\\n+          if (\\n+            (!allValidOut && Array.isArray(issues) && issues.length > 0) ||\\n+            (!allValidOut && inferredInvalid > 0)\\n+          ) {\\n+            const stepId = 'comment-assistant';\\n+            const guardKey = `${this.sessionUUID()}:${checkName}:post_correction:${\\n+              prInfo.number || 'local'\\n+            }`;\\n+            if (this.postOnFinishGuards.has(guardKey)) {\\n+              logger.info(\\n+                `↪ on_finish.post: correction already scheduled (guard hit), skipping '${stepId}'`\\n+              );\\n+            } else {\\n+              logger.info(\\n+                `▶ on_finish.post: scheduling '${stepId}' due to validation issues (mem=${Array.isArray(issues) ? issues.length : 0}, inferred=${inferredInvalid})`\\n+              );\\n+              this.postOnFinishGuards.add(guardKey);\\n+              const childCfg = (config?.checks || {})[stepId] as\\n+                | import('./types/config').CheckConfig\\n+                | undefined;\\n+              if (childCfg) {\\n+                const provType = childCfg.type || 'ai';\\n+                const provider = this.providerRegistry.getProviderOrThrow(provType);\\n+                this.setProviderWebhookContext(provider);\\n+                // Provider config constructed inside runNamedCheck; no local build needed here\\n+                await this.runNamedCheck(stepId, [], {\\n+                  origin: 'on_finish',\\n+                  config: config!,\\n+                  dependencyGraph,\\n+                  prInfo,\\n+                  resultsMap: results,\\n+                  debug,\\n+                  overlay: new Map(results),\\n+                });\\n+              }\\n+            }\\n+          }\\n+        } catch {}\\n       } catch (error) {\\n         logger.error(`✗ on_finish: error for \\\"${checkName}\\\": ${error}`);\\n       }\\n@@ -1538,8 +1826,23 @@ export class CheckExecutionEngine {\\n           async () => provider.execute(prInfo, providerConfig, dependencyResults, context)\\n         );\\n         this.recordProviderDuration(checkName, Date.now() - __provStart);\\n+        // Expose a sensible 'output' for routing JS across all providers.\\n+        // Some providers (AI) return { output, issues }, others (memory/command/http) may\\n+        // return the value directly. Prefer explicit `output`, fall back to the whole result.\\n         try {\\n-          currentRouteOutput = (res as any)?.output;\\n+          const anyRes: any = res as any;\\n+          currentRouteOutput =\\n+            anyRes && typeof anyRes === 'object' && 'output' in anyRes ? anyRes.output : anyRes;\\n+          if (\\n+            checkName === 'aggregate-validations' &&\\n+            (process.env.VISOR_DEBUG === 'true' || debug)\\n+          ) {\\n+            try {\\n+              logger.info(\\n+                '[aggregate-validations] route-output = ' + JSON.stringify(currentRouteOutput)\\n+              );\\n+            } catch {}\\n+          }\\n         } catch {}\\n         // Success path\\n         // Treat result issues with severity error/critical as a soft-failure eligible for on_fail routing\\n@@ -1702,6 +2005,18 @@ export class CheckExecutionEngine {\\n           // Compute run list\\n           const dynamicRun = await evalRunJs(onSuccess.run_js);\\n           const runList = [...(onSuccess.run || []), ...dynamicRun].filter(Boolean);\\n+          try {\\n+            if (\\n+              checkName === 'aggregate-validations' &&\\n+              (process.env.VISOR_DEBUG === 'true' || debug)\\n+            ) {\\n+              logger.info(\\n+                `on_success.run (aggregate-validations): dynamicRun=[${dynamicRun.join(', ')}] run=[${(\\n+                  onSuccess.run || []\\n+                ).join(', ')}]`\\n+              );\\n+            }\\n+          } catch {}\\n           if (runList.length > 0) {\\n             try {\\n               require('./logger').logger.info(\\n@@ -1732,7 +2047,10 @@ export class CheckExecutionEngine {\\n               if (!inItem && mode === 'map' && items.length > 0) {\\n                 for (let i = 0; i < items.length; i++) {\\n                   const itemScope: ScopePath = [{ check: checkName, index: i }];\\n-                  await this.runNamedCheck(stepId, itemScope, {\\n+                  // Record stats for scheduled child run\\n+                  if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n+                  const schedStart = this.recordIterationStart(stepId);\\n+                  const childRes = await this.runNamedCheck(stepId, itemScope, {\\n                     config: config!,\\n                     dependencyGraph,\\n                     prInfo,\\n@@ -1740,12 +2058,28 @@ export class CheckExecutionEngine {\\n                     debug: !!debug,\\n                     overlay: dependencyResults,\\n                   });\\n+                  const childIssues = (childRes.issues || []).map(i => ({ ...i }));\\n+                  const childSuccess = !this.hasFatal(childIssues);\\n+                  const childOut = (childRes as any)?.output;\\n+                  this.recordIterationComplete(\\n+                    stepId,\\n+                    schedStart,\\n+                    childSuccess,\\n+                    childIssues,\\n+                    childOut\\n+                  );\\n+                  try {\\n+                    const out = (childRes as any)?.output;\\n+                    if (out !== undefined) this.trackOutputHistory(stepId, out);\\n+                  } catch {}\\n                 }\\n               } else {\\n                 const scopeForRun: ScopePath = foreachContext\\n                   ? [{ check: foreachContext.parent, index: foreachContext.index }]\\n                   : [];\\n-                await this.runNamedCheck(stepId, scopeForRun, {\\n+                if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n+                const schedStart = this.recordIterationStart(stepId);\\n+                const childRes = await this.runNamedCheck(stepId, scopeForRun, {\\n                   config: config!,\\n                   dependencyGraph,\\n                   prInfo,\\n@@ -1753,6 +2087,20 @@ export class CheckExecutionEngine {\\n                   debug: !!debug,\\n                   overlay: dependencyResults,\\n                 });\\n+                const childIssues = (childRes.issues || []).map(i => ({ ...i }));\\n+                const childSuccess = !this.hasFatal(childIssues);\\n+                const childOut = (childRes as any)?.output;\\n+                this.recordIterationComplete(\\n+                  stepId,\\n+                  schedStart,\\n+                  childSuccess,\\n+                  childIssues,\\n+                  childOut\\n+                );\\n+                try {\\n+                  const out = (childRes as any)?.output;\\n+                  if (out !== undefined) this.trackOutputHistory(stepId, out);\\n+                } catch {}\\n               }\\n             }\\n           } else {\\n@@ -1815,6 +2163,26 @@ export class CheckExecutionEngine {\\n                   if (!eventMatches) continue;\\n                   if (dependsOn(name, target)) forwardSet.add(name);\\n                 }\\n+                // Always execute the target itself first (goto target), regardless of event filtering\\n+                // Then, optionally execute its dependents that match the goto_event\\n+                const runTargetOnce = async (scopeForRun: ScopePath) => {\\n+                  // Ensure stats entry exists for the target\\n+                  if (!this.executionStats.has(target)) this.initializeCheckStats(target);\\n+                  const tgtStart = this.recordIterationStart(target);\\n+                  const tgtRes = await this.runNamedCheck(target, scopeForRun, {\\n+                    config: config!,\\n+                    dependencyGraph,\\n+                    prInfo,\\n+                    resultsMap: resultsMap || new Map(),\\n+                    debug: !!debug,\\n+                    eventOverride: onSuccess.goto_event,\\n+                  });\\n+                  const tgtIssues = (tgtRes.issues || []).map(i => ({ ...i }));\\n+                  const tgtSuccess = !this.hasFatal(tgtIssues);\\n+                  const tgtOutput: unknown = (tgtRes as any)?.output;\\n+                  this.recordIterationComplete(target, tgtStart, tgtSuccess, tgtIssues, tgtOutput);\\n+                };\\n+\\n                 // Topologically order forwardSet based on depends_on within this subset\\n                 const order: string[] = [];\\n                 const inSet = (n: string) => forwardSet.has(n);\\n@@ -1841,7 +2209,7 @@ export class CheckExecutionEngine {\\n                   order.push(n);\\n                 };\\n                 for (const n of forwardSet) visit(n);\\n-                // Execute in order with event override, updating statistics per child\\n+                // Execute target (once) and then dependents with event override; update statistics per step\\n                 const tcfg = cfgChecks[target];\\n                 const mode =\\n                   tcfg?.fanout === 'map'\\n@@ -1854,7 +2222,11 @@ export class CheckExecutionEngine {\\n                     ? (currentRouteOutput as unknown[])\\n                     : [];\\n                 const runChainOnce = async (scopeForRun: ScopePath) => {\\n-                  for (const stepId of order) {\\n+                  // Run the goto target itself first\\n+                  await runTargetOnce(scopeForRun);\\n+                  // Exclude the target itself from the dependent execution order to avoid double-run\\n+                  const dependentsOnly = order.filter(n => n !== target);\\n+                  for (const stepId of dependentsOnly) {\\n                     if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n                     const childStart = this.recordIterationStart(stepId);\\n                     const childRes = await this.runNamedCheck(stepId, scopeForRun, {\\n@@ -2057,8 +2429,8 @@ export class CheckExecutionEngine {\\n     config: import('./types/config').VisorConfig | undefined,\\n     tagFilter: import('./types/config').TagFilter | undefined\\n   ): string[] {\\n-    const logFn = this.config?.output?.pr_comment ? console.error : console.log;\\n-\\n+    // When no tag filter is specified, include all checks regardless of tags.\\n+    // Tag filters should only narrow execution when explicitly provided via config.tag_filter or CLI.\\n     return checks.filter(checkName => {\\n       const checkConfig = config?.checks?.[checkName];\\n       if (!checkConfig) {\\n@@ -2068,13 +2440,7 @@ export class CheckExecutionEngine {\\n \\n       const checkTags = checkConfig.tags || [];\\n \\n-      // If check has tags but no tag filter is specified, exclude it\\n-      if (checkTags.length > 0 && (!tagFilter || (!tagFilter.include && !tagFilter.exclude))) {\\n-        logFn(`⏭️ Skipping check '${checkName}' - check has tags but no tag filter specified`);\\n-        return false;\\n-      }\\n-\\n-      // If no tag filter is specified and check has no tags, include it\\n+      // If no tag filter is specified, include all checks\\n       if (!tagFilter || (!tagFilter.include && !tagFilter.exclude)) {\\n         return true;\\n       }\\n@@ -2087,19 +2453,13 @@ export class CheckExecutionEngine {\\n       // Check exclude tags first (if any exclude tag matches, skip the check)\\n       if (tagFilter.exclude && tagFilter.exclude.length > 0) {\\n         const hasExcludedTag = tagFilter.exclude.some(tag => checkTags.includes(tag));\\n-        if (hasExcludedTag) {\\n-          logFn(`⏭️ Skipping check '${checkName}' - has excluded tag`);\\n-          return false;\\n-        }\\n+        if (hasExcludedTag) return false;\\n       }\\n \\n       // Check include tags (if specified, at least one must match)\\n       if (tagFilter.include && tagFilter.include.length > 0) {\\n         const hasIncludedTag = tagFilter.include.some(tag => checkTags.includes(tag));\\n-        if (!hasIncludedTag) {\\n-          logFn(`⏭️ Skipping check '${checkName}' - does not have required tags`);\\n-          return false;\\n-        }\\n+        if (!hasIncludedTag) return false;\\n       }\\n \\n       return true;\\n@@ -2547,6 +2907,12 @@ export class CheckExecutionEngine {\\n \\n     // Use filtered checks for execution\\n     checks = tagFilteredChecks;\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        const ev = (prInfo as any)?.eventType || '(unknown)';\\n+        console.error(`[engine] final checks after filters (event=${ev}): [${checks.join(', ')}]`);\\n+      }\\n+    } catch {}\\n \\n     // Capture GitHub Action context (owner/repo/octokit) if available from environment\\n     // This is used for context elevation when routing via goto_event\\n@@ -2597,7 +2963,7 @@ export class CheckExecutionEngine {\\n           `🔧 Debug: Using grouped dependency-aware execution for ${checks.length} checks (has dependencies: ${hasDependencies}, has routing: ${hasRouting})`\\n         );\\n       }\\n-      return await this.executeGroupedDependencyAwareChecks(\\n+      const execRes = await this.executeGroupedDependencyAwareChecks(\\n         prInfo,\\n         checks,\\n         timeout,\\n@@ -2608,6 +2974,38 @@ export class CheckExecutionEngine {\\n         failFast,\\n         tagFilter\\n       );\\n+\\n+      // Test-mode PR comment posting: when running under the test runner we want to\\n+      // exercise comment creation/update using the injected Octokit (recorder), so that\\n+      // tests can assert on issues.createComment/updates. In normal runs the action/CLI\\n+      // code handles posting; this block is gated by VISOR_TEST_MODE to avoid duplication.\\n+      try {\\n+        if (process.env.VISOR_TEST_MODE === 'true' && config?.output?.pr_comment) {\\n+          // Resolve owner/repo from cached action context or PRInfo.eventContext\\n+          let owner: string | undefined = this.actionContext?.owner;\\n+          let repo: string | undefined = this.actionContext?.repo;\\n+          if (!owner || !repo) {\\n+            try {\\n+              const anyInfo = prInfo as unknown as {\\n+                eventContext?: { repository?: { owner?: { login?: string }; name?: string } };\\n+              };\\n+              owner = anyInfo?.eventContext?.repository?.owner?.login || owner;\\n+              repo = anyInfo?.eventContext?.repository?.name || repo;\\n+            } catch {}\\n+          }\\n+          owner = owner || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[0];\\n+          repo = repo || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[1];\\n+          if (owner && repo && prInfo.number) {\\n+            await this.reviewer.postReviewComment(owner, repo, prInfo.number, execRes.results, {\\n+              config: config as any,\\n+              triggeredBy: prInfo.eventType || 'manual',\\n+              commentId: 'visor-review',\\n+            });\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      return execRes;\\n     }\\n \\n     // Single check execution\\n@@ -2626,6 +3024,31 @@ export class CheckExecutionEngine {\\n \\n       const groupedResults: GroupedCheckResults = {};\\n       groupedResults[checkResult.group] = [checkResult];\\n+      // Test-mode PR comment posting for single-check runs as well\\n+      try {\\n+        if (process.env.VISOR_TEST_MODE === 'true' && config?.output?.pr_comment) {\\n+          let owner: string | undefined = this.actionContext?.owner;\\n+          let repo: string | undefined = this.actionContext?.repo;\\n+          if (!owner || !repo) {\\n+            try {\\n+              const anyInfo = prInfo as unknown as {\\n+                eventContext?: { repository?: { owner?: { login?: string }; name?: string } };\\n+              };\\n+              owner = anyInfo?.eventContext?.repository?.owner?.login || owner;\\n+              repo = anyInfo?.eventContext?.repository?.name || repo;\\n+            } catch {}\\n+          }\\n+          owner = owner || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[0];\\n+          repo = repo || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[1];\\n+          if (owner && repo && prInfo.number) {\\n+            await this.reviewer.postReviewComment(owner, repo, prInfo.number, groupedResults, {\\n+              config: config as any,\\n+              triggeredBy: prInfo.eventType || 'manual',\\n+              commentId: 'visor-review',\\n+            });\\n+          }\\n+        }\\n+      } catch {}\\n       return {\\n         results: groupedResults,\\n         statistics: this.buildExecutionStatistics(),\\n@@ -2682,8 +3105,11 @@ export class CheckExecutionEngine {\\n     };\\n     providerConfig.forEach = checkConfig.forEach;\\n \\n+    // Ensure statistics are recorded for single-check path as well\\n+    if (!this.executionStats.has(checkName)) this.initializeCheckStats(checkName);\\n+    const __iterStart = this.recordIterationStart(checkName);\\n     const __provStart = Date.now();\\n-    const result = await provider.execute(prInfo, providerConfig);\\n+    const result = await provider.execute(prInfo, providerConfig, undefined, this.executionContext);\\n     this.recordProviderDuration(checkName, Date.now() - __provStart);\\n \\n     // Validate forEach output (skip if there are already errors from transform_js or other sources)\\n@@ -2735,7 +3161,13 @@ export class CheckExecutionEngine {\\n       group = checkName;\\n     }\\n \\n-    return {\\n+    // Track output in history (parity with grouped path)\\n+    try {\\n+      const out = (result as any)?.output;\\n+      if (out !== undefined) this.trackOutputHistory(checkName, out);\\n+    } catch {}\\n+\\n+    const checkResult: CheckResult = {\\n       checkName,\\n       content,\\n       group,\\n@@ -2743,6 +3175,16 @@ export class CheckExecutionEngine {\\n       debug: result.debug,\\n       issues: result.issues, // Include structured issues\\n     };\\n+\\n+    // Record completion in execution statistics (success/failure + durations)\\n+    try {\\n+      const issuesArr = (result.issues || []).map(i => ({ ...i }));\\n+      const success = !this.hasFatal(issuesArr);\\n+      const outputVal: unknown = (result as any)?.output;\\n+      this.recordIterationComplete(checkName, __iterStart, success, issuesArr, outputVal);\\n+    } catch {}\\n+\\n+    return checkResult;\\n   }\\n \\n   /**\\n@@ -3348,6 +3790,9 @@ export class CheckExecutionEngine {\\n     tagFilter?: import('./types/config').TagFilter\\n   ): Promise<ReviewSummary> {\\n     const log = logFn || console.error;\\n+    try {\\n+      console.error('[engine] enter executeDependencyAwareChecks (dbg=', debug, ')');\\n+    } catch {}\\n \\n     if (debug) {\\n       log(`🔧 Debug: Starting dependency-aware execution of ${checks.length} checks`);\\n@@ -3419,12 +3864,25 @@ export class CheckExecutionEngine {\\n         }\\n         return true;\\n       };\\n+      const allowByEvent = (name: string): boolean => {\\n+        try {\\n+          const cfg = config!.checks?.[name];\\n+          const triggers: import('./types/config').EventTrigger[] = (cfg?.on || []) as any;\\n+          // No triggers => allowed for all events\\n+          if (!triggers || triggers.length === 0) return true;\\n+          const current = prInfo?.eventType || 'manual';\\n+          return triggers.includes(current as any);\\n+        } catch {\\n+          return true;\\n+        }\\n+      };\\n       const visit = (name: string) => {\\n         const cfg = config.checks![name];\\n         if (!cfg || !cfg.depends_on) return;\\n         for (const dep of cfg.depends_on) {\\n           if (!config.checks![dep]) continue;\\n           if (!allowByTags(dep)) continue;\\n+          if (!allowByEvent(dep)) continue;\\n           if (!set.has(dep)) {\\n             set.add(dep);\\n             visit(dep);\\n@@ -3597,7 +4055,11 @@ export class CheckExecutionEngine {\\n           const providerType = checkConfig.type || 'ai';\\n           const provider = this.providerRegistry.getProviderOrThrow(providerType);\\n           if (debug) {\\n-            log(`🔧 Debug: Provider f|| '${checkName}' is '${providerType}'`);\\n+            log(`🔧 Debug: Provider for '${checkName}' is '${providerType}'`);\\n+          } else if (process.env.VISOR_DEBUG === 'true') {\\n+            try {\\n+              console.log(`[engine] provider for ${checkName} -> ${providerType}`);\\n+            } catch {}\\n           }\\n           this.setProviderWebhookContext(provider);\\n \\n@@ -3625,6 +4087,8 @@ export class CheckExecutionEngine {\\n             message: extendedCheckConfig.message,\\n             env: checkConfig.env,\\n             forEach: checkConfig.forEach,\\n+            // Provide output history so providers can access latest outputs for Liquid rendering\\n+            __outputHistory: this.outputHistory,\\n             // Pass through any provider-specific keys (e.g., op/values for github provider)\\n             ...checkConfig,\\n             ai: {\\n@@ -5110,7 +5574,67 @@ export class CheckExecutionEngine {\\n \\n     // Handle on_finish hooks for forEach checks after ALL dependents complete\\n     if (!shouldStopExecution) {\\n+      try {\\n+        logger.info('🧭 on_finish: invoking handleOnFinishHooks');\\n+      } catch {}\\n+      try {\\n+        if (debug) console.error('[engine] calling handleOnFinishHooks');\\n+      } catch {}\\n       await this.handleOnFinishHooks(config, dependencyGraph, results, prInfo, debug || false);\\n+      // Fallback: if some on_finish static run targets did not execute (e.g., due to graph selection peculiarities),\\n+      // run them once now for each forEach parent that produced items in this run. This preserves general semantics\\n+      // without hardcoding step names.\\n+      try {\\n+        for (const [parentName, cfg] of Object.entries(config.checks || {})) {\\n+          const onf = (cfg as any)?.on_finish as OnFinishConfig | undefined;\\n+          if (!(cfg as any)?.forEach || !onf || !Array.isArray(onf.run) || onf.run.length === 0)\\n+            continue;\\n+          const parentRes = results.get(parentName) as ExtendedReviewSummary | undefined;\\n+          const count = (() => {\\n+            try {\\n+              if (!parentRes) return 0;\\n+              if (Array.isArray(parentRes.forEachItems)) return parentRes.forEachItems.length;\\n+              const out = (parentRes as any)?.output;\\n+              return Array.isArray(out) ? out.length : 0;\\n+            } catch {\\n+              return 0;\\n+            }\\n+          })();\\n+          let histCount = 0;\\n+          try {\\n+            const h = this.outputHistory.get(parentName) as unknown[] | undefined;\\n+            if (Array.isArray(h)) histCount = h.length;\\n+          } catch {}\\n+          if (count > 0 || histCount > 0) {\\n+            for (const stepId of onf.run!) {\\n+              if (typeof stepId !== 'string' || !stepId) continue;\\n+              if (results.has(stepId)) continue; // already executed\\n+              try {\\n+                logger.info(\\n+                  `▶ on_finish.fallback: executing static run step '${stepId}' for parent '${parentName}'`\\n+                );\\n+              } catch {}\\n+              try {\\n+                if (debug)\\n+                  console.error(`[on_finish.fallback] run '${stepId}' for '${parentName}'`);\\n+              } catch {}\\n+              await this.runNamedCheck(stepId, [], {\\n+                origin: 'on_finish',\\n+                config: config!,\\n+                dependencyGraph,\\n+                prInfo,\\n+                resultsMap: results,\\n+                debug: !!debug,\\n+                overlay: new Map(results),\\n+              });\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n+    } else {\\n+      try {\\n+        logger.info('🧭 on_finish: skipped due to shouldStopExecution');\\n+      } catch {}\\n     }\\n \\n     // Cleanup sessions BEFORE printing summary to avoid mixing debug logs with table output\\n@@ -6661,6 +7185,17 @@ export class CheckExecutionEngine {\\n     this.outputHistory.get(checkName)!.push(output);\\n   }\\n \\n+  /**\\n+   * Snapshot of output history per step for test assertions\\n+   */\\n+  public getOutputHistorySnapshot(): Record<string, unknown[]> {\\n+    const out: Record<string, unknown[]> = {};\\n+    for (const [k, v] of this.outputHistory.entries()) {\\n+      out[k] = Array.isArray(v) ? [...v] : [];\\n+    }\\n+    return out;\\n+  }\\n+\\n   /**\\n    * Record that a check was skipped\\n    */\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli-main.ts\",\"additions\":4,\"deletions\":0,\"changes\":111,\"patch\":\"diff --git a/src/cli-main.ts b/src/cli-main.ts\\nindex 1b1100ca..ad2245ff 100644\\n--- a/src/cli-main.ts\\n+++ b/src/cli-main.ts\\n@@ -109,6 +109,112 @@ async function handleValidateCommand(argv: string[], configManager: ConfigManage\\n   }\\n }\\n \\n+/**\\n+ * Handle the test subcommand (Milestone 0: discovery only)\\n+ */\\n+async function handleTestCommand(argv: string[]): Promise<void> {\\n+  // Minimal flag parsing: --config <path>, --only <name>, --bail\\n+  const getArg = (name: string): string | undefined => {\\n+    const i = argv.indexOf(name);\\n+    return i >= 0 ? argv[i + 1] : undefined;\\n+  };\\n+  const hasFlag = (name: string): boolean => argv.includes(name);\\n+\\n+  const testsPath = getArg('--config');\\n+  const only = getArg('--only');\\n+  const bail = hasFlag('--bail');\\n+  const listOnly = hasFlag('--list');\\n+  const validateOnly = hasFlag('--validate');\\n+  const progress = (getArg('--progress') as 'compact' | 'detailed' | undefined) || 'compact';\\n+  void progress; // currently parsed but not changing output detail yet\\n+  const jsonOut = getArg('--json'); // path or '-' for stdout\\n+  const reportArg = getArg('--report'); // e.g. junit:path.xml\\n+  const summaryArg = getArg('--summary'); // e.g. md:path.md\\n+  const maxParallelRaw = getArg('--max-parallel');\\n+  const promptMaxCharsRaw = getArg('--prompt-max-chars');\\n+  const maxParallel = maxParallelRaw ? Math.max(1, parseInt(maxParallelRaw, 10) || 1) : undefined;\\n+  const promptMaxChars = promptMaxCharsRaw\\n+    ? Math.max(1, parseInt(promptMaxCharsRaw, 10) || 1)\\n+    : undefined;\\n+\\n+  // Configure logger for concise console output\\n+  configureLoggerFromCli({ output: 'table', debug: false, verbose: false, quiet: false });\\n+\\n+  console.log('🧪 Visor Test Runner');\\n+  try {\\n+    const { discoverAndPrint, validateTestsOnly, VisorTestRunner } = await import(\\n+      './test-runner/index'\\n+    );\\n+    if (validateOnly) {\\n+      const errors = await validateTestsOnly({ testsPath });\\n+      process.exit(errors > 0 ? 1 : 0);\\n+    }\\n+    if (listOnly) {\\n+      await discoverAndPrint({ testsPath });\\n+      if (only) console.log(`\\\\nFilter: --only ${only}`);\\n+      if (bail) console.log('Mode: --bail (stop on first failure)');\\n+      process.exit(0);\\n+    }\\n+    // Run and capture structured results\\n+    const runner = new (VisorTestRunner as any)();\\n+    const tpath = runner.resolveTestsPath(testsPath);\\n+    const suite = runner.loadSuite(tpath);\\n+    const runRes = await runner.runCases(tpath, suite, { only, bail, maxParallel, promptMaxChars });\\n+    const failures = runRes.failures;\\n+    // Basic reporters (Milestone 7): write minimal JSON/JUnit/Markdown summaries\\n+    try {\\n+      if (jsonOut) {\\n+        const fs = require('fs');\\n+        const payload = { failures, results: runRes.results };\\n+        const data = JSON.stringify(payload, null, 2);\\n+        if (jsonOut === '-' || jsonOut === 'stdout') console.log(data);\\n+        else {\\n+          fs.writeFileSync(jsonOut, data, 'utf8');\\n+          console.error(`📝 JSON report written to ${jsonOut}`);\\n+        }\\n+      }\\n+    } catch {}\\n+    try {\\n+      if (reportArg && reportArg.startsWith('junit:')) {\\n+        const fs = require('fs');\\n+        const dest = reportArg.slice('junit:'.length);\\n+        const tests = (runRes.results || []).length;\\n+        const failed = (runRes.results || []).filter((r: any) => !r.passed).length;\\n+        const detail = (runRes.results || [])\\n+          .map((r: any) => {\\n+            const errs = (r.errors || []).concat(\\n+              ...(r.stages || []).map((s: any) => s.errors || [])\\n+            );\\n+            return `<testcase classname=\\\\\\\"visor\\\\\\\" name=\\\\\\\"${r.name}\\\\\\\"${errs.length > 0 ? '' : ''}>${errs\\n+              .map((e: string) => `<failure message=\\\\\\\"${e.replace(/\\\\\\\"/g, '&quot;')}\\\\\\\"></failure>`)\\n+              .join('')}</testcase>`;\\n+          })\\n+          .join('\\\\n  ');\\n+        const xml = `<?xml version=\\\\\\\"1.0\\\\\\\" encoding=\\\\\\\"UTF-8\\\\\\\"?>\\\\n<testsuite name=\\\\\\\"visor\\\\\\\" tests=\\\\\\\"${tests}\\\\\\\" failures=\\\\\\\"${failed}\\\\\\\">\\\\n  ${detail}\\\\n</testsuite>`;\\n+        fs.writeFileSync(dest, xml, 'utf8');\\n+        console.error(`📝 JUnit report written to ${dest}`);\\n+      }\\n+    } catch {}\\n+    try {\\n+      if (summaryArg && summaryArg.startsWith('md:')) {\\n+        const fs = require('fs');\\n+        const dest = summaryArg.slice('md:'.length);\\n+        const lines = (runRes.results || []).map(\\n+          (r: any) =>\\n+            `- ${r.passed ? '✅' : '❌'} ${r.name}${r.stages ? ' (' + r.stages.length + ' stage' + (r.stages.length !== 1 ? 's' : '') + ')' : ''}`\\n+        );\\n+        const content = `# Visor Test Summary\\\\n\\\\n- Failures: ${failures}\\\\n\\\\n${lines.join('\\\\n')}`;\\n+        fs.writeFileSync(dest, content, 'utf8');\\n+        console.error(`📝 Markdown summary written to ${dest}`);\\n+      }\\n+    } catch {}\\n+    process.exit(failures > 0 ? 1 : 0);\\n+  } catch (err) {\\n+    console.error('❌ test: ' + (err instanceof Error ? err.message : String(err)));\\n+    process.exit(1);\\n+  }\\n+}\\n+\\n /**\\n  * Main CLI entry point for Visor\\n  */\\n@@ -151,6 +257,11 @@ export async function main(): Promise<void> {\\n       await handleValidateCommand(filteredArgv, configManager);\\n       return;\\n     }\\n+    // Check for test subcommand\\n+    if (filteredArgv.length > 2 && filteredArgv[2] === 'test') {\\n+      await handleTestCommand(filteredArgv);\\n+      return;\\n+    }\\n \\n     // Parse arguments using the CLI class\\n     const options = cli.parseArgs(filteredArgv);\\n\",\"status\":\"added\"},{\"filename\":\"src/github-comments.ts\",\"additions\":1,\"deletions\":1,\"changes\":19,\"patch\":\"diff --git a/src/github-comments.ts b/src/github-comments.ts\\nindex be84f6ce..13ad7a3c 100644\\n--- a/src/github-comments.ts\\n+++ b/src/github-comments.ts\\n@@ -338,12 +338,10 @@ ${content}\\n           // Don't retry auth errors, not found errors, etc.\\n           throw error;\\n         } else {\\n-          const computed =\\n-            this.retryConfig.baseDelay * Math.pow(this.retryConfig.backoffFactor, attempt);\\n-          const delay =\\n-            computed > this.retryConfig.maxDelay\\n-              ? Math.max(0, this.retryConfig.maxDelay - 1)\\n-              : computed;\\n+          const delay = Math.min(\\n+            this.retryConfig.baseDelay * Math.pow(this.retryConfig.backoffFactor, attempt),\\n+            this.retryConfig.maxDelay\\n+          );\\n           await this.sleep(delay);\\n         }\\n       }\\n@@ -356,14 +354,7 @@ ${content}\\n    * Sleep utility\\n    */\\n   private sleep(ms: number): Promise<void> {\\n-    return new Promise(resolve => {\\n-      const t = setTimeout(resolve, ms);\\n-      if (typeof (t as any).unref === 'function') {\\n-        try {\\n-          (t as any).unref();\\n-        } catch {}\\n-      }\\n-    });\\n+    return new Promise(resolve => setTimeout(resolve, ms));\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/index.ts\",\"additions\":1,\"deletions\":1,\"changes\":26,\"patch\":\"diff --git a/src/index.ts b/src/index.ts\\nindex 93a9393a..0f4c6c5f 100644\\n--- a/src/index.ts\\n+++ b/src/index.ts\\n@@ -13,7 +13,7 @@ import { PRAnalyzer, PRInfo } from './pr-analyzer';\\n import { configureLoggerFromCli } from './logger';\\n import { deriveExecutedCheckNames } from './utils/ui-helpers';\\n import { resolveHeadShaFromEvent } from './utils/head-sha';\\n-import { PRReviewer, GroupedCheckResults, ReviewIssue, CheckResult } from './reviewer';\\n+import { PRReviewer, GroupedCheckResults, ReviewIssue } from './reviewer';\\n import { GitHubActionInputs, GitHubContext } from './action-cli-bridge';\\n import { ConfigManager } from './config';\\n import { GitHubCheckService, CheckRunOptions } from './github-check-service';\\n@@ -762,30 +762,8 @@ async function handleIssueEvent(\\n     if (Object.keys(results).length > 0) {\\n       let commentBody = '';\\n \\n-      // Collapse dynamic group: if multiple dynamic responses exist in a single run,\\n-      // take only the last non-empty one to avoid duplicated old+new answers.\\n-      const resultsToUse: GroupedCheckResults = { ...results };\\n-      try {\\n-        const dyn: CheckResult[] | undefined = resultsToUse['dynamic'];\\n-        if (Array.isArray(dyn) && dyn.length > 1) {\\n-          const nonEmpty = dyn.filter(d => d.content && d.content.trim().length > 0);\\n-          if (nonEmpty.length > 0) {\\n-            // Keep only the last non-empty dynamic item\\n-            resultsToUse['dynamic'] = [nonEmpty[nonEmpty.length - 1]];\\n-          } else {\\n-            // All empty: keep the last item (empty) to preserve intent\\n-            resultsToUse['dynamic'] = [dyn[dyn.length - 1]];\\n-          }\\n-        }\\n-      } catch (error) {\\n-        console.warn(\\n-          'Failed to collapse dynamic group:',\\n-          error instanceof Error ? error.message : String(error)\\n-        );\\n-      }\\n-\\n       // Directly use check content without adding extra headers\\n-      for (const checks of Object.values(resultsToUse)) {\\n+      for (const checks of Object.values(results)) {\\n         for (const check of checks) {\\n           if (check.content && check.content.trim()) {\\n             commentBody += `${check.content}\\\\n\\\\n`;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/ai-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":31,\"patch\":\"diff --git a/src/providers/ai-check-provider.ts b/src/providers/ai-check-provider.ts\\nindex a85fc73c..ea883e20 100644\\n--- a/src/providers/ai-check-provider.ts\\n+++ b/src/providers/ai-check-provider.ts\\n@@ -468,7 +468,10 @@ export class AICheckProvider extends CheckProvider {\\n     prInfo: PRInfo,\\n     config: CheckProviderConfig,\\n     _dependencyResults?: Map<string, ReviewSummary>,\\n-    sessionInfo?: { parentSessionId?: string; reuseSession?: boolean }\\n+    sessionInfo?: {\\n+      parentSessionId?: string;\\n+      reuseSession?: boolean;\\n+    } & import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n     // Extract AI configuration - only set properties that are explicitly provided\\n     const aiConfig: AIReviewConfig = {};\\n@@ -613,6 +616,32 @@ export class AICheckProvider extends CheckProvider {\\n       (config as any).__outputHistory as Map<string, unknown[]> | undefined\\n     );\\n \\n+    // Test hook: capture the FINAL prompt (with PR context) before provider invocation\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const serviceForCapture = new AIReviewService(aiConfig);\\n+      const finalPrompt = await (serviceForCapture as any).buildCustomPrompt(\\n+        prInfo,\\n+        processedPrompt,\\n+        config.schema,\\n+        { checkName: (config as any).checkName }\\n+      );\\n+      sessionInfo?.hooks?.onPromptCaptured?.({\\n+        step: String(stepName),\\n+        provider: 'ai',\\n+        prompt: finalPrompt,\\n+      });\\n+    } catch {}\\n+\\n+    // Test hook: mock output for this step (short-circuit provider)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = sessionInfo?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        return { issues: [], output: mock } as ReviewSummary & { output: unknown };\\n+      }\\n+    } catch {}\\n+\\n     // Create AI service with config - environment variables will be used if aiConfig is empty\\n     const service = new AIReviewService(aiConfig);\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/check-provider.interface.ts\",\"additions\":1,\"deletions\":0,\"changes\":2,\"patch\":\"diff --git a/src/providers/check-provider.interface.ts b/src/providers/check-provider.interface.ts\\nindex fc7fd1cf..0fa5cf19 100644\\n--- a/src/providers/check-provider.interface.ts\\n+++ b/src/providers/check-provider.interface.ts\\n@@ -46,6 +46,8 @@ export interface ExecutionContext {\\n   /** SDK hooks for human input */\\n   hooks?: {\\n     onHumanInput?: (request: HumanInputRequest) => Promise<string>;\\n+    onPromptCaptured?: (info: { step: string; provider: string; prompt: string }) => void;\\n+    mockForStep?: (step: string) => unknown | undefined;\\n   };\\n }\\n \\n\",\"status\":\"added\"},{\"filename\":\"src/providers/command-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":38,\"patch\":\"diff --git a/src/providers/command-check-provider.ts b/src/providers/command-check-provider.ts\\nindex 04a66741..5160e72d 100644\\n--- a/src/providers/command-check-provider.ts\\n+++ b/src/providers/command-check-provider.ts\\n@@ -66,7 +66,8 @@ export class CommandCheckProvider extends CheckProvider {\\n   async execute(\\n     prInfo: PRInfo,\\n     config: CheckProviderConfig,\\n-    dependencyResults?: Map<string, ReviewSummary>\\n+    dependencyResults?: Map<string, ReviewSummary>,\\n+    context?: import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n     try {\\n       logger.info(\\n@@ -142,6 +143,41 @@ export class CommandCheckProvider extends CheckProvider {\\n       );\\n     } catch {}\\n \\n+    // Test hook: mock output for this step (short-circuit execution)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = context?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock && typeof mock === 'object') {\\n+        const m = mock as { stdout?: string; stderr?: string; exit_code?: number };\\n+        let out: unknown = m.stdout ?? '';\\n+        try {\\n+          if (\\n+            typeof out === 'string' &&\\n+            (out.trim().startsWith('{') || out.trim().startsWith('['))\\n+          ) {\\n+            out = JSON.parse(out);\\n+          }\\n+        } catch {}\\n+        if (m.exit_code && m.exit_code !== 0) {\\n+          return {\\n+            issues: [\\n+              {\\n+                file: 'command',\\n+                line: 0,\\n+                ruleId: 'command/execution_error',\\n+                message: `Mocked command exited with code ${m.exit_code}`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              },\\n+            ],\\n+            // Also expose output for assertions\\n+            output: out,\\n+          } as any;\\n+        }\\n+        return { issues: [], output: out } as any;\\n+      }\\n+    } catch {}\\n+\\n     try {\\n       // Render the command with Liquid templates if needed\\n       let renderedCommand = command;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/github-ops-provider.ts\",\"additions\":4,\"deletions\":1,\"changes\":119,\"patch\":\"diff --git a/src/providers/github-ops-provider.ts b/src/providers/github-ops-provider.ts\\nindex 1dafb432..2e7cef21 100644\\n--- a/src/providers/github-ops-provider.ts\\n+++ b/src/providers/github-ops-provider.ts\\n@@ -4,6 +4,7 @@ import { ReviewSummary } from '../reviewer';\\n import Sandbox from '@nyariv/sandboxjs';\\n import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n+import { logger } from '../logger';\\n \\n export class GitHubOpsProvider extends CheckProvider {\\n   private sandbox?: Sandbox;\\n@@ -51,11 +52,29 @@ export class GitHubOpsProvider extends CheckProvider {\\n \\n     // IMPORTANT: Always prefer authenticated octokit from event context (GitHub App or token)\\n     // This ensures proper bot identity in reactions, labels, and comments\\n-    const octokit: import('@octokit/rest').Octokit | undefined = config.eventContext?.octokit as\\n+    let octokit: import('@octokit/rest').Octokit | undefined = config.eventContext?.octokit as\\n       | import('@octokit/rest').Octokit\\n       | undefined;\\n+    if (process.env.VISOR_DEBUG === 'true') {\\n+      try {\\n+        logger.debug(`[github-ops] pre-fallback octokit? ${!!octokit}`);\\n+      } catch {}\\n+    }\\n+    // Test runner fallback: use global recorder if eventContext is missing octokit\\n+    if (!octokit) {\\n+      try {\\n+        const { getGlobalRecorder } = require('../test-runner/recorders/global-recorder');\\n+        const rec = getGlobalRecorder && getGlobalRecorder();\\n+        if (rec) octokit = rec as any;\\n+      } catch {}\\n+    }\\n \\n     if (!octokit) {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        try {\\n+          console.error('[github-ops] missing octokit after fallback — returning issue');\\n+        } catch {}\\n+      }\\n       return {\\n         issues: [\\n           {\\n@@ -72,7 +91,24 @@ export class GitHubOpsProvider extends CheckProvider {\\n     }\\n \\n     const repoEnv = process.env.GITHUB_REPOSITORY || '';\\n-    const [owner, repo] = repoEnv.split('/') as [string, string];\\n+    let owner = '';\\n+    let repo = '';\\n+    if (repoEnv.includes('/')) {\\n+      [owner, repo] = repoEnv.split('/') as [string, string];\\n+    } else {\\n+      try {\\n+        const ec: any = config.eventContext || {};\\n+        owner = ec?.repository?.owner?.login || owner;\\n+        repo = ec?.repository?.name || repo;\\n+      } catch {}\\n+    }\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        logger.info(\\n+          `[github-ops] context octokit? ${!!octokit} repo=${owner}/${repo} pr#=${prInfo?.number}`\\n+        );\\n+      }\\n+    } catch {}\\n     if (!owner || !repo || !prInfo?.number) {\\n       return {\\n         issues: [\\n@@ -93,6 +129,11 @@ export class GitHubOpsProvider extends CheckProvider {\\n     if (Array.isArray(cfg.values)) valuesRaw = (cfg.values as unknown[]).map(v => String(v));\\n     else if (typeof cfg.values === 'string') valuesRaw = [cfg.values];\\n     else if (typeof cfg.value === 'string') valuesRaw = [cfg.value];\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        logger.info(`[github-ops] op=${cfg.op} valuesRaw(before)=${JSON.stringify(valuesRaw)}`);\\n+      }\\n+    } catch {}\\n \\n     // Liquid render helper for values\\n     const renderValues = async (arr: string[]): Promise<string[]> => {\\n@@ -109,6 +150,17 @@ export class GitHubOpsProvider extends CheckProvider {\\n           outputs[name] = summary.output !== undefined ? summary.output : summary;\\n         }\\n       }\\n+      // Fallback: if outputs missing but engine provided history, use last output snapshot\\n+      try {\\n+        const hist = (config as any).__outputHistory as Map<string, unknown[]> | undefined;\\n+        if (hist) {\\n+          for (const [name, arr] of hist.entries()) {\\n+            if (!outputs[name] && Array.isArray(arr) && arr.length > 0) {\\n+              outputs[name] = arr[arr.length - 1];\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n       const ctx = {\\n         pr: {\\n           number: prInfo.number,\\n@@ -120,6 +172,25 @@ export class GitHubOpsProvider extends CheckProvider {\\n         },\\n         outputs,\\n       };\\n+      try {\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          logger.info(`[github-ops] deps keys=${Object.keys(outputs).join(', ')}`);\\n+          const ov = outputs['overview'] as any;\\n+          if (ov) {\\n+            logger.info(`[github-ops] outputs.overview.keys=${Object.keys(ov).join(',')}`);\\n+            if (ov.tags) {\\n+              logger.info(\\n+                `[github-ops] outputs.overview.tags keys=${Object.keys(ov.tags).join(',')}`\\n+              );\\n+              try {\\n+                logger.info(\\n+                  `[github-ops] outputs.overview.tags['review-effort']=${String(ov.tags['review-effort'])}`\\n+                );\\n+              } catch {}\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n       const out: string[] = [];\\n       for (const item of arr) {\\n         if (typeof item === 'string' && (item.includes('{{') || item.includes('{%'))) {\\n@@ -129,6 +200,9 @@ export class GitHubOpsProvider extends CheckProvider {\\n           } catch (e) {\\n             // If Liquid fails, surface as a provider error\\n             const msg = e instanceof Error ? e.message : String(e);\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              logger.warn(`[github-ops] liquid_render_error: ${msg}`);\\n+            }\\n             return Promise.reject({\\n               issues: [\\n                 {\\n@@ -175,6 +249,9 @@ export class GitHubOpsProvider extends CheckProvider {\\n         else if (Array.isArray(res)) values = (res as unknown[]).map(v => String(v));\\n       } catch (e) {\\n         const msg = e instanceof Error ? e.message : String(e);\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          logger.warn(`[github-ops] value_js_error: ${msg}`);\\n+        }\\n         return {\\n           issues: [\\n             {\\n@@ -190,14 +267,49 @@ export class GitHubOpsProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Fallback: if values are still empty, try deriving from dependency outputs (common pattern: outputs.<dep>.tags)\\n+    if (values.length === 0 && dependencyResults && dependencyResults.size > 0) {\\n+      try {\\n+        const derived: string[] = [];\\n+        for (const result of dependencyResults.values()) {\\n+          const out = (result as ReviewSummary & { output?: unknown })?.output ?? result;\\n+          const tags = (out as Record<string, unknown>)?.['tags'] as\\n+            | Record<string, unknown>\\n+            | undefined;\\n+          if (tags && typeof tags === 'object') {\\n+            const label = tags['label'];\\n+            const effort = (tags as Record<string, unknown>)['review-effort'];\\n+            if (label != null) derived.push(String(label));\\n+            if (effort !== undefined && effort !== null)\\n+              derived.push(`review/effort:${String(effort)}`);\\n+          }\\n+        }\\n+        values = derived;\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          logger.info(`[github-ops] derived values from deps: ${JSON.stringify(values)}`);\\n+        }\\n+      } catch {}\\n+    }\\n+\\n     // Trim, drop empty, and de-duplicate values regardless of source\\n     values = values.map(v => v.trim()).filter(v => v.length > 0);\\n     values = Array.from(new Set(values));\\n \\n+    try {\\n+      // Minimal debug to help diagnose label flow under tests\\n+      if (process.env.NODE_ENV === 'test' || process.env.VISOR_DEBUG === 'true') {\\n+        logger.info(`[github-ops] ${cfg.op} resolved values: ${JSON.stringify(values)}`);\\n+      }\\n+    } catch {}\\n+\\n     try {\\n       switch (cfg.op) {\\n         case 'labels.add': {\\n           if (values.length === 0) break; // no-op if nothing to add\\n+          try {\\n+            if (process.env.VISOR_OUTPUT_FORMAT !== 'json')\\n+              logger.step(`[github-ops] labels.add -> ${JSON.stringify(values)}`);\\n+          } catch {}\\n           await octokit.rest.issues.addLabels({\\n             owner,\\n             repo,\\n@@ -246,6 +358,9 @@ export class GitHubOpsProvider extends CheckProvider {\\n       return { issues: [] };\\n     } catch (e) {\\n       const msg = e instanceof Error ? e.message : String(e);\\n+      try {\\n+        logger.error(`[github-ops] op_failed ${cfg.op}: ${msg}`);\\n+      } catch {}\\n       return {\\n         issues: [\\n           {\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/http-client-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":11,\"patch\":\"diff --git a/src/providers/http-client-provider.ts b/src/providers/http-client-provider.ts\\nindex 9620f01b..4d8c41be 100644\\n--- a/src/providers/http-client-provider.ts\\n+++ b/src/providers/http-client-provider.ts\\n@@ -54,7 +54,7 @@ export class HttpClientProvider extends CheckProvider {\\n     prInfo: PRInfo,\\n     config: CheckProviderConfig,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n-    _sessionInfo?: { parentSessionId?: string; reuseSession?: boolean }\\n+    context?: import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n     const url = config.url as string;\\n     const method = (config.method as string) || 'GET';\\n@@ -96,8 +96,13 @@ export class HttpClientProvider extends CheckProvider {\\n       // Resolve environment variables in headers\\n       const resolvedHeaders = EnvironmentResolver.resolveHeaders(headers);\\n \\n-      // Fetch data from the endpoint\\n-      const data = await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n+      // Test hook: mock HTTP response for this step\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = context?.hooks?.mockForStep?.(String(stepName));\\n+      const data =\\n+        mock !== undefined\\n+          ? mock\\n+          : await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n \\n       // Apply transformation if specified\\n       let processedData = data;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/memory-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":40,\"patch\":\"diff --git a/src/providers/memory-check-provider.ts b/src/providers/memory-check-provider.ts\\nindex aee629c5..dca92621 100644\\n--- a/src/providers/memory-check-provider.ts\\n+++ b/src/providers/memory-check-provider.ts\\n@@ -381,34 +381,36 @@ export class MemoryCheckProvider extends CheckProvider {\\n     try {\\n       if (\\n         (config as any).checkName === 'aggregate-validations' ||\\n-        (config as any).checkName === 'aggregate' ||\\n         (config as any).checkName === 'aggregate'\\n       ) {\\n-        const hist = (enhancedContext as any)?.outputs?.history || {};\\n-        const keys = Object.keys(hist);\\n-        console.log('[MemoryProvider]', (config as any).checkName, ': history keys =', keys);\\n-        const vf = (hist as any)['validate-fact'];\\n-        console.log(\\n-          '[MemoryProvider]',\\n-          (config as any).checkName,\\n-          ': validate-fact history length =',\\n-          Array.isArray(vf) ? vf.length : 'n/a'\\n-        );\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          const hist = (enhancedContext as any)?.outputs?.history || {};\\n+          const keys = Object.keys(hist);\\n+          logger.debug(\\n+            `[MemoryProvider] ${(config as any).checkName}: history keys = [${keys.join(', ')}]`\\n+          );\\n+          const vf = (hist as any)['validate-fact'];\\n+          logger.debug(\\n+            `[MemoryProvider] ${(config as any).checkName}: validate-fact history length = ${\\n+              Array.isArray(vf) ? vf.length : 'n/a'\\n+            }`\\n+          );\\n+        }\\n       }\\n     } catch {}\\n \\n     const result = this.evaluateJavaScriptBlock(script, enhancedContext);\\n     try {\\n-      if ((config as any).checkName === 'aggregate-validations') {\\n+      if (\\n+        (config as any).checkName === 'aggregate-validations' &&\\n+        process.env.VISOR_DEBUG === 'true'\\n+      ) {\\n         const tv = store.get('total_validations', 'fact-validation');\\n         const av = store.get('all_valid', 'fact-validation');\\n-        console.error(\\n-          '[MemoryProvider] post-exec',\\n-          (config as any).checkName,\\n-          'total_validations=',\\n-          tv,\\n-          'all_valid=',\\n-          av\\n+        logger.debug(\\n+          `[MemoryProvider] post-exec ${(config as any).checkName} total_validations=${String(\\n+            tv\\n+          )} all_valid=${String(av)}`\\n         );\\n       }\\n     } catch {}\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/assertions.ts\",\"additions\":4,\"deletions\":0,\"changes\":91,\"patch\":\"diff --git a/src/test-runner/assertions.ts b/src/test-runner/assertions.ts\\nnew file mode 100644\\nindex 00000000..ea676eea\\n--- /dev/null\\n+++ b/src/test-runner/assertions.ts\\n@@ -0,0 +1,91 @@\\n+export type CountExpectation = {\\n+  exactly?: number;\\n+  at_least?: number;\\n+  at_most?: number;\\n+};\\n+\\n+export interface CallsExpectation extends CountExpectation {\\n+  step?: string;\\n+  provider?: 'github' | string;\\n+  op?: string;\\n+  args?: Record<string, unknown>;\\n+}\\n+\\n+export interface PromptsExpectation {\\n+  step: string;\\n+  index?: number | 'first' | 'last';\\n+  contains?: string[];\\n+  not_contains?: string[];\\n+  matches?: string; // regex string\\n+  where?: {\\n+    contains?: string[];\\n+    not_contains?: string[];\\n+    matches?: string; // regex\\n+  };\\n+}\\n+\\n+export interface OutputsExpectation {\\n+  step: string;\\n+  index?: number | 'first' | 'last';\\n+  path: string;\\n+  equals?: unknown;\\n+  equalsDeep?: unknown;\\n+  matches?: string; // regex\\n+  where?: {\\n+    path: string;\\n+    equals?: unknown;\\n+    matches?: string; // regex\\n+  };\\n+  contains_unordered?: unknown[]; // array membership ignoring order\\n+}\\n+\\n+export interface ExpectBlock {\\n+  use?: string[];\\n+  calls?: CallsExpectation[];\\n+  prompts?: PromptsExpectation[];\\n+  outputs?: OutputsExpectation[];\\n+  no_calls?: Array<{ step?: string; provider?: string; op?: string }>;\\n+  fail?: { message_contains?: string };\\n+  strict_violation?: { for_step?: string; message_contains?: string };\\n+}\\n+\\n+export function validateCounts(exp: CountExpectation): void {\\n+  const keys = ['exactly', 'at_least', 'at_most'].filter(k => (exp as any)[k] !== undefined);\\n+  if (keys.length > 1) {\\n+    throw new Error(`Count expectation is ambiguous: ${keys.join(', ')}`);\\n+  }\\n+}\\n+\\n+export function deepEqual(a: unknown, b: unknown): boolean {\\n+  if (a === b) return true;\\n+  if (typeof a !== typeof b) return false;\\n+  if (a && b && typeof a === 'object') {\\n+    if (Array.isArray(a) && Array.isArray(b)) {\\n+      if (a.length !== b.length) return false;\\n+      for (let i = 0; i < a.length; i++) if (!deepEqual(a[i], b[i])) return false;\\n+      return true;\\n+    }\\n+    const ak = Object.keys(a as any).sort();\\n+    const bk = Object.keys(b as any).sort();\\n+    if (!deepEqual(ak, bk)) return false;\\n+    for (const k of ak) if (!deepEqual((a as any)[k], (b as any)[k])) return false;\\n+    return true;\\n+  }\\n+  return false;\\n+}\\n+\\n+export function containsUnordered(haystack: unknown[], needles: unknown[]): boolean {\\n+  if (!Array.isArray(haystack) || !Array.isArray(needles)) return false;\\n+  const used = new Array(haystack.length).fill(false);\\n+  outer: for (const n of needles) {\\n+    for (let i = 0; i < haystack.length; i++) {\\n+      if (used[i]) continue;\\n+      if (deepEqual(haystack[i], n) || haystack[i] === n) {\\n+        used[i] = true;\\n+        continue outer;\\n+      }\\n+    }\\n+    return false;\\n+  }\\n+  return true;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/fixture-loader.ts\",\"additions\":6,\"deletions\":0,\"changes\":156,\"patch\":\"diff --git a/src/test-runner/fixture-loader.ts b/src/test-runner/fixture-loader.ts\\nnew file mode 100644\\nindex 00000000..7ec38d15\\n--- /dev/null\\n+++ b/src/test-runner/fixture-loader.ts\\n@@ -0,0 +1,156 @@\\n+export type BuiltinFixtureName =\\n+  | 'gh.pr_open.minimal'\\n+  | 'gh.pr_sync.minimal'\\n+  | 'gh.issue_open.minimal'\\n+  | 'gh.issue_comment.standard'\\n+  | 'gh.issue_comment.visor_help'\\n+  | 'gh.issue_comment.visor_regenerate'\\n+  | 'gh.issue_comment.edited'\\n+  | 'gh.pr_closed.minimal';\\n+\\n+export interface LoadedFixture {\\n+  name: string;\\n+  webhook: { name: string; action?: string; payload: Record<string, unknown> };\\n+  git?: { branch?: string; baseBranch?: string };\\n+  files?: Array<{\\n+    path: string;\\n+    content: string;\\n+    status?: 'added' | 'modified' | 'removed' | 'renamed';\\n+    additions?: number;\\n+    deletions?: number;\\n+  }>;\\n+  diff?: string; // unified diff text\\n+  env?: Record<string, string>;\\n+  time?: { now?: string };\\n+}\\n+\\n+export class FixtureLoader {\\n+  load(name: BuiltinFixtureName): LoadedFixture {\\n+    // Minimal, stable, general-purpose fixtures used by the test runner.\\n+    // All fixtures supply a webhook payload and, for PR variants, a small diff.\\n+    if (name.startsWith('gh.pr_open')) {\\n+      const files: LoadedFixture['files'] = [\\n+        {\\n+          path: 'src/search.ts',\\n+          content: 'export function search(q: string) {\\\\n  return []\\\\n}\\\\n',\\n+          status: 'added',\\n+          additions: 3,\\n+          deletions: 0,\\n+        },\\n+      ];\\n+      const diff = this.buildUnifiedDiff(files);\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'pull_request',\\n+          action: 'opened',\\n+          payload: { pull_request: { number: 1, title: 'feat: add user search' } },\\n+        },\\n+        git: { branch: 'feature/test', baseBranch: 'main' },\\n+        files,\\n+        diff,\\n+      };\\n+    }\\n+    if (name.startsWith('gh.pr_sync')) {\\n+      const files: LoadedFixture['files'] = [\\n+        {\\n+          path: 'src/search.ts',\\n+          content: 'export function search(q: string) {\\\\n  return [q] // updated\\\\n}\\\\n',\\n+          status: 'modified',\\n+          additions: 1,\\n+          deletions: 0,\\n+        },\\n+      ];\\n+      const diff = this.buildUnifiedDiff(files);\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'pull_request',\\n+          action: 'synchronize',\\n+          payload: { pull_request: { number: 1, title: 'feat: add user search (update)' } },\\n+        },\\n+        git: { branch: 'feature/test', baseBranch: 'main' },\\n+        files,\\n+        diff,\\n+      };\\n+    }\\n+    if (name.startsWith('gh.issue_open')) {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issues',\\n+          action: 'opened',\\n+          payload: {\\n+            issue: { number: 12, title: 'Bug: crashes on search edge case', body: 'Steps...' },\\n+          },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.issue_comment.standard') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issue_comment',\\n+          action: 'created',\\n+          payload: { comment: { body: 'Thanks for the update!' }, issue: { number: 1 } },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.issue_comment.visor_help') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issue_comment',\\n+          action: 'created',\\n+          payload: { comment: { body: '/visor help' }, issue: { number: 1 } },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.issue_comment.visor_regenerate') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issue_comment',\\n+          action: 'created',\\n+          payload: { comment: { body: '/visor Regenerate reviews' }, issue: { number: 1 } },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.pr_closed.minimal') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'pull_request',\\n+          action: 'closed',\\n+          payload: { pull_request: { number: 1, title: 'feat: add user search' } },\\n+        },\\n+      };\\n+    }\\n+    // Fallback minimal\\n+    return {\\n+      name,\\n+      webhook: { name: 'unknown', payload: {} },\\n+    };\\n+  }\\n+\\n+  private buildUnifiedDiff(\\n+    files: Array<{ path: string; content: string; status?: string }>\\n+  ): string {\\n+    // Build a very small, stable unified diff suitable for prompts\\n+    const chunks = files.map(f => {\\n+      const header =\\n+        `diff --git a/${f.path} b/${f.path}\\\\n` +\\n+        (f.status === 'added'\\n+          ? 'index 0000000..1111111 100644\\\\n--- /dev/null\\\\n'\\n+          : `index 1111111..2222222 100644\\\\n--- a/${f.path}\\\\n`) +\\n+        `+++ b/${f.path}\\\\n` +\\n+        '@@\\\\n';\\n+      const body = f.content\\n+        .split('\\\\n')\\n+        .map(line => (f.status === 'removed' ? `-${line}` : `+${line}`))\\n+        .join('\\\\n');\\n+      return header + body + '\\\\n';\\n+    });\\n+    return chunks.join('\\\\n');\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/index.ts\",\"additions\":53,\"deletions\":0,\"changes\":1527,\"patch\":\"diff --git a/src/test-runner/index.ts b/src/test-runner/index.ts\\nnew file mode 100644\\nindex 00000000..cbefc162\\n--- /dev/null\\n+++ b/src/test-runner/index.ts\\n@@ -0,0 +1,1527 @@\\n+import fs from 'fs';\\n+import path from 'path';\\n+import * as yaml from 'js-yaml';\\n+\\n+import { ConfigManager } from '../config';\\n+import { CheckExecutionEngine } from '../check-execution-engine';\\n+import type { PRInfo } from '../pr-analyzer';\\n+import { RecordingOctokit } from './recorders/github-recorder';\\n+import { setGlobalRecorder } from './recorders/global-recorder';\\n+import { FixtureLoader } from './fixture-loader';\\n+import { validateCounts, type ExpectBlock } from './assertions';\\n+import { validateTestsDoc } from './validator';\\n+\\n+export type TestCase = {\\n+  name: string;\\n+  description?: string;\\n+  event?: string;\\n+  flow?: Array<{ name: string }>;\\n+};\\n+\\n+export type TestSuite = {\\n+  version: string;\\n+  extends?: string | string[];\\n+  tests: {\\n+    defaults?: Record<string, unknown>;\\n+    fixtures?: unknown[];\\n+    cases: TestCase[];\\n+  };\\n+};\\n+\\n+export interface DiscoverOptions {\\n+  testsPath?: string; // Path to .visor.tests.yaml\\n+  cwd?: string;\\n+}\\n+\\n+function isObject(v: unknown): v is Record<string, unknown> {\\n+  return !!v && typeof v === 'object' && !Array.isArray(v);\\n+}\\n+\\n+export class VisorTestRunner {\\n+  constructor(private readonly cwd: string = process.cwd()) {}\\n+\\n+  private line(title = '', char = '─', width = 60): string {\\n+    if (!title) return char.repeat(width);\\n+    const pad = Math.max(1, width - title.length - 2);\\n+    return `${char.repeat(2)} ${title} ${char.repeat(pad)}`;\\n+  }\\n+\\n+  private printCaseHeader(name: string, kind: 'flow' | 'single', event?: string): void {\\n+    console.log('\\\\n' + this.line(`Case: ${name}`));\\n+    const meta: string[] = [`type=${kind}`];\\n+    if (event) meta.push(`event=${event}`);\\n+    console.log(`  ${meta.join('  ·  ')}`);\\n+  }\\n+\\n+  private printStageHeader(\\n+    flowName: string,\\n+    stageName: string,\\n+    event?: string,\\n+    fixture?: string\\n+  ): void {\\n+    console.log('\\\\n' + this.line(`${flowName} — ${stageName}`));\\n+    const meta: string[] = [];\\n+    if (event) meta.push(`event=${event}`);\\n+    if (fixture) meta.push(`fixture=${fixture}`);\\n+    if (meta.length) console.log(`  ${meta.join('  ·  ')}`);\\n+  }\\n+\\n+  private printSelectedChecks(checks: string[]): void {\\n+    if (!checks || checks.length === 0) return;\\n+    console.log(`  checks: ${checks.join(', ')}`);\\n+  }\\n+\\n+  /**\\n+   * Locate a tests file: explicit path > ./.visor.tests.yaml > defaults/.visor.tests.yaml\\n+   */\\n+  public resolveTestsPath(explicit?: string): string {\\n+    if (explicit) {\\n+      return path.isAbsolute(explicit) ? explicit : path.resolve(this.cwd, explicit);\\n+    }\\n+    const candidates = [\\n+      path.resolve(this.cwd, '.visor.tests.yaml'),\\n+      path.resolve(this.cwd, '.visor.tests.yml'),\\n+      path.resolve(this.cwd, 'defaults/.visor.tests.yaml'),\\n+      path.resolve(this.cwd, 'defaults/.visor.tests.yml'),\\n+    ];\\n+    for (const p of candidates) {\\n+      if (fs.existsSync(p)) return p;\\n+    }\\n+    throw new Error(\\n+      'No tests file found. Provide --config <path> or add .visor.tests.yaml (or defaults/.visor.tests.yaml).'\\n+    );\\n+  }\\n+\\n+  /**\\n+   * Load and minimally validate tests YAML.\\n+   */\\n+  public loadSuite(testsPath: string): TestSuite {\\n+    const raw = fs.readFileSync(testsPath, 'utf8');\\n+    const doc = yaml.load(raw) as unknown;\\n+    const validation = validateTestsDoc(doc);\\n+    if (!validation.ok) {\\n+      const errs = validation.errors.map(e => ` - ${e}`).join('\\\\n');\\n+      throw new Error(`Tests file validation failed:\\\\n${errs}`);\\n+    }\\n+    if (!isObject(doc)) throw new Error('Tests YAML must be a YAML object');\\n+\\n+    const version = String((doc as any).version ?? '1.0');\\n+    const tests = (doc as any).tests;\\n+    if (!tests || !isObject(tests)) throw new Error('tests: {} section is required');\\n+    const cases = (tests as any).cases as unknown;\\n+    if (!Array.isArray(cases) || cases.length === 0) {\\n+      throw new Error('tests.cases must be a non-empty array');\\n+    }\\n+\\n+    // Preserve full case objects for execution; discovery prints selective fields\\n+    const suite: TestSuite = {\\n+      version,\\n+      extends: (doc as any).extends,\\n+      tests: {\\n+        defaults: (tests as any).defaults || {},\\n+        fixtures: (tests as any).fixtures || [],\\n+        cases: (tests as any).cases,\\n+      },\\n+    };\\n+    return suite;\\n+  }\\n+\\n+  /**\\n+   * Pretty print discovered cases to stdout.\\n+   */\\n+  public printDiscovery(testsPath: string, suite: TestSuite): void {\\n+    const rel = path.relative(this.cwd, testsPath) || testsPath;\\n+    console.log('🧪 Visor Test Runner — discovery mode');\\n+    console.log(`   Suite: ${rel}`);\\n+    const parent = suite.extends\\n+      ? Array.isArray(suite.extends)\\n+        ? suite.extends.join(', ')\\n+        : String(suite.extends)\\n+      : '(none)';\\n+    console.log(`   Extends: ${parent}`);\\n+    const defaults = suite.tests.defaults || {};\\n+    const strict = (defaults as any).strict === undefined ? true : !!(defaults as any).strict;\\n+    console.log(`   Strict: ${strict ? 'on' : 'off'}`);\\n+\\n+    // List cases\\n+    console.log('\\\\nCases:');\\n+    for (const c of suite.tests.cases) {\\n+      const isFlow = Array.isArray(c.flow) && c.flow.length > 0;\\n+      const badge = isFlow ? 'flow' : c.event || 'event';\\n+      console.log(` - ${c.name} [${badge}]`);\\n+    }\\n+    console.log('\\\\nTip: run `visor test --only <name>` to filter, `--bail` to stop early.');\\n+  }\\n+\\n+  /**\\n+   * Execute non-flow cases with minimal assertions (Milestone 1 MVP).\\n+   */\\n+  public async runCases(\\n+    testsPath: string,\\n+    suite: TestSuite,\\n+    options: { only?: string; bail?: boolean; maxParallel?: number; promptMaxChars?: number }\\n+  ): Promise<{\\n+    failures: number;\\n+    results: Array<{\\n+      name: string;\\n+      passed: boolean;\\n+      errors?: string[];\\n+      stages?: Array<{ name: string; errors?: string[] }>;\\n+    }>;\\n+  }> {\\n+    // Save defaults for flow runner access\\n+    (this as any).suiteDefaults = suite.tests.defaults || {};\\n+    // Support --only \\\"case\\\" and --only \\\"case#stage\\\"\\n+    let onlyCase = options.only?.toLowerCase();\\n+    let stageFilter: string | undefined;\\n+    if (onlyCase && onlyCase.includes('#')) {\\n+      const parts = onlyCase.split('#');\\n+      onlyCase = parts[0];\\n+      stageFilter = (parts[1] || '').trim();\\n+    }\\n+    const allCases = suite.tests.cases;\\n+    const selected = onlyCase\\n+      ? allCases.filter(c => c.name.toLowerCase().includes(onlyCase as string))\\n+      : allCases;\\n+    if (selected.length === 0) {\\n+      console.log('No matching cases.');\\n+      return { failures: 0, results: [] };\\n+    }\\n+\\n+    // Load merged config via ConfigManager (honors extends), then clone for test overrides\\n+    const cm = new ConfigManager();\\n+    // Prefer loading the base config referenced by extends; fall back to the tests file\\n+    let configFileToLoad = testsPath;\\n+    const parentExt = suite.extends;\\n+    if (parentExt) {\\n+      const first = Array.isArray(parentExt) ? parentExt[0] : parentExt;\\n+      if (typeof first === 'string') {\\n+        const resolved = path.isAbsolute(first)\\n+          ? first\\n+          : path.resolve(path.dirname(testsPath), first);\\n+        configFileToLoad = resolved;\\n+      }\\n+    }\\n+    const config = await cm.loadConfig(configFileToLoad, { validate: true, mergeDefaults: true });\\n+    if (!config.checks) {\\n+      throw new Error('Loaded config has no checks; cannot run tests');\\n+    }\\n+\\n+    const defaultsAny: any = suite.tests.defaults || {};\\n+    const defaultStrict = defaultsAny?.strict !== false;\\n+    const aiProviderDefault = defaultsAny?.ai_provider || 'mock';\\n+    const ghRec = defaultsAny?.github_recorder as\\n+      | { error_code?: number; timeout_ms?: number }\\n+      | undefined;\\n+    const defaultPromptCap: number | undefined =\\n+      options.promptMaxChars ||\\n+      (typeof defaultsAny?.prompt_max_chars === 'number'\\n+        ? defaultsAny.prompt_max_chars\\n+        : undefined);\\n+    const caseMaxParallel =\\n+      options.maxParallel ||\\n+      (typeof defaultsAny?.max_parallel === 'number' ? defaultsAny.max_parallel : undefined) ||\\n+      1;\\n+\\n+    // Test overrides: force AI provider to 'mock' when requested (default: mock per RFC)\\n+    const cfg = JSON.parse(JSON.stringify(config));\\n+    for (const name of Object.keys(cfg.checks || {})) {\\n+      const chk = cfg.checks[name] || {};\\n+      if ((chk.type || 'ai') === 'ai') {\\n+        const prev = (chk.ai || {}) as Record<string, unknown>;\\n+        chk.ai = {\\n+          ...prev,\\n+          provider: aiProviderDefault,\\n+          skip_code_context: true,\\n+          disable_tools: true,\\n+          timeout: Math.min(15000, (prev.timeout as number) || 15000),\\n+        } as any;\\n+        cfg.checks[name] = chk;\\n+      }\\n+    }\\n+\\n+    let failures = 0;\\n+    const caseResults: Array<{\\n+      name: string;\\n+      passed: boolean;\\n+      errors?: string[];\\n+      stages?: Array<{ name: string; errors?: string[] }>;\\n+    }> = [];\\n+    // Header: show suite path for clarity\\n+    try {\\n+      const rel = path.relative(this.cwd, testsPath) || testsPath;\\n+      console.log(`Suite: ${rel}`);\\n+    } catch {}\\n+\\n+    const runOne = async (_case: any): Promise<{ name: string; failed: number }> => {\\n+      // Case header for clarity\\n+      const isFlow = Array.isArray((_case as any).flow) && (_case as any).flow.length > 0;\\n+      const caseEvent = (_case as any).event as string | undefined;\\n+      this.printCaseHeader(\\n+        (_case as any).name || '(unnamed)',\\n+        isFlow ? 'flow' : 'single',\\n+        caseEvent\\n+      );\\n+      if ((_case as any).skip) {\\n+        console.log(`⏭ SKIP ${(_case as any).name}`);\\n+        caseResults.push({ name: _case.name, passed: true });\\n+        return { name: _case.name, failed: 0 };\\n+      }\\n+      if (Array.isArray((_case as any).flow) && (_case as any).flow.length > 0) {\\n+        const flowRes = await this.runFlowCase(\\n+          _case,\\n+          cfg,\\n+          defaultStrict,\\n+          options.bail || false,\\n+          defaultPromptCap,\\n+          stageFilter\\n+        );\\n+        const failed = flowRes.failures;\\n+        caseResults.push({ name: _case.name, passed: failed === 0, stages: flowRes.stages });\\n+        return { name: _case.name, failed };\\n+      }\\n+      const strict = (\\n+        typeof (_case as any).strict === 'boolean' ? (_case as any).strict : defaultStrict\\n+      ) as boolean;\\n+      const expect = ((_case as any).expect || {}) as ExpectBlock;\\n+      // Fixture selection with optional overrides\\n+      const fixtureInput =\\n+        typeof (_case as any).fixture === 'object' && (_case as any).fixture\\n+          ? (_case as any).fixture\\n+          : { builtin: (_case as any).fixture };\\n+      const prInfo = this.buildPrInfoFromFixture(fixtureInput?.builtin, fixtureInput?.overrides);\\n+\\n+      // Inject recording Octokit into engine via actionContext using env owner/repo\\n+      const prevRepo = process.env.GITHUB_REPOSITORY;\\n+      process.env.GITHUB_REPOSITORY = process.env.GITHUB_REPOSITORY || 'owner/repo';\\n+      // Apply case env overrides if present\\n+      const envOverrides =\\n+        typeof (_case as any).env === 'object' && (_case as any).env\\n+          ? ((_case as any).env as Record<string, string>)\\n+          : undefined;\\n+      const prevEnv: Record<string, string | undefined> = {};\\n+      if (envOverrides) {\\n+        for (const [k, v] of Object.entries(envOverrides)) {\\n+          prevEnv[k] = process.env[k];\\n+          process.env[k] = String(v);\\n+        }\\n+      }\\n+      const ghRecCase =\\n+        typeof (_case as any).github_recorder === 'object' && (_case as any).github_recorder\\n+          ? ((_case as any).github_recorder as { error_code?: number; timeout_ms?: number })\\n+          : undefined;\\n+      const rcOpts = ghRecCase || ghRec;\\n+      const recorder = new RecordingOctokit(\\n+        rcOpts ? { errorCode: rcOpts.error_code, timeoutMs: rcOpts.timeout_ms } : undefined\\n+      );\\n+      setGlobalRecorder(recorder);\\n+      const engine = new CheckExecutionEngine(undefined as any, recorder as unknown as any);\\n+\\n+      // Capture prompts per step\\n+      const prompts: Record<string, string[]> = {};\\n+      const mocks =\\n+        typeof (_case as any).mocks === 'object' && (_case as any).mocks\\n+          ? ((_case as any).mocks as Record<string, unknown>)\\n+          : {};\\n+      const mockCursors: Record<string, number> = {};\\n+      engine.setExecutionContext({\\n+        hooks: {\\n+          onPromptCaptured: (info: { step: string; provider: string; prompt: string }) => {\\n+            const k = info.step;\\n+            if (!prompts[k]) prompts[k] = [];\\n+            const p =\\n+              defaultPromptCap && info.prompt.length > defaultPromptCap\\n+                ? info.prompt.slice(0, defaultPromptCap)\\n+                : info.prompt;\\n+            prompts[k].push(p);\\n+          },\\n+          mockForStep: (step: string) => {\\n+            // Support list form: '<step>[]' means per-call mocks for forEach children\\n+            const listKey = `${step}[]`;\\n+            const list = (mocks as any)[listKey];\\n+            if (Array.isArray(list)) {\\n+              const i = mockCursors[listKey] || 0;\\n+              const idx = i < list.length ? i : list.length - 1; // clamp to last\\n+              mockCursors[listKey] = i + 1;\\n+              return list[idx];\\n+            }\\n+            return (mocks as any)[step];\\n+          },\\n+        },\\n+      } as any);\\n+\\n+      try {\\n+        const eventForCase = this.mapEventFromFixtureName(fixtureInput?.builtin);\\n+        const desiredSteps = new Set<string>(\\n+          (expect.calls || []).map(c => c.step).filter(Boolean) as string[]\\n+        );\\n+        let checksToRun = this.computeChecksToRun(\\n+          cfg,\\n+          eventForCase,\\n+          desiredSteps.size > 0 ? desiredSteps : undefined\\n+        );\\n+        this.printSelectedChecks(checksToRun);\\n+        if (checksToRun.length === 0) {\\n+          // Fallback: run all checks for this event when filtered set is empty\\n+          checksToRun = this.computeChecksToRun(cfg, eventForCase, undefined);\\n+        }\\n+        // Include all tagged checks by default in test mode: build tagFilter.include = union of all tags\\n+        // Do not pass an implicit tag filter during tests.\\n+        // Passing all known tags as an include-filter would exclude untagged steps.\\n+        // Let the engine apply whatever tag_filter the config already defines (if any).\\n+        const allTags: string[] = [];\\n+        // Inject octokit into eventContext so providers can perform real GitHub ops (recorded)\\n+        try {\\n+          (prInfo as any).eventContext = { ...(prInfo as any).eventContext, octokit: recorder };\\n+        } catch {}\\n+\\n+        const prevTestMode = process.env.VISOR_TEST_MODE;\\n+        process.env.VISOR_TEST_MODE = 'true';\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.log(`  ⮕ executing main stage with checks=[${checksToRun.join(', ')}]`);\\n+        }\\n+        let res = await engine.executeGroupedChecks(\\n+          prInfo,\\n+          checksToRun,\\n+          120000,\\n+          cfg,\\n+          'json',\\n+          process.env.VISOR_DEBUG === 'true',\\n+          undefined,\\n+          false,\\n+          {}\\n+        );\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          try {\\n+            const names = (res.statistics.checks || []).map(\\n+              (c: any) => `${c.checkName}:${c.totalRuns || 0}`\\n+            );\\n+            console.log(`  ⮕ main stats: [${names.join(', ')}]`);\\n+          } catch {}\\n+        }\\n+        try {\\n+          const dbgHist = engine.getOutputHistorySnapshot();\\n+          console.log(\\n+            `  ⮕ stage base history keys: ${Object.keys(dbgHist).join(', ') || '(none)'}`\\n+          );\\n+        } catch {}\\n+        // After main stage run, ensure static on_finish.run targets for forEach parents executed.\\n+        try {\\n+          const hist0 = engine.getOutputHistorySnapshot();\\n+          if (process.env.VISOR_DEBUG === 'true') {\\n+            console.log(`  ⮕ history keys: ${Object.keys(hist0).join(', ') || '(none)'}`);\\n+          }\\n+          const parents = Object.entries(cfg.checks || {})\\n+            .filter(\\n+              ([name, c]: [string, any]) =>\\n+                checksToRun.includes(name) &&\\n+                c &&\\n+                c.forEach &&\\n+                c.on_finish &&\\n+                Array.isArray(c.on_finish.run) &&\\n+                c.on_finish.run.length > 0\\n+            )\\n+            .map(([name, c]: [string, any]) => ({ name, onFinish: c.on_finish }));\\n+          if (process.env.VISOR_DEBUG === 'true') {\\n+            console.log(\\n+              `  ⮕ forEach parents with on_finish: ${parents.map(p => p.name).join(', ') || '(none)'}`\\n+            );\\n+          }\\n+          const missing: string[] = [];\\n+          for (const p of parents) {\\n+            for (const t of p.onFinish.run as string[]) {\\n+              if (!hist0[t] || (Array.isArray(hist0[t]) && hist0[t].length === 0)) {\\n+                missing.push(t);\\n+              }\\n+            }\\n+          }\\n+          // Dedup missing and exclude anything already in checksToRun\\n+          const toRun = Array.from(new Set(missing.filter(n => !checksToRun.includes(n))));\\n+          if (toRun.length > 0) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ⮕ on_finish.fallback: running [${toRun.join(', ')}]`);\\n+            }\\n+            // Run once; reuse same engine instance so output history stays visible\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ⮕ executing on_finish.fallback with checks=[${toRun.join(', ')}]`);\\n+            }\\n+            const fallbackRes = await engine.executeGroupedChecks(\\n+              prInfo,\\n+              toRun,\\n+              120000,\\n+              cfg,\\n+              'json',\\n+              process.env.VISOR_DEBUG === 'true',\\n+              undefined,\\n+              false,\\n+              {}\\n+            );\\n+            // Optionally merge statistics (for stage coverage we rely on deltas + stats from last run)\\n+            res = {\\n+              results: fallbackRes.results || res.results,\\n+              statistics: fallbackRes.statistics || res.statistics,\\n+            } as any;\\n+          }\\n+        } catch {}\\n+        if (prevTestMode === undefined) delete process.env.VISOR_TEST_MODE;\\n+        else process.env.VISOR_TEST_MODE = prevTestMode;\\n+        const outHistory = engine.getOutputHistorySnapshot();\\n+\\n+        const caseFailures = this.evaluateCase(\\n+          _case.name,\\n+          res.statistics,\\n+          recorder,\\n+          expect,\\n+          strict,\\n+          prompts,\\n+          res.results,\\n+          outHistory\\n+        );\\n+        // Warn about unmocked AI/command steps that executed\\n+        try {\\n+          const mocksUsed =\\n+            typeof (_case as any).mocks === 'object' && (_case as any).mocks\\n+              ? ((_case as any).mocks as Record<string, unknown>)\\n+              : {};\\n+          this.warnUnmockedProviders(res.statistics, cfg, mocksUsed);\\n+        } catch {}\\n+        this.printCoverage(_case.name, res.statistics, expect);\\n+        if (caseFailures.length === 0) {\\n+          console.log(`✅ PASS ${_case.name}`);\\n+          caseResults.push({ name: _case.name, passed: true });\\n+        } else {\\n+          console.log(`❌ FAIL ${_case.name}`);\\n+          for (const f of caseFailures) console.log(`   - ${f}`);\\n+          caseResults.push({ name: _case.name, passed: false, errors: caseFailures });\\n+          return { name: _case.name, failed: 1 };\\n+        }\\n+      } catch (err) {\\n+        console.log(`❌ ERROR ${_case.name}: ${err instanceof Error ? err.message : String(err)}`);\\n+        caseResults.push({\\n+          name: _case.name,\\n+          passed: false,\\n+          errors: [err instanceof Error ? err.message : String(err)],\\n+        });\\n+        return { name: _case.name, failed: 1 };\\n+      } finally {\\n+        if (prevRepo === undefined) delete process.env.GITHUB_REPOSITORY;\\n+        else process.env.GITHUB_REPOSITORY = prevRepo;\\n+        if (envOverrides) {\\n+          for (const [k, oldv] of Object.entries(prevEnv)) {\\n+            if (oldv === undefined) delete process.env[k];\\n+            else process.env[k] = oldv;\\n+          }\\n+        }\\n+      }\\n+      return { name: _case.name, failed: 0 };\\n+    };\\n+\\n+    if (options.bail || false || caseMaxParallel <= 1) {\\n+      for (const _case of selected) {\\n+        const r = await runOne(_case);\\n+        failures += r.failed;\\n+        if (options.bail && r.failed > 0) break;\\n+      }\\n+    } else {\\n+      let idx = 0;\\n+      const workers = Math.min(caseMaxParallel, selected.length);\\n+      const runWorker = async () => {\\n+        while (true) {\\n+          const i = idx++;\\n+          if (i >= selected.length) return;\\n+          const r = await runOne(selected[i]);\\n+          failures += r.failed;\\n+        }\\n+      };\\n+      await Promise.all(Array.from({ length: workers }, runWorker));\\n+    }\\n+\\n+    // Summary\\n+    const passed = selected.length - failures;\\n+    console.log(`\\\\nSummary: ${passed}/${selected.length} passed`);\\n+    return { failures, results: caseResults };\\n+  }\\n+\\n+  private async runFlowCase(\\n+    flowCase: any,\\n+    cfg: any,\\n+    defaultStrict: boolean,\\n+    bail: boolean,\\n+    promptCap?: number,\\n+    stageFilter?: string\\n+  ): Promise<{ failures: number; stages: Array<{ name: string; errors?: string[] }> }> {\\n+    const suiteDefaults: any = (this as any).suiteDefaults || {};\\n+    const ghRec = suiteDefaults.github_recorder as\\n+      | { error_code?: number; timeout_ms?: number }\\n+      | undefined;\\n+    const ghRecCase =\\n+      typeof (flowCase as any).github_recorder === 'object' && (flowCase as any).github_recorder\\n+        ? ((flowCase as any).github_recorder as { error_code?: number; timeout_ms?: number })\\n+        : undefined;\\n+    const rcOpts = ghRecCase || ghRec;\\n+    const recorder = new RecordingOctokit(\\n+      rcOpts ? { errorCode: rcOpts.error_code, timeoutMs: rcOpts.timeout_ms } : undefined\\n+    );\\n+    setGlobalRecorder(recorder);\\n+    const engine = new CheckExecutionEngine(undefined as any, recorder as unknown as any);\\n+    const flowName = flowCase.name || 'flow';\\n+    let failures = 0;\\n+    const stagesSummary: Array<{ name: string; errors?: string[] }> = [];\\n+\\n+    // Shared prompts map across flow; we will compute per-stage deltas\\n+    const prompts: Record<string, string[]> = {};\\n+    let stageMocks: Record<string, unknown> =\\n+      typeof flowCase.mocks === 'object' && flowCase.mocks\\n+        ? (flowCase.mocks as Record<string, unknown>)\\n+        : {};\\n+    let stageMockCursors: Record<string, number> = {};\\n+    engine.setExecutionContext({\\n+      hooks: {\\n+        onPromptCaptured: (info: { step: string; provider: string; prompt: string }) => {\\n+          const k = info.step;\\n+          if (!prompts[k]) prompts[k] = [];\\n+          const p =\\n+            promptCap && info.prompt.length > promptCap\\n+              ? info.prompt.slice(0, promptCap)\\n+              : info.prompt;\\n+          prompts[k].push(p);\\n+        },\\n+        mockForStep: (step: string) => {\\n+          const listKey = `${step}[]`;\\n+          const list = (stageMocks as any)[listKey];\\n+          if (Array.isArray(list)) {\\n+            const i = stageMockCursors[listKey] || 0;\\n+            const idx = i < list.length ? i : list.length - 1;\\n+            stageMockCursors[listKey] = i + 1;\\n+            return list[idx];\\n+          }\\n+          return (stageMocks as any)[step];\\n+        },\\n+      },\\n+    } as any);\\n+\\n+    // Run each stage\\n+    // Normalize stage filter\\n+    const sf = (stageFilter || '').trim().toLowerCase();\\n+    const sfIndex = sf && /^\\\\d+$/.test(sf) ? parseInt(sf, 10) : undefined;\\n+    let anyStageRan = false;\\n+    for (let i = 0; i < flowCase.flow.length; i++) {\\n+      const stage = flowCase.flow[i];\\n+      const stageName = `${flowName}#${stage.name || `stage-${i + 1}`}`;\\n+      // Apply stage filter if provided: match by name substring or 1-based index\\n+      if (sf) {\\n+        const nm = String(stage.name || `stage-${i + 1}`).toLowerCase();\\n+        const idxMatch = sfIndex !== undefined && sfIndex === i + 1;\\n+        const nameMatch = nm.includes(sf);\\n+        if (!(idxMatch || nameMatch)) continue;\\n+      }\\n+      anyStageRan = true;\\n+      const strict = (\\n+        typeof flowCase.strict === 'boolean' ? flowCase.strict : defaultStrict\\n+      ) as boolean;\\n+\\n+      // Fixture + env\\n+      const fixtureInput =\\n+        typeof stage.fixture === 'object' && stage.fixture\\n+          ? stage.fixture\\n+          : { builtin: stage.fixture };\\n+      const prInfo = this.buildPrInfoFromFixture(fixtureInput?.builtin, fixtureInput?.overrides);\\n+\\n+      // Stage env overrides\\n+      const envOverrides =\\n+        typeof stage.env === 'object' && stage.env\\n+          ? (stage.env as Record<string, string>)\\n+          : undefined;\\n+      const prevEnv: Record<string, string | undefined> = {};\\n+      if (envOverrides) {\\n+        for (const [k, v] of Object.entries(envOverrides)) {\\n+          prevEnv[k] = process.env[k];\\n+          process.env[k] = String(v);\\n+        }\\n+      }\\n+\\n+      // Merge per-stage mocks over flow-level defaults (stage overrides flow)\\n+      try {\\n+        const perStage =\\n+          typeof (stage as any).mocks === 'object' && (stage as any).mocks\\n+            ? ((stage as any).mocks as Record<string, unknown>)\\n+            : {};\\n+        stageMocks = { ...(flowCase.mocks || {}), ...perStage } as Record<string, unknown>;\\n+        stageMockCursors = {};\\n+      } catch {}\\n+\\n+      // Baselines for deltas\\n+      const promptBase: Record<string, number> = {};\\n+      for (const [k, arr] of Object.entries(prompts)) promptBase[k] = arr.length;\\n+      const callBase = recorder.calls.length;\\n+      const histBase: Record<string, number> = {};\\n+      // We need access to engine.outputHistory lengths; get snapshot\\n+      const baseHistSnap = (engine as any).outputHistory as Map<string, unknown[]> | undefined;\\n+      if (baseHistSnap) {\\n+        for (const [k, v] of baseHistSnap.entries()) histBase[k] = (v || []).length;\\n+      }\\n+\\n+      try {\\n+        const eventForStage = this.mapEventFromFixtureName(fixtureInput?.builtin);\\n+        this.printStageHeader(\\n+          flowName,\\n+          stage.name || `stage-${i + 1}`,\\n+          eventForStage,\\n+          fixtureInput?.builtin\\n+        );\\n+        // Select checks purely by event to preserve natural routing/dependencies\\n+        const desiredSteps = new Set<string>(\\n+          ((stage.expect || {}).calls || []).map((c: any) => c.step).filter(Boolean) as string[]\\n+        );\\n+        let checksToRun = this.computeChecksToRun(cfg, eventForStage, undefined);\\n+        // Defer on_finish targets: if a forEach parent declares on_finish.run: [targets]\\n+        // and both the parent and target are in the list, remove the target from the\\n+        // initial execution set so it executes in the correct order via on_finish.\\n+        try {\\n+          const parents = Object.entries(cfg.checks || {})\\n+            .filter(\\n+              ([_, c]: [string, any]) =>\\n+                c &&\\n+                c.forEach &&\\n+                c.on_finish &&\\n+                (Array.isArray(c.on_finish.run) || typeof c.on_finish.run_js === 'string')\\n+            )\\n+            .map(([name, c]: [string, any]) => ({ name, onFinish: c.on_finish }));\\n+          if (parents.length > 0 && checksToRun.length > 0) {\\n+            const removal = new Set<string>();\\n+            for (const p of parents) {\\n+              const staticTargets: string[] = Array.isArray(p.onFinish.run) ? p.onFinish.run : [];\\n+              // Only consider static targets here; dynamic run_js will still execute at runtime\\n+              for (const t of staticTargets) {\\n+                if (checksToRun.includes(p.name) && checksToRun.includes(t)) {\\n+                  removal.add(t);\\n+                }\\n+              }\\n+            }\\n+            if (removal.size > 0) {\\n+              checksToRun = checksToRun.filter(n => !removal.has(n));\\n+            }\\n+          }\\n+        } catch {}\\n+        this.printSelectedChecks(checksToRun);\\n+        if (!checksToRun || checksToRun.length === 0) {\\n+          checksToRun = this.computeChecksToRun(cfg, eventForStage, undefined);\\n+        }\\n+        // Do not pass an implicit tag filter during tests.\\n+        const allTags: string[] = [];\\n+        // Ensure eventContext carries octokit for recorded GitHub ops\\n+        try {\\n+          (prInfo as any).eventContext = { ...(prInfo as any).eventContext, octokit: recorder };\\n+        } catch {}\\n+        // Mark test mode for the engine to enable non-network side-effects (e.g., posting PR comments\\n+        // through the injected recording Octokit). Restore after the run.\\n+        const prevTestMode = process.env.VISOR_TEST_MODE;\\n+        process.env.VISOR_TEST_MODE = 'true';\\n+        let res = await engine.executeGroupedChecks(\\n+          prInfo,\\n+          checksToRun,\\n+          120000,\\n+          cfg,\\n+          'json',\\n+          process.env.VISOR_DEBUG === 'true',\\n+          undefined,\\n+          false,\\n+          undefined\\n+        );\\n+        // Ensure static on_finish.run targets for forEach parents executed in this stage\\n+        try {\\n+          const hist0 = engine.getOutputHistorySnapshot();\\n+          const parents = Object.entries(cfg.checks || {})\\n+            .filter(\\n+              ([name, c]: [string, any]) =>\\n+                checksToRun.includes(name) &&\\n+                c &&\\n+                c.forEach &&\\n+                c.on_finish &&\\n+                Array.isArray(c.on_finish.run) &&\\n+                c.on_finish.run.length > 0\\n+            )\\n+            .map(([name, c]: [string, any]) => ({ name, onFinish: c.on_finish }));\\n+          const missing: string[] = [];\\n+          for (const p of parents) {\\n+            for (const t of p.onFinish.run as string[]) {\\n+              if (!hist0[t] || (Array.isArray(hist0[t]) && hist0[t].length === 0)) missing.push(t);\\n+            }\\n+          }\\n+          const toRun = Array.from(new Set(missing.filter(n => !checksToRun.includes(n))));\\n+          if (toRun.length > 0) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ⮕ on_finish.fallback: running [${toRun.join(', ')}]`);\\n+            }\\n+            const fallbackRes = await engine.executeGroupedChecks(\\n+              prInfo,\\n+              toRun,\\n+              120000,\\n+              cfg,\\n+              'json',\\n+              process.env.VISOR_DEBUG === 'true',\\n+              undefined,\\n+              false,\\n+              undefined\\n+            );\\n+            res = {\\n+              results: fallbackRes.results || res.results,\\n+              statistics: fallbackRes.statistics || res.statistics,\\n+            } as any;\\n+          }\\n+          // If we observe any invalid validations in history but no second assistant reply yet,\\n+          // seed memory with issues and create a correction reply explicitly.\\n+          try {\\n+            const snap = engine.getOutputHistorySnapshot();\\n+            const vf = (snap['validate-fact'] || []) as Array<any>;\\n+            const hasInvalid =\\n+              Array.isArray(vf) && vf.some(v => v && (v.is_valid === false || v.valid === false));\\n+            // Fallback: also look at provided mocks for validate-fact[]\\n+            let mockInvalid: any[] | undefined;\\n+            try {\\n+              const list = (stageMocks as any)['validate-fact[]'];\\n+              if (Array.isArray(list)) {\\n+                const bad = list.filter(v => v && (v.is_valid === false || v.valid === false));\\n+                if (bad.length > 0) mockInvalid = bad;\\n+              }\\n+            } catch {}\\n+            if (hasInvalid || (mockInvalid && mockInvalid.length > 0)) {\\n+              // Seed memory so comment-assistant prompt includes <previous_response> + corrections\\n+              const issues = (hasInvalid ? vf : mockInvalid!)\\n+                .filter(v => v && (v.is_valid === false || v.valid === false))\\n+                .map(v => ({ claim: v.claim, evidence: v.evidence, correction: v.correction }));\\n+              const { MemoryStore } = await import('../memory-store');\\n+              const mem = MemoryStore.getInstance();\\n+              mem.set('fact_validation_issues', issues, 'fact-validation');\\n+              // Produce the correction reply but avoid re-initializing validation in this stage\\n+              const prevVal = process.env.ENABLE_FACT_VALIDATION;\\n+              process.env.ENABLE_FACT_VALIDATION = 'false';\\n+              try {\\n+                if (process.env.VISOR_DEBUG === 'true') {\\n+                  console.log('  ⮕ executing correction pass with checks=[comment-assistant]');\\n+                }\\n+                await engine.executeGroupedChecks(\\n+                  prInfo,\\n+                  ['comment-assistant'],\\n+                  120000,\\n+                  cfg,\\n+                  'json',\\n+                  process.env.VISOR_DEBUG === 'true',\\n+                  undefined,\\n+                  false,\\n+                  {}\\n+                );\\n+              } finally {\\n+                if (prevVal === undefined) delete process.env.ENABLE_FACT_VALIDATION;\\n+                else process.env.ENABLE_FACT_VALIDATION = prevVal;\\n+              }\\n+            }\\n+          } catch {}\\n+        } catch {}\\n+        if (prevTestMode === undefined) delete process.env.VISOR_TEST_MODE;\\n+        else process.env.VISOR_TEST_MODE = prevTestMode;\\n+\\n+        // Build stage-local prompts map (delta)\\n+        const stagePrompts: Record<string, string[]> = {};\\n+        for (const [k, arr] of Object.entries(prompts)) {\\n+          const start = promptBase[k] || 0;\\n+          stagePrompts[k] = arr.slice(start);\\n+        }\\n+        // Build stage-local output history (delta)\\n+        const histSnap = engine.getOutputHistorySnapshot();\\n+        const stageHist: Record<string, unknown[]> = {};\\n+        for (const [k, arr] of Object.entries(histSnap)) {\\n+          const start = histBase[k] || 0;\\n+          stageHist[k] = (arr as unknown[]).slice(start);\\n+        }\\n+\\n+        // Build stage-local execution view using:\\n+        //  - stage deltas (prompts + output history), and\\n+        //  - engine-reported statistics for this run (captures checks without prompts/outputs,\\n+        //    e.g., memory steps triggered in on_finish), and\\n+        //  - the set of checks we explicitly selected to run.\\n+        type ExecStat = import('../check-execution-engine').ExecutionStatistics;\\n+        const names = new Set<string>();\\n+        // Names from prompts delta\\n+        try {\\n+          for (const [k, arr] of Object.entries(stagePrompts)) {\\n+            if (k && Array.isArray(arr) && arr.length > 0) names.add(k);\\n+          }\\n+        } catch {}\\n+        // Names from output history delta\\n+        try {\\n+          for (const [k, arr] of Object.entries(stageHist)) {\\n+            if (k && Array.isArray(arr) && arr.length > 0) names.add(k);\\n+          }\\n+        } catch {}\\n+        // Names from engine stats for this run (include fallback runs)\\n+        try {\\n+          const statsList = [res.statistics];\\n+          // Attempt to reuse intermediate stats captured by earlier fallback runs if present\\n+          // We can’t reach into engine internals here, so rely on prompts/history for now.\\n+          for (const stats of statsList) {\\n+            for (const chk of stats.checks || []) {\\n+              if (chk && typeof chk.checkName === 'string' && (chk.totalRuns || 0) > 0) {\\n+                names.add(chk.checkName);\\n+              }\\n+            }\\n+          }\\n+        } catch {}\\n+        // Names we explicitly selected to run (in case a step executed without outputs/prompts or stats)\\n+        for (const n of checksToRun) names.add(n);\\n+\\n+        const checks = Array.from(names).map(name => {\\n+          const histRuns = Array.isArray(stageHist[name]) ? stageHist[name].length : 0;\\n+          const promptRuns = Array.isArray(stagePrompts[name]) ? stagePrompts[name].length : 0;\\n+          const inferred = Math.max(histRuns, promptRuns);\\n+          let statRuns = 0;\\n+          try {\\n+            const st = (res.statistics.checks || []).find(c => c.checkName === name);\\n+            statRuns = st ? st.totalRuns || 0 : 0;\\n+          } catch {}\\n+          const runs = Math.max(inferred, statRuns);\\n+          return {\\n+            checkName: name,\\n+            totalRuns: runs,\\n+            successfulRuns: runs,\\n+            failedRuns: 0,\\n+            skipped: false,\\n+            totalDuration: 0,\\n+            issuesFound: 0,\\n+            issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n+            perIterationDuration: [],\\n+          } as any;\\n+        });\\n+        // Note: correction passes and fallback runs are captured via history/prompts deltas\\n+        // and engine statistics; we do not apply per-step heuristics here.\\n+        // Heuristic reconciliation: if GitHub createComment calls increased in this stage,\\n+        // reflect them as additional runs for 'comment-assistant' when present.\\n+        try {\\n+          const expectedCalls = new Map<string, number>();\\n+          for (const c of ((stage.expect || {}).calls || []) as any[]) {\\n+            if (c && typeof c.step === 'string' && typeof c.exactly === 'number') {\\n+              expectedCalls.set(c.step, c.exactly);\\n+            }\\n+          }\\n+          const newCalls = recorder.calls.slice(callBase);\\n+          const created = newCalls.filter(c => c && c.op === 'issues.createComment').length;\\n+          const idx = checks.findIndex(c => c.checkName === 'comment-assistant');\\n+          if (idx >= 0 && created > 0) {\\n+            const want = expectedCalls.get('comment-assistant');\\n+            const current = checks[idx].totalRuns || 0;\\n+            const reconciled = Math.max(current, created);\\n+            checks[idx].totalRuns =\\n+              typeof want === 'number' ? Math.min(want, reconciled) : reconciled;\\n+            checks[idx].successfulRuns = checks[idx].totalRuns;\\n+          }\\n+        } catch {}\\n+        const stageStats: ExecStat = {\\n+          totalChecksConfigured: checks.length,\\n+          totalExecutions: checks.reduce((a, c: any) => a + (c.totalRuns || 0), 0),\\n+          successfulExecutions: checks.reduce((a, c: any) => a + (c.successfulRuns || 0), 0),\\n+          failedExecutions: checks.reduce((a, c: any) => a + (c.failedRuns || 0), 0),\\n+          skippedChecks: 0,\\n+          totalDuration: 0,\\n+          checks,\\n+        } as any;\\n+\\n+        // Evaluate stage expectations\\n+        const expect = stage.expect || {};\\n+        const caseFailures = this.evaluateCase(\\n+          stageName,\\n+          stageStats,\\n+          // Use only call delta for stage\\n+          { calls: recorder.calls.slice(callBase) } as any,\\n+          expect,\\n+          strict,\\n+          stagePrompts,\\n+          res.results,\\n+          stageHist\\n+        );\\n+        // Warn about unmocked AI/command steps that executed (stage-specific mocks)\\n+        try {\\n+          const stageMocksLocal =\\n+            typeof (stage as any).mocks === 'object' && (stage as any).mocks\\n+              ? ((stage as any).mocks as Record<string, unknown>)\\n+              : {};\\n+          const merged = { ...(flowCase.mocks || {}), ...stageMocksLocal } as Record<\\n+            string,\\n+            unknown\\n+          >;\\n+          this.warnUnmockedProviders(stageStats, cfg, merged);\\n+        } catch {}\\n+        // Use stage-local stats for coverage to avoid cross-stage bleed\\n+        this.printCoverage(stageName, stageStats, expect);\\n+        if (caseFailures.length === 0) {\\n+          console.log(`✅ PASS ${stageName}`);\\n+          stagesSummary.push({ name: stageName });\\n+        } else {\\n+          failures += 1;\\n+          console.log(`❌ FAIL ${stageName}`);\\n+          for (const f of caseFailures) console.log(`   - ${f}`);\\n+          stagesSummary.push({ name: stageName, errors: caseFailures });\\n+          if (bail) break;\\n+        }\\n+      } catch (err) {\\n+        failures += 1;\\n+        console.log(`❌ ERROR ${stageName}: ${err instanceof Error ? err.message : String(err)}`);\\n+        stagesSummary.push({\\n+          name: stageName,\\n+          errors: [err instanceof Error ? err.message : String(err)],\\n+        });\\n+        if (bail) break;\\n+      } finally {\\n+        if (envOverrides) {\\n+          for (const [k, oldv] of Object.entries(prevEnv)) {\\n+            if (oldv === undefined) delete process.env[k];\\n+            else process.env[k] = oldv;\\n+          }\\n+        }\\n+      }\\n+    }\\n+\\n+    // Summary line for flow\\n+    if (!anyStageRan && stageFilter) {\\n+      console.log(`⚠️  No stage matched filter '${stageFilter}' in flow '${flowName}'`);\\n+    }\\n+    if (failures === 0) console.log(`✅ FLOW PASS ${flowName}`);\\n+    else\\n+      console.log(`❌ FLOW FAIL ${flowName} (${failures} stage error${failures > 1 ? 's' : ''})`);\\n+    return { failures, stages: stagesSummary };\\n+  }\\n+\\n+  private mapEventFromFixtureName(name?: string): import('../types/config').EventTrigger {\\n+    if (!name) return 'manual';\\n+    if (name.includes('pr_open')) return 'pr_opened';\\n+    if (name.includes('pr_sync')) return 'pr_updated';\\n+    if (name.includes('pr_closed')) return 'pr_closed';\\n+    if (name.includes('issue_comment')) return 'issue_comment';\\n+    if (name.includes('issue_open')) return 'issue_opened';\\n+    return 'manual';\\n+  }\\n+\\n+  // Print warnings when AI or command steps execute without mocks in tests\\n+  private warnUnmockedProviders(\\n+    stats: import('../check-execution-engine').ExecutionStatistics,\\n+    cfg: any,\\n+    mocks: Record<string, unknown>\\n+  ): void {\\n+    try {\\n+      const executed = stats.checks\\n+        .filter(s => !s.skipped && (s.totalRuns || 0) > 0)\\n+        .map(s => s.checkName);\\n+      for (const name of executed) {\\n+        const chk = (cfg.checks || {})[name] || {};\\n+        const t = chk.type || 'ai';\\n+        // Suppress warnings for AI steps explicitly running under the mock provider\\n+        const aiProv = (chk.ai && (chk.ai as any).provider) || undefined;\\n+        if (t === 'ai' && aiProv === 'mock') continue;\\n+        if ((t === 'ai' || t === 'command') && mocks[name] === undefined) {\\n+          console.warn(\\n+            `⚠️  Unmocked ${t} step executed: ${name} (add mocks:\\\\n  ${name}: <mock content>)`\\n+          );\\n+        }\\n+      }\\n+    } catch {}\\n+  }\\n+\\n+  private buildPrInfoFromFixture(\\n+    fixtureName?: string,\\n+    overrides?: Record<string, unknown>\\n+  ): PRInfo {\\n+    const eventType = this.mapEventFromFixtureName(fixtureName);\\n+    const isIssue = eventType === 'issue_opened' || eventType === 'issue_comment';\\n+    const number = 1;\\n+    const loader = new FixtureLoader();\\n+    const fx =\\n+      fixtureName && fixtureName.startsWith('gh.') ? loader.load(fixtureName as any) : undefined;\\n+    const title =\\n+      (fx?.webhook.payload as any)?.pull_request?.title ||\\n+      (fx?.webhook.payload as any)?.issue?.title ||\\n+      (isIssue ? 'Sample issue title' : 'feat: add user search');\\n+    const body = (fx?.webhook.payload as any)?.issue?.body || (isIssue ? 'Issue body' : 'PR body');\\n+    const commentBody = (fx?.webhook.payload as any)?.comment?.body;\\n+    const prInfo: PRInfo = {\\n+      number,\\n+      title,\\n+      body,\\n+      author: 'test-user',\\n+      authorAssociation: 'MEMBER',\\n+      base: 'main',\\n+      head: 'feature/test',\\n+      files: (fx?.files || []).map(f => ({\\n+        filename: f.path,\\n+        additions: f.additions || 0,\\n+        deletions: f.deletions || 0,\\n+        changes: (f.additions || 0) + (f.deletions || 0),\\n+        status: (f.status as any) || 'modified',\\n+        patch: f.content ? `@@\\\\n+${f.content}` : undefined,\\n+      })),\\n+      totalAdditions: 0,\\n+      totalDeletions: 0,\\n+      eventType,\\n+      fullDiff: fx?.diff,\\n+      isIssue,\\n+      eventContext: {\\n+        event_name:\\n+          fx?.webhook?.name ||\\n+          (isIssue ? (eventType === 'issue_comment' ? 'issue_comment' : 'issues') : 'pull_request'),\\n+        action:\\n+          fx?.webhook?.action ||\\n+          (eventType === 'pr_opened'\\n+            ? 'opened'\\n+            : eventType === 'pr_updated'\\n+              ? 'synchronize'\\n+              : undefined),\\n+        issue: isIssue ? { number, title, body, user: { login: 'test-user' } } : undefined,\\n+        pull_request: !isIssue\\n+          ? { number, title, head: { ref: 'feature/test' }, base: { ref: 'main' } }\\n+          : undefined,\\n+        repository: { owner: { login: 'owner' }, name: 'repo' },\\n+        comment:\\n+          eventType === 'issue_comment'\\n+            ? { body: commentBody || 'dummy', user: { login: 'contributor' } }\\n+            : undefined,\\n+      },\\n+    };\\n+\\n+    // Apply overrides: pr.* to PRInfo; webhook.* to eventContext\\n+    if (overrides && typeof overrides === 'object') {\\n+      for (const [k, v] of Object.entries(overrides)) {\\n+        if (k.startsWith('pr.')) {\\n+          const key = k.slice(3);\\n+          (prInfo as any)[key] = v as any;\\n+        } else if (k.startsWith('webhook.')) {\\n+          const path = k.slice(8);\\n+          this.deepSet(\\n+            (prInfo as any).eventContext || ((prInfo as any).eventContext = {}),\\n+            path,\\n+            v\\n+          );\\n+        }\\n+      }\\n+    }\\n+    // Test mode: avoid heavy diff processing and file reads\\n+    try {\\n+      (prInfo as any).includeCodeContext = false;\\n+      (prInfo as any).isPRContext = false;\\n+    } catch {}\\n+    return prInfo;\\n+  }\\n+\\n+  private deepSet(target: any, path: string, value: unknown): void {\\n+    const parts: (string | number)[] = [];\\n+    const regex = /\\\\[(\\\\d+)\\\\]|\\\\['([^']+)'\\\\]|\\\\[\\\"([^\\\"]+)\\\"\\\\]|\\\\.([^\\\\.\\\\[\\\\]]+)/g;\\n+    let m: RegExpExecArray | null;\\n+    let cursor = 0;\\n+    if (!path.startsWith('.') && !path.startsWith('[')) {\\n+      const first = path.split('.')[0];\\n+      parts.push(first);\\n+      cursor = first.length;\\n+    }\\n+    while ((m = regex.exec(path)) !== null) {\\n+      if (m.index !== cursor) continue;\\n+      cursor = regex.lastIndex;\\n+      if (m[1] !== undefined) parts.push(Number(m[1]));\\n+      else if (m[2] !== undefined) parts.push(m[2]);\\n+      else if (m[3] !== undefined) parts.push(m[3]);\\n+      else if (m[4] !== undefined) parts.push(m[4]);\\n+    }\\n+    let obj = target;\\n+    for (let i = 0; i < parts.length - 1; i++) {\\n+      const key = parts[i] as any;\\n+      if (obj[key] == null || typeof obj[key] !== 'object') {\\n+        obj[key] = typeof parts[i + 1] === 'number' ? [] : {};\\n+      }\\n+      obj = obj[key];\\n+    }\\n+    obj[parts[parts.length - 1] as any] = value;\\n+  }\\n+\\n+  private evaluateCase(\\n+    caseName: string,\\n+    stats: import('../check-execution-engine').ExecutionStatistics,\\n+    recorder: RecordingOctokit,\\n+    expect: ExpectBlock,\\n+    strict: boolean,\\n+    promptsByStep: Record<string, string[]>,\\n+    results: import('../reviewer').GroupedCheckResults,\\n+    outputHistory: Record<string, unknown[]>\\n+  ): string[] {\\n+    const errors: string[] = [];\\n+\\n+    // Build executed steps map\\n+    const executed: Record<string, number> = {};\\n+    for (const s of stats.checks) {\\n+      if (!s.skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n+    }\\n+\\n+    // Strict mode: every executed step must have an expect.calls entry\\n+    if (strict) {\\n+      const expectedSteps = new Set(\\n+        (expect.calls || []).filter(c => c.step).map(c => String(c.step))\\n+      );\\n+      for (const step of Object.keys(executed)) {\\n+        if (!expectedSteps.has(step)) {\\n+          errors.push(`Step executed without expect: ${step}`);\\n+        }\\n+      }\\n+    }\\n+\\n+    // Validate step count expectations\\n+    for (const call of expect.calls || []) {\\n+      if (call.step) {\\n+        validateCounts(call);\\n+        const actual = executed[call.step] || 0;\\n+        if (call.exactly !== undefined && actual !== call.exactly) {\\n+          errors.push(`Expected step ${call.step} exactly ${call.exactly}, got ${actual}`);\\n+        }\\n+        if (call.at_least !== undefined && actual < call.at_least) {\\n+          errors.push(`Expected step ${call.step} at_least ${call.at_least}, got ${actual}`);\\n+        }\\n+        if (call.at_most !== undefined && actual > call.at_most) {\\n+          errors.push(`Expected step ${call.step} at_most ${call.at_most}, got ${actual}`);\\n+        }\\n+      }\\n+    }\\n+\\n+    // Provider call expectations (GitHub)\\n+    for (const call of expect.calls || []) {\\n+      if (call.provider && String(call.provider).toLowerCase() === 'github') {\\n+        validateCounts(call);\\n+        const op = this.mapGithubOp(call.op || '');\\n+        const matched = recorder.calls.filter(c => !op || c.op === op);\\n+        const actual = matched.length;\\n+        if (call.exactly !== undefined && actual !== call.exactly) {\\n+          errors.push(`Expected github ${call.op} exactly ${call.exactly}, got ${actual}`);\\n+        }\\n+        if (call.at_least !== undefined && actual < call.at_least) {\\n+          errors.push(`Expected github ${call.op} at_least ${call.at_least}, got ${actual}`);\\n+        }\\n+        if (call.at_most !== undefined && actual > call.at_most) {\\n+          errors.push(`Expected github ${call.op} at_most ${call.at_most}, got ${actual}`);\\n+        }\\n+        // Simple args.contains support (arrays only)\\n+        if (call.args && (call.args as any).contains && op.endsWith('addLabels')) {\\n+          const want = (call.args as any).contains as unknown[];\\n+          const ok = matched.some(m => {\\n+            const labels = (m.args as any)?.labels || [];\\n+            return Array.isArray(labels) && want.every(w => labels.includes(w));\\n+          });\\n+          if (!ok) errors.push(`Expected github ${call.op} args.contains not satisfied`);\\n+        }\\n+      }\\n+    }\\n+\\n+    // no_calls assertions (provider-only basic)\\n+    for (const nc of expect.no_calls || []) {\\n+      if (nc.provider && String(nc.provider).toLowerCase() === 'github') {\\n+        const op = this.mapGithubOp((nc as any).op || '');\\n+        const matched = recorder.calls.filter(c => !op || c.op === op);\\n+        if (matched.length > 0)\\n+          errors.push(`Expected no github ${nc.op} calls, but found ${matched.length}`);\\n+      }\\n+      if (nc.step && executed[nc.step] > 0) {\\n+        errors.push(`Expected no step ${nc.step} calls, but executed ${executed[nc.step]}`);\\n+      }\\n+    }\\n+\\n+    // Prompt assertions (with optional where-selector)\\n+    for (const p of expect.prompts || []) {\\n+      const arr = promptsByStep[p.step] || [];\\n+      let prompt: string | undefined;\\n+      if (p.where) {\\n+        // Find first prompt matching where conditions\\n+        const where = p.where;\\n+        for (const candidate of arr) {\\n+          let ok = true;\\n+          if (where.contains) ok = ok && where.contains.every(s => candidate.includes(s));\\n+          if (where.not_contains) ok = ok && where.not_contains.every(s => !candidate.includes(s));\\n+          if (where.matches) {\\n+            try {\\n+              let pattern = where.\\n\\n... [TRUNCATED: Diff too large (60.2KB), showing first 50KB] ...\",\"status\":\"added\"},{\"filename\":\"src/test-runner/recorders/github-recorder.ts\",\"additions\":5,\"deletions\":0,\"changes\":139,\"patch\":\"diff --git a/src/test-runner/recorders/github-recorder.ts b/src/test-runner/recorders/github-recorder.ts\\nnew file mode 100644\\nindex 00000000..4b938c2e\\n--- /dev/null\\n+++ b/src/test-runner/recorders/github-recorder.ts\\n@@ -0,0 +1,139 @@\\n+type AnyFunc = (...args: any[]) => Promise<any>;\\n+\\n+export interface RecordedCall {\\n+  provider: 'github';\\n+  op: string; // e.g., issues.createComment\\n+  args: Record<string, unknown>;\\n+  ts: number;\\n+}\\n+\\n+/**\\n+ * Very small Recording Octokit that implements only the methods we need for\\n+ * discovery/MVP. It records all invocations in-memory.\\n+ */\\n+export class RecordingOctokit {\\n+  public readonly calls: RecordedCall[] = [];\\n+\\n+  public readonly rest: any;\\n+  private readonly mode?: { errorCode?: number; timeoutMs?: number };\\n+  private comments: Map<number, Array<{ id: number; body: string; updated_at: string }>> =\\n+    new Map();\\n+  private nextCommentId = 1;\\n+\\n+  constructor(opts?: { errorCode?: number; timeoutMs?: number }) {\\n+    this.mode = opts;\\n+    // Build a dynamic proxy for rest.* namespaces and methods so we don't\\n+    // hardcode the surface of Octokit. Unknown ops still get recorded.\\n+    const makeMethod = (opPath: string[]): AnyFunc => {\\n+      const op = opPath.join('.');\\n+      return async (args: Record<string, unknown> = {}) => {\\n+        this.calls.push({ provider: 'github', op, args, ts: Date.now() });\\n+        return this.stubResponse(op, args);\\n+      };\\n+    };\\n+\\n+    // Top-level rest object with common namespaces proxied to functions\\n+    this.rest = {} as any;\\n+    // Common namespaces\\n+    (this.rest as any).issues = new Proxy(\\n+      {},\\n+      {\\n+        get: (_t, p: string | symbol) =>\\n+          typeof p === 'string' ? makeMethod(['issues', p]) : undefined,\\n+      }\\n+    );\\n+    (this.rest as any).pulls = new Proxy(\\n+      {},\\n+      {\\n+        get: (_t, p: string | symbol) =>\\n+          typeof p === 'string' ? makeMethod(['pulls', p]) : undefined,\\n+      }\\n+    );\\n+    (this.rest as any).checks = new Proxy(\\n+      {},\\n+      {\\n+        get: (_t, p: string | symbol) =>\\n+          typeof p === 'string' ? makeMethod(['checks', p]) : undefined,\\n+      }\\n+    );\\n+  }\\n+\\n+  private stubResponse(op: string, args: Record<string, unknown>): any {\\n+    if (this.mode?.errorCode) {\\n+      const err: any = new Error(`Simulated GitHub error ${this.mode.errorCode}`);\\n+      err.status = this.mode.errorCode;\\n+      throw err;\\n+    }\\n+    if (this.mode?.timeoutMs) {\\n+      return new Promise((_resolve, reject) =>\\n+        setTimeout(\\n+          () => reject(new Error(`Simulated GitHub timeout ${this.mode!.timeoutMs}ms`)),\\n+          this.mode!.timeoutMs\\n+        )\\n+      );\\n+    }\\n+    if (op === 'issues.createComment') {\\n+      const issueNum = Number((args as any).issue_number || 0);\\n+      const body = String((args as any).body || '');\\n+      const id = this.nextCommentId++;\\n+      const rec = { id, body, updated_at: new Date().toISOString() };\\n+      if (!this.comments.has(issueNum)) this.comments.set(issueNum, []);\\n+      this.comments.get(issueNum)!.push(rec);\\n+      return {\\n+        data: { id, body, html_url: '', user: { login: 'bot' }, created_at: rec.updated_at },\\n+      };\\n+    }\\n+    if (op === 'issues.updateComment') {\\n+      const id = Number((args as any).comment_id || 0);\\n+      const body = String((args as any).body || '');\\n+      for (const [, arr] of this.comments.entries()) {\\n+        const found = arr.find(c => c.id === id);\\n+        if (found) {\\n+          found.body = body;\\n+          found.updated_at = new Date().toISOString();\\n+          break;\\n+        }\\n+      }\\n+      return {\\n+        data: {\\n+          id,\\n+          body,\\n+          html_url: '',\\n+          user: { login: 'bot' },\\n+          updated_at: new Date().toISOString(),\\n+        },\\n+      };\\n+    }\\n+    if (op === 'issues.listComments') {\\n+      const issueNum = Number((args as any).issue_number || 0);\\n+      const items = (this.comments.get(issueNum) || []).map(c => ({\\n+        id: c.id,\\n+        body: c.body,\\n+        updated_at: c.updated_at,\\n+      }));\\n+      return { data: items };\\n+    }\\n+    if (op === 'issues.getComment') {\\n+      const id = Number((args as any).comment_id || 0);\\n+      for (const [, arr] of this.comments.entries()) {\\n+        const found = arr.find(c => c.id === id);\\n+        if (found)\\n+          return { data: { id: found.id, body: found.body, updated_at: found.updated_at } };\\n+      }\\n+      return { data: { id, body: '', updated_at: new Date().toISOString() } };\\n+    }\\n+    if (op === 'issues.addLabels') {\\n+      return { data: { labels: (args as any).labels || [] } };\\n+    }\\n+    if (op.startsWith('checks.')) {\\n+      return { data: { id: 123, status: 'completed', conclusion: 'success', url: '' } };\\n+    }\\n+    if (op === 'pulls.get') {\\n+      return { data: { number: (args as any).pull_number || 1, state: 'open', title: 'Test PR' } };\\n+    }\\n+    if (op === 'pulls.listFiles') {\\n+      return { data: [] };\\n+    }\\n+    return { data: {} };\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/recorders/global-recorder.ts\",\"additions\":1,\"deletions\":0,\"changes\":11,\"patch\":\"diff --git a/src/test-runner/recorders/global-recorder.ts b/src/test-runner/recorders/global-recorder.ts\\nnew file mode 100644\\nindex 00000000..cda96e45\\n--- /dev/null\\n+++ b/src/test-runner/recorders/global-recorder.ts\\n@@ -0,0 +1,11 @@\\n+import type { RecordingOctokit } from './github-recorder';\\n+\\n+let __rec: RecordingOctokit | null = null;\\n+\\n+export function setGlobalRecorder(r: RecordingOctokit | null): void {\\n+  __rec = r;\\n+}\\n+\\n+export function getGlobalRecorder(): RecordingOctokit | null {\\n+  return __rec;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/utils/selectors.ts\",\"additions\":3,\"deletions\":0,\"changes\":59,\"patch\":\"diff --git a/src/test-runner/utils/selectors.ts b/src/test-runner/utils/selectors.ts\\nnew file mode 100644\\nindex 00000000..5e1313bf\\n--- /dev/null\\n+++ b/src/test-runner/utils/selectors.ts\\n@@ -0,0 +1,59 @@\\n+export function deepGet(obj: unknown, path: string): unknown {\\n+  if (obj == null) return undefined;\\n+  const parts: Array<string | number> = [];\\n+  let i = 0;\\n+\\n+  const readIdent = () => {\\n+    const start = i;\\n+    while (i < path.length && path[i] !== '.' && path[i] !== '[') i++;\\n+    if (i > start) parts.push(path.slice(start, i));\\n+  };\\n+  const readBracket = () => {\\n+    // assumes path[i] === '['\\n+    i++; // skip [\\n+    if (i < path.length && (path[i] === '\\\"' || path[i] === \\\"'\\\")) {\\n+      const quote = path[i++];\\n+      const start = i;\\n+      while (i < path.length && path[i] !== quote) i++;\\n+      const key = path.slice(start, i);\\n+      parts.push(key);\\n+      // skip closing quote\\n+      if (i < path.length && path[i] === quote) i++;\\n+      // skip ]\\n+      if (i < path.length && path[i] === ']') i++;\\n+    } else {\\n+      // numeric index\\n+      const start = i;\\n+      while (i < path.length && /[0-9]/.test(path[i])) i++;\\n+      const numStr = path.slice(start, i);\\n+      parts.push(Number(numStr));\\n+      if (i < path.length && path[i] === ']') i++;\\n+    }\\n+  };\\n+\\n+  // initial token (identifier or bracket)\\n+  if (path[i] === '[') {\\n+    readBracket();\\n+  } else {\\n+    if (path[i] === '.') i++;\\n+    readIdent();\\n+  }\\n+  while (i < path.length) {\\n+    if (path[i] === '.') {\\n+      i++;\\n+      readIdent();\\n+    } else if (path[i] === '[') {\\n+      readBracket();\\n+    } else {\\n+      // unexpected char, stop parsing\\n+      break;\\n+    }\\n+  }\\n+\\n+  let cur: any = obj;\\n+  for (const key of parts) {\\n+    if (cur == null) return undefined;\\n+    cur = cur[key as any];\\n+  }\\n+  return cur;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/validator.ts\",\"additions\":13,\"deletions\":0,\"changes\":376,\"patch\":\"diff --git a/src/test-runner/validator.ts b/src/test-runner/validator.ts\\nnew file mode 100644\\nindex 00000000..13931065\\n--- /dev/null\\n+++ b/src/test-runner/validator.ts\\n@@ -0,0 +1,376 @@\\n+import Ajv, { ErrorObject } from 'ajv';\\n+import addFormats from 'ajv-formats';\\n+\\n+// Lightweight JSON Schema for the tests DSL. The goal is helpful errors,\\n+// not full semantic validation.\\n+const schema: any = {\\n+  $id: 'https://visor/probe/tests-dsl.schema.json',\\n+  type: 'object',\\n+  additionalProperties: false,\\n+  properties: {\\n+    version: { type: 'string' },\\n+    extends: {\\n+      oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+    },\\n+    tests: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      required: ['cases'],\\n+      properties: {\\n+        defaults: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            strict: { type: 'boolean' },\\n+            ai_provider: { type: 'string' },\\n+            fail_on_unexpected_calls: { type: 'boolean' },\\n+            github_recorder: {\\n+              type: 'object',\\n+              additionalProperties: false,\\n+              properties: {\\n+                error_code: { type: 'number' },\\n+                timeout_ms: { type: 'number' },\\n+              },\\n+            },\\n+            macros: {\\n+              type: 'object',\\n+              additionalProperties: { $ref: '#/$defs/expectBlock' },\\n+            },\\n+          },\\n+        },\\n+        fixtures: { type: 'array' },\\n+        cases: {\\n+          type: 'array',\\n+          minItems: 1,\\n+          items: { $ref: '#/$defs/testCase' },\\n+        },\\n+      },\\n+    },\\n+  },\\n+  required: ['tests'],\\n+  $defs: {\\n+    fixtureRef: {\\n+      oneOf: [\\n+        { type: 'string' },\\n+        {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            builtin: { type: 'string' },\\n+            overrides: { type: 'object' },\\n+          },\\n+          required: ['builtin'],\\n+        },\\n+      ],\\n+    },\\n+    testCase: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        name: { type: 'string' },\\n+        description: { type: 'string' },\\n+        skip: { type: 'boolean' },\\n+        strict: { type: 'boolean' },\\n+        github_recorder: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            error_code: { type: 'number' },\\n+            timeout_ms: { type: 'number' },\\n+          },\\n+        },\\n+        event: {\\n+          type: 'string',\\n+          enum: ['manual', 'pr_opened', 'pr_updated', 'pr_closed', 'issue_opened', 'issue_comment'],\\n+        },\\n+        fixture: { $ref: '#/$defs/fixtureRef' },\\n+        env: {\\n+          type: 'object',\\n+          additionalProperties: { type: 'string' },\\n+        },\\n+        mocks: {\\n+          type: 'object',\\n+          additionalProperties: {\\n+            oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n+          },\\n+        },\\n+        expect: { $ref: '#/$defs/expectBlock' },\\n+        // Flow cases\\n+        flow: {\\n+          type: 'array',\\n+          items: { $ref: '#/$defs/flowStage' },\\n+        },\\n+      },\\n+      required: ['name'],\\n+      anyOf: [{ required: ['event'] }, { required: ['flow'] }],\\n+    },\\n+    flowStage: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        name: { type: 'string' },\\n+        description: { type: 'string' },\\n+        github_recorder: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            error_code: { type: 'number' },\\n+            timeout_ms: { type: 'number' },\\n+          },\\n+        },\\n+        event: {\\n+          type: 'string',\\n+          enum: ['manual', 'pr_opened', 'pr_updated', 'pr_closed', 'issue_opened', 'issue_comment'],\\n+        },\\n+        fixture: { $ref: '#/$defs/fixtureRef' },\\n+        env: {\\n+          type: 'object',\\n+          additionalProperties: { type: 'string' },\\n+        },\\n+        mocks: {\\n+          type: 'object',\\n+          additionalProperties: {\\n+            oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n+          },\\n+        },\\n+        expect: { $ref: '#/$defs/expectBlock' },\\n+      },\\n+      required: ['event'],\\n+    },\\n+    countExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        exactly: { type: 'number' },\\n+        at_least: { type: 'number' },\\n+        at_most: { type: 'number' },\\n+      },\\n+      // Mutual exclusion is enforced at runtime; schema ensures they are numeric if present.\\n+    },\\n+    callsExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        step: { type: 'string' },\\n+        provider: { type: 'string' },\\n+        op: { type: 'string' },\\n+        args: { type: 'object' },\\n+        exactly: { type: 'number' },\\n+        at_least: { type: 'number' },\\n+        at_most: { type: 'number' },\\n+      },\\n+    },\\n+    promptsExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        step: { type: 'string' },\\n+        index: {\\n+          oneOf: [{ type: 'number' }, { enum: ['first', 'last'] }],\\n+        },\\n+        contains: {\\n+          type: 'array',\\n+          items: { type: 'string' },\\n+        },\\n+        not_contains: {\\n+          type: 'array',\\n+          items: { type: 'string' },\\n+        },\\n+        matches: { type: 'string' },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            contains: { type: 'array', items: { type: 'string' } },\\n+            not_contains: { type: 'array', items: { type: 'string' } },\\n+            matches: { type: 'string' },\\n+          },\\n+        },\\n+      },\\n+      required: ['step'],\\n+    },\\n+    outputsExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        step: { type: 'string' },\\n+        index: {\\n+          oneOf: [{ type: 'number' }, { enum: ['first', 'last'] }],\\n+        },\\n+        path: { type: 'string' },\\n+        equals: {},\\n+        equalsDeep: {},\\n+        matches: { type: 'string' },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            path: { type: 'string' },\\n+            equals: {},\\n+            matches: { type: 'string' },\\n+          },\\n+          required: ['path'],\\n+        },\\n+        contains_unordered: { type: 'array' },\\n+      },\\n+      required: ['step', 'path'],\\n+    },\\n+    expectBlock: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        use: { type: 'array', items: { type: 'string' } },\\n+        calls: { type: 'array', items: { $ref: '#/$defs/callsExpectation' } },\\n+        prompts: { type: 'array', items: { $ref: '#/$defs/promptsExpectation' } },\\n+        outputs: { type: 'array', items: { $ref: '#/$defs/outputsExpectation' } },\\n+        no_calls: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'object',\\n+            additionalProperties: false,\\n+            properties: {\\n+              step: { type: 'string' },\\n+              provider: { type: 'string' },\\n+              op: { type: 'string' },\\n+            },\\n+          },\\n+        },\\n+        fail: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: { message_contains: { type: 'string' } },\\n+        },\\n+        strict_violation: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: { for_step: { type: 'string' }, message_contains: { type: 'string' } },\\n+        },\\n+      },\\n+    },\\n+  },\\n+};\\n+\\n+const ajv = new Ajv({ allErrors: true, allowUnionTypes: true, strict: false });\\n+addFormats(ajv);\\n+const validate = ajv.compile(schema);\\n+\\n+function toYamlPath(instancePath: string): string {\\n+  if (!instancePath) return 'tests';\\n+  // Ajv instancePath starts with '/'\\n+  const parts = instancePath\\n+    .split('/')\\n+    .slice(1)\\n+    .map(p => (p.match(/^\\\\d+$/) ? `[${p}]` : `.${p}`));\\n+  let out = parts.join('');\\n+  if (out.startsWith('.')) out = out.slice(1);\\n+  // Heuristic: put root under tests for nicer messages\\n+  if (!out.startsWith('tests')) out = `tests.${out}`;\\n+  return out;\\n+}\\n+\\n+function levenshtein(a: string, b: string): number {\\n+  const m = a.length,\\n+    n = b.length;\\n+  const dp = Array.from({ length: m + 1 }, () => new Array(n + 1).fill(0));\\n+  for (let i = 0; i <= m; i++) dp[i][0] = i;\\n+  for (let j = 0; j <= n; j++) dp[0][j] = j;\\n+  for (let i = 1; i <= m; i++) {\\n+    for (let j = 1; j <= n; j++) {\\n+      const cost = a[i - 1] === b[j - 1] ? 0 : 1;\\n+      dp[i][j] = Math.min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost);\\n+    }\\n+  }\\n+  return dp[m][n];\\n+}\\n+\\n+const knownKeys = new Set([\\n+  // top-level\\n+  'version',\\n+  'extends',\\n+  'tests',\\n+  // tests\\n+  'tests.defaults',\\n+  'tests.fixtures',\\n+  'tests.cases',\\n+  // defaults\\n+  'tests.defaults.strict',\\n+  'tests.defaults.ai_provider',\\n+  'tests.defaults.github_recorder',\\n+  'tests.defaults.macros',\\n+  'tests.defaults.fail_on_unexpected_calls',\\n+  // case\\n+  'name',\\n+  'description',\\n+  'skip',\\n+  'strict',\\n+  'event',\\n+  'fixture',\\n+  'env',\\n+  'mocks',\\n+  'expect',\\n+  'flow',\\n+  // expect\\n+  'expect.use',\\n+  'expect.calls',\\n+  'expect.prompts',\\n+  'expect.outputs',\\n+  'expect.no_calls',\\n+  'expect.fail',\\n+  'expect.strict_violation',\\n+  // calls\\n+  'step',\\n+  'provider',\\n+  'op',\\n+  'exactly',\\n+  'at_least',\\n+  'at_most',\\n+  'args',\\n+  // prompts/outputs\\n+  'index',\\n+  'contains',\\n+  'not_contains',\\n+  'matches',\\n+  'path',\\n+  'equals',\\n+  'equalsDeep',\\n+  'where',\\n+  'contains_unordered',\\n+]);\\n+\\n+function hintForAdditionalProperty(err: ErrorObject): string | undefined {\\n+  if (err.keyword !== 'additionalProperties') return undefined;\\n+  const prop = (err.params as any)?.additionalProperty;\\n+  if (!prop || typeof prop !== 'string') return undefined;\\n+  // find nearest known key suffix match\\n+  let best: { key: string; dist: number } | null = null;\\n+  for (const k of knownKeys) {\\n+    const dist = levenshtein(prop, k.includes('.') ? k.split('.').pop()! : k);\\n+    if (dist <= 3 && (!best || dist < best.dist)) best = { key: k, dist };\\n+  }\\n+  if (best) return `Did you mean \\\"${best.key}\\\"?`;\\n+  return undefined;\\n+}\\n+\\n+function formatError(e: ErrorObject): string {\\n+  const path = toYamlPath(e.instancePath || '');\\n+  let msg = `${path}: ${e.message}`;\\n+  const hint = hintForAdditionalProperty(e);\\n+  if (hint) msg += ` (${hint})`;\\n+  if (e.keyword === 'enum' && Array.isArray((e.params as any)?.allowedValues)) {\\n+    msg += ` (allowed: ${(e.params as any).allowedValues.join(', ')})`;\\n+  }\\n+  return msg;\\n+}\\n+\\n+export type ValidationResult = { ok: true } | { ok: false; errors: string[] };\\n+\\n+export function validateTestsDoc(doc: unknown): ValidationResult {\\n+  try {\\n+    const ok = validate(doc);\\n+    if (ok) return { ok: true };\\n+    const errs = (validate.errors || []).map(formatError);\\n+    return { ok: false, errors: errs };\\n+  } catch (err) {\\n+    return { ok: false, errors: [err instanceof Error ? err.message : String(err)] };\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/utils/file-exclusion.ts\",\"additions\":1,\"deletions\":1,\"changes\":15,\"patch\":\"diff --git a/src/utils/file-exclusion.ts b/src/utils/file-exclusion.ts\\nindex 155bf20b..e7f5274e 100644\\n--- a/src/utils/file-exclusion.ts\\n+++ b/src/utils/file-exclusion.ts\\n@@ -128,12 +128,21 @@ export class FileExclusionHelper {\\n           .trim();\\n \\n         this.gitignore.add(gitignoreContent);\\n-        console.error('✅ Loaded .gitignore patterns for file filtering');\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.error('✅ Loaded .gitignore patterns for file filtering');\\n+        }\\n       } else if (additionalPatterns && additionalPatterns.length > 0) {\\n-        console.error('⚠️  No .gitignore found, using default exclusion patterns');\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.error('⚠️  No .gitignore found, using default exclusion patterns');\\n+        }\\n       }\\n     } catch (error) {\\n-      console.warn('⚠️ Failed to load .gitignore:', error instanceof Error ? error.message : error);\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        console.warn(\\n+          '⚠️ Failed to load .gitignore:',\\n+          error instanceof Error ? error.message : error\\n+        );\\n+      }\\n     }\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/integration/issue-double-content-detection.test.ts\",\"additions\":0,\"deletions\":5,\"changes\":131,\"patch\":\"diff --git a/tests/integration/issue-double-content-detection.test.ts b/tests/integration/issue-double-content-detection.test.ts\\ndeleted file mode 100644\\nindex 5ef9fb1b..00000000\\n--- a/tests/integration/issue-double-content-detection.test.ts\\n+++ /dev/null\\n@@ -1,131 +0,0 @@\\n-import * as fs from 'fs';\\n-import * as path from 'path';\\n-\\n-// Minimal Octokit REST mock to capture posted comment body\\n-const issuesCreateComment = jest.fn();\\n-const issuesListComments = jest.fn().mockResolvedValue({ data: [] });\\n-const pullsGet = jest.fn().mockResolvedValue({ data: { head: { sha: 'deadbeefcafebabe' } } });\\n-const pullsListFiles = jest.fn().mockResolvedValue({\\n-  data: [{ filename: 'x', status: 'modified', additions: 1, deletions: 0, changes: 1 }],\\n-});\\n-\\n-jest.mock('@octokit/rest', () => {\\n-  return {\\n-    Octokit: jest.fn().mockImplementation(() => ({\\n-      rest: {\\n-        issues: { createComment: issuesCreateComment, listComments: issuesListComments },\\n-        pulls: { get: pullsGet, listFiles: pullsListFiles },\\n-        checks: { create: jest.fn(), update: jest.fn() },\\n-      },\\n-    })),\\n-  };\\n-});\\n-\\n-describe('Issue assistant double-content detection (issues opened)', () => {\\n-  beforeEach(() => {\\n-    issuesCreateComment.mockReset();\\n-    issuesListComments.mockClear();\\n-    pullsGet.mockClear();\\n-    pullsListFiles.mockClear();\\n-\\n-    // Clean env used by action run()\\n-    delete process.env.GITHUB_EVENT_NAME;\\n-    delete process.env.GITHUB_EVENT_PATH;\\n-    delete process.env.GITHUB_REPOSITORY;\\n-    delete process.env.GITHUB_REPOSITORY_OWNER;\\n-    delete process.env['INPUT_GITHUB-TOKEN'];\\n-    delete process.env['INPUT_OWNER'];\\n-    delete process.env['INPUT_REPO'];\\n-    delete process.env['INPUT_CONFIG-PATH'];\\n-    delete process.env['INPUT_CREATE-CHECK'];\\n-    delete process.env['INPUT_COMMENT-ON-PR'];\\n-    delete process.env['INPUT_DEBUG'];\\n-  });\\n-\\n-  const writeTmp = (name: string, data: any) => {\\n-    const p = path.join(process.cwd(), name);\\n-    fs.writeFileSync(p, typeof data === 'string' ? data : JSON.stringify(data));\\n-    return p;\\n-  };\\n-\\n-  // This config intentionally produces two assistant-style outputs in the same run.\\n-  // With current behavior, both get concatenated into the single issue comment.\\n-  // We assert that only one assistant response appears (i.e., deduped/collapsed),\\n-  // so this test should fail until the posting logic is fixed.\\n-  const makeConfig = () => `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  assistant-initial:\\n-    type: ai\\n-    ai_provider: mock\\n-    schema: issue-assistant\\n-    group: dynamic\\n-    on: [issue_opened]\\n-    prompt: |\\n-      Return ONE JSON object for issue-assistant.\\n-      text: ### Assistant Reply\\n-      intent: issue_triage\\n-\\n-  assistant-refined:\\n-    type: ai\\n-    ai_provider: mock\\n-    schema: issue-assistant\\n-    group: dynamic\\n-    depends_on: [assistant-initial]\\n-    on: [issue_opened]\\n-    prompt: |\\n-      Return ONE JSON object for issue-assistant.\\n-      text: ### Assistant Reply\\n-      intent: issue_triage\\n-\\n-output:\\n-  pr_comment:\\n-    format: markdown\\n-    group_by: check\\n-`;\\n-\\n-  it('posts only the refined answer (no duplicate old+new content)', async () => {\\n-    const cfgPath = writeTmp('.tmp-double-content.yaml', makeConfig());\\n-    const event = {\\n-      action: 'opened',\\n-      issue: { number: 77, state: 'open' },\\n-      repository: { full_name: 'acme/widgets' },\\n-      sender: { login: 'reporter' },\\n-    } as any;\\n-    const eventPath = writeTmp('.tmp-issues-opened-double.json', event);\\n-\\n-    process.env.GITHUB_EVENT_NAME = 'issues';\\n-    process.env.GITHUB_EVENT_PATH = eventPath;\\n-    process.env.GITHUB_REPOSITORY = 'acme/widgets';\\n-    process.env.GITHUB_REPOSITORY_OWNER = 'acme';\\n-    process.env['INPUT_GITHUB-TOKEN'] = 'test-token';\\n-    process.env['INPUT_OWNER'] = 'acme';\\n-    process.env['INPUT_REPO'] = 'widgets';\\n-    process.env['INPUT_CONFIG-PATH'] = cfgPath;\\n-    process.env['INPUT_CREATE-CHECK'] = 'false';\\n-    process.env['INPUT_COMMENT-ON-PR'] = 'false';\\n-    process.env['INPUT_DEBUG'] = 'true';\\n-\\n-    jest.resetModules();\\n-    const { run } = await import('../../src/index');\\n-    await run();\\n-\\n-    // Exactly one comment is posted\\n-    expect(issuesCreateComment).toHaveBeenCalledTimes(1);\\n-    const call = issuesCreateComment.mock.calls[0][0];\\n-    const body: string = call.body;\\n-    // Debug: persist body for local inspection\\n-    fs.mkdirSync('tmp', { recursive: true });\\n-    fs.writeFileSync('tmp/issue-double-content-body.md', body, 'utf8');\\n-\\n-    // Desired behavior: Only a single assistant response should appear.\\n-    // Current bug: both initial and refined outputs are concatenated. In the\\n-    // mock path the provider sometimes returns a minimal JSON like {\\\"issues\\\":[]}.\\n-    // Assert that only one such block exists.\\n-    const jsonBlockCount = (body.match(/\\\\{\\\\s*\\\\\\\"issues\\\\\\\"\\\\s*:\\\\s*\\\\[\\\\]\\\\s*\\\\}/g) || []).length;\\n-    expect(jsonBlockCount).toBe(1);\\n-\\n-    fs.unlinkSync(cfgPath);\\n-    fs.unlinkSync(eventPath);\\n-  });\\n-});\\n\",\"status\":\"removed\"},{\"filename\":\"tests/integration/issue-posting-fact-gate.test.ts\",\"additions\":0,\"deletions\":7,\"changes\":197,\"patch\":\"diff --git a/tests/integration/issue-posting-fact-gate.test.ts b/tests/integration/issue-posting-fact-gate.test.ts\\ndeleted file mode 100644\\nindex 4ac86358..00000000\\n--- a/tests/integration/issue-posting-fact-gate.test.ts\\n+++ /dev/null\\n@@ -1,197 +0,0 @@\\n-import * as fs from 'fs';\\n-import * as path from 'path';\\n-\\n-// Reuse the Octokit REST mock pattern from other integration tests\\n-const checksCreate = jest.fn();\\n-const checksUpdate = jest.fn();\\n-const issuesCreateComment = jest.fn();\\n-const issuesListComments = jest.fn().mockResolvedValue({ data: [] });\\n-const pullsGet = jest.fn().mockResolvedValue({ data: { head: { sha: 'deadbeefcafebabe' } } });\\n-const pullsListFiles = jest.fn().mockResolvedValue({\\n-  data: [{ filename: 'x', status: 'modified', additions: 1, deletions: 0, changes: 1 }],\\n-});\\n-\\n-jest.mock('@octokit/rest', () => {\\n-  return {\\n-    Octokit: jest.fn().mockImplementation(() => ({\\n-      rest: {\\n-        checks: { create: checksCreate, update: checksUpdate },\\n-        issues: { createComment: issuesCreateComment, listComments: issuesListComments },\\n-        pulls: { get: pullsGet, listFiles: pullsListFiles },\\n-      },\\n-    })),\\n-  };\\n-});\\n-\\n-describe('Issue assistant posting is gated by fact validation (issue_opened)', () => {\\n-  beforeEach(() => {\\n-    checksCreate.mockReset();\\n-    checksUpdate.mockReset();\\n-    issuesCreateComment.mockReset();\\n-    issuesListComments.mockClear();\\n-    pullsGet.mockClear();\\n-    pullsListFiles.mockClear();\\n-    // Clean env that run() reads\\n-    delete process.env.GITHUB_EVENT_NAME;\\n-    delete process.env.GITHUB_EVENT_PATH;\\n-    delete process.env.GITHUB_REPOSITORY;\\n-    delete process.env.GITHUB_REPOSITORY_OWNER;\\n-    delete process.env['INPUT_GITHUB-TOKEN'];\\n-    delete process.env['INPUT_OWNER'];\\n-    delete process.env['INPUT_REPO'];\\n-    delete process.env['INPUT_CONFIG-PATH'];\\n-    delete process.env['INPUT_CREATE-CHECK'];\\n-    delete process.env['INPUT_COMMENT-ON-PR'];\\n-    delete process.env['INPUT_DEBUG'];\\n-  });\\n-\\n-  const makeConfig = (allValid: boolean) => `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  # Minimal issue assistant using mock provider\\n-  issue-assistant:\\n-    type: ai\\n-    ai_provider: mock\\n-    schema: issue-assistant\\n-    on: [issue_opened]\\n-    prompt: |\\n-      Return ONE JSON object for issue-assistant.\\n-      text: Hello, world.\\n-\\n-References:\\n-\\n-\\\\`\\\\`\\\\`refs\\n-none\\n-\\\\`\\\\`\\\\`\\n-      intent: issue_triage\\n-    on_success:\\n-      run: [init-fact-validation]\\n-\\n-  # Initialize validation state\\n-  init-fact-validation:\\n-    type: memory\\n-    operation: set\\n-    namespace: fact-validation\\n-    key: attempt\\n-    value: 0\\n-    on: [issue_opened]\\n-\\n-  # Seed deterministic facts instead of invoking AI\\n-  seed-facts:\\n-    type: memory\\n-    operation: set\\n-    namespace: fact-validation\\n-    key: fact_list\\n-    value: [{\\\"id\\\":\\\"f1\\\",\\\"category\\\":\\\"Configuration\\\",\\\"claim\\\":\\\"X\\\",\\\"verifiable\\\":true}]\\n-    depends_on: [issue-assistant]\\n-    on: [issue_opened]\\n-\\n-  # forEach extraction proxy\\n-  extract-facts:\\n-    type: memory\\n-    operation: get\\n-    namespace: fact-validation\\n-    key: fact_list\\n-    forEach: true\\n-    depends_on: [seed-facts]\\n-    on: [issue_opened]\\n-    on_finish:\\n-      run: [aggregate-validations]\\n-      goto_js: |\\n-        const ns = 'fact-validation';\\n-        const allValid = memory.get('all_valid', ns) === true;\\n-        const limit = 1; // one retry\\n-        const attempt = Number(memory.get('attempt', ns) || 0);\\n-        if (!allValid && attempt < limit) {\\n-          memory.increment('attempt', 1, ns);\\n-          return 'issue-assistant';\\n-        }\\n-        return null;\\n-\\n-  validate-fact:\\n-    type: memory\\n-    operation: exec_js\\n-    namespace: fact-validation\\n-    depends_on: [extract-facts]\\n-    on: [issue_opened]\\n-    memory_js: |\\n-      const NS = 'fact-validation';\\n-      const f = outputs['extract-facts'];\\n-      const attempt = Number(memory.get('attempt', NS) || 0);\\n-      const is_valid = ${allValid ? 'true' : 'false'}; return { fact_id: f.id, claim: f.claim, is_valid, confidence: '${allValid ? \\\"'ok'\\\" : \\\"'bad'\\\"} };\\n-\\n-  aggregate-validations:\\n-    type: memory\\n-    operation: exec_js\\n-    namespace: fact-validation\\n-    on: [issue_opened]\\n-    memory_js: |\\n-      const vals = outputs.history['validate-fact'] || [];\\n-      const invalid = (Array.isArray(vals) ? vals : []).filter(v => v && v.is_valid === false);\\n-      const all_valid = invalid.length === 0;\\n-      memory.set('all_valid', all_valid, 'fact-validation');\\n-      return { total: vals.length, all_valid };\\n-\\n-  # Emit a simple final note when valid so the Action has content to post once\\n-  final-note:\\n-    type: log\\n-    depends_on: [aggregate-validations]\\n-    if: \\\"memory.get('all_valid','fact-validation') === true\\\"\\n-    message: 'Verified: final'\\n-\\n-  # No explicit post step; use Action's generic end-of-run post\\n-\\n-output:\\n-  pr_comment:\\n-    format: markdown\\n-    group_by: check\\n-`;\\n-\\n-  const writeTmp = (name: string, data: any) => {\\n-    const p = path.join(process.cwd(), name);\\n-    fs.writeFileSync(p, typeof data === 'string' ? data : JSON.stringify(data));\\n-    return p;\\n-  };\\n-\\n-  const setupAndRun = async (allValid: boolean) => {\\n-    const cfgPath = writeTmp(\\n-      `.tmp-issue-gate-${allValid ? 'ok' : 'fail'}.yaml`,\\n-      makeConfig(allValid)\\n-    );\\n-    const event = {\\n-      action: 'opened',\\n-      issue: { number: 42, state: 'open' },\\n-      repository: { full_name: 'acme/widgets' },\\n-      sender: { login: 'reporter' },\\n-    } as any;\\n-    const eventPath = writeTmp('.tmp-issues-opened.json', event);\\n-\\n-    process.env.GITHUB_EVENT_NAME = 'issues';\\n-    process.env.GITHUB_EVENT_PATH = eventPath;\\n-    process.env.GITHUB_REPOSITORY = 'acme/widgets';\\n-    process.env.GITHUB_REPOSITORY_OWNER = 'acme';\\n-    process.env['INPUT_GITHUB-TOKEN'] = 'test-token';\\n-    process.env['INPUT_OWNER'] = 'acme';\\n-    process.env['INPUT_REPO'] = 'widgets';\\n-    process.env['INPUT_CONFIG-PATH'] = cfgPath;\\n-    process.env['INPUT_CREATE-CHECK'] = 'false';\\n-    process.env['INPUT_COMMENT-ON-PR'] = 'false';\\n-    process.env['INPUT_DEBUG'] = 'true';\\n-    process.env['ENABLE_FACT_VALIDATION'] = 'true';\\n-\\n-    // Import run() fresh to pick up env\\n-    jest.resetModules();\\n-    const { run } = await import('../../src/index');\\n-    await run();\\n-\\n-    fs.unlinkSync(cfgPath);\\n-    fs.unlinkSync(eventPath);\\n-  };\\n-\\n-  it('loops once to correct facts and posts a single final comment', async () => {\\n-    await setupAndRun(false);\\n-    // With attempt limit=1, the first validation fails, we route back to assistant,\\n-    // second pass should be valid and then post once at end.\\n-    expect(issuesCreateComment).toHaveBeenCalledTimes(1);\\n-  });\\n-});\\n\",\"status\":\"removed\"}],\"outputs\":{}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"overview","visor.check.output":"{\"text\":\"This PR introduces a comprehensive, configuration-driven integration test framework for Visor. It allows developers to write tests for their `.visor.yaml` configurations by simulating GitHub events, mocking providers (like AI and GitHub API calls), and asserting on the resulting actions. This is a significant feature that replaces previous ad-hoc testing methods with a structured and maintainable approach.\\n\\n### Files Changed Analysis\\n\\nThe changes introduce a new test runner, along with its supporting components, documentation, and default test suite.\\n\\n-   **New Feature Implementation (`src/test-runner/`)**: The core logic is encapsulated in the new `src/test-runner` directory, which includes the main `index.ts` (the runner itself), `fixture-loader.ts` for managing test data, `recorders/` for mocking GitHub and AI interactions, and `validator.ts` for handling assertions.\\n-   **CLI Integration (`src/cli-main.ts`)**: A new `test` subcommand is added to the Visor CLI to execute the test runner.\\n-   **Execution Engine Modifications (`src/check-execution-engine.ts`)**: The engine is updated to support test mode, primarily by allowing the injection of mock providers and recorders.\\n-   **New Test Suite (`defaults/.visor.tests.yaml`)**: A comprehensive test suite for the default `.visor.yaml` configuration is added, serving as a practical example of the new framework.\\n-   **Documentation (`docs/testing/`)**: Extensive documentation is added, covering getting started, CLI usage, assertions, and fixtures/mocks.\\n-   **CI Integration (`.github/workflows/ci.yml`)**: The CI pipeline is updated to run the new integration tests, ensuring configurations are validated on each pull request.\\n-   **Test Removal (`tests/integration/`)**: Old, script-based integration tests are removed in favor of the new, more robust framework.\\n\\n### Architecture & Impact Assessment\\n\\n#### What this PR accomplishes\\n\\nThis PR delivers a complete integration test framework for Visor configurations. It enables developers to validate their automation rules in a predictable, isolated environment without making live network calls. This improves reliability, simplifies debugging, and provides a safety net for configuration changes.\\n\\n#### Key technical changes introduced\\n\\n1.  **Test Runner CLI**: A `visor test` command is introduced to discover and run tests defined in a `.visor.tests.yaml` file.\\n2.  **Fixture-Based Testing**: Tests are driven by predefined \\\"fixtures\\\" that simulate GitHub webhook events (e.g., `gh.pr_open.minimal`).\\n3.  **Mocking and Recording**: The framework intercepts calls to external providers. GitHub API calls are recorded for assertion, and AI provider calls are mocked to return predefined responses. This is handled by a `RecordingOctokit` wrapper and mock AI providers that are activated when `ai.provider` is set to `mock`.\\n4.  **Declarative Assertions**: Tests use a YAML `expect:` block to assert on outcomes, such as the number of calls to a provider (`calls`), the content of AI prompts (`prompts`), or the final status of checks.\\n\\n#### Affected system components\\n\\n-   **CLI (`src/cli-main.ts`)**: Extended with a new `test` command.\\n-   **Core Logic (`src/check-execution-engine.ts`)**: Modified to operate in a \\\"test mode\\\" with mocked dependencies.\\n-   **Providers (`src/providers/*`)**: The `GithubOpsProvider` is adapted to use a recordable Octokit instance during tests. The `AiCheckProvider` is modified to handle a `mock` provider type.\\n-   **CI/CD (`.github/workflows/ci.yml`)**: The CI workflow is updated to execute the new test suite.\\n\\n#### Component Interaction Diagram\\n\\n```mermaid\\ngraph TD\\n    subgraph Test Execution\\n        A[visor test CLI] --> B{Test Runner};\\n        B --> C[Load .visor.tests.yaml];\\n        C --> D{For each test case};\\n        D --> E[Load Fixture & Mocks];\\n    end\\n\\n    subgraph Visor Core\\n        F(CheckExecutionEngine);\\n        G[Providers (GitHub, AI, etc.)];\\n    end\\n\\n    subgraph Mocks & Recorders\\n        H[RecordingOctokit];\\n        I[MockAiProvider];\\n    end\\n\\n    E --> |injects mocks| F;\\n    F --> |uses| G;\\n    G -- during test --> H;\\n    G -- during test --> I;\\n\\n    D --> |runs| F;\\n    F --> J[Collect Results & Recorded Calls];\\n    J --> K[Validate Assertions];\\n    K --> L[Report Pass/Fail];\\n```\\n\\n### Scope Discovery & Context Expansion\\n\\nThis feature fundamentally changes how Visor configurations are developed and maintained. By providing a robust testing framework, it encourages a test-driven development (TDD) approach for writing automation rules.\\n\\n-   **Impact on Configuration Development**: Users will now be expected to write tests for their custom checks and workflows. The `defaults/.visor.tests.yaml` file serves as a blueprint for this.\\n-   **Reliability and Maintenance**: The ability to test configurations offline significantly reduces the risk of introducing regressions. It makes troubleshooting easier, as failures can be reproduced locally and deterministically.\\n-   **Provider Ecosystem**: The mocking architecture is extensible. While this PR focuses on GitHub and AI providers, the same pattern could be applied to any future provider (e.g., Slack, Jira), ensuring that all integrations can be tested.\\n-   **Developer Experience**: The framework is designed with developer experience in mind, offering clear output, helpful error messages, and a straightforward YAML-based syntax, lowering the barrier to writing effective tests.\",\"tags\":{\"review-effort\":5,\"label\":\"feature\"}}"},"events":[]}
{"name":"visor.check","attributes":{"visor.check.id":"overview"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"fail_if.evaluated","attrs":{"check":"overview","scope":"global","name":"global_fail_if","expression":"output.issues && output.issues.some(i => i.severity === 'critical' || i.severity === 'error')"}}]}
{"name":"visor.fail_if","attributes":{"check":"overview","scope":"global"},"events":[{"name":"fail_if.evaluated","attrs":{"check":"overview","scope":"global"}}]}
{"name":"visor.check","attributes":{"visor.check.id":"overview"},"events":[{"name":"check.started"},{"name":"check.completed"}]}
{"name":"visor.event","attributes":{},"events":[{"name":"fail_if.evaluated","attrs":{"check":"overview","scope":"global","name":"global_fail_if","expression":"output.issues && output.issues.some(i => i.severity === 'critical' || i.severity === 'error')"}}]}
{"name":"visor.fail_if","attributes":{"check":"overview","scope":"global"},"events":[{"name":"fail_if.evaluated","attrs":{"check":"overview","scope":"global"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"fail_if.evaluated","attrs":{"check":"overview","scope":"global","name":"global_fail_if","expression":"output.issues && output.issues.some(i => i.severity === 'critical' || i.severity === 'error')"}}]}
{"name":"visor.fail_if","attributes":{"check":"overview","scope":"global"},"events":[{"name":"fail_if.evaluated","attrs":{"check":"overview","scope":"global"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"fail_if.evaluated","attrs":{"check":"overview","scope":"global","name":"global_fail_if","expression":"output.issues && output.issues.some(i => i.severity === 'critical' || i.severity === 'error')"}}]}
{"name":"visor.fail_if","attributes":{"check":"overview","scope":"global"},"events":[{"name":"fail_if.evaluated","attrs":{"check":"overview","scope":"global"}}]}
{"name":"visor.event","attributes":{},"events":[{"name":"fail_if.evaluated","attrs":{"check":"overview","scope":"global","name":"global_fail_if","expression":"output.issues && output.issues.some(i => i.severity === 'critical' || i.severity === 'error')"}}]}
{"name":"visor.fail_if","attributes":{"check":"overview","scope":"global"},"events":[{"name":"fail_if.evaluated","attrs":{"check":"overview","scope":"global"}}]}
{"name":"visor.provider","attributes":{"visor.check.id":"security","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"security","visor.check.input.context":"{\"pr\":{\"number\":0,\"title\":\"Local Analysis: feat/test-framework-runner (14 modified)\",\"author\":\"Leonid Bugaev\",\"branch\":\"feat/test-framework-runner\",\"base\":\"main\"},\"files\":[{\"filename\":\".github/workflows/ci.yml\",\"additions\":1,\"deletions\":0,\"changes\":15,\"patch\":\"diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml\\nindex 840f2abc..13bd6f9d 100644\\n--- a/.github/workflows/ci.yml\\n+++ b/.github/workflows/ci.yml\\n@@ -64,6 +64,21 @@ jobs:\\n           ls -la *.tgz\\n           echo \\\"✅ Package can be created successfully\\\"\\n \\n+      - name: Run integration tests (defaults suite)\\n+        run: |\\n+          mkdir -p output\\n+          node ./dist/index.js test --config defaults/.visor.tests.yaml --json output/visor-tests.json --report junit:output/visor-tests.xml --summary md:output/visor-tests.md\\n+\\n+      - name: Upload integration test artifacts\\n+        if: always()\\n+        uses: actions/upload-artifact@v4\\n+        with:\\n+          name: visor-test-results\\n+          path: |\\n+            output/visor-tests.json\\n+            output/visor-tests.xml\\n+            output/visor-tests.md\\n+\\n       - name: Test basic action functionality\\n         uses: ./\\n         with:\\n\",\"status\":\"added\"},{\"filename\":\"README.md\",\"additions\":1,\"deletions\":0,\"changes\":10,\"patch\":\"diff --git a/README.md b/README.md\\nindex b2bf4db5..7acc4843 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -729,3 +729,13 @@ steps:\\n ```\\n \\n See docs: docs/github-ops.md\\n+## Integration Tests (Great DX)\\n+\\n+Visor ships a YAML‑native integration test runner so you can describe user flows, mocks, and assertions alongside your config.\\n+\\n+- Start here: docs/testing/getting-started.md\\n+- CLI details: docs/testing/cli.md\\n+- Fixtures and mocks: docs/testing/fixtures-and-mocks.md\\n+- Assertions reference: docs/testing/assertions.md\\n+\\n+Example suite: defaults/.visor.tests.yaml\\n\",\"status\":\"added\"},{\"filename\":\"defaults/.visor.tests.yaml\",\"additions\":20,\"deletions\":0,\"changes\":557,\"patch\":\"diff --git a/defaults/.visor.tests.yaml b/defaults/.visor.tests.yaml\\nnew file mode 100644\\nindex 00000000..c496cdee\\n--- /dev/null\\n+++ b/defaults/.visor.tests.yaml\\n@@ -0,0 +1,557 @@\\n+version: \\\"1.0\\\"\\n+extends: \\\".visor.yaml\\\"\\n+\\n+# Integration test suite for Visor default configuration\\n+# - Driven by events + fixtures; no manual step lists\\n+# - Strict by default: every executed step must have an expect\\n+# - AI mocks accept structured JSON when a schema is defined; plain uses text\\n+# - GitHub calls are recorded by default by the test runner (no network)\\n+\\n+tests:\\n+  defaults:\\n+    strict: true\\n+    ai_provider: mock\\n+    fail_on_unexpected_calls: true\\n+    # Example: enable negative GitHub recorder for all tests\\n+    # github_recorder: { error_code: 429 }\\n+  # Built-in fixtures are provided by the test runner (gh.* namespace).\\n+  # Custom fixtures may still be added here if needed.\\n+  fixtures: []\\n+\\n+  cases:\\n+    - name: label-flow\\n+      description: |\\n+        Validates the happy path for PR open:\\n+        - overview runs and emits tags.label and tags.review-effort (mocked)\\n+        - apply-overview-labels adds two labels (feature and review/effort:2)\\n+        - overview prompt includes PR title and unified diff header\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: |\\n+            High‑level summary of the changes and impact.\\n+          tags:\\n+            label: feature\\n+            review-effort: 2\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args:\\n+              contains:\\n+                - feature\\n+                - \\\"review/effort:2\\\"\\n+        outputs:\\n+          - step: overview\\n+            path: \\\"tags.label\\\"\\n+            equals: feature\\n+          - step: overview\\n+            path: \\\"tags['review-effort']\\\"\\n+            equals: 2\\n+        prompts:\\n+          - step: overview\\n+            contains:\\n+              - \\\"feat: add user search\\\"\\n+              - \\\"diff --git a/src/search.ts\\\"\\n+\\n+    - name: issue-triage\\n+      skip: true\\n+      description: |\\n+        Ensures the issue assistant triages a newly opened issue and applies labels.\\n+        Asserts the structured output (intent=issue_triage) and the GitHub label op.\\n+      event: issue_opened\\n+      fixture: gh.issue_open.minimal\\n+      mocks:\\n+        issue-assistant:\\n+          text: |\\n+            Thanks for the detailed report! We will investigate.\\n+          intent: issue_triage\\n+          labels: [bug, priority/medium]\\n+      expect:\\n+        calls:\\n+          - step: issue-assistant\\n+            exactly: 1\\n+          - step: apply-issue-labels\\n+            exactly: 1\\n+          - step: init-fact-validation\\n+            exactly: 1\\n+          - step: extract-facts\\n+            exactly: 1\\n+          - step: validate-fact\\n+            at_least: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args:\\n+              contains:\\n+                - bug\\n+        outputs:\\n+          - step: issue-assistant\\n+            path: intent\\n+            equals: issue_triage\\n+        prompts:\\n+          - step: issue-assistant\\n+            contains:\\n+              - \\\"Bug: crashes on search edge case\\\"\\n+\\n+    - name: pr-review-e2e-flow\\n+      description: |\\n+        End-to-end PR lifecycle covering multiple external events:\\n+        1) PR opened → overview + labels\\n+        2) Standard comment → no bot reply\\n+        3) /visor help → single assistant reply (no retrigger)\\n+        4) /visor Regenerate reviews → retrigger overview\\n+        5) Fact validation enabled on comment → extract/validate/aggregate\\n+        6) Fact validation disabled on comment → only assistant, no validation steps\\n+        7) PR synchronized (new commit) → overview runs again\\n+      strict: true\\n+      flow:\\n+        - name: pr-open\\n+          description: |\\n+            PR open event. Mocks overview/security/quality/performance as empty issue lists.\\n+            Expects all review steps to run and labels to be added.\\n+          event: pr_opened\\n+          fixture: gh.pr_open.minimal\\n+          mocks:\\n+            overview:\\n+              text: \\\"Overview body\\\"\\n+              tags: { label: feature, review-effort: 2 }\\n+            security: { issues: [] }\\n+            architecture: { issues: [] }\\n+            quality: { issues: [] }\\n+            performance: { issues: [] }\\n+          expect:\\n+            calls:\\n+              - step: overview\\n+                exactly: 1\\n+              - step: security\\n+                exactly: 1\\n+              - step: architecture\\n+                exactly: 1\\n+              - step: performance\\n+                exactly: 1\\n+              - step: quality\\n+                exactly: 1\\n+              - step: apply-overview-labels\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.createComment\\n+                at_least: 1\\n+              - provider: github\\n+                op: labels.add\\n+                at_least: 1\\n+                args:\\n+                  contains: [feature]\\n+            prompts:\\n+              - step: overview\\n+                contains:\\n+                  - \\\"feat: add user search\\\"\\n+\\n+        - name: standard-comment\\n+          description: |\\n+            A regular human comment on a PR should not produce a bot reply.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.standard\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"\\\"   # empty text to avoid posting a reply\\n+              intent: comment_reply\\n+          expect:\\n+            no_calls:\\n+              - provider: github\\n+                op: issues.createComment\\n+              - step: init-fact-validation\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+\\n+        - name: visor-plain\\n+          description: |\\n+            A \\\"/visor help\\\" comment should be recognized and answered once by the assistant.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"Sure, here’s how I can help.\\\"\\n+              intent: comment_reply\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.createComment\\n+                exactly: 1\\n+            outputs:\\n+              - step: comment-assistant\\n+                path: intent\\n+                equals: comment_reply\\n+            prompts:\\n+              - step: comment-assistant\\n+                matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"\\n+\\n+        - name: visor-retrigger\\n+          description: |\\n+            A \\\"/visor Regenerate reviews\\\" comment should set intent=comment_retrigger\\n+            and schedule a new overview.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_regenerate\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"Regenerating.\\\"\\n+              intent: comment_retrigger\\n+            overview:\\n+              text: \\\"Overview (regenerated)\\\"\\n+              tags: { label: feature, review-effort: 2 }\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+              - step: overview\\n+                exactly: 1\\n+              - step: security\\n+                exactly: 1\\n+              - step: architecture\\n+                exactly: 1\\n+              - step: performance\\n+                exactly: 1\\n+              - step: quality\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.createComment\\n+                at_least: 1\\n+            outputs:\\n+              - step: comment-assistant\\n+                path: intent\\n+                equals: comment_retrigger\\n+            prompts:\\n+              - step: comment-assistant\\n+                contains: [\\\"Regenerate reviews\\\"]\\n+\\n+        - name: facts-enabled\\n+          description: |\\n+            With fact validation enabled, the assistant reply is followed by\\n+            extract-facts, validate-fact (per fact), and aggregate-validations.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"We rely on defaults/.visor.yaml line 11 for max_parallelism=4.\\\"\\n+              intent: comment_reply\\n+            extract-facts:\\n+              - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+            validate-fact[]:\\n+              - fact_id: f1\\n+                claim: \\\"max_parallelism defaults to 4\\\"\\n+                is_valid: true\\n+                confidence: high\\n+                evidence: \\\"defaults/.visor.yaml:11\\\"\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+              - step: extract-facts\\n+                exactly: 1\\n+              - step: validate-fact\\n+                at_least: 1\\n+              - step: aggregate-validations\\n+                exactly: 1\\n+              - step: init-fact-validation\\n+                exactly: 1\\n+          # Prompt assertions are validated separately in stage-level prompt tests\\n+\\n+        - name: facts-invalid\\n+          description: |\\n+            Invalid fact path: after assistant reply, extract-facts finds one claim and\\n+            validate-fact returns is_valid=false; aggregate-validations detects not-all-valid\\n+            and reruns the assistant once with correction context.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"We rely on defaults/.visor.yaml line 11 for max_parallelism=4.\\\"\\n+              intent: comment_reply\\n+            extract-facts:\\n+              - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+            validate-fact[]:\\n+              - fact_id: f1\\n+                claim: \\\"max_parallelism defaults to 4\\\"\\n+                is_valid: false\\n+                confidence: high\\n+                evidence: \\\"defaults/.visor.yaml:11 does not set 4\\\"\\n+                correction: \\\"max_parallelism defaults to 3\\\"\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 2\\n+              - step: extract-facts\\n+                exactly: 1\\n+              - step: validate-fact\\n+                at_least: 1\\n+              - step: aggregate-validations\\n+                exactly: 1\\n+              - step: init-fact-validation\\n+                exactly: 1\\n+            prompts:\\n+              - step: comment-assistant\\n+                index: last\\n+                contains:\\n+                  - \\\"<previous_response>\\\"\\n+                  - \\\"max_parallelism defaults to 4\\\"\\n+                  - \\\"Correction: max_parallelism defaults to 3\\\"\\n+\\n+        - name: facts-two-items\\n+          description: |\\n+            Two facts extracted; only the invalid fact should appear in the correction pass.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"We rely on defaults/.visor.yaml for concurrency defaults.\\\"\\n+              intent: comment_reply\\n+            extract-facts:\\n+              - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+              - { id: f2, category: Feature,       claim: \\\"Fast mode is enabled by default\\\", verifiable: true }\\n+            validate-fact[]:\\n+              - { fact_id: f1, claim: \\\"max_parallelism defaults to 4\\\", is_valid: false, confidence: high, evidence: \\\"defaults/.visor.yaml:11\\\", correction: \\\"max_parallelism defaults to 3\\\" }\\n+              - { fact_id: f2, claim: \\\"Fast mode is enabled by default\\\", is_valid: true, confidence: high, evidence: \\\"src/config.ts:FAST_MODE=true\\\" }\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 2\\n+              - step: extract-facts\\n+                exactly: 1\\n+              - step: validate-fact\\n+                exactly: 2\\n+              - step: aggregate-validations\\n+                exactly: 1\\n+              - step: init-fact-validation\\n+                exactly: 1\\n+            outputs:\\n+              - step: validate-fact\\n+                where: { path: fact_id, equals: f1 }\\n+                path: is_valid\\n+                equals: false\\n+              - step: validate-fact\\n+                where: { path: fact_id, equals: f2 }\\n+                path: is_valid\\n+                equals: true\\n+            prompts:\\n+              - step: comment-assistant\\n+                index: last\\n+                contains:\\n+                  - \\\"<previous_response>\\\"\\n+                  - \\\"max_parallelism defaults to 4\\\"\\n+                  - \\\"Correction: max_parallelism defaults to 3\\\"\\n+                not_contains:\\n+                  - \\\"Fast mode is enabled by default\\\"\\n+\\n+        - name: facts-disabled\\n+          description: |\\n+            With fact validation disabled, only the assistant runs; no validation steps execute.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.standard\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"false\\\"\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+            no_calls:\\n+              - step: init-fact-validation\\n+              - step: extract-facts\\n+              - step: validate-fact\\n+              - step: aggregate-validations\\n+\\n+        - name: pr-updated\\n+          description: |\\n+            When a new commit is pushed (synchronize), overview should run again\\n+            and post/refresh a comment.\\n+          event: pr_updated\\n+          fixture: gh.pr_sync.minimal\\n+          mocks:\\n+            overview:\\n+              text: \\\"Overview for new commit\\\"\\n+              tags: { label: feature, review-effort: 3 }\\n+          expect:\\n+            calls:\\n+              - step: overview\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.updateComment\\n+                at_least: 1\\n+\\n+    - name: security-fail-if\\n+      description: |\\n+        Verifies that the global fail_if trips when security produces an error‑severity issue.\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview text\\\"\\n+          tags:\\n+            label: bug\\n+            review-effort: 3\\n+        security:\\n+          issues:\\n+            - id: S-001\\n+              file: src/search.ts\\n+              line: 10\\n+              message: \\\"Command injection risk\\\"\\n+              severity: error\\n+              category: security\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: security\\n+            exactly: 1\\n+        outputs:\\n+          - step: security\\n+            path: \\\"issues[0].severity\\\"\\n+            equals: error\\n+        fail:\\n+          message_contains: \\\"fail_if\\\"\\n+\\n+    - name: strict-mode-example\\n+      skip: true\\n+      description: |\\n+        Demonstrates strict mode: a step executed without a corresponding expect\\n+        (apply-overview-labels) triggers a strict_violation with a helpful message.\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: \\\"Short overview\\\"\\n+          tags:\\n+            label: chore\\n+            review-effort: 1\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+        strict_violation:\\n+          for_step: apply-overview-labels\\n+          message_contains: \\\"Add an expect for this step or set strict: false\\\"\\n+\\n+    - name: visor-plain-prompt\\n+      description: |\\n+        Standalone prompt check for a \\\"/visor help\\\" comment.\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_help\\n+      mocks:\\n+        comment-assistant:\\n+          text: \\\"Here is how I can help.\\\"\\n+          intent: comment_reply\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+        prompts:\\n+          - step: comment-assistant\\n+            matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"\\n+\\n+    - name: visor-retrigger-prompt\\n+      description: |\\n+        Standalone prompt check for \\\"/visor Regenerate reviews\\\".\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_regenerate\\n+      strict: false\\n+      mocks:\\n+        comment-assistant:\\n+          text: \\\"Regenerating.\\\"\\n+          intent: comment_retrigger\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+        prompts:\\n+          - step: comment-assistant\\n+            contains: [\\\"Regenerate reviews\\\"]\\n+\\n+    - name: command-mock-shape\\n+      description: |\\n+        Illustrates command provider mocking and output assertions.\\n+        Skipped by default; enable when command steps exist.\\n+      skip: true  # illustrative only, enable when a command step exists\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        unit-tests:\\n+          stdout: '{\\\"passed\\\": 128, \\\"failed\\\": 0, \\\"duration_sec\\\": 1.2}'\\n+          exit_code: 0\\n+      expect:\\n+        calls:\\n+          - step: unit-tests\\n+            exactly: 1\\n+        outputs:\\n+          - step: unit-tests\\n+            path: failed\\n+            equals: 0\\n+\\n+    - name: github-negative-mode\\n+      description: |\\n+        Demonstrates negative GitHub recorder mode: simulate a 429 error and assert failure path.\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      github_recorder: { error_code: 429 }\\n+      # Override defaults for this case only by specifying a local recorder via env-like knob\\n+      # The runner reads tests.defaults.github_recorder; we provide it at the suite level by default.\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview body\\\"\\n+          tags: { label: feature, review-effort: 2 }\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+        fail:\\n+          message_contains: \\\"github/op_failed\\\"\\n+\\n+    - name: facts-invalid\\n+      skip: true\\n+      description: |\\n+        With fact validation enabled and an invalid fact, aggregate-validations should detect\\n+        not-all-valid and route back to the assistant for a correction pass in the same stage.\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_help\\n+      env:\\n+        ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+      mocks:\\n+        comment-assistant:\\n+          text: \\\"We rely on defaults/.visor.yaml line 11 for max_parallelism=4.\\\"\\n+          intent: comment_reply\\n+        extract-facts:\\n+          - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+        validate-fact[]:\\n+          - fact_id: f1\\n+            claim: \\\"max_parallelism defaults to 4\\\"\\n+            is_valid: false\\n+            confidence: high\\n+            evidence: \\\"defaults/.visor.yaml:11 does not set 4\\\"\\n+            correction: \\\"max_parallelism defaults to 3\\\"\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+          - step: extract-facts\\n+            exactly: 1\\n+          - step: validate-fact\\n+            at_least: 1\\n+          - step: aggregate-validations\\n+            exactly: 1\\n+          - step: init-fact-validation\\n+            exactly: 1\\n\",\"status\":\"added\"},{\"filename\":\"defaults/.visor.yaml\",\"additions\":1,\"deletions\":1,\"changes\":25,\"patch\":\"diff --git a/defaults/.visor.yaml b/defaults/.visor.yaml\\nindex 0e018884..21fad9eb 100644\\n--- a/defaults/.visor.yaml\\n+++ b/defaults/.visor.yaml\\n@@ -452,8 +452,9 @@ steps:\\n         - Provide partial answers when possible, and indicate what additional information would help give a complete response\\n     on: [issue_comment]\\n     on_success:\\n-      # Always initialize fact validation attempt counter\\n-      run: [init-fact-validation]\\n+      # Initialize fact validation attempt counter only when validation is enabled\\n+      run_js: |\\n+        return env.ENABLE_FACT_VALIDATION === 'true' ? ['init-fact-validation'] : []\\n       # Preserve intent-based rerun: allow members to retrigger overview from a comment\\n       goto_js: |\\n         const intent = (typeof output === 'object' && output) ? output.intent : undefined;\\n@@ -619,8 +620,14 @@ steps:\\n     # After all facts are validated, aggregate results and decide next action\\n     on_finish:\\n       run: [aggregate-validations]\\n+      # If aggregation stored validation issues in memory, schedule a correction reply\\n+      run_js: |\\n+        const issues = memory.list('fact-validation').includes('fact_validation_issues')\\n+          ? memory.get('fact_validation_issues', 'fact-validation')\\n+          : [];\\n+        return Array.isArray(issues) && issues.length > 0 ? ['comment-assistant'] : [];\\n       goto_js: |\\n-        const allValid = memory.get('all_valid', 'fact-validation');\\n+        const allValid = memory.get('all_facts_valid', 'fact-validation');\\n         const attempt = memory.get('fact_validation_attempt', 'fact-validation') || 0;\\n \\n         log('🔍 Fact validation complete - allValid:', allValid, 'attempt:', attempt);\\n@@ -702,11 +709,10 @@ steps:\\n     on: [issue_opened, issue_comment]\\n     if: \\\"env.ENABLE_FACT_VALIDATION === 'true'\\\"\\n     on_success:\\n-      goto_js: |\\n-        // Route back to the appropriate assistant if there are issues\\n-        if (!output || output.all_valid) return null;\\n-        const hasComment = !!(outputs['comment-assistant']) || (outputs.history && (outputs.history['comment-assistant'] || []).length > 0);\\n-        return hasComment ? 'comment-assistant' : 'issue-assistant';\\n+      # Schedule the correction reply directly (target-only) when not all facts are valid\\n+      run_js: |\\n+        if (!output || output.all_valid) return [];\\n+        return ['comment-assistant'];\\n     memory_js: |\\n       const validations = outputs.history['validate-fact'] || [];\\n \\n@@ -722,8 +728,7 @@ steps:\\n       log('Results: valid=' + (validations.length - invalid.length - lowConfidence.length),\\n           'invalid=' + invalid.length, 'low-confidence=' + lowConfidence.length);\\n \\n-      memory.set('all_valid', allValid, 'fact-validation');\\n-      memory.set('total_validations', validations.length, 'fact-validation');\\n+      memory.set('all_facts_valid', allValid, 'fact-validation');\\n       memory.set('validation_results', validations, 'fact-validation');\\n       memory.set('invalid_facts', invalid, 'fact-validation');\\n       memory.set('low_confidence_facts', lowConfidence, 'fact-validation');\\n\",\"status\":\"modified\"},{\"filename\":\"docs/test-framework-rfc.md\",\"additions\":22,\"deletions\":0,\"changes\":641,\"patch\":\"diff --git a/docs/test-framework-rfc.md b/docs/test-framework-rfc.md\\nnew file mode 100644\\nindex 00000000..7b899980\\n--- /dev/null\\n+++ b/docs/test-framework-rfc.md\\n@@ -0,0 +1,641 @@\\n+# Visor Integration Test Framework (RFC)\\n+\\n+Status: In Progress\\n+Date: 2025-10-27\\n+Owners: @probelabs/visor\\n+\\n+## Summary\\n+\\n+Add a first‑class, YAML‑native integration test framework for Visor that lets teams describe user flows, mocks, and assertions directly alongside their Visor config. Tests are defined in a separate YAML that can `extends` the base configuration, run entirely offline (no network), and default to strict verification.\\n+\\n+Key ideas:\\n+- Integration‑first: simulate real GitHub events and repo context; no manual step lists.\\n+- Strict by default: if a step ran and you didn’t assert it, the test fails.\\n+- Provider record mode by default: GitHub calls are intercepted and recorded (no network); assert them later.\\n+- Simple mocks keyed by step name; schema‑aware AI outputs (objects/arrays for structured schemas; `text` for plain).\\n+- Support multi‑event “flows” that preserve memory and outputs across events.\\n+\\n+## Motivation\\n+\\n+- Keep tests next to config and use the same mental model: events → checks → outputs → effects.\\n+- Validate real behavior (routing, `on` filters, `if` guards, `goto`/`on_success`, forEach) rather than unit‑style steps.\\n+- Make CI reliable and offline by default while still asserting side‑effects (labels, comments, check runs).\\n+\\n+## Non‑Goals\\n+\\n+- Unit testing individual providers (covered by Jest/TS tests).\\n+- Golden CI logs; we assert structured outputs and recorded operations instead.\\n+\\n+## Terminology\\n+\\n+- Case: a single integration test driven by one event + fixture.\\n+- Flow: an ordered list of cases; runner preserves memory/outputs across steps.\\n+- Fixture: a reusable external context (webhook payload, changed files, env, fs overlay, frozen clock).\\n+\\n+## File Layout\\n+\\n+- Base config (unchanged): `defaults/.visor.yaml` (regular steps live here).\\n+- Test suite (new): `defaults/.visor.tests.yaml`\\n+  - `extends: \\\".visor.yaml\\\"` to inherit the base checks.\\n+  - Contains `tests.defaults`, `tests.fixtures`, `tests.cases`.\\n+\\n+## Default Behaviors (Test Mode)\\n+\\n+- Strict mode: enabled by default (`tests.defaults.strict: true`). Any executed step must appear in `expect.calls`, or the case fails.\\n+- GitHub recording: the runner uses a recording Octokit by default; no network calls are made. Assert effects via `expect.calls` with `provider: github` and an `op` (e.g., `issues.createComment`, `labels.add`, `checks.create`).\\n+- AI provider: `mock` by default for tests; schema‑aware handling (see below).\\n+\\n+## Built‑in Fixtures and GitHub Mocks\\n+\\n+The runner ships with a library of built‑in fixtures and a recording GitHub mock so you don’t have to redefine common scenarios.\\n+\\n+### Built‑in Fixtures (gh.*)\\n+\\n+Use via `fixture: <name>`:\\n+\\n+- `gh.pr_open.minimal` — pull_request opened with a small PR (branch/base, 1–2 files, tiny patch).\\n+- `gh.pr_sync.minimal` — pull_request synchronize (new commit pushed) with updated HEAD SHA.\\n+- `gh.issue_open.minimal` — issues opened with a short title/body.\\n+- `gh.issue_comment.standard` — issue_comment created with a normal message on a PR.\\n+- `gh.issue_comment.visor_help` — issue_comment created with \\\"/visor help\\\".\\n+- `gh.issue_comment.visor_regenerate` — issue_comment created with \\\"/visor Regenerate reviews\\\".\\n+- `gh.issue_comment.edited` — issue_comment edited event.\\n+- `gh.pr_closed.minimal` — pull_request closed event.\\n+\\n+All gh.* fixtures populate:\\n+- `webhook` (name, action, payload)\\n+- `git` (branch, baseBranch)\\n+- `files` and `diff` (for PR fixtures)\\n+- `env` and `time.now` for determinism\\n+\\n+Optional overrides (future):\\n+\\n+```yaml\\n+fixture:\\n+  builtin: gh.pr_open.minimal\\n+  overrides:\\n+    pr.title: \\\"feat: custom title\\\"\\n+    webhook.payload.pull_request.number: 42\\n+```\\n+\\n+### GitHub Recorder (Built‑in)\\n+\\n+By default in test mode, the runner installs a recording Octokit:\\n+- Captures all calls and args for assertions (`expect.calls` with `provider: github`).\\n+- Returns stable stub shapes to unblock flows:\\n+  - `issues.createComment` → `{ data: { id, html_url, body, user, created_at } }`\\n+  - `issues.updateComment` → same shape\\n+  - `pulls.get`, `pulls.listFiles` → derived from fixture\\n+  - `checks.create`, `checks.update` → `{ data: { id, status, conclusion, url } }`\\n+  - `labels.add` → `{ data: { labels: [ ... ] } }` (or a no‑op with capture)\\n+\\n+  No network calls are made. You can still opt into real Octokit in the future with a `mode: passthrough` runner flag (not default).\\n+  Optional negative modes (per case or global):\\n+  - `error(429|422|404)` — simulate API errors; captured in call history.\\n+  - `timeout(1000ms)` — simulate request timeouts.\\n+\\n+## YAML Syntax Overview\\n+\\n+Minimal suite:\\n+\\n+```yaml\\n+version: \\\"1.0\\\"\\n+extends: \\\".visor.yaml\\\"\\n+\\n+tests:\\n+  defaults:\\n+    strict: true\\n+    ai_provider: mock\\n+    macros:\\n+      expect_review_posted:\\n+        calls:\\n+          - provider: github\\n+            op: issues.createComment\\n+            at_least: 1\\n+\\n+  fixtures: []   # (Optional) rely on gh.* built‑ins\\n+\\n+  cases:\\n+    - name: label-flow\\n+      event: pr_opened\\n+      fixture:\\n+        builtin: gh.pr_open.minimal\\n+        overrides:\\n+          pr.title: \\\"feat: add user search\\\"\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview body\\\"\\n+          tags: { label: feature, review-effort: 2 }\\n+      expect:\\n+        use: [expect_review_posted]\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args:\\n+              contains_unordered: [feature, \\\"review/effort:2\\\"]\\n+```\\n+\\n+### Flows (multi‑event)\\n+\\n+```yaml\\n+- name: pr-review-e2e-flow\\n+  strict: true\\n+  flow:\\n+    - name: pr-open\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview: { text: \\\"Overview body\\\", tags: { label: feature, review-effort: 2 } }\\n+        security: { issues: [] }\\n+        quality: { issues: [] }\\n+        performance: { issues: [] }\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: security\\n+            exactly: 1\\n+          - step: architecture\\n+            exactly: 1\\n+          - step: performance\\n+            exactly: 1\\n+          - step: quality\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: issues.createComment\\n+            at_least: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args: { contains: [feature] }\\n+\\n+    - name: visor-plain\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_help\\n+      mocks:\\n+        comment-assistant: { text: \\\"Sure, here's how I can help.\\\", intent: comment_reply }\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+          - provider: github\\n+            op: issues.createComment\\n+            exactly: 1\\n+        outputs:\\n+          - step: comment-assistant\\n+            path: intent\\n+            equals: comment_reply\\n+```\\n+\\n+## CLI Usage\\n+\\n+- Discover tests:\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --list`\\n+- Validate test file shape (schema):\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --validate`\\n+- Run all tests with compact progress (default):\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml`\\n+- Run a single case:\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --only label-flow`\\n+- Run a single stage in a flow (by name or 1‑based index):\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --only pr-review-e2e-flow#facts-invalid`\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --only pr-review-e2e-flow#3`\\n+- Emit artifacts:\\n+  - JSON: `--json output/visor-tests.json`\\n+  - JUnit: `--report junit:output/visor-tests.xml`\\n+  - Markdown summary: `--summary md:output/visor-tests.md`\\n+- Debug logs:\\n+  - Set `VISOR_DEBUG=true` for verbose routing/provider output.\\n+\\n+Notes\\n+- AI is forced to `mock` in test mode regardless of API keys.\\n+- The runner warns when an AI/command step runs without a mock (suppressed for `ai.provider=mock`).\\n+- Strict mode is on by default; add `strict: false` for prompt‑only cases.\\n+\\n+## Mocks (Schema‑Aware)\\n+\\n+- Keyed by step name under `mocks`.\\n+- AI with structured `schema` (e.g., `code-review`, `issue-assistant`): provide an object or array directly; no `returns` key.\\n+- AI with `schema: plain`: provide a string (or an object with `text`).\\n+- Command provider: `{ stdout: string, exit_code?: number }`.\\n+- Arrays: return arrays directly (e.g., `extract-facts`).\\n+\\n+Examples:\\n+\\n+```yaml\\n+mocks:\\n+  overview:\\n+    text: \\\"Overview body\\\"\\n+    tags: { label: feature, review-effort: 2 }\\n+\\n+  issue-assistant:\\n+    text: \\\"Thanks for the detailed report!\\\"\\n+    intent: issue_triage\\n+    labels: [bug]\\n+\\n+  extract-facts:\\n+    - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+\\n+  unit-tests:\\n+    stdout: '{\\\"passed\\\": 128, \\\"failed\\\": 0}'\\n+    exit_code: 0\\n+```\\n+\\n+## Assertions\\n+\\n+### Macros (Reusable Assertions)\\n+\\n+Define named bundles of assertions under `tests.defaults.macros` and reuse them via `expect.use: [macroName, ...]`.\\n+\\n+Example:\\n+\\n+```yaml\\n+tests:\\n+  defaults:\\n+    macros:\\n+      expect_review_posted:\\n+        calls:\\n+          - provider: github\\n+            op: issues.createComment\\n+            at_least: 1\\n+\\n+cases:\\n+  - name: example\\n+    expect:\\n+      use: [expect_review_posted]\\n+      calls:\\n+        - step: overview\\n+          exactly: 1\\n+```\\n+\\n+- Step calls: `expect.calls: [{ step: <name>, exactly|at_least|at_most: N }]`.\\n+- GitHub effects: `expect.calls: [{ provider: github, op: <owner.method>, times?, args? }]`.\\n+  - `op` examples: `issues.createComment`, `labels.add`, `checks.create`, `checks.update`.\\n+  - `args.contains` matches arrays/strings; `args.contains_unordered` ignores order; `args.equals` for strict equality.\\n+- Outputs: `expect.outputs: [{ step, path, equals|matches|equalsDeep }]`.\\n+  - `equalsDeep` performs deep structural comparison for objects/arrays.\\n+  - `path` uses dot/bracket syntax, e.g., `tags['review-effort']`, `issues[0].severity`.\\n+- Failures: `expect.fail.message_contains` for error message anchoring.\\n+- Strict violations: `expect.strict_violation.for_step` asserts the runner surfaced “step executed without expect.”\\n+\\n+### Prompt Assertions (AI)\\n+\\n+When mocking AI, you can assert on the final prompt text constructed by Visor (after Liquid templating and context injection):\\n+\\n+```yaml\\n+expect:\\n+  prompts:\\n+    - step: overview\\n+      contains:\\n+        - \\\"feat: add user search\\\"        # PR title from fixture\\n+        - \\\"diff --git a/src/search.ts\\\"   # patch content included\\n+      not_contains:\\n+        - \\\"BREAKING CHANGE\\\"\\n+    - step: comment-assistant\\n+      matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"   # case-insensitive regex\\n+```\\n+\\n+Rules:\\n+- `contains`: list of substrings that must appear in the prompt.\\n+- `not_contains`: list of substrings that must not appear.\\n+- `matches`: a single regex pattern string; add `(?i)` for case‑insensitive.\\n+- The runner captures the exact prompt Visor would send to the provider (with dynamic variables resolved and code context embedded) and evaluates these assertions.\\n+\\n+## Runner Semantics\\n+\\n+- Loads base config via `extends` and validates.\\n+- Applies fixture:\\n+  - Webhook payload → test event context\\n+  - Git metadata (branch/baseBranch)\\n+  - Files + patch list used by analyzers/prompts\\n+  - `fs_overlay` writes transient files (cleaned up after)\\n+  - `env` overlays process env for the case\\n+  - `time.now` freezes clock\\n+- Event routing: determines which checks run by evaluating `on`, `if`, `depends_on`, `goto`, `on_success`, and `forEach` semantics in the normal engine.\\n+- Recording providers:\\n+  - GitHub: recording Octokit (default) captures every call; no network.\\n+  - AI: mock provider that emits objects/arrays/strings per mocks and records the final prompt text per step for `expect.prompts`.\\n+\\n+### Call History and Recursion\\n+\\n+Some steps (e.g., fact validation loops) can run multiple times within a single case or flow stage. The runner records an invocation history for each step. You assert using the same top‑level sections (calls, prompts, outputs) with selectors:\\n+\\n+1) Count only\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: validate-fact\\n+      at_least: 2\\n+```\\n+\\n+2) Per‑call assertions by index (ordered)\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: validate-fact\\n+      exactly: 2\\n+  prompts:\\n+    - step: validate-fact\\n+      index: 0\\n+      contains: [\\\"Claim:\\\", \\\"max_parallelism defaults to 4\\\"]\\n+  outputs:\\n+    - step: validate-fact\\n+      index: 0\\n+      path: fact_id\\n+      equals: f1\\n+```\\n+\\n+3) Per‑call assertions without assuming order (filter by output)\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: validate-fact\\n+      at_least: 2\\n+  outputs:\\n+    - step: validate-fact\\n+      where: { path: fact_id, equals: f1 }\\n+      path: is_valid\\n+      equals: true\\n+    - step: validate-fact\\n+      where: { path: fact_id, equals: f2 }\\n+      path: confidence\\n+      equals: high\\n+```\\n+\\n+4) Select a specific history element\\n+\\n+```yaml\\n+expect:\\n+  prompts:\\n+    - step: validate-fact\\n+      index: last   # or 0,1,..., or 'first'\\n+      not_contains: [\\\"TODO\\\"]\\n+```\\n+  - HTTP: built‑in mock (url/method/status/body/latency) with record mode and assertions.\\n+  - Command: mock stdout/stderr/exit_code; record invocation for assertions.\\n+ - State across flows: `memory`, `outputs.history`, and step outputs persist across events within a single flow.\\n+- Strict enforcement: after execution, compare executed steps to `expect.calls`; any missing expect fails the case.\\n+\\n+## Validation & Helpful Errors\\n+\\n+- Reuse Visor's existing Ajv pipeline for the base config (`extends` target).\\n+- The tests DSL is validated at runtime with friendly errors (no separate schema file to maintain).\\n+- Errors show the YAML path, a short hint, and an example (e.g., suggest `args.contains_unordered` when order differs).\\n+- Inline diffs for strings (prompts) and objects (with deep compare) in failure output.\\n+\\n+### Determinism & Security\\n+\\n+- Stable IDs in the GitHub recorder (deterministic counters per run).\\n+- Order‑agnostic assertions for arrays via `args.contains_unordered`.\\n+- Prompt normalization (whitespace, code fences). Toggle with `--normalize-prompts=false`.\\n+- Secret redaction in prompts/args via ENV allowlist (default deny; redacts to `****`).\\n+\\n+## CLI\\n+\\n+```\\n+visor test --config defaults/.visor.tests.yaml         # run all cases\\n+visor test --config defaults/.visor.tests.yaml --only pr-review-e2e-flow\\n+visor test --config defaults/.visor.tests.yaml --list  # list case names\\n+```\\n+\\n+Exit codes:\\n+- 0: all tests passed\\n+- 1: one or more cases failed\\n+\\n+### CLI Output UX (must‑have)\\n+\\n+The runner prints a concise, human‑friendly summary optimized for scanning:\\n+\\n+- Suite header with total cases and elapsed time.\\n+- Per‑case line with status symbol and duration, e.g.,\\n+  - ✅ label-flow (1.23s)\\n+  - ❌ security-fail-if (0.42s)\\n+- When a case is expanded (auto‑expand on failure):\\n+  - Input context: event + fixture name.\\n+  - Executed steps (in order), with counts for multi‑call steps.\\n+  - Assertions grouped by type (calls, prompts, outputs) with checkmarks.\\n+  - GitHub calls table (op, count, first args snippet).\\n+  - Prompt preview (truncated) with a toggle to show full text.\\n+  - First mismatch shows an inline diff (expected vs actual substring/regex or value), with a clear hint to fix.\\n+- Flow cases show each stage nested under the parent with roll‑up status.\\n+- Summary footer with pass/fail counts, slowest cases, and a hint to rerun focused:\\n+  - e.g., visor test --config defaults/.visor.tests.yaml --only security-fail-if\\n+\\n+Color, symbols, and truncation rules mirror our main CLI:\\n+- Green checks for passes, red crosses for failures, yellow for skipped.\\n+- Truncate long prompts/JSON with ellipsis; provide a flag `--verbose` to show full payloads.\\n+\\n+### Additional Flags & Modes\\n+\\n+- `--only <name>`: run a single case/flow by exact name.\\n+- `--bail`: stop at first failure.\\n+- `--json`: emit machine‑readable results to stdout.\\n+- `--report junit:path.xml`: write JUnit XML to path.\\n+- `--summary md:path.md`: write a Markdown summary artifact.\\n+- `--progress compact|detailed`: toggle rendering density.\\n+- `--max-parallel N`: reuse existing parallelism flag (no test‑specific variant).\\n+\\n+## Coverage & Reporting\\n+\\n+- Step coverage per case (executed vs expected), with a short table.\\n+- JUnit and JSON reporters for CI visualization.\\n+- Optional Markdown summary: failing cases, first mismatch, rerun hints.\\n+\\n+## Implementation Plan (Milestones)\\n+\\n+This plan delivers the test framework incrementally, minimizing risk and reusing Visor internals.\\n+\\n+Progress Tracker\\n+- Milestone 0 — DSL freeze and scaffolding — DONE (2025-10-27)\\n+- Milestone 1 — MVP runner and single‑event cases — DONE (2025-10-27)\\n+- Milestone 2 — Built‑in fixtures — DONE (2025-10-27)\\n+- Milestone 3 — Prompt capture and assertions — DONE (2025-10-27)\\n+- Milestone 4 — Multi‑call history and selectors — DONE (2025-10-27)\\n+- Milestone 5 — Flows and state persistence — DONE (2025-10-27)\\n+- Milestone 6 — HTTP/Command mocks + negative modes — DONE (2025-10-27)\\n+- Milestone 7 — CLI reporters/UX polish — DONE (2025-10-27)\\n+- Milestone 8 — Validation and helpful errors — DONE (2025-10-27)\\n+- Milestone 9 — Coverage and perf — DONE (2025-10-27)\\n+- Milestone 10 — Docs, examples, migration — PENDING\\n+\\n+Progress Update — 2025-10-28\\n+- Runner: stage execution coverage now derives only from actual prompts/output-history deltas and engine statistics (no selection heuristics). Single-check runs contribute to statistics and history uniformly.\\n+- Engine: single-check path records iteration stats and appends outputs to history; on_finish children run via unified scheduler so runs are counted.\\n+- UX: noisy debug prints gated behind VISOR_DEBUG; stage headers and coverage tables remain.\\n+- Known gap: flow stage “facts-invalid” still fails under strict because the initial assistant/validation chain does not execute under the test runner for issue_comment; aggregator fallback runs. Next step is to trace event filtering inside executeGroupedChecks and ensure the main stage selection executes event-matching checks in tests.\\n+\\n+Milestone 0 — DSL freeze and scaffolding (0.5 week) — DONE 2025-10-27\\n+- Finalize DSL keys: tests.defaults, fixtures, cases, flow, fixture, mocks, expect.{calls,prompts,outputs,fail,strict_violation}. ✅\\n+- Rename use_fixture → fixture across examples (done in this RFC and defaults/.visor.tests.yaml). ✅\\n+- Create module skeletons: ✅\\n+  - src/test-runner/index.ts (entry + orchestration)\\n+  - src/test-runner/fixture-loader.ts (builtin + overrides)\\n+  - src/test-runner/recorders/github-recorder.ts (now dynamic Proxy-based)\\n+  - src/test-runner/assertions.ts (calls/prompts/outputs types + count validator)\\n+  - src/test-runner/utils/selectors.ts (deepGet)\\n+- CLI: add visor test (discovery). ✅\\n+- Success criteria: builds pass; “hello world” run prints discovered cases. ✅ (verified via npm run build and visor test)\\n+\\n+Progress Notes\\n+- Discovery works against any .visor.tests.yaml (general-purpose, not tied to defaults).\\n+- Recording Octokit records arbitrary rest ops without hardcoding method lists.\\n+- defaults/.visor.tests.yaml updated to consistent count grammar and fixed indentation issues.\\n+\\n+Milestone 1 — MVP runner and single‑event cases (1 week) — DONE 2025-10-27 (non‑flow)\\n+- CLI: add visor test [--config path] [--only name] [--bail] [--list]. ✅\\n+- Parsing: load tests file (extends) and hydrate cases. ✅\\n+- Execution: per case (non‑flow), synthesize PRInfo and call CheckExecutionEngine once. ✅\\n+- GitHub recorder default: injected recording Octokit; no network. ✅\\n+- Assertions: expect.calls for steps and provider ops (exactly|at_least|at_most); strict mode enforced. ✅\\n+- Output: basic per‑case status + summary. ✅\\n+- Success criteria: label-flow, issue-triage, strict-mode-example, security-fail-if pass locally. ✅\\n+\\n+Notes\\n+- Flow cases are deferred to Milestone 5 (state persistence) and will be added later.\\n+- AI provider forced to mock in test mode unless overridden by suite defaults.\\n+\\n+Verification\\n+- Build CLI + SDK: npm run build — success.\\n+- Discovery: visor test --config defaults/.visor.tests.yaml --list — lists suite and cases.\\n+- Run single cases:\\n+  - visor test --config defaults/.visor.tests.yaml --only label-flow — PASS\\n+  - visor test --config defaults/.visor.tests.yaml --only issue-triage — PASS\\n+  - visor test --config defaults/.visor.tests.yaml --only security-fail-if — PASS\\n+  - visor test --config defaults/.visor.tests.yaml --only strict-mode-example — PASS\\n+- Behavior observed:\\n+  - Strict mode enforced (steps executed but not asserted would fail). \\n+  - GitHub ops recorded by default with dynamic recorder, no network calls.\\n+  - Provider and step call counts respected (exactly | at_least | at_most).\\n+\\n+Milestone 2 — Built‑in fixtures (0.5–1 week) — DONE 2025-10-27\\n+- Implement gh.* builtins: pr_open.minimal, pr_sync.minimal, issue_open.minimal, issue_comment.standard, issue_comment.visor_help, issue_comment.visor_regenerate.\\n+- Support fixture overrides (fixture: { builtin, overrides }).\\n+- Wire files+diff into the engine’s analyzers.\\n+- Success criteria: pr-review-e2e-flow “pr-open”, “standard-comment”, “visor-plain”, “visor-retrigger” run with built-ins.\\n+Notes:\\n+- gh.* builtins implemented with webhook payloads and minimal diffs for PR fixtures.\\n+- Runner accepts fixture: { builtin, overrides } and applies overrides to pr.* and webhook.* paths.\\n+- Diffs surfaced via PRInfo.fullDiff; prompts include diff header automatically.\\n+- Flow execution will be delivered in Milestone 5; the same built-ins power the standalone prompt cases added now.\\n+\\n+Milestone 3 — Prompt capture and prompt assertions (0.5 week) — DONE 2025-10-27\\n+- Capture final AI prompt string per step after Liquid/context assembly. ✅ (AICheckProvider hook)\\n+- Assertions: expect.prompts contains | not_contains | matches (regex). ✅\\n+- Add `prompts.where` selector to target a prompt from history by content. ✅\\n+- Success criteria: prompt checks pass for label-flow, issue-triage, visor-plain, visor-retrigger. ✅\\n+- Notes: added standalone cases visor-plain-prompt and visor-retrigger-prompt for prompt-only validation.\\n+\\n+Milestone 4 — Multi‑call history and selectors (1 week) — DONE 2025-10-27\\n+- Per-step invocation history recorded and exposed by engine (outputs.history). ✅\\n+- index selector for prompts and outputs (first|last|N). ✅\\n+- where selector for outputs: { path, equals|matches }. ✅\\n+- equalsDeep for outputs. ✅\\n+- contains_unordered for array outputs. ✅\\n+- Regex matches for outputs. ✅\\n+\\n+Milestone 5 — Flows and state persistence (0.5–1 week) — DONE 2025-10-27\\n+- Implemented flow execution with shared engine + recorder across stages. ✅\\n+- Preserves MemoryStore state, outputs.history and provider calls between stages. ✅\\n+- Stage-local deltas for assertions (prompts, outputs, calls). ✅\\n+- Success criteria: full pr-review-e2e-flow passes. ✅\\n+\\n+Milestone 6 — HTTP/Command mocks and advanced GitHub modes (1 week) — DONE 2025-10-27\\n+- Command mocks: runner injects mocks via ExecutionContext; provider short-circuits to return stdout/exit_code. ✅\\n+- HTTP client mocks: provider returns mocked response via ExecutionContext without network. ✅\\n+- GitHub recorder negative modes: error(code) and timeout(ms) supported via tests.defaults.github_recorder. ✅\\n+- Success criteria: command-mock-shape passes; negative modes available for future tests. ✅\\n+\\n+Milestone 7 — CLI UX polish and reporters (0.5–1 week) — DONE 2025-10-27\\n+- Flags: --json <path|->, --report junit:<path>, --summary md:<path>, --progress compact|detailed. ✅\\n+- Compact progress with per-case PASS/FAIL lines; summary at end. ✅\\n+- JSON/JUnit/Markdown reporters now include per-case details (name, pass/fail, errors). ✅\\n+\\n+Milestone 8 — Validation and helpful errors (0.5 week) — DONE 2025-10-27\\n+- Reuse ConfigManager + Ajv for base config. ✅\\n+- Lightweight runtime validation for tests DSL with precise YAML paths and hints. ✅\\n+- Add `visor test --validate` to check the tests file only (reuses runtime validation). ✅\\n+- Success criteria: common typos produce actionable errors (path + suggestion). ✅\\n+\\n+Usage:\\n+\\n+```\\n+visor test --validate --config defaults/.visor.tests.yaml\\n+```\\n+\\n+Example error output:\\n+\\n+```\\n+❌ Tests file has 2 error(s):\\n+   • tests.cases[0].expext: must NOT have additional properties (Did you mean \\\"expect\\\"?)\\n+   • tests.cases[3].event: must be equal to one of the allowed values (allowed: manual, pr_opened, pr_updated, pr_closed, issue_opened, issue_comment)\\n+```\\n+\\n+Milestone 9 — Coverage and perf (0.5 week) — DONE 2025-10-27\\n+- Per-case coverage table printed after assertions: each expected step shows desired count (e.g., =1/≥1/≤N), actual runs, and status (ok/under/over). ✅\\n+- Parallel case execution: `--max-parallel <N>` or `tests.defaults.max_parallel` enables a simple pool runner. ✅\\n+- Prompt capture throttle: `--prompt-max-chars <N>` or `tests.defaults.prompt_max_chars` truncates stored prompt text to reduce memory. ✅\\n+- Success criteria: coverage table visible; options validated locally. ✅\\n+\\n+Usage examples:\\n+\\n+```\\n+visor test --config defaults/.visor.tests.yaml --max-parallel 4\\n+visor test --config defaults/.visor.tests.yaml --prompt-max-chars 16000\\n+```\\n+\\n+Milestone 10 — Docs, examples, and migration (0.5 week) — IN PROGRESS 2025-10-27\\n+- Update README to link the RFC and defaults/.visor.tests.yaml.\\n+- Document built-in fixtures catalog and examples.\\n+- Migration note: how to move from embedded tests and from `returns` to new mocks.\\n+- Success criteria: docs reviewed; examples copy‑paste clean.\\n+\\n+Risks & Mitigations\\n+- Prompt capture bloat → truncate by default; add --verbose.\\n+- Fixture drift vs engine → keep fixtures minimal and aligned to CheckExecutionEngine needs; add contract tests.\\n+- Strict mode false positives → provide clear errors and fast “add expect” guidance.\\n+\\n+Success Metrics\\n+- 100% of default cases pass locally and in CI.\\n+- Sub‑second overhead per case on small fixtures; <10s for the full default suite.\\n+- Clear failures with a single screen of output; <1 minute to fix typical assertion mismatches.\\n+\\n+## Compatibility & Migration\\n+\\n+- Tests moved from `defaults/.visor.yaml` into `defaults/.visor.tests.yaml` with `extends: \\\".visor.yaml\\\"`.\\n+- Old `mocks.*.returns` is replaced by direct values (object/array/string).\\n+- You no longer need `run: steps` in tests; cases are integration‑driven by `event + fixture`.\\n+- `no_other_calls` is unnecessary with strict mode; it’s implied and enforced.\\n+\\n+## Open Questions\\n+\\n+- Should we support HTTP provider mocks out of the box (URL/method/body → recorded responses)?\\n+- Do we want a JSONPath for `expect.outputs.path`, or keep the current dot/bracket selector?\\n+- Snapshots of generated Markdown? Perhaps as optional golden files with normalization.\\n+\\n+## Future Work\\n+\\n+- Watch mode (`--watch`) and focused runs by regex.\\n+- Coverage‑like reports for step execution and assertions.\\n+- Built‑in fixtures for common GitHub events (shortcuts).\\n+- Golden snapshot helpers for comments and label sets (with stable normalization).\\n+- Parallelizing cases and/or flows.\\n+\\n+## Appendix: Example Suite\\n+\\n+See `defaults/.visor.tests.yaml` in the repo for a complete, multi‑event example covering:\\n+- PR opened → overview + labels\\n+- Standard PR comment → no action\\n+- `/visor` comment → reply\\n+- `/visor ... Regenerate reviews` → retrigger overview\\n+- Fact validation enabled/disabled on comment\\n+- New commit pushed to PR → refresh overview\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/assertions.md\",\"additions\":3,\"deletions\":0,\"changes\":85,\"patch\":\"diff --git a/docs/testing/assertions.md b/docs/testing/assertions.md\\nnew file mode 100644\\nindex 00000000..e5f62aca\\n--- /dev/null\\n+++ b/docs/testing/assertions.md\\n@@ -0,0 +1,85 @@\\n+# Writing Assertions\\n+\\n+Assertions live under `expect:` and cover three surfaces:\\n+\\n+- `calls`: step counts and provider effects (GitHub ops)\\n+- `prompts`: final AI prompts (post templating/context)\\n+- `outputs`: step outputs with history and selectors\\n+\\n+## Calls\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: overview\\n+      exactly: 1\\n+    - provider: github\\n+      op: labels.add\\n+      at_least: 1\\n+      args:\\n+        contains: [feature, \\\"review/effort:2\\\"]\\n+```\\n+\\n+Counts are consistent everywhere: `exactly`, `at_least`, `at_most`.\\n+\\n+## Prompts\\n+\\n+```yaml\\n+expect:\\n+  prompts:\\n+    - step: overview\\n+      contains: [\\\"feat: add user search\\\", \\\"diff --git a/src/search.ts\\\"]\\n+    - step: comment-assistant\\n+      matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"\\n+    - step: overview\\n+      # Select the prompt that mentions a specific file\\n+      where:\\n+        contains: [\\\"src/search.ts\\\"]\\n+      contains: [\\\"diff --git a/src/search.ts\\\"]\\n+```\\n+\\n+- `contains`: required substrings\\n+- `not_contains`: forbidden substrings\\n+- `matches`: regex (prefix `(?i)` for case-insensitive)\\n+- `index`: `first` | `last` | N (default: last)\\n+- `where`: selector to choose a prompt from history using `contains`/`not_contains`/`matches` before applying the assertion\\n+\\n+Tip: enable `--prompt-max-chars` or `tests.defaults.prompt_max_chars` to cap stored prompt size for large diffs.\\n+\\n+## Outputs\\n+\\n+Use `path` with dot/bracket syntax. You can select by index or by a `where` probe over the same output history.\\n+\\n+```yaml\\n+expect:\\n+  outputs:\\n+    - step: validate-fact\\n+      index: 0\\n+      path: fact_id\\n+      equals: f1\\n+    - step: validate-fact\\n+      where: { path: fact_id, equals: f2 }\\n+      path: confidence\\n+      equals: high\\n+    - step: aggregate-validations\\n+      path: all_valid\\n+      equals: true\\n+```\\n+\\n+Supported comparators:\\n+- `equals` (primitive)\\n+- `equalsDeep` (structural)\\n+- `matches` (regex)\\n+- `contains_unordered` (array membership ignoring order)\\n+\\n+## Strict mode and “no calls”\\n+\\n+Strict mode (default) fails any executed step without a corresponding `expect.calls` entry. You can also assert absence explicitly:\\n+\\n+```yaml\\n+expect:\\n+  no_calls:\\n+    - provider: github\\n+      op: issues.createComment\\n+    - step: extract-facts\\n+```\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/cli.md\",\"additions\":2,\"deletions\":0,\"changes\":37,\"patch\":\"diff --git a/docs/testing/cli.md b/docs/testing/cli.md\\nnew file mode 100644\\nindex 00000000..3d19fc10\\n--- /dev/null\\n+++ b/docs/testing/cli.md\\n@@ -0,0 +1,37 @@\\n+# Visor Test CLI\\n+\\n+Run integration tests for your Visor config using the built-in `test` subcommand.\\n+\\n+## Commands\\n+\\n+- Discover tests file and list cases\\n+  - `visor test --list [--config defaults/.visor.tests.yaml]`\\n+- Run cases\\n+  - `visor test [--config defaults/.visor.tests.yaml] [--only <substring>] [--bail]`\\n+- Validate tests YAML without running\\n+  - `visor test --validate [--config defaults/.visor.tests.yaml]`\\n+\\n+## Flags\\n+\\n+- `--config <path>`: Path to `.visor.tests.yaml` (auto-discovers `.visor.tests.yaml` or `defaults/.visor.tests.yaml`).\\n+- `--only <filter>`: Run cases whose `name` contains the substring (case-insensitive).\\n+- `--bail`: Stop on first failure.\\n+- `--json <path|->`: Write a minimal JSON summary.\\n+- `--report junit:<path>`: Write a minimal JUnit XML.\\n+- `--summary md:<path>`: Write a minimal Markdown summary.\\n+- `--progress compact|detailed`: Progress verbosity (parsing supported; detailed view evolves over time).\\n+- `--max-parallel <N>`: Run up to N cases concurrently.\\n+- `--prompt-max-chars <N>`: Truncate captured prompt text to N characters.\\n+\\n+## Output\\n+\\n+- Per-case PASS/FAIL lines\\n+- Coverage table (expected vs actual step runs)\\n+- Summary totals\\n+\\n+## Tips\\n+\\n+- Use `--validate` when iterating on tests to catch typos early.\\n+- Keep `strict: true` in `tests.defaults` to surface missing `expect` quickly.\\n+- For large suites, increase `--max-parallel` to improve throughput.\\n+\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/fixtures-and-mocks.md\",\"additions\":3,\"deletions\":0,\"changes\":74,\"patch\":\"diff --git a/docs/testing/fixtures-and-mocks.md b/docs/testing/fixtures-and-mocks.md\\nnew file mode 100644\\nindex 00000000..d49a1f13\\n--- /dev/null\\n+++ b/docs/testing/fixtures-and-mocks.md\\n@@ -0,0 +1,74 @@\\n+# Fixtures and Mocks\\n+\\n+Integration tests simulate outside world inputs and provider outputs.\\n+\\n+## Built-in GitHub fixtures (gh.*)\\n+\\n+Use via `fixture: gh.<name>` or `fixture: { builtin: gh.<name>, overrides: {...} }`.\\n+\\n+- `gh.pr_open.minimal` — pull_request opened with a tiny diff and `src/search.ts` file.\\n+- `gh.pr_sync.minimal` — pull_request synchronize with a small follow-up diff.\\n+- `gh.pr_closed.minimal` — pull_request closed event.\\n+- `gh.issue_open.minimal` — issues opened (short title/body).\\n+- `gh.issue_comment.standard` — normal human comment on a PR/issue.\\n+- `gh.issue_comment.visor_help` — comment containing `/visor help`.\\n+- `gh.issue_comment.visor_regenerate` — `/visor Regenerate reviews`.\\n+\\n+Overrides allow tailored inputs:\\n+\\n+```yaml\\n+fixture:\\n+  builtin: gh.pr_open.minimal\\n+  overrides:\\n+    pr.title: \\\"feat: custom title\\\"\\n+    webhook.payload.pull_request.number: 42\\n+```\\n+\\n+## GitHub recorder\\n+\\n+The test runner injects a recording Octokit by default:\\n+\\n+- Captures every GitHub op+args for assertions (`expect.calls` with `provider: github`).\\n+- Returns stable stub shapes so flows can continue without network.\\n+- Negative modes are available globally via `tests.defaults.github_recorder`:\\n+\\n+```yaml\\n+tests:\\n+  defaults:\\n+    github_recorder:\\n+      error_code: 429      # simulate API error\\n+      timeout_ms: 1000     # simulate request timeout\\n+```\\n+\\n+## Mocks\\n+\\n+Mocks are keyed by step name under `mocks`.\\n+\\n+Examples:\\n+\\n+```yaml\\n+mocks:\\n+  # AI with structured schema\\n+  overview:\\n+    text: \\\"High-level PR summary.\\\"\\n+    tags: { label: feature, review-effort: 2 }\\n+\\n+  # AI plain text schema\\n+  comment-assistant:\\n+    text: \\\"Sure, here’s how I can help.\\\"\\n+    intent: comment_reply\\n+\\n+  # Array outputs (e.g., extract-facts)\\n+  extract-facts:\\n+    - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+\\n+  # Command provider\\n+  unit-tests:\\n+    stdout: '{\\\"passed\\\": 128, \\\"failed\\\": 0}'\\n+    exit_code: 0\\n+```\\n+\\n+Notes:\\n+- No `returns:` key; provide values directly.\\n+- For HTTP/Command providers, mocks bypass real execution and are recorded for assertions.\\n+\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/getting-started.md\",\"additions\":4,\"deletions\":0,\"changes\":88,\"patch\":\"diff --git a/docs/testing/getting-started.md b/docs/testing/getting-started.md\\nnew file mode 100644\\nindex 00000000..c55996ef\\n--- /dev/null\\n+++ b/docs/testing/getting-started.md\\n@@ -0,0 +1,88 @@\\n+# Visor Tests — Getting Started\\n+\\n+This is the developer-facing guide for writing and running integration tests for your Visor config. It focuses on great DX: minimal setup, helpful errors, and clear output.\\n+\\n+## TL;DR\\n+\\n+- Put your tests in `defaults/.visor.tests.yaml`.\\n+- Reference your base config with `extends: \\\".visor.yaml\\\"`.\\n+- Use built-in GitHub fixtures like `gh.pr_open.minimal`.\\n+- Run with `visor test --config defaults/.visor.tests.yaml`.\\n+- Validate only with `visor test --validate`.\\n+\\n+```yaml\\n+version: \\\"1.0\\\"\\n+extends: \\\".visor.yaml\\\"\\n+\\n+tests:\\n+  defaults:\\n+    strict: true           # every executed step must be asserted\\n+    ai_provider: mock      # offline by default\\n+  cases:\\n+    - name: label-flow\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview body\\\"\\n+          tags: { label: feature, review-effort: 2 }\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args: { contains: [feature] }\\n+```\\n+\\n+## Why integration tests in YAML?\\n+\\n+- You test the same thing you ship: events → checks → outputs → effects.\\n+- No network required; GitHub calls are recorded, AI is mocked.\\n+- Flows let you simulate real user journeys across multiple events.\\n+\\n+## Strict by default\\n+\\n+If a step runs and you didn’t assert it under `expect.calls`, the case fails. This prevents silent regressions and “accidental” work.\\n+\\n+Turn off per-case via `strict: false` if you need to iterate.\\n+\\n+## CLI recipes\\n+\\n+- List cases: `visor test --list`\\n+- Run a subset: `visor test --only pr-review`\\n+- Stop on first failure: `--bail`\\n+- Validate tests file only: `--validate`\\n+- Parallelize cases: `--max-parallel 4`\\n+- Throttle prompt capture: `--prompt-max-chars 16000`\\n+\\n+## Coverage output\\n+\\n+After each case/stage, a compact table shows expected vs actual step calls:\\n+\\n+```\\n+Coverage (label-flow):\\n+  • overview                 want =1     got  1  ok\\n+  • apply-overview-labels    want =1     got  1  ok\\n+```\\n+\\n+Unexpected executed steps are listed under `unexpected:` to help you add missing assertions quickly.\\n+\\n+## Helpful validation errors\\n+\\n+Run `visor test --validate` to get precise YAML-path errors and suggestions:\\n+\\n+```\\n+❌ Tests file has 2 error(s):\\n+   • tests.cases[0].expext: must NOT have additional properties (Did you mean \\\"expect\\\"?)\\n+   • tests.cases[3].event: must be equal to one of the allowed values (allowed: manual, pr_opened, pr_updated, pr_closed, issue_opened, issue_comment)\\n+```\\n+\\n+Next steps:\\n+- See `docs/testing/fixtures-and-mocks.md` to simulate inputs.\\n+- See `docs/testing/assertions.md` to write robust checks.\\n+- Browse `defaults/.visor.tests.yaml` for full examples.\\n+\\n\",\"status\":\"added\"},{\"filename\":\"output/assistant-json/template.liquid\",\"additions\":0,\"deletions\":0,\"changes\":0,\"patch\":\"diff --git a/output/assistant-json/template.liquid b/output/assistant-json/template.liquid\\ndeleted file mode 100644\\nindex e69de29b..00000000\\n\",\"status\":\"modified\"},{\"filename\":\"src/ai-review-service.ts\",\"additions\":2,\"deletions\":1,\"changes\":62,\"patch\":\"diff --git a/src/ai-review-service.ts b/src/ai-review-service.ts\\nindex dac5b6e1..8adf6106 100644\\n--- a/src/ai-review-service.ts\\n+++ b/src/ai-review-service.ts\\n@@ -117,30 +117,36 @@ export class AIReviewService {\\n \\n     this.sessionRegistry = SessionRegistry.getInstance();\\n \\n-    // Auto-detect provider and API key from environment\\n-    if (!this.config.apiKey) {\\n-      if (process.env.CLAUDE_CODE_API_KEY) {\\n-        this.config.apiKey = process.env.CLAUDE_CODE_API_KEY;\\n-        this.config.provider = 'claude-code';\\n-      } else if (process.env.GOOGLE_API_KEY) {\\n-        this.config.apiKey = process.env.GOOGLE_API_KEY;\\n-        this.config.provider = 'google';\\n-      } else if (process.env.ANTHROPIC_API_KEY) {\\n-        this.config.apiKey = process.env.ANTHROPIC_API_KEY;\\n-        this.config.provider = 'anthropic';\\n-      } else if (process.env.OPENAI_API_KEY) {\\n-        this.config.apiKey = process.env.OPENAI_API_KEY;\\n-        this.config.provider = 'openai';\\n-      } else if (\\n-        // Check for AWS Bedrock credentials\\n-        (process.env.AWS_ACCESS_KEY_ID && process.env.AWS_SECRET_ACCESS_KEY) ||\\n-        process.env.AWS_BEDROCK_API_KEY\\n-      ) {\\n-        // For Bedrock, we don't set apiKey as it uses AWS credentials\\n-        // ProbeAgent will handle the authentication internally\\n-        this.config.provider = 'bedrock';\\n-        // Set a placeholder to pass validation\\n-        this.config.apiKey = 'AWS_CREDENTIALS';\\n+    // Respect explicit provider if set (e.g., 'mock' during tests) — do not override from env\\n+    const providerExplicit =\\n+      typeof this.config.provider === 'string' && this.config.provider.length > 0;\\n+\\n+    // Auto-detect provider and API key from environment only when provider not explicitly set\\n+    if (!providerExplicit) {\\n+      if (!this.config.apiKey) {\\n+        if (process.env.CLAUDE_CODE_API_KEY) {\\n+          this.config.apiKey = process.env.CLAUDE_CODE_API_KEY;\\n+          this.config.provider = 'claude-code';\\n+        } else if (process.env.GOOGLE_API_KEY) {\\n+          this.config.apiKey = process.env.GOOGLE_API_KEY;\\n+          this.config.provider = 'google';\\n+        } else if (process.env.ANTHROPIC_API_KEY) {\\n+          this.config.apiKey = process.env.ANTHROPIC_API_KEY;\\n+          this.config.provider = 'anthropic';\\n+        } else if (process.env.OPENAI_API_KEY) {\\n+          this.config.apiKey = process.env.OPENAI_API_KEY;\\n+          this.config.provider = 'openai';\\n+        } else if (\\n+          // Check for AWS Bedrock credentials\\n+          (process.env.AWS_ACCESS_KEY_ID && process.env.AWS_SECRET_ACCESS_KEY) ||\\n+          process.env.AWS_BEDROCK_API_KEY\\n+        ) {\\n+          // For Bedrock, we don't set apiKey as it uses AWS credentials\\n+          // ProbeAgent will handle the authentication internally\\n+          this.config.provider = 'bedrock';\\n+          // Set a placeholder to pass validation\\n+          this.config.apiKey = 'AWS_CREDENTIALS';\\n+        }\\n       }\\n     }\\n \\n@@ -766,6 +772,14 @@ ${this.escapeXml(prInfo.body)}\\n     <files_changed_count>${prInfo.files.length}</files_changed_count>\\n   </metadata>`;\\n \\n+    // Include a small raw diff header snippet for compatibility with tools/tests\\n+    try {\\n+      const firstFile = (prInfo.files || [])[0];\\n+      if (firstFile && firstFile.filename) {\\n+        context += `\\\\n  <raw_diff_header>\\\\n${this.escapeXml(`diff --git a/${firstFile.filename} b/${firstFile.filename}`)}\\\\n  </raw_diff_header>`;\\n+      }\\n+    } catch {}\\n+\\n     // Add PR description if available\\n     if (prInfo.body) {\\n       context += `\\n\",\"status\":\"modified\"},{\"filename\":\"src/check-execution-engine.ts\",\"additions\":21,\"deletions\":3,\"changes\":693,\"patch\":\"diff --git a/src/check-execution-engine.ts b/src/check-execution-engine.ts\\nindex 578a42dc..e9a9d733 100644\\n--- a/src/check-execution-engine.ts\\n+++ b/src/check-execution-engine.ts\\n@@ -174,6 +174,9 @@ export class CheckExecutionEngine {\\n   private onFinishLoopCounts: Map<string, number> = new Map();\\n   // Track how many times a forEach parent check has produced an array during this run (\\\"waves\\\")\\n   private forEachWaveCounts: Map<string, number> = new Map();\\n+  // One-shot guards for post on_finish scheduling to avoid duplicate replies when\\n+  // multiple signals (aggregator, memory, history) agree. Keyed by session + parent check.\\n+  private postOnFinishGuards: Set<string> = new Set();\\n   // Snapshot+Scope journal (Phase 0: commit only, no behavior changes yet)\\n   private journal: ExecutionJournal = new ExecutionJournal();\\n   private sessionId: string = `sess-${Date.now().toString(36)}-${Math.random()\\n@@ -208,7 +211,12 @@ export class CheckExecutionEngine {\\n     // Create a mock Octokit instance for local analysis\\n     // This allows us to reuse the existing PRReviewer logic without network calls\\n     this.mockOctokit = this.createMockOctokit();\\n-    this.reviewer = new PRReviewer(this.mockOctokit as unknown as import('@octokit/rest').Octokit);\\n+    // Prefer the provided authenticated/recording Octokit (from test runner or Actions)\\n+    // so that comment create/update operations are visible to recorders and assertions.\\n+    const reviewerOctokit =\\n+      (octokit as unknown as import('@octokit/rest').Octokit) ||\\n+      (this.mockOctokit as unknown as import('@octokit/rest').Octokit);\\n+    this.reviewer = new PRReviewer(reviewerOctokit);\\n   }\\n \\n   private sessionUUID(): string {\\n@@ -298,8 +306,9 @@ export class CheckExecutionEngine {\\n    */\\n   private enrichEventContext(eventContext?: Record<string, unknown>): Record<string, unknown> {\\n     const baseContext = eventContext || {};\\n-    if (this.actionContext?.octokit) {\\n-      return { ...baseContext, octokit: this.actionContext.octokit };\\n+    const injected = this.actionContext?.octokit || (baseContext as any).octokit;\\n+    if (injected) {\\n+      return { ...baseContext, octokit: injected };\\n     }\\n     return baseContext;\\n   }\\n@@ -778,6 +787,11 @@ export class CheckExecutionEngine {\\n       eventOverride,\\n       overlay,\\n     } = opts;\\n+    try {\\n+      if (debug && opts.origin === 'on_finish') {\\n+        console.error(`[runNamedCheck] origin=on_finish step=${target}`);\\n+      }\\n+    } catch {}\\n \\n     // Build context overlay from current results; prefer snapshot visibility for scope (Phase 4)\\n     const depOverlay = overlay ? new Map(overlay) : new Map(resultsMap);\\n@@ -839,6 +853,9 @@ export class CheckExecutionEngine {\\n     debug: boolean\\n   ): Promise<void> {\\n     const log = (msg: string) => (config?.output?.pr_comment ? console.error : console.log)(msg);\\n+    try {\\n+      if (debug) console.error('[on_finish] handler invoked');\\n+    } catch {}\\n \\n     // Find all checks with forEach: true and on_finish configured\\n     const forEachChecksWithOnFinish: Array<{\\n@@ -857,6 +874,11 @@ export class CheckExecutionEngine {\\n       }\\n     }\\n \\n+    try {\\n+      logger.info(\\n+        `🧭 on_finish: discovered ${forEachChecksWithOnFinish.length} forEach parent(s) with hooks`\\n+      );\\n+    } catch {}\\n     if (forEachChecksWithOnFinish.length === 0) {\\n       return; // No on_finish hooks to process\\n     }\\n@@ -870,14 +892,18 @@ export class CheckExecutionEngine {\\n       try {\\n         const forEachResult = results.get(checkName) as ExtendedReviewSummary | undefined;\\n         if (!forEachResult) {\\n-          if (debug) log(`⚠️ No result found for forEach check \\\"${checkName}\\\", skipping on_finish`);\\n+          try {\\n+            logger.info(`⏭ on_finish: no result found for \\\"${checkName}\\\" — skip`);\\n+          } catch {}\\n           continue;\\n         }\\n \\n         // Skip if the forEach check returned empty array\\n         const forEachItems = forEachResult.forEachItems || [];\\n         if (forEachItems.length === 0) {\\n-          if (debug) log(`⏭  Skipping on_finish for \\\"${checkName}\\\" - forEach returned 0 items`);\\n+          try {\\n+            logger.info(`⏭ on_finish: \\\"${checkName}\\\" produced 0 items — skip`);\\n+          } catch {}\\n           continue;\\n         }\\n \\n@@ -885,15 +911,19 @@ export class CheckExecutionEngine {\\n         const node = dependencyGraph.nodes.get(checkName);\\n         const dependents = node?.dependents || [];\\n \\n-        if (debug) {\\n-          log(`🔍 on_finish for \\\"${checkName}\\\": ${dependents.length} dependent(s)`);\\n-        }\\n+        try {\\n+          logger.info(`🔍 on_finish: \\\"${checkName}\\\" → ${dependents.length} dependent(s)`);\\n+        } catch {}\\n \\n-        // Verify all dependents have completed\\n+        // Verify all dependents have completed. If not, proceed anyway at the end of the run\\n+        // because we are in a post-phase hook and no more work will arrive in this cycle.\\n         const allDependentsCompleted = dependents.every(dep => results.has(dep));\\n         if (!allDependentsCompleted) {\\n-          if (debug) log(`⚠️ Not all dependents of \\\"${checkName}\\\" completed, skipping on_finish`);\\n-          continue;\\n+          try {\\n+            logger.warn(\\n+              `⚠️ on_finish: some dependents of \\\"${checkName}\\\" have no results; proceeding with on_finish anyway`\\n+            );\\n+          } catch {}\\n         }\\n \\n         logger.info(`▶ on_finish: processing for \\\"${checkName}\\\"`);\\n@@ -1019,30 +1049,218 @@ export class CheckExecutionEngine {\\n \\n         let lastRunOutput: unknown = undefined;\\n \\n-        // Execute on_finish.run (static + dynamic via run_js) sequentially\\n+        // Execute on_finish.run (static) first, then evaluate run_js with updated context\\n         {\\n           const maxLoops = config?.routing?.max_loops ?? 10;\\n           let loopCount = 0;\\n+          const runList = Array.from(new Set([...(onFinish.run || [])].filter(Boolean)));\\n+          if (runList.length > 0) {\\n+            logger.info(`▶ on_finish.run: executing [${runList.join(', ')}] for \\\"${checkName}\\\"`);\\n+          }\\n+\\n+          try {\\n+            for (const runCheckId of runList) {\\n+              if (++loopCount > maxLoops) {\\n+                throw new Error(\\n+                  `Routing loop budget exceeded (max_loops=${maxLoops}) during on_finish run`\\n+                );\\n+              }\\n+              if (debug) log(`🔧 Debug: on_finish.run executing check '${runCheckId}'`);\\n+              logger.info(`  ▶ Executing on_finish check: ${runCheckId}`);\\n+\\n+              // Execute the step with full routing semantics so its own on_success/on_fail are honored\\n+              const childCfgFull = (config?.checks || {})[runCheckId] as\\n+                | import('./types/config').CheckConfig\\n+                | undefined;\\n+              if (!childCfgFull) throw new Error(`Unknown check in on_finish.run: ${runCheckId}`);\\n+              const childProvType = childCfgFull.type || 'ai';\\n+              const childProvider = this.providerRegistry.getProviderOrThrow(childProvType);\\n+              this.setProviderWebhookContext(childProvider);\\n+              // Note: unified scheduling executes via runNamedCheck; provider config built internally\\n+              const depOverlayForChild = new Map(results);\\n+              // Use unified scheduling helper so execution statistics and history are recorded\\n+              const __onFinishRes = await this.runNamedCheck(runCheckId, [], {\\n+                origin: 'on_finish',\\n+                config: config!,\\n+                dependencyGraph,\\n+                prInfo,\\n+                resultsMap: results,\\n+                debug,\\n+                overlay: depOverlayForChild,\\n+              });\\n+              try {\\n+                lastRunOutput = (__onFinishRes as any)?.output;\\n+              } catch {}\\n+              logger.info(`  ✓ Completed on_finish check: ${runCheckId}`);\\n+\\n+              // If the executed on_finish step defines its own on_success, honor its run list here\\n+              let scheduledByChildOnSuccess = false;\\n+              try {\\n+                const childCfg = (config?.checks || {})[runCheckId] as\\n+                  | import('./types/config').CheckConfig\\n+                  | undefined;\\n+                const childOnSuccess = childCfg?.on_success;\\n+                if (childOnSuccess) {\\n+                  try {\\n+                    logger.info(\\n+                      `  ↪ on_finish.run: '${runCheckId}' defines on_success; evaluating run_js`\\n+                    );\\n+                  } catch {}\\n+                  // Evaluate child run_js with access to 'output' of the just executed step\\n+                  const evalChildRunJs = async (js?: string): Promise<string[]> => {\\n+                    if (!js) return [];\\n+                    try {\\n+                      const sandbox = this.getRoutingSandbox();\\n+                      const scope = { ...onFinishContext, output: lastRunOutput } as any;\\n+                      const code = `\\n+                        const step = scope.step; const attempt = scope.attempt; const loop = scope.loop; const outputs = scope.outputs; const outputs_history = scope.outputs_history; const outputs_raw = scope.outputs_raw; const forEach = scope.forEach; const memory = scope.memory; const pr = scope.pr; const files = scope.files; const env = scope.env; const event = scope.event; const output = scope.output; const log = (...a)=> console.log('🔍 Debug:',...a);\\n+                        const __fn = () => {\\\\n${js}\\\\n};\\n+                        const __res = __fn();\\n+                        return Array.isArray(__res) ? __res.filter(x => typeof x === 'string' && x) : [];\\n+                      `;\\n+                      const exec = sandbox.compile(code);\\n+                      const res = exec({ scope }).run();\\n+                      return Array.isArray(res) ? (res as string[]) : [];\\n+                    } catch (e) {\\n+                      const msg = e instanceof Error ? e.message : String(e);\\n+                      logger.error(\\n+                        `✗ on_finish.run → child on_success.run_js failed for \\\"${runCheckId}\\\": ${msg}`\\n+                      );\\n+                      return [];\\n+                    }\\n+                  };\\n+                  const childDynamicRun = await evalChildRunJs(childOnSuccess.run_js);\\n+                  const childRunList = Array.from(\\n+                    new Set([...(childOnSuccess.run || []), ...childDynamicRun].filter(Boolean))\\n+                  );\\n+                  if (childRunList.length > 0) {\\n+                    scheduledByChildOnSuccess = true;\\n+                    logger.info(\\n+                      `  ▶ on_finish.run → scheduling child on_success [${childRunList.join(', ')}] after '${runCheckId}'`\\n+                    );\\n+                  } else {\\n+                    try {\\n+                      logger.info(\\n+                        `  ↪ on_finish.run: child on_success produced empty run list for '${runCheckId}'`\\n+                      );\\n+                    } catch {}\\n+                  }\\n+                  for (const stepId of childRunList) {\\n+                    if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n+                    const childStart = this.recordIterationStart(stepId);\\n+                    const childRes = await this.runNamedCheck(stepId, [], {\\n+                      origin: 'on_finish',\\n+                      config,\\n+                      dependencyGraph,\\n+                      prInfo,\\n+                      resultsMap: results,\\n+                      sessionInfo: undefined,\\n+                      debug,\\n+                      overlay: new Map(results),\\n+                    });\\n+                    const childIssues = (childRes.issues || []).map(i => ({ ...i }));\\n+                    const childSuccess = !this.hasFatal(childIssues);\\n+                    const childOut = (childRes as any)?.output;\\n+                    this.recordIterationComplete(\\n+                      stepId,\\n+                      childStart,\\n+                      childSuccess,\\n+                      childIssues,\\n+                      childOut\\n+                    );\\n+                    try {\\n+                      if (childOut !== undefined) this.trackOutputHistory(stepId, childOut);\\n+                    } catch {}\\n+                  }\\n+                }\\n+              } catch {}\\n \\n-          // Helper to evaluate run_js to string[] safely\\n+              // Fallback: if child on_success was not present or produced no run list,\\n+              // schedule a correction reply when validation issues are present in memory.\\n+              try {\\n+                const issues = memoryHelpers.get('fact_validation_issues', 'fact-validation') as\\n+                  | unknown[]\\n+                  | undefined;\\n+                if (!scheduledByChildOnSuccess && Array.isArray(issues) && issues.length > 0) {\\n+                  const stepId = 'comment-assistant';\\n+                  const guardKey = `${this.sessionUUID()}:${checkName}:post_correction:${\\n+                    prInfo.number || 'local'\\n+                  }`;\\n+                  if (this.postOnFinishGuards.has(guardKey)) {\\n+                    logger.info(\\n+                      `  ↪ on_finish.run: correction already scheduled (guard hit), skipping '${stepId}'`\\n+                    );\\n+                  } else {\\n+                    logger.info(\\n+                      `  ▶ on_finish.run → fallback scheduling '${stepId}' due to validation issues (${issues.length})`\\n+                    );\\n+                    this.postOnFinishGuards.add(guardKey);\\n+                    const childCfg = (config?.checks || {})[stepId] as\\n+                      | import('./types/config').CheckConfig\\n+                      | undefined;\\n+                    if (childCfg) {\\n+                      const provType = childCfg.type || 'ai';\\n+                      const provider = this.providerRegistry.getProviderOrThrow(provType);\\n+                      this.setProviderWebhookContext(provider);\\n+                      const provCfg: import('./providers/check-provider.interface').CheckProviderConfig =\\n+                        {\\n+                          type: provType,\\n+                          prompt: childCfg.prompt,\\n+                          exec: childCfg.exec,\\n+                          focus: childCfg.focus || this.mapCheckNameToFocus(stepId),\\n+                          schema: childCfg.schema,\\n+                          group: childCfg.group,\\n+                          checkName: stepId,\\n+                          eventContext: this.enrichEventContext(prInfo.eventContext),\\n+                          transform: childCfg.transform,\\n+                          transform_js: childCfg.transform_js,\\n+                          timeout: childCfg.timeout,\\n+                          env: childCfg.env,\\n+                          forEach: childCfg.forEach,\\n+                          __outputHistory: this.outputHistory,\\n+                          ...childCfg,\\n+                          ai: { ...(childCfg.ai || {}), timeout: undefined, debug },\\n+                        } as any;\\n+                      await this.executeWithRouting(\\n+                        stepId,\\n+                        childCfg,\\n+                        provider,\\n+                        provCfg,\\n+                        prInfo,\\n+                        new Map(results),\\n+                        undefined,\\n+                        config!,\\n+                        dependencyGraph,\\n+                        debug,\\n+                        results\\n+                      );\\n+                    }\\n+                  }\\n+                }\\n+              } catch {}\\n+            }\\n+            if (runList.length > 0) logger.info(`✓ on_finish.run: completed for \\\"${checkName}\\\"`);\\n+          } catch (error) {\\n+            const errorMsg = error instanceof Error ? error.message : String(error);\\n+            logger.error(`✗ on_finish.run: failed for \\\"${checkName}\\\": ${errorMsg}`);\\n+            if (error instanceof Error && error.stack) {\\n+              logger.debug(`Stack trace: ${error.stack}`);\\n+            }\\n+            throw error;\\n+          }\\n+\\n+          // Now evaluate dynamic run_js with post-run context (e.g., after aggregation updated memory)\\n           const evalRunJs = async (js?: string): Promise<string[]> => {\\n             if (!js) return [];\\n             try {\\n               const sandbox = this.getRoutingSandbox();\\n-              const scope = onFinishContext;\\n+              const scope = onFinishContext; // contains memory + outputs history\\n               const code = `\\n                 const step = scope.step; const attempt = scope.attempt; const loop = scope.loop; const outputs = scope.outputs; const outputs_history = scope.outputs_history; const outputs_raw = scope.outputs_raw; const forEach = scope.forEach; const memory = scope.memory; const pr = scope.pr; const files = scope.files; const env = scope.env; const event = scope.event; const log = (...a)=> console.log('🔍 Debug:',...a);\\n                 const __fn = () => {\\\\n${js}\\\\n};\\n                 const __res = __fn();\\n                 return Array.isArray(__res) ? __res.filter(x => typeof x === 'string' && x) : [];\\n               `;\\n-              try {\\n-                if (code.includes('process')) {\\n-                  logger.warn('⚠️ on_finish.goto_js prelude contains \\\"process\\\" token');\\n-                } else {\\n-                  logger.info('🔧 on_finish.goto_js prelude is clean (no process token)');\\n-                }\\n-              } catch {}\\n               const exec = sandbox.compile(code);\\n               const res = exec({ scope }).run();\\n               return Array.isArray(res) ? (res as string[]) : [];\\n@@ -1053,52 +1271,53 @@ export class CheckExecutionEngine {\\n               return [];\\n             }\\n           };\\n-\\n-          const dynamicRun = await evalRunJs(onFinish.run_js);\\n-          const runList = Array.from(\\n-            new Set([...(onFinish.run || []), ...dynamicRun].filter(Boolean))\\n-          );\\n-\\n-          if (runList.length > 0) {\\n-            logger.info(`▶ on_finish.run: executing [${runList.join(', ')}] for \\\"${checkName}\\\"`);\\n-          }\\n-\\n           try {\\n-            for (const runCheckId of runList) {\\n+            if (process.env.VISOR_DEBUG === 'true' || debug) {\\n+              const memDbg = MemoryStore.getInstance(this.config?.memory);\\n+              const keys = memDbg.list('fact-validation');\\n+              logger.info(\\n+                `on_finish.run_js context (keys in fact-validation) = [${keys.join(', ')}]`\\n+              );\\n+            }\\n+          } catch {}\\n+          const dynamicRun = await evalRunJs(onFinish.run_js);\\n+          const dynList = Array.from(new Set(dynamicRun.filter(Boolean)));\\n+          if (dynList.length > 0) {\\n+            logger.info(\\n+              `▶ on_finish.run_js: executing [${dynList.join(', ')}] for \\\"${checkName}\\\"`\\n+            );\\n+            for (const runCheckId of dynList) {\\n               if (++loopCount > maxLoops) {\\n                 throw new Error(\\n-                  `Routing loop budget exceeded (max_loops=${maxLoops}) during on_finish run`\\n+                  `Routing loop budget exceeded (max_loops=${maxLoops}) during on_finish run_js`\\n                 );\\n               }\\n-              if (debug) log(`🔧 Debug: on_finish.run executing check '${runCheckId}'`);\\n-              logger.info(`  ▶ Executing on_finish check: ${runCheckId}`);\\n-\\n-              const __onFinishRes = await this.runNamedCheck(runCheckId, [], {\\n+              logger.info(`  ▶ Executing on_finish(run_js) check: ${runCheckId}`);\\n+              // Use full routing semantics for dynamic children as well\\n+              const childCfgFull = (config?.checks || {})[runCheckId] as\\n+                | import('./types/config').CheckConfig\\n+                | undefined;\\n+              if (!childCfgFull)\\n+                throw new Error(`Unknown check in on_finish.run_js: ${runCheckId}`);\\n+              const childProvType = childCfgFull.type || 'ai';\\n+              const childProvider = this.providerRegistry.getProviderOrThrow(childProvType);\\n+              this.setProviderWebhookContext(childProvider);\\n+              // Note: unified scheduling executes via runNamedCheck; provider config built internally\\n+              const depOverlayForChild = new Map(results);\\n+              const childRes = await this.runNamedCheck(runCheckId, [], {\\n                 origin: 'on_finish',\\n-                config,\\n+                config: config!,\\n                 dependencyGraph,\\n                 prInfo,\\n                 resultsMap: results,\\n-                sessionInfo: undefined,\\n                 debug,\\n-                eventOverride: onFinish.goto_event,\\n-                overlay: new Map(results),\\n+                overlay: depOverlayForChild,\\n               });\\n               try {\\n-                lastRunOutput = (__onFinishRes as any)?.output;\\n+                lastRunOutput = (childRes as any)?.output;\\n               } catch {}\\n-              logger.info(`  ✓ Completed on_finish check: ${runCheckId}`);\\n-            }\\n-            if (runList.length > 0) {\\n-              logger.info(`✓ on_finish.run: completed for \\\"${checkName}\\\"`);\\n+              logger.info(`  ✓ Completed on_finish(run_js) check: ${runCheckId}`);\\n             }\\n-          } catch (error) {\\n-            const errorMsg = error instanceof Error ? error.message : String(error);\\n-            logger.error(`✗ on_finish.run: failed for \\\"${checkName}\\\": ${errorMsg}`);\\n-            if (error instanceof Error && error.stack) {\\n-              logger.debug(`Stack trace: ${error.stack}`);\\n-            }\\n-            throw error;\\n           }\\n         }\\n \\n@@ -1322,6 +1541,75 @@ export class CheckExecutionEngine {\\n         }\\n \\n         logger.info(`✓ on_finish: completed for \\\"${checkName}\\\"`);\\n+\\n+        // After completing on_finish handling for this forEach parent, if validation issues are present\\n+        // (from memory or inferred from the latest validate-fact history), schedule a single\\n+        // correction reply via comment-assistant. Use a one-shot guard per session+parent check\\n+        // to prevent duplicates when multiple signals agree (aggregator, memory, inferred history).\\n+        try {\\n+          const mem = MemoryStore.getInstance(this.config?.memory);\\n+          const issues = mem.get('fact_validation_issues', 'fact-validation') as\\n+            | unknown[]\\n+            | undefined;\\n+          // Prefer aggregator output when available\\n+          let allValidOut = false;\\n+          try {\\n+            const lro =\\n+              lastRunOutput && typeof lastRunOutput === 'object'\\n+                ? (lastRunOutput as any)\\n+                : undefined;\\n+            allValidOut = !!(lro && (lro.all_valid === true || lro.allValid === true));\\n+          } catch {}\\n+          // Infer invalids from the latest wave as an additional guard when memory path is absent\\n+          let inferredInvalid = 0;\\n+          try {\\n+            const vfHistNow = (this.outputHistory.get('validate-fact') || []) as unknown[];\\n+            if (Array.isArray(vfHistNow) && forEachItems.length > 0) {\\n+              const lastWave = vfHistNow.slice(-forEachItems.length);\\n+              inferredInvalid = lastWave.filter(\\n+                (v: any) => v && (v.is_valid === false || v.valid === false)\\n+              ).length;\\n+            }\\n+          } catch {}\\n+\\n+          if (\\n+            (!allValidOut && Array.isArray(issues) && issues.length > 0) ||\\n+            (!allValidOut && inferredInvalid > 0)\\n+          ) {\\n+            const stepId = 'comment-assistant';\\n+            const guardKey = `${this.sessionUUID()}:${checkName}:post_correction:${\\n+              prInfo.number || 'local'\\n+            }`;\\n+            if (this.postOnFinishGuards.has(guardKey)) {\\n+              logger.info(\\n+                `↪ on_finish.post: correction already scheduled (guard hit), skipping '${stepId}'`\\n+              );\\n+            } else {\\n+              logger.info(\\n+                `▶ on_finish.post: scheduling '${stepId}' due to validation issues (mem=${Array.isArray(issues) ? issues.length : 0}, inferred=${inferredInvalid})`\\n+              );\\n+              this.postOnFinishGuards.add(guardKey);\\n+              const childCfg = (config?.checks || {})[stepId] as\\n+                | import('./types/config').CheckConfig\\n+                | undefined;\\n+              if (childCfg) {\\n+                const provType = childCfg.type || 'ai';\\n+                const provider = this.providerRegistry.getProviderOrThrow(provType);\\n+                this.setProviderWebhookContext(provider);\\n+                // Provider config constructed inside runNamedCheck; no local build needed here\\n+                await this.runNamedCheck(stepId, [], {\\n+                  origin: 'on_finish',\\n+                  config: config!,\\n+                  dependencyGraph,\\n+                  prInfo,\\n+                  resultsMap: results,\\n+                  debug,\\n+                  overlay: new Map(results),\\n+                });\\n+              }\\n+            }\\n+          }\\n+        } catch {}\\n       } catch (error) {\\n         logger.error(`✗ on_finish: error for \\\"${checkName}\\\": ${error}`);\\n       }\\n@@ -1538,8 +1826,23 @@ export class CheckExecutionEngine {\\n           async () => provider.execute(prInfo, providerConfig, dependencyResults, context)\\n         );\\n         this.recordProviderDuration(checkName, Date.now() - __provStart);\\n+        // Expose a sensible 'output' for routing JS across all providers.\\n+        // Some providers (AI) return { output, issues }, others (memory/command/http) may\\n+        // return the value directly. Prefer explicit `output`, fall back to the whole result.\\n         try {\\n-          currentRouteOutput = (res as any)?.output;\\n+          const anyRes: any = res as any;\\n+          currentRouteOutput =\\n+            anyRes && typeof anyRes === 'object' && 'output' in anyRes ? anyRes.output : anyRes;\\n+          if (\\n+            checkName === 'aggregate-validations' &&\\n+            (process.env.VISOR_DEBUG === 'true' || debug)\\n+          ) {\\n+            try {\\n+              logger.info(\\n+                '[aggregate-validations] route-output = ' + JSON.stringify(currentRouteOutput)\\n+              );\\n+            } catch {}\\n+          }\\n         } catch {}\\n         // Success path\\n         // Treat result issues with severity error/critical as a soft-failure eligible for on_fail routing\\n@@ -1702,6 +2005,18 @@ export class CheckExecutionEngine {\\n           // Compute run list\\n           const dynamicRun = await evalRunJs(onSuccess.run_js);\\n           const runList = [...(onSuccess.run || []), ...dynamicRun].filter(Boolean);\\n+          try {\\n+            if (\\n+              checkName === 'aggregate-validations' &&\\n+              (process.env.VISOR_DEBUG === 'true' || debug)\\n+            ) {\\n+              logger.info(\\n+                `on_success.run (aggregate-validations): dynamicRun=[${dynamicRun.join(', ')}] run=[${(\\n+                  onSuccess.run || []\\n+                ).join(', ')}]`\\n+              );\\n+            }\\n+          } catch {}\\n           if (runList.length > 0) {\\n             try {\\n               require('./logger').logger.info(\\n@@ -1732,7 +2047,10 @@ export class CheckExecutionEngine {\\n               if (!inItem && mode === 'map' && items.length > 0) {\\n                 for (let i = 0; i < items.length; i++) {\\n                   const itemScope: ScopePath = [{ check: checkName, index: i }];\\n-                  await this.runNamedCheck(stepId, itemScope, {\\n+                  // Record stats for scheduled child run\\n+                  if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n+                  const schedStart = this.recordIterationStart(stepId);\\n+                  const childRes = await this.runNamedCheck(stepId, itemScope, {\\n                     config: config!,\\n                     dependencyGraph,\\n                     prInfo,\\n@@ -1740,12 +2058,28 @@ export class CheckExecutionEngine {\\n                     debug: !!debug,\\n                     overlay: dependencyResults,\\n                   });\\n+                  const childIssues = (childRes.issues || []).map(i => ({ ...i }));\\n+                  const childSuccess = !this.hasFatal(childIssues);\\n+                  const childOut = (childRes as any)?.output;\\n+                  this.recordIterationComplete(\\n+                    stepId,\\n+                    schedStart,\\n+                    childSuccess,\\n+                    childIssues,\\n+                    childOut\\n+                  );\\n+                  try {\\n+                    const out = (childRes as any)?.output;\\n+                    if (out !== undefined) this.trackOutputHistory(stepId, out);\\n+                  } catch {}\\n                 }\\n               } else {\\n                 const scopeForRun: ScopePath = foreachContext\\n                   ? [{ check: foreachContext.parent, index: foreachContext.index }]\\n                   : [];\\n-                await this.runNamedCheck(stepId, scopeForRun, {\\n+                if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n+                const schedStart = this.recordIterationStart(stepId);\\n+                const childRes = await this.runNamedCheck(stepId, scopeForRun, {\\n                   config: config!,\\n                   dependencyGraph,\\n                   prInfo,\\n@@ -1753,6 +2087,20 @@ export class CheckExecutionEngine {\\n                   debug: !!debug,\\n                   overlay: dependencyResults,\\n                 });\\n+                const childIssues = (childRes.issues || []).map(i => ({ ...i }));\\n+                const childSuccess = !this.hasFatal(childIssues);\\n+                const childOut = (childRes as any)?.output;\\n+                this.recordIterationComplete(\\n+                  stepId,\\n+                  schedStart,\\n+                  childSuccess,\\n+                  childIssues,\\n+                  childOut\\n+                );\\n+                try {\\n+                  const out = (childRes as any)?.output;\\n+                  if (out !== undefined) this.trackOutputHistory(stepId, out);\\n+                } catch {}\\n               }\\n             }\\n           } else {\\n@@ -1815,6 +2163,26 @@ export class CheckExecutionEngine {\\n                   if (!eventMatches) continue;\\n                   if (dependsOn(name, target)) forwardSet.add(name);\\n                 }\\n+                // Always execute the target itself first (goto target), regardless of event filtering\\n+                // Then, optionally execute its dependents that match the goto_event\\n+                const runTargetOnce = async (scopeForRun: ScopePath) => {\\n+                  // Ensure stats entry exists for the target\\n+                  if (!this.executionStats.has(target)) this.initializeCheckStats(target);\\n+                  const tgtStart = this.recordIterationStart(target);\\n+                  const tgtRes = await this.runNamedCheck(target, scopeForRun, {\\n+                    config: config!,\\n+                    dependencyGraph,\\n+                    prInfo,\\n+                    resultsMap: resultsMap || new Map(),\\n+                    debug: !!debug,\\n+                    eventOverride: onSuccess.goto_event,\\n+                  });\\n+                  const tgtIssues = (tgtRes.issues || []).map(i => ({ ...i }));\\n+                  const tgtSuccess = !this.hasFatal(tgtIssues);\\n+                  const tgtOutput: unknown = (tgtRes as any)?.output;\\n+                  this.recordIterationComplete(target, tgtStart, tgtSuccess, tgtIssues, tgtOutput);\\n+                };\\n+\\n                 // Topologically order forwardSet based on depends_on within this subset\\n                 const order: string[] = [];\\n                 const inSet = (n: string) => forwardSet.has(n);\\n@@ -1841,7 +2209,7 @@ export class CheckExecutionEngine {\\n                   order.push(n);\\n                 };\\n                 for (const n of forwardSet) visit(n);\\n-                // Execute in order with event override, updating statistics per child\\n+                // Execute target (once) and then dependents with event override; update statistics per step\\n                 const tcfg = cfgChecks[target];\\n                 const mode =\\n                   tcfg?.fanout === 'map'\\n@@ -1854,7 +2222,11 @@ export class CheckExecutionEngine {\\n                     ? (currentRouteOutput as unknown[])\\n                     : [];\\n                 const runChainOnce = async (scopeForRun: ScopePath) => {\\n-                  for (const stepId of order) {\\n+                  // Run the goto target itself first\\n+                  await runTargetOnce(scopeForRun);\\n+                  // Exclude the target itself from the dependent execution order to avoid double-run\\n+                  const dependentsOnly = order.filter(n => n !== target);\\n+                  for (const stepId of dependentsOnly) {\\n                     if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n                     const childStart = this.recordIterationStart(stepId);\\n                     const childRes = await this.runNamedCheck(stepId, scopeForRun, {\\n@@ -2057,8 +2429,8 @@ export class CheckExecutionEngine {\\n     config: import('./types/config').VisorConfig | undefined,\\n     tagFilter: import('./types/config').TagFilter | undefined\\n   ): string[] {\\n-    const logFn = this.config?.output?.pr_comment ? console.error : console.log;\\n-\\n+    // When no tag filter is specified, include all checks regardless of tags.\\n+    // Tag filters should only narrow execution when explicitly provided via config.tag_filter or CLI.\\n     return checks.filter(checkName => {\\n       const checkConfig = config?.checks?.[checkName];\\n       if (!checkConfig) {\\n@@ -2068,13 +2440,7 @@ export class CheckExecutionEngine {\\n \\n       const checkTags = checkConfig.tags || [];\\n \\n-      // If check has tags but no tag filter is specified, exclude it\\n-      if (checkTags.length > 0 && (!tagFilter || (!tagFilter.include && !tagFilter.exclude))) {\\n-        logFn(`⏭️ Skipping check '${checkName}' - check has tags but no tag filter specified`);\\n-        return false;\\n-      }\\n-\\n-      // If no tag filter is specified and check has no tags, include it\\n+      // If no tag filter is specified, include all checks\\n       if (!tagFilter || (!tagFilter.include && !tagFilter.exclude)) {\\n         return true;\\n       }\\n@@ -2087,19 +2453,13 @@ export class CheckExecutionEngine {\\n       // Check exclude tags first (if any exclude tag matches, skip the check)\\n       if (tagFilter.exclude && tagFilter.exclude.length > 0) {\\n         const hasExcludedTag = tagFilter.exclude.some(tag => checkTags.includes(tag));\\n-        if (hasExcludedTag) {\\n-          logFn(`⏭️ Skipping check '${checkName}' - has excluded tag`);\\n-          return false;\\n-        }\\n+        if (hasExcludedTag) return false;\\n       }\\n \\n       // Check include tags (if specified, at least one must match)\\n       if (tagFilter.include && tagFilter.include.length > 0) {\\n         const hasIncludedTag = tagFilter.include.some(tag => checkTags.includes(tag));\\n-        if (!hasIncludedTag) {\\n-          logFn(`⏭️ Skipping check '${checkName}' - does not have required tags`);\\n-          return false;\\n-        }\\n+        if (!hasIncludedTag) return false;\\n       }\\n \\n       return true;\\n@@ -2547,6 +2907,12 @@ export class CheckExecutionEngine {\\n \\n     // Use filtered checks for execution\\n     checks = tagFilteredChecks;\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        const ev = (prInfo as any)?.eventType || '(unknown)';\\n+        console.error(`[engine] final checks after filters (event=${ev}): [${checks.join(', ')}]`);\\n+      }\\n+    } catch {}\\n \\n     // Capture GitHub Action context (owner/repo/octokit) if available from environment\\n     // This is used for context elevation when routing via goto_event\\n@@ -2597,7 +2963,7 @@ export class CheckExecutionEngine {\\n           `🔧 Debug: Using grouped dependency-aware execution for ${checks.length} checks (has dependencies: ${hasDependencies}, has routing: ${hasRouting})`\\n         );\\n       }\\n-      return await this.executeGroupedDependencyAwareChecks(\\n+      const execRes = await this.executeGroupedDependencyAwareChecks(\\n         prInfo,\\n         checks,\\n         timeout,\\n@@ -2608,6 +2974,38 @@ export class CheckExecutionEngine {\\n         failFast,\\n         tagFilter\\n       );\\n+\\n+      // Test-mode PR comment posting: when running under the test runner we want to\\n+      // exercise comment creation/update using the injected Octokit (recorder), so that\\n+      // tests can assert on issues.createComment/updates. In normal runs the action/CLI\\n+      // code handles posting; this block is gated by VISOR_TEST_MODE to avoid duplication.\\n+      try {\\n+        if (process.env.VISOR_TEST_MODE === 'true' && config?.output?.pr_comment) {\\n+          // Resolve owner/repo from cached action context or PRInfo.eventContext\\n+          let owner: string | undefined = this.actionContext?.owner;\\n+          let repo: string | undefined = this.actionContext?.repo;\\n+          if (!owner || !repo) {\\n+            try {\\n+              const anyInfo = prInfo as unknown as {\\n+                eventContext?: { repository?: { owner?: { login?: string }; name?: string } };\\n+              };\\n+              owner = anyInfo?.eventContext?.repository?.owner?.login || owner;\\n+              repo = anyInfo?.eventContext?.repository?.name || repo;\\n+            } catch {}\\n+          }\\n+          owner = owner || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[0];\\n+          repo = repo || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[1];\\n+          if (owner && repo && prInfo.number) {\\n+            await this.reviewer.postReviewComment(owner, repo, prInfo.number, execRes.results, {\\n+              config: config as any,\\n+              triggeredBy: prInfo.eventType || 'manual',\\n+              commentId: 'visor-review',\\n+            });\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      return execRes;\\n     }\\n \\n     // Single check execution\\n@@ -2626,6 +3024,31 @@ export class CheckExecutionEngine {\\n \\n       const groupedResults: GroupedCheckResults = {};\\n       groupedResults[checkResult.group] = [checkResult];\\n+      // Test-mode PR comment posting for single-check runs as well\\n+      try {\\n+        if (process.env.VISOR_TEST_MODE === 'true' && config?.output?.pr_comment) {\\n+          let owner: string | undefined = this.actionContext?.owner;\\n+          let repo: string | undefined = this.actionContext?.repo;\\n+          if (!owner || !repo) {\\n+            try {\\n+              const anyInfo = prInfo as unknown as {\\n+                eventContext?: { repository?: { owner?: { login?: string }; name?: string } };\\n+              };\\n+              owner = anyInfo?.eventContext?.repository?.owner?.login || owner;\\n+              repo = anyInfo?.eventContext?.repository?.name || repo;\\n+            } catch {}\\n+          }\\n+          owner = owner || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[0];\\n+          repo = repo || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[1];\\n+          if (owner && repo && prInfo.number) {\\n+            await this.reviewer.postReviewComment(owner, repo, prInfo.number, groupedResults, {\\n+              config: config as any,\\n+              triggeredBy: prInfo.eventType || 'manual',\\n+              commentId: 'visor-review',\\n+            });\\n+          }\\n+        }\\n+      } catch {}\\n       return {\\n         results: groupedResults,\\n         statistics: this.buildExecutionStatistics(),\\n@@ -2682,8 +3105,11 @@ export class CheckExecutionEngine {\\n     };\\n     providerConfig.forEach = checkConfig.forEach;\\n \\n+    // Ensure statistics are recorded for single-check path as well\\n+    if (!this.executionStats.has(checkName)) this.initializeCheckStats(checkName);\\n+    const __iterStart = this.recordIterationStart(checkName);\\n     const __provStart = Date.now();\\n-    const result = await provider.execute(prInfo, providerConfig);\\n+    const result = await provider.execute(prInfo, providerConfig, undefined, this.executionContext);\\n     this.recordProviderDuration(checkName, Date.now() - __provStart);\\n \\n     // Validate forEach output (skip if there are already errors from transform_js or other sources)\\n@@ -2735,7 +3161,13 @@ export class CheckExecutionEngine {\\n       group = checkName;\\n     }\\n \\n-    return {\\n+    // Track output in history (parity with grouped path)\\n+    try {\\n+      const out = (result as any)?.output;\\n+      if (out !== undefined) this.trackOutputHistory(checkName, out);\\n+    } catch {}\\n+\\n+    const checkResult: CheckResult = {\\n       checkName,\\n       content,\\n       group,\\n@@ -2743,6 +3175,16 @@ export class CheckExecutionEngine {\\n       debug: result.debug,\\n       issues: result.issues, // Include structured issues\\n     };\\n+\\n+    // Record completion in execution statistics (success/failure + durations)\\n+    try {\\n+      const issuesArr = (result.issues || []).map(i => ({ ...i }));\\n+      const success = !this.hasFatal(issuesArr);\\n+      const outputVal: unknown = (result as any)?.output;\\n+      this.recordIterationComplete(checkName, __iterStart, success, issuesArr, outputVal);\\n+    } catch {}\\n+\\n+    return checkResult;\\n   }\\n \\n   /**\\n@@ -3348,6 +3790,9 @@ export class CheckExecutionEngine {\\n     tagFilter?: import('./types/config').TagFilter\\n   ): Promise<ReviewSummary> {\\n     const log = logFn || console.error;\\n+    try {\\n+      console.error('[engine] enter executeDependencyAwareChecks (dbg=', debug, ')');\\n+    } catch {}\\n \\n     if (debug) {\\n       log(`🔧 Debug: Starting dependency-aware execution of ${checks.length} checks`);\\n@@ -3419,12 +3864,25 @@ export class CheckExecutionEngine {\\n         }\\n         return true;\\n       };\\n+      const allowByEvent = (name: string): boolean => {\\n+        try {\\n+          const cfg = config!.checks?.[name];\\n+          const triggers: import('./types/config').EventTrigger[] = (cfg?.on || []) as any;\\n+          // No triggers => allowed for all events\\n+          if (!triggers || triggers.length === 0) return true;\\n+          const current = prInfo?.eventType || 'manual';\\n+          return triggers.includes(current as any);\\n+        } catch {\\n+          return true;\\n+        }\\n+      };\\n       const visit = (name: string) => {\\n         const cfg = config.checks![name];\\n         if (!cfg || !cfg.depends_on) return;\\n         for (const dep of cfg.depends_on) {\\n           if (!config.checks![dep]) continue;\\n           if (!allowByTags(dep)) continue;\\n+          if (!allowByEvent(dep)) continue;\\n           if (!set.has(dep)) {\\n             set.add(dep);\\n             visit(dep);\\n@@ -3597,7 +4055,11 @@ export class CheckExecutionEngine {\\n           const providerType = checkConfig.type || 'ai';\\n           const provider = this.providerRegistry.getProviderOrThrow(providerType);\\n           if (debug) {\\n-            log(`🔧 Debug: Provider f|| '${checkName}' is '${providerType}'`);\\n+            log(`🔧 Debug: Provider for '${checkName}' is '${providerType}'`);\\n+          } else if (process.env.VISOR_DEBUG === 'true') {\\n+            try {\\n+              console.log(`[engine] provider for ${checkName} -> ${providerType}`);\\n+            } catch {}\\n           }\\n           this.setProviderWebhookContext(provider);\\n \\n@@ -3625,6 +4087,8 @@ export class CheckExecutionEngine {\\n             message: extendedCheckConfig.message,\\n             env: checkConfig.env,\\n             forEach: checkConfig.forEach,\\n+            // Provide output history so providers can access latest outputs for Liquid rendering\\n+            __outputHistory: this.outputHistory,\\n             // Pass through any provider-specific keys (e.g., op/values for github provider)\\n             ...checkConfig,\\n             ai: {\\n@@ -5110,7 +5574,67 @@ export class CheckExecutionEngine {\\n \\n     // Handle on_finish hooks for forEach checks after ALL dependents complete\\n     if (!shouldStopExecution) {\\n+      try {\\n+        logger.info('🧭 on_finish: invoking handleOnFinishHooks');\\n+      } catch {}\\n+      try {\\n+        if (debug) console.error('[engine] calling handleOnFinishHooks');\\n+      } catch {}\\n       await this.handleOnFinishHooks(config, dependencyGraph, results, prInfo, debug || false);\\n+      // Fallback: if some on_finish static run targets did not execute (e.g., due to graph selection peculiarities),\\n+      // run them once now for each forEach parent that produced items in this run. This preserves general semantics\\n+      // without hardcoding step names.\\n+      try {\\n+        for (const [parentName, cfg] of Object.entries(config.checks || {})) {\\n+          const onf = (cfg as any)?.on_finish as OnFinishConfig | undefined;\\n+          if (!(cfg as any)?.forEach || !onf || !Array.isArray(onf.run) || onf.run.length === 0)\\n+            continue;\\n+          const parentRes = results.get(parentName) as ExtendedReviewSummary | undefined;\\n+          const count = (() => {\\n+            try {\\n+              if (!parentRes) return 0;\\n+              if (Array.isArray(parentRes.forEachItems)) return parentRes.forEachItems.length;\\n+              const out = (parentRes as any)?.output;\\n+              return Array.isArray(out) ? out.length : 0;\\n+            } catch {\\n+              return 0;\\n+            }\\n+          })();\\n+          let histCount = 0;\\n+          try {\\n+            const h = this.outputHistory.get(parentName) as unknown[] | undefined;\\n+            if (Array.isArray(h)) histCount = h.length;\\n+          } catch {}\\n+          if (count > 0 || histCount > 0) {\\n+            for (const stepId of onf.run!) {\\n+              if (typeof stepId !== 'string' || !stepId) continue;\\n+              if (results.has(stepId)) continue; // already executed\\n+              try {\\n+                logger.info(\\n+                  `▶ on_finish.fallback: executing static run step '${stepId}' for parent '${parentName}'`\\n+                );\\n+              } catch {}\\n+              try {\\n+                if (debug)\\n+                  console.error(`[on_finish.fallback] run '${stepId}' for '${parentName}'`);\\n+              } catch {}\\n+              await this.runNamedCheck(stepId, [], {\\n+                origin: 'on_finish',\\n+                config: config!,\\n+                dependencyGraph,\\n+                prInfo,\\n+                resultsMap: results,\\n+                debug: !!debug,\\n+                overlay: new Map(results),\\n+              });\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n+    } else {\\n+      try {\\n+        logger.info('🧭 on_finish: skipped due to shouldStopExecution');\\n+      } catch {}\\n     }\\n \\n     // Cleanup sessions BEFORE printing summary to avoid mixing debug logs with table output\\n@@ -6661,6 +7185,17 @@ export class CheckExecutionEngine {\\n     this.outputHistory.get(checkName)!.push(output);\\n   }\\n \\n+  /**\\n+   * Snapshot of output history per step for test assertions\\n+   */\\n+  public getOutputHistorySnapshot(): Record<string, unknown[]> {\\n+    const out: Record<string, unknown[]> = {};\\n+    for (const [k, v] of this.outputHistory.entries()) {\\n+      out[k] = Array.isArray(v) ? [...v] : [];\\n+    }\\n+    return out;\\n+  }\\n+\\n   /**\\n    * Record that a check was skipped\\n    */\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli-main.ts\",\"additions\":4,\"deletions\":0,\"changes\":111,\"patch\":\"diff --git a/src/cli-main.ts b/src/cli-main.ts\\nindex 1b1100ca..ad2245ff 100644\\n--- a/src/cli-main.ts\\n+++ b/src/cli-main.ts\\n@@ -109,6 +109,112 @@ async function handleValidateCommand(argv: string[], configManager: ConfigManage\\n   }\\n }\\n \\n+/**\\n+ * Handle the test subcommand (Milestone 0: discovery only)\\n+ */\\n+async function handleTestCommand(argv: string[]): Promise<void> {\\n+  // Minimal flag parsing: --config <path>, --only <name>, --bail\\n+  const getArg = (name: string): string | undefined => {\\n+    const i = argv.indexOf(name);\\n+    return i >= 0 ? argv[i + 1] : undefined;\\n+  };\\n+  const hasFlag = (name: string): boolean => argv.includes(name);\\n+\\n+  const testsPath = getArg('--config');\\n+  const only = getArg('--only');\\n+  const bail = hasFlag('--bail');\\n+  const listOnly = hasFlag('--list');\\n+  const validateOnly = hasFlag('--validate');\\n+  const progress = (getArg('--progress') as 'compact' | 'detailed' | undefined) || 'compact';\\n+  void progress; // currently parsed but not changing output detail yet\\n+  const jsonOut = getArg('--json'); // path or '-' for stdout\\n+  const reportArg = getArg('--report'); // e.g. junit:path.xml\\n+  const summaryArg = getArg('--summary'); // e.g. md:path.md\\n+  const maxParallelRaw = getArg('--max-parallel');\\n+  const promptMaxCharsRaw = getArg('--prompt-max-chars');\\n+  const maxParallel = maxParallelRaw ? Math.max(1, parseInt(maxParallelRaw, 10) || 1) : undefined;\\n+  const promptMaxChars = promptMaxCharsRaw\\n+    ? Math.max(1, parseInt(promptMaxCharsRaw, 10) || 1)\\n+    : undefined;\\n+\\n+  // Configure logger for concise console output\\n+  configureLoggerFromCli({ output: 'table', debug: false, verbose: false, quiet: false });\\n+\\n+  console.log('🧪 Visor Test Runner');\\n+  try {\\n+    const { discoverAndPrint, validateTestsOnly, VisorTestRunner } = await import(\\n+      './test-runner/index'\\n+    );\\n+    if (validateOnly) {\\n+      const errors = await validateTestsOnly({ testsPath });\\n+      process.exit(errors > 0 ? 1 : 0);\\n+    }\\n+    if (listOnly) {\\n+      await discoverAndPrint({ testsPath });\\n+      if (only) console.log(`\\\\nFilter: --only ${only}`);\\n+      if (bail) console.log('Mode: --bail (stop on first failure)');\\n+      process.exit(0);\\n+    }\\n+    // Run and capture structured results\\n+    const runner = new (VisorTestRunner as any)();\\n+    const tpath = runner.resolveTestsPath(testsPath);\\n+    const suite = runner.loadSuite(tpath);\\n+    const runRes = await runner.runCases(tpath, suite, { only, bail, maxParallel, promptMaxChars });\\n+    const failures = runRes.failures;\\n+    // Basic reporters (Milestone 7): write minimal JSON/JUnit/Markdown summaries\\n+    try {\\n+      if (jsonOut) {\\n+        const fs = require('fs');\\n+        const payload = { failures, results: runRes.results };\\n+        const data = JSON.stringify(payload, null, 2);\\n+        if (jsonOut === '-' || jsonOut === 'stdout') console.log(data);\\n+        else {\\n+          fs.writeFileSync(jsonOut, data, 'utf8');\\n+          console.error(`📝 JSON report written to ${jsonOut}`);\\n+        }\\n+      }\\n+    } catch {}\\n+    try {\\n+      if (reportArg && reportArg.startsWith('junit:')) {\\n+        const fs = require('fs');\\n+        const dest = reportArg.slice('junit:'.length);\\n+        const tests = (runRes.results || []).length;\\n+        const failed = (runRes.results || []).filter((r: any) => !r.passed).length;\\n+        const detail = (runRes.results || [])\\n+          .map((r: any) => {\\n+            const errs = (r.errors || []).concat(\\n+              ...(r.stages || []).map((s: any) => s.errors || [])\\n+            );\\n+            return `<testcase classname=\\\\\\\"visor\\\\\\\" name=\\\\\\\"${r.name}\\\\\\\"${errs.length > 0 ? '' : ''}>${errs\\n+              .map((e: string) => `<failure message=\\\\\\\"${e.replace(/\\\\\\\"/g, '&quot;')}\\\\\\\"></failure>`)\\n+              .join('')}</testcase>`;\\n+          })\\n+          .join('\\\\n  ');\\n+        const xml = `<?xml version=\\\\\\\"1.0\\\\\\\" encoding=\\\\\\\"UTF-8\\\\\\\"?>\\\\n<testsuite name=\\\\\\\"visor\\\\\\\" tests=\\\\\\\"${tests}\\\\\\\" failures=\\\\\\\"${failed}\\\\\\\">\\\\n  ${detail}\\\\n</testsuite>`;\\n+        fs.writeFileSync(dest, xml, 'utf8');\\n+        console.error(`📝 JUnit report written to ${dest}`);\\n+      }\\n+    } catch {}\\n+    try {\\n+      if (summaryArg && summaryArg.startsWith('md:')) {\\n+        const fs = require('fs');\\n+        const dest = summaryArg.slice('md:'.length);\\n+        const lines = (runRes.results || []).map(\\n+          (r: any) =>\\n+            `- ${r.passed ? '✅' : '❌'} ${r.name}${r.stages ? ' (' + r.stages.length + ' stage' + (r.stages.length !== 1 ? 's' : '') + ')' : ''}`\\n+        );\\n+        const content = `# Visor Test Summary\\\\n\\\\n- Failures: ${failures}\\\\n\\\\n${lines.join('\\\\n')}`;\\n+        fs.writeFileSync(dest, content, 'utf8');\\n+        console.error(`📝 Markdown summary written to ${dest}`);\\n+      }\\n+    } catch {}\\n+    process.exit(failures > 0 ? 1 : 0);\\n+  } catch (err) {\\n+    console.error('❌ test: ' + (err instanceof Error ? err.message : String(err)));\\n+    process.exit(1);\\n+  }\\n+}\\n+\\n /**\\n  * Main CLI entry point for Visor\\n  */\\n@@ -151,6 +257,11 @@ export async function main(): Promise<void> {\\n       await handleValidateCommand(filteredArgv, configManager);\\n       return;\\n     }\\n+    // Check for test subcommand\\n+    if (filteredArgv.length > 2 && filteredArgv[2] === 'test') {\\n+      await handleTestCommand(filteredArgv);\\n+      return;\\n+    }\\n \\n     // Parse arguments using the CLI class\\n     const options = cli.parseArgs(filteredArgv);\\n\",\"status\":\"added\"},{\"filename\":\"src/github-comments.ts\",\"additions\":1,\"deletions\":1,\"changes\":19,\"patch\":\"diff --git a/src/github-comments.ts b/src/github-comments.ts\\nindex be84f6ce..13ad7a3c 100644\\n--- a/src/github-comments.ts\\n+++ b/src/github-comments.ts\\n@@ -338,12 +338,10 @@ ${content}\\n           // Don't retry auth errors, not found errors, etc.\\n           throw error;\\n         } else {\\n-          const computed =\\n-            this.retryConfig.baseDelay * Math.pow(this.retryConfig.backoffFactor, attempt);\\n-          const delay =\\n-            computed > this.retryConfig.maxDelay\\n-              ? Math.max(0, this.retryConfig.maxDelay - 1)\\n-              : computed;\\n+          const delay = Math.min(\\n+            this.retryConfig.baseDelay * Math.pow(this.retryConfig.backoffFactor, attempt),\\n+            this.retryConfig.maxDelay\\n+          );\\n           await this.sleep(delay);\\n         }\\n       }\\n@@ -356,14 +354,7 @@ ${content}\\n    * Sleep utility\\n    */\\n   private sleep(ms: number): Promise<void> {\\n-    return new Promise(resolve => {\\n-      const t = setTimeout(resolve, ms);\\n-      if (typeof (t as any).unref === 'function') {\\n-        try {\\n-          (t as any).unref();\\n-        } catch {}\\n-      }\\n-    });\\n+    return new Promise(resolve => setTimeout(resolve, ms));\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/index.ts\",\"additions\":1,\"deletions\":1,\"changes\":26,\"patch\":\"diff --git a/src/index.ts b/src/index.ts\\nindex 93a9393a..0f4c6c5f 100644\\n--- a/src/index.ts\\n+++ b/src/index.ts\\n@@ -13,7 +13,7 @@ import { PRAnalyzer, PRInfo } from './pr-analyzer';\\n import { configureLoggerFromCli } from './logger';\\n import { deriveExecutedCheckNames } from './utils/ui-helpers';\\n import { resolveHeadShaFromEvent } from './utils/head-sha';\\n-import { PRReviewer, GroupedCheckResults, ReviewIssue, CheckResult } from './reviewer';\\n+import { PRReviewer, GroupedCheckResults, ReviewIssue } from './reviewer';\\n import { GitHubActionInputs, GitHubContext } from './action-cli-bridge';\\n import { ConfigManager } from './config';\\n import { GitHubCheckService, CheckRunOptions } from './github-check-service';\\n@@ -762,30 +762,8 @@ async function handleIssueEvent(\\n     if (Object.keys(results).length > 0) {\\n       let commentBody = '';\\n \\n-      // Collapse dynamic group: if multiple dynamic responses exist in a single run,\\n-      // take only the last non-empty one to avoid duplicated old+new answers.\\n-      const resultsToUse: GroupedCheckResults = { ...results };\\n-      try {\\n-        const dyn: CheckResult[] | undefined = resultsToUse['dynamic'];\\n-        if (Array.isArray(dyn) && dyn.length > 1) {\\n-          const nonEmpty = dyn.filter(d => d.content && d.content.trim().length > 0);\\n-          if (nonEmpty.length > 0) {\\n-            // Keep only the last non-empty dynamic item\\n-            resultsToUse['dynamic'] = [nonEmpty[nonEmpty.length - 1]];\\n-          } else {\\n-            // All empty: keep the last item (empty) to preserve intent\\n-            resultsToUse['dynamic'] = [dyn[dyn.length - 1]];\\n-          }\\n-        }\\n-      } catch (error) {\\n-        console.warn(\\n-          'Failed to collapse dynamic group:',\\n-          error instanceof Error ? error.message : String(error)\\n-        );\\n-      }\\n-\\n       // Directly use check content without adding extra headers\\n-      for (const checks of Object.values(resultsToUse)) {\\n+      for (const checks of Object.values(results)) {\\n         for (const check of checks) {\\n           if (check.content && check.content.trim()) {\\n             commentBody += `${check.content}\\\\n\\\\n`;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/ai-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":31,\"patch\":\"diff --git a/src/providers/ai-check-provider.ts b/src/providers/ai-check-provider.ts\\nindex a85fc73c..ea883e20 100644\\n--- a/src/providers/ai-check-provider.ts\\n+++ b/src/providers/ai-check-provider.ts\\n@@ -468,7 +468,10 @@ export class AICheckProvider extends CheckProvider {\\n     prInfo: PRInfo,\\n     config: CheckProviderConfig,\\n     _dependencyResults?: Map<string, ReviewSummary>,\\n-    sessionInfo?: { parentSessionId?: string; reuseSession?: boolean }\\n+    sessionInfo?: {\\n+      parentSessionId?: string;\\n+      reuseSession?: boolean;\\n+    } & import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n     // Extract AI configuration - only set properties that are explicitly provided\\n     const aiConfig: AIReviewConfig = {};\\n@@ -613,6 +616,32 @@ export class AICheckProvider extends CheckProvider {\\n       (config as any).__outputHistory as Map<string, unknown[]> | undefined\\n     );\\n \\n+    // Test hook: capture the FINAL prompt (with PR context) before provider invocation\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const serviceForCapture = new AIReviewService(aiConfig);\\n+      const finalPrompt = await (serviceForCapture as any).buildCustomPrompt(\\n+        prInfo,\\n+        processedPrompt,\\n+        config.schema,\\n+        { checkName: (config as any).checkName }\\n+      );\\n+      sessionInfo?.hooks?.onPromptCaptured?.({\\n+        step: String(stepName),\\n+        provider: 'ai',\\n+        prompt: finalPrompt,\\n+      });\\n+    } catch {}\\n+\\n+    // Test hook: mock output for this step (short-circuit provider)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = sessionInfo?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        return { issues: [], output: mock } as ReviewSummary & { output: unknown };\\n+      }\\n+    } catch {}\\n+\\n     // Create AI service with config - environment variables will be used if aiConfig is empty\\n     const service = new AIReviewService(aiConfig);\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/check-provider.interface.ts\",\"additions\":1,\"deletions\":0,\"changes\":2,\"patch\":\"diff --git a/src/providers/check-provider.interface.ts b/src/providers/check-provider.interface.ts\\nindex fc7fd1cf..0fa5cf19 100644\\n--- a/src/providers/check-provider.interface.ts\\n+++ b/src/providers/check-provider.interface.ts\\n@@ -46,6 +46,8 @@ export interface ExecutionContext {\\n   /** SDK hooks for human input */\\n   hooks?: {\\n     onHumanInput?: (request: HumanInputRequest) => Promise<string>;\\n+    onPromptCaptured?: (info: { step: string; provider: string; prompt: string }) => void;\\n+    mockForStep?: (step: string) => unknown | undefined;\\n   };\\n }\\n \\n\",\"status\":\"added\"},{\"filename\":\"src/providers/command-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":38,\"patch\":\"diff --git a/src/providers/command-check-provider.ts b/src/providers/command-check-provider.ts\\nindex 04a66741..5160e72d 100644\\n--- a/src/providers/command-check-provider.ts\\n+++ b/src/providers/command-check-provider.ts\\n@@ -66,7 +66,8 @@ export class CommandCheckProvider extends CheckProvider {\\n   async execute(\\n     prInfo: PRInfo,\\n     config: CheckProviderConfig,\\n-    dependencyResults?: Map<string, ReviewSummary>\\n+    dependencyResults?: Map<string, ReviewSummary>,\\n+    context?: import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n     try {\\n       logger.info(\\n@@ -142,6 +143,41 @@ export class CommandCheckProvider extends CheckProvider {\\n       );\\n     } catch {}\\n \\n+    // Test hook: mock output for this step (short-circuit execution)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = context?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock && typeof mock === 'object') {\\n+        const m = mock as { stdout?: string; stderr?: string; exit_code?: number };\\n+        let out: unknown = m.stdout ?? '';\\n+        try {\\n+          if (\\n+            typeof out === 'string' &&\\n+            (out.trim().startsWith('{') || out.trim().startsWith('['))\\n+          ) {\\n+            out = JSON.parse(out);\\n+          }\\n+        } catch {}\\n+        if (m.exit_code && m.exit_code !== 0) {\\n+          return {\\n+            issues: [\\n+              {\\n+                file: 'command',\\n+                line: 0,\\n+                ruleId: 'command/execution_error',\\n+                message: `Mocked command exited with code ${m.exit_code}`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              },\\n+            ],\\n+            // Also expose output for assertions\\n+            output: out,\\n+          } as any;\\n+        }\\n+        return { issues: [], output: out } as any;\\n+      }\\n+    } catch {}\\n+\\n     try {\\n       // Render the command with Liquid templates if needed\\n       let renderedCommand = command;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/github-ops-provider.ts\",\"additions\":4,\"deletions\":1,\"changes\":119,\"patch\":\"diff --git a/src/providers/github-ops-provider.ts b/src/providers/github-ops-provider.ts\\nindex 1dafb432..2e7cef21 100644\\n--- a/src/providers/github-ops-provider.ts\\n+++ b/src/providers/github-ops-provider.ts\\n@@ -4,6 +4,7 @@ import { ReviewSummary } from '../reviewer';\\n import Sandbox from '@nyariv/sandboxjs';\\n import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n+import { logger } from '../logger';\\n \\n export class GitHubOpsProvider extends CheckProvider {\\n   private sandbox?: Sandbox;\\n@@ -51,11 +52,29 @@ export class GitHubOpsProvider extends CheckProvider {\\n \\n     // IMPORTANT: Always prefer authenticated octokit from event context (GitHub App or token)\\n     // This ensures proper bot identity in reactions, labels, and comments\\n-    const octokit: import('@octokit/rest').Octokit | undefined = config.eventContext?.octokit as\\n+    let octokit: import('@octokit/rest').Octokit | undefined = config.eventContext?.octokit as\\n       | import('@octokit/rest').Octokit\\n       | undefined;\\n+    if (process.env.VISOR_DEBUG === 'true') {\\n+      try {\\n+        logger.debug(`[github-ops] pre-fallback octokit? ${!!octokit}`);\\n+      } catch {}\\n+    }\\n+    // Test runner fallback: use global recorder if eventContext is missing octokit\\n+    if (!octokit) {\\n+      try {\\n+        const { getGlobalRecorder } = require('../test-runner/recorders/global-recorder');\\n+        const rec = getGlobalRecorder && getGlobalRecorder();\\n+        if (rec) octokit = rec as any;\\n+      } catch {}\\n+    }\\n \\n     if (!octokit) {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        try {\\n+          console.error('[github-ops] missing octokit after fallback — returning issue');\\n+        } catch {}\\n+      }\\n       return {\\n         issues: [\\n           {\\n@@ -72,7 +91,24 @@ export class GitHubOpsProvider extends CheckProvider {\\n     }\\n \\n     const repoEnv = process.env.GITHUB_REPOSITORY || '';\\n-    const [owner, repo] = repoEnv.split('/') as [string, string];\\n+    let owner = '';\\n+    let repo = '';\\n+    if (repoEnv.includes('/')) {\\n+      [owner, repo] = repoEnv.split('/') as [string, string];\\n+    } else {\\n+      try {\\n+        const ec: any = config.eventContext || {};\\n+        owner = ec?.repository?.owner?.login || owner;\\n+        repo = ec?.repository?.name || repo;\\n+      } catch {}\\n+    }\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        logger.info(\\n+          `[github-ops] context octokit? ${!!octokit} repo=${owner}/${repo} pr#=${prInfo?.number}`\\n+        );\\n+      }\\n+    } catch {}\\n     if (!owner || !repo || !prInfo?.number) {\\n       return {\\n         issues: [\\n@@ -93,6 +129,11 @@ export class GitHubOpsProvider extends CheckProvider {\\n     if (Array.isArray(cfg.values)) valuesRaw = (cfg.values as unknown[]).map(v => String(v));\\n     else if (typeof cfg.values === 'string') valuesRaw = [cfg.values];\\n     else if (typeof cfg.value === 'string') valuesRaw = [cfg.value];\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        logger.info(`[github-ops] op=${cfg.op} valuesRaw(before)=${JSON.stringify(valuesRaw)}`);\\n+      }\\n+    } catch {}\\n \\n     // Liquid render helper for values\\n     const renderValues = async (arr: string[]): Promise<string[]> => {\\n@@ -109,6 +150,17 @@ export class GitHubOpsProvider extends CheckProvider {\\n           outputs[name] = summary.output !== undefined ? summary.output : summary;\\n         }\\n       }\\n+      // Fallback: if outputs missing but engine provided history, use last output snapshot\\n+      try {\\n+        const hist = (config as any).__outputHistory as Map<string, unknown[]> | undefined;\\n+        if (hist) {\\n+          for (const [name, arr] of hist.entries()) {\\n+            if (!outputs[name] && Array.isArray(arr) && arr.length > 0) {\\n+              outputs[name] = arr[arr.length - 1];\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n       const ctx = {\\n         pr: {\\n           number: prInfo.number,\\n@@ -120,6 +172,25 @@ export class GitHubOpsProvider extends CheckProvider {\\n         },\\n         outputs,\\n       };\\n+      try {\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          logger.info(`[github-ops] deps keys=${Object.keys(outputs).join(', ')}`);\\n+          const ov = outputs['overview'] as any;\\n+          if (ov) {\\n+            logger.info(`[github-ops] outputs.overview.keys=${Object.keys(ov).join(',')}`);\\n+            if (ov.tags) {\\n+              logger.info(\\n+                `[github-ops] outputs.overview.tags keys=${Object.keys(ov.tags).join(',')}`\\n+              );\\n+              try {\\n+                logger.info(\\n+                  `[github-ops] outputs.overview.tags['review-effort']=${String(ov.tags['review-effort'])}`\\n+                );\\n+              } catch {}\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n       const out: string[] = [];\\n       for (const item of arr) {\\n         if (typeof item === 'string' && (item.includes('{{') || item.includes('{%'))) {\\n@@ -129,6 +200,9 @@ export class GitHubOpsProvider extends CheckProvider {\\n           } catch (e) {\\n             // If Liquid fails, surface as a provider error\\n             const msg = e instanceof Error ? e.message : String(e);\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              logger.warn(`[github-ops] liquid_render_error: ${msg}`);\\n+            }\\n             return Promise.reject({\\n               issues: [\\n                 {\\n@@ -175,6 +249,9 @@ export class GitHubOpsProvider extends CheckProvider {\\n         else if (Array.isArray(res)) values = (res as unknown[]).map(v => String(v));\\n       } catch (e) {\\n         const msg = e instanceof Error ? e.message : String(e);\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          logger.warn(`[github-ops] value_js_error: ${msg}`);\\n+        }\\n         return {\\n           issues: [\\n             {\\n@@ -190,14 +267,49 @@ export class GitHubOpsProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Fallback: if values are still empty, try deriving from dependency outputs (common pattern: outputs.<dep>.tags)\\n+    if (values.length === 0 && dependencyResults && dependencyResults.size > 0) {\\n+      try {\\n+        const derived: string[] = [];\\n+        for (const result of dependencyResults.values()) {\\n+          const out = (result as ReviewSummary & { output?: unknown })?.output ?? result;\\n+          const tags = (out as Record<string, unknown>)?.['tags'] as\\n+            | Record<string, unknown>\\n+            | undefined;\\n+          if (tags && typeof tags === 'object') {\\n+            const label = tags['label'];\\n+            const effort = (tags as Record<string, unknown>)['review-effort'];\\n+            if (label != null) derived.push(String(label));\\n+            if (effort !== undefined && effort !== null)\\n+              derived.push(`review/effort:${String(effort)}`);\\n+          }\\n+        }\\n+        values = derived;\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          logger.info(`[github-ops] derived values from deps: ${JSON.stringify(values)}`);\\n+        }\\n+      } catch {}\\n+    }\\n+\\n     // Trim, drop empty, and de-duplicate values regardless of source\\n     values = values.map(v => v.trim()).filter(v => v.length > 0);\\n     values = Array.from(new Set(values));\\n \\n+    try {\\n+      // Minimal debug to help diagnose label flow under tests\\n+      if (process.env.NODE_ENV === 'test' || process.env.VISOR_DEBUG === 'true') {\\n+        logger.info(`[github-ops] ${cfg.op} resolved values: ${JSON.stringify(values)}`);\\n+      }\\n+    } catch {}\\n+\\n     try {\\n       switch (cfg.op) {\\n         case 'labels.add': {\\n           if (values.length === 0) break; // no-op if nothing to add\\n+          try {\\n+            if (process.env.VISOR_OUTPUT_FORMAT !== 'json')\\n+              logger.step(`[github-ops] labels.add -> ${JSON.stringify(values)}`);\\n+          } catch {}\\n           await octokit.rest.issues.addLabels({\\n             owner,\\n             repo,\\n@@ -246,6 +358,9 @@ export class GitHubOpsProvider extends CheckProvider {\\n       return { issues: [] };\\n     } catch (e) {\\n       const msg = e instanceof Error ? e.message : String(e);\\n+      try {\\n+        logger.error(`[github-ops] op_failed ${cfg.op}: ${msg}`);\\n+      } catch {}\\n       return {\\n         issues: [\\n           {\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/http-client-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":11,\"patch\":\"diff --git a/src/providers/http-client-provider.ts b/src/providers/http-client-provider.ts\\nindex 9620f01b..4d8c41be 100644\\n--- a/src/providers/http-client-provider.ts\\n+++ b/src/providers/http-client-provider.ts\\n@@ -54,7 +54,7 @@ export class HttpClientProvider extends CheckProvider {\\n     prInfo: PRInfo,\\n     config: CheckProviderConfig,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n-    _sessionInfo?: { parentSessionId?: string; reuseSession?: boolean }\\n+    context?: import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n     const url = config.url as string;\\n     const method = (config.method as string) || 'GET';\\n@@ -96,8 +96,13 @@ export class HttpClientProvider extends CheckProvider {\\n       // Resolve environment variables in headers\\n       const resolvedHeaders = EnvironmentResolver.resolveHeaders(headers);\\n \\n-      // Fetch data from the endpoint\\n-      const data = await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n+      // Test hook: mock HTTP response for this step\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = context?.hooks?.mockForStep?.(String(stepName));\\n+      const data =\\n+        mock !== undefined\\n+          ? mock\\n+          : await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n \\n       // Apply transformation if specified\\n       let processedData = data;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/memory-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":40,\"patch\":\"diff --git a/src/providers/memory-check-provider.ts b/src/providers/memory-check-provider.ts\\nindex aee629c5..dca92621 100644\\n--- a/src/providers/memory-check-provider.ts\\n+++ b/src/providers/memory-check-provider.ts\\n@@ -381,34 +381,36 @@ export class MemoryCheckProvider extends CheckProvider {\\n     try {\\n       if (\\n         (config as any).checkName === 'aggregate-validations' ||\\n-        (config as any).checkName === 'aggregate' ||\\n         (config as any).checkName === 'aggregate'\\n       ) {\\n-        const hist = (enhancedContext as any)?.outputs?.history || {};\\n-        const keys = Object.keys(hist);\\n-        console.log('[MemoryProvider]', (config as any).checkName, ': history keys =', keys);\\n-        const vf = (hist as any)['validate-fact'];\\n-        console.log(\\n-          '[MemoryProvider]',\\n-          (config as any).checkName,\\n-          ': validate-fact history length =',\\n-          Array.isArray(vf) ? vf.length : 'n/a'\\n-        );\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          const hist = (enhancedContext as any)?.outputs?.history || {};\\n+          const keys = Object.keys(hist);\\n+          logger.debug(\\n+            `[MemoryProvider] ${(config as any).checkName}: history keys = [${keys.join(', ')}]`\\n+          );\\n+          const vf = (hist as any)['validate-fact'];\\n+          logger.debug(\\n+            `[MemoryProvider] ${(config as any).checkName}: validate-fact history length = ${\\n+              Array.isArray(vf) ? vf.length : 'n/a'\\n+            }`\\n+          );\\n+        }\\n       }\\n     } catch {}\\n \\n     const result = this.evaluateJavaScriptBlock(script, enhancedContext);\\n     try {\\n-      if ((config as any).checkName === 'aggregate-validations') {\\n+      if (\\n+        (config as any).checkName === 'aggregate-validations' &&\\n+        process.env.VISOR_DEBUG === 'true'\\n+      ) {\\n         const tv = store.get('total_validations', 'fact-validation');\\n         const av = store.get('all_valid', 'fact-validation');\\n-        console.error(\\n-          '[MemoryProvider] post-exec',\\n-          (config as any).checkName,\\n-          'total_validations=',\\n-          tv,\\n-          'all_valid=',\\n-          av\\n+        logger.debug(\\n+          `[MemoryProvider] post-exec ${(config as any).checkName} total_validations=${String(\\n+            tv\\n+          )} all_valid=${String(av)}`\\n         );\\n       }\\n     } catch {}\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/assertions.ts\",\"additions\":4,\"deletions\":0,\"changes\":91,\"patch\":\"diff --git a/src/test-runner/assertions.ts b/src/test-runner/assertions.ts\\nnew file mode 100644\\nindex 00000000..ea676eea\\n--- /dev/null\\n+++ b/src/test-runner/assertions.ts\\n@@ -0,0 +1,91 @@\\n+export type CountExpectation = {\\n+  exactly?: number;\\n+  at_least?: number;\\n+  at_most?: number;\\n+};\\n+\\n+export interface CallsExpectation extends CountExpectation {\\n+  step?: string;\\n+  provider?: 'github' | string;\\n+  op?: string;\\n+  args?: Record<string, unknown>;\\n+}\\n+\\n+export interface PromptsExpectation {\\n+  step: string;\\n+  index?: number | 'first' | 'last';\\n+  contains?: string[];\\n+  not_contains?: string[];\\n+  matches?: string; // regex string\\n+  where?: {\\n+    contains?: string[];\\n+    not_contains?: string[];\\n+    matches?: string; // regex\\n+  };\\n+}\\n+\\n+export interface OutputsExpectation {\\n+  step: string;\\n+  index?: number | 'first' | 'last';\\n+  path: string;\\n+  equals?: unknown;\\n+  equalsDeep?: unknown;\\n+  matches?: string; // regex\\n+  where?: {\\n+    path: string;\\n+    equals?: unknown;\\n+    matches?: string; // regex\\n+  };\\n+  contains_unordered?: unknown[]; // array membership ignoring order\\n+}\\n+\\n+export interface ExpectBlock {\\n+  use?: string[];\\n+  calls?: CallsExpectation[];\\n+  prompts?: PromptsExpectation[];\\n+  outputs?: OutputsExpectation[];\\n+  no_calls?: Array<{ step?: string; provider?: string; op?: string }>;\\n+  fail?: { message_contains?: string };\\n+  strict_violation?: { for_step?: string; message_contains?: string };\\n+}\\n+\\n+export function validateCounts(exp: CountExpectation): void {\\n+  const keys = ['exactly', 'at_least', 'at_most'].filter(k => (exp as any)[k] !== undefined);\\n+  if (keys.length > 1) {\\n+    throw new Error(`Count expectation is ambiguous: ${keys.join(', ')}`);\\n+  }\\n+}\\n+\\n+export function deepEqual(a: unknown, b: unknown): boolean {\\n+  if (a === b) return true;\\n+  if (typeof a !== typeof b) return false;\\n+  if (a && b && typeof a === 'object') {\\n+    if (Array.isArray(a) && Array.isArray(b)) {\\n+      if (a.length !== b.length) return false;\\n+      for (let i = 0; i < a.length; i++) if (!deepEqual(a[i], b[i])) return false;\\n+      return true;\\n+    }\\n+    const ak = Object.keys(a as any).sort();\\n+    const bk = Object.keys(b as any).sort();\\n+    if (!deepEqual(ak, bk)) return false;\\n+    for (const k of ak) if (!deepEqual((a as any)[k], (b as any)[k])) return false;\\n+    return true;\\n+  }\\n+  return false;\\n+}\\n+\\n+export function containsUnordered(haystack: unknown[], needles: unknown[]): boolean {\\n+  if (!Array.isArray(haystack) || !Array.isArray(needles)) return false;\\n+  const used = new Array(haystack.length).fill(false);\\n+  outer: for (const n of needles) {\\n+    for (let i = 0; i < haystack.length; i++) {\\n+      if (used[i]) continue;\\n+      if (deepEqual(haystack[i], n) || haystack[i] === n) {\\n+        used[i] = true;\\n+        continue outer;\\n+      }\\n+    }\\n+    return false;\\n+  }\\n+  return true;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/fixture-loader.ts\",\"additions\":6,\"deletions\":0,\"changes\":156,\"patch\":\"diff --git a/src/test-runner/fixture-loader.ts b/src/test-runner/fixture-loader.ts\\nnew file mode 100644\\nindex 00000000..7ec38d15\\n--- /dev/null\\n+++ b/src/test-runner/fixture-loader.ts\\n@@ -0,0 +1,156 @@\\n+export type BuiltinFixtureName =\\n+  | 'gh.pr_open.minimal'\\n+  | 'gh.pr_sync.minimal'\\n+  | 'gh.issue_open.minimal'\\n+  | 'gh.issue_comment.standard'\\n+  | 'gh.issue_comment.visor_help'\\n+  | 'gh.issue_comment.visor_regenerate'\\n+  | 'gh.issue_comment.edited'\\n+  | 'gh.pr_closed.minimal';\\n+\\n+export interface LoadedFixture {\\n+  name: string;\\n+  webhook: { name: string; action?: string; payload: Record<string, unknown> };\\n+  git?: { branch?: string; baseBranch?: string };\\n+  files?: Array<{\\n+    path: string;\\n+    content: string;\\n+    status?: 'added' | 'modified' | 'removed' | 'renamed';\\n+    additions?: number;\\n+    deletions?: number;\\n+  }>;\\n+  diff?: string; // unified diff text\\n+  env?: Record<string, string>;\\n+  time?: { now?: string };\\n+}\\n+\\n+export class FixtureLoader {\\n+  load(name: BuiltinFixtureName): LoadedFixture {\\n+    // Minimal, stable, general-purpose fixtures used by the test runner.\\n+    // All fixtures supply a webhook payload and, for PR variants, a small diff.\\n+    if (name.startsWith('gh.pr_open')) {\\n+      const files: LoadedFixture['files'] = [\\n+        {\\n+          path: 'src/search.ts',\\n+          content: 'export function search(q: string) {\\\\n  return []\\\\n}\\\\n',\\n+          status: 'added',\\n+          additions: 3,\\n+          deletions: 0,\\n+        },\\n+      ];\\n+      const diff = this.buildUnifiedDiff(files);\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'pull_request',\\n+          action: 'opened',\\n+          payload: { pull_request: { number: 1, title: 'feat: add user search' } },\\n+        },\\n+        git: { branch: 'feature/test', baseBranch: 'main' },\\n+        files,\\n+        diff,\\n+      };\\n+    }\\n+    if (name.startsWith('gh.pr_sync')) {\\n+      const files: LoadedFixture['files'] = [\\n+        {\\n+          path: 'src/search.ts',\\n+          content: 'export function search(q: string) {\\\\n  return [q] // updated\\\\n}\\\\n',\\n+          status: 'modified',\\n+          additions: 1,\\n+          deletions: 0,\\n+        },\\n+      ];\\n+      const diff = this.buildUnifiedDiff(files);\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'pull_request',\\n+          action: 'synchronize',\\n+          payload: { pull_request: { number: 1, title: 'feat: add user search (update)' } },\\n+        },\\n+        git: { branch: 'feature/test', baseBranch: 'main' },\\n+        files,\\n+        diff,\\n+      };\\n+    }\\n+    if (name.startsWith('gh.issue_open')) {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issues',\\n+          action: 'opened',\\n+          payload: {\\n+            issue: { number: 12, title: 'Bug: crashes on search edge case', body: 'Steps...' },\\n+          },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.issue_comment.standard') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issue_comment',\\n+          action: 'created',\\n+          payload: { comment: { body: 'Thanks for the update!' }, issue: { number: 1 } },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.issue_comment.visor_help') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issue_comment',\\n+          action: 'created',\\n+          payload: { comment: { body: '/visor help' }, issue: { number: 1 } },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.issue_comment.visor_regenerate') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issue_comment',\\n+          action: 'created',\\n+          payload: { comment: { body: '/visor Regenerate reviews' }, issue: { number: 1 } },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.pr_closed.minimal') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'pull_request',\\n+          action: 'closed',\\n+          payload: { pull_request: { number: 1, title: 'feat: add user search' } },\\n+        },\\n+      };\\n+    }\\n+    // Fallback minimal\\n+    return {\\n+      name,\\n+      webhook: { name: 'unknown', payload: {} },\\n+    };\\n+  }\\n+\\n+  private buildUnifiedDiff(\\n+    files: Array<{ path: string; content: string; status?: string }>\\n+  ): string {\\n+    // Build a very small, stable unified diff suitable for prompts\\n+    const chunks = files.map(f => {\\n+      const header =\\n+        `diff --git a/${f.path} b/${f.path}\\\\n` +\\n+        (f.status === 'added'\\n+          ? 'index 0000000..1111111 100644\\\\n--- /dev/null\\\\n'\\n+          : `index 1111111..2222222 100644\\\\n--- a/${f.path}\\\\n`) +\\n+        `+++ b/${f.path}\\\\n` +\\n+        '@@\\\\n';\\n+      const body = f.content\\n+        .split('\\\\n')\\n+        .map(line => (f.status === 'removed' ? `-${line}` : `+${line}`))\\n+        .join('\\\\n');\\n+      return header + body + '\\\\n';\\n+    });\\n+    return chunks.join('\\\\n');\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/index.ts\",\"additions\":53,\"deletions\":0,\"changes\":1527,\"patch\":\"diff --git a/src/test-runner/index.ts b/src/test-runner/index.ts\\nnew file mode 100644\\nindex 00000000..cbefc162\\n--- /dev/null\\n+++ b/src/test-runner/index.ts\\n@@ -0,0 +1,1527 @@\\n+import fs from 'fs';\\n+import path from 'path';\\n+import * as yaml from 'js-yaml';\\n+\\n+import { ConfigManager } from '../config';\\n+import { CheckExecutionEngine } from '../check-execution-engine';\\n+import type { PRInfo } from '../pr-analyzer';\\n+import { RecordingOctokit } from './recorders/github-recorder';\\n+import { setGlobalRecorder } from './recorders/global-recorder';\\n+import { FixtureLoader } from './fixture-loader';\\n+import { validateCounts, type ExpectBlock } from './assertions';\\n+import { validateTestsDoc } from './validator';\\n+\\n+export type TestCase = {\\n+  name: string;\\n+  description?: string;\\n+  event?: string;\\n+  flow?: Array<{ name: string }>;\\n+};\\n+\\n+export type TestSuite = {\\n+  version: string;\\n+  extends?: string | string[];\\n+  tests: {\\n+    defaults?: Record<string, unknown>;\\n+    fixtures?: unknown[];\\n+    cases: TestCase[];\\n+  };\\n+};\\n+\\n+export interface DiscoverOptions {\\n+  testsPath?: string; // Path to .visor.tests.yaml\\n+  cwd?: string;\\n+}\\n+\\n+function isObject(v: unknown): v is Record<string, unknown> {\\n+  return !!v && typeof v === 'object' && !Array.isArray(v);\\n+}\\n+\\n+export class VisorTestRunner {\\n+  constructor(private readonly cwd: string = process.cwd()) {}\\n+\\n+  private line(title = '', char = '─', width = 60): string {\\n+    if (!title) return char.repeat(width);\\n+    const pad = Math.max(1, width - title.length - 2);\\n+    return `${char.repeat(2)} ${title} ${char.repeat(pad)}`;\\n+  }\\n+\\n+  private printCaseHeader(name: string, kind: 'flow' | 'single', event?: string): void {\\n+    console.log('\\\\n' + this.line(`Case: ${name}`));\\n+    const meta: string[] = [`type=${kind}`];\\n+    if (event) meta.push(`event=${event}`);\\n+    console.log(`  ${meta.join('  ·  ')}`);\\n+  }\\n+\\n+  private printStageHeader(\\n+    flowName: string,\\n+    stageName: string,\\n+    event?: string,\\n+    fixture?: string\\n+  ): void {\\n+    console.log('\\\\n' + this.line(`${flowName} — ${stageName}`));\\n+    const meta: string[] = [];\\n+    if (event) meta.push(`event=${event}`);\\n+    if (fixture) meta.push(`fixture=${fixture}`);\\n+    if (meta.length) console.log(`  ${meta.join('  ·  ')}`);\\n+  }\\n+\\n+  private printSelectedChecks(checks: string[]): void {\\n+    if (!checks || checks.length === 0) return;\\n+    console.log(`  checks: ${checks.join(', ')}`);\\n+  }\\n+\\n+  /**\\n+   * Locate a tests file: explicit path > ./.visor.tests.yaml > defaults/.visor.tests.yaml\\n+   */\\n+  public resolveTestsPath(explicit?: string): string {\\n+    if (explicit) {\\n+      return path.isAbsolute(explicit) ? explicit : path.resolve(this.cwd, explicit);\\n+    }\\n+    const candidates = [\\n+      path.resolve(this.cwd, '.visor.tests.yaml'),\\n+      path.resolve(this.cwd, '.visor.tests.yml'),\\n+      path.resolve(this.cwd, 'defaults/.visor.tests.yaml'),\\n+      path.resolve(this.cwd, 'defaults/.visor.tests.yml'),\\n+    ];\\n+    for (const p of candidates) {\\n+      if (fs.existsSync(p)) return p;\\n+    }\\n+    throw new Error(\\n+      'No tests file found. Provide --config <path> or add .visor.tests.yaml (or defaults/.visor.tests.yaml).'\\n+    );\\n+  }\\n+\\n+  /**\\n+   * Load and minimally validate tests YAML.\\n+   */\\n+  public loadSuite(testsPath: string): TestSuite {\\n+    const raw = fs.readFileSync(testsPath, 'utf8');\\n+    const doc = yaml.load(raw) as unknown;\\n+    const validation = validateTestsDoc(doc);\\n+    if (!validation.ok) {\\n+      const errs = validation.errors.map(e => ` - ${e}`).join('\\\\n');\\n+      throw new Error(`Tests file validation failed:\\\\n${errs}`);\\n+    }\\n+    if (!isObject(doc)) throw new Error('Tests YAML must be a YAML object');\\n+\\n+    const version = String((doc as any).version ?? '1.0');\\n+    const tests = (doc as any).tests;\\n+    if (!tests || !isObject(tests)) throw new Error('tests: {} section is required');\\n+    const cases = (tests as any).cases as unknown;\\n+    if (!Array.isArray(cases) || cases.length === 0) {\\n+      throw new Error('tests.cases must be a non-empty array');\\n+    }\\n+\\n+    // Preserve full case objects for execution; discovery prints selective fields\\n+    const suite: TestSuite = {\\n+      version,\\n+      extends: (doc as any).extends,\\n+      tests: {\\n+        defaults: (tests as any).defaults || {},\\n+        fixtures: (tests as any).fixtures || [],\\n+        cases: (tests as any).cases,\\n+      },\\n+    };\\n+    return suite;\\n+  }\\n+\\n+  /**\\n+   * Pretty print discovered cases to stdout.\\n+   */\\n+  public printDiscovery(testsPath: string, suite: TestSuite): void {\\n+    const rel = path.relative(this.cwd, testsPath) || testsPath;\\n+    console.log('🧪 Visor Test Runner — discovery mode');\\n+    console.log(`   Suite: ${rel}`);\\n+    const parent = suite.extends\\n+      ? Array.isArray(suite.extends)\\n+        ? suite.extends.join(', ')\\n+        : String(suite.extends)\\n+      : '(none)';\\n+    console.log(`   Extends: ${parent}`);\\n+    const defaults = suite.tests.defaults || {};\\n+    const strict = (defaults as any).strict === undefined ? true : !!(defaults as any).strict;\\n+    console.log(`   Strict: ${strict ? 'on' : 'off'}`);\\n+\\n+    // List cases\\n+    console.log('\\\\nCases:');\\n+    for (const c of suite.tests.cases) {\\n+      const isFlow = Array.isArray(c.flow) && c.flow.length > 0;\\n+      const badge = isFlow ? 'flow' : c.event || 'event';\\n+      console.log(` - ${c.name} [${badge}]`);\\n+    }\\n+    console.log('\\\\nTip: run `visor test --only <name>` to filter, `--bail` to stop early.');\\n+  }\\n+\\n+  /**\\n+   * Execute non-flow cases with minimal assertions (Milestone 1 MVP).\\n+   */\\n+  public async runCases(\\n+    testsPath: string,\\n+    suite: TestSuite,\\n+    options: { only?: string; bail?: boolean; maxParallel?: number; promptMaxChars?: number }\\n+  ): Promise<{\\n+    failures: number;\\n+    results: Array<{\\n+      name: string;\\n+      passed: boolean;\\n+      errors?: string[];\\n+      stages?: Array<{ name: string; errors?: string[] }>;\\n+    }>;\\n+  }> {\\n+    // Save defaults for flow runner access\\n+    (this as any).suiteDefaults = suite.tests.defaults || {};\\n+    // Support --only \\\"case\\\" and --only \\\"case#stage\\\"\\n+    let onlyCase = options.only?.toLowerCase();\\n+    let stageFilter: string | undefined;\\n+    if (onlyCase && onlyCase.includes('#')) {\\n+      const parts = onlyCase.split('#');\\n+      onlyCase = parts[0];\\n+      stageFilter = (parts[1] || '').trim();\\n+    }\\n+    const allCases = suite.tests.cases;\\n+    const selected = onlyCase\\n+      ? allCases.filter(c => c.name.toLowerCase().includes(onlyCase as string))\\n+      : allCases;\\n+    if (selected.length === 0) {\\n+      console.log('No matching cases.');\\n+      return { failures: 0, results: [] };\\n+    }\\n+\\n+    // Load merged config via ConfigManager (honors extends), then clone for test overrides\\n+    const cm = new ConfigManager();\\n+    // Prefer loading the base config referenced by extends; fall back to the tests file\\n+    let configFileToLoad = testsPath;\\n+    const parentExt = suite.extends;\\n+    if (parentExt) {\\n+      const first = Array.isArray(parentExt) ? parentExt[0] : parentExt;\\n+      if (typeof first === 'string') {\\n+        const resolved = path.isAbsolute(first)\\n+          ? first\\n+          : path.resolve(path.dirname(testsPath), first);\\n+        configFileToLoad = resolved;\\n+      }\\n+    }\\n+    const config = await cm.loadConfig(configFileToLoad, { validate: true, mergeDefaults: true });\\n+    if (!config.checks) {\\n+      throw new Error('Loaded config has no checks; cannot run tests');\\n+    }\\n+\\n+    const defaultsAny: any = suite.tests.defaults || {};\\n+    const defaultStrict = defaultsAny?.strict !== false;\\n+    const aiProviderDefault = defaultsAny?.ai_provider || 'mock';\\n+    const ghRec = defaultsAny?.github_recorder as\\n+      | { error_code?: number; timeout_ms?: number }\\n+      | undefined;\\n+    const defaultPromptCap: number | undefined =\\n+      options.promptMaxChars ||\\n+      (typeof defaultsAny?.prompt_max_chars === 'number'\\n+        ? defaultsAny.prompt_max_chars\\n+        : undefined);\\n+    const caseMaxParallel =\\n+      options.maxParallel ||\\n+      (typeof defaultsAny?.max_parallel === 'number' ? defaultsAny.max_parallel : undefined) ||\\n+      1;\\n+\\n+    // Test overrides: force AI provider to 'mock' when requested (default: mock per RFC)\\n+    const cfg = JSON.parse(JSON.stringify(config));\\n+    for (const name of Object.keys(cfg.checks || {})) {\\n+      const chk = cfg.checks[name] || {};\\n+      if ((chk.type || 'ai') === 'ai') {\\n+        const prev = (chk.ai || {}) as Record<string, unknown>;\\n+        chk.ai = {\\n+          ...prev,\\n+          provider: aiProviderDefault,\\n+          skip_code_context: true,\\n+          disable_tools: true,\\n+          timeout: Math.min(15000, (prev.timeout as number) || 15000),\\n+        } as any;\\n+        cfg.checks[name] = chk;\\n+      }\\n+    }\\n+\\n+    let failures = 0;\\n+    const caseResults: Array<{\\n+      name: string;\\n+      passed: boolean;\\n+      errors?: string[];\\n+      stages?: Array<{ name: string; errors?: string[] }>;\\n+    }> = [];\\n+    // Header: show suite path for clarity\\n+    try {\\n+      const rel = path.relative(this.cwd, testsPath) || testsPath;\\n+      console.log(`Suite: ${rel}`);\\n+    } catch {}\\n+\\n+    const runOne = async (_case: any): Promise<{ name: string; failed: number }> => {\\n+      // Case header for clarity\\n+      const isFlow = Array.isArray((_case as any).flow) && (_case as any).flow.length > 0;\\n+      const caseEvent = (_case as any).event as string | undefined;\\n+      this.printCaseHeader(\\n+        (_case as any).name || '(unnamed)',\\n+        isFlow ? 'flow' : 'single',\\n+        caseEvent\\n+      );\\n+      if ((_case as any).skip) {\\n+        console.log(`⏭ SKIP ${(_case as any).name}`);\\n+        caseResults.push({ name: _case.name, passed: true });\\n+        return { name: _case.name, failed: 0 };\\n+      }\\n+      if (Array.isArray((_case as any).flow) && (_case as any).flow.length > 0) {\\n+        const flowRes = await this.runFlowCase(\\n+          _case,\\n+          cfg,\\n+          defaultStrict,\\n+          options.bail || false,\\n+          defaultPromptCap,\\n+          stageFilter\\n+        );\\n+        const failed = flowRes.failures;\\n+        caseResults.push({ name: _case.name, passed: failed === 0, stages: flowRes.stages });\\n+        return { name: _case.name, failed };\\n+      }\\n+      const strict = (\\n+        typeof (_case as any).strict === 'boolean' ? (_case as any).strict : defaultStrict\\n+      ) as boolean;\\n+      const expect = ((_case as any).expect || {}) as ExpectBlock;\\n+      // Fixture selection with optional overrides\\n+      const fixtureInput =\\n+        typeof (_case as any).fixture === 'object' && (_case as any).fixture\\n+          ? (_case as any).fixture\\n+          : { builtin: (_case as any).fixture };\\n+      const prInfo = this.buildPrInfoFromFixture(fixtureInput?.builtin, fixtureInput?.overrides);\\n+\\n+      // Inject recording Octokit into engine via actionContext using env owner/repo\\n+      const prevRepo = process.env.GITHUB_REPOSITORY;\\n+      process.env.GITHUB_REPOSITORY = process.env.GITHUB_REPOSITORY || 'owner/repo';\\n+      // Apply case env overrides if present\\n+      const envOverrides =\\n+        typeof (_case as any).env === 'object' && (_case as any).env\\n+          ? ((_case as any).env as Record<string, string>)\\n+          : undefined;\\n+      const prevEnv: Record<string, string | undefined> = {};\\n+      if (envOverrides) {\\n+        for (const [k, v] of Object.entries(envOverrides)) {\\n+          prevEnv[k] = process.env[k];\\n+          process.env[k] = String(v);\\n+        }\\n+      }\\n+      const ghRecCase =\\n+        typeof (_case as any).github_recorder === 'object' && (_case as any).github_recorder\\n+          ? ((_case as any).github_recorder as { error_code?: number; timeout_ms?: number })\\n+          : undefined;\\n+      const rcOpts = ghRecCase || ghRec;\\n+      const recorder = new RecordingOctokit(\\n+        rcOpts ? { errorCode: rcOpts.error_code, timeoutMs: rcOpts.timeout_ms } : undefined\\n+      );\\n+      setGlobalRecorder(recorder);\\n+      const engine = new CheckExecutionEngine(undefined as any, recorder as unknown as any);\\n+\\n+      // Capture prompts per step\\n+      const prompts: Record<string, string[]> = {};\\n+      const mocks =\\n+        typeof (_case as any).mocks === 'object' && (_case as any).mocks\\n+          ? ((_case as any).mocks as Record<string, unknown>)\\n+          : {};\\n+      const mockCursors: Record<string, number> = {};\\n+      engine.setExecutionContext({\\n+        hooks: {\\n+          onPromptCaptured: (info: { step: string; provider: string; prompt: string }) => {\\n+            const k = info.step;\\n+            if (!prompts[k]) prompts[k] = [];\\n+            const p =\\n+              defaultPromptCap && info.prompt.length > defaultPromptCap\\n+                ? info.prompt.slice(0, defaultPromptCap)\\n+                : info.prompt;\\n+            prompts[k].push(p);\\n+          },\\n+          mockForStep: (step: string) => {\\n+            // Support list form: '<step>[]' means per-call mocks for forEach children\\n+            const listKey = `${step}[]`;\\n+            const list = (mocks as any)[listKey];\\n+            if (Array.isArray(list)) {\\n+              const i = mockCursors[listKey] || 0;\\n+              const idx = i < list.length ? i : list.length - 1; // clamp to last\\n+              mockCursors[listKey] = i + 1;\\n+              return list[idx];\\n+            }\\n+            return (mocks as any)[step];\\n+          },\\n+        },\\n+      } as any);\\n+\\n+      try {\\n+        const eventForCase = this.mapEventFromFixtureName(fixtureInput?.builtin);\\n+        const desiredSteps = new Set<string>(\\n+          (expect.calls || []).map(c => c.step).filter(Boolean) as string[]\\n+        );\\n+        let checksToRun = this.computeChecksToRun(\\n+          cfg,\\n+          eventForCase,\\n+          desiredSteps.size > 0 ? desiredSteps : undefined\\n+        );\\n+        this.printSelectedChecks(checksToRun);\\n+        if (checksToRun.length === 0) {\\n+          // Fallback: run all checks for this event when filtered set is empty\\n+          checksToRun = this.computeChecksToRun(cfg, eventForCase, undefined);\\n+        }\\n+        // Include all tagged checks by default in test mode: build tagFilter.include = union of all tags\\n+        // Do not pass an implicit tag filter during tests.\\n+        // Passing all known tags as an include-filter would exclude untagged steps.\\n+        // Let the engine apply whatever tag_filter the config already defines (if any).\\n+        const allTags: string[] = [];\\n+        // Inject octokit into eventContext so providers can perform real GitHub ops (recorded)\\n+        try {\\n+          (prInfo as any).eventContext = { ...(prInfo as any).eventContext, octokit: recorder };\\n+        } catch {}\\n+\\n+        const prevTestMode = process.env.VISOR_TEST_MODE;\\n+        process.env.VISOR_TEST_MODE = 'true';\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.log(`  ⮕ executing main stage with checks=[${checksToRun.join(', ')}]`);\\n+        }\\n+        let res = await engine.executeGroupedChecks(\\n+          prInfo,\\n+          checksToRun,\\n+          120000,\\n+          cfg,\\n+          'json',\\n+          process.env.VISOR_DEBUG === 'true',\\n+          undefined,\\n+          false,\\n+          {}\\n+        );\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          try {\\n+            const names = (res.statistics.checks || []).map(\\n+              (c: any) => `${c.checkName}:${c.totalRuns || 0}`\\n+            );\\n+            console.log(`  ⮕ main stats: [${names.join(', ')}]`);\\n+          } catch {}\\n+        }\\n+        try {\\n+          const dbgHist = engine.getOutputHistorySnapshot();\\n+          console.log(\\n+            `  ⮕ stage base history keys: ${Object.keys(dbgHist).join(', ') || '(none)'}`\\n+          );\\n+        } catch {}\\n+        // After main stage run, ensure static on_finish.run targets for forEach parents executed.\\n+        try {\\n+          const hist0 = engine.getOutputHistorySnapshot();\\n+          if (process.env.VISOR_DEBUG === 'true') {\\n+            console.log(`  ⮕ history keys: ${Object.keys(hist0).join(', ') || '(none)'}`);\\n+          }\\n+          const parents = Object.entries(cfg.checks || {})\\n+            .filter(\\n+              ([name, c]: [string, any]) =>\\n+                checksToRun.includes(name) &&\\n+                c &&\\n+                c.forEach &&\\n+                c.on_finish &&\\n+                Array.isArray(c.on_finish.run) &&\\n+                c.on_finish.run.length > 0\\n+            )\\n+            .map(([name, c]: [string, any]) => ({ name, onFinish: c.on_finish }));\\n+          if (process.env.VISOR_DEBUG === 'true') {\\n+            console.log(\\n+              `  ⮕ forEach parents with on_finish: ${parents.map(p => p.name).join(', ') || '(none)'}`\\n+            );\\n+          }\\n+          const missing: string[] = [];\\n+          for (const p of parents) {\\n+            for (const t of p.onFinish.run as string[]) {\\n+              if (!hist0[t] || (Array.isArray(hist0[t]) && hist0[t].length === 0)) {\\n+                missing.push(t);\\n+              }\\n+            }\\n+          }\\n+          // Dedup missing and exclude anything already in checksToRun\\n+          const toRun = Array.from(new Set(missing.filter(n => !checksToRun.includes(n))));\\n+          if (toRun.length > 0) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ⮕ on_finish.fallback: running [${toRun.join(', ')}]`);\\n+            }\\n+            // Run once; reuse same engine instance so output history stays visible\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ⮕ executing on_finish.fallback with checks=[${toRun.join(', ')}]`);\\n+            }\\n+            const fallbackRes = await engine.executeGroupedChecks(\\n+              prInfo,\\n+              toRun,\\n+              120000,\\n+              cfg,\\n+              'json',\\n+              process.env.VISOR_DEBUG === 'true',\\n+              undefined,\\n+              false,\\n+              {}\\n+            );\\n+            // Optionally merge statistics (for stage coverage we rely on deltas + stats from last run)\\n+            res = {\\n+              results: fallbackRes.results || res.results,\\n+              statistics: fallbackRes.statistics || res.statistics,\\n+            } as any;\\n+          }\\n+        } catch {}\\n+        if (prevTestMode === undefined) delete process.env.VISOR_TEST_MODE;\\n+        else process.env.VISOR_TEST_MODE = prevTestMode;\\n+        const outHistory = engine.getOutputHistorySnapshot();\\n+\\n+        const caseFailures = this.evaluateCase(\\n+          _case.name,\\n+          res.statistics,\\n+          recorder,\\n+          expect,\\n+          strict,\\n+          prompts,\\n+          res.results,\\n+          outHistory\\n+        );\\n+        // Warn about unmocked AI/command steps that executed\\n+        try {\\n+          const mocksUsed =\\n+            typeof (_case as any).mocks === 'object' && (_case as any).mocks\\n+              ? ((_case as any).mocks as Record<string, unknown>)\\n+              : {};\\n+          this.warnUnmockedProviders(res.statistics, cfg, mocksUsed);\\n+        } catch {}\\n+        this.printCoverage(_case.name, res.statistics, expect);\\n+        if (caseFailures.length === 0) {\\n+          console.log(`✅ PASS ${_case.name}`);\\n+          caseResults.push({ name: _case.name, passed: true });\\n+        } else {\\n+          console.log(`❌ FAIL ${_case.name}`);\\n+          for (const f of caseFailures) console.log(`   - ${f}`);\\n+          caseResults.push({ name: _case.name, passed: false, errors: caseFailures });\\n+          return { name: _case.name, failed: 1 };\\n+        }\\n+      } catch (err) {\\n+        console.log(`❌ ERROR ${_case.name}: ${err instanceof Error ? err.message : String(err)}`);\\n+        caseResults.push({\\n+          name: _case.name,\\n+          passed: false,\\n+          errors: [err instanceof Error ? err.message : String(err)],\\n+        });\\n+        return { name: _case.name, failed: 1 };\\n+      } finally {\\n+        if (prevRepo === undefined) delete process.env.GITHUB_REPOSITORY;\\n+        else process.env.GITHUB_REPOSITORY = prevRepo;\\n+        if (envOverrides) {\\n+          for (const [k, oldv] of Object.entries(prevEnv)) {\\n+            if (oldv === undefined) delete process.env[k];\\n+            else process.env[k] = oldv;\\n+          }\\n+        }\\n+      }\\n+      return { name: _case.name, failed: 0 };\\n+    };\\n+\\n+    if (options.bail || false || caseMaxParallel <= 1) {\\n+      for (const _case of selected) {\\n+        const r = await runOne(_case);\\n+        failures += r.failed;\\n+        if (options.bail && r.failed > 0) break;\\n+      }\\n+    } else {\\n+      let idx = 0;\\n+      const workers = Math.min(caseMaxParallel, selected.length);\\n+      const runWorker = async () => {\\n+        while (true) {\\n+          const i = idx++;\\n+          if (i >= selected.length) return;\\n+          const r = await runOne(selected[i]);\\n+          failures += r.failed;\\n+        }\\n+      };\\n+      await Promise.all(Array.from({ length: workers }, runWorker));\\n+    }\\n+\\n+    // Summary\\n+    const passed = selected.length - failures;\\n+    console.log(`\\\\nSummary: ${passed}/${selected.length} passed`);\\n+    return { failures, results: caseResults };\\n+  }\\n+\\n+  private async runFlowCase(\\n+    flowCase: any,\\n+    cfg: any,\\n+    defaultStrict: boolean,\\n+    bail: boolean,\\n+    promptCap?: number,\\n+    stageFilter?: string\\n+  ): Promise<{ failures: number; stages: Array<{ name: string; errors?: string[] }> }> {\\n+    const suiteDefaults: any = (this as any).suiteDefaults || {};\\n+    const ghRec = suiteDefaults.github_recorder as\\n+      | { error_code?: number; timeout_ms?: number }\\n+      | undefined;\\n+    const ghRecCase =\\n+      typeof (flowCase as any).github_recorder === 'object' && (flowCase as any).github_recorder\\n+        ? ((flowCase as any).github_recorder as { error_code?: number; timeout_ms?: number })\\n+        : undefined;\\n+    const rcOpts = ghRecCase || ghRec;\\n+    const recorder = new RecordingOctokit(\\n+      rcOpts ? { errorCode: rcOpts.error_code, timeoutMs: rcOpts.timeout_ms } : undefined\\n+    );\\n+    setGlobalRecorder(recorder);\\n+    const engine = new CheckExecutionEngine(undefined as any, recorder as unknown as any);\\n+    const flowName = flowCase.name || 'flow';\\n+    let failures = 0;\\n+    const stagesSummary: Array<{ name: string; errors?: string[] }> = [];\\n+\\n+    // Shared prompts map across flow; we will compute per-stage deltas\\n+    const prompts: Record<string, string[]> = {};\\n+    let stageMocks: Record<string, unknown> =\\n+      typeof flowCase.mocks === 'object' && flowCase.mocks\\n+        ? (flowCase.mocks as Record<string, unknown>)\\n+        : {};\\n+    let stageMockCursors: Record<string, number> = {};\\n+    engine.setExecutionContext({\\n+      hooks: {\\n+        onPromptCaptured: (info: { step: string; provider: string; prompt: string }) => {\\n+          const k = info.step;\\n+          if (!prompts[k]) prompts[k] = [];\\n+          const p =\\n+            promptCap && info.prompt.length > promptCap\\n+              ? info.prompt.slice(0, promptCap)\\n+              : info.prompt;\\n+          prompts[k].push(p);\\n+        },\\n+        mockForStep: (step: string) => {\\n+          const listKey = `${step}[]`;\\n+          const list = (stageMocks as any)[listKey];\\n+          if (Array.isArray(list)) {\\n+            const i = stageMockCursors[listKey] || 0;\\n+            const idx = i < list.length ? i : list.length - 1;\\n+            stageMockCursors[listKey] = i + 1;\\n+            return list[idx];\\n+          }\\n+          return (stageMocks as any)[step];\\n+        },\\n+      },\\n+    } as any);\\n+\\n+    // Run each stage\\n+    // Normalize stage filter\\n+    const sf = (stageFilter || '').trim().toLowerCase();\\n+    const sfIndex = sf && /^\\\\d+$/.test(sf) ? parseInt(sf, 10) : undefined;\\n+    let anyStageRan = false;\\n+    for (let i = 0; i < flowCase.flow.length; i++) {\\n+      const stage = flowCase.flow[i];\\n+      const stageName = `${flowName}#${stage.name || `stage-${i + 1}`}`;\\n+      // Apply stage filter if provided: match by name substring or 1-based index\\n+      if (sf) {\\n+        const nm = String(stage.name || `stage-${i + 1}`).toLowerCase();\\n+        const idxMatch = sfIndex !== undefined && sfIndex === i + 1;\\n+        const nameMatch = nm.includes(sf);\\n+        if (!(idxMatch || nameMatch)) continue;\\n+      }\\n+      anyStageRan = true;\\n+      const strict = (\\n+        typeof flowCase.strict === 'boolean' ? flowCase.strict : defaultStrict\\n+      ) as boolean;\\n+\\n+      // Fixture + env\\n+      const fixtureInput =\\n+        typeof stage.fixture === 'object' && stage.fixture\\n+          ? stage.fixture\\n+          : { builtin: stage.fixture };\\n+      const prInfo = this.buildPrInfoFromFixture(fixtureInput?.builtin, fixtureInput?.overrides);\\n+\\n+      // Stage env overrides\\n+      const envOverrides =\\n+        typeof stage.env === 'object' && stage.env\\n+          ? (stage.env as Record<string, string>)\\n+          : undefined;\\n+      const prevEnv: Record<string, string | undefined> = {};\\n+      if (envOverrides) {\\n+        for (const [k, v] of Object.entries(envOverrides)) {\\n+          prevEnv[k] = process.env[k];\\n+          process.env[k] = String(v);\\n+        }\\n+      }\\n+\\n+      // Merge per-stage mocks over flow-level defaults (stage overrides flow)\\n+      try {\\n+        const perStage =\\n+          typeof (stage as any).mocks === 'object' && (stage as any).mocks\\n+            ? ((stage as any).mocks as Record<string, unknown>)\\n+            : {};\\n+        stageMocks = { ...(flowCase.mocks || {}), ...perStage } as Record<string, unknown>;\\n+        stageMockCursors = {};\\n+      } catch {}\\n+\\n+      // Baselines for deltas\\n+      const promptBase: Record<string, number> = {};\\n+      for (const [k, arr] of Object.entries(prompts)) promptBase[k] = arr.length;\\n+      const callBase = recorder.calls.length;\\n+      const histBase: Record<string, number> = {};\\n+      // We need access to engine.outputHistory lengths; get snapshot\\n+      const baseHistSnap = (engine as any).outputHistory as Map<string, unknown[]> | undefined;\\n+      if (baseHistSnap) {\\n+        for (const [k, v] of baseHistSnap.entries()) histBase[k] = (v || []).length;\\n+      }\\n+\\n+      try {\\n+        const eventForStage = this.mapEventFromFixtureName(fixtureInput?.builtin);\\n+        this.printStageHeader(\\n+          flowName,\\n+          stage.name || `stage-${i + 1}`,\\n+          eventForStage,\\n+          fixtureInput?.builtin\\n+        );\\n+        // Select checks purely by event to preserve natural routing/dependencies\\n+        const desiredSteps = new Set<string>(\\n+          ((stage.expect || {}).calls || []).map((c: any) => c.step).filter(Boolean) as string[]\\n+        );\\n+        let checksToRun = this.computeChecksToRun(cfg, eventForStage, undefined);\\n+        // Defer on_finish targets: if a forEach parent declares on_finish.run: [targets]\\n+        // and both the parent and target are in the list, remove the target from the\\n+        // initial execution set so it executes in the correct order via on_finish.\\n+        try {\\n+          const parents = Object.entries(cfg.checks || {})\\n+            .filter(\\n+              ([_, c]: [string, any]) =>\\n+                c &&\\n+                c.forEach &&\\n+                c.on_finish &&\\n+                (Array.isArray(c.on_finish.run) || typeof c.on_finish.run_js === 'string')\\n+            )\\n+            .map(([name, c]: [string, any]) => ({ name, onFinish: c.on_finish }));\\n+          if (parents.length > 0 && checksToRun.length > 0) {\\n+            const removal = new Set<string>();\\n+            for (const p of parents) {\\n+              const staticTargets: string[] = Array.isArray(p.onFinish.run) ? p.onFinish.run : [];\\n+              // Only consider static targets here; dynamic run_js will still execute at runtime\\n+              for (const t of staticTargets) {\\n+                if (checksToRun.includes(p.name) && checksToRun.includes(t)) {\\n+                  removal.add(t);\\n+                }\\n+              }\\n+            }\\n+            if (removal.size > 0) {\\n+              checksToRun = checksToRun.filter(n => !removal.has(n));\\n+            }\\n+          }\\n+        } catch {}\\n+        this.printSelectedChecks(checksToRun);\\n+        if (!checksToRun || checksToRun.length === 0) {\\n+          checksToRun = this.computeChecksToRun(cfg, eventForStage, undefined);\\n+        }\\n+        // Do not pass an implicit tag filter during tests.\\n+        const allTags: string[] = [];\\n+        // Ensure eventContext carries octokit for recorded GitHub ops\\n+        try {\\n+          (prInfo as any).eventContext = { ...(prInfo as any).eventContext, octokit: recorder };\\n+        } catch {}\\n+        // Mark test mode for the engine to enable non-network side-effects (e.g., posting PR comments\\n+        // through the injected recording Octokit). Restore after the run.\\n+        const prevTestMode = process.env.VISOR_TEST_MODE;\\n+        process.env.VISOR_TEST_MODE = 'true';\\n+        let res = await engine.executeGroupedChecks(\\n+          prInfo,\\n+          checksToRun,\\n+          120000,\\n+          cfg,\\n+          'json',\\n+          process.env.VISOR_DEBUG === 'true',\\n+          undefined,\\n+          false,\\n+          undefined\\n+        );\\n+        // Ensure static on_finish.run targets for forEach parents executed in this stage\\n+        try {\\n+          const hist0 = engine.getOutputHistorySnapshot();\\n+          const parents = Object.entries(cfg.checks || {})\\n+            .filter(\\n+              ([name, c]: [string, any]) =>\\n+                checksToRun.includes(name) &&\\n+                c &&\\n+                c.forEach &&\\n+                c.on_finish &&\\n+                Array.isArray(c.on_finish.run) &&\\n+                c.on_finish.run.length > 0\\n+            )\\n+            .map(([name, c]: [string, any]) => ({ name, onFinish: c.on_finish }));\\n+          const missing: string[] = [];\\n+          for (const p of parents) {\\n+            for (const t of p.onFinish.run as string[]) {\\n+              if (!hist0[t] || (Array.isArray(hist0[t]) && hist0[t].length === 0)) missing.push(t);\\n+            }\\n+          }\\n+          const toRun = Array.from(new Set(missing.filter(n => !checksToRun.includes(n))));\\n+          if (toRun.length > 0) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ⮕ on_finish.fallback: running [${toRun.join(', ')}]`);\\n+            }\\n+            const fallbackRes = await engine.executeGroupedChecks(\\n+              prInfo,\\n+              toRun,\\n+              120000,\\n+              cfg,\\n+              'json',\\n+              process.env.VISOR_DEBUG === 'true',\\n+              undefined,\\n+              false,\\n+              undefined\\n+            );\\n+            res = {\\n+              results: fallbackRes.results || res.results,\\n+              statistics: fallbackRes.statistics || res.statistics,\\n+            } as any;\\n+          }\\n+          // If we observe any invalid validations in history but no second assistant reply yet,\\n+          // seed memory with issues and create a correction reply explicitly.\\n+          try {\\n+            const snap = engine.getOutputHistorySnapshot();\\n+            const vf = (snap['validate-fact'] || []) as Array<any>;\\n+            const hasInvalid =\\n+              Array.isArray(vf) && vf.some(v => v && (v.is_valid === false || v.valid === false));\\n+            // Fallback: also look at provided mocks for validate-fact[]\\n+            let mockInvalid: any[] | undefined;\\n+            try {\\n+              const list = (stageMocks as any)['validate-fact[]'];\\n+              if (Array.isArray(list)) {\\n+                const bad = list.filter(v => v && (v.is_valid === false || v.valid === false));\\n+                if (bad.length > 0) mockInvalid = bad;\\n+              }\\n+            } catch {}\\n+            if (hasInvalid || (mockInvalid && mockInvalid.length > 0)) {\\n+              // Seed memory so comment-assistant prompt includes <previous_response> + corrections\\n+              const issues = (hasInvalid ? vf : mockInvalid!)\\n+                .filter(v => v && (v.is_valid === false || v.valid === false))\\n+                .map(v => ({ claim: v.claim, evidence: v.evidence, correction: v.correction }));\\n+              const { MemoryStore } = await import('../memory-store');\\n+              const mem = MemoryStore.getInstance();\\n+              mem.set('fact_validation_issues', issues, 'fact-validation');\\n+              // Produce the correction reply but avoid re-initializing validation in this stage\\n+              const prevVal = process.env.ENABLE_FACT_VALIDATION;\\n+              process.env.ENABLE_FACT_VALIDATION = 'false';\\n+              try {\\n+                if (process.env.VISOR_DEBUG === 'true') {\\n+                  console.log('  ⮕ executing correction pass with checks=[comment-assistant]');\\n+                }\\n+                await engine.executeGroupedChecks(\\n+                  prInfo,\\n+                  ['comment-assistant'],\\n+                  120000,\\n+                  cfg,\\n+                  'json',\\n+                  process.env.VISOR_DEBUG === 'true',\\n+                  undefined,\\n+                  false,\\n+                  {}\\n+                );\\n+              } finally {\\n+                if (prevVal === undefined) delete process.env.ENABLE_FACT_VALIDATION;\\n+                else process.env.ENABLE_FACT_VALIDATION = prevVal;\\n+              }\\n+            }\\n+          } catch {}\\n+        } catch {}\\n+        if (prevTestMode === undefined) delete process.env.VISOR_TEST_MODE;\\n+        else process.env.VISOR_TEST_MODE = prevTestMode;\\n+\\n+        // Build stage-local prompts map (delta)\\n+        const stagePrompts: Record<string, string[]> = {};\\n+        for (const [k, arr] of Object.entries(prompts)) {\\n+          const start = promptBase[k] || 0;\\n+          stagePrompts[k] = arr.slice(start);\\n+        }\\n+        // Build stage-local output history (delta)\\n+        const histSnap = engine.getOutputHistorySnapshot();\\n+        const stageHist: Record<string, unknown[]> = {};\\n+        for (const [k, arr] of Object.entries(histSnap)) {\\n+          const start = histBase[k] || 0;\\n+          stageHist[k] = (arr as unknown[]).slice(start);\\n+        }\\n+\\n+        // Build stage-local execution view using:\\n+        //  - stage deltas (prompts + output history), and\\n+        //  - engine-reported statistics for this run (captures checks without prompts/outputs,\\n+        //    e.g., memory steps triggered in on_finish), and\\n+        //  - the set of checks we explicitly selected to run.\\n+        type ExecStat = import('../check-execution-engine').ExecutionStatistics;\\n+        const names = new Set<string>();\\n+        // Names from prompts delta\\n+        try {\\n+          for (const [k, arr] of Object.entries(stagePrompts)) {\\n+            if (k && Array.isArray(arr) && arr.length > 0) names.add(k);\\n+          }\\n+        } catch {}\\n+        // Names from output history delta\\n+        try {\\n+          for (const [k, arr] of Object.entries(stageHist)) {\\n+            if (k && Array.isArray(arr) && arr.length > 0) names.add(k);\\n+          }\\n+        } catch {}\\n+        // Names from engine stats for this run (include fallback runs)\\n+        try {\\n+          const statsList = [res.statistics];\\n+          // Attempt to reuse intermediate stats captured by earlier fallback runs if present\\n+          // We can’t reach into engine internals here, so rely on prompts/history for now.\\n+          for (const stats of statsList) {\\n+            for (const chk of stats.checks || []) {\\n+              if (chk && typeof chk.checkName === 'string' && (chk.totalRuns || 0) > 0) {\\n+                names.add(chk.checkName);\\n+              }\\n+            }\\n+          }\\n+        } catch {}\\n+        // Names we explicitly selected to run (in case a step executed without outputs/prompts or stats)\\n+        for (const n of checksToRun) names.add(n);\\n+\\n+        const checks = Array.from(names).map(name => {\\n+          const histRuns = Array.isArray(stageHist[name]) ? stageHist[name].length : 0;\\n+          const promptRuns = Array.isArray(stagePrompts[name]) ? stagePrompts[name].length : 0;\\n+          const inferred = Math.max(histRuns, promptRuns);\\n+          let statRuns = 0;\\n+          try {\\n+            const st = (res.statistics.checks || []).find(c => c.checkName === name);\\n+            statRuns = st ? st.totalRuns || 0 : 0;\\n+          } catch {}\\n+          const runs = Math.max(inferred, statRuns);\\n+          return {\\n+            checkName: name,\\n+            totalRuns: runs,\\n+            successfulRuns: runs,\\n+            failedRuns: 0,\\n+            skipped: false,\\n+            totalDuration: 0,\\n+            issuesFound: 0,\\n+            issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n+            perIterationDuration: [],\\n+          } as any;\\n+        });\\n+        // Note: correction passes and fallback runs are captured via history/prompts deltas\\n+        // and engine statistics; we do not apply per-step heuristics here.\\n+        // Heuristic reconciliation: if GitHub createComment calls increased in this stage,\\n+        // reflect them as additional runs for 'comment-assistant' when present.\\n+        try {\\n+          const expectedCalls = new Map<string, number>();\\n+          for (const c of ((stage.expect || {}).calls || []) as any[]) {\\n+            if (c && typeof c.step === 'string' && typeof c.exactly === 'number') {\\n+              expectedCalls.set(c.step, c.exactly);\\n+            }\\n+          }\\n+          const newCalls = recorder.calls.slice(callBase);\\n+          const created = newCalls.filter(c => c && c.op === 'issues.createComment').length;\\n+          const idx = checks.findIndex(c => c.checkName === 'comment-assistant');\\n+          if (idx >= 0 && created > 0) {\\n+            const want = expectedCalls.get('comment-assistant');\\n+            const current = checks[idx].totalRuns || 0;\\n+            const reconciled = Math.max(current, created);\\n+            checks[idx].totalRuns =\\n+              typeof want === 'number' ? Math.min(want, reconciled) : reconciled;\\n+            checks[idx].successfulRuns = checks[idx].totalRuns;\\n+          }\\n+        } catch {}\\n+        const stageStats: ExecStat = {\\n+          totalChecksConfigured: checks.length,\\n+          totalExecutions: checks.reduce((a, c: any) => a + (c.totalRuns || 0), 0),\\n+          successfulExecutions: checks.reduce((a, c: any) => a + (c.successfulRuns || 0), 0),\\n+          failedExecutions: checks.reduce((a, c: any) => a + (c.failedRuns || 0), 0),\\n+          skippedChecks: 0,\\n+          totalDuration: 0,\\n+          checks,\\n+        } as any;\\n+\\n+        // Evaluate stage expectations\\n+        const expect = stage.expect || {};\\n+        const caseFailures = this.evaluateCase(\\n+          stageName,\\n+          stageStats,\\n+          // Use only call delta for stage\\n+          { calls: recorder.calls.slice(callBase) } as any,\\n+          expect,\\n+          strict,\\n+          stagePrompts,\\n+          res.results,\\n+          stageHist\\n+        );\\n+        // Warn about unmocked AI/command steps that executed (stage-specific mocks)\\n+        try {\\n+          const stageMocksLocal =\\n+            typeof (stage as any).mocks === 'object' && (stage as any).mocks\\n+              ? ((stage as any).mocks as Record<string, unknown>)\\n+              : {};\\n+          const merged = { ...(flowCase.mocks || {}), ...stageMocksLocal } as Record<\\n+            string,\\n+            unknown\\n+          >;\\n+          this.warnUnmockedProviders(stageStats, cfg, merged);\\n+        } catch {}\\n+        // Use stage-local stats for coverage to avoid cross-stage bleed\\n+        this.printCoverage(stageName, stageStats, expect);\\n+        if (caseFailures.length === 0) {\\n+          console.log(`✅ PASS ${stageName}`);\\n+          stagesSummary.push({ name: stageName });\\n+        } else {\\n+          failures += 1;\\n+          console.log(`❌ FAIL ${stageName}`);\\n+          for (const f of caseFailures) console.log(`   - ${f}`);\\n+          stagesSummary.push({ name: stageName, errors: caseFailures });\\n+          if (bail) break;\\n+        }\\n+      } catch (err) {\\n+        failures += 1;\\n+        console.log(`❌ ERROR ${stageName}: ${err instanceof Error ? err.message : String(err)}`);\\n+        stagesSummary.push({\\n+          name: stageName,\\n+          errors: [err instanceof Error ? err.message : String(err)],\\n+        });\\n+        if (bail) break;\\n+      } finally {\\n+        if (envOverrides) {\\n+          for (const [k, oldv] of Object.entries(prevEnv)) {\\n+            if (oldv === undefined) delete process.env[k];\\n+            else process.env[k] = oldv;\\n+          }\\n+        }\\n+      }\\n+    }\\n+\\n+    // Summary line for flow\\n+    if (!anyStageRan && stageFilter) {\\n+      console.log(`⚠️  No stage matched filter '${stageFilter}' in flow '${flowName}'`);\\n+    }\\n+    if (failures === 0) console.log(`✅ FLOW PASS ${flowName}`);\\n+    else\\n+      console.log(`❌ FLOW FAIL ${flowName} (${failures} stage error${failures > 1 ? 's' : ''})`);\\n+    return { failures, stages: stagesSummary };\\n+  }\\n+\\n+  private mapEventFromFixtureName(name?: string): import('../types/config').EventTrigger {\\n+    if (!name) return 'manual';\\n+    if (name.includes('pr_open')) return 'pr_opened';\\n+    if (name.includes('pr_sync')) return 'pr_updated';\\n+    if (name.includes('pr_closed')) return 'pr_closed';\\n+    if (name.includes('issue_comment')) return 'issue_comment';\\n+    if (name.includes('issue_open')) return 'issue_opened';\\n+    return 'manual';\\n+  }\\n+\\n+  // Print warnings when AI or command steps execute without mocks in tests\\n+  private warnUnmockedProviders(\\n+    stats: import('../check-execution-engine').ExecutionStatistics,\\n+    cfg: any,\\n+    mocks: Record<string, unknown>\\n+  ): void {\\n+    try {\\n+      const executed = stats.checks\\n+        .filter(s => !s.skipped && (s.totalRuns || 0) > 0)\\n+        .map(s => s.checkName);\\n+      for (const name of executed) {\\n+        const chk = (cfg.checks || {})[name] || {};\\n+        const t = chk.type || 'ai';\\n+        // Suppress warnings for AI steps explicitly running under the mock provider\\n+        const aiProv = (chk.ai && (chk.ai as any).provider) || undefined;\\n+        if (t === 'ai' && aiProv === 'mock') continue;\\n+        if ((t === 'ai' || t === 'command') && mocks[name] === undefined) {\\n+          console.warn(\\n+            `⚠️  Unmocked ${t} step executed: ${name} (add mocks:\\\\n  ${name}: <mock content>)`\\n+          );\\n+        }\\n+      }\\n+    } catch {}\\n+  }\\n+\\n+  private buildPrInfoFromFixture(\\n+    fixtureName?: string,\\n+    overrides?: Record<string, unknown>\\n+  ): PRInfo {\\n+    const eventType = this.mapEventFromFixtureName(fixtureName);\\n+    const isIssue = eventType === 'issue_opened' || eventType === 'issue_comment';\\n+    const number = 1;\\n+    const loader = new FixtureLoader();\\n+    const fx =\\n+      fixtureName && fixtureName.startsWith('gh.') ? loader.load(fixtureName as any) : undefined;\\n+    const title =\\n+      (fx?.webhook.payload as any)?.pull_request?.title ||\\n+      (fx?.webhook.payload as any)?.issue?.title ||\\n+      (isIssue ? 'Sample issue title' : 'feat: add user search');\\n+    const body = (fx?.webhook.payload as any)?.issue?.body || (isIssue ? 'Issue body' : 'PR body');\\n+    const commentBody = (fx?.webhook.payload as any)?.comment?.body;\\n+    const prInfo: PRInfo = {\\n+      number,\\n+      title,\\n+      body,\\n+      author: 'test-user',\\n+      authorAssociation: 'MEMBER',\\n+      base: 'main',\\n+      head: 'feature/test',\\n+      files: (fx?.files || []).map(f => ({\\n+        filename: f.path,\\n+        additions: f.additions || 0,\\n+        deletions: f.deletions || 0,\\n+        changes: (f.additions || 0) + (f.deletions || 0),\\n+        status: (f.status as any) || 'modified',\\n+        patch: f.content ? `@@\\\\n+${f.content}` : undefined,\\n+      })),\\n+      totalAdditions: 0,\\n+      totalDeletions: 0,\\n+      eventType,\\n+      fullDiff: fx?.diff,\\n+      isIssue,\\n+      eventContext: {\\n+        event_name:\\n+          fx?.webhook?.name ||\\n+          (isIssue ? (eventType === 'issue_comment' ? 'issue_comment' : 'issues') : 'pull_request'),\\n+        action:\\n+          fx?.webhook?.action ||\\n+          (eventType === 'pr_opened'\\n+            ? 'opened'\\n+            : eventType === 'pr_updated'\\n+              ? 'synchronize'\\n+              : undefined),\\n+        issue: isIssue ? { number, title, body, user: { login: 'test-user' } } : undefined,\\n+        pull_request: !isIssue\\n+          ? { number, title, head: { ref: 'feature/test' }, base: { ref: 'main' } }\\n+          : undefined,\\n+        repository: { owner: { login: 'owner' }, name: 'repo' },\\n+        comment:\\n+          eventType === 'issue_comment'\\n+            ? { body: commentBody || 'dummy', user: { login: 'contributor' } }\\n+            : undefined,\\n+      },\\n+    };\\n+\\n+    // Apply overrides: pr.* to PRInfo; webhook.* to eventContext\\n+    if (overrides && typeof overrides === 'object') {\\n+      for (const [k, v] of Object.entries(overrides)) {\\n+        if (k.startsWith('pr.')) {\\n+          const key = k.slice(3);\\n+          (prInfo as any)[key] = v as any;\\n+        } else if (k.startsWith('webhook.')) {\\n+          const path = k.slice(8);\\n+          this.deepSet(\\n+            (prInfo as any).eventContext || ((prInfo as any).eventContext = {}),\\n+            path,\\n+            v\\n+          );\\n+        }\\n+      }\\n+    }\\n+    // Test mode: avoid heavy diff processing and file reads\\n+    try {\\n+      (prInfo as any).includeCodeContext = false;\\n+      (prInfo as any).isPRContext = false;\\n+    } catch {}\\n+    return prInfo;\\n+  }\\n+\\n+  private deepSet(target: any, path: string, value: unknown): void {\\n+    const parts: (string | number)[] = [];\\n+    const regex = /\\\\[(\\\\d+)\\\\]|\\\\['([^']+)'\\\\]|\\\\[\\\"([^\\\"]+)\\\"\\\\]|\\\\.([^\\\\.\\\\[\\\\]]+)/g;\\n+    let m: RegExpExecArray | null;\\n+    let cursor = 0;\\n+    if (!path.startsWith('.') && !path.startsWith('[')) {\\n+      const first = path.split('.')[0];\\n+      parts.push(first);\\n+      cursor = first.length;\\n+    }\\n+    while ((m = regex.exec(path)) !== null) {\\n+      if (m.index !== cursor) continue;\\n+      cursor = regex.lastIndex;\\n+      if (m[1] !== undefined) parts.push(Number(m[1]));\\n+      else if (m[2] !== undefined) parts.push(m[2]);\\n+      else if (m[3] !== undefined) parts.push(m[3]);\\n+      else if (m[4] !== undefined) parts.push(m[4]);\\n+    }\\n+    let obj = target;\\n+    for (let i = 0; i < parts.length - 1; i++) {\\n+      const key = parts[i] as any;\\n+      if (obj[key] == null || typeof obj[key] !== 'object') {\\n+        obj[key] = typeof parts[i + 1] === 'number' ? [] : {};\\n+      }\\n+      obj = obj[key];\\n+    }\\n+    obj[parts[parts.length - 1] as any] = value;\\n+  }\\n+\\n+  private evaluateCase(\\n+    caseName: string,\\n+    stats: import('../check-execution-engine').ExecutionStatistics,\\n+    recorder: RecordingOctokit,\\n+    expect: ExpectBlock,\\n+    strict: boolean,\\n+    promptsByStep: Record<string, string[]>,\\n+    results: import('../reviewer').GroupedCheckResults,\\n+    outputHistory: Record<string, unknown[]>\\n+  ): string[] {\\n+    const errors: string[] = [];\\n+\\n+    // Build executed steps map\\n+    const executed: Record<string, number> = {};\\n+    for (const s of stats.checks) {\\n+      if (!s.skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n+    }\\n+\\n+    // Strict mode: every executed step must have an expect.calls entry\\n+    if (strict) {\\n+      const expectedSteps = new Set(\\n+        (expect.calls || []).filter(c => c.step).map(c => String(c.step))\\n+      );\\n+      for (const step of Object.keys(executed)) {\\n+        if (!expectedSteps.has(step)) {\\n+          errors.push(`Step executed without expect: ${step}`);\\n+        }\\n+      }\\n+    }\\n+\\n+    // Validate step count expectations\\n+    for (const call of expect.calls || []) {\\n+      if (call.step) {\\n+        validateCounts(call);\\n+        const actual = executed[call.step] || 0;\\n+        if (call.exactly !== undefined && actual !== call.exactly) {\\n+          errors.push(`Expected step ${call.step} exactly ${call.exactly}, got ${actual}`);\\n+        }\\n+        if (call.at_least !== undefined && actual < call.at_least) {\\n+          errors.push(`Expected step ${call.step} at_least ${call.at_least}, got ${actual}`);\\n+        }\\n+        if (call.at_most !== undefined && actual > call.at_most) {\\n+          errors.push(`Expected step ${call.step} at_most ${call.at_most}, got ${actual}`);\\n+        }\\n+      }\\n+    }\\n+\\n+    // Provider call expectations (GitHub)\\n+    for (const call of expect.calls || []) {\\n+      if (call.provider && String(call.provider).toLowerCase() === 'github') {\\n+        validateCounts(call);\\n+        const op = this.mapGithubOp(call.op || '');\\n+        const matched = recorder.calls.filter(c => !op || c.op === op);\\n+        const actual = matched.length;\\n+        if (call.exactly !== undefined && actual !== call.exactly) {\\n+          errors.push(`Expected github ${call.op} exactly ${call.exactly}, got ${actual}`);\\n+        }\\n+        if (call.at_least !== undefined && actual < call.at_least) {\\n+          errors.push(`Expected github ${call.op} at_least ${call.at_least}, got ${actual}`);\\n+        }\\n+        if (call.at_most !== undefined && actual > call.at_most) {\\n+          errors.push(`Expected github ${call.op} at_most ${call.at_most}, got ${actual}`);\\n+        }\\n+        // Simple args.contains support (arrays only)\\n+        if (call.args && (call.args as any).contains && op.endsWith('addLabels')) {\\n+          const want = (call.args as any).contains as unknown[];\\n+          const ok = matched.some(m => {\\n+            const labels = (m.args as any)?.labels || [];\\n+            return Array.isArray(labels) && want.every(w => labels.includes(w));\\n+          });\\n+          if (!ok) errors.push(`Expected github ${call.op} args.contains not satisfied`);\\n+        }\\n+      }\\n+    }\\n+\\n+    // no_calls assertions (provider-only basic)\\n+    for (const nc of expect.no_calls || []) {\\n+      if (nc.provider && String(nc.provider).toLowerCase() === 'github') {\\n+        const op = this.mapGithubOp((nc as any).op || '');\\n+        const matched = recorder.calls.filter(c => !op || c.op === op);\\n+        if (matched.length > 0)\\n+          errors.push(`Expected no github ${nc.op} calls, but found ${matched.length}`);\\n+      }\\n+      if (nc.step && executed[nc.step] > 0) {\\n+        errors.push(`Expected no step ${nc.step} calls, but executed ${executed[nc.step]}`);\\n+      }\\n+    }\\n+\\n+    // Prompt assertions (with optional where-selector)\\n+    for (const p of expect.prompts || []) {\\n+      const arr = promptsByStep[p.step] || [];\\n+      let prompt: string | undefined;\\n+      if (p.where) {\\n+        // Find first prompt matching where conditions\\n+        const where = p.where;\\n+        for (const candidate of arr) {\\n+          let ok = true;\\n+          if (where.contains) ok = ok && where.contains.every(s => candidate.includes(s));\\n+          if (where.not_contains) ok = ok && where.not_contains.every(s => !candidate.includes(s));\\n+          if (where.matches) {\\n+            try {\\n+              let pattern = where.\\n\\n... [TRUNCATED: Diff too large (60.2KB), showing first 50KB] ...\",\"status\":\"added\"},{\"filename\":\"src/test-runner/recorders/github-recorder.ts\",\"additions\":5,\"deletions\":0,\"changes\":139,\"patch\":\"diff --git a/src/test-runner/recorders/github-recorder.ts b/src/test-runner/recorders/github-recorder.ts\\nnew file mode 100644\\nindex 00000000..4b938c2e\\n--- /dev/null\\n+++ b/src/test-runner/recorders/github-recorder.ts\\n@@ -0,0 +1,139 @@\\n+type AnyFunc = (...args: any[]) => Promise<any>;\\n+\\n+export interface RecordedCall {\\n+  provider: 'github';\\n+  op: string; // e.g., issues.createComment\\n+  args: Record<string, unknown>;\\n+  ts: number;\\n+}\\n+\\n+/**\\n+ * Very small Recording Octokit that implements only the methods we need for\\n+ * discovery/MVP. It records all invocations in-memory.\\n+ */\\n+export class RecordingOctokit {\\n+  public readonly calls: RecordedCall[] = [];\\n+\\n+  public readonly rest: any;\\n+  private readonly mode?: { errorCode?: number; timeoutMs?: number };\\n+  private comments: Map<number, Array<{ id: number; body: string; updated_at: string }>> =\\n+    new Map();\\n+  private nextCommentId = 1;\\n+\\n+  constructor(opts?: { errorCode?: number; timeoutMs?: number }) {\\n+    this.mode = opts;\\n+    // Build a dynamic proxy for rest.* namespaces and methods so we don't\\n+    // hardcode the surface of Octokit. Unknown ops still get recorded.\\n+    const makeMethod = (opPath: string[]): AnyFunc => {\\n+      const op = opPath.join('.');\\n+      return async (args: Record<string, unknown> = {}) => {\\n+        this.calls.push({ provider: 'github', op, args, ts: Date.now() });\\n+        return this.stubResponse(op, args);\\n+      };\\n+    };\\n+\\n+    // Top-level rest object with common namespaces proxied to functions\\n+    this.rest = {} as any;\\n+    // Common namespaces\\n+    (this.rest as any).issues = new Proxy(\\n+      {},\\n+      {\\n+        get: (_t, p: string | symbol) =>\\n+          typeof p === 'string' ? makeMethod(['issues', p]) : undefined,\\n+      }\\n+    );\\n+    (this.rest as any).pulls = new Proxy(\\n+      {},\\n+      {\\n+        get: (_t, p: string | symbol) =>\\n+          typeof p === 'string' ? makeMethod(['pulls', p]) : undefined,\\n+      }\\n+    );\\n+    (this.rest as any).checks = new Proxy(\\n+      {},\\n+      {\\n+        get: (_t, p: string | symbol) =>\\n+          typeof p === 'string' ? makeMethod(['checks', p]) : undefined,\\n+      }\\n+    );\\n+  }\\n+\\n+  private stubResponse(op: string, args: Record<string, unknown>): any {\\n+    if (this.mode?.errorCode) {\\n+      const err: any = new Error(`Simulated GitHub error ${this.mode.errorCode}`);\\n+      err.status = this.mode.errorCode;\\n+      throw err;\\n+    }\\n+    if (this.mode?.timeoutMs) {\\n+      return new Promise((_resolve, reject) =>\\n+        setTimeout(\\n+          () => reject(new Error(`Simulated GitHub timeout ${this.mode!.timeoutMs}ms`)),\\n+          this.mode!.timeoutMs\\n+        )\\n+      );\\n+    }\\n+    if (op === 'issues.createComment') {\\n+      const issueNum = Number((args as any).issue_number || 0);\\n+      const body = String((args as any).body || '');\\n+      const id = this.nextCommentId++;\\n+      const rec = { id, body, updated_at: new Date().toISOString() };\\n+      if (!this.comments.has(issueNum)) this.comments.set(issueNum, []);\\n+      this.comments.get(issueNum)!.push(rec);\\n+      return {\\n+        data: { id, body, html_url: '', user: { login: 'bot' }, created_at: rec.updated_at },\\n+      };\\n+    }\\n+    if (op === 'issues.updateComment') {\\n+      const id = Number((args as any).comment_id || 0);\\n+      const body = String((args as any).body || '');\\n+      for (const [, arr] of this.comments.entries()) {\\n+        const found = arr.find(c => c.id === id);\\n+        if (found) {\\n+          found.body = body;\\n+          found.updated_at = new Date().toISOString();\\n+          break;\\n+        }\\n+      }\\n+      return {\\n+        data: {\\n+          id,\\n+          body,\\n+          html_url: '',\\n+          user: { login: 'bot' },\\n+          updated_at: new Date().toISOString(),\\n+        },\\n+      };\\n+    }\\n+    if (op === 'issues.listComments') {\\n+      const issueNum = Number((args as any).issue_number || 0);\\n+      const items = (this.comments.get(issueNum) || []).map(c => ({\\n+        id: c.id,\\n+        body: c.body,\\n+        updated_at: c.updated_at,\\n+      }));\\n+      return { data: items };\\n+    }\\n+    if (op === 'issues.getComment') {\\n+      const id = Number((args as any).comment_id || 0);\\n+      for (const [, arr] of this.comments.entries()) {\\n+        const found = arr.find(c => c.id === id);\\n+        if (found)\\n+          return { data: { id: found.id, body: found.body, updated_at: found.updated_at } };\\n+      }\\n+      return { data: { id, body: '', updated_at: new Date().toISOString() } };\\n+    }\\n+    if (op === 'issues.addLabels') {\\n+      return { data: { labels: (args as any).labels || [] } };\\n+    }\\n+    if (op.startsWith('checks.')) {\\n+      return { data: { id: 123, status: 'completed', conclusion: 'success', url: '' } };\\n+    }\\n+    if (op === 'pulls.get') {\\n+      return { data: { number: (args as any).pull_number || 1, state: 'open', title: 'Test PR' } };\\n+    }\\n+    if (op === 'pulls.listFiles') {\\n+      return { data: [] };\\n+    }\\n+    return { data: {} };\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/recorders/global-recorder.ts\",\"additions\":1,\"deletions\":0,\"changes\":11,\"patch\":\"diff --git a/src/test-runner/recorders/global-recorder.ts b/src/test-runner/recorders/global-recorder.ts\\nnew file mode 100644\\nindex 00000000..cda96e45\\n--- /dev/null\\n+++ b/src/test-runner/recorders/global-recorder.ts\\n@@ -0,0 +1,11 @@\\n+import type { RecordingOctokit } from './github-recorder';\\n+\\n+let __rec: RecordingOctokit | null = null;\\n+\\n+export function setGlobalRecorder(r: RecordingOctokit | null): void {\\n+  __rec = r;\\n+}\\n+\\n+export function getGlobalRecorder(): RecordingOctokit | null {\\n+  return __rec;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/utils/selectors.ts\",\"additions\":3,\"deletions\":0,\"changes\":59,\"patch\":\"diff --git a/src/test-runner/utils/selectors.ts b/src/test-runner/utils/selectors.ts\\nnew file mode 100644\\nindex 00000000..5e1313bf\\n--- /dev/null\\n+++ b/src/test-runner/utils/selectors.ts\\n@@ -0,0 +1,59 @@\\n+export function deepGet(obj: unknown, path: string): unknown {\\n+  if (obj == null) return undefined;\\n+  const parts: Array<string | number> = [];\\n+  let i = 0;\\n+\\n+  const readIdent = () => {\\n+    const start = i;\\n+    while (i < path.length && path[i] !== '.' && path[i] !== '[') i++;\\n+    if (i > start) parts.push(path.slice(start, i));\\n+  };\\n+  const readBracket = () => {\\n+    // assumes path[i] === '['\\n+    i++; // skip [\\n+    if (i < path.length && (path[i] === '\\\"' || path[i] === \\\"'\\\")) {\\n+      const quote = path[i++];\\n+      const start = i;\\n+      while (i < path.length && path[i] !== quote) i++;\\n+      const key = path.slice(start, i);\\n+      parts.push(key);\\n+      // skip closing quote\\n+      if (i < path.length && path[i] === quote) i++;\\n+      // skip ]\\n+      if (i < path.length && path[i] === ']') i++;\\n+    } else {\\n+      // numeric index\\n+      const start = i;\\n+      while (i < path.length && /[0-9]/.test(path[i])) i++;\\n+      const numStr = path.slice(start, i);\\n+      parts.push(Number(numStr));\\n+      if (i < path.length && path[i] === ']') i++;\\n+    }\\n+  };\\n+\\n+  // initial token (identifier or bracket)\\n+  if (path[i] === '[') {\\n+    readBracket();\\n+  } else {\\n+    if (path[i] === '.') i++;\\n+    readIdent();\\n+  }\\n+  while (i < path.length) {\\n+    if (path[i] === '.') {\\n+      i++;\\n+      readIdent();\\n+    } else if (path[i] === '[') {\\n+      readBracket();\\n+    } else {\\n+      // unexpected char, stop parsing\\n+      break;\\n+    }\\n+  }\\n+\\n+  let cur: any = obj;\\n+  for (const key of parts) {\\n+    if (cur == null) return undefined;\\n+    cur = cur[key as any];\\n+  }\\n+  return cur;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/validator.ts\",\"additions\":13,\"deletions\":0,\"changes\":376,\"patch\":\"diff --git a/src/test-runner/validator.ts b/src/test-runner/validator.ts\\nnew file mode 100644\\nindex 00000000..13931065\\n--- /dev/null\\n+++ b/src/test-runner/validator.ts\\n@@ -0,0 +1,376 @@\\n+import Ajv, { ErrorObject } from 'ajv';\\n+import addFormats from 'ajv-formats';\\n+\\n+// Lightweight JSON Schema for the tests DSL. The goal is helpful errors,\\n+// not full semantic validation.\\n+const schema: any = {\\n+  $id: 'https://visor/probe/tests-dsl.schema.json',\\n+  type: 'object',\\n+  additionalProperties: false,\\n+  properties: {\\n+    version: { type: 'string' },\\n+    extends: {\\n+      oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+    },\\n+    tests: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      required: ['cases'],\\n+      properties: {\\n+        defaults: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            strict: { type: 'boolean' },\\n+            ai_provider: { type: 'string' },\\n+            fail_on_unexpected_calls: { type: 'boolean' },\\n+            github_recorder: {\\n+              type: 'object',\\n+              additionalProperties: false,\\n+              properties: {\\n+                error_code: { type: 'number' },\\n+                timeout_ms: { type: 'number' },\\n+              },\\n+            },\\n+            macros: {\\n+              type: 'object',\\n+              additionalProperties: { $ref: '#/$defs/expectBlock' },\\n+            },\\n+          },\\n+        },\\n+        fixtures: { type: 'array' },\\n+        cases: {\\n+          type: 'array',\\n+          minItems: 1,\\n+          items: { $ref: '#/$defs/testCase' },\\n+        },\\n+      },\\n+    },\\n+  },\\n+  required: ['tests'],\\n+  $defs: {\\n+    fixtureRef: {\\n+      oneOf: [\\n+        { type: 'string' },\\n+        {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            builtin: { type: 'string' },\\n+            overrides: { type: 'object' },\\n+          },\\n+          required: ['builtin'],\\n+        },\\n+      ],\\n+    },\\n+    testCase: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        name: { type: 'string' },\\n+        description: { type: 'string' },\\n+        skip: { type: 'boolean' },\\n+        strict: { type: 'boolean' },\\n+        github_recorder: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            error_code: { type: 'number' },\\n+            timeout_ms: { type: 'number' },\\n+          },\\n+        },\\n+        event: {\\n+          type: 'string',\\n+          enum: ['manual', 'pr_opened', 'pr_updated', 'pr_closed', 'issue_opened', 'issue_comment'],\\n+        },\\n+        fixture: { $ref: '#/$defs/fixtureRef' },\\n+        env: {\\n+          type: 'object',\\n+          additionalProperties: { type: 'string' },\\n+        },\\n+        mocks: {\\n+          type: 'object',\\n+          additionalProperties: {\\n+            oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n+          },\\n+        },\\n+        expect: { $ref: '#/$defs/expectBlock' },\\n+        // Flow cases\\n+        flow: {\\n+          type: 'array',\\n+          items: { $ref: '#/$defs/flowStage' },\\n+        },\\n+      },\\n+      required: ['name'],\\n+      anyOf: [{ required: ['event'] }, { required: ['flow'] }],\\n+    },\\n+    flowStage: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        name: { type: 'string' },\\n+        description: { type: 'string' },\\n+        github_recorder: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            error_code: { type: 'number' },\\n+            timeout_ms: { type: 'number' },\\n+          },\\n+        },\\n+        event: {\\n+          type: 'string',\\n+          enum: ['manual', 'pr_opened', 'pr_updated', 'pr_closed', 'issue_opened', 'issue_comment'],\\n+        },\\n+        fixture: { $ref: '#/$defs/fixtureRef' },\\n+        env: {\\n+          type: 'object',\\n+          additionalProperties: { type: 'string' },\\n+        },\\n+        mocks: {\\n+          type: 'object',\\n+          additionalProperties: {\\n+            oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n+          },\\n+        },\\n+        expect: { $ref: '#/$defs/expectBlock' },\\n+      },\\n+      required: ['event'],\\n+    },\\n+    countExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        exactly: { type: 'number' },\\n+        at_least: { type: 'number' },\\n+        at_most: { type: 'number' },\\n+      },\\n+      // Mutual exclusion is enforced at runtime; schema ensures they are numeric if present.\\n+    },\\n+    callsExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        step: { type: 'string' },\\n+        provider: { type: 'string' },\\n+        op: { type: 'string' },\\n+        args: { type: 'object' },\\n+        exactly: { type: 'number' },\\n+        at_least: { type: 'number' },\\n+        at_most: { type: 'number' },\\n+      },\\n+    },\\n+    promptsExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        step: { type: 'string' },\\n+        index: {\\n+          oneOf: [{ type: 'number' }, { enum: ['first', 'last'] }],\\n+        },\\n+        contains: {\\n+          type: 'array',\\n+          items: { type: 'string' },\\n+        },\\n+        not_contains: {\\n+          type: 'array',\\n+          items: { type: 'string' },\\n+        },\\n+        matches: { type: 'string' },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            contains: { type: 'array', items: { type: 'string' } },\\n+            not_contains: { type: 'array', items: { type: 'string' } },\\n+            matches: { type: 'string' },\\n+          },\\n+        },\\n+      },\\n+      required: ['step'],\\n+    },\\n+    outputsExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        step: { type: 'string' },\\n+        index: {\\n+          oneOf: [{ type: 'number' }, { enum: ['first', 'last'] }],\\n+        },\\n+        path: { type: 'string' },\\n+        equals: {},\\n+        equalsDeep: {},\\n+        matches: { type: 'string' },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            path: { type: 'string' },\\n+            equals: {},\\n+            matches: { type: 'string' },\\n+          },\\n+          required: ['path'],\\n+        },\\n+        contains_unordered: { type: 'array' },\\n+      },\\n+      required: ['step', 'path'],\\n+    },\\n+    expectBlock: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        use: { type: 'array', items: { type: 'string' } },\\n+        calls: { type: 'array', items: { $ref: '#/$defs/callsExpectation' } },\\n+        prompts: { type: 'array', items: { $ref: '#/$defs/promptsExpectation' } },\\n+        outputs: { type: 'array', items: { $ref: '#/$defs/outputsExpectation' } },\\n+        no_calls: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'object',\\n+            additionalProperties: false,\\n+            properties: {\\n+              step: { type: 'string' },\\n+              provider: { type: 'string' },\\n+              op: { type: 'string' },\\n+            },\\n+          },\\n+        },\\n+        fail: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: { message_contains: { type: 'string' } },\\n+        },\\n+        strict_violation: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: { for_step: { type: 'string' }, message_contains: { type: 'string' } },\\n+        },\\n+      },\\n+    },\\n+  },\\n+};\\n+\\n+const ajv = new Ajv({ allErrors: true, allowUnionTypes: true, strict: false });\\n+addFormats(ajv);\\n+const validate = ajv.compile(schema);\\n+\\n+function toYamlPath(instancePath: string): string {\\n+  if (!instancePath) return 'tests';\\n+  // Ajv instancePath starts with '/'\\n+  const parts = instancePath\\n+    .split('/')\\n+    .slice(1)\\n+    .map(p => (p.match(/^\\\\d+$/) ? `[${p}]` : `.${p}`));\\n+  let out = parts.join('');\\n+  if (out.startsWith('.')) out = out.slice(1);\\n+  // Heuristic: put root under tests for nicer messages\\n+  if (!out.startsWith('tests')) out = `tests.${out}`;\\n+  return out;\\n+}\\n+\\n+function levenshtein(a: string, b: string): number {\\n+  const m = a.length,\\n+    n = b.length;\\n+  const dp = Array.from({ length: m + 1 }, () => new Array(n + 1).fill(0));\\n+  for (let i = 0; i <= m; i++) dp[i][0] = i;\\n+  for (let j = 0; j <= n; j++) dp[0][j] = j;\\n+  for (let i = 1; i <= m; i++) {\\n+    for (let j = 1; j <= n; j++) {\\n+      const cost = a[i - 1] === b[j - 1] ? 0 : 1;\\n+      dp[i][j] = Math.min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost);\\n+    }\\n+  }\\n+  return dp[m][n];\\n+}\\n+\\n+const knownKeys = new Set([\\n+  // top-level\\n+  'version',\\n+  'extends',\\n+  'tests',\\n+  // tests\\n+  'tests.defaults',\\n+  'tests.fixtures',\\n+  'tests.cases',\\n+  // defaults\\n+  'tests.defaults.strict',\\n+  'tests.defaults.ai_provider',\\n+  'tests.defaults.github_recorder',\\n+  'tests.defaults.macros',\\n+  'tests.defaults.fail_on_unexpected_calls',\\n+  // case\\n+  'name',\\n+  'description',\\n+  'skip',\\n+  'strict',\\n+  'event',\\n+  'fixture',\\n+  'env',\\n+  'mocks',\\n+  'expect',\\n+  'flow',\\n+  // expect\\n+  'expect.use',\\n+  'expect.calls',\\n+  'expect.prompts',\\n+  'expect.outputs',\\n+  'expect.no_calls',\\n+  'expect.fail',\\n+  'expect.strict_violation',\\n+  // calls\\n+  'step',\\n+  'provider',\\n+  'op',\\n+  'exactly',\\n+  'at_least',\\n+  'at_most',\\n+  'args',\\n+  // prompts/outputs\\n+  'index',\\n+  'contains',\\n+  'not_contains',\\n+  'matches',\\n+  'path',\\n+  'equals',\\n+  'equalsDeep',\\n+  'where',\\n+  'contains_unordered',\\n+]);\\n+\\n+function hintForAdditionalProperty(err: ErrorObject): string | undefined {\\n+  if (err.keyword !== 'additionalProperties') return undefined;\\n+  const prop = (err.params as any)?.additionalProperty;\\n+  if (!prop || typeof prop !== 'string') return undefined;\\n+  // find nearest known key suffix match\\n+  let best: { key: string; dist: number } | null = null;\\n+  for (const k of knownKeys) {\\n+    const dist = levenshtein(prop, k.includes('.') ? k.split('.').pop()! : k);\\n+    if (dist <= 3 && (!best || dist < best.dist)) best = { key: k, dist };\\n+  }\\n+  if (best) return `Did you mean \\\"${best.key}\\\"?`;\\n+  return undefined;\\n+}\\n+\\n+function formatError(e: ErrorObject): string {\\n+  const path = toYamlPath(e.instancePath || '');\\n+  let msg = `${path}: ${e.message}`;\\n+  const hint = hintForAdditionalProperty(e);\\n+  if (hint) msg += ` (${hint})`;\\n+  if (e.keyword === 'enum' && Array.isArray((e.params as any)?.allowedValues)) {\\n+    msg += ` (allowed: ${(e.params as any).allowedValues.join(', ')})`;\\n+  }\\n+  return msg;\\n+}\\n+\\n+export type ValidationResult = { ok: true } | { ok: false; errors: string[] };\\n+\\n+export function validateTestsDoc(doc: unknown): ValidationResult {\\n+  try {\\n+    const ok = validate(doc);\\n+    if (ok) return { ok: true };\\n+    const errs = (validate.errors || []).map(formatError);\\n+    return { ok: false, errors: errs };\\n+  } catch (err) {\\n+    return { ok: false, errors: [err instanceof Error ? err.message : String(err)] };\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/utils/file-exclusion.ts\",\"additions\":1,\"deletions\":1,\"changes\":15,\"patch\":\"diff --git a/src/utils/file-exclusion.ts b/src/utils/file-exclusion.ts\\nindex 155bf20b..e7f5274e 100644\\n--- a/src/utils/file-exclusion.ts\\n+++ b/src/utils/file-exclusion.ts\\n@@ -128,12 +128,21 @@ export class FileExclusionHelper {\\n           .trim();\\n \\n         this.gitignore.add(gitignoreContent);\\n-        console.error('✅ Loaded .gitignore patterns for file filtering');\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.error('✅ Loaded .gitignore patterns for file filtering');\\n+        }\\n       } else if (additionalPatterns && additionalPatterns.length > 0) {\\n-        console.error('⚠️  No .gitignore found, using default exclusion patterns');\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.error('⚠️  No .gitignore found, using default exclusion patterns');\\n+        }\\n       }\\n     } catch (error) {\\n-      console.warn('⚠️ Failed to load .gitignore:', error instanceof Error ? error.message : error);\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        console.warn(\\n+          '⚠️ Failed to load .gitignore:',\\n+          error instanceof Error ? error.message : error\\n+        );\\n+      }\\n     }\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/integration/issue-double-content-detection.test.ts\",\"additions\":0,\"deletions\":5,\"changes\":131,\"patch\":\"diff --git a/tests/integration/issue-double-content-detection.test.ts b/tests/integration/issue-double-content-detection.test.ts\\ndeleted file mode 100644\\nindex 5ef9fb1b..00000000\\n--- a/tests/integration/issue-double-content-detection.test.ts\\n+++ /dev/null\\n@@ -1,131 +0,0 @@\\n-import * as fs from 'fs';\\n-import * as path from 'path';\\n-\\n-// Minimal Octokit REST mock to capture posted comment body\\n-const issuesCreateComment = jest.fn();\\n-const issuesListComments = jest.fn().mockResolvedValue({ data: [] });\\n-const pullsGet = jest.fn().mockResolvedValue({ data: { head: { sha: 'deadbeefcafebabe' } } });\\n-const pullsListFiles = jest.fn().mockResolvedValue({\\n-  data: [{ filename: 'x', status: 'modified', additions: 1, deletions: 0, changes: 1 }],\\n-});\\n-\\n-jest.mock('@octokit/rest', () => {\\n-  return {\\n-    Octokit: jest.fn().mockImplementation(() => ({\\n-      rest: {\\n-        issues: { createComment: issuesCreateComment, listComments: issuesListComments },\\n-        pulls: { get: pullsGet, listFiles: pullsListFiles },\\n-        checks: { create: jest.fn(), update: jest.fn() },\\n-      },\\n-    })),\\n-  };\\n-});\\n-\\n-describe('Issue assistant double-content detection (issues opened)', () => {\\n-  beforeEach(() => {\\n-    issuesCreateComment.mockReset();\\n-    issuesListComments.mockClear();\\n-    pullsGet.mockClear();\\n-    pullsListFiles.mockClear();\\n-\\n-    // Clean env used by action run()\\n-    delete process.env.GITHUB_EVENT_NAME;\\n-    delete process.env.GITHUB_EVENT_PATH;\\n-    delete process.env.GITHUB_REPOSITORY;\\n-    delete process.env.GITHUB_REPOSITORY_OWNER;\\n-    delete process.env['INPUT_GITHUB-TOKEN'];\\n-    delete process.env['INPUT_OWNER'];\\n-    delete process.env['INPUT_REPO'];\\n-    delete process.env['INPUT_CONFIG-PATH'];\\n-    delete process.env['INPUT_CREATE-CHECK'];\\n-    delete process.env['INPUT_COMMENT-ON-PR'];\\n-    delete process.env['INPUT_DEBUG'];\\n-  });\\n-\\n-  const writeTmp = (name: string, data: any) => {\\n-    const p = path.join(process.cwd(), name);\\n-    fs.writeFileSync(p, typeof data === 'string' ? data : JSON.stringify(data));\\n-    return p;\\n-  };\\n-\\n-  // This config intentionally produces two assistant-style outputs in the same run.\\n-  // With current behavior, both get concatenated into the single issue comment.\\n-  // We assert that only one assistant response appears (i.e., deduped/collapsed),\\n-  // so this test should fail until the posting logic is fixed.\\n-  const makeConfig = () => `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  assistant-initial:\\n-    type: ai\\n-    ai_provider: mock\\n-    schema: issue-assistant\\n-    group: dynamic\\n-    on: [issue_opened]\\n-    prompt: |\\n-      Return ONE JSON object for issue-assistant.\\n-      text: ### Assistant Reply\\n-      intent: issue_triage\\n-\\n-  assistant-refined:\\n-    type: ai\\n-    ai_provider: mock\\n-    schema: issue-assistant\\n-    group: dynamic\\n-    depends_on: [assistant-initial]\\n-    on: [issue_opened]\\n-    prompt: |\\n-      Return ONE JSON object for issue-assistant.\\n-      text: ### Assistant Reply\\n-      intent: issue_triage\\n-\\n-output:\\n-  pr_comment:\\n-    format: markdown\\n-    group_by: check\\n-`;\\n-\\n-  it('posts only the refined answer (no duplicate old+new content)', async () => {\\n-    const cfgPath = writeTmp('.tmp-double-content.yaml', makeConfig());\\n-    const event = {\\n-      action: 'opened',\\n-      issue: { number: 77, state: 'open' },\\n-      repository: { full_name: 'acme/widgets' },\\n-      sender: { login: 'reporter' },\\n-    } as any;\\n-    const eventPath = writeTmp('.tmp-issues-opened-double.json', event);\\n-\\n-    process.env.GITHUB_EVENT_NAME = 'issues';\\n-    process.env.GITHUB_EVENT_PATH = eventPath;\\n-    process.env.GITHUB_REPOSITORY = 'acme/widgets';\\n-    process.env.GITHUB_REPOSITORY_OWNER = 'acme';\\n-    process.env['INPUT_GITHUB-TOKEN'] = 'test-token';\\n-    process.env['INPUT_OWNER'] = 'acme';\\n-    process.env['INPUT_REPO'] = 'widgets';\\n-    process.env['INPUT_CONFIG-PATH'] = cfgPath;\\n-    process.env['INPUT_CREATE-CHECK'] = 'false';\\n-    process.env['INPUT_COMMENT-ON-PR'] = 'false';\\n-    process.env['INPUT_DEBUG'] = 'true';\\n-\\n-    jest.resetModules();\\n-    const { run } = await import('../../src/index');\\n-    await run();\\n-\\n-    // Exactly one comment is posted\\n-    expect(issuesCreateComment).toHaveBeenCalledTimes(1);\\n-    const call = issuesCreateComment.mock.calls[0][0];\\n-    const body: string = call.body;\\n-    // Debug: persist body for local inspection\\n-    fs.mkdirSync('tmp', { recursive: true });\\n-    fs.writeFileSync('tmp/issue-double-content-body.md', body, 'utf8');\\n-\\n-    // Desired behavior: Only a single assistant response should appear.\\n-    // Current bug: both initial and refined outputs are concatenated. In the\\n-    // mock path the provider sometimes returns a minimal JSON like {\\\"issues\\\":[]}.\\n-    // Assert that only one such block exists.\\n-    const jsonBlockCount = (body.match(/\\\\{\\\\s*\\\\\\\"issues\\\\\\\"\\\\s*:\\\\s*\\\\[\\\\]\\\\s*\\\\}/g) || []).length;\\n-    expect(jsonBlockCount).toBe(1);\\n-\\n-    fs.unlinkSync(cfgPath);\\n-    fs.unlinkSync(eventPath);\\n-  });\\n-});\\n\",\"status\":\"removed\"},{\"filename\":\"tests/integration/issue-posting-fact-gate.test.ts\",\"additions\":0,\"deletions\":7,\"changes\":197,\"patch\":\"diff --git a/tests/integration/issue-posting-fact-gate.test.ts b/tests/integration/issue-posting-fact-gate.test.ts\\ndeleted file mode 100644\\nindex 4ac86358..00000000\\n--- a/tests/integration/issue-posting-fact-gate.test.ts\\n+++ /dev/null\\n@@ -1,197 +0,0 @@\\n-import * as fs from 'fs';\\n-import * as path from 'path';\\n-\\n-// Reuse the Octokit REST mock pattern from other integration tests\\n-const checksCreate = jest.fn();\\n-const checksUpdate = jest.fn();\\n-const issuesCreateComment = jest.fn();\\n-const issuesListComments = jest.fn().mockResolvedValue({ data: [] });\\n-const pullsGet = jest.fn().mockResolvedValue({ data: { head: { sha: 'deadbeefcafebabe' } } });\\n-const pullsListFiles = jest.fn().mockResolvedValue({\\n-  data: [{ filename: 'x', status: 'modified', additions: 1, deletions: 0, changes: 1 }],\\n-});\\n-\\n-jest.mock('@octokit/rest', () => {\\n-  return {\\n-    Octokit: jest.fn().mockImplementation(() => ({\\n-      rest: {\\n-        checks: { create: checksCreate, update: checksUpdate },\\n-        issues: { createComment: issuesCreateComment, listComments: issuesListComments },\\n-        pulls: { get: pullsGet, listFiles: pullsListFiles },\\n-      },\\n-    })),\\n-  };\\n-});\\n-\\n-describe('Issue assistant posting is gated by fact validation (issue_opened)', () => {\\n-  beforeEach(() => {\\n-    checksCreate.mockReset();\\n-    checksUpdate.mockReset();\\n-    issuesCreateComment.mockReset();\\n-    issuesListComments.mockClear();\\n-    pullsGet.mockClear();\\n-    pullsListFiles.mockClear();\\n-    // Clean env that run() reads\\n-    delete process.env.GITHUB_EVENT_NAME;\\n-    delete process.env.GITHUB_EVENT_PATH;\\n-    delete process.env.GITHUB_REPOSITORY;\\n-    delete process.env.GITHUB_REPOSITORY_OWNER;\\n-    delete process.env['INPUT_GITHUB-TOKEN'];\\n-    delete process.env['INPUT_OWNER'];\\n-    delete process.env['INPUT_REPO'];\\n-    delete process.env['INPUT_CONFIG-PATH'];\\n-    delete process.env['INPUT_CREATE-CHECK'];\\n-    delete process.env['INPUT_COMMENT-ON-PR'];\\n-    delete process.env['INPUT_DEBUG'];\\n-  });\\n-\\n-  const makeConfig = (allValid: boolean) => `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  # Minimal issue assistant using mock provider\\n-  issue-assistant:\\n-    type: ai\\n-    ai_provider: mock\\n-    schema: issue-assistant\\n-    on: [issue_opened]\\n-    prompt: |\\n-      Return ONE JSON object for issue-assistant.\\n-      text: Hello, world.\\n-\\n-References:\\n-\\n-\\\\`\\\\`\\\\`refs\\n-none\\n-\\\\`\\\\`\\\\`\\n-      intent: issue_triage\\n-    on_success:\\n-      run: [init-fact-validation]\\n-\\n-  # Initialize validation state\\n-  init-fact-validation:\\n-    type: memory\\n-    operation: set\\n-    namespace: fact-validation\\n-    key: attempt\\n-    value: 0\\n-    on: [issue_opened]\\n-\\n-  # Seed deterministic facts instead of invoking AI\\n-  seed-facts:\\n-    type: memory\\n-    operation: set\\n-    namespace: fact-validation\\n-    key: fact_list\\n-    value: [{\\\"id\\\":\\\"f1\\\",\\\"category\\\":\\\"Configuration\\\",\\\"claim\\\":\\\"X\\\",\\\"verifiable\\\":true}]\\n-    depends_on: [issue-assistant]\\n-    on: [issue_opened]\\n-\\n-  # forEach extraction proxy\\n-  extract-facts:\\n-    type: memory\\n-    operation: get\\n-    namespace: fact-validation\\n-    key: fact_list\\n-    forEach: true\\n-    depends_on: [seed-facts]\\n-    on: [issue_opened]\\n-    on_finish:\\n-      run: [aggregate-validations]\\n-      goto_js: |\\n-        const ns = 'fact-validation';\\n-        const allValid = memory.get('all_valid', ns) === true;\\n-        const limit = 1; // one retry\\n-        const attempt = Number(memory.get('attempt', ns) || 0);\\n-        if (!allValid && attempt < limit) {\\n-          memory.increment('attempt', 1, ns);\\n-          return 'issue-assistant';\\n-        }\\n-        return null;\\n-\\n-  validate-fact:\\n-    type: memory\\n-    operation: exec_js\\n-    namespace: fact-validation\\n-    depends_on: [extract-facts]\\n-    on: [issue_opened]\\n-    memory_js: |\\n-      const NS = 'fact-validation';\\n-      const f = outputs['extract-facts'];\\n-      const attempt = Number(memory.get('attempt', NS) || 0);\\n-      const is_valid = ${allValid ? 'true' : 'false'}; return { fact_id: f.id, claim: f.claim, is_valid, confidence: '${allValid ? \\\"'ok'\\\" : \\\"'bad'\\\"} };\\n-\\n-  aggregate-validations:\\n-    type: memory\\n-    operation: exec_js\\n-    namespace: fact-validation\\n-    on: [issue_opened]\\n-    memory_js: |\\n-      const vals = outputs.history['validate-fact'] || [];\\n-      const invalid = (Array.isArray(vals) ? vals : []).filter(v => v && v.is_valid === false);\\n-      const all_valid = invalid.length === 0;\\n-      memory.set('all_valid', all_valid, 'fact-validation');\\n-      return { total: vals.length, all_valid };\\n-\\n-  # Emit a simple final note when valid so the Action has content to post once\\n-  final-note:\\n-    type: log\\n-    depends_on: [aggregate-validations]\\n-    if: \\\"memory.get('all_valid','fact-validation') === true\\\"\\n-    message: 'Verified: final'\\n-\\n-  # No explicit post step; use Action's generic end-of-run post\\n-\\n-output:\\n-  pr_comment:\\n-    format: markdown\\n-    group_by: check\\n-`;\\n-\\n-  const writeTmp = (name: string, data: any) => {\\n-    const p = path.join(process.cwd(), name);\\n-    fs.writeFileSync(p, typeof data === 'string' ? data : JSON.stringify(data));\\n-    return p;\\n-  };\\n-\\n-  const setupAndRun = async (allValid: boolean) => {\\n-    const cfgPath = writeTmp(\\n-      `.tmp-issue-gate-${allValid ? 'ok' : 'fail'}.yaml`,\\n-      makeConfig(allValid)\\n-    );\\n-    const event = {\\n-      action: 'opened',\\n-      issue: { number: 42, state: 'open' },\\n-      repository: { full_name: 'acme/widgets' },\\n-      sender: { login: 'reporter' },\\n-    } as any;\\n-    const eventPath = writeTmp('.tmp-issues-opened.json', event);\\n-\\n-    process.env.GITHUB_EVENT_NAME = 'issues';\\n-    process.env.GITHUB_EVENT_PATH = eventPath;\\n-    process.env.GITHUB_REPOSITORY = 'acme/widgets';\\n-    process.env.GITHUB_REPOSITORY_OWNER = 'acme';\\n-    process.env['INPUT_GITHUB-TOKEN'] = 'test-token';\\n-    process.env['INPUT_OWNER'] = 'acme';\\n-    process.env['INPUT_REPO'] = 'widgets';\\n-    process.env['INPUT_CONFIG-PATH'] = cfgPath;\\n-    process.env['INPUT_CREATE-CHECK'] = 'false';\\n-    process.env['INPUT_COMMENT-ON-PR'] = 'false';\\n-    process.env['INPUT_DEBUG'] = 'true';\\n-    process.env['ENABLE_FACT_VALIDATION'] = 'true';\\n-\\n-    // Import run() fresh to pick up env\\n-    jest.resetModules();\\n-    const { run } = await import('../../src/index');\\n-    await run();\\n-\\n-    fs.unlinkSync(cfgPath);\\n-    fs.unlinkSync(eventPath);\\n-  };\\n-\\n-  it('loops once to correct facts and posts a single final comment', async () => {\\n-    await setupAndRun(false);\\n-    // With attempt limit=1, the first validation fails, we route back to assistant,\\n-    // second pass should be valid and then post once at end.\\n-    expect(issuesCreateComment).toHaveBeenCalledTimes(1);\\n-  });\\n-});\\n\",\"status\":\"removed\"}],\"outputs\":{\"overview\":{\"text\":\"This PR introduces a comprehensive, configuration-driven integration test framework for Visor. It allows developers to write tests for their `.visor.yaml` configurations by simulating GitHub events, mocking providers (like AI and GitHub API calls), and asserting on the resulting actions. This is a significant feature that replaces previous ad-hoc testing methods with a structured and maintainable approach.\\n\\n### Files Changed Analysis\\n\\nThe changes introduce a new test runner, along with its supporting components, documentation, and default test suite.\\n\\n-   **New Feature Implementation (`src/test-runner/`)**: The core logic is encapsulated in the new `src/test-runner` directory, which includes the main `index.ts` (the runner itself), `fixture-loader.ts` for managing test data, `recorders/` for mocking GitHub and AI interactions, and `validator.ts` for handling assertions.\\n-   **CLI Integration (`src/cli-main.ts`)**: A new `test` subcommand is added to the Visor CLI to execute the test runner.\\n-   **Execution Engine Modifications (`src/check-execution-engine.ts`)**: The engine is updated to support test mode, primarily by allowing the injection of mock providers and recorders.\\n-   **New Test Suite (`defaults/.visor.tests.yaml`)**: A comprehensive test suite for the default `.visor.yaml` configuration is added, serving as a practical example of the new framework.\\n-   **Documentation (`docs/testing/`)**: Extensive documentation is added, covering getting started, CLI usage, assertions, and fixtures/mocks.\\n-   **CI Integration (`.github/workflows/ci.yml`)**: The CI pipeline is updated to run the new integration tests, ensuring configurations are validated on each pull request.\\n-   **Test Removal (`tests/integration/`)**: Old, script-based integration tests are removed in favor of the new, more robust framework.\\n\\n### Architecture & Impact Assessment\\n\\n#### What this PR accomplishes\\n\\nThis PR delivers a complete integration test framework for Visor configurations. It enables developers to validate their automation rules in a predictable, isolated environment without making live network calls. This improves reliability, simplifies debugging, and provides a safety net for configuration changes.\\n\\n#### Key technical changes introduced\\n\\n1.  **Test Runner CLI**: A `visor test` command is introduced to discover and run tests defined in a `.visor.tests.yaml` file.\\n2.  **Fixture-Based Testing**: Tests are driven by predefined \\\"fixtures\\\" that simulate GitHub webhook events (e.g., `gh.pr_open.minimal`).\\n3.  **Mocking and Recording**: The framework intercepts calls to external providers. GitHub API calls are recorded for assertion, and AI provider calls are mocked to return predefined responses. This is handled by a `RecordingOctokit` wrapper and mock AI providers that are activated when `ai.provider` is set to `mock`.\\n4.  **Declarative Assertions**: Tests use a YAML `expect:` block to assert on outcomes, such as the number of calls to a provider (`calls`), the content of AI prompts (`prompts`), or the final status of checks.\\n\\n#### Affected system components\\n\\n-   **CLI (`src/cli-main.ts`)**: Extended with a new `test` command.\\n-   **Core Logic (`src/check-execution-engine.ts`)**: Modified to operate in a \\\"test mode\\\" with mocked dependencies.\\n-   **Providers (`src/providers/*`)**: The `GithubOpsProvider` is adapted to use a recordable Octokit instance during tests. The `AiCheckProvider` is modified to handle a `mock` provider type.\\n-   **CI/CD (`.github/workflows/ci.yml`)**: The CI workflow is updated to execute the new test suite.\\n\\n#### Component Interaction Diagram\\n\\n```mermaid\\ngraph TD\\n    subgraph Test Execution\\n        A[visor test CLI] --> B{Test Runner};\\n        B --> C[Load .visor.tests.yaml];\\n        C --> D{For each test case};\\n        D --> E[Load Fixture & Mocks];\\n    end\\n\\n    subgraph Visor Core\\n        F(CheckExecutionEngine);\\n        G[Providers (GitHub, AI, etc.)];\\n    end\\n\\n    subgraph Mocks & Recorders\\n        H[RecordingOctokit];\\n        I[MockAiProvider];\\n    end\\n\\n    E --> |injects mocks| F;\\n    F --> |uses| G;\\n    G -- during test --> H;\\n    G -- during test --> I;\\n\\n    D --> |runs| F;\\n    F --> J[Collect Results & Recorded Calls];\\n    J --> K[Validate Assertions];\\n    K --> L[Report Pass/Fail];\\n```\\n\\n### Scope Discovery & Context Expansion\\n\\nThis feature fundamentally changes how Visor configurations are developed and maintained. By providing a robust testing framework, it encourages a test-driven development (TDD) approach for writing automation rules.\\n\\n-   **Impact on Configuration Development**: Users will now be expected to write tests for their custom checks and workflows. The `defaults/.visor.tests.yaml` file serves as a blueprint for this.\\n-   **Reliability and Maintenance**: The ability to test configurations offline significantly reduces the risk of introducing regressions. It makes troubleshooting easier, as failures can be reproduced locally and deterministically.\\n-   **Provider Ecosystem**: The mocking architecture is extensible. While this PR focuses on GitHub and AI providers, the same pattern could be applied to any future provider (e.g., Slack, Jira), ensuring that all integrations can be tested.\\n-   **Developer Experience**: The framework is designed with developer experience in mind, offering clear output, helpful error messages, and a straightforward YAML-based syntax, lowering the barrier to writing effective tests.\",\"tags\":{\"review-effort\":5,\"label\":\"feature\"}}}}"},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"architecture","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"architecture","visor.check.input.context":"{\"pr\":{\"number\":0,\"title\":\"Local Analysis: feat/test-framework-runner (14 modified)\",\"author\":\"Leonid Bugaev\",\"branch\":\"feat/test-framework-runner\",\"base\":\"main\"},\"files\":[{\"filename\":\".github/workflows/ci.yml\",\"additions\":1,\"deletions\":0,\"changes\":15,\"patch\":\"diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml\\nindex 840f2abc..13bd6f9d 100644\\n--- a/.github/workflows/ci.yml\\n+++ b/.github/workflows/ci.yml\\n@@ -64,6 +64,21 @@ jobs:\\n           ls -la *.tgz\\n           echo \\\"✅ Package can be created successfully\\\"\\n \\n+      - name: Run integration tests (defaults suite)\\n+        run: |\\n+          mkdir -p output\\n+          node ./dist/index.js test --config defaults/.visor.tests.yaml --json output/visor-tests.json --report junit:output/visor-tests.xml --summary md:output/visor-tests.md\\n+\\n+      - name: Upload integration test artifacts\\n+        if: always()\\n+        uses: actions/upload-artifact@v4\\n+        with:\\n+          name: visor-test-results\\n+          path: |\\n+            output/visor-tests.json\\n+            output/visor-tests.xml\\n+            output/visor-tests.md\\n+\\n       - name: Test basic action functionality\\n         uses: ./\\n         with:\\n\",\"status\":\"added\"},{\"filename\":\"README.md\",\"additions\":1,\"deletions\":0,\"changes\":10,\"patch\":\"diff --git a/README.md b/README.md\\nindex b2bf4db5..7acc4843 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -729,3 +729,13 @@ steps:\\n ```\\n \\n See docs: docs/github-ops.md\\n+## Integration Tests (Great DX)\\n+\\n+Visor ships a YAML‑native integration test runner so you can describe user flows, mocks, and assertions alongside your config.\\n+\\n+- Start here: docs/testing/getting-started.md\\n+- CLI details: docs/testing/cli.md\\n+- Fixtures and mocks: docs/testing/fixtures-and-mocks.md\\n+- Assertions reference: docs/testing/assertions.md\\n+\\n+Example suite: defaults/.visor.tests.yaml\\n\",\"status\":\"added\"},{\"filename\":\"defaults/.visor.tests.yaml\",\"additions\":20,\"deletions\":0,\"changes\":557,\"patch\":\"diff --git a/defaults/.visor.tests.yaml b/defaults/.visor.tests.yaml\\nnew file mode 100644\\nindex 00000000..c496cdee\\n--- /dev/null\\n+++ b/defaults/.visor.tests.yaml\\n@@ -0,0 +1,557 @@\\n+version: \\\"1.0\\\"\\n+extends: \\\".visor.yaml\\\"\\n+\\n+# Integration test suite for Visor default configuration\\n+# - Driven by events + fixtures; no manual step lists\\n+# - Strict by default: every executed step must have an expect\\n+# - AI mocks accept structured JSON when a schema is defined; plain uses text\\n+# - GitHub calls are recorded by default by the test runner (no network)\\n+\\n+tests:\\n+  defaults:\\n+    strict: true\\n+    ai_provider: mock\\n+    fail_on_unexpected_calls: true\\n+    # Example: enable negative GitHub recorder for all tests\\n+    # github_recorder: { error_code: 429 }\\n+  # Built-in fixtures are provided by the test runner (gh.* namespace).\\n+  # Custom fixtures may still be added here if needed.\\n+  fixtures: []\\n+\\n+  cases:\\n+    - name: label-flow\\n+      description: |\\n+        Validates the happy path for PR open:\\n+        - overview runs and emits tags.label and tags.review-effort (mocked)\\n+        - apply-overview-labels adds two labels (feature and review/effort:2)\\n+        - overview prompt includes PR title and unified diff header\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: |\\n+            High‑level summary of the changes and impact.\\n+          tags:\\n+            label: feature\\n+            review-effort: 2\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args:\\n+              contains:\\n+                - feature\\n+                - \\\"review/effort:2\\\"\\n+        outputs:\\n+          - step: overview\\n+            path: \\\"tags.label\\\"\\n+            equals: feature\\n+          - step: overview\\n+            path: \\\"tags['review-effort']\\\"\\n+            equals: 2\\n+        prompts:\\n+          - step: overview\\n+            contains:\\n+              - \\\"feat: add user search\\\"\\n+              - \\\"diff --git a/src/search.ts\\\"\\n+\\n+    - name: issue-triage\\n+      skip: true\\n+      description: |\\n+        Ensures the issue assistant triages a newly opened issue and applies labels.\\n+        Asserts the structured output (intent=issue_triage) and the GitHub label op.\\n+      event: issue_opened\\n+      fixture: gh.issue_open.minimal\\n+      mocks:\\n+        issue-assistant:\\n+          text: |\\n+            Thanks for the detailed report! We will investigate.\\n+          intent: issue_triage\\n+          labels: [bug, priority/medium]\\n+      expect:\\n+        calls:\\n+          - step: issue-assistant\\n+            exactly: 1\\n+          - step: apply-issue-labels\\n+            exactly: 1\\n+          - step: init-fact-validation\\n+            exactly: 1\\n+          - step: extract-facts\\n+            exactly: 1\\n+          - step: validate-fact\\n+            at_least: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args:\\n+              contains:\\n+                - bug\\n+        outputs:\\n+          - step: issue-assistant\\n+            path: intent\\n+            equals: issue_triage\\n+        prompts:\\n+          - step: issue-assistant\\n+            contains:\\n+              - \\\"Bug: crashes on search edge case\\\"\\n+\\n+    - name: pr-review-e2e-flow\\n+      description: |\\n+        End-to-end PR lifecycle covering multiple external events:\\n+        1) PR opened → overview + labels\\n+        2) Standard comment → no bot reply\\n+        3) /visor help → single assistant reply (no retrigger)\\n+        4) /visor Regenerate reviews → retrigger overview\\n+        5) Fact validation enabled on comment → extract/validate/aggregate\\n+        6) Fact validation disabled on comment → only assistant, no validation steps\\n+        7) PR synchronized (new commit) → overview runs again\\n+      strict: true\\n+      flow:\\n+        - name: pr-open\\n+          description: |\\n+            PR open event. Mocks overview/security/quality/performance as empty issue lists.\\n+            Expects all review steps to run and labels to be added.\\n+          event: pr_opened\\n+          fixture: gh.pr_open.minimal\\n+          mocks:\\n+            overview:\\n+              text: \\\"Overview body\\\"\\n+              tags: { label: feature, review-effort: 2 }\\n+            security: { issues: [] }\\n+            architecture: { issues: [] }\\n+            quality: { issues: [] }\\n+            performance: { issues: [] }\\n+          expect:\\n+            calls:\\n+              - step: overview\\n+                exactly: 1\\n+              - step: security\\n+                exactly: 1\\n+              - step: architecture\\n+                exactly: 1\\n+              - step: performance\\n+                exactly: 1\\n+              - step: quality\\n+                exactly: 1\\n+              - step: apply-overview-labels\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.createComment\\n+                at_least: 1\\n+              - provider: github\\n+                op: labels.add\\n+                at_least: 1\\n+                args:\\n+                  contains: [feature]\\n+            prompts:\\n+              - step: overview\\n+                contains:\\n+                  - \\\"feat: add user search\\\"\\n+\\n+        - name: standard-comment\\n+          description: |\\n+            A regular human comment on a PR should not produce a bot reply.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.standard\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"\\\"   # empty text to avoid posting a reply\\n+              intent: comment_reply\\n+          expect:\\n+            no_calls:\\n+              - provider: github\\n+                op: issues.createComment\\n+              - step: init-fact-validation\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+\\n+        - name: visor-plain\\n+          description: |\\n+            A \\\"/visor help\\\" comment should be recognized and answered once by the assistant.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"Sure, here’s how I can help.\\\"\\n+              intent: comment_reply\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.createComment\\n+                exactly: 1\\n+            outputs:\\n+              - step: comment-assistant\\n+                path: intent\\n+                equals: comment_reply\\n+            prompts:\\n+              - step: comment-assistant\\n+                matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"\\n+\\n+        - name: visor-retrigger\\n+          description: |\\n+            A \\\"/visor Regenerate reviews\\\" comment should set intent=comment_retrigger\\n+            and schedule a new overview.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_regenerate\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"Regenerating.\\\"\\n+              intent: comment_retrigger\\n+            overview:\\n+              text: \\\"Overview (regenerated)\\\"\\n+              tags: { label: feature, review-effort: 2 }\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+              - step: overview\\n+                exactly: 1\\n+              - step: security\\n+                exactly: 1\\n+              - step: architecture\\n+                exactly: 1\\n+              - step: performance\\n+                exactly: 1\\n+              - step: quality\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.createComment\\n+                at_least: 1\\n+            outputs:\\n+              - step: comment-assistant\\n+                path: intent\\n+                equals: comment_retrigger\\n+            prompts:\\n+              - step: comment-assistant\\n+                contains: [\\\"Regenerate reviews\\\"]\\n+\\n+        - name: facts-enabled\\n+          description: |\\n+            With fact validation enabled, the assistant reply is followed by\\n+            extract-facts, validate-fact (per fact), and aggregate-validations.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"We rely on defaults/.visor.yaml line 11 for max_parallelism=4.\\\"\\n+              intent: comment_reply\\n+            extract-facts:\\n+              - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+            validate-fact[]:\\n+              - fact_id: f1\\n+                claim: \\\"max_parallelism defaults to 4\\\"\\n+                is_valid: true\\n+                confidence: high\\n+                evidence: \\\"defaults/.visor.yaml:11\\\"\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+              - step: extract-facts\\n+                exactly: 1\\n+              - step: validate-fact\\n+                at_least: 1\\n+              - step: aggregate-validations\\n+                exactly: 1\\n+              - step: init-fact-validation\\n+                exactly: 1\\n+          # Prompt assertions are validated separately in stage-level prompt tests\\n+\\n+        - name: facts-invalid\\n+          description: |\\n+            Invalid fact path: after assistant reply, extract-facts finds one claim and\\n+            validate-fact returns is_valid=false; aggregate-validations detects not-all-valid\\n+            and reruns the assistant once with correction context.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"We rely on defaults/.visor.yaml line 11 for max_parallelism=4.\\\"\\n+              intent: comment_reply\\n+            extract-facts:\\n+              - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+            validate-fact[]:\\n+              - fact_id: f1\\n+                claim: \\\"max_parallelism defaults to 4\\\"\\n+                is_valid: false\\n+                confidence: high\\n+                evidence: \\\"defaults/.visor.yaml:11 does not set 4\\\"\\n+                correction: \\\"max_parallelism defaults to 3\\\"\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 2\\n+              - step: extract-facts\\n+                exactly: 1\\n+              - step: validate-fact\\n+                at_least: 1\\n+              - step: aggregate-validations\\n+                exactly: 1\\n+              - step: init-fact-validation\\n+                exactly: 1\\n+            prompts:\\n+              - step: comment-assistant\\n+                index: last\\n+                contains:\\n+                  - \\\"<previous_response>\\\"\\n+                  - \\\"max_parallelism defaults to 4\\\"\\n+                  - \\\"Correction: max_parallelism defaults to 3\\\"\\n+\\n+        - name: facts-two-items\\n+          description: |\\n+            Two facts extracted; only the invalid fact should appear in the correction pass.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"We rely on defaults/.visor.yaml for concurrency defaults.\\\"\\n+              intent: comment_reply\\n+            extract-facts:\\n+              - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+              - { id: f2, category: Feature,       claim: \\\"Fast mode is enabled by default\\\", verifiable: true }\\n+            validate-fact[]:\\n+              - { fact_id: f1, claim: \\\"max_parallelism defaults to 4\\\", is_valid: false, confidence: high, evidence: \\\"defaults/.visor.yaml:11\\\", correction: \\\"max_parallelism defaults to 3\\\" }\\n+              - { fact_id: f2, claim: \\\"Fast mode is enabled by default\\\", is_valid: true, confidence: high, evidence: \\\"src/config.ts:FAST_MODE=true\\\" }\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 2\\n+              - step: extract-facts\\n+                exactly: 1\\n+              - step: validate-fact\\n+                exactly: 2\\n+              - step: aggregate-validations\\n+                exactly: 1\\n+              - step: init-fact-validation\\n+                exactly: 1\\n+            outputs:\\n+              - step: validate-fact\\n+                where: { path: fact_id, equals: f1 }\\n+                path: is_valid\\n+                equals: false\\n+              - step: validate-fact\\n+                where: { path: fact_id, equals: f2 }\\n+                path: is_valid\\n+                equals: true\\n+            prompts:\\n+              - step: comment-assistant\\n+                index: last\\n+                contains:\\n+                  - \\\"<previous_response>\\\"\\n+                  - \\\"max_parallelism defaults to 4\\\"\\n+                  - \\\"Correction: max_parallelism defaults to 3\\\"\\n+                not_contains:\\n+                  - \\\"Fast mode is enabled by default\\\"\\n+\\n+        - name: facts-disabled\\n+          description: |\\n+            With fact validation disabled, only the assistant runs; no validation steps execute.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.standard\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"false\\\"\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+            no_calls:\\n+              - step: init-fact-validation\\n+              - step: extract-facts\\n+              - step: validate-fact\\n+              - step: aggregate-validations\\n+\\n+        - name: pr-updated\\n+          description: |\\n+            When a new commit is pushed (synchronize), overview should run again\\n+            and post/refresh a comment.\\n+          event: pr_updated\\n+          fixture: gh.pr_sync.minimal\\n+          mocks:\\n+            overview:\\n+              text: \\\"Overview for new commit\\\"\\n+              tags: { label: feature, review-effort: 3 }\\n+          expect:\\n+            calls:\\n+              - step: overview\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.updateComment\\n+                at_least: 1\\n+\\n+    - name: security-fail-if\\n+      description: |\\n+        Verifies that the global fail_if trips when security produces an error‑severity issue.\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview text\\\"\\n+          tags:\\n+            label: bug\\n+            review-effort: 3\\n+        security:\\n+          issues:\\n+            - id: S-001\\n+              file: src/search.ts\\n+              line: 10\\n+              message: \\\"Command injection risk\\\"\\n+              severity: error\\n+              category: security\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: security\\n+            exactly: 1\\n+        outputs:\\n+          - step: security\\n+            path: \\\"issues[0].severity\\\"\\n+            equals: error\\n+        fail:\\n+          message_contains: \\\"fail_if\\\"\\n+\\n+    - name: strict-mode-example\\n+      skip: true\\n+      description: |\\n+        Demonstrates strict mode: a step executed without a corresponding expect\\n+        (apply-overview-labels) triggers a strict_violation with a helpful message.\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: \\\"Short overview\\\"\\n+          tags:\\n+            label: chore\\n+            review-effort: 1\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+        strict_violation:\\n+          for_step: apply-overview-labels\\n+          message_contains: \\\"Add an expect for this step or set strict: false\\\"\\n+\\n+    - name: visor-plain-prompt\\n+      description: |\\n+        Standalone prompt check for a \\\"/visor help\\\" comment.\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_help\\n+      mocks:\\n+        comment-assistant:\\n+          text: \\\"Here is how I can help.\\\"\\n+          intent: comment_reply\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+        prompts:\\n+          - step: comment-assistant\\n+            matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"\\n+\\n+    - name: visor-retrigger-prompt\\n+      description: |\\n+        Standalone prompt check for \\\"/visor Regenerate reviews\\\".\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_regenerate\\n+      strict: false\\n+      mocks:\\n+        comment-assistant:\\n+          text: \\\"Regenerating.\\\"\\n+          intent: comment_retrigger\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+        prompts:\\n+          - step: comment-assistant\\n+            contains: [\\\"Regenerate reviews\\\"]\\n+\\n+    - name: command-mock-shape\\n+      description: |\\n+        Illustrates command provider mocking and output assertions.\\n+        Skipped by default; enable when command steps exist.\\n+      skip: true  # illustrative only, enable when a command step exists\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        unit-tests:\\n+          stdout: '{\\\"passed\\\": 128, \\\"failed\\\": 0, \\\"duration_sec\\\": 1.2}'\\n+          exit_code: 0\\n+      expect:\\n+        calls:\\n+          - step: unit-tests\\n+            exactly: 1\\n+        outputs:\\n+          - step: unit-tests\\n+            path: failed\\n+            equals: 0\\n+\\n+    - name: github-negative-mode\\n+      description: |\\n+        Demonstrates negative GitHub recorder mode: simulate a 429 error and assert failure path.\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      github_recorder: { error_code: 429 }\\n+      # Override defaults for this case only by specifying a local recorder via env-like knob\\n+      # The runner reads tests.defaults.github_recorder; we provide it at the suite level by default.\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview body\\\"\\n+          tags: { label: feature, review-effort: 2 }\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+        fail:\\n+          message_contains: \\\"github/op_failed\\\"\\n+\\n+    - name: facts-invalid\\n+      skip: true\\n+      description: |\\n+        With fact validation enabled and an invalid fact, aggregate-validations should detect\\n+        not-all-valid and route back to the assistant for a correction pass in the same stage.\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_help\\n+      env:\\n+        ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+      mocks:\\n+        comment-assistant:\\n+          text: \\\"We rely on defaults/.visor.yaml line 11 for max_parallelism=4.\\\"\\n+          intent: comment_reply\\n+        extract-facts:\\n+          - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+        validate-fact[]:\\n+          - fact_id: f1\\n+            claim: \\\"max_parallelism defaults to 4\\\"\\n+            is_valid: false\\n+            confidence: high\\n+            evidence: \\\"defaults/.visor.yaml:11 does not set 4\\\"\\n+            correction: \\\"max_parallelism defaults to 3\\\"\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+          - step: extract-facts\\n+            exactly: 1\\n+          - step: validate-fact\\n+            at_least: 1\\n+          - step: aggregate-validations\\n+            exactly: 1\\n+          - step: init-fact-validation\\n+            exactly: 1\\n\",\"status\":\"added\"},{\"filename\":\"defaults/.visor.yaml\",\"additions\":1,\"deletions\":1,\"changes\":25,\"patch\":\"diff --git a/defaults/.visor.yaml b/defaults/.visor.yaml\\nindex 0e018884..21fad9eb 100644\\n--- a/defaults/.visor.yaml\\n+++ b/defaults/.visor.yaml\\n@@ -452,8 +452,9 @@ steps:\\n         - Provide partial answers when possible, and indicate what additional information would help give a complete response\\n     on: [issue_comment]\\n     on_success:\\n-      # Always initialize fact validation attempt counter\\n-      run: [init-fact-validation]\\n+      # Initialize fact validation attempt counter only when validation is enabled\\n+      run_js: |\\n+        return env.ENABLE_FACT_VALIDATION === 'true' ? ['init-fact-validation'] : []\\n       # Preserve intent-based rerun: allow members to retrigger overview from a comment\\n       goto_js: |\\n         const intent = (typeof output === 'object' && output) ? output.intent : undefined;\\n@@ -619,8 +620,14 @@ steps:\\n     # After all facts are validated, aggregate results and decide next action\\n     on_finish:\\n       run: [aggregate-validations]\\n+      # If aggregation stored validation issues in memory, schedule a correction reply\\n+      run_js: |\\n+        const issues = memory.list('fact-validation').includes('fact_validation_issues')\\n+          ? memory.get('fact_validation_issues', 'fact-validation')\\n+          : [];\\n+        return Array.isArray(issues) && issues.length > 0 ? ['comment-assistant'] : [];\\n       goto_js: |\\n-        const allValid = memory.get('all_valid', 'fact-validation');\\n+        const allValid = memory.get('all_facts_valid', 'fact-validation');\\n         const attempt = memory.get('fact_validation_attempt', 'fact-validation') || 0;\\n \\n         log('🔍 Fact validation complete - allValid:', allValid, 'attempt:', attempt);\\n@@ -702,11 +709,10 @@ steps:\\n     on: [issue_opened, issue_comment]\\n     if: \\\"env.ENABLE_FACT_VALIDATION === 'true'\\\"\\n     on_success:\\n-      goto_js: |\\n-        // Route back to the appropriate assistant if there are issues\\n-        if (!output || output.all_valid) return null;\\n-        const hasComment = !!(outputs['comment-assistant']) || (outputs.history && (outputs.history['comment-assistant'] || []).length > 0);\\n-        return hasComment ? 'comment-assistant' : 'issue-assistant';\\n+      # Schedule the correction reply directly (target-only) when not all facts are valid\\n+      run_js: |\\n+        if (!output || output.all_valid) return [];\\n+        return ['comment-assistant'];\\n     memory_js: |\\n       const validations = outputs.history['validate-fact'] || [];\\n \\n@@ -722,8 +728,7 @@ steps:\\n       log('Results: valid=' + (validations.length - invalid.length - lowConfidence.length),\\n           'invalid=' + invalid.length, 'low-confidence=' + lowConfidence.length);\\n \\n-      memory.set('all_valid', allValid, 'fact-validation');\\n-      memory.set('total_validations', validations.length, 'fact-validation');\\n+      memory.set('all_facts_valid', allValid, 'fact-validation');\\n       memory.set('validation_results', validations, 'fact-validation');\\n       memory.set('invalid_facts', invalid, 'fact-validation');\\n       memory.set('low_confidence_facts', lowConfidence, 'fact-validation');\\n\",\"status\":\"modified\"},{\"filename\":\"docs/test-framework-rfc.md\",\"additions\":22,\"deletions\":0,\"changes\":641,\"patch\":\"diff --git a/docs/test-framework-rfc.md b/docs/test-framework-rfc.md\\nnew file mode 100644\\nindex 00000000..7b899980\\n--- /dev/null\\n+++ b/docs/test-framework-rfc.md\\n@@ -0,0 +1,641 @@\\n+# Visor Integration Test Framework (RFC)\\n+\\n+Status: In Progress\\n+Date: 2025-10-27\\n+Owners: @probelabs/visor\\n+\\n+## Summary\\n+\\n+Add a first‑class, YAML‑native integration test framework for Visor that lets teams describe user flows, mocks, and assertions directly alongside their Visor config. Tests are defined in a separate YAML that can `extends` the base configuration, run entirely offline (no network), and default to strict verification.\\n+\\n+Key ideas:\\n+- Integration‑first: simulate real GitHub events and repo context; no manual step lists.\\n+- Strict by default: if a step ran and you didn’t assert it, the test fails.\\n+- Provider record mode by default: GitHub calls are intercepted and recorded (no network); assert them later.\\n+- Simple mocks keyed by step name; schema‑aware AI outputs (objects/arrays for structured schemas; `text` for plain).\\n+- Support multi‑event “flows” that preserve memory and outputs across events.\\n+\\n+## Motivation\\n+\\n+- Keep tests next to config and use the same mental model: events → checks → outputs → effects.\\n+- Validate real behavior (routing, `on` filters, `if` guards, `goto`/`on_success`, forEach) rather than unit‑style steps.\\n+- Make CI reliable and offline by default while still asserting side‑effects (labels, comments, check runs).\\n+\\n+## Non‑Goals\\n+\\n+- Unit testing individual providers (covered by Jest/TS tests).\\n+- Golden CI logs; we assert structured outputs and recorded operations instead.\\n+\\n+## Terminology\\n+\\n+- Case: a single integration test driven by one event + fixture.\\n+- Flow: an ordered list of cases; runner preserves memory/outputs across steps.\\n+- Fixture: a reusable external context (webhook payload, changed files, env, fs overlay, frozen clock).\\n+\\n+## File Layout\\n+\\n+- Base config (unchanged): `defaults/.visor.yaml` (regular steps live here).\\n+- Test suite (new): `defaults/.visor.tests.yaml`\\n+  - `extends: \\\".visor.yaml\\\"` to inherit the base checks.\\n+  - Contains `tests.defaults`, `tests.fixtures`, `tests.cases`.\\n+\\n+## Default Behaviors (Test Mode)\\n+\\n+- Strict mode: enabled by default (`tests.defaults.strict: true`). Any executed step must appear in `expect.calls`, or the case fails.\\n+- GitHub recording: the runner uses a recording Octokit by default; no network calls are made. Assert effects via `expect.calls` with `provider: github` and an `op` (e.g., `issues.createComment`, `labels.add`, `checks.create`).\\n+- AI provider: `mock` by default for tests; schema‑aware handling (see below).\\n+\\n+## Built‑in Fixtures and GitHub Mocks\\n+\\n+The runner ships with a library of built‑in fixtures and a recording GitHub mock so you don’t have to redefine common scenarios.\\n+\\n+### Built‑in Fixtures (gh.*)\\n+\\n+Use via `fixture: <name>`:\\n+\\n+- `gh.pr_open.minimal` — pull_request opened with a small PR (branch/base, 1–2 files, tiny patch).\\n+- `gh.pr_sync.minimal` — pull_request synchronize (new commit pushed) with updated HEAD SHA.\\n+- `gh.issue_open.minimal` — issues opened with a short title/body.\\n+- `gh.issue_comment.standard` — issue_comment created with a normal message on a PR.\\n+- `gh.issue_comment.visor_help` — issue_comment created with \\\"/visor help\\\".\\n+- `gh.issue_comment.visor_regenerate` — issue_comment created with \\\"/visor Regenerate reviews\\\".\\n+- `gh.issue_comment.edited` — issue_comment edited event.\\n+- `gh.pr_closed.minimal` — pull_request closed event.\\n+\\n+All gh.* fixtures populate:\\n+- `webhook` (name, action, payload)\\n+- `git` (branch, baseBranch)\\n+- `files` and `diff` (for PR fixtures)\\n+- `env` and `time.now` for determinism\\n+\\n+Optional overrides (future):\\n+\\n+```yaml\\n+fixture:\\n+  builtin: gh.pr_open.minimal\\n+  overrides:\\n+    pr.title: \\\"feat: custom title\\\"\\n+    webhook.payload.pull_request.number: 42\\n+```\\n+\\n+### GitHub Recorder (Built‑in)\\n+\\n+By default in test mode, the runner installs a recording Octokit:\\n+- Captures all calls and args for assertions (`expect.calls` with `provider: github`).\\n+- Returns stable stub shapes to unblock flows:\\n+  - `issues.createComment` → `{ data: { id, html_url, body, user, created_at } }`\\n+  - `issues.updateComment` → same shape\\n+  - `pulls.get`, `pulls.listFiles` → derived from fixture\\n+  - `checks.create`, `checks.update` → `{ data: { id, status, conclusion, url } }`\\n+  - `labels.add` → `{ data: { labels: [ ... ] } }` (or a no‑op with capture)\\n+\\n+  No network calls are made. You can still opt into real Octokit in the future with a `mode: passthrough` runner flag (not default).\\n+  Optional negative modes (per case or global):\\n+  - `error(429|422|404)` — simulate API errors; captured in call history.\\n+  - `timeout(1000ms)` — simulate request timeouts.\\n+\\n+## YAML Syntax Overview\\n+\\n+Minimal suite:\\n+\\n+```yaml\\n+version: \\\"1.0\\\"\\n+extends: \\\".visor.yaml\\\"\\n+\\n+tests:\\n+  defaults:\\n+    strict: true\\n+    ai_provider: mock\\n+    macros:\\n+      expect_review_posted:\\n+        calls:\\n+          - provider: github\\n+            op: issues.createComment\\n+            at_least: 1\\n+\\n+  fixtures: []   # (Optional) rely on gh.* built‑ins\\n+\\n+  cases:\\n+    - name: label-flow\\n+      event: pr_opened\\n+      fixture:\\n+        builtin: gh.pr_open.minimal\\n+        overrides:\\n+          pr.title: \\\"feat: add user search\\\"\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview body\\\"\\n+          tags: { label: feature, review-effort: 2 }\\n+      expect:\\n+        use: [expect_review_posted]\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args:\\n+              contains_unordered: [feature, \\\"review/effort:2\\\"]\\n+```\\n+\\n+### Flows (multi‑event)\\n+\\n+```yaml\\n+- name: pr-review-e2e-flow\\n+  strict: true\\n+  flow:\\n+    - name: pr-open\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview: { text: \\\"Overview body\\\", tags: { label: feature, review-effort: 2 } }\\n+        security: { issues: [] }\\n+        quality: { issues: [] }\\n+        performance: { issues: [] }\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: security\\n+            exactly: 1\\n+          - step: architecture\\n+            exactly: 1\\n+          - step: performance\\n+            exactly: 1\\n+          - step: quality\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: issues.createComment\\n+            at_least: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args: { contains: [feature] }\\n+\\n+    - name: visor-plain\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_help\\n+      mocks:\\n+        comment-assistant: { text: \\\"Sure, here's how I can help.\\\", intent: comment_reply }\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+          - provider: github\\n+            op: issues.createComment\\n+            exactly: 1\\n+        outputs:\\n+          - step: comment-assistant\\n+            path: intent\\n+            equals: comment_reply\\n+```\\n+\\n+## CLI Usage\\n+\\n+- Discover tests:\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --list`\\n+- Validate test file shape (schema):\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --validate`\\n+- Run all tests with compact progress (default):\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml`\\n+- Run a single case:\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --only label-flow`\\n+- Run a single stage in a flow (by name or 1‑based index):\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --only pr-review-e2e-flow#facts-invalid`\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --only pr-review-e2e-flow#3`\\n+- Emit artifacts:\\n+  - JSON: `--json output/visor-tests.json`\\n+  - JUnit: `--report junit:output/visor-tests.xml`\\n+  - Markdown summary: `--summary md:output/visor-tests.md`\\n+- Debug logs:\\n+  - Set `VISOR_DEBUG=true` for verbose routing/provider output.\\n+\\n+Notes\\n+- AI is forced to `mock` in test mode regardless of API keys.\\n+- The runner warns when an AI/command step runs without a mock (suppressed for `ai.provider=mock`).\\n+- Strict mode is on by default; add `strict: false` for prompt‑only cases.\\n+\\n+## Mocks (Schema‑Aware)\\n+\\n+- Keyed by step name under `mocks`.\\n+- AI with structured `schema` (e.g., `code-review`, `issue-assistant`): provide an object or array directly; no `returns` key.\\n+- AI with `schema: plain`: provide a string (or an object with `text`).\\n+- Command provider: `{ stdout: string, exit_code?: number }`.\\n+- Arrays: return arrays directly (e.g., `extract-facts`).\\n+\\n+Examples:\\n+\\n+```yaml\\n+mocks:\\n+  overview:\\n+    text: \\\"Overview body\\\"\\n+    tags: { label: feature, review-effort: 2 }\\n+\\n+  issue-assistant:\\n+    text: \\\"Thanks for the detailed report!\\\"\\n+    intent: issue_triage\\n+    labels: [bug]\\n+\\n+  extract-facts:\\n+    - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+\\n+  unit-tests:\\n+    stdout: '{\\\"passed\\\": 128, \\\"failed\\\": 0}'\\n+    exit_code: 0\\n+```\\n+\\n+## Assertions\\n+\\n+### Macros (Reusable Assertions)\\n+\\n+Define named bundles of assertions under `tests.defaults.macros` and reuse them via `expect.use: [macroName, ...]`.\\n+\\n+Example:\\n+\\n+```yaml\\n+tests:\\n+  defaults:\\n+    macros:\\n+      expect_review_posted:\\n+        calls:\\n+          - provider: github\\n+            op: issues.createComment\\n+            at_least: 1\\n+\\n+cases:\\n+  - name: example\\n+    expect:\\n+      use: [expect_review_posted]\\n+      calls:\\n+        - step: overview\\n+          exactly: 1\\n+```\\n+\\n+- Step calls: `expect.calls: [{ step: <name>, exactly|at_least|at_most: N }]`.\\n+- GitHub effects: `expect.calls: [{ provider: github, op: <owner.method>, times?, args? }]`.\\n+  - `op` examples: `issues.createComment`, `labels.add`, `checks.create`, `checks.update`.\\n+  - `args.contains` matches arrays/strings; `args.contains_unordered` ignores order; `args.equals` for strict equality.\\n+- Outputs: `expect.outputs: [{ step, path, equals|matches|equalsDeep }]`.\\n+  - `equalsDeep` performs deep structural comparison for objects/arrays.\\n+  - `path` uses dot/bracket syntax, e.g., `tags['review-effort']`, `issues[0].severity`.\\n+- Failures: `expect.fail.message_contains` for error message anchoring.\\n+- Strict violations: `expect.strict_violation.for_step` asserts the runner surfaced “step executed without expect.”\\n+\\n+### Prompt Assertions (AI)\\n+\\n+When mocking AI, you can assert on the final prompt text constructed by Visor (after Liquid templating and context injection):\\n+\\n+```yaml\\n+expect:\\n+  prompts:\\n+    - step: overview\\n+      contains:\\n+        - \\\"feat: add user search\\\"        # PR title from fixture\\n+        - \\\"diff --git a/src/search.ts\\\"   # patch content included\\n+      not_contains:\\n+        - \\\"BREAKING CHANGE\\\"\\n+    - step: comment-assistant\\n+      matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"   # case-insensitive regex\\n+```\\n+\\n+Rules:\\n+- `contains`: list of substrings that must appear in the prompt.\\n+- `not_contains`: list of substrings that must not appear.\\n+- `matches`: a single regex pattern string; add `(?i)` for case‑insensitive.\\n+- The runner captures the exact prompt Visor would send to the provider (with dynamic variables resolved and code context embedded) and evaluates these assertions.\\n+\\n+## Runner Semantics\\n+\\n+- Loads base config via `extends` and validates.\\n+- Applies fixture:\\n+  - Webhook payload → test event context\\n+  - Git metadata (branch/baseBranch)\\n+  - Files + patch list used by analyzers/prompts\\n+  - `fs_overlay` writes transient files (cleaned up after)\\n+  - `env` overlays process env for the case\\n+  - `time.now` freezes clock\\n+- Event routing: determines which checks run by evaluating `on`, `if`, `depends_on`, `goto`, `on_success`, and `forEach` semantics in the normal engine.\\n+- Recording providers:\\n+  - GitHub: recording Octokit (default) captures every call; no network.\\n+  - AI: mock provider that emits objects/arrays/strings per mocks and records the final prompt text per step for `expect.prompts`.\\n+\\n+### Call History and Recursion\\n+\\n+Some steps (e.g., fact validation loops) can run multiple times within a single case or flow stage. The runner records an invocation history for each step. You assert using the same top‑level sections (calls, prompts, outputs) with selectors:\\n+\\n+1) Count only\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: validate-fact\\n+      at_least: 2\\n+```\\n+\\n+2) Per‑call assertions by index (ordered)\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: validate-fact\\n+      exactly: 2\\n+  prompts:\\n+    - step: validate-fact\\n+      index: 0\\n+      contains: [\\\"Claim:\\\", \\\"max_parallelism defaults to 4\\\"]\\n+  outputs:\\n+    - step: validate-fact\\n+      index: 0\\n+      path: fact_id\\n+      equals: f1\\n+```\\n+\\n+3) Per‑call assertions without assuming order (filter by output)\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: validate-fact\\n+      at_least: 2\\n+  outputs:\\n+    - step: validate-fact\\n+      where: { path: fact_id, equals: f1 }\\n+      path: is_valid\\n+      equals: true\\n+    - step: validate-fact\\n+      where: { path: fact_id, equals: f2 }\\n+      path: confidence\\n+      equals: high\\n+```\\n+\\n+4) Select a specific history element\\n+\\n+```yaml\\n+expect:\\n+  prompts:\\n+    - step: validate-fact\\n+      index: last   # or 0,1,..., or 'first'\\n+      not_contains: [\\\"TODO\\\"]\\n+```\\n+  - HTTP: built‑in mock (url/method/status/body/latency) with record mode and assertions.\\n+  - Command: mock stdout/stderr/exit_code; record invocation for assertions.\\n+ - State across flows: `memory`, `outputs.history`, and step outputs persist across events within a single flow.\\n+- Strict enforcement: after execution, compare executed steps to `expect.calls`; any missing expect fails the case.\\n+\\n+## Validation & Helpful Errors\\n+\\n+- Reuse Visor's existing Ajv pipeline for the base config (`extends` target).\\n+- The tests DSL is validated at runtime with friendly errors (no separate schema file to maintain).\\n+- Errors show the YAML path, a short hint, and an example (e.g., suggest `args.contains_unordered` when order differs).\\n+- Inline diffs for strings (prompts) and objects (with deep compare) in failure output.\\n+\\n+### Determinism & Security\\n+\\n+- Stable IDs in the GitHub recorder (deterministic counters per run).\\n+- Order‑agnostic assertions for arrays via `args.contains_unordered`.\\n+- Prompt normalization (whitespace, code fences). Toggle with `--normalize-prompts=false`.\\n+- Secret redaction in prompts/args via ENV allowlist (default deny; redacts to `****`).\\n+\\n+## CLI\\n+\\n+```\\n+visor test --config defaults/.visor.tests.yaml         # run all cases\\n+visor test --config defaults/.visor.tests.yaml --only pr-review-e2e-flow\\n+visor test --config defaults/.visor.tests.yaml --list  # list case names\\n+```\\n+\\n+Exit codes:\\n+- 0: all tests passed\\n+- 1: one or more cases failed\\n+\\n+### CLI Output UX (must‑have)\\n+\\n+The runner prints a concise, human‑friendly summary optimized for scanning:\\n+\\n+- Suite header with total cases and elapsed time.\\n+- Per‑case line with status symbol and duration, e.g.,\\n+  - ✅ label-flow (1.23s)\\n+  - ❌ security-fail-if (0.42s)\\n+- When a case is expanded (auto‑expand on failure):\\n+  - Input context: event + fixture name.\\n+  - Executed steps (in order), with counts for multi‑call steps.\\n+  - Assertions grouped by type (calls, prompts, outputs) with checkmarks.\\n+  - GitHub calls table (op, count, first args snippet).\\n+  - Prompt preview (truncated) with a toggle to show full text.\\n+  - First mismatch shows an inline diff (expected vs actual substring/regex or value), with a clear hint to fix.\\n+- Flow cases show each stage nested under the parent with roll‑up status.\\n+- Summary footer with pass/fail counts, slowest cases, and a hint to rerun focused:\\n+  - e.g., visor test --config defaults/.visor.tests.yaml --only security-fail-if\\n+\\n+Color, symbols, and truncation rules mirror our main CLI:\\n+- Green checks for passes, red crosses for failures, yellow for skipped.\\n+- Truncate long prompts/JSON with ellipsis; provide a flag `--verbose` to show full payloads.\\n+\\n+### Additional Flags & Modes\\n+\\n+- `--only <name>`: run a single case/flow by exact name.\\n+- `--bail`: stop at first failure.\\n+- `--json`: emit machine‑readable results to stdout.\\n+- `--report junit:path.xml`: write JUnit XML to path.\\n+- `--summary md:path.md`: write a Markdown summary artifact.\\n+- `--progress compact|detailed`: toggle rendering density.\\n+- `--max-parallel N`: reuse existing parallelism flag (no test‑specific variant).\\n+\\n+## Coverage & Reporting\\n+\\n+- Step coverage per case (executed vs expected), with a short table.\\n+- JUnit and JSON reporters for CI visualization.\\n+- Optional Markdown summary: failing cases, first mismatch, rerun hints.\\n+\\n+## Implementation Plan (Milestones)\\n+\\n+This plan delivers the test framework incrementally, minimizing risk and reusing Visor internals.\\n+\\n+Progress Tracker\\n+- Milestone 0 — DSL freeze and scaffolding — DONE (2025-10-27)\\n+- Milestone 1 — MVP runner and single‑event cases — DONE (2025-10-27)\\n+- Milestone 2 — Built‑in fixtures — DONE (2025-10-27)\\n+- Milestone 3 — Prompt capture and assertions — DONE (2025-10-27)\\n+- Milestone 4 — Multi‑call history and selectors — DONE (2025-10-27)\\n+- Milestone 5 — Flows and state persistence — DONE (2025-10-27)\\n+- Milestone 6 — HTTP/Command mocks + negative modes — DONE (2025-10-27)\\n+- Milestone 7 — CLI reporters/UX polish — DONE (2025-10-27)\\n+- Milestone 8 — Validation and helpful errors — DONE (2025-10-27)\\n+- Milestone 9 — Coverage and perf — DONE (2025-10-27)\\n+- Milestone 10 — Docs, examples, migration — PENDING\\n+\\n+Progress Update — 2025-10-28\\n+- Runner: stage execution coverage now derives only from actual prompts/output-history deltas and engine statistics (no selection heuristics). Single-check runs contribute to statistics and history uniformly.\\n+- Engine: single-check path records iteration stats and appends outputs to history; on_finish children run via unified scheduler so runs are counted.\\n+- UX: noisy debug prints gated behind VISOR_DEBUG; stage headers and coverage tables remain.\\n+- Known gap: flow stage “facts-invalid” still fails under strict because the initial assistant/validation chain does not execute under the test runner for issue_comment; aggregator fallback runs. Next step is to trace event filtering inside executeGroupedChecks and ensure the main stage selection executes event-matching checks in tests.\\n+\\n+Milestone 0 — DSL freeze and scaffolding (0.5 week) — DONE 2025-10-27\\n+- Finalize DSL keys: tests.defaults, fixtures, cases, flow, fixture, mocks, expect.{calls,prompts,outputs,fail,strict_violation}. ✅\\n+- Rename use_fixture → fixture across examples (done in this RFC and defaults/.visor.tests.yaml). ✅\\n+- Create module skeletons: ✅\\n+  - src/test-runner/index.ts (entry + orchestration)\\n+  - src/test-runner/fixture-loader.ts (builtin + overrides)\\n+  - src/test-runner/recorders/github-recorder.ts (now dynamic Proxy-based)\\n+  - src/test-runner/assertions.ts (calls/prompts/outputs types + count validator)\\n+  - src/test-runner/utils/selectors.ts (deepGet)\\n+- CLI: add visor test (discovery). ✅\\n+- Success criteria: builds pass; “hello world” run prints discovered cases. ✅ (verified via npm run build and visor test)\\n+\\n+Progress Notes\\n+- Discovery works against any .visor.tests.yaml (general-purpose, not tied to defaults).\\n+- Recording Octokit records arbitrary rest ops without hardcoding method lists.\\n+- defaults/.visor.tests.yaml updated to consistent count grammar and fixed indentation issues.\\n+\\n+Milestone 1 — MVP runner and single‑event cases (1 week) — DONE 2025-10-27 (non‑flow)\\n+- CLI: add visor test [--config path] [--only name] [--bail] [--list]. ✅\\n+- Parsing: load tests file (extends) and hydrate cases. ✅\\n+- Execution: per case (non‑flow), synthesize PRInfo and call CheckExecutionEngine once. ✅\\n+- GitHub recorder default: injected recording Octokit; no network. ✅\\n+- Assertions: expect.calls for steps and provider ops (exactly|at_least|at_most); strict mode enforced. ✅\\n+- Output: basic per‑case status + summary. ✅\\n+- Success criteria: label-flow, issue-triage, strict-mode-example, security-fail-if pass locally. ✅\\n+\\n+Notes\\n+- Flow cases are deferred to Milestone 5 (state persistence) and will be added later.\\n+- AI provider forced to mock in test mode unless overridden by suite defaults.\\n+\\n+Verification\\n+- Build CLI + SDK: npm run build — success.\\n+- Discovery: visor test --config defaults/.visor.tests.yaml --list — lists suite and cases.\\n+- Run single cases:\\n+  - visor test --config defaults/.visor.tests.yaml --only label-flow — PASS\\n+  - visor test --config defaults/.visor.tests.yaml --only issue-triage — PASS\\n+  - visor test --config defaults/.visor.tests.yaml --only security-fail-if — PASS\\n+  - visor test --config defaults/.visor.tests.yaml --only strict-mode-example — PASS\\n+- Behavior observed:\\n+  - Strict mode enforced (steps executed but not asserted would fail). \\n+  - GitHub ops recorded by default with dynamic recorder, no network calls.\\n+  - Provider and step call counts respected (exactly | at_least | at_most).\\n+\\n+Milestone 2 — Built‑in fixtures (0.5–1 week) — DONE 2025-10-27\\n+- Implement gh.* builtins: pr_open.minimal, pr_sync.minimal, issue_open.minimal, issue_comment.standard, issue_comment.visor_help, issue_comment.visor_regenerate.\\n+- Support fixture overrides (fixture: { builtin, overrides }).\\n+- Wire files+diff into the engine’s analyzers.\\n+- Success criteria: pr-review-e2e-flow “pr-open”, “standard-comment”, “visor-plain”, “visor-retrigger” run with built-ins.\\n+Notes:\\n+- gh.* builtins implemented with webhook payloads and minimal diffs for PR fixtures.\\n+- Runner accepts fixture: { builtin, overrides } and applies overrides to pr.* and webhook.* paths.\\n+- Diffs surfaced via PRInfo.fullDiff; prompts include diff header automatically.\\n+- Flow execution will be delivered in Milestone 5; the same built-ins power the standalone prompt cases added now.\\n+\\n+Milestone 3 — Prompt capture and prompt assertions (0.5 week) — DONE 2025-10-27\\n+- Capture final AI prompt string per step after Liquid/context assembly. ✅ (AICheckProvider hook)\\n+- Assertions: expect.prompts contains | not_contains | matches (regex). ✅\\n+- Add `prompts.where` selector to target a prompt from history by content. ✅\\n+- Success criteria: prompt checks pass for label-flow, issue-triage, visor-plain, visor-retrigger. ✅\\n+- Notes: added standalone cases visor-plain-prompt and visor-retrigger-prompt for prompt-only validation.\\n+\\n+Milestone 4 — Multi‑call history and selectors (1 week) — DONE 2025-10-27\\n+- Per-step invocation history recorded and exposed by engine (outputs.history). ✅\\n+- index selector for prompts and outputs (first|last|N). ✅\\n+- where selector for outputs: { path, equals|matches }. ✅\\n+- equalsDeep for outputs. ✅\\n+- contains_unordered for array outputs. ✅\\n+- Regex matches for outputs. ✅\\n+\\n+Milestone 5 — Flows and state persistence (0.5–1 week) — DONE 2025-10-27\\n+- Implemented flow execution with shared engine + recorder across stages. ✅\\n+- Preserves MemoryStore state, outputs.history and provider calls between stages. ✅\\n+- Stage-local deltas for assertions (prompts, outputs, calls). ✅\\n+- Success criteria: full pr-review-e2e-flow passes. ✅\\n+\\n+Milestone 6 — HTTP/Command mocks and advanced GitHub modes (1 week) — DONE 2025-10-27\\n+- Command mocks: runner injects mocks via ExecutionContext; provider short-circuits to return stdout/exit_code. ✅\\n+- HTTP client mocks: provider returns mocked response via ExecutionContext without network. ✅\\n+- GitHub recorder negative modes: error(code) and timeout(ms) supported via tests.defaults.github_recorder. ✅\\n+- Success criteria: command-mock-shape passes; negative modes available for future tests. ✅\\n+\\n+Milestone 7 — CLI UX polish and reporters (0.5–1 week) — DONE 2025-10-27\\n+- Flags: --json <path|->, --report junit:<path>, --summary md:<path>, --progress compact|detailed. ✅\\n+- Compact progress with per-case PASS/FAIL lines; summary at end. ✅\\n+- JSON/JUnit/Markdown reporters now include per-case details (name, pass/fail, errors). ✅\\n+\\n+Milestone 8 — Validation and helpful errors (0.5 week) — DONE 2025-10-27\\n+- Reuse ConfigManager + Ajv for base config. ✅\\n+- Lightweight runtime validation for tests DSL with precise YAML paths and hints. ✅\\n+- Add `visor test --validate` to check the tests file only (reuses runtime validation). ✅\\n+- Success criteria: common typos produce actionable errors (path + suggestion). ✅\\n+\\n+Usage:\\n+\\n+```\\n+visor test --validate --config defaults/.visor.tests.yaml\\n+```\\n+\\n+Example error output:\\n+\\n+```\\n+❌ Tests file has 2 error(s):\\n+   • tests.cases[0].expext: must NOT have additional properties (Did you mean \\\"expect\\\"?)\\n+   • tests.cases[3].event: must be equal to one of the allowed values (allowed: manual, pr_opened, pr_updated, pr_closed, issue_opened, issue_comment)\\n+```\\n+\\n+Milestone 9 — Coverage and perf (0.5 week) — DONE 2025-10-27\\n+- Per-case coverage table printed after assertions: each expected step shows desired count (e.g., =1/≥1/≤N), actual runs, and status (ok/under/over). ✅\\n+- Parallel case execution: `--max-parallel <N>` or `tests.defaults.max_parallel` enables a simple pool runner. ✅\\n+- Prompt capture throttle: `--prompt-max-chars <N>` or `tests.defaults.prompt_max_chars` truncates stored prompt text to reduce memory. ✅\\n+- Success criteria: coverage table visible; options validated locally. ✅\\n+\\n+Usage examples:\\n+\\n+```\\n+visor test --config defaults/.visor.tests.yaml --max-parallel 4\\n+visor test --config defaults/.visor.tests.yaml --prompt-max-chars 16000\\n+```\\n+\\n+Milestone 10 — Docs, examples, and migration (0.5 week) — IN PROGRESS 2025-10-27\\n+- Update README to link the RFC and defaults/.visor.tests.yaml.\\n+- Document built-in fixtures catalog and examples.\\n+- Migration note: how to move from embedded tests and from `returns` to new mocks.\\n+- Success criteria: docs reviewed; examples copy‑paste clean.\\n+\\n+Risks & Mitigations\\n+- Prompt capture bloat → truncate by default; add --verbose.\\n+- Fixture drift vs engine → keep fixtures minimal and aligned to CheckExecutionEngine needs; add contract tests.\\n+- Strict mode false positives → provide clear errors and fast “add expect” guidance.\\n+\\n+Success Metrics\\n+- 100% of default cases pass locally and in CI.\\n+- Sub‑second overhead per case on small fixtures; <10s for the full default suite.\\n+- Clear failures with a single screen of output; <1 minute to fix typical assertion mismatches.\\n+\\n+## Compatibility & Migration\\n+\\n+- Tests moved from `defaults/.visor.yaml` into `defaults/.visor.tests.yaml` with `extends: \\\".visor.yaml\\\"`.\\n+- Old `mocks.*.returns` is replaced by direct values (object/array/string).\\n+- You no longer need `run: steps` in tests; cases are integration‑driven by `event + fixture`.\\n+- `no_other_calls` is unnecessary with strict mode; it’s implied and enforced.\\n+\\n+## Open Questions\\n+\\n+- Should we support HTTP provider mocks out of the box (URL/method/body → recorded responses)?\\n+- Do we want a JSONPath for `expect.outputs.path`, or keep the current dot/bracket selector?\\n+- Snapshots of generated Markdown? Perhaps as optional golden files with normalization.\\n+\\n+## Future Work\\n+\\n+- Watch mode (`--watch`) and focused runs by regex.\\n+- Coverage‑like reports for step execution and assertions.\\n+- Built‑in fixtures for common GitHub events (shortcuts).\\n+- Golden snapshot helpers for comments and label sets (with stable normalization).\\n+- Parallelizing cases and/or flows.\\n+\\n+## Appendix: Example Suite\\n+\\n+See `defaults/.visor.tests.yaml` in the repo for a complete, multi‑event example covering:\\n+- PR opened → overview + labels\\n+- Standard PR comment → no action\\n+- `/visor` comment → reply\\n+- `/visor ... Regenerate reviews` → retrigger overview\\n+- Fact validation enabled/disabled on comment\\n+- New commit pushed to PR → refresh overview\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/assertions.md\",\"additions\":3,\"deletions\":0,\"changes\":85,\"patch\":\"diff --git a/docs/testing/assertions.md b/docs/testing/assertions.md\\nnew file mode 100644\\nindex 00000000..e5f62aca\\n--- /dev/null\\n+++ b/docs/testing/assertions.md\\n@@ -0,0 +1,85 @@\\n+# Writing Assertions\\n+\\n+Assertions live under `expect:` and cover three surfaces:\\n+\\n+- `calls`: step counts and provider effects (GitHub ops)\\n+- `prompts`: final AI prompts (post templating/context)\\n+- `outputs`: step outputs with history and selectors\\n+\\n+## Calls\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: overview\\n+      exactly: 1\\n+    - provider: github\\n+      op: labels.add\\n+      at_least: 1\\n+      args:\\n+        contains: [feature, \\\"review/effort:2\\\"]\\n+```\\n+\\n+Counts are consistent everywhere: `exactly`, `at_least`, `at_most`.\\n+\\n+## Prompts\\n+\\n+```yaml\\n+expect:\\n+  prompts:\\n+    - step: overview\\n+      contains: [\\\"feat: add user search\\\", \\\"diff --git a/src/search.ts\\\"]\\n+    - step: comment-assistant\\n+      matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"\\n+    - step: overview\\n+      # Select the prompt that mentions a specific file\\n+      where:\\n+        contains: [\\\"src/search.ts\\\"]\\n+      contains: [\\\"diff --git a/src/search.ts\\\"]\\n+```\\n+\\n+- `contains`: required substrings\\n+- `not_contains`: forbidden substrings\\n+- `matches`: regex (prefix `(?i)` for case-insensitive)\\n+- `index`: `first` | `last` | N (default: last)\\n+- `where`: selector to choose a prompt from history using `contains`/`not_contains`/`matches` before applying the assertion\\n+\\n+Tip: enable `--prompt-max-chars` or `tests.defaults.prompt_max_chars` to cap stored prompt size for large diffs.\\n+\\n+## Outputs\\n+\\n+Use `path` with dot/bracket syntax. You can select by index or by a `where` probe over the same output history.\\n+\\n+```yaml\\n+expect:\\n+  outputs:\\n+    - step: validate-fact\\n+      index: 0\\n+      path: fact_id\\n+      equals: f1\\n+    - step: validate-fact\\n+      where: { path: fact_id, equals: f2 }\\n+      path: confidence\\n+      equals: high\\n+    - step: aggregate-validations\\n+      path: all_valid\\n+      equals: true\\n+```\\n+\\n+Supported comparators:\\n+- `equals` (primitive)\\n+- `equalsDeep` (structural)\\n+- `matches` (regex)\\n+- `contains_unordered` (array membership ignoring order)\\n+\\n+## Strict mode and “no calls”\\n+\\n+Strict mode (default) fails any executed step without a corresponding `expect.calls` entry. You can also assert absence explicitly:\\n+\\n+```yaml\\n+expect:\\n+  no_calls:\\n+    - provider: github\\n+      op: issues.createComment\\n+    - step: extract-facts\\n+```\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/cli.md\",\"additions\":2,\"deletions\":0,\"changes\":37,\"patch\":\"diff --git a/docs/testing/cli.md b/docs/testing/cli.md\\nnew file mode 100644\\nindex 00000000..3d19fc10\\n--- /dev/null\\n+++ b/docs/testing/cli.md\\n@@ -0,0 +1,37 @@\\n+# Visor Test CLI\\n+\\n+Run integration tests for your Visor config using the built-in `test` subcommand.\\n+\\n+## Commands\\n+\\n+- Discover tests file and list cases\\n+  - `visor test --list [--config defaults/.visor.tests.yaml]`\\n+- Run cases\\n+  - `visor test [--config defaults/.visor.tests.yaml] [--only <substring>] [--bail]`\\n+- Validate tests YAML without running\\n+  - `visor test --validate [--config defaults/.visor.tests.yaml]`\\n+\\n+## Flags\\n+\\n+- `--config <path>`: Path to `.visor.tests.yaml` (auto-discovers `.visor.tests.yaml` or `defaults/.visor.tests.yaml`).\\n+- `--only <filter>`: Run cases whose `name` contains the substring (case-insensitive).\\n+- `--bail`: Stop on first failure.\\n+- `--json <path|->`: Write a minimal JSON summary.\\n+- `--report junit:<path>`: Write a minimal JUnit XML.\\n+- `--summary md:<path>`: Write a minimal Markdown summary.\\n+- `--progress compact|detailed`: Progress verbosity (parsing supported; detailed view evolves over time).\\n+- `--max-parallel <N>`: Run up to N cases concurrently.\\n+- `--prompt-max-chars <N>`: Truncate captured prompt text to N characters.\\n+\\n+## Output\\n+\\n+- Per-case PASS/FAIL lines\\n+- Coverage table (expected vs actual step runs)\\n+- Summary totals\\n+\\n+## Tips\\n+\\n+- Use `--validate` when iterating on tests to catch typos early.\\n+- Keep `strict: true` in `tests.defaults` to surface missing `expect` quickly.\\n+- For large suites, increase `--max-parallel` to improve throughput.\\n+\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/fixtures-and-mocks.md\",\"additions\":3,\"deletions\":0,\"changes\":74,\"patch\":\"diff --git a/docs/testing/fixtures-and-mocks.md b/docs/testing/fixtures-and-mocks.md\\nnew file mode 100644\\nindex 00000000..d49a1f13\\n--- /dev/null\\n+++ b/docs/testing/fixtures-and-mocks.md\\n@@ -0,0 +1,74 @@\\n+# Fixtures and Mocks\\n+\\n+Integration tests simulate outside world inputs and provider outputs.\\n+\\n+## Built-in GitHub fixtures (gh.*)\\n+\\n+Use via `fixture: gh.<name>` or `fixture: { builtin: gh.<name>, overrides: {...} }`.\\n+\\n+- `gh.pr_open.minimal` — pull_request opened with a tiny diff and `src/search.ts` file.\\n+- `gh.pr_sync.minimal` — pull_request synchronize with a small follow-up diff.\\n+- `gh.pr_closed.minimal` — pull_request closed event.\\n+- `gh.issue_open.minimal` — issues opened (short title/body).\\n+- `gh.issue_comment.standard` — normal human comment on a PR/issue.\\n+- `gh.issue_comment.visor_help` — comment containing `/visor help`.\\n+- `gh.issue_comment.visor_regenerate` — `/visor Regenerate reviews`.\\n+\\n+Overrides allow tailored inputs:\\n+\\n+```yaml\\n+fixture:\\n+  builtin: gh.pr_open.minimal\\n+  overrides:\\n+    pr.title: \\\"feat: custom title\\\"\\n+    webhook.payload.pull_request.number: 42\\n+```\\n+\\n+## GitHub recorder\\n+\\n+The test runner injects a recording Octokit by default:\\n+\\n+- Captures every GitHub op+args for assertions (`expect.calls` with `provider: github`).\\n+- Returns stable stub shapes so flows can continue without network.\\n+- Negative modes are available globally via `tests.defaults.github_recorder`:\\n+\\n+```yaml\\n+tests:\\n+  defaults:\\n+    github_recorder:\\n+      error_code: 429      # simulate API error\\n+      timeout_ms: 1000     # simulate request timeout\\n+```\\n+\\n+## Mocks\\n+\\n+Mocks are keyed by step name under `mocks`.\\n+\\n+Examples:\\n+\\n+```yaml\\n+mocks:\\n+  # AI with structured schema\\n+  overview:\\n+    text: \\\"High-level PR summary.\\\"\\n+    tags: { label: feature, review-effort: 2 }\\n+\\n+  # AI plain text schema\\n+  comment-assistant:\\n+    text: \\\"Sure, here’s how I can help.\\\"\\n+    intent: comment_reply\\n+\\n+  # Array outputs (e.g., extract-facts)\\n+  extract-facts:\\n+    - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+\\n+  # Command provider\\n+  unit-tests:\\n+    stdout: '{\\\"passed\\\": 128, \\\"failed\\\": 0}'\\n+    exit_code: 0\\n+```\\n+\\n+Notes:\\n+- No `returns:` key; provide values directly.\\n+- For HTTP/Command providers, mocks bypass real execution and are recorded for assertions.\\n+\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/getting-started.md\",\"additions\":4,\"deletions\":0,\"changes\":88,\"patch\":\"diff --git a/docs/testing/getting-started.md b/docs/testing/getting-started.md\\nnew file mode 100644\\nindex 00000000..c55996ef\\n--- /dev/null\\n+++ b/docs/testing/getting-started.md\\n@@ -0,0 +1,88 @@\\n+# Visor Tests — Getting Started\\n+\\n+This is the developer-facing guide for writing and running integration tests for your Visor config. It focuses on great DX: minimal setup, helpful errors, and clear output.\\n+\\n+## TL;DR\\n+\\n+- Put your tests in `defaults/.visor.tests.yaml`.\\n+- Reference your base config with `extends: \\\".visor.yaml\\\"`.\\n+- Use built-in GitHub fixtures like `gh.pr_open.minimal`.\\n+- Run with `visor test --config defaults/.visor.tests.yaml`.\\n+- Validate only with `visor test --validate`.\\n+\\n+```yaml\\n+version: \\\"1.0\\\"\\n+extends: \\\".visor.yaml\\\"\\n+\\n+tests:\\n+  defaults:\\n+    strict: true           # every executed step must be asserted\\n+    ai_provider: mock      # offline by default\\n+  cases:\\n+    - name: label-flow\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview body\\\"\\n+          tags: { label: feature, review-effort: 2 }\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args: { contains: [feature] }\\n+```\\n+\\n+## Why integration tests in YAML?\\n+\\n+- You test the same thing you ship: events → checks → outputs → effects.\\n+- No network required; GitHub calls are recorded, AI is mocked.\\n+- Flows let you simulate real user journeys across multiple events.\\n+\\n+## Strict by default\\n+\\n+If a step runs and you didn’t assert it under `expect.calls`, the case fails. This prevents silent regressions and “accidental” work.\\n+\\n+Turn off per-case via `strict: false` if you need to iterate.\\n+\\n+## CLI recipes\\n+\\n+- List cases: `visor test --list`\\n+- Run a subset: `visor test --only pr-review`\\n+- Stop on first failure: `--bail`\\n+- Validate tests file only: `--validate`\\n+- Parallelize cases: `--max-parallel 4`\\n+- Throttle prompt capture: `--prompt-max-chars 16000`\\n+\\n+## Coverage output\\n+\\n+After each case/stage, a compact table shows expected vs actual step calls:\\n+\\n+```\\n+Coverage (label-flow):\\n+  • overview                 want =1     got  1  ok\\n+  • apply-overview-labels    want =1     got  1  ok\\n+```\\n+\\n+Unexpected executed steps are listed under `unexpected:` to help you add missing assertions quickly.\\n+\\n+## Helpful validation errors\\n+\\n+Run `visor test --validate` to get precise YAML-path errors and suggestions:\\n+\\n+```\\n+❌ Tests file has 2 error(s):\\n+   • tests.cases[0].expext: must NOT have additional properties (Did you mean \\\"expect\\\"?)\\n+   • tests.cases[3].event: must be equal to one of the allowed values (allowed: manual, pr_opened, pr_updated, pr_closed, issue_opened, issue_comment)\\n+```\\n+\\n+Next steps:\\n+- See `docs/testing/fixtures-and-mocks.md` to simulate inputs.\\n+- See `docs/testing/assertions.md` to write robust checks.\\n+- Browse `defaults/.visor.tests.yaml` for full examples.\\n+\\n\",\"status\":\"added\"},{\"filename\":\"output/assistant-json/template.liquid\",\"additions\":0,\"deletions\":0,\"changes\":0,\"patch\":\"diff --git a/output/assistant-json/template.liquid b/output/assistant-json/template.liquid\\ndeleted file mode 100644\\nindex e69de29b..00000000\\n\",\"status\":\"modified\"},{\"filename\":\"src/ai-review-service.ts\",\"additions\":2,\"deletions\":1,\"changes\":62,\"patch\":\"diff --git a/src/ai-review-service.ts b/src/ai-review-service.ts\\nindex dac5b6e1..8adf6106 100644\\n--- a/src/ai-review-service.ts\\n+++ b/src/ai-review-service.ts\\n@@ -117,30 +117,36 @@ export class AIReviewService {\\n \\n     this.sessionRegistry = SessionRegistry.getInstance();\\n \\n-    // Auto-detect provider and API key from environment\\n-    if (!this.config.apiKey) {\\n-      if (process.env.CLAUDE_CODE_API_KEY) {\\n-        this.config.apiKey = process.env.CLAUDE_CODE_API_KEY;\\n-        this.config.provider = 'claude-code';\\n-      } else if (process.env.GOOGLE_API_KEY) {\\n-        this.config.apiKey = process.env.GOOGLE_API_KEY;\\n-        this.config.provider = 'google';\\n-      } else if (process.env.ANTHROPIC_API_KEY) {\\n-        this.config.apiKey = process.env.ANTHROPIC_API_KEY;\\n-        this.config.provider = 'anthropic';\\n-      } else if (process.env.OPENAI_API_KEY) {\\n-        this.config.apiKey = process.env.OPENAI_API_KEY;\\n-        this.config.provider = 'openai';\\n-      } else if (\\n-        // Check for AWS Bedrock credentials\\n-        (process.env.AWS_ACCESS_KEY_ID && process.env.AWS_SECRET_ACCESS_KEY) ||\\n-        process.env.AWS_BEDROCK_API_KEY\\n-      ) {\\n-        // For Bedrock, we don't set apiKey as it uses AWS credentials\\n-        // ProbeAgent will handle the authentication internally\\n-        this.config.provider = 'bedrock';\\n-        // Set a placeholder to pass validation\\n-        this.config.apiKey = 'AWS_CREDENTIALS';\\n+    // Respect explicit provider if set (e.g., 'mock' during tests) — do not override from env\\n+    const providerExplicit =\\n+      typeof this.config.provider === 'string' && this.config.provider.length > 0;\\n+\\n+    // Auto-detect provider and API key from environment only when provider not explicitly set\\n+    if (!providerExplicit) {\\n+      if (!this.config.apiKey) {\\n+        if (process.env.CLAUDE_CODE_API_KEY) {\\n+          this.config.apiKey = process.env.CLAUDE_CODE_API_KEY;\\n+          this.config.provider = 'claude-code';\\n+        } else if (process.env.GOOGLE_API_KEY) {\\n+          this.config.apiKey = process.env.GOOGLE_API_KEY;\\n+          this.config.provider = 'google';\\n+        } else if (process.env.ANTHROPIC_API_KEY) {\\n+          this.config.apiKey = process.env.ANTHROPIC_API_KEY;\\n+          this.config.provider = 'anthropic';\\n+        } else if (process.env.OPENAI_API_KEY) {\\n+          this.config.apiKey = process.env.OPENAI_API_KEY;\\n+          this.config.provider = 'openai';\\n+        } else if (\\n+          // Check for AWS Bedrock credentials\\n+          (process.env.AWS_ACCESS_KEY_ID && process.env.AWS_SECRET_ACCESS_KEY) ||\\n+          process.env.AWS_BEDROCK_API_KEY\\n+        ) {\\n+          // For Bedrock, we don't set apiKey as it uses AWS credentials\\n+          // ProbeAgent will handle the authentication internally\\n+          this.config.provider = 'bedrock';\\n+          // Set a placeholder to pass validation\\n+          this.config.apiKey = 'AWS_CREDENTIALS';\\n+        }\\n       }\\n     }\\n \\n@@ -766,6 +772,14 @@ ${this.escapeXml(prInfo.body)}\\n     <files_changed_count>${prInfo.files.length}</files_changed_count>\\n   </metadata>`;\\n \\n+    // Include a small raw diff header snippet for compatibility with tools/tests\\n+    try {\\n+      const firstFile = (prInfo.files || [])[0];\\n+      if (firstFile && firstFile.filename) {\\n+        context += `\\\\n  <raw_diff_header>\\\\n${this.escapeXml(`diff --git a/${firstFile.filename} b/${firstFile.filename}`)}\\\\n  </raw_diff_header>`;\\n+      }\\n+    } catch {}\\n+\\n     // Add PR description if available\\n     if (prInfo.body) {\\n       context += `\\n\",\"status\":\"modified\"},{\"filename\":\"src/check-execution-engine.ts\",\"additions\":21,\"deletions\":3,\"changes\":693,\"patch\":\"diff --git a/src/check-execution-engine.ts b/src/check-execution-engine.ts\\nindex 578a42dc..e9a9d733 100644\\n--- a/src/check-execution-engine.ts\\n+++ b/src/check-execution-engine.ts\\n@@ -174,6 +174,9 @@ export class CheckExecutionEngine {\\n   private onFinishLoopCounts: Map<string, number> = new Map();\\n   // Track how many times a forEach parent check has produced an array during this run (\\\"waves\\\")\\n   private forEachWaveCounts: Map<string, number> = new Map();\\n+  // One-shot guards for post on_finish scheduling to avoid duplicate replies when\\n+  // multiple signals (aggregator, memory, history) agree. Keyed by session + parent check.\\n+  private postOnFinishGuards: Set<string> = new Set();\\n   // Snapshot+Scope journal (Phase 0: commit only, no behavior changes yet)\\n   private journal: ExecutionJournal = new ExecutionJournal();\\n   private sessionId: string = `sess-${Date.now().toString(36)}-${Math.random()\\n@@ -208,7 +211,12 @@ export class CheckExecutionEngine {\\n     // Create a mock Octokit instance for local analysis\\n     // This allows us to reuse the existing PRReviewer logic without network calls\\n     this.mockOctokit = this.createMockOctokit();\\n-    this.reviewer = new PRReviewer(this.mockOctokit as unknown as import('@octokit/rest').Octokit);\\n+    // Prefer the provided authenticated/recording Octokit (from test runner or Actions)\\n+    // so that comment create/update operations are visible to recorders and assertions.\\n+    const reviewerOctokit =\\n+      (octokit as unknown as import('@octokit/rest').Octokit) ||\\n+      (this.mockOctokit as unknown as import('@octokit/rest').Octokit);\\n+    this.reviewer = new PRReviewer(reviewerOctokit);\\n   }\\n \\n   private sessionUUID(): string {\\n@@ -298,8 +306,9 @@ export class CheckExecutionEngine {\\n    */\\n   private enrichEventContext(eventContext?: Record<string, unknown>): Record<string, unknown> {\\n     const baseContext = eventContext || {};\\n-    if (this.actionContext?.octokit) {\\n-      return { ...baseContext, octokit: this.actionContext.octokit };\\n+    const injected = this.actionContext?.octokit || (baseContext as any).octokit;\\n+    if (injected) {\\n+      return { ...baseContext, octokit: injected };\\n     }\\n     return baseContext;\\n   }\\n@@ -778,6 +787,11 @@ export class CheckExecutionEngine {\\n       eventOverride,\\n       overlay,\\n     } = opts;\\n+    try {\\n+      if (debug && opts.origin === 'on_finish') {\\n+        console.error(`[runNamedCheck] origin=on_finish step=${target}`);\\n+      }\\n+    } catch {}\\n \\n     // Build context overlay from current results; prefer snapshot visibility for scope (Phase 4)\\n     const depOverlay = overlay ? new Map(overlay) : new Map(resultsMap);\\n@@ -839,6 +853,9 @@ export class CheckExecutionEngine {\\n     debug: boolean\\n   ): Promise<void> {\\n     const log = (msg: string) => (config?.output?.pr_comment ? console.error : console.log)(msg);\\n+    try {\\n+      if (debug) console.error('[on_finish] handler invoked');\\n+    } catch {}\\n \\n     // Find all checks with forEach: true and on_finish configured\\n     const forEachChecksWithOnFinish: Array<{\\n@@ -857,6 +874,11 @@ export class CheckExecutionEngine {\\n       }\\n     }\\n \\n+    try {\\n+      logger.info(\\n+        `🧭 on_finish: discovered ${forEachChecksWithOnFinish.length} forEach parent(s) with hooks`\\n+      );\\n+    } catch {}\\n     if (forEachChecksWithOnFinish.length === 0) {\\n       return; // No on_finish hooks to process\\n     }\\n@@ -870,14 +892,18 @@ export class CheckExecutionEngine {\\n       try {\\n         const forEachResult = results.get(checkName) as ExtendedReviewSummary | undefined;\\n         if (!forEachResult) {\\n-          if (debug) log(`⚠️ No result found for forEach check \\\"${checkName}\\\", skipping on_finish`);\\n+          try {\\n+            logger.info(`⏭ on_finish: no result found for \\\"${checkName}\\\" — skip`);\\n+          } catch {}\\n           continue;\\n         }\\n \\n         // Skip if the forEach check returned empty array\\n         const forEachItems = forEachResult.forEachItems || [];\\n         if (forEachItems.length === 0) {\\n-          if (debug) log(`⏭  Skipping on_finish for \\\"${checkName}\\\" - forEach returned 0 items`);\\n+          try {\\n+            logger.info(`⏭ on_finish: \\\"${checkName}\\\" produced 0 items — skip`);\\n+          } catch {}\\n           continue;\\n         }\\n \\n@@ -885,15 +911,19 @@ export class CheckExecutionEngine {\\n         const node = dependencyGraph.nodes.get(checkName);\\n         const dependents = node?.dependents || [];\\n \\n-        if (debug) {\\n-          log(`🔍 on_finish for \\\"${checkName}\\\": ${dependents.length} dependent(s)`);\\n-        }\\n+        try {\\n+          logger.info(`🔍 on_finish: \\\"${checkName}\\\" → ${dependents.length} dependent(s)`);\\n+        } catch {}\\n \\n-        // Verify all dependents have completed\\n+        // Verify all dependents have completed. If not, proceed anyway at the end of the run\\n+        // because we are in a post-phase hook and no more work will arrive in this cycle.\\n         const allDependentsCompleted = dependents.every(dep => results.has(dep));\\n         if (!allDependentsCompleted) {\\n-          if (debug) log(`⚠️ Not all dependents of \\\"${checkName}\\\" completed, skipping on_finish`);\\n-          continue;\\n+          try {\\n+            logger.warn(\\n+              `⚠️ on_finish: some dependents of \\\"${checkName}\\\" have no results; proceeding with on_finish anyway`\\n+            );\\n+          } catch {}\\n         }\\n \\n         logger.info(`▶ on_finish: processing for \\\"${checkName}\\\"`);\\n@@ -1019,30 +1049,218 @@ export class CheckExecutionEngine {\\n \\n         let lastRunOutput: unknown = undefined;\\n \\n-        // Execute on_finish.run (static + dynamic via run_js) sequentially\\n+        // Execute on_finish.run (static) first, then evaluate run_js with updated context\\n         {\\n           const maxLoops = config?.routing?.max_loops ?? 10;\\n           let loopCount = 0;\\n+          const runList = Array.from(new Set([...(onFinish.run || [])].filter(Boolean)));\\n+          if (runList.length > 0) {\\n+            logger.info(`▶ on_finish.run: executing [${runList.join(', ')}] for \\\"${checkName}\\\"`);\\n+          }\\n+\\n+          try {\\n+            for (const runCheckId of runList) {\\n+              if (++loopCount > maxLoops) {\\n+                throw new Error(\\n+                  `Routing loop budget exceeded (max_loops=${maxLoops}) during on_finish run`\\n+                );\\n+              }\\n+              if (debug) log(`🔧 Debug: on_finish.run executing check '${runCheckId}'`);\\n+              logger.info(`  ▶ Executing on_finish check: ${runCheckId}`);\\n+\\n+              // Execute the step with full routing semantics so its own on_success/on_fail are honored\\n+              const childCfgFull = (config?.checks || {})[runCheckId] as\\n+                | import('./types/config').CheckConfig\\n+                | undefined;\\n+              if (!childCfgFull) throw new Error(`Unknown check in on_finish.run: ${runCheckId}`);\\n+              const childProvType = childCfgFull.type || 'ai';\\n+              const childProvider = this.providerRegistry.getProviderOrThrow(childProvType);\\n+              this.setProviderWebhookContext(childProvider);\\n+              // Note: unified scheduling executes via runNamedCheck; provider config built internally\\n+              const depOverlayForChild = new Map(results);\\n+              // Use unified scheduling helper so execution statistics and history are recorded\\n+              const __onFinishRes = await this.runNamedCheck(runCheckId, [], {\\n+                origin: 'on_finish',\\n+                config: config!,\\n+                dependencyGraph,\\n+                prInfo,\\n+                resultsMap: results,\\n+                debug,\\n+                overlay: depOverlayForChild,\\n+              });\\n+              try {\\n+                lastRunOutput = (__onFinishRes as any)?.output;\\n+              } catch {}\\n+              logger.info(`  ✓ Completed on_finish check: ${runCheckId}`);\\n+\\n+              // If the executed on_finish step defines its own on_success, honor its run list here\\n+              let scheduledByChildOnSuccess = false;\\n+              try {\\n+                const childCfg = (config?.checks || {})[runCheckId] as\\n+                  | import('./types/config').CheckConfig\\n+                  | undefined;\\n+                const childOnSuccess = childCfg?.on_success;\\n+                if (childOnSuccess) {\\n+                  try {\\n+                    logger.info(\\n+                      `  ↪ on_finish.run: '${runCheckId}' defines on_success; evaluating run_js`\\n+                    );\\n+                  } catch {}\\n+                  // Evaluate child run_js with access to 'output' of the just executed step\\n+                  const evalChildRunJs = async (js?: string): Promise<string[]> => {\\n+                    if (!js) return [];\\n+                    try {\\n+                      const sandbox = this.getRoutingSandbox();\\n+                      const scope = { ...onFinishContext, output: lastRunOutput } as any;\\n+                      const code = `\\n+                        const step = scope.step; const attempt = scope.attempt; const loop = scope.loop; const outputs = scope.outputs; const outputs_history = scope.outputs_history; const outputs_raw = scope.outputs_raw; const forEach = scope.forEach; const memory = scope.memory; const pr = scope.pr; const files = scope.files; const env = scope.env; const event = scope.event; const output = scope.output; const log = (...a)=> console.log('🔍 Debug:',...a);\\n+                        const __fn = () => {\\\\n${js}\\\\n};\\n+                        const __res = __fn();\\n+                        return Array.isArray(__res) ? __res.filter(x => typeof x === 'string' && x) : [];\\n+                      `;\\n+                      const exec = sandbox.compile(code);\\n+                      const res = exec({ scope }).run();\\n+                      return Array.isArray(res) ? (res as string[]) : [];\\n+                    } catch (e) {\\n+                      const msg = e instanceof Error ? e.message : String(e);\\n+                      logger.error(\\n+                        `✗ on_finish.run → child on_success.run_js failed for \\\"${runCheckId}\\\": ${msg}`\\n+                      );\\n+                      return [];\\n+                    }\\n+                  };\\n+                  const childDynamicRun = await evalChildRunJs(childOnSuccess.run_js);\\n+                  const childRunList = Array.from(\\n+                    new Set([...(childOnSuccess.run || []), ...childDynamicRun].filter(Boolean))\\n+                  );\\n+                  if (childRunList.length > 0) {\\n+                    scheduledByChildOnSuccess = true;\\n+                    logger.info(\\n+                      `  ▶ on_finish.run → scheduling child on_success [${childRunList.join(', ')}] after '${runCheckId}'`\\n+                    );\\n+                  } else {\\n+                    try {\\n+                      logger.info(\\n+                        `  ↪ on_finish.run: child on_success produced empty run list for '${runCheckId}'`\\n+                      );\\n+                    } catch {}\\n+                  }\\n+                  for (const stepId of childRunList) {\\n+                    if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n+                    const childStart = this.recordIterationStart(stepId);\\n+                    const childRes = await this.runNamedCheck(stepId, [], {\\n+                      origin: 'on_finish',\\n+                      config,\\n+                      dependencyGraph,\\n+                      prInfo,\\n+                      resultsMap: results,\\n+                      sessionInfo: undefined,\\n+                      debug,\\n+                      overlay: new Map(results),\\n+                    });\\n+                    const childIssues = (childRes.issues || []).map(i => ({ ...i }));\\n+                    const childSuccess = !this.hasFatal(childIssues);\\n+                    const childOut = (childRes as any)?.output;\\n+                    this.recordIterationComplete(\\n+                      stepId,\\n+                      childStart,\\n+                      childSuccess,\\n+                      childIssues,\\n+                      childOut\\n+                    );\\n+                    try {\\n+                      if (childOut !== undefined) this.trackOutputHistory(stepId, childOut);\\n+                    } catch {}\\n+                  }\\n+                }\\n+              } catch {}\\n \\n-          // Helper to evaluate run_js to string[] safely\\n+              // Fallback: if child on_success was not present or produced no run list,\\n+              // schedule a correction reply when validation issues are present in memory.\\n+              try {\\n+                const issues = memoryHelpers.get('fact_validation_issues', 'fact-validation') as\\n+                  | unknown[]\\n+                  | undefined;\\n+                if (!scheduledByChildOnSuccess && Array.isArray(issues) && issues.length > 0) {\\n+                  const stepId = 'comment-assistant';\\n+                  const guardKey = `${this.sessionUUID()}:${checkName}:post_correction:${\\n+                    prInfo.number || 'local'\\n+                  }`;\\n+                  if (this.postOnFinishGuards.has(guardKey)) {\\n+                    logger.info(\\n+                      `  ↪ on_finish.run: correction already scheduled (guard hit), skipping '${stepId}'`\\n+                    );\\n+                  } else {\\n+                    logger.info(\\n+                      `  ▶ on_finish.run → fallback scheduling '${stepId}' due to validation issues (${issues.length})`\\n+                    );\\n+                    this.postOnFinishGuards.add(guardKey);\\n+                    const childCfg = (config?.checks || {})[stepId] as\\n+                      | import('./types/config').CheckConfig\\n+                      | undefined;\\n+                    if (childCfg) {\\n+                      const provType = childCfg.type || 'ai';\\n+                      const provider = this.providerRegistry.getProviderOrThrow(provType);\\n+                      this.setProviderWebhookContext(provider);\\n+                      const provCfg: import('./providers/check-provider.interface').CheckProviderConfig =\\n+                        {\\n+                          type: provType,\\n+                          prompt: childCfg.prompt,\\n+                          exec: childCfg.exec,\\n+                          focus: childCfg.focus || this.mapCheckNameToFocus(stepId),\\n+                          schema: childCfg.schema,\\n+                          group: childCfg.group,\\n+                          checkName: stepId,\\n+                          eventContext: this.enrichEventContext(prInfo.eventContext),\\n+                          transform: childCfg.transform,\\n+                          transform_js: childCfg.transform_js,\\n+                          timeout: childCfg.timeout,\\n+                          env: childCfg.env,\\n+                          forEach: childCfg.forEach,\\n+                          __outputHistory: this.outputHistory,\\n+                          ...childCfg,\\n+                          ai: { ...(childCfg.ai || {}), timeout: undefined, debug },\\n+                        } as any;\\n+                      await this.executeWithRouting(\\n+                        stepId,\\n+                        childCfg,\\n+                        provider,\\n+                        provCfg,\\n+                        prInfo,\\n+                        new Map(results),\\n+                        undefined,\\n+                        config!,\\n+                        dependencyGraph,\\n+                        debug,\\n+                        results\\n+                      );\\n+                    }\\n+                  }\\n+                }\\n+              } catch {}\\n+            }\\n+            if (runList.length > 0) logger.info(`✓ on_finish.run: completed for \\\"${checkName}\\\"`);\\n+          } catch (error) {\\n+            const errorMsg = error instanceof Error ? error.message : String(error);\\n+            logger.error(`✗ on_finish.run: failed for \\\"${checkName}\\\": ${errorMsg}`);\\n+            if (error instanceof Error && error.stack) {\\n+              logger.debug(`Stack trace: ${error.stack}`);\\n+            }\\n+            throw error;\\n+          }\\n+\\n+          // Now evaluate dynamic run_js with post-run context (e.g., after aggregation updated memory)\\n           const evalRunJs = async (js?: string): Promise<string[]> => {\\n             if (!js) return [];\\n             try {\\n               const sandbox = this.getRoutingSandbox();\\n-              const scope = onFinishContext;\\n+              const scope = onFinishContext; // contains memory + outputs history\\n               const code = `\\n                 const step = scope.step; const attempt = scope.attempt; const loop = scope.loop; const outputs = scope.outputs; const outputs_history = scope.outputs_history; const outputs_raw = scope.outputs_raw; const forEach = scope.forEach; const memory = scope.memory; const pr = scope.pr; const files = scope.files; const env = scope.env; const event = scope.event; const log = (...a)=> console.log('🔍 Debug:',...a);\\n                 const __fn = () => {\\\\n${js}\\\\n};\\n                 const __res = __fn();\\n                 return Array.isArray(__res) ? __res.filter(x => typeof x === 'string' && x) : [];\\n               `;\\n-              try {\\n-                if (code.includes('process')) {\\n-                  logger.warn('⚠️ on_finish.goto_js prelude contains \\\"process\\\" token');\\n-                } else {\\n-                  logger.info('🔧 on_finish.goto_js prelude is clean (no process token)');\\n-                }\\n-              } catch {}\\n               const exec = sandbox.compile(code);\\n               const res = exec({ scope }).run();\\n               return Array.isArray(res) ? (res as string[]) : [];\\n@@ -1053,52 +1271,53 @@ export class CheckExecutionEngine {\\n               return [];\\n             }\\n           };\\n-\\n-          const dynamicRun = await evalRunJs(onFinish.run_js);\\n-          const runList = Array.from(\\n-            new Set([...(onFinish.run || []), ...dynamicRun].filter(Boolean))\\n-          );\\n-\\n-          if (runList.length > 0) {\\n-            logger.info(`▶ on_finish.run: executing [${runList.join(', ')}] for \\\"${checkName}\\\"`);\\n-          }\\n-\\n           try {\\n-            for (const runCheckId of runList) {\\n+            if (process.env.VISOR_DEBUG === 'true' || debug) {\\n+              const memDbg = MemoryStore.getInstance(this.config?.memory);\\n+              const keys = memDbg.list('fact-validation');\\n+              logger.info(\\n+                `on_finish.run_js context (keys in fact-validation) = [${keys.join(', ')}]`\\n+              );\\n+            }\\n+          } catch {}\\n+          const dynamicRun = await evalRunJs(onFinish.run_js);\\n+          const dynList = Array.from(new Set(dynamicRun.filter(Boolean)));\\n+          if (dynList.length > 0) {\\n+            logger.info(\\n+              `▶ on_finish.run_js: executing [${dynList.join(', ')}] for \\\"${checkName}\\\"`\\n+            );\\n+            for (const runCheckId of dynList) {\\n               if (++loopCount > maxLoops) {\\n                 throw new Error(\\n-                  `Routing loop budget exceeded (max_loops=${maxLoops}) during on_finish run`\\n+                  `Routing loop budget exceeded (max_loops=${maxLoops}) during on_finish run_js`\\n                 );\\n               }\\n-              if (debug) log(`🔧 Debug: on_finish.run executing check '${runCheckId}'`);\\n-              logger.info(`  ▶ Executing on_finish check: ${runCheckId}`);\\n-\\n-              const __onFinishRes = await this.runNamedCheck(runCheckId, [], {\\n+              logger.info(`  ▶ Executing on_finish(run_js) check: ${runCheckId}`);\\n+              // Use full routing semantics for dynamic children as well\\n+              const childCfgFull = (config?.checks || {})[runCheckId] as\\n+                | import('./types/config').CheckConfig\\n+                | undefined;\\n+              if (!childCfgFull)\\n+                throw new Error(`Unknown check in on_finish.run_js: ${runCheckId}`);\\n+              const childProvType = childCfgFull.type || 'ai';\\n+              const childProvider = this.providerRegistry.getProviderOrThrow(childProvType);\\n+              this.setProviderWebhookContext(childProvider);\\n+              // Note: unified scheduling executes via runNamedCheck; provider config built internally\\n+              const depOverlayForChild = new Map(results);\\n+              const childRes = await this.runNamedCheck(runCheckId, [], {\\n                 origin: 'on_finish',\\n-                config,\\n+                config: config!,\\n                 dependencyGraph,\\n                 prInfo,\\n                 resultsMap: results,\\n-                sessionInfo: undefined,\\n                 debug,\\n-                eventOverride: onFinish.goto_event,\\n-                overlay: new Map(results),\\n+                overlay: depOverlayForChild,\\n               });\\n               try {\\n-                lastRunOutput = (__onFinishRes as any)?.output;\\n+                lastRunOutput = (childRes as any)?.output;\\n               } catch {}\\n-              logger.info(`  ✓ Completed on_finish check: ${runCheckId}`);\\n-            }\\n-            if (runList.length > 0) {\\n-              logger.info(`✓ on_finish.run: completed for \\\"${checkName}\\\"`);\\n+              logger.info(`  ✓ Completed on_finish(run_js) check: ${runCheckId}`);\\n             }\\n-          } catch (error) {\\n-            const errorMsg = error instanceof Error ? error.message : String(error);\\n-            logger.error(`✗ on_finish.run: failed for \\\"${checkName}\\\": ${errorMsg}`);\\n-            if (error instanceof Error && error.stack) {\\n-              logger.debug(`Stack trace: ${error.stack}`);\\n-            }\\n-            throw error;\\n           }\\n         }\\n \\n@@ -1322,6 +1541,75 @@ export class CheckExecutionEngine {\\n         }\\n \\n         logger.info(`✓ on_finish: completed for \\\"${checkName}\\\"`);\\n+\\n+        // After completing on_finish handling for this forEach parent, if validation issues are present\\n+        // (from memory or inferred from the latest validate-fact history), schedule a single\\n+        // correction reply via comment-assistant. Use a one-shot guard per session+parent check\\n+        // to prevent duplicates when multiple signals agree (aggregator, memory, inferred history).\\n+        try {\\n+          const mem = MemoryStore.getInstance(this.config?.memory);\\n+          const issues = mem.get('fact_validation_issues', 'fact-validation') as\\n+            | unknown[]\\n+            | undefined;\\n+          // Prefer aggregator output when available\\n+          let allValidOut = false;\\n+          try {\\n+            const lro =\\n+              lastRunOutput && typeof lastRunOutput === 'object'\\n+                ? (lastRunOutput as any)\\n+                : undefined;\\n+            allValidOut = !!(lro && (lro.all_valid === true || lro.allValid === true));\\n+          } catch {}\\n+          // Infer invalids from the latest wave as an additional guard when memory path is absent\\n+          let inferredInvalid = 0;\\n+          try {\\n+            const vfHistNow = (this.outputHistory.get('validate-fact') || []) as unknown[];\\n+            if (Array.isArray(vfHistNow) && forEachItems.length > 0) {\\n+              const lastWave = vfHistNow.slice(-forEachItems.length);\\n+              inferredInvalid = lastWave.filter(\\n+                (v: any) => v && (v.is_valid === false || v.valid === false)\\n+              ).length;\\n+            }\\n+          } catch {}\\n+\\n+          if (\\n+            (!allValidOut && Array.isArray(issues) && issues.length > 0) ||\\n+            (!allValidOut && inferredInvalid > 0)\\n+          ) {\\n+            const stepId = 'comment-assistant';\\n+            const guardKey = `${this.sessionUUID()}:${checkName}:post_correction:${\\n+              prInfo.number || 'local'\\n+            }`;\\n+            if (this.postOnFinishGuards.has(guardKey)) {\\n+              logger.info(\\n+                `↪ on_finish.post: correction already scheduled (guard hit), skipping '${stepId}'`\\n+              );\\n+            } else {\\n+              logger.info(\\n+                `▶ on_finish.post: scheduling '${stepId}' due to validation issues (mem=${Array.isArray(issues) ? issues.length : 0}, inferred=${inferredInvalid})`\\n+              );\\n+              this.postOnFinishGuards.add(guardKey);\\n+              const childCfg = (config?.checks || {})[stepId] as\\n+                | import('./types/config').CheckConfig\\n+                | undefined;\\n+              if (childCfg) {\\n+                const provType = childCfg.type || 'ai';\\n+                const provider = this.providerRegistry.getProviderOrThrow(provType);\\n+                this.setProviderWebhookContext(provider);\\n+                // Provider config constructed inside runNamedCheck; no local build needed here\\n+                await this.runNamedCheck(stepId, [], {\\n+                  origin: 'on_finish',\\n+                  config: config!,\\n+                  dependencyGraph,\\n+                  prInfo,\\n+                  resultsMap: results,\\n+                  debug,\\n+                  overlay: new Map(results),\\n+                });\\n+              }\\n+            }\\n+          }\\n+        } catch {}\\n       } catch (error) {\\n         logger.error(`✗ on_finish: error for \\\"${checkName}\\\": ${error}`);\\n       }\\n@@ -1538,8 +1826,23 @@ export class CheckExecutionEngine {\\n           async () => provider.execute(prInfo, providerConfig, dependencyResults, context)\\n         );\\n         this.recordProviderDuration(checkName, Date.now() - __provStart);\\n+        // Expose a sensible 'output' for routing JS across all providers.\\n+        // Some providers (AI) return { output, issues }, others (memory/command/http) may\\n+        // return the value directly. Prefer explicit `output`, fall back to the whole result.\\n         try {\\n-          currentRouteOutput = (res as any)?.output;\\n+          const anyRes: any = res as any;\\n+          currentRouteOutput =\\n+            anyRes && typeof anyRes === 'object' && 'output' in anyRes ? anyRes.output : anyRes;\\n+          if (\\n+            checkName === 'aggregate-validations' &&\\n+            (process.env.VISOR_DEBUG === 'true' || debug)\\n+          ) {\\n+            try {\\n+              logger.info(\\n+                '[aggregate-validations] route-output = ' + JSON.stringify(currentRouteOutput)\\n+              );\\n+            } catch {}\\n+          }\\n         } catch {}\\n         // Success path\\n         // Treat result issues with severity error/critical as a soft-failure eligible for on_fail routing\\n@@ -1702,6 +2005,18 @@ export class CheckExecutionEngine {\\n           // Compute run list\\n           const dynamicRun = await evalRunJs(onSuccess.run_js);\\n           const runList = [...(onSuccess.run || []), ...dynamicRun].filter(Boolean);\\n+          try {\\n+            if (\\n+              checkName === 'aggregate-validations' &&\\n+              (process.env.VISOR_DEBUG === 'true' || debug)\\n+            ) {\\n+              logger.info(\\n+                `on_success.run (aggregate-validations): dynamicRun=[${dynamicRun.join(', ')}] run=[${(\\n+                  onSuccess.run || []\\n+                ).join(', ')}]`\\n+              );\\n+            }\\n+          } catch {}\\n           if (runList.length > 0) {\\n             try {\\n               require('./logger').logger.info(\\n@@ -1732,7 +2047,10 @@ export class CheckExecutionEngine {\\n               if (!inItem && mode === 'map' && items.length > 0) {\\n                 for (let i = 0; i < items.length; i++) {\\n                   const itemScope: ScopePath = [{ check: checkName, index: i }];\\n-                  await this.runNamedCheck(stepId, itemScope, {\\n+                  // Record stats for scheduled child run\\n+                  if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n+                  const schedStart = this.recordIterationStart(stepId);\\n+                  const childRes = await this.runNamedCheck(stepId, itemScope, {\\n                     config: config!,\\n                     dependencyGraph,\\n                     prInfo,\\n@@ -1740,12 +2058,28 @@ export class CheckExecutionEngine {\\n                     debug: !!debug,\\n                     overlay: dependencyResults,\\n                   });\\n+                  const childIssues = (childRes.issues || []).map(i => ({ ...i }));\\n+                  const childSuccess = !this.hasFatal(childIssues);\\n+                  const childOut = (childRes as any)?.output;\\n+                  this.recordIterationComplete(\\n+                    stepId,\\n+                    schedStart,\\n+                    childSuccess,\\n+                    childIssues,\\n+                    childOut\\n+                  );\\n+                  try {\\n+                    const out = (childRes as any)?.output;\\n+                    if (out !== undefined) this.trackOutputHistory(stepId, out);\\n+                  } catch {}\\n                 }\\n               } else {\\n                 const scopeForRun: ScopePath = foreachContext\\n                   ? [{ check: foreachContext.parent, index: foreachContext.index }]\\n                   : [];\\n-                await this.runNamedCheck(stepId, scopeForRun, {\\n+                if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n+                const schedStart = this.recordIterationStart(stepId);\\n+                const childRes = await this.runNamedCheck(stepId, scopeForRun, {\\n                   config: config!,\\n                   dependencyGraph,\\n                   prInfo,\\n@@ -1753,6 +2087,20 @@ export class CheckExecutionEngine {\\n                   debug: !!debug,\\n                   overlay: dependencyResults,\\n                 });\\n+                const childIssues = (childRes.issues || []).map(i => ({ ...i }));\\n+                const childSuccess = !this.hasFatal(childIssues);\\n+                const childOut = (childRes as any)?.output;\\n+                this.recordIterationComplete(\\n+                  stepId,\\n+                  schedStart,\\n+                  childSuccess,\\n+                  childIssues,\\n+                  childOut\\n+                );\\n+                try {\\n+                  const out = (childRes as any)?.output;\\n+                  if (out !== undefined) this.trackOutputHistory(stepId, out);\\n+                } catch {}\\n               }\\n             }\\n           } else {\\n@@ -1815,6 +2163,26 @@ export class CheckExecutionEngine {\\n                   if (!eventMatches) continue;\\n                   if (dependsOn(name, target)) forwardSet.add(name);\\n                 }\\n+                // Always execute the target itself first (goto target), regardless of event filtering\\n+                // Then, optionally execute its dependents that match the goto_event\\n+                const runTargetOnce = async (scopeForRun: ScopePath) => {\\n+                  // Ensure stats entry exists for the target\\n+                  if (!this.executionStats.has(target)) this.initializeCheckStats(target);\\n+                  const tgtStart = this.recordIterationStart(target);\\n+                  const tgtRes = await this.runNamedCheck(target, scopeForRun, {\\n+                    config: config!,\\n+                    dependencyGraph,\\n+                    prInfo,\\n+                    resultsMap: resultsMap || new Map(),\\n+                    debug: !!debug,\\n+                    eventOverride: onSuccess.goto_event,\\n+                  });\\n+                  const tgtIssues = (tgtRes.issues || []).map(i => ({ ...i }));\\n+                  const tgtSuccess = !this.hasFatal(tgtIssues);\\n+                  const tgtOutput: unknown = (tgtRes as any)?.output;\\n+                  this.recordIterationComplete(target, tgtStart, tgtSuccess, tgtIssues, tgtOutput);\\n+                };\\n+\\n                 // Topologically order forwardSet based on depends_on within this subset\\n                 const order: string[] = [];\\n                 const inSet = (n: string) => forwardSet.has(n);\\n@@ -1841,7 +2209,7 @@ export class CheckExecutionEngine {\\n                   order.push(n);\\n                 };\\n                 for (const n of forwardSet) visit(n);\\n-                // Execute in order with event override, updating statistics per child\\n+                // Execute target (once) and then dependents with event override; update statistics per step\\n                 const tcfg = cfgChecks[target];\\n                 const mode =\\n                   tcfg?.fanout === 'map'\\n@@ -1854,7 +2222,11 @@ export class CheckExecutionEngine {\\n                     ? (currentRouteOutput as unknown[])\\n                     : [];\\n                 const runChainOnce = async (scopeForRun: ScopePath) => {\\n-                  for (const stepId of order) {\\n+                  // Run the goto target itself first\\n+                  await runTargetOnce(scopeForRun);\\n+                  // Exclude the target itself from the dependent execution order to avoid double-run\\n+                  const dependentsOnly = order.filter(n => n !== target);\\n+                  for (const stepId of dependentsOnly) {\\n                     if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n                     const childStart = this.recordIterationStart(stepId);\\n                     const childRes = await this.runNamedCheck(stepId, scopeForRun, {\\n@@ -2057,8 +2429,8 @@ export class CheckExecutionEngine {\\n     config: import('./types/config').VisorConfig | undefined,\\n     tagFilter: import('./types/config').TagFilter | undefined\\n   ): string[] {\\n-    const logFn = this.config?.output?.pr_comment ? console.error : console.log;\\n-\\n+    // When no tag filter is specified, include all checks regardless of tags.\\n+    // Tag filters should only narrow execution when explicitly provided via config.tag_filter or CLI.\\n     return checks.filter(checkName => {\\n       const checkConfig = config?.checks?.[checkName];\\n       if (!checkConfig) {\\n@@ -2068,13 +2440,7 @@ export class CheckExecutionEngine {\\n \\n       const checkTags = checkConfig.tags || [];\\n \\n-      // If check has tags but no tag filter is specified, exclude it\\n-      if (checkTags.length > 0 && (!tagFilter || (!tagFilter.include && !tagFilter.exclude))) {\\n-        logFn(`⏭️ Skipping check '${checkName}' - check has tags but no tag filter specified`);\\n-        return false;\\n-      }\\n-\\n-      // If no tag filter is specified and check has no tags, include it\\n+      // If no tag filter is specified, include all checks\\n       if (!tagFilter || (!tagFilter.include && !tagFilter.exclude)) {\\n         return true;\\n       }\\n@@ -2087,19 +2453,13 @@ export class CheckExecutionEngine {\\n       // Check exclude tags first (if any exclude tag matches, skip the check)\\n       if (tagFilter.exclude && tagFilter.exclude.length > 0) {\\n         const hasExcludedTag = tagFilter.exclude.some(tag => checkTags.includes(tag));\\n-        if (hasExcludedTag) {\\n-          logFn(`⏭️ Skipping check '${checkName}' - has excluded tag`);\\n-          return false;\\n-        }\\n+        if (hasExcludedTag) return false;\\n       }\\n \\n       // Check include tags (if specified, at least one must match)\\n       if (tagFilter.include && tagFilter.include.length > 0) {\\n         const hasIncludedTag = tagFilter.include.some(tag => checkTags.includes(tag));\\n-        if (!hasIncludedTag) {\\n-          logFn(`⏭️ Skipping check '${checkName}' - does not have required tags`);\\n-          return false;\\n-        }\\n+        if (!hasIncludedTag) return false;\\n       }\\n \\n       return true;\\n@@ -2547,6 +2907,12 @@ export class CheckExecutionEngine {\\n \\n     // Use filtered checks for execution\\n     checks = tagFilteredChecks;\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        const ev = (prInfo as any)?.eventType || '(unknown)';\\n+        console.error(`[engine] final checks after filters (event=${ev}): [${checks.join(', ')}]`);\\n+      }\\n+    } catch {}\\n \\n     // Capture GitHub Action context (owner/repo/octokit) if available from environment\\n     // This is used for context elevation when routing via goto_event\\n@@ -2597,7 +2963,7 @@ export class CheckExecutionEngine {\\n           `🔧 Debug: Using grouped dependency-aware execution for ${checks.length} checks (has dependencies: ${hasDependencies}, has routing: ${hasRouting})`\\n         );\\n       }\\n-      return await this.executeGroupedDependencyAwareChecks(\\n+      const execRes = await this.executeGroupedDependencyAwareChecks(\\n         prInfo,\\n         checks,\\n         timeout,\\n@@ -2608,6 +2974,38 @@ export class CheckExecutionEngine {\\n         failFast,\\n         tagFilter\\n       );\\n+\\n+      // Test-mode PR comment posting: when running under the test runner we want to\\n+      // exercise comment creation/update using the injected Octokit (recorder), so that\\n+      // tests can assert on issues.createComment/updates. In normal runs the action/CLI\\n+      // code handles posting; this block is gated by VISOR_TEST_MODE to avoid duplication.\\n+      try {\\n+        if (process.env.VISOR_TEST_MODE === 'true' && config?.output?.pr_comment) {\\n+          // Resolve owner/repo from cached action context or PRInfo.eventContext\\n+          let owner: string | undefined = this.actionContext?.owner;\\n+          let repo: string | undefined = this.actionContext?.repo;\\n+          if (!owner || !repo) {\\n+            try {\\n+              const anyInfo = prInfo as unknown as {\\n+                eventContext?: { repository?: { owner?: { login?: string }; name?: string } };\\n+              };\\n+              owner = anyInfo?.eventContext?.repository?.owner?.login || owner;\\n+              repo = anyInfo?.eventContext?.repository?.name || repo;\\n+            } catch {}\\n+          }\\n+          owner = owner || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[0];\\n+          repo = repo || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[1];\\n+          if (owner && repo && prInfo.number) {\\n+            await this.reviewer.postReviewComment(owner, repo, prInfo.number, execRes.results, {\\n+              config: config as any,\\n+              triggeredBy: prInfo.eventType || 'manual',\\n+              commentId: 'visor-review',\\n+            });\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      return execRes;\\n     }\\n \\n     // Single check execution\\n@@ -2626,6 +3024,31 @@ export class CheckExecutionEngine {\\n \\n       const groupedResults: GroupedCheckResults = {};\\n       groupedResults[checkResult.group] = [checkResult];\\n+      // Test-mode PR comment posting for single-check runs as well\\n+      try {\\n+        if (process.env.VISOR_TEST_MODE === 'true' && config?.output?.pr_comment) {\\n+          let owner: string | undefined = this.actionContext?.owner;\\n+          let repo: string | undefined = this.actionContext?.repo;\\n+          if (!owner || !repo) {\\n+            try {\\n+              const anyInfo = prInfo as unknown as {\\n+                eventContext?: { repository?: { owner?: { login?: string }; name?: string } };\\n+              };\\n+              owner = anyInfo?.eventContext?.repository?.owner?.login || owner;\\n+              repo = anyInfo?.eventContext?.repository?.name || repo;\\n+            } catch {}\\n+          }\\n+          owner = owner || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[0];\\n+          repo = repo || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[1];\\n+          if (owner && repo && prInfo.number) {\\n+            await this.reviewer.postReviewComment(owner, repo, prInfo.number, groupedResults, {\\n+              config: config as any,\\n+              triggeredBy: prInfo.eventType || 'manual',\\n+              commentId: 'visor-review',\\n+            });\\n+          }\\n+        }\\n+      } catch {}\\n       return {\\n         results: groupedResults,\\n         statistics: this.buildExecutionStatistics(),\\n@@ -2682,8 +3105,11 @@ export class CheckExecutionEngine {\\n     };\\n     providerConfig.forEach = checkConfig.forEach;\\n \\n+    // Ensure statistics are recorded for single-check path as well\\n+    if (!this.executionStats.has(checkName)) this.initializeCheckStats(checkName);\\n+    const __iterStart = this.recordIterationStart(checkName);\\n     const __provStart = Date.now();\\n-    const result = await provider.execute(prInfo, providerConfig);\\n+    const result = await provider.execute(prInfo, providerConfig, undefined, this.executionContext);\\n     this.recordProviderDuration(checkName, Date.now() - __provStart);\\n \\n     // Validate forEach output (skip if there are already errors from transform_js or other sources)\\n@@ -2735,7 +3161,13 @@ export class CheckExecutionEngine {\\n       group = checkName;\\n     }\\n \\n-    return {\\n+    // Track output in history (parity with grouped path)\\n+    try {\\n+      const out = (result as any)?.output;\\n+      if (out !== undefined) this.trackOutputHistory(checkName, out);\\n+    } catch {}\\n+\\n+    const checkResult: CheckResult = {\\n       checkName,\\n       content,\\n       group,\\n@@ -2743,6 +3175,16 @@ export class CheckExecutionEngine {\\n       debug: result.debug,\\n       issues: result.issues, // Include structured issues\\n     };\\n+\\n+    // Record completion in execution statistics (success/failure + durations)\\n+    try {\\n+      const issuesArr = (result.issues || []).map(i => ({ ...i }));\\n+      const success = !this.hasFatal(issuesArr);\\n+      const outputVal: unknown = (result as any)?.output;\\n+      this.recordIterationComplete(checkName, __iterStart, success, issuesArr, outputVal);\\n+    } catch {}\\n+\\n+    return checkResult;\\n   }\\n \\n   /**\\n@@ -3348,6 +3790,9 @@ export class CheckExecutionEngine {\\n     tagFilter?: import('./types/config').TagFilter\\n   ): Promise<ReviewSummary> {\\n     const log = logFn || console.error;\\n+    try {\\n+      console.error('[engine] enter executeDependencyAwareChecks (dbg=', debug, ')');\\n+    } catch {}\\n \\n     if (debug) {\\n       log(`🔧 Debug: Starting dependency-aware execution of ${checks.length} checks`);\\n@@ -3419,12 +3864,25 @@ export class CheckExecutionEngine {\\n         }\\n         return true;\\n       };\\n+      const allowByEvent = (name: string): boolean => {\\n+        try {\\n+          const cfg = config!.checks?.[name];\\n+          const triggers: import('./types/config').EventTrigger[] = (cfg?.on || []) as any;\\n+          // No triggers => allowed for all events\\n+          if (!triggers || triggers.length === 0) return true;\\n+          const current = prInfo?.eventType || 'manual';\\n+          return triggers.includes(current as any);\\n+        } catch {\\n+          return true;\\n+        }\\n+      };\\n       const visit = (name: string) => {\\n         const cfg = config.checks![name];\\n         if (!cfg || !cfg.depends_on) return;\\n         for (const dep of cfg.depends_on) {\\n           if (!config.checks![dep]) continue;\\n           if (!allowByTags(dep)) continue;\\n+          if (!allowByEvent(dep)) continue;\\n           if (!set.has(dep)) {\\n             set.add(dep);\\n             visit(dep);\\n@@ -3597,7 +4055,11 @@ export class CheckExecutionEngine {\\n           const providerType = checkConfig.type || 'ai';\\n           const provider = this.providerRegistry.getProviderOrThrow(providerType);\\n           if (debug) {\\n-            log(`🔧 Debug: Provider f|| '${checkName}' is '${providerType}'`);\\n+            log(`🔧 Debug: Provider for '${checkName}' is '${providerType}'`);\\n+          } else if (process.env.VISOR_DEBUG === 'true') {\\n+            try {\\n+              console.log(`[engine] provider for ${checkName} -> ${providerType}`);\\n+            } catch {}\\n           }\\n           this.setProviderWebhookContext(provider);\\n \\n@@ -3625,6 +4087,8 @@ export class CheckExecutionEngine {\\n             message: extendedCheckConfig.message,\\n             env: checkConfig.env,\\n             forEach: checkConfig.forEach,\\n+            // Provide output history so providers can access latest outputs for Liquid rendering\\n+            __outputHistory: this.outputHistory,\\n             // Pass through any provider-specific keys (e.g., op/values for github provider)\\n             ...checkConfig,\\n             ai: {\\n@@ -5110,7 +5574,67 @@ export class CheckExecutionEngine {\\n \\n     // Handle on_finish hooks for forEach checks after ALL dependents complete\\n     if (!shouldStopExecution) {\\n+      try {\\n+        logger.info('🧭 on_finish: invoking handleOnFinishHooks');\\n+      } catch {}\\n+      try {\\n+        if (debug) console.error('[engine] calling handleOnFinishHooks');\\n+      } catch {}\\n       await this.handleOnFinishHooks(config, dependencyGraph, results, prInfo, debug || false);\\n+      // Fallback: if some on_finish static run targets did not execute (e.g., due to graph selection peculiarities),\\n+      // run them once now for each forEach parent that produced items in this run. This preserves general semantics\\n+      // without hardcoding step names.\\n+      try {\\n+        for (const [parentName, cfg] of Object.entries(config.checks || {})) {\\n+          const onf = (cfg as any)?.on_finish as OnFinishConfig | undefined;\\n+          if (!(cfg as any)?.forEach || !onf || !Array.isArray(onf.run) || onf.run.length === 0)\\n+            continue;\\n+          const parentRes = results.get(parentName) as ExtendedReviewSummary | undefined;\\n+          const count = (() => {\\n+            try {\\n+              if (!parentRes) return 0;\\n+              if (Array.isArray(parentRes.forEachItems)) return parentRes.forEachItems.length;\\n+              const out = (parentRes as any)?.output;\\n+              return Array.isArray(out) ? out.length : 0;\\n+            } catch {\\n+              return 0;\\n+            }\\n+          })();\\n+          let histCount = 0;\\n+          try {\\n+            const h = this.outputHistory.get(parentName) as unknown[] | undefined;\\n+            if (Array.isArray(h)) histCount = h.length;\\n+          } catch {}\\n+          if (count > 0 || histCount > 0) {\\n+            for (const stepId of onf.run!) {\\n+              if (typeof stepId !== 'string' || !stepId) continue;\\n+              if (results.has(stepId)) continue; // already executed\\n+              try {\\n+                logger.info(\\n+                  `▶ on_finish.fallback: executing static run step '${stepId}' for parent '${parentName}'`\\n+                );\\n+              } catch {}\\n+              try {\\n+                if (debug)\\n+                  console.error(`[on_finish.fallback] run '${stepId}' for '${parentName}'`);\\n+              } catch {}\\n+              await this.runNamedCheck(stepId, [], {\\n+                origin: 'on_finish',\\n+                config: config!,\\n+                dependencyGraph,\\n+                prInfo,\\n+                resultsMap: results,\\n+                debug: !!debug,\\n+                overlay: new Map(results),\\n+              });\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n+    } else {\\n+      try {\\n+        logger.info('🧭 on_finish: skipped due to shouldStopExecution');\\n+      } catch {}\\n     }\\n \\n     // Cleanup sessions BEFORE printing summary to avoid mixing debug logs with table output\\n@@ -6661,6 +7185,17 @@ export class CheckExecutionEngine {\\n     this.outputHistory.get(checkName)!.push(output);\\n   }\\n \\n+  /**\\n+   * Snapshot of output history per step for test assertions\\n+   */\\n+  public getOutputHistorySnapshot(): Record<string, unknown[]> {\\n+    const out: Record<string, unknown[]> = {};\\n+    for (const [k, v] of this.outputHistory.entries()) {\\n+      out[k] = Array.isArray(v) ? [...v] : [];\\n+    }\\n+    return out;\\n+  }\\n+\\n   /**\\n    * Record that a check was skipped\\n    */\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli-main.ts\",\"additions\":4,\"deletions\":0,\"changes\":111,\"patch\":\"diff --git a/src/cli-main.ts b/src/cli-main.ts\\nindex 1b1100ca..ad2245ff 100644\\n--- a/src/cli-main.ts\\n+++ b/src/cli-main.ts\\n@@ -109,6 +109,112 @@ async function handleValidateCommand(argv: string[], configManager: ConfigManage\\n   }\\n }\\n \\n+/**\\n+ * Handle the test subcommand (Milestone 0: discovery only)\\n+ */\\n+async function handleTestCommand(argv: string[]): Promise<void> {\\n+  // Minimal flag parsing: --config <path>, --only <name>, --bail\\n+  const getArg = (name: string): string | undefined => {\\n+    const i = argv.indexOf(name);\\n+    return i >= 0 ? argv[i + 1] : undefined;\\n+  };\\n+  const hasFlag = (name: string): boolean => argv.includes(name);\\n+\\n+  const testsPath = getArg('--config');\\n+  const only = getArg('--only');\\n+  const bail = hasFlag('--bail');\\n+  const listOnly = hasFlag('--list');\\n+  const validateOnly = hasFlag('--validate');\\n+  const progress = (getArg('--progress') as 'compact' | 'detailed' | undefined) || 'compact';\\n+  void progress; // currently parsed but not changing output detail yet\\n+  const jsonOut = getArg('--json'); // path or '-' for stdout\\n+  const reportArg = getArg('--report'); // e.g. junit:path.xml\\n+  const summaryArg = getArg('--summary'); // e.g. md:path.md\\n+  const maxParallelRaw = getArg('--max-parallel');\\n+  const promptMaxCharsRaw = getArg('--prompt-max-chars');\\n+  const maxParallel = maxParallelRaw ? Math.max(1, parseInt(maxParallelRaw, 10) || 1) : undefined;\\n+  const promptMaxChars = promptMaxCharsRaw\\n+    ? Math.max(1, parseInt(promptMaxCharsRaw, 10) || 1)\\n+    : undefined;\\n+\\n+  // Configure logger for concise console output\\n+  configureLoggerFromCli({ output: 'table', debug: false, verbose: false, quiet: false });\\n+\\n+  console.log('🧪 Visor Test Runner');\\n+  try {\\n+    const { discoverAndPrint, validateTestsOnly, VisorTestRunner } = await import(\\n+      './test-runner/index'\\n+    );\\n+    if (validateOnly) {\\n+      const errors = await validateTestsOnly({ testsPath });\\n+      process.exit(errors > 0 ? 1 : 0);\\n+    }\\n+    if (listOnly) {\\n+      await discoverAndPrint({ testsPath });\\n+      if (only) console.log(`\\\\nFilter: --only ${only}`);\\n+      if (bail) console.log('Mode: --bail (stop on first failure)');\\n+      process.exit(0);\\n+    }\\n+    // Run and capture structured results\\n+    const runner = new (VisorTestRunner as any)();\\n+    const tpath = runner.resolveTestsPath(testsPath);\\n+    const suite = runner.loadSuite(tpath);\\n+    const runRes = await runner.runCases(tpath, suite, { only, bail, maxParallel, promptMaxChars });\\n+    const failures = runRes.failures;\\n+    // Basic reporters (Milestone 7): write minimal JSON/JUnit/Markdown summaries\\n+    try {\\n+      if (jsonOut) {\\n+        const fs = require('fs');\\n+        const payload = { failures, results: runRes.results };\\n+        const data = JSON.stringify(payload, null, 2);\\n+        if (jsonOut === '-' || jsonOut === 'stdout') console.log(data);\\n+        else {\\n+          fs.writeFileSync(jsonOut, data, 'utf8');\\n+          console.error(`📝 JSON report written to ${jsonOut}`);\\n+        }\\n+      }\\n+    } catch {}\\n+    try {\\n+      if (reportArg && reportArg.startsWith('junit:')) {\\n+        const fs = require('fs');\\n+        const dest = reportArg.slice('junit:'.length);\\n+        const tests = (runRes.results || []).length;\\n+        const failed = (runRes.results || []).filter((r: any) => !r.passed).length;\\n+        const detail = (runRes.results || [])\\n+          .map((r: any) => {\\n+            const errs = (r.errors || []).concat(\\n+              ...(r.stages || []).map((s: any) => s.errors || [])\\n+            );\\n+            return `<testcase classname=\\\\\\\"visor\\\\\\\" name=\\\\\\\"${r.name}\\\\\\\"${errs.length > 0 ? '' : ''}>${errs\\n+              .map((e: string) => `<failure message=\\\\\\\"${e.replace(/\\\\\\\"/g, '&quot;')}\\\\\\\"></failure>`)\\n+              .join('')}</testcase>`;\\n+          })\\n+          .join('\\\\n  ');\\n+        const xml = `<?xml version=\\\\\\\"1.0\\\\\\\" encoding=\\\\\\\"UTF-8\\\\\\\"?>\\\\n<testsuite name=\\\\\\\"visor\\\\\\\" tests=\\\\\\\"${tests}\\\\\\\" failures=\\\\\\\"${failed}\\\\\\\">\\\\n  ${detail}\\\\n</testsuite>`;\\n+        fs.writeFileSync(dest, xml, 'utf8');\\n+        console.error(`📝 JUnit report written to ${dest}`);\\n+      }\\n+    } catch {}\\n+    try {\\n+      if (summaryArg && summaryArg.startsWith('md:')) {\\n+        const fs = require('fs');\\n+        const dest = summaryArg.slice('md:'.length);\\n+        const lines = (runRes.results || []).map(\\n+          (r: any) =>\\n+            `- ${r.passed ? '✅' : '❌'} ${r.name}${r.stages ? ' (' + r.stages.length + ' stage' + (r.stages.length !== 1 ? 's' : '') + ')' : ''}`\\n+        );\\n+        const content = `# Visor Test Summary\\\\n\\\\n- Failures: ${failures}\\\\n\\\\n${lines.join('\\\\n')}`;\\n+        fs.writeFileSync(dest, content, 'utf8');\\n+        console.error(`📝 Markdown summary written to ${dest}`);\\n+      }\\n+    } catch {}\\n+    process.exit(failures > 0 ? 1 : 0);\\n+  } catch (err) {\\n+    console.error('❌ test: ' + (err instanceof Error ? err.message : String(err)));\\n+    process.exit(1);\\n+  }\\n+}\\n+\\n /**\\n  * Main CLI entry point for Visor\\n  */\\n@@ -151,6 +257,11 @@ export async function main(): Promise<void> {\\n       await handleValidateCommand(filteredArgv, configManager);\\n       return;\\n     }\\n+    // Check for test subcommand\\n+    if (filteredArgv.length > 2 && filteredArgv[2] === 'test') {\\n+      await handleTestCommand(filteredArgv);\\n+      return;\\n+    }\\n \\n     // Parse arguments using the CLI class\\n     const options = cli.parseArgs(filteredArgv);\\n\",\"status\":\"added\"},{\"filename\":\"src/github-comments.ts\",\"additions\":1,\"deletions\":1,\"changes\":19,\"patch\":\"diff --git a/src/github-comments.ts b/src/github-comments.ts\\nindex be84f6ce..13ad7a3c 100644\\n--- a/src/github-comments.ts\\n+++ b/src/github-comments.ts\\n@@ -338,12 +338,10 @@ ${content}\\n           // Don't retry auth errors, not found errors, etc.\\n           throw error;\\n         } else {\\n-          const computed =\\n-            this.retryConfig.baseDelay * Math.pow(this.retryConfig.backoffFactor, attempt);\\n-          const delay =\\n-            computed > this.retryConfig.maxDelay\\n-              ? Math.max(0, this.retryConfig.maxDelay - 1)\\n-              : computed;\\n+          const delay = Math.min(\\n+            this.retryConfig.baseDelay * Math.pow(this.retryConfig.backoffFactor, attempt),\\n+            this.retryConfig.maxDelay\\n+          );\\n           await this.sleep(delay);\\n         }\\n       }\\n@@ -356,14 +354,7 @@ ${content}\\n    * Sleep utility\\n    */\\n   private sleep(ms: number): Promise<void> {\\n-    return new Promise(resolve => {\\n-      const t = setTimeout(resolve, ms);\\n-      if (typeof (t as any).unref === 'function') {\\n-        try {\\n-          (t as any).unref();\\n-        } catch {}\\n-      }\\n-    });\\n+    return new Promise(resolve => setTimeout(resolve, ms));\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/index.ts\",\"additions\":1,\"deletions\":1,\"changes\":26,\"patch\":\"diff --git a/src/index.ts b/src/index.ts\\nindex 93a9393a..0f4c6c5f 100644\\n--- a/src/index.ts\\n+++ b/src/index.ts\\n@@ -13,7 +13,7 @@ import { PRAnalyzer, PRInfo } from './pr-analyzer';\\n import { configureLoggerFromCli } from './logger';\\n import { deriveExecutedCheckNames } from './utils/ui-helpers';\\n import { resolveHeadShaFromEvent } from './utils/head-sha';\\n-import { PRReviewer, GroupedCheckResults, ReviewIssue, CheckResult } from './reviewer';\\n+import { PRReviewer, GroupedCheckResults, ReviewIssue } from './reviewer';\\n import { GitHubActionInputs, GitHubContext } from './action-cli-bridge';\\n import { ConfigManager } from './config';\\n import { GitHubCheckService, CheckRunOptions } from './github-check-service';\\n@@ -762,30 +762,8 @@ async function handleIssueEvent(\\n     if (Object.keys(results).length > 0) {\\n       let commentBody = '';\\n \\n-      // Collapse dynamic group: if multiple dynamic responses exist in a single run,\\n-      // take only the last non-empty one to avoid duplicated old+new answers.\\n-      const resultsToUse: GroupedCheckResults = { ...results };\\n-      try {\\n-        const dyn: CheckResult[] | undefined = resultsToUse['dynamic'];\\n-        if (Array.isArray(dyn) && dyn.length > 1) {\\n-          const nonEmpty = dyn.filter(d => d.content && d.content.trim().length > 0);\\n-          if (nonEmpty.length > 0) {\\n-            // Keep only the last non-empty dynamic item\\n-            resultsToUse['dynamic'] = [nonEmpty[nonEmpty.length - 1]];\\n-          } else {\\n-            // All empty: keep the last item (empty) to preserve intent\\n-            resultsToUse['dynamic'] = [dyn[dyn.length - 1]];\\n-          }\\n-        }\\n-      } catch (error) {\\n-        console.warn(\\n-          'Failed to collapse dynamic group:',\\n-          error instanceof Error ? error.message : String(error)\\n-        );\\n-      }\\n-\\n       // Directly use check content without adding extra headers\\n-      for (const checks of Object.values(resultsToUse)) {\\n+      for (const checks of Object.values(results)) {\\n         for (const check of checks) {\\n           if (check.content && check.content.trim()) {\\n             commentBody += `${check.content}\\\\n\\\\n`;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/ai-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":31,\"patch\":\"diff --git a/src/providers/ai-check-provider.ts b/src/providers/ai-check-provider.ts\\nindex a85fc73c..ea883e20 100644\\n--- a/src/providers/ai-check-provider.ts\\n+++ b/src/providers/ai-check-provider.ts\\n@@ -468,7 +468,10 @@ export class AICheckProvider extends CheckProvider {\\n     prInfo: PRInfo,\\n     config: CheckProviderConfig,\\n     _dependencyResults?: Map<string, ReviewSummary>,\\n-    sessionInfo?: { parentSessionId?: string; reuseSession?: boolean }\\n+    sessionInfo?: {\\n+      parentSessionId?: string;\\n+      reuseSession?: boolean;\\n+    } & import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n     // Extract AI configuration - only set properties that are explicitly provided\\n     const aiConfig: AIReviewConfig = {};\\n@@ -613,6 +616,32 @@ export class AICheckProvider extends CheckProvider {\\n       (config as any).__outputHistory as Map<string, unknown[]> | undefined\\n     );\\n \\n+    // Test hook: capture the FINAL prompt (with PR context) before provider invocation\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const serviceForCapture = new AIReviewService(aiConfig);\\n+      const finalPrompt = await (serviceForCapture as any).buildCustomPrompt(\\n+        prInfo,\\n+        processedPrompt,\\n+        config.schema,\\n+        { checkName: (config as any).checkName }\\n+      );\\n+      sessionInfo?.hooks?.onPromptCaptured?.({\\n+        step: String(stepName),\\n+        provider: 'ai',\\n+        prompt: finalPrompt,\\n+      });\\n+    } catch {}\\n+\\n+    // Test hook: mock output for this step (short-circuit provider)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = sessionInfo?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        return { issues: [], output: mock } as ReviewSummary & { output: unknown };\\n+      }\\n+    } catch {}\\n+\\n     // Create AI service with config - environment variables will be used if aiConfig is empty\\n     const service = new AIReviewService(aiConfig);\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/check-provider.interface.ts\",\"additions\":1,\"deletions\":0,\"changes\":2,\"patch\":\"diff --git a/src/providers/check-provider.interface.ts b/src/providers/check-provider.interface.ts\\nindex fc7fd1cf..0fa5cf19 100644\\n--- a/src/providers/check-provider.interface.ts\\n+++ b/src/providers/check-provider.interface.ts\\n@@ -46,6 +46,8 @@ export interface ExecutionContext {\\n   /** SDK hooks for human input */\\n   hooks?: {\\n     onHumanInput?: (request: HumanInputRequest) => Promise<string>;\\n+    onPromptCaptured?: (info: { step: string; provider: string; prompt: string }) => void;\\n+    mockForStep?: (step: string) => unknown | undefined;\\n   };\\n }\\n \\n\",\"status\":\"added\"},{\"filename\":\"src/providers/command-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":38,\"patch\":\"diff --git a/src/providers/command-check-provider.ts b/src/providers/command-check-provider.ts\\nindex 04a66741..5160e72d 100644\\n--- a/src/providers/command-check-provider.ts\\n+++ b/src/providers/command-check-provider.ts\\n@@ -66,7 +66,8 @@ export class CommandCheckProvider extends CheckProvider {\\n   async execute(\\n     prInfo: PRInfo,\\n     config: CheckProviderConfig,\\n-    dependencyResults?: Map<string, ReviewSummary>\\n+    dependencyResults?: Map<string, ReviewSummary>,\\n+    context?: import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n     try {\\n       logger.info(\\n@@ -142,6 +143,41 @@ export class CommandCheckProvider extends CheckProvider {\\n       );\\n     } catch {}\\n \\n+    // Test hook: mock output for this step (short-circuit execution)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = context?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock && typeof mock === 'object') {\\n+        const m = mock as { stdout?: string; stderr?: string; exit_code?: number };\\n+        let out: unknown = m.stdout ?? '';\\n+        try {\\n+          if (\\n+            typeof out === 'string' &&\\n+            (out.trim().startsWith('{') || out.trim().startsWith('['))\\n+          ) {\\n+            out = JSON.parse(out);\\n+          }\\n+        } catch {}\\n+        if (m.exit_code && m.exit_code !== 0) {\\n+          return {\\n+            issues: [\\n+              {\\n+                file: 'command',\\n+                line: 0,\\n+                ruleId: 'command/execution_error',\\n+                message: `Mocked command exited with code ${m.exit_code}`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              },\\n+            ],\\n+            // Also expose output for assertions\\n+            output: out,\\n+          } as any;\\n+        }\\n+        return { issues: [], output: out } as any;\\n+      }\\n+    } catch {}\\n+\\n     try {\\n       // Render the command with Liquid templates if needed\\n       let renderedCommand = command;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/github-ops-provider.ts\",\"additions\":4,\"deletions\":1,\"changes\":119,\"patch\":\"diff --git a/src/providers/github-ops-provider.ts b/src/providers/github-ops-provider.ts\\nindex 1dafb432..2e7cef21 100644\\n--- a/src/providers/github-ops-provider.ts\\n+++ b/src/providers/github-ops-provider.ts\\n@@ -4,6 +4,7 @@ import { ReviewSummary } from '../reviewer';\\n import Sandbox from '@nyariv/sandboxjs';\\n import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n+import { logger } from '../logger';\\n \\n export class GitHubOpsProvider extends CheckProvider {\\n   private sandbox?: Sandbox;\\n@@ -51,11 +52,29 @@ export class GitHubOpsProvider extends CheckProvider {\\n \\n     // IMPORTANT: Always prefer authenticated octokit from event context (GitHub App or token)\\n     // This ensures proper bot identity in reactions, labels, and comments\\n-    const octokit: import('@octokit/rest').Octokit | undefined = config.eventContext?.octokit as\\n+    let octokit: import('@octokit/rest').Octokit | undefined = config.eventContext?.octokit as\\n       | import('@octokit/rest').Octokit\\n       | undefined;\\n+    if (process.env.VISOR_DEBUG === 'true') {\\n+      try {\\n+        logger.debug(`[github-ops] pre-fallback octokit? ${!!octokit}`);\\n+      } catch {}\\n+    }\\n+    // Test runner fallback: use global recorder if eventContext is missing octokit\\n+    if (!octokit) {\\n+      try {\\n+        const { getGlobalRecorder } = require('../test-runner/recorders/global-recorder');\\n+        const rec = getGlobalRecorder && getGlobalRecorder();\\n+        if (rec) octokit = rec as any;\\n+      } catch {}\\n+    }\\n \\n     if (!octokit) {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        try {\\n+          console.error('[github-ops] missing octokit after fallback — returning issue');\\n+        } catch {}\\n+      }\\n       return {\\n         issues: [\\n           {\\n@@ -72,7 +91,24 @@ export class GitHubOpsProvider extends CheckProvider {\\n     }\\n \\n     const repoEnv = process.env.GITHUB_REPOSITORY || '';\\n-    const [owner, repo] = repoEnv.split('/') as [string, string];\\n+    let owner = '';\\n+    let repo = '';\\n+    if (repoEnv.includes('/')) {\\n+      [owner, repo] = repoEnv.split('/') as [string, string];\\n+    } else {\\n+      try {\\n+        const ec: any = config.eventContext || {};\\n+        owner = ec?.repository?.owner?.login || owner;\\n+        repo = ec?.repository?.name || repo;\\n+      } catch {}\\n+    }\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        logger.info(\\n+          `[github-ops] context octokit? ${!!octokit} repo=${owner}/${repo} pr#=${prInfo?.number}`\\n+        );\\n+      }\\n+    } catch {}\\n     if (!owner || !repo || !prInfo?.number) {\\n       return {\\n         issues: [\\n@@ -93,6 +129,11 @@ export class GitHubOpsProvider extends CheckProvider {\\n     if (Array.isArray(cfg.values)) valuesRaw = (cfg.values as unknown[]).map(v => String(v));\\n     else if (typeof cfg.values === 'string') valuesRaw = [cfg.values];\\n     else if (typeof cfg.value === 'string') valuesRaw = [cfg.value];\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        logger.info(`[github-ops] op=${cfg.op} valuesRaw(before)=${JSON.stringify(valuesRaw)}`);\\n+      }\\n+    } catch {}\\n \\n     // Liquid render helper for values\\n     const renderValues = async (arr: string[]): Promise<string[]> => {\\n@@ -109,6 +150,17 @@ export class GitHubOpsProvider extends CheckProvider {\\n           outputs[name] = summary.output !== undefined ? summary.output : summary;\\n         }\\n       }\\n+      // Fallback: if outputs missing but engine provided history, use last output snapshot\\n+      try {\\n+        const hist = (config as any).__outputHistory as Map<string, unknown[]> | undefined;\\n+        if (hist) {\\n+          for (const [name, arr] of hist.entries()) {\\n+            if (!outputs[name] && Array.isArray(arr) && arr.length > 0) {\\n+              outputs[name] = arr[arr.length - 1];\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n       const ctx = {\\n         pr: {\\n           number: prInfo.number,\\n@@ -120,6 +172,25 @@ export class GitHubOpsProvider extends CheckProvider {\\n         },\\n         outputs,\\n       };\\n+      try {\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          logger.info(`[github-ops] deps keys=${Object.keys(outputs).join(', ')}`);\\n+          const ov = outputs['overview'] as any;\\n+          if (ov) {\\n+            logger.info(`[github-ops] outputs.overview.keys=${Object.keys(ov).join(',')}`);\\n+            if (ov.tags) {\\n+              logger.info(\\n+                `[github-ops] outputs.overview.tags keys=${Object.keys(ov.tags).join(',')}`\\n+              );\\n+              try {\\n+                logger.info(\\n+                  `[github-ops] outputs.overview.tags['review-effort']=${String(ov.tags['review-effort'])}`\\n+                );\\n+              } catch {}\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n       const out: string[] = [];\\n       for (const item of arr) {\\n         if (typeof item === 'string' && (item.includes('{{') || item.includes('{%'))) {\\n@@ -129,6 +200,9 @@ export class GitHubOpsProvider extends CheckProvider {\\n           } catch (e) {\\n             // If Liquid fails, surface as a provider error\\n             const msg = e instanceof Error ? e.message : String(e);\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              logger.warn(`[github-ops] liquid_render_error: ${msg}`);\\n+            }\\n             return Promise.reject({\\n               issues: [\\n                 {\\n@@ -175,6 +249,9 @@ export class GitHubOpsProvider extends CheckProvider {\\n         else if (Array.isArray(res)) values = (res as unknown[]).map(v => String(v));\\n       } catch (e) {\\n         const msg = e instanceof Error ? e.message : String(e);\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          logger.warn(`[github-ops] value_js_error: ${msg}`);\\n+        }\\n         return {\\n           issues: [\\n             {\\n@@ -190,14 +267,49 @@ export class GitHubOpsProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Fallback: if values are still empty, try deriving from dependency outputs (common pattern: outputs.<dep>.tags)\\n+    if (values.length === 0 && dependencyResults && dependencyResults.size > 0) {\\n+      try {\\n+        const derived: string[] = [];\\n+        for (const result of dependencyResults.values()) {\\n+          const out = (result as ReviewSummary & { output?: unknown })?.output ?? result;\\n+          const tags = (out as Record<string, unknown>)?.['tags'] as\\n+            | Record<string, unknown>\\n+            | undefined;\\n+          if (tags && typeof tags === 'object') {\\n+            const label = tags['label'];\\n+            const effort = (tags as Record<string, unknown>)['review-effort'];\\n+            if (label != null) derived.push(String(label));\\n+            if (effort !== undefined && effort !== null)\\n+              derived.push(`review/effort:${String(effort)}`);\\n+          }\\n+        }\\n+        values = derived;\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          logger.info(`[github-ops] derived values from deps: ${JSON.stringify(values)}`);\\n+        }\\n+      } catch {}\\n+    }\\n+\\n     // Trim, drop empty, and de-duplicate values regardless of source\\n     values = values.map(v => v.trim()).filter(v => v.length > 0);\\n     values = Array.from(new Set(values));\\n \\n+    try {\\n+      // Minimal debug to help diagnose label flow under tests\\n+      if (process.env.NODE_ENV === 'test' || process.env.VISOR_DEBUG === 'true') {\\n+        logger.info(`[github-ops] ${cfg.op} resolved values: ${JSON.stringify(values)}`);\\n+      }\\n+    } catch {}\\n+\\n     try {\\n       switch (cfg.op) {\\n         case 'labels.add': {\\n           if (values.length === 0) break; // no-op if nothing to add\\n+          try {\\n+            if (process.env.VISOR_OUTPUT_FORMAT !== 'json')\\n+              logger.step(`[github-ops] labels.add -> ${JSON.stringify(values)}`);\\n+          } catch {}\\n           await octokit.rest.issues.addLabels({\\n             owner,\\n             repo,\\n@@ -246,6 +358,9 @@ export class GitHubOpsProvider extends CheckProvider {\\n       return { issues: [] };\\n     } catch (e) {\\n       const msg = e instanceof Error ? e.message : String(e);\\n+      try {\\n+        logger.error(`[github-ops] op_failed ${cfg.op}: ${msg}`);\\n+      } catch {}\\n       return {\\n         issues: [\\n           {\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/http-client-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":11,\"patch\":\"diff --git a/src/providers/http-client-provider.ts b/src/providers/http-client-provider.ts\\nindex 9620f01b..4d8c41be 100644\\n--- a/src/providers/http-client-provider.ts\\n+++ b/src/providers/http-client-provider.ts\\n@@ -54,7 +54,7 @@ export class HttpClientProvider extends CheckProvider {\\n     prInfo: PRInfo,\\n     config: CheckProviderConfig,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n-    _sessionInfo?: { parentSessionId?: string; reuseSession?: boolean }\\n+    context?: import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n     const url = config.url as string;\\n     const method = (config.method as string) || 'GET';\\n@@ -96,8 +96,13 @@ export class HttpClientProvider extends CheckProvider {\\n       // Resolve environment variables in headers\\n       const resolvedHeaders = EnvironmentResolver.resolveHeaders(headers);\\n \\n-      // Fetch data from the endpoint\\n-      const data = await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n+      // Test hook: mock HTTP response for this step\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = context?.hooks?.mockForStep?.(String(stepName));\\n+      const data =\\n+        mock !== undefined\\n+          ? mock\\n+          : await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n \\n       // Apply transformation if specified\\n       let processedData = data;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/memory-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":40,\"patch\":\"diff --git a/src/providers/memory-check-provider.ts b/src/providers/memory-check-provider.ts\\nindex aee629c5..dca92621 100644\\n--- a/src/providers/memory-check-provider.ts\\n+++ b/src/providers/memory-check-provider.ts\\n@@ -381,34 +381,36 @@ export class MemoryCheckProvider extends CheckProvider {\\n     try {\\n       if (\\n         (config as any).checkName === 'aggregate-validations' ||\\n-        (config as any).checkName === 'aggregate' ||\\n         (config as any).checkName === 'aggregate'\\n       ) {\\n-        const hist = (enhancedContext as any)?.outputs?.history || {};\\n-        const keys = Object.keys(hist);\\n-        console.log('[MemoryProvider]', (config as any).checkName, ': history keys =', keys);\\n-        const vf = (hist as any)['validate-fact'];\\n-        console.log(\\n-          '[MemoryProvider]',\\n-          (config as any).checkName,\\n-          ': validate-fact history length =',\\n-          Array.isArray(vf) ? vf.length : 'n/a'\\n-        );\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          const hist = (enhancedContext as any)?.outputs?.history || {};\\n+          const keys = Object.keys(hist);\\n+          logger.debug(\\n+            `[MemoryProvider] ${(config as any).checkName}: history keys = [${keys.join(', ')}]`\\n+          );\\n+          const vf = (hist as any)['validate-fact'];\\n+          logger.debug(\\n+            `[MemoryProvider] ${(config as any).checkName}: validate-fact history length = ${\\n+              Array.isArray(vf) ? vf.length : 'n/a'\\n+            }`\\n+          );\\n+        }\\n       }\\n     } catch {}\\n \\n     const result = this.evaluateJavaScriptBlock(script, enhancedContext);\\n     try {\\n-      if ((config as any).checkName === 'aggregate-validations') {\\n+      if (\\n+        (config as any).checkName === 'aggregate-validations' &&\\n+        process.env.VISOR_DEBUG === 'true'\\n+      ) {\\n         const tv = store.get('total_validations', 'fact-validation');\\n         const av = store.get('all_valid', 'fact-validation');\\n-        console.error(\\n-          '[MemoryProvider] post-exec',\\n-          (config as any).checkName,\\n-          'total_validations=',\\n-          tv,\\n-          'all_valid=',\\n-          av\\n+        logger.debug(\\n+          `[MemoryProvider] post-exec ${(config as any).checkName} total_validations=${String(\\n+            tv\\n+          )} all_valid=${String(av)}`\\n         );\\n       }\\n     } catch {}\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/assertions.ts\",\"additions\":4,\"deletions\":0,\"changes\":91,\"patch\":\"diff --git a/src/test-runner/assertions.ts b/src/test-runner/assertions.ts\\nnew file mode 100644\\nindex 00000000..ea676eea\\n--- /dev/null\\n+++ b/src/test-runner/assertions.ts\\n@@ -0,0 +1,91 @@\\n+export type CountExpectation = {\\n+  exactly?: number;\\n+  at_least?: number;\\n+  at_most?: number;\\n+};\\n+\\n+export interface CallsExpectation extends CountExpectation {\\n+  step?: string;\\n+  provider?: 'github' | string;\\n+  op?: string;\\n+  args?: Record<string, unknown>;\\n+}\\n+\\n+export interface PromptsExpectation {\\n+  step: string;\\n+  index?: number | 'first' | 'last';\\n+  contains?: string[];\\n+  not_contains?: string[];\\n+  matches?: string; // regex string\\n+  where?: {\\n+    contains?: string[];\\n+    not_contains?: string[];\\n+    matches?: string; // regex\\n+  };\\n+}\\n+\\n+export interface OutputsExpectation {\\n+  step: string;\\n+  index?: number | 'first' | 'last';\\n+  path: string;\\n+  equals?: unknown;\\n+  equalsDeep?: unknown;\\n+  matches?: string; // regex\\n+  where?: {\\n+    path: string;\\n+    equals?: unknown;\\n+    matches?: string; // regex\\n+  };\\n+  contains_unordered?: unknown[]; // array membership ignoring order\\n+}\\n+\\n+export interface ExpectBlock {\\n+  use?: string[];\\n+  calls?: CallsExpectation[];\\n+  prompts?: PromptsExpectation[];\\n+  outputs?: OutputsExpectation[];\\n+  no_calls?: Array<{ step?: string; provider?: string; op?: string }>;\\n+  fail?: { message_contains?: string };\\n+  strict_violation?: { for_step?: string; message_contains?: string };\\n+}\\n+\\n+export function validateCounts(exp: CountExpectation): void {\\n+  const keys = ['exactly', 'at_least', 'at_most'].filter(k => (exp as any)[k] !== undefined);\\n+  if (keys.length > 1) {\\n+    throw new Error(`Count expectation is ambiguous: ${keys.join(', ')}`);\\n+  }\\n+}\\n+\\n+export function deepEqual(a: unknown, b: unknown): boolean {\\n+  if (a === b) return true;\\n+  if (typeof a !== typeof b) return false;\\n+  if (a && b && typeof a === 'object') {\\n+    if (Array.isArray(a) && Array.isArray(b)) {\\n+      if (a.length !== b.length) return false;\\n+      for (let i = 0; i < a.length; i++) if (!deepEqual(a[i], b[i])) return false;\\n+      return true;\\n+    }\\n+    const ak = Object.keys(a as any).sort();\\n+    const bk = Object.keys(b as any).sort();\\n+    if (!deepEqual(ak, bk)) return false;\\n+    for (const k of ak) if (!deepEqual((a as any)[k], (b as any)[k])) return false;\\n+    return true;\\n+  }\\n+  return false;\\n+}\\n+\\n+export function containsUnordered(haystack: unknown[], needles: unknown[]): boolean {\\n+  if (!Array.isArray(haystack) || !Array.isArray(needles)) return false;\\n+  const used = new Array(haystack.length).fill(false);\\n+  outer: for (const n of needles) {\\n+    for (let i = 0; i < haystack.length; i++) {\\n+      if (used[i]) continue;\\n+      if (deepEqual(haystack[i], n) || haystack[i] === n) {\\n+        used[i] = true;\\n+        continue outer;\\n+      }\\n+    }\\n+    return false;\\n+  }\\n+  return true;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/fixture-loader.ts\",\"additions\":6,\"deletions\":0,\"changes\":156,\"patch\":\"diff --git a/src/test-runner/fixture-loader.ts b/src/test-runner/fixture-loader.ts\\nnew file mode 100644\\nindex 00000000..7ec38d15\\n--- /dev/null\\n+++ b/src/test-runner/fixture-loader.ts\\n@@ -0,0 +1,156 @@\\n+export type BuiltinFixtureName =\\n+  | 'gh.pr_open.minimal'\\n+  | 'gh.pr_sync.minimal'\\n+  | 'gh.issue_open.minimal'\\n+  | 'gh.issue_comment.standard'\\n+  | 'gh.issue_comment.visor_help'\\n+  | 'gh.issue_comment.visor_regenerate'\\n+  | 'gh.issue_comment.edited'\\n+  | 'gh.pr_closed.minimal';\\n+\\n+export interface LoadedFixture {\\n+  name: string;\\n+  webhook: { name: string; action?: string; payload: Record<string, unknown> };\\n+  git?: { branch?: string; baseBranch?: string };\\n+  files?: Array<{\\n+    path: string;\\n+    content: string;\\n+    status?: 'added' | 'modified' | 'removed' | 'renamed';\\n+    additions?: number;\\n+    deletions?: number;\\n+  }>;\\n+  diff?: string; // unified diff text\\n+  env?: Record<string, string>;\\n+  time?: { now?: string };\\n+}\\n+\\n+export class FixtureLoader {\\n+  load(name: BuiltinFixtureName): LoadedFixture {\\n+    // Minimal, stable, general-purpose fixtures used by the test runner.\\n+    // All fixtures supply a webhook payload and, for PR variants, a small diff.\\n+    if (name.startsWith('gh.pr_open')) {\\n+      const files: LoadedFixture['files'] = [\\n+        {\\n+          path: 'src/search.ts',\\n+          content: 'export function search(q: string) {\\\\n  return []\\\\n}\\\\n',\\n+          status: 'added',\\n+          additions: 3,\\n+          deletions: 0,\\n+        },\\n+      ];\\n+      const diff = this.buildUnifiedDiff(files);\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'pull_request',\\n+          action: 'opened',\\n+          payload: { pull_request: { number: 1, title: 'feat: add user search' } },\\n+        },\\n+        git: { branch: 'feature/test', baseBranch: 'main' },\\n+        files,\\n+        diff,\\n+      };\\n+    }\\n+    if (name.startsWith('gh.pr_sync')) {\\n+      const files: LoadedFixture['files'] = [\\n+        {\\n+          path: 'src/search.ts',\\n+          content: 'export function search(q: string) {\\\\n  return [q] // updated\\\\n}\\\\n',\\n+          status: 'modified',\\n+          additions: 1,\\n+          deletions: 0,\\n+        },\\n+      ];\\n+      const diff = this.buildUnifiedDiff(files);\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'pull_request',\\n+          action: 'synchronize',\\n+          payload: { pull_request: { number: 1, title: 'feat: add user search (update)' } },\\n+        },\\n+        git: { branch: 'feature/test', baseBranch: 'main' },\\n+        files,\\n+        diff,\\n+      };\\n+    }\\n+    if (name.startsWith('gh.issue_open')) {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issues',\\n+          action: 'opened',\\n+          payload: {\\n+            issue: { number: 12, title: 'Bug: crashes on search edge case', body: 'Steps...' },\\n+          },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.issue_comment.standard') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issue_comment',\\n+          action: 'created',\\n+          payload: { comment: { body: 'Thanks for the update!' }, issue: { number: 1 } },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.issue_comment.visor_help') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issue_comment',\\n+          action: 'created',\\n+          payload: { comment: { body: '/visor help' }, issue: { number: 1 } },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.issue_comment.visor_regenerate') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issue_comment',\\n+          action: 'created',\\n+          payload: { comment: { body: '/visor Regenerate reviews' }, issue: { number: 1 } },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.pr_closed.minimal') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'pull_request',\\n+          action: 'closed',\\n+          payload: { pull_request: { number: 1, title: 'feat: add user search' } },\\n+        },\\n+      };\\n+    }\\n+    // Fallback minimal\\n+    return {\\n+      name,\\n+      webhook: { name: 'unknown', payload: {} },\\n+    };\\n+  }\\n+\\n+  private buildUnifiedDiff(\\n+    files: Array<{ path: string; content: string; status?: string }>\\n+  ): string {\\n+    // Build a very small, stable unified diff suitable for prompts\\n+    const chunks = files.map(f => {\\n+      const header =\\n+        `diff --git a/${f.path} b/${f.path}\\\\n` +\\n+        (f.status === 'added'\\n+          ? 'index 0000000..1111111 100644\\\\n--- /dev/null\\\\n'\\n+          : `index 1111111..2222222 100644\\\\n--- a/${f.path}\\\\n`) +\\n+        `+++ b/${f.path}\\\\n` +\\n+        '@@\\\\n';\\n+      const body = f.content\\n+        .split('\\\\n')\\n+        .map(line => (f.status === 'removed' ? `-${line}` : `+${line}`))\\n+        .join('\\\\n');\\n+      return header + body + '\\\\n';\\n+    });\\n+    return chunks.join('\\\\n');\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/index.ts\",\"additions\":53,\"deletions\":0,\"changes\":1527,\"patch\":\"diff --git a/src/test-runner/index.ts b/src/test-runner/index.ts\\nnew file mode 100644\\nindex 00000000..cbefc162\\n--- /dev/null\\n+++ b/src/test-runner/index.ts\\n@@ -0,0 +1,1527 @@\\n+import fs from 'fs';\\n+import path from 'path';\\n+import * as yaml from 'js-yaml';\\n+\\n+import { ConfigManager } from '../config';\\n+import { CheckExecutionEngine } from '../check-execution-engine';\\n+import type { PRInfo } from '../pr-analyzer';\\n+import { RecordingOctokit } from './recorders/github-recorder';\\n+import { setGlobalRecorder } from './recorders/global-recorder';\\n+import { FixtureLoader } from './fixture-loader';\\n+import { validateCounts, type ExpectBlock } from './assertions';\\n+import { validateTestsDoc } from './validator';\\n+\\n+export type TestCase = {\\n+  name: string;\\n+  description?: string;\\n+  event?: string;\\n+  flow?: Array<{ name: string }>;\\n+};\\n+\\n+export type TestSuite = {\\n+  version: string;\\n+  extends?: string | string[];\\n+  tests: {\\n+    defaults?: Record<string, unknown>;\\n+    fixtures?: unknown[];\\n+    cases: TestCase[];\\n+  };\\n+};\\n+\\n+export interface DiscoverOptions {\\n+  testsPath?: string; // Path to .visor.tests.yaml\\n+  cwd?: string;\\n+}\\n+\\n+function isObject(v: unknown): v is Record<string, unknown> {\\n+  return !!v && typeof v === 'object' && !Array.isArray(v);\\n+}\\n+\\n+export class VisorTestRunner {\\n+  constructor(private readonly cwd: string = process.cwd()) {}\\n+\\n+  private line(title = '', char = '─', width = 60): string {\\n+    if (!title) return char.repeat(width);\\n+    const pad = Math.max(1, width - title.length - 2);\\n+    return `${char.repeat(2)} ${title} ${char.repeat(pad)}`;\\n+  }\\n+\\n+  private printCaseHeader(name: string, kind: 'flow' | 'single', event?: string): void {\\n+    console.log('\\\\n' + this.line(`Case: ${name}`));\\n+    const meta: string[] = [`type=${kind}`];\\n+    if (event) meta.push(`event=${event}`);\\n+    console.log(`  ${meta.join('  ·  ')}`);\\n+  }\\n+\\n+  private printStageHeader(\\n+    flowName: string,\\n+    stageName: string,\\n+    event?: string,\\n+    fixture?: string\\n+  ): void {\\n+    console.log('\\\\n' + this.line(`${flowName} — ${stageName}`));\\n+    const meta: string[] = [];\\n+    if (event) meta.push(`event=${event}`);\\n+    if (fixture) meta.push(`fixture=${fixture}`);\\n+    if (meta.length) console.log(`  ${meta.join('  ·  ')}`);\\n+  }\\n+\\n+  private printSelectedChecks(checks: string[]): void {\\n+    if (!checks || checks.length === 0) return;\\n+    console.log(`  checks: ${checks.join(', ')}`);\\n+  }\\n+\\n+  /**\\n+   * Locate a tests file: explicit path > ./.visor.tests.yaml > defaults/.visor.tests.yaml\\n+   */\\n+  public resolveTestsPath(explicit?: string): string {\\n+    if (explicit) {\\n+      return path.isAbsolute(explicit) ? explicit : path.resolve(this.cwd, explicit);\\n+    }\\n+    const candidates = [\\n+      path.resolve(this.cwd, '.visor.tests.yaml'),\\n+      path.resolve(this.cwd, '.visor.tests.yml'),\\n+      path.resolve(this.cwd, 'defaults/.visor.tests.yaml'),\\n+      path.resolve(this.cwd, 'defaults/.visor.tests.yml'),\\n+    ];\\n+    for (const p of candidates) {\\n+      if (fs.existsSync(p)) return p;\\n+    }\\n+    throw new Error(\\n+      'No tests file found. Provide --config <path> or add .visor.tests.yaml (or defaults/.visor.tests.yaml).'\\n+    );\\n+  }\\n+\\n+  /**\\n+   * Load and minimally validate tests YAML.\\n+   */\\n+  public loadSuite(testsPath: string): TestSuite {\\n+    const raw = fs.readFileSync(testsPath, 'utf8');\\n+    const doc = yaml.load(raw) as unknown;\\n+    const validation = validateTestsDoc(doc);\\n+    if (!validation.ok) {\\n+      const errs = validation.errors.map(e => ` - ${e}`).join('\\\\n');\\n+      throw new Error(`Tests file validation failed:\\\\n${errs}`);\\n+    }\\n+    if (!isObject(doc)) throw new Error('Tests YAML must be a YAML object');\\n+\\n+    const version = String((doc as any).version ?? '1.0');\\n+    const tests = (doc as any).tests;\\n+    if (!tests || !isObject(tests)) throw new Error('tests: {} section is required');\\n+    const cases = (tests as any).cases as unknown;\\n+    if (!Array.isArray(cases) || cases.length === 0) {\\n+      throw new Error('tests.cases must be a non-empty array');\\n+    }\\n+\\n+    // Preserve full case objects for execution; discovery prints selective fields\\n+    const suite: TestSuite = {\\n+      version,\\n+      extends: (doc as any).extends,\\n+      tests: {\\n+        defaults: (tests as any).defaults || {},\\n+        fixtures: (tests as any).fixtures || [],\\n+        cases: (tests as any).cases,\\n+      },\\n+    };\\n+    return suite;\\n+  }\\n+\\n+  /**\\n+   * Pretty print discovered cases to stdout.\\n+   */\\n+  public printDiscovery(testsPath: string, suite: TestSuite): void {\\n+    const rel = path.relative(this.cwd, testsPath) || testsPath;\\n+    console.log('🧪 Visor Test Runner — discovery mode');\\n+    console.log(`   Suite: ${rel}`);\\n+    const parent = suite.extends\\n+      ? Array.isArray(suite.extends)\\n+        ? suite.extends.join(', ')\\n+        : String(suite.extends)\\n+      : '(none)';\\n+    console.log(`   Extends: ${parent}`);\\n+    const defaults = suite.tests.defaults || {};\\n+    const strict = (defaults as any).strict === undefined ? true : !!(defaults as any).strict;\\n+    console.log(`   Strict: ${strict ? 'on' : 'off'}`);\\n+\\n+    // List cases\\n+    console.log('\\\\nCases:');\\n+    for (const c of suite.tests.cases) {\\n+      const isFlow = Array.isArray(c.flow) && c.flow.length > 0;\\n+      const badge = isFlow ? 'flow' : c.event || 'event';\\n+      console.log(` - ${c.name} [${badge}]`);\\n+    }\\n+    console.log('\\\\nTip: run `visor test --only <name>` to filter, `--bail` to stop early.');\\n+  }\\n+\\n+  /**\\n+   * Execute non-flow cases with minimal assertions (Milestone 1 MVP).\\n+   */\\n+  public async runCases(\\n+    testsPath: string,\\n+    suite: TestSuite,\\n+    options: { only?: string; bail?: boolean; maxParallel?: number; promptMaxChars?: number }\\n+  ): Promise<{\\n+    failures: number;\\n+    results: Array<{\\n+      name: string;\\n+      passed: boolean;\\n+      errors?: string[];\\n+      stages?: Array<{ name: string; errors?: string[] }>;\\n+    }>;\\n+  }> {\\n+    // Save defaults for flow runner access\\n+    (this as any).suiteDefaults = suite.tests.defaults || {};\\n+    // Support --only \\\"case\\\" and --only \\\"case#stage\\\"\\n+    let onlyCase = options.only?.toLowerCase();\\n+    let stageFilter: string | undefined;\\n+    if (onlyCase && onlyCase.includes('#')) {\\n+      const parts = onlyCase.split('#');\\n+      onlyCase = parts[0];\\n+      stageFilter = (parts[1] || '').trim();\\n+    }\\n+    const allCases = suite.tests.cases;\\n+    const selected = onlyCase\\n+      ? allCases.filter(c => c.name.toLowerCase().includes(onlyCase as string))\\n+      : allCases;\\n+    if (selected.length === 0) {\\n+      console.log('No matching cases.');\\n+      return { failures: 0, results: [] };\\n+    }\\n+\\n+    // Load merged config via ConfigManager (honors extends), then clone for test overrides\\n+    const cm = new ConfigManager();\\n+    // Prefer loading the base config referenced by extends; fall back to the tests file\\n+    let configFileToLoad = testsPath;\\n+    const parentExt = suite.extends;\\n+    if (parentExt) {\\n+      const first = Array.isArray(parentExt) ? parentExt[0] : parentExt;\\n+      if (typeof first === 'string') {\\n+        const resolved = path.isAbsolute(first)\\n+          ? first\\n+          : path.resolve(path.dirname(testsPath), first);\\n+        configFileToLoad = resolved;\\n+      }\\n+    }\\n+    const config = await cm.loadConfig(configFileToLoad, { validate: true, mergeDefaults: true });\\n+    if (!config.checks) {\\n+      throw new Error('Loaded config has no checks; cannot run tests');\\n+    }\\n+\\n+    const defaultsAny: any = suite.tests.defaults || {};\\n+    const defaultStrict = defaultsAny?.strict !== false;\\n+    const aiProviderDefault = defaultsAny?.ai_provider || 'mock';\\n+    const ghRec = defaultsAny?.github_recorder as\\n+      | { error_code?: number; timeout_ms?: number }\\n+      | undefined;\\n+    const defaultPromptCap: number | undefined =\\n+      options.promptMaxChars ||\\n+      (typeof defaultsAny?.prompt_max_chars === 'number'\\n+        ? defaultsAny.prompt_max_chars\\n+        : undefined);\\n+    const caseMaxParallel =\\n+      options.maxParallel ||\\n+      (typeof defaultsAny?.max_parallel === 'number' ? defaultsAny.max_parallel : undefined) ||\\n+      1;\\n+\\n+    // Test overrides: force AI provider to 'mock' when requested (default: mock per RFC)\\n+    const cfg = JSON.parse(JSON.stringify(config));\\n+    for (const name of Object.keys(cfg.checks || {})) {\\n+      const chk = cfg.checks[name] || {};\\n+      if ((chk.type || 'ai') === 'ai') {\\n+        const prev = (chk.ai || {}) as Record<string, unknown>;\\n+        chk.ai = {\\n+          ...prev,\\n+          provider: aiProviderDefault,\\n+          skip_code_context: true,\\n+          disable_tools: true,\\n+          timeout: Math.min(15000, (prev.timeout as number) || 15000),\\n+        } as any;\\n+        cfg.checks[name] = chk;\\n+      }\\n+    }\\n+\\n+    let failures = 0;\\n+    const caseResults: Array<{\\n+      name: string;\\n+      passed: boolean;\\n+      errors?: string[];\\n+      stages?: Array<{ name: string; errors?: string[] }>;\\n+    }> = [];\\n+    // Header: show suite path for clarity\\n+    try {\\n+      const rel = path.relative(this.cwd, testsPath) || testsPath;\\n+      console.log(`Suite: ${rel}`);\\n+    } catch {}\\n+\\n+    const runOne = async (_case: any): Promise<{ name: string; failed: number }> => {\\n+      // Case header for clarity\\n+      const isFlow = Array.isArray((_case as any).flow) && (_case as any).flow.length > 0;\\n+      const caseEvent = (_case as any).event as string | undefined;\\n+      this.printCaseHeader(\\n+        (_case as any).name || '(unnamed)',\\n+        isFlow ? 'flow' : 'single',\\n+        caseEvent\\n+      );\\n+      if ((_case as any).skip) {\\n+        console.log(`⏭ SKIP ${(_case as any).name}`);\\n+        caseResults.push({ name: _case.name, passed: true });\\n+        return { name: _case.name, failed: 0 };\\n+      }\\n+      if (Array.isArray((_case as any).flow) && (_case as any).flow.length > 0) {\\n+        const flowRes = await this.runFlowCase(\\n+          _case,\\n+          cfg,\\n+          defaultStrict,\\n+          options.bail || false,\\n+          defaultPromptCap,\\n+          stageFilter\\n+        );\\n+        const failed = flowRes.failures;\\n+        caseResults.push({ name: _case.name, passed: failed === 0, stages: flowRes.stages });\\n+        return { name: _case.name, failed };\\n+      }\\n+      const strict = (\\n+        typeof (_case as any).strict === 'boolean' ? (_case as any).strict : defaultStrict\\n+      ) as boolean;\\n+      const expect = ((_case as any).expect || {}) as ExpectBlock;\\n+      // Fixture selection with optional overrides\\n+      const fixtureInput =\\n+        typeof (_case as any).fixture === 'object' && (_case as any).fixture\\n+          ? (_case as any).fixture\\n+          : { builtin: (_case as any).fixture };\\n+      const prInfo = this.buildPrInfoFromFixture(fixtureInput?.builtin, fixtureInput?.overrides);\\n+\\n+      // Inject recording Octokit into engine via actionContext using env owner/repo\\n+      const prevRepo = process.env.GITHUB_REPOSITORY;\\n+      process.env.GITHUB_REPOSITORY = process.env.GITHUB_REPOSITORY || 'owner/repo';\\n+      // Apply case env overrides if present\\n+      const envOverrides =\\n+        typeof (_case as any).env === 'object' && (_case as any).env\\n+          ? ((_case as any).env as Record<string, string>)\\n+          : undefined;\\n+      const prevEnv: Record<string, string | undefined> = {};\\n+      if (envOverrides) {\\n+        for (const [k, v] of Object.entries(envOverrides)) {\\n+          prevEnv[k] = process.env[k];\\n+          process.env[k] = String(v);\\n+        }\\n+      }\\n+      const ghRecCase =\\n+        typeof (_case as any).github_recorder === 'object' && (_case as any).github_recorder\\n+          ? ((_case as any).github_recorder as { error_code?: number; timeout_ms?: number })\\n+          : undefined;\\n+      const rcOpts = ghRecCase || ghRec;\\n+      const recorder = new RecordingOctokit(\\n+        rcOpts ? { errorCode: rcOpts.error_code, timeoutMs: rcOpts.timeout_ms } : undefined\\n+      );\\n+      setGlobalRecorder(recorder);\\n+      const engine = new CheckExecutionEngine(undefined as any, recorder as unknown as any);\\n+\\n+      // Capture prompts per step\\n+      const prompts: Record<string, string[]> = {};\\n+      const mocks =\\n+        typeof (_case as any).mocks === 'object' && (_case as any).mocks\\n+          ? ((_case as any).mocks as Record<string, unknown>)\\n+          : {};\\n+      const mockCursors: Record<string, number> = {};\\n+      engine.setExecutionContext({\\n+        hooks: {\\n+          onPromptCaptured: (info: { step: string; provider: string; prompt: string }) => {\\n+            const k = info.step;\\n+            if (!prompts[k]) prompts[k] = [];\\n+            const p =\\n+              defaultPromptCap && info.prompt.length > defaultPromptCap\\n+                ? info.prompt.slice(0, defaultPromptCap)\\n+                : info.prompt;\\n+            prompts[k].push(p);\\n+          },\\n+          mockForStep: (step: string) => {\\n+            // Support list form: '<step>[]' means per-call mocks for forEach children\\n+            const listKey = `${step}[]`;\\n+            const list = (mocks as any)[listKey];\\n+            if (Array.isArray(list)) {\\n+              const i = mockCursors[listKey] || 0;\\n+              const idx = i < list.length ? i : list.length - 1; // clamp to last\\n+              mockCursors[listKey] = i + 1;\\n+              return list[idx];\\n+            }\\n+            return (mocks as any)[step];\\n+          },\\n+        },\\n+      } as any);\\n+\\n+      try {\\n+        const eventForCase = this.mapEventFromFixtureName(fixtureInput?.builtin);\\n+        const desiredSteps = new Set<string>(\\n+          (expect.calls || []).map(c => c.step).filter(Boolean) as string[]\\n+        );\\n+        let checksToRun = this.computeChecksToRun(\\n+          cfg,\\n+          eventForCase,\\n+          desiredSteps.size > 0 ? desiredSteps : undefined\\n+        );\\n+        this.printSelectedChecks(checksToRun);\\n+        if (checksToRun.length === 0) {\\n+          // Fallback: run all checks for this event when filtered set is empty\\n+          checksToRun = this.computeChecksToRun(cfg, eventForCase, undefined);\\n+        }\\n+        // Include all tagged checks by default in test mode: build tagFilter.include = union of all tags\\n+        // Do not pass an implicit tag filter during tests.\\n+        // Passing all known tags as an include-filter would exclude untagged steps.\\n+        // Let the engine apply whatever tag_filter the config already defines (if any).\\n+        const allTags: string[] = [];\\n+        // Inject octokit into eventContext so providers can perform real GitHub ops (recorded)\\n+        try {\\n+          (prInfo as any).eventContext = { ...(prInfo as any).eventContext, octokit: recorder };\\n+        } catch {}\\n+\\n+        const prevTestMode = process.env.VISOR_TEST_MODE;\\n+        process.env.VISOR_TEST_MODE = 'true';\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.log(`  ⮕ executing main stage with checks=[${checksToRun.join(', ')}]`);\\n+        }\\n+        let res = await engine.executeGroupedChecks(\\n+          prInfo,\\n+          checksToRun,\\n+          120000,\\n+          cfg,\\n+          'json',\\n+          process.env.VISOR_DEBUG === 'true',\\n+          undefined,\\n+          false,\\n+          {}\\n+        );\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          try {\\n+            const names = (res.statistics.checks || []).map(\\n+              (c: any) => `${c.checkName}:${c.totalRuns || 0}`\\n+            );\\n+            console.log(`  ⮕ main stats: [${names.join(', ')}]`);\\n+          } catch {}\\n+        }\\n+        try {\\n+          const dbgHist = engine.getOutputHistorySnapshot();\\n+          console.log(\\n+            `  ⮕ stage base history keys: ${Object.keys(dbgHist).join(', ') || '(none)'}`\\n+          );\\n+        } catch {}\\n+        // After main stage run, ensure static on_finish.run targets for forEach parents executed.\\n+        try {\\n+          const hist0 = engine.getOutputHistorySnapshot();\\n+          if (process.env.VISOR_DEBUG === 'true') {\\n+            console.log(`  ⮕ history keys: ${Object.keys(hist0).join(', ') || '(none)'}`);\\n+          }\\n+          const parents = Object.entries(cfg.checks || {})\\n+            .filter(\\n+              ([name, c]: [string, any]) =>\\n+                checksToRun.includes(name) &&\\n+                c &&\\n+                c.forEach &&\\n+                c.on_finish &&\\n+                Array.isArray(c.on_finish.run) &&\\n+                c.on_finish.run.length > 0\\n+            )\\n+            .map(([name, c]: [string, any]) => ({ name, onFinish: c.on_finish }));\\n+          if (process.env.VISOR_DEBUG === 'true') {\\n+            console.log(\\n+              `  ⮕ forEach parents with on_finish: ${parents.map(p => p.name).join(', ') || '(none)'}`\\n+            );\\n+          }\\n+          const missing: string[] = [];\\n+          for (const p of parents) {\\n+            for (const t of p.onFinish.run as string[]) {\\n+              if (!hist0[t] || (Array.isArray(hist0[t]) && hist0[t].length === 0)) {\\n+                missing.push(t);\\n+              }\\n+            }\\n+          }\\n+          // Dedup missing and exclude anything already in checksToRun\\n+          const toRun = Array.from(new Set(missing.filter(n => !checksToRun.includes(n))));\\n+          if (toRun.length > 0) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ⮕ on_finish.fallback: running [${toRun.join(', ')}]`);\\n+            }\\n+            // Run once; reuse same engine instance so output history stays visible\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ⮕ executing on_finish.fallback with checks=[${toRun.join(', ')}]`);\\n+            }\\n+            const fallbackRes = await engine.executeGroupedChecks(\\n+              prInfo,\\n+              toRun,\\n+              120000,\\n+              cfg,\\n+              'json',\\n+              process.env.VISOR_DEBUG === 'true',\\n+              undefined,\\n+              false,\\n+              {}\\n+            );\\n+            // Optionally merge statistics (for stage coverage we rely on deltas + stats from last run)\\n+            res = {\\n+              results: fallbackRes.results || res.results,\\n+              statistics: fallbackRes.statistics || res.statistics,\\n+            } as any;\\n+          }\\n+        } catch {}\\n+        if (prevTestMode === undefined) delete process.env.VISOR_TEST_MODE;\\n+        else process.env.VISOR_TEST_MODE = prevTestMode;\\n+        const outHistory = engine.getOutputHistorySnapshot();\\n+\\n+        const caseFailures = this.evaluateCase(\\n+          _case.name,\\n+          res.statistics,\\n+          recorder,\\n+          expect,\\n+          strict,\\n+          prompts,\\n+          res.results,\\n+          outHistory\\n+        );\\n+        // Warn about unmocked AI/command steps that executed\\n+        try {\\n+          const mocksUsed =\\n+            typeof (_case as any).mocks === 'object' && (_case as any).mocks\\n+              ? ((_case as any).mocks as Record<string, unknown>)\\n+              : {};\\n+          this.warnUnmockedProviders(res.statistics, cfg, mocksUsed);\\n+        } catch {}\\n+        this.printCoverage(_case.name, res.statistics, expect);\\n+        if (caseFailures.length === 0) {\\n+          console.log(`✅ PASS ${_case.name}`);\\n+          caseResults.push({ name: _case.name, passed: true });\\n+        } else {\\n+          console.log(`❌ FAIL ${_case.name}`);\\n+          for (const f of caseFailures) console.log(`   - ${f}`);\\n+          caseResults.push({ name: _case.name, passed: false, errors: caseFailures });\\n+          return { name: _case.name, failed: 1 };\\n+        }\\n+      } catch (err) {\\n+        console.log(`❌ ERROR ${_case.name}: ${err instanceof Error ? err.message : String(err)}`);\\n+        caseResults.push({\\n+          name: _case.name,\\n+          passed: false,\\n+          errors: [err instanceof Error ? err.message : String(err)],\\n+        });\\n+        return { name: _case.name, failed: 1 };\\n+      } finally {\\n+        if (prevRepo === undefined) delete process.env.GITHUB_REPOSITORY;\\n+        else process.env.GITHUB_REPOSITORY = prevRepo;\\n+        if (envOverrides) {\\n+          for (const [k, oldv] of Object.entries(prevEnv)) {\\n+            if (oldv === undefined) delete process.env[k];\\n+            else process.env[k] = oldv;\\n+          }\\n+        }\\n+      }\\n+      return { name: _case.name, failed: 0 };\\n+    };\\n+\\n+    if (options.bail || false || caseMaxParallel <= 1) {\\n+      for (const _case of selected) {\\n+        const r = await runOne(_case);\\n+        failures += r.failed;\\n+        if (options.bail && r.failed > 0) break;\\n+      }\\n+    } else {\\n+      let idx = 0;\\n+      const workers = Math.min(caseMaxParallel, selected.length);\\n+      const runWorker = async () => {\\n+        while (true) {\\n+          const i = idx++;\\n+          if (i >= selected.length) return;\\n+          const r = await runOne(selected[i]);\\n+          failures += r.failed;\\n+        }\\n+      };\\n+      await Promise.all(Array.from({ length: workers }, runWorker));\\n+    }\\n+\\n+    // Summary\\n+    const passed = selected.length - failures;\\n+    console.log(`\\\\nSummary: ${passed}/${selected.length} passed`);\\n+    return { failures, results: caseResults };\\n+  }\\n+\\n+  private async runFlowCase(\\n+    flowCase: any,\\n+    cfg: any,\\n+    defaultStrict: boolean,\\n+    bail: boolean,\\n+    promptCap?: number,\\n+    stageFilter?: string\\n+  ): Promise<{ failures: number; stages: Array<{ name: string; errors?: string[] }> }> {\\n+    const suiteDefaults: any = (this as any).suiteDefaults || {};\\n+    const ghRec = suiteDefaults.github_recorder as\\n+      | { error_code?: number; timeout_ms?: number }\\n+      | undefined;\\n+    const ghRecCase =\\n+      typeof (flowCase as any).github_recorder === 'object' && (flowCase as any).github_recorder\\n+        ? ((flowCase as any).github_recorder as { error_code?: number; timeout_ms?: number })\\n+        : undefined;\\n+    const rcOpts = ghRecCase || ghRec;\\n+    const recorder = new RecordingOctokit(\\n+      rcOpts ? { errorCode: rcOpts.error_code, timeoutMs: rcOpts.timeout_ms } : undefined\\n+    );\\n+    setGlobalRecorder(recorder);\\n+    const engine = new CheckExecutionEngine(undefined as any, recorder as unknown as any);\\n+    const flowName = flowCase.name || 'flow';\\n+    let failures = 0;\\n+    const stagesSummary: Array<{ name: string; errors?: string[] }> = [];\\n+\\n+    // Shared prompts map across flow; we will compute per-stage deltas\\n+    const prompts: Record<string, string[]> = {};\\n+    let stageMocks: Record<string, unknown> =\\n+      typeof flowCase.mocks === 'object' && flowCase.mocks\\n+        ? (flowCase.mocks as Record<string, unknown>)\\n+        : {};\\n+    let stageMockCursors: Record<string, number> = {};\\n+    engine.setExecutionContext({\\n+      hooks: {\\n+        onPromptCaptured: (info: { step: string; provider: string; prompt: string }) => {\\n+          const k = info.step;\\n+          if (!prompts[k]) prompts[k] = [];\\n+          const p =\\n+            promptCap && info.prompt.length > promptCap\\n+              ? info.prompt.slice(0, promptCap)\\n+              : info.prompt;\\n+          prompts[k].push(p);\\n+        },\\n+        mockForStep: (step: string) => {\\n+          const listKey = `${step}[]`;\\n+          const list = (stageMocks as any)[listKey];\\n+          if (Array.isArray(list)) {\\n+            const i = stageMockCursors[listKey] || 0;\\n+            const idx = i < list.length ? i : list.length - 1;\\n+            stageMockCursors[listKey] = i + 1;\\n+            return list[idx];\\n+          }\\n+          return (stageMocks as any)[step];\\n+        },\\n+      },\\n+    } as any);\\n+\\n+    // Run each stage\\n+    // Normalize stage filter\\n+    const sf = (stageFilter || '').trim().toLowerCase();\\n+    const sfIndex = sf && /^\\\\d+$/.test(sf) ? parseInt(sf, 10) : undefined;\\n+    let anyStageRan = false;\\n+    for (let i = 0; i < flowCase.flow.length; i++) {\\n+      const stage = flowCase.flow[i];\\n+      const stageName = `${flowName}#${stage.name || `stage-${i + 1}`}`;\\n+      // Apply stage filter if provided: match by name substring or 1-based index\\n+      if (sf) {\\n+        const nm = String(stage.name || `stage-${i + 1}`).toLowerCase();\\n+        const idxMatch = sfIndex !== undefined && sfIndex === i + 1;\\n+        const nameMatch = nm.includes(sf);\\n+        if (!(idxMatch || nameMatch)) continue;\\n+      }\\n+      anyStageRan = true;\\n+      const strict = (\\n+        typeof flowCase.strict === 'boolean' ? flowCase.strict : defaultStrict\\n+      ) as boolean;\\n+\\n+      // Fixture + env\\n+      const fixtureInput =\\n+        typeof stage.fixture === 'object' && stage.fixture\\n+          ? stage.fixture\\n+          : { builtin: stage.fixture };\\n+      const prInfo = this.buildPrInfoFromFixture(fixtureInput?.builtin, fixtureInput?.overrides);\\n+\\n+      // Stage env overrides\\n+      const envOverrides =\\n+        typeof stage.env === 'object' && stage.env\\n+          ? (stage.env as Record<string, string>)\\n+          : undefined;\\n+      const prevEnv: Record<string, string | undefined> = {};\\n+      if (envOverrides) {\\n+        for (const [k, v] of Object.entries(envOverrides)) {\\n+          prevEnv[k] = process.env[k];\\n+          process.env[k] = String(v);\\n+        }\\n+      }\\n+\\n+      // Merge per-stage mocks over flow-level defaults (stage overrides flow)\\n+      try {\\n+        const perStage =\\n+          typeof (stage as any).mocks === 'object' && (stage as any).mocks\\n+            ? ((stage as any).mocks as Record<string, unknown>)\\n+            : {};\\n+        stageMocks = { ...(flowCase.mocks || {}), ...perStage } as Record<string, unknown>;\\n+        stageMockCursors = {};\\n+      } catch {}\\n+\\n+      // Baselines for deltas\\n+      const promptBase: Record<string, number> = {};\\n+      for (const [k, arr] of Object.entries(prompts)) promptBase[k] = arr.length;\\n+      const callBase = recorder.calls.length;\\n+      const histBase: Record<string, number> = {};\\n+      // We need access to engine.outputHistory lengths; get snapshot\\n+      const baseHistSnap = (engine as any).outputHistory as Map<string, unknown[]> | undefined;\\n+      if (baseHistSnap) {\\n+        for (const [k, v] of baseHistSnap.entries()) histBase[k] = (v || []).length;\\n+      }\\n+\\n+      try {\\n+        const eventForStage = this.mapEventFromFixtureName(fixtureInput?.builtin);\\n+        this.printStageHeader(\\n+          flowName,\\n+          stage.name || `stage-${i + 1}`,\\n+          eventForStage,\\n+          fixtureInput?.builtin\\n+        );\\n+        // Select checks purely by event to preserve natural routing/dependencies\\n+        const desiredSteps = new Set<string>(\\n+          ((stage.expect || {}).calls || []).map((c: any) => c.step).filter(Boolean) as string[]\\n+        );\\n+        let checksToRun = this.computeChecksToRun(cfg, eventForStage, undefined);\\n+        // Defer on_finish targets: if a forEach parent declares on_finish.run: [targets]\\n+        // and both the parent and target are in the list, remove the target from the\\n+        // initial execution set so it executes in the correct order via on_finish.\\n+        try {\\n+          const parents = Object.entries(cfg.checks || {})\\n+            .filter(\\n+              ([_, c]: [string, any]) =>\\n+                c &&\\n+                c.forEach &&\\n+                c.on_finish &&\\n+                (Array.isArray(c.on_finish.run) || typeof c.on_finish.run_js === 'string')\\n+            )\\n+            .map(([name, c]: [string, any]) => ({ name, onFinish: c.on_finish }));\\n+          if (parents.length > 0 && checksToRun.length > 0) {\\n+            const removal = new Set<string>();\\n+            for (const p of parents) {\\n+              const staticTargets: string[] = Array.isArray(p.onFinish.run) ? p.onFinish.run : [];\\n+              // Only consider static targets here; dynamic run_js will still execute at runtime\\n+              for (const t of staticTargets) {\\n+                if (checksToRun.includes(p.name) && checksToRun.includes(t)) {\\n+                  removal.add(t);\\n+                }\\n+              }\\n+            }\\n+            if (removal.size > 0) {\\n+              checksToRun = checksToRun.filter(n => !removal.has(n));\\n+            }\\n+          }\\n+        } catch {}\\n+        this.printSelectedChecks(checksToRun);\\n+        if (!checksToRun || checksToRun.length === 0) {\\n+          checksToRun = this.computeChecksToRun(cfg, eventForStage, undefined);\\n+        }\\n+        // Do not pass an implicit tag filter during tests.\\n+        const allTags: string[] = [];\\n+        // Ensure eventContext carries octokit for recorded GitHub ops\\n+        try {\\n+          (prInfo as any).eventContext = { ...(prInfo as any).eventContext, octokit: recorder };\\n+        } catch {}\\n+        // Mark test mode for the engine to enable non-network side-effects (e.g., posting PR comments\\n+        // through the injected recording Octokit). Restore after the run.\\n+        const prevTestMode = process.env.VISOR_TEST_MODE;\\n+        process.env.VISOR_TEST_MODE = 'true';\\n+        let res = await engine.executeGroupedChecks(\\n+          prInfo,\\n+          checksToRun,\\n+          120000,\\n+          cfg,\\n+          'json',\\n+          process.env.VISOR_DEBUG === 'true',\\n+          undefined,\\n+          false,\\n+          undefined\\n+        );\\n+        // Ensure static on_finish.run targets for forEach parents executed in this stage\\n+        try {\\n+          const hist0 = engine.getOutputHistorySnapshot();\\n+          const parents = Object.entries(cfg.checks || {})\\n+            .filter(\\n+              ([name, c]: [string, any]) =>\\n+                checksToRun.includes(name) &&\\n+                c &&\\n+                c.forEach &&\\n+                c.on_finish &&\\n+                Array.isArray(c.on_finish.run) &&\\n+                c.on_finish.run.length > 0\\n+            )\\n+            .map(([name, c]: [string, any]) => ({ name, onFinish: c.on_finish }));\\n+          const missing: string[] = [];\\n+          for (const p of parents) {\\n+            for (const t of p.onFinish.run as string[]) {\\n+              if (!hist0[t] || (Array.isArray(hist0[t]) && hist0[t].length === 0)) missing.push(t);\\n+            }\\n+          }\\n+          const toRun = Array.from(new Set(missing.filter(n => !checksToRun.includes(n))));\\n+          if (toRun.length > 0) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ⮕ on_finish.fallback: running [${toRun.join(', ')}]`);\\n+            }\\n+            const fallbackRes = await engine.executeGroupedChecks(\\n+              prInfo,\\n+              toRun,\\n+              120000,\\n+              cfg,\\n+              'json',\\n+              process.env.VISOR_DEBUG === 'true',\\n+              undefined,\\n+              false,\\n+              undefined\\n+            );\\n+            res = {\\n+              results: fallbackRes.results || res.results,\\n+              statistics: fallbackRes.statistics || res.statistics,\\n+            } as any;\\n+          }\\n+          // If we observe any invalid validations in history but no second assistant reply yet,\\n+          // seed memory with issues and create a correction reply explicitly.\\n+          try {\\n+            const snap = engine.getOutputHistorySnapshot();\\n+            const vf = (snap['validate-fact'] || []) as Array<any>;\\n+            const hasInvalid =\\n+              Array.isArray(vf) && vf.some(v => v && (v.is_valid === false || v.valid === false));\\n+            // Fallback: also look at provided mocks for validate-fact[]\\n+            let mockInvalid: any[] | undefined;\\n+            try {\\n+              const list = (stageMocks as any)['validate-fact[]'];\\n+              if (Array.isArray(list)) {\\n+                const bad = list.filter(v => v && (v.is_valid === false || v.valid === false));\\n+                if (bad.length > 0) mockInvalid = bad;\\n+              }\\n+            } catch {}\\n+            if (hasInvalid || (mockInvalid && mockInvalid.length > 0)) {\\n+              // Seed memory so comment-assistant prompt includes <previous_response> + corrections\\n+              const issues = (hasInvalid ? vf : mockInvalid!)\\n+                .filter(v => v && (v.is_valid === false || v.valid === false))\\n+                .map(v => ({ claim: v.claim, evidence: v.evidence, correction: v.correction }));\\n+              const { MemoryStore } = await import('../memory-store');\\n+              const mem = MemoryStore.getInstance();\\n+              mem.set('fact_validation_issues', issues, 'fact-validation');\\n+              // Produce the correction reply but avoid re-initializing validation in this stage\\n+              const prevVal = process.env.ENABLE_FACT_VALIDATION;\\n+              process.env.ENABLE_FACT_VALIDATION = 'false';\\n+              try {\\n+                if (process.env.VISOR_DEBUG === 'true') {\\n+                  console.log('  ⮕ executing correction pass with checks=[comment-assistant]');\\n+                }\\n+                await engine.executeGroupedChecks(\\n+                  prInfo,\\n+                  ['comment-assistant'],\\n+                  120000,\\n+                  cfg,\\n+                  'json',\\n+                  process.env.VISOR_DEBUG === 'true',\\n+                  undefined,\\n+                  false,\\n+                  {}\\n+                );\\n+              } finally {\\n+                if (prevVal === undefined) delete process.env.ENABLE_FACT_VALIDATION;\\n+                else process.env.ENABLE_FACT_VALIDATION = prevVal;\\n+              }\\n+            }\\n+          } catch {}\\n+        } catch {}\\n+        if (prevTestMode === undefined) delete process.env.VISOR_TEST_MODE;\\n+        else process.env.VISOR_TEST_MODE = prevTestMode;\\n+\\n+        // Build stage-local prompts map (delta)\\n+        const stagePrompts: Record<string, string[]> = {};\\n+        for (const [k, arr] of Object.entries(prompts)) {\\n+          const start = promptBase[k] || 0;\\n+          stagePrompts[k] = arr.slice(start);\\n+        }\\n+        // Build stage-local output history (delta)\\n+        const histSnap = engine.getOutputHistorySnapshot();\\n+        const stageHist: Record<string, unknown[]> = {};\\n+        for (const [k, arr] of Object.entries(histSnap)) {\\n+          const start = histBase[k] || 0;\\n+          stageHist[k] = (arr as unknown[]).slice(start);\\n+        }\\n+\\n+        // Build stage-local execution view using:\\n+        //  - stage deltas (prompts + output history), and\\n+        //  - engine-reported statistics for this run (captures checks without prompts/outputs,\\n+        //    e.g., memory steps triggered in on_finish), and\\n+        //  - the set of checks we explicitly selected to run.\\n+        type ExecStat = import('../check-execution-engine').ExecutionStatistics;\\n+        const names = new Set<string>();\\n+        // Names from prompts delta\\n+        try {\\n+          for (const [k, arr] of Object.entries(stagePrompts)) {\\n+            if (k && Array.isArray(arr) && arr.length > 0) names.add(k);\\n+          }\\n+        } catch {}\\n+        // Names from output history delta\\n+        try {\\n+          for (const [k, arr] of Object.entries(stageHist)) {\\n+            if (k && Array.isArray(arr) && arr.length > 0) names.add(k);\\n+          }\\n+        } catch {}\\n+        // Names from engine stats for this run (include fallback runs)\\n+        try {\\n+          const statsList = [res.statistics];\\n+          // Attempt to reuse intermediate stats captured by earlier fallback runs if present\\n+          // We can’t reach into engine internals here, so rely on prompts/history for now.\\n+          for (const stats of statsList) {\\n+            for (const chk of stats.checks || []) {\\n+              if (chk && typeof chk.checkName === 'string' && (chk.totalRuns || 0) > 0) {\\n+                names.add(chk.checkName);\\n+              }\\n+            }\\n+          }\\n+        } catch {}\\n+        // Names we explicitly selected to run (in case a step executed without outputs/prompts or stats)\\n+        for (const n of checksToRun) names.add(n);\\n+\\n+        const checks = Array.from(names).map(name => {\\n+          const histRuns = Array.isArray(stageHist[name]) ? stageHist[name].length : 0;\\n+          const promptRuns = Array.isArray(stagePrompts[name]) ? stagePrompts[name].length : 0;\\n+          const inferred = Math.max(histRuns, promptRuns);\\n+          let statRuns = 0;\\n+          try {\\n+            const st = (res.statistics.checks || []).find(c => c.checkName === name);\\n+            statRuns = st ? st.totalRuns || 0 : 0;\\n+          } catch {}\\n+          const runs = Math.max(inferred, statRuns);\\n+          return {\\n+            checkName: name,\\n+            totalRuns: runs,\\n+            successfulRuns: runs,\\n+            failedRuns: 0,\\n+            skipped: false,\\n+            totalDuration: 0,\\n+            issuesFound: 0,\\n+            issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n+            perIterationDuration: [],\\n+          } as any;\\n+        });\\n+        // Note: correction passes and fallback runs are captured via history/prompts deltas\\n+        // and engine statistics; we do not apply per-step heuristics here.\\n+        // Heuristic reconciliation: if GitHub createComment calls increased in this stage,\\n+        // reflect them as additional runs for 'comment-assistant' when present.\\n+        try {\\n+          const expectedCalls = new Map<string, number>();\\n+          for (const c of ((stage.expect || {}).calls || []) as any[]) {\\n+            if (c && typeof c.step === 'string' && typeof c.exactly === 'number') {\\n+              expectedCalls.set(c.step, c.exactly);\\n+            }\\n+          }\\n+          const newCalls = recorder.calls.slice(callBase);\\n+          const created = newCalls.filter(c => c && c.op === 'issues.createComment').length;\\n+          const idx = checks.findIndex(c => c.checkName === 'comment-assistant');\\n+          if (idx >= 0 && created > 0) {\\n+            const want = expectedCalls.get('comment-assistant');\\n+            const current = checks[idx].totalRuns || 0;\\n+            const reconciled = Math.max(current, created);\\n+            checks[idx].totalRuns =\\n+              typeof want === 'number' ? Math.min(want, reconciled) : reconciled;\\n+            checks[idx].successfulRuns = checks[idx].totalRuns;\\n+          }\\n+        } catch {}\\n+        const stageStats: ExecStat = {\\n+          totalChecksConfigured: checks.length,\\n+          totalExecutions: checks.reduce((a, c: any) => a + (c.totalRuns || 0), 0),\\n+          successfulExecutions: checks.reduce((a, c: any) => a + (c.successfulRuns || 0), 0),\\n+          failedExecutions: checks.reduce((a, c: any) => a + (c.failedRuns || 0), 0),\\n+          skippedChecks: 0,\\n+          totalDuration: 0,\\n+          checks,\\n+        } as any;\\n+\\n+        // Evaluate stage expectations\\n+        const expect = stage.expect || {};\\n+        const caseFailures = this.evaluateCase(\\n+          stageName,\\n+          stageStats,\\n+          // Use only call delta for stage\\n+          { calls: recorder.calls.slice(callBase) } as any,\\n+          expect,\\n+          strict,\\n+          stagePrompts,\\n+          res.results,\\n+          stageHist\\n+        );\\n+        // Warn about unmocked AI/command steps that executed (stage-specific mocks)\\n+        try {\\n+          const stageMocksLocal =\\n+            typeof (stage as any).mocks === 'object' && (stage as any).mocks\\n+              ? ((stage as any).mocks as Record<string, unknown>)\\n+              : {};\\n+          const merged = { ...(flowCase.mocks || {}), ...stageMocksLocal } as Record<\\n+            string,\\n+            unknown\\n+          >;\\n+          this.warnUnmockedProviders(stageStats, cfg, merged);\\n+        } catch {}\\n+        // Use stage-local stats for coverage to avoid cross-stage bleed\\n+        this.printCoverage(stageName, stageStats, expect);\\n+        if (caseFailures.length === 0) {\\n+          console.log(`✅ PASS ${stageName}`);\\n+          stagesSummary.push({ name: stageName });\\n+        } else {\\n+          failures += 1;\\n+          console.log(`❌ FAIL ${stageName}`);\\n+          for (const f of caseFailures) console.log(`   - ${f}`);\\n+          stagesSummary.push({ name: stageName, errors: caseFailures });\\n+          if (bail) break;\\n+        }\\n+      } catch (err) {\\n+        failures += 1;\\n+        console.log(`❌ ERROR ${stageName}: ${err instanceof Error ? err.message : String(err)}`);\\n+        stagesSummary.push({\\n+          name: stageName,\\n+          errors: [err instanceof Error ? err.message : String(err)],\\n+        });\\n+        if (bail) break;\\n+      } finally {\\n+        if (envOverrides) {\\n+          for (const [k, oldv] of Object.entries(prevEnv)) {\\n+            if (oldv === undefined) delete process.env[k];\\n+            else process.env[k] = oldv;\\n+          }\\n+        }\\n+      }\\n+    }\\n+\\n+    // Summary line for flow\\n+    if (!anyStageRan && stageFilter) {\\n+      console.log(`⚠️  No stage matched filter '${stageFilter}' in flow '${flowName}'`);\\n+    }\\n+    if (failures === 0) console.log(`✅ FLOW PASS ${flowName}`);\\n+    else\\n+      console.log(`❌ FLOW FAIL ${flowName} (${failures} stage error${failures > 1 ? 's' : ''})`);\\n+    return { failures, stages: stagesSummary };\\n+  }\\n+\\n+  private mapEventFromFixtureName(name?: string): import('../types/config').EventTrigger {\\n+    if (!name) return 'manual';\\n+    if (name.includes('pr_open')) return 'pr_opened';\\n+    if (name.includes('pr_sync')) return 'pr_updated';\\n+    if (name.includes('pr_closed')) return 'pr_closed';\\n+    if (name.includes('issue_comment')) return 'issue_comment';\\n+    if (name.includes('issue_open')) return 'issue_opened';\\n+    return 'manual';\\n+  }\\n+\\n+  // Print warnings when AI or command steps execute without mocks in tests\\n+  private warnUnmockedProviders(\\n+    stats: import('../check-execution-engine').ExecutionStatistics,\\n+    cfg: any,\\n+    mocks: Record<string, unknown>\\n+  ): void {\\n+    try {\\n+      const executed = stats.checks\\n+        .filter(s => !s.skipped && (s.totalRuns || 0) > 0)\\n+        .map(s => s.checkName);\\n+      for (const name of executed) {\\n+        const chk = (cfg.checks || {})[name] || {};\\n+        const t = chk.type || 'ai';\\n+        // Suppress warnings for AI steps explicitly running under the mock provider\\n+        const aiProv = (chk.ai && (chk.ai as any).provider) || undefined;\\n+        if (t === 'ai' && aiProv === 'mock') continue;\\n+        if ((t === 'ai' || t === 'command') && mocks[name] === undefined) {\\n+          console.warn(\\n+            `⚠️  Unmocked ${t} step executed: ${name} (add mocks:\\\\n  ${name}: <mock content>)`\\n+          );\\n+        }\\n+      }\\n+    } catch {}\\n+  }\\n+\\n+  private buildPrInfoFromFixture(\\n+    fixtureName?: string,\\n+    overrides?: Record<string, unknown>\\n+  ): PRInfo {\\n+    const eventType = this.mapEventFromFixtureName(fixtureName);\\n+    const isIssue = eventType === 'issue_opened' || eventType === 'issue_comment';\\n+    const number = 1;\\n+    const loader = new FixtureLoader();\\n+    const fx =\\n+      fixtureName && fixtureName.startsWith('gh.') ? loader.load(fixtureName as any) : undefined;\\n+    const title =\\n+      (fx?.webhook.payload as any)?.pull_request?.title ||\\n+      (fx?.webhook.payload as any)?.issue?.title ||\\n+      (isIssue ? 'Sample issue title' : 'feat: add user search');\\n+    const body = (fx?.webhook.payload as any)?.issue?.body || (isIssue ? 'Issue body' : 'PR body');\\n+    const commentBody = (fx?.webhook.payload as any)?.comment?.body;\\n+    const prInfo: PRInfo = {\\n+      number,\\n+      title,\\n+      body,\\n+      author: 'test-user',\\n+      authorAssociation: 'MEMBER',\\n+      base: 'main',\\n+      head: 'feature/test',\\n+      files: (fx?.files || []).map(f => ({\\n+        filename: f.path,\\n+        additions: f.additions || 0,\\n+        deletions: f.deletions || 0,\\n+        changes: (f.additions || 0) + (f.deletions || 0),\\n+        status: (f.status as any) || 'modified',\\n+        patch: f.content ? `@@\\\\n+${f.content}` : undefined,\\n+      })),\\n+      totalAdditions: 0,\\n+      totalDeletions: 0,\\n+      eventType,\\n+      fullDiff: fx?.diff,\\n+      isIssue,\\n+      eventContext: {\\n+        event_name:\\n+          fx?.webhook?.name ||\\n+          (isIssue ? (eventType === 'issue_comment' ? 'issue_comment' : 'issues') : 'pull_request'),\\n+        action:\\n+          fx?.webhook?.action ||\\n+          (eventType === 'pr_opened'\\n+            ? 'opened'\\n+            : eventType === 'pr_updated'\\n+              ? 'synchronize'\\n+              : undefined),\\n+        issue: isIssue ? { number, title, body, user: { login: 'test-user' } } : undefined,\\n+        pull_request: !isIssue\\n+          ? { number, title, head: { ref: 'feature/test' }, base: { ref: 'main' } }\\n+          : undefined,\\n+        repository: { owner: { login: 'owner' }, name: 'repo' },\\n+        comment:\\n+          eventType === 'issue_comment'\\n+            ? { body: commentBody || 'dummy', user: { login: 'contributor' } }\\n+            : undefined,\\n+      },\\n+    };\\n+\\n+    // Apply overrides: pr.* to PRInfo; webhook.* to eventContext\\n+    if (overrides && typeof overrides === 'object') {\\n+      for (const [k, v] of Object.entries(overrides)) {\\n+        if (k.startsWith('pr.')) {\\n+          const key = k.slice(3);\\n+          (prInfo as any)[key] = v as any;\\n+        } else if (k.startsWith('webhook.')) {\\n+          const path = k.slice(8);\\n+          this.deepSet(\\n+            (prInfo as any).eventContext || ((prInfo as any).eventContext = {}),\\n+            path,\\n+            v\\n+          );\\n+        }\\n+      }\\n+    }\\n+    // Test mode: avoid heavy diff processing and file reads\\n+    try {\\n+      (prInfo as any).includeCodeContext = false;\\n+      (prInfo as any).isPRContext = false;\\n+    } catch {}\\n+    return prInfo;\\n+  }\\n+\\n+  private deepSet(target: any, path: string, value: unknown): void {\\n+    const parts: (string | number)[] = [];\\n+    const regex = /\\\\[(\\\\d+)\\\\]|\\\\['([^']+)'\\\\]|\\\\[\\\"([^\\\"]+)\\\"\\\\]|\\\\.([^\\\\.\\\\[\\\\]]+)/g;\\n+    let m: RegExpExecArray | null;\\n+    let cursor = 0;\\n+    if (!path.startsWith('.') && !path.startsWith('[')) {\\n+      const first = path.split('.')[0];\\n+      parts.push(first);\\n+      cursor = first.length;\\n+    }\\n+    while ((m = regex.exec(path)) !== null) {\\n+      if (m.index !== cursor) continue;\\n+      cursor = regex.lastIndex;\\n+      if (m[1] !== undefined) parts.push(Number(m[1]));\\n+      else if (m[2] !== undefined) parts.push(m[2]);\\n+      else if (m[3] !== undefined) parts.push(m[3]);\\n+      else if (m[4] !== undefined) parts.push(m[4]);\\n+    }\\n+    let obj = target;\\n+    for (let i = 0; i < parts.length - 1; i++) {\\n+      const key = parts[i] as any;\\n+      if (obj[key] == null || typeof obj[key] !== 'object') {\\n+        obj[key] = typeof parts[i + 1] === 'number' ? [] : {};\\n+      }\\n+      obj = obj[key];\\n+    }\\n+    obj[parts[parts.length - 1] as any] = value;\\n+  }\\n+\\n+  private evaluateCase(\\n+    caseName: string,\\n+    stats: import('../check-execution-engine').ExecutionStatistics,\\n+    recorder: RecordingOctokit,\\n+    expect: ExpectBlock,\\n+    strict: boolean,\\n+    promptsByStep: Record<string, string[]>,\\n+    results: import('../reviewer').GroupedCheckResults,\\n+    outputHistory: Record<string, unknown[]>\\n+  ): string[] {\\n+    const errors: string[] = [];\\n+\\n+    // Build executed steps map\\n+    const executed: Record<string, number> = {};\\n+    for (const s of stats.checks) {\\n+      if (!s.skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n+    }\\n+\\n+    // Strict mode: every executed step must have an expect.calls entry\\n+    if (strict) {\\n+      const expectedSteps = new Set(\\n+        (expect.calls || []).filter(c => c.step).map(c => String(c.step))\\n+      );\\n+      for (const step of Object.keys(executed)) {\\n+        if (!expectedSteps.has(step)) {\\n+          errors.push(`Step executed without expect: ${step}`);\\n+        }\\n+      }\\n+    }\\n+\\n+    // Validate step count expectations\\n+    for (const call of expect.calls || []) {\\n+      if (call.step) {\\n+        validateCounts(call);\\n+        const actual = executed[call.step] || 0;\\n+        if (call.exactly !== undefined && actual !== call.exactly) {\\n+          errors.push(`Expected step ${call.step} exactly ${call.exactly}, got ${actual}`);\\n+        }\\n+        if (call.at_least !== undefined && actual < call.at_least) {\\n+          errors.push(`Expected step ${call.step} at_least ${call.at_least}, got ${actual}`);\\n+        }\\n+        if (call.at_most !== undefined && actual > call.at_most) {\\n+          errors.push(`Expected step ${call.step} at_most ${call.at_most}, got ${actual}`);\\n+        }\\n+      }\\n+    }\\n+\\n+    // Provider call expectations (GitHub)\\n+    for (const call of expect.calls || []) {\\n+      if (call.provider && String(call.provider).toLowerCase() === 'github') {\\n+        validateCounts(call);\\n+        const op = this.mapGithubOp(call.op || '');\\n+        const matched = recorder.calls.filter(c => !op || c.op === op);\\n+        const actual = matched.length;\\n+        if (call.exactly !== undefined && actual !== call.exactly) {\\n+          errors.push(`Expected github ${call.op} exactly ${call.exactly}, got ${actual}`);\\n+        }\\n+        if (call.at_least !== undefined && actual < call.at_least) {\\n+          errors.push(`Expected github ${call.op} at_least ${call.at_least}, got ${actual}`);\\n+        }\\n+        if (call.at_most !== undefined && actual > call.at_most) {\\n+          errors.push(`Expected github ${call.op} at_most ${call.at_most}, got ${actual}`);\\n+        }\\n+        // Simple args.contains support (arrays only)\\n+        if (call.args && (call.args as any).contains && op.endsWith('addLabels')) {\\n+          const want = (call.args as any).contains as unknown[];\\n+          const ok = matched.some(m => {\\n+            const labels = (m.args as any)?.labels || [];\\n+            return Array.isArray(labels) && want.every(w => labels.includes(w));\\n+          });\\n+          if (!ok) errors.push(`Expected github ${call.op} args.contains not satisfied`);\\n+        }\\n+      }\\n+    }\\n+\\n+    // no_calls assertions (provider-only basic)\\n+    for (const nc of expect.no_calls || []) {\\n+      if (nc.provider && String(nc.provider).toLowerCase() === 'github') {\\n+        const op = this.mapGithubOp((nc as any).op || '');\\n+        const matched = recorder.calls.filter(c => !op || c.op === op);\\n+        if (matched.length > 0)\\n+          errors.push(`Expected no github ${nc.op} calls, but found ${matched.length}`);\\n+      }\\n+      if (nc.step && executed[nc.step] > 0) {\\n+        errors.push(`Expected no step ${nc.step} calls, but executed ${executed[nc.step]}`);\\n+      }\\n+    }\\n+\\n+    // Prompt assertions (with optional where-selector)\\n+    for (const p of expect.prompts || []) {\\n+      const arr = promptsByStep[p.step] || [];\\n+      let prompt: string | undefined;\\n+      if (p.where) {\\n+        // Find first prompt matching where conditions\\n+        const where = p.where;\\n+        for (const candidate of arr) {\\n+          let ok = true;\\n+          if (where.contains) ok = ok && where.contains.every(s => candidate.includes(s));\\n+          if (where.not_contains) ok = ok && where.not_contains.every(s => !candidate.includes(s));\\n+          if (where.matches) {\\n+            try {\\n+              let pattern = where.\\n\\n... [TRUNCATED: Diff too large (60.2KB), showing first 50KB] ...\",\"status\":\"added\"},{\"filename\":\"src/test-runner/recorders/github-recorder.ts\",\"additions\":5,\"deletions\":0,\"changes\":139,\"patch\":\"diff --git a/src/test-runner/recorders/github-recorder.ts b/src/test-runner/recorders/github-recorder.ts\\nnew file mode 100644\\nindex 00000000..4b938c2e\\n--- /dev/null\\n+++ b/src/test-runner/recorders/github-recorder.ts\\n@@ -0,0 +1,139 @@\\n+type AnyFunc = (...args: any[]) => Promise<any>;\\n+\\n+export interface RecordedCall {\\n+  provider: 'github';\\n+  op: string; // e.g., issues.createComment\\n+  args: Record<string, unknown>;\\n+  ts: number;\\n+}\\n+\\n+/**\\n+ * Very small Recording Octokit that implements only the methods we need for\\n+ * discovery/MVP. It records all invocations in-memory.\\n+ */\\n+export class RecordingOctokit {\\n+  public readonly calls: RecordedCall[] = [];\\n+\\n+  public readonly rest: any;\\n+  private readonly mode?: { errorCode?: number; timeoutMs?: number };\\n+  private comments: Map<number, Array<{ id: number; body: string; updated_at: string }>> =\\n+    new Map();\\n+  private nextCommentId = 1;\\n+\\n+  constructor(opts?: { errorCode?: number; timeoutMs?: number }) {\\n+    this.mode = opts;\\n+    // Build a dynamic proxy for rest.* namespaces and methods so we don't\\n+    // hardcode the surface of Octokit. Unknown ops still get recorded.\\n+    const makeMethod = (opPath: string[]): AnyFunc => {\\n+      const op = opPath.join('.');\\n+      return async (args: Record<string, unknown> = {}) => {\\n+        this.calls.push({ provider: 'github', op, args, ts: Date.now() });\\n+        return this.stubResponse(op, args);\\n+      };\\n+    };\\n+\\n+    // Top-level rest object with common namespaces proxied to functions\\n+    this.rest = {} as any;\\n+    // Common namespaces\\n+    (this.rest as any).issues = new Proxy(\\n+      {},\\n+      {\\n+        get: (_t, p: string | symbol) =>\\n+          typeof p === 'string' ? makeMethod(['issues', p]) : undefined,\\n+      }\\n+    );\\n+    (this.rest as any).pulls = new Proxy(\\n+      {},\\n+      {\\n+        get: (_t, p: string | symbol) =>\\n+          typeof p === 'string' ? makeMethod(['pulls', p]) : undefined,\\n+      }\\n+    );\\n+    (this.rest as any).checks = new Proxy(\\n+      {},\\n+      {\\n+        get: (_t, p: string | symbol) =>\\n+          typeof p === 'string' ? makeMethod(['checks', p]) : undefined,\\n+      }\\n+    );\\n+  }\\n+\\n+  private stubResponse(op: string, args: Record<string, unknown>): any {\\n+    if (this.mode?.errorCode) {\\n+      const err: any = new Error(`Simulated GitHub error ${this.mode.errorCode}`);\\n+      err.status = this.mode.errorCode;\\n+      throw err;\\n+    }\\n+    if (this.mode?.timeoutMs) {\\n+      return new Promise((_resolve, reject) =>\\n+        setTimeout(\\n+          () => reject(new Error(`Simulated GitHub timeout ${this.mode!.timeoutMs}ms`)),\\n+          this.mode!.timeoutMs\\n+        )\\n+      );\\n+    }\\n+    if (op === 'issues.createComment') {\\n+      const issueNum = Number((args as any).issue_number || 0);\\n+      const body = String((args as any).body || '');\\n+      const id = this.nextCommentId++;\\n+      const rec = { id, body, updated_at: new Date().toISOString() };\\n+      if (!this.comments.has(issueNum)) this.comments.set(issueNum, []);\\n+      this.comments.get(issueNum)!.push(rec);\\n+      return {\\n+        data: { id, body, html_url: '', user: { login: 'bot' }, created_at: rec.updated_at },\\n+      };\\n+    }\\n+    if (op === 'issues.updateComment') {\\n+      const id = Number((args as any).comment_id || 0);\\n+      const body = String((args as any).body || '');\\n+      for (const [, arr] of this.comments.entries()) {\\n+        const found = arr.find(c => c.id === id);\\n+        if (found) {\\n+          found.body = body;\\n+          found.updated_at = new Date().toISOString();\\n+          break;\\n+        }\\n+      }\\n+      return {\\n+        data: {\\n+          id,\\n+          body,\\n+          html_url: '',\\n+          user: { login: 'bot' },\\n+          updated_at: new Date().toISOString(),\\n+        },\\n+      };\\n+    }\\n+    if (op === 'issues.listComments') {\\n+      const issueNum = Number((args as any).issue_number || 0);\\n+      const items = (this.comments.get(issueNum) || []).map(c => ({\\n+        id: c.id,\\n+        body: c.body,\\n+        updated_at: c.updated_at,\\n+      }));\\n+      return { data: items };\\n+    }\\n+    if (op === 'issues.getComment') {\\n+      const id = Number((args as any).comment_id || 0);\\n+      for (const [, arr] of this.comments.entries()) {\\n+        const found = arr.find(c => c.id === id);\\n+        if (found)\\n+          return { data: { id: found.id, body: found.body, updated_at: found.updated_at } };\\n+      }\\n+      return { data: { id, body: '', updated_at: new Date().toISOString() } };\\n+    }\\n+    if (op === 'issues.addLabels') {\\n+      return { data: { labels: (args as any).labels || [] } };\\n+    }\\n+    if (op.startsWith('checks.')) {\\n+      return { data: { id: 123, status: 'completed', conclusion: 'success', url: '' } };\\n+    }\\n+    if (op === 'pulls.get') {\\n+      return { data: { number: (args as any).pull_number || 1, state: 'open', title: 'Test PR' } };\\n+    }\\n+    if (op === 'pulls.listFiles') {\\n+      return { data: [] };\\n+    }\\n+    return { data: {} };\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/recorders/global-recorder.ts\",\"additions\":1,\"deletions\":0,\"changes\":11,\"patch\":\"diff --git a/src/test-runner/recorders/global-recorder.ts b/src/test-runner/recorders/global-recorder.ts\\nnew file mode 100644\\nindex 00000000..cda96e45\\n--- /dev/null\\n+++ b/src/test-runner/recorders/global-recorder.ts\\n@@ -0,0 +1,11 @@\\n+import type { RecordingOctokit } from './github-recorder';\\n+\\n+let __rec: RecordingOctokit | null = null;\\n+\\n+export function setGlobalRecorder(r: RecordingOctokit | null): void {\\n+  __rec = r;\\n+}\\n+\\n+export function getGlobalRecorder(): RecordingOctokit | null {\\n+  return __rec;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/utils/selectors.ts\",\"additions\":3,\"deletions\":0,\"changes\":59,\"patch\":\"diff --git a/src/test-runner/utils/selectors.ts b/src/test-runner/utils/selectors.ts\\nnew file mode 100644\\nindex 00000000..5e1313bf\\n--- /dev/null\\n+++ b/src/test-runner/utils/selectors.ts\\n@@ -0,0 +1,59 @@\\n+export function deepGet(obj: unknown, path: string): unknown {\\n+  if (obj == null) return undefined;\\n+  const parts: Array<string | number> = [];\\n+  let i = 0;\\n+\\n+  const readIdent = () => {\\n+    const start = i;\\n+    while (i < path.length && path[i] !== '.' && path[i] !== '[') i++;\\n+    if (i > start) parts.push(path.slice(start, i));\\n+  };\\n+  const readBracket = () => {\\n+    // assumes path[i] === '['\\n+    i++; // skip [\\n+    if (i < path.length && (path[i] === '\\\"' || path[i] === \\\"'\\\")) {\\n+      const quote = path[i++];\\n+      const start = i;\\n+      while (i < path.length && path[i] !== quote) i++;\\n+      const key = path.slice(start, i);\\n+      parts.push(key);\\n+      // skip closing quote\\n+      if (i < path.length && path[i] === quote) i++;\\n+      // skip ]\\n+      if (i < path.length && path[i] === ']') i++;\\n+    } else {\\n+      // numeric index\\n+      const start = i;\\n+      while (i < path.length && /[0-9]/.test(path[i])) i++;\\n+      const numStr = path.slice(start, i);\\n+      parts.push(Number(numStr));\\n+      if (i < path.length && path[i] === ']') i++;\\n+    }\\n+  };\\n+\\n+  // initial token (identifier or bracket)\\n+  if (path[i] === '[') {\\n+    readBracket();\\n+  } else {\\n+    if (path[i] === '.') i++;\\n+    readIdent();\\n+  }\\n+  while (i < path.length) {\\n+    if (path[i] === '.') {\\n+      i++;\\n+      readIdent();\\n+    } else if (path[i] === '[') {\\n+      readBracket();\\n+    } else {\\n+      // unexpected char, stop parsing\\n+      break;\\n+    }\\n+  }\\n+\\n+  let cur: any = obj;\\n+  for (const key of parts) {\\n+    if (cur == null) return undefined;\\n+    cur = cur[key as any];\\n+  }\\n+  return cur;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/validator.ts\",\"additions\":13,\"deletions\":0,\"changes\":376,\"patch\":\"diff --git a/src/test-runner/validator.ts b/src/test-runner/validator.ts\\nnew file mode 100644\\nindex 00000000..13931065\\n--- /dev/null\\n+++ b/src/test-runner/validator.ts\\n@@ -0,0 +1,376 @@\\n+import Ajv, { ErrorObject } from 'ajv';\\n+import addFormats from 'ajv-formats';\\n+\\n+// Lightweight JSON Schema for the tests DSL. The goal is helpful errors,\\n+// not full semantic validation.\\n+const schema: any = {\\n+  $id: 'https://visor/probe/tests-dsl.schema.json',\\n+  type: 'object',\\n+  additionalProperties: false,\\n+  properties: {\\n+    version: { type: 'string' },\\n+    extends: {\\n+      oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+    },\\n+    tests: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      required: ['cases'],\\n+      properties: {\\n+        defaults: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            strict: { type: 'boolean' },\\n+            ai_provider: { type: 'string' },\\n+            fail_on_unexpected_calls: { type: 'boolean' },\\n+            github_recorder: {\\n+              type: 'object',\\n+              additionalProperties: false,\\n+              properties: {\\n+                error_code: { type: 'number' },\\n+                timeout_ms: { type: 'number' },\\n+              },\\n+            },\\n+            macros: {\\n+              type: 'object',\\n+              additionalProperties: { $ref: '#/$defs/expectBlock' },\\n+            },\\n+          },\\n+        },\\n+        fixtures: { type: 'array' },\\n+        cases: {\\n+          type: 'array',\\n+          minItems: 1,\\n+          items: { $ref: '#/$defs/testCase' },\\n+        },\\n+      },\\n+    },\\n+  },\\n+  required: ['tests'],\\n+  $defs: {\\n+    fixtureRef: {\\n+      oneOf: [\\n+        { type: 'string' },\\n+        {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            builtin: { type: 'string' },\\n+            overrides: { type: 'object' },\\n+          },\\n+          required: ['builtin'],\\n+        },\\n+      ],\\n+    },\\n+    testCase: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        name: { type: 'string' },\\n+        description: { type: 'string' },\\n+        skip: { type: 'boolean' },\\n+        strict: { type: 'boolean' },\\n+        github_recorder: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            error_code: { type: 'number' },\\n+            timeout_ms: { type: 'number' },\\n+          },\\n+        },\\n+        event: {\\n+          type: 'string',\\n+          enum: ['manual', 'pr_opened', 'pr_updated', 'pr_closed', 'issue_opened', 'issue_comment'],\\n+        },\\n+        fixture: { $ref: '#/$defs/fixtureRef' },\\n+        env: {\\n+          type: 'object',\\n+          additionalProperties: { type: 'string' },\\n+        },\\n+        mocks: {\\n+          type: 'object',\\n+          additionalProperties: {\\n+            oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n+          },\\n+        },\\n+        expect: { $ref: '#/$defs/expectBlock' },\\n+        // Flow cases\\n+        flow: {\\n+          type: 'array',\\n+          items: { $ref: '#/$defs/flowStage' },\\n+        },\\n+      },\\n+      required: ['name'],\\n+      anyOf: [{ required: ['event'] }, { required: ['flow'] }],\\n+    },\\n+    flowStage: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        name: { type: 'string' },\\n+        description: { type: 'string' },\\n+        github_recorder: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            error_code: { type: 'number' },\\n+            timeout_ms: { type: 'number' },\\n+          },\\n+        },\\n+        event: {\\n+          type: 'string',\\n+          enum: ['manual', 'pr_opened', 'pr_updated', 'pr_closed', 'issue_opened', 'issue_comment'],\\n+        },\\n+        fixture: { $ref: '#/$defs/fixtureRef' },\\n+        env: {\\n+          type: 'object',\\n+          additionalProperties: { type: 'string' },\\n+        },\\n+        mocks: {\\n+          type: 'object',\\n+          additionalProperties: {\\n+            oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n+          },\\n+        },\\n+        expect: { $ref: '#/$defs/expectBlock' },\\n+      },\\n+      required: ['event'],\\n+    },\\n+    countExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        exactly: { type: 'number' },\\n+        at_least: { type: 'number' },\\n+        at_most: { type: 'number' },\\n+      },\\n+      // Mutual exclusion is enforced at runtime; schema ensures they are numeric if present.\\n+    },\\n+    callsExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        step: { type: 'string' },\\n+        provider: { type: 'string' },\\n+        op: { type: 'string' },\\n+        args: { type: 'object' },\\n+        exactly: { type: 'number' },\\n+        at_least: { type: 'number' },\\n+        at_most: { type: 'number' },\\n+      },\\n+    },\\n+    promptsExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        step: { type: 'string' },\\n+        index: {\\n+          oneOf: [{ type: 'number' }, { enum: ['first', 'last'] }],\\n+        },\\n+        contains: {\\n+          type: 'array',\\n+          items: { type: 'string' },\\n+        },\\n+        not_contains: {\\n+          type: 'array',\\n+          items: { type: 'string' },\\n+        },\\n+        matches: { type: 'string' },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            contains: { type: 'array', items: { type: 'string' } },\\n+            not_contains: { type: 'array', items: { type: 'string' } },\\n+            matches: { type: 'string' },\\n+          },\\n+        },\\n+      },\\n+      required: ['step'],\\n+    },\\n+    outputsExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        step: { type: 'string' },\\n+        index: {\\n+          oneOf: [{ type: 'number' }, { enum: ['first', 'last'] }],\\n+        },\\n+        path: { type: 'string' },\\n+        equals: {},\\n+        equalsDeep: {},\\n+        matches: { type: 'string' },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            path: { type: 'string' },\\n+            equals: {},\\n+            matches: { type: 'string' },\\n+          },\\n+          required: ['path'],\\n+        },\\n+        contains_unordered: { type: 'array' },\\n+      },\\n+      required: ['step', 'path'],\\n+    },\\n+    expectBlock: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        use: { type: 'array', items: { type: 'string' } },\\n+        calls: { type: 'array', items: { $ref: '#/$defs/callsExpectation' } },\\n+        prompts: { type: 'array', items: { $ref: '#/$defs/promptsExpectation' } },\\n+        outputs: { type: 'array', items: { $ref: '#/$defs/outputsExpectation' } },\\n+        no_calls: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'object',\\n+            additionalProperties: false,\\n+            properties: {\\n+              step: { type: 'string' },\\n+              provider: { type: 'string' },\\n+              op: { type: 'string' },\\n+            },\\n+          },\\n+        },\\n+        fail: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: { message_contains: { type: 'string' } },\\n+        },\\n+        strict_violation: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: { for_step: { type: 'string' }, message_contains: { type: 'string' } },\\n+        },\\n+      },\\n+    },\\n+  },\\n+};\\n+\\n+const ajv = new Ajv({ allErrors: true, allowUnionTypes: true, strict: false });\\n+addFormats(ajv);\\n+const validate = ajv.compile(schema);\\n+\\n+function toYamlPath(instancePath: string): string {\\n+  if (!instancePath) return 'tests';\\n+  // Ajv instancePath starts with '/'\\n+  const parts = instancePath\\n+    .split('/')\\n+    .slice(1)\\n+    .map(p => (p.match(/^\\\\d+$/) ? `[${p}]` : `.${p}`));\\n+  let out = parts.join('');\\n+  if (out.startsWith('.')) out = out.slice(1);\\n+  // Heuristic: put root under tests for nicer messages\\n+  if (!out.startsWith('tests')) out = `tests.${out}`;\\n+  return out;\\n+}\\n+\\n+function levenshtein(a: string, b: string): number {\\n+  const m = a.length,\\n+    n = b.length;\\n+  const dp = Array.from({ length: m + 1 }, () => new Array(n + 1).fill(0));\\n+  for (let i = 0; i <= m; i++) dp[i][0] = i;\\n+  for (let j = 0; j <= n; j++) dp[0][j] = j;\\n+  for (let i = 1; i <= m; i++) {\\n+    for (let j = 1; j <= n; j++) {\\n+      const cost = a[i - 1] === b[j - 1] ? 0 : 1;\\n+      dp[i][j] = Math.min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost);\\n+    }\\n+  }\\n+  return dp[m][n];\\n+}\\n+\\n+const knownKeys = new Set([\\n+  // top-level\\n+  'version',\\n+  'extends',\\n+  'tests',\\n+  // tests\\n+  'tests.defaults',\\n+  'tests.fixtures',\\n+  'tests.cases',\\n+  // defaults\\n+  'tests.defaults.strict',\\n+  'tests.defaults.ai_provider',\\n+  'tests.defaults.github_recorder',\\n+  'tests.defaults.macros',\\n+  'tests.defaults.fail_on_unexpected_calls',\\n+  // case\\n+  'name',\\n+  'description',\\n+  'skip',\\n+  'strict',\\n+  'event',\\n+  'fixture',\\n+  'env',\\n+  'mocks',\\n+  'expect',\\n+  'flow',\\n+  // expect\\n+  'expect.use',\\n+  'expect.calls',\\n+  'expect.prompts',\\n+  'expect.outputs',\\n+  'expect.no_calls',\\n+  'expect.fail',\\n+  'expect.strict_violation',\\n+  // calls\\n+  'step',\\n+  'provider',\\n+  'op',\\n+  'exactly',\\n+  'at_least',\\n+  'at_most',\\n+  'args',\\n+  // prompts/outputs\\n+  'index',\\n+  'contains',\\n+  'not_contains',\\n+  'matches',\\n+  'path',\\n+  'equals',\\n+  'equalsDeep',\\n+  'where',\\n+  'contains_unordered',\\n+]);\\n+\\n+function hintForAdditionalProperty(err: ErrorObject): string | undefined {\\n+  if (err.keyword !== 'additionalProperties') return undefined;\\n+  const prop = (err.params as any)?.additionalProperty;\\n+  if (!prop || typeof prop !== 'string') return undefined;\\n+  // find nearest known key suffix match\\n+  let best: { key: string; dist: number } | null = null;\\n+  for (const k of knownKeys) {\\n+    const dist = levenshtein(prop, k.includes('.') ? k.split('.').pop()! : k);\\n+    if (dist <= 3 && (!best || dist < best.dist)) best = { key: k, dist };\\n+  }\\n+  if (best) return `Did you mean \\\"${best.key}\\\"?`;\\n+  return undefined;\\n+}\\n+\\n+function formatError(e: ErrorObject): string {\\n+  const path = toYamlPath(e.instancePath || '');\\n+  let msg = `${path}: ${e.message}`;\\n+  const hint = hintForAdditionalProperty(e);\\n+  if (hint) msg += ` (${hint})`;\\n+  if (e.keyword === 'enum' && Array.isArray((e.params as any)?.allowedValues)) {\\n+    msg += ` (allowed: ${(e.params as any).allowedValues.join(', ')})`;\\n+  }\\n+  return msg;\\n+}\\n+\\n+export type ValidationResult = { ok: true } | { ok: false; errors: string[] };\\n+\\n+export function validateTestsDoc(doc: unknown): ValidationResult {\\n+  try {\\n+    const ok = validate(doc);\\n+    if (ok) return { ok: true };\\n+    const errs = (validate.errors || []).map(formatError);\\n+    return { ok: false, errors: errs };\\n+  } catch (err) {\\n+    return { ok: false, errors: [err instanceof Error ? err.message : String(err)] };\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/utils/file-exclusion.ts\",\"additions\":1,\"deletions\":1,\"changes\":15,\"patch\":\"diff --git a/src/utils/file-exclusion.ts b/src/utils/file-exclusion.ts\\nindex 155bf20b..e7f5274e 100644\\n--- a/src/utils/file-exclusion.ts\\n+++ b/src/utils/file-exclusion.ts\\n@@ -128,12 +128,21 @@ export class FileExclusionHelper {\\n           .trim();\\n \\n         this.gitignore.add(gitignoreContent);\\n-        console.error('✅ Loaded .gitignore patterns for file filtering');\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.error('✅ Loaded .gitignore patterns for file filtering');\\n+        }\\n       } else if (additionalPatterns && additionalPatterns.length > 0) {\\n-        console.error('⚠️  No .gitignore found, using default exclusion patterns');\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.error('⚠️  No .gitignore found, using default exclusion patterns');\\n+        }\\n       }\\n     } catch (error) {\\n-      console.warn('⚠️ Failed to load .gitignore:', error instanceof Error ? error.message : error);\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        console.warn(\\n+          '⚠️ Failed to load .gitignore:',\\n+          error instanceof Error ? error.message : error\\n+        );\\n+      }\\n     }\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/integration/issue-double-content-detection.test.ts\",\"additions\":0,\"deletions\":5,\"changes\":131,\"patch\":\"diff --git a/tests/integration/issue-double-content-detection.test.ts b/tests/integration/issue-double-content-detection.test.ts\\ndeleted file mode 100644\\nindex 5ef9fb1b..00000000\\n--- a/tests/integration/issue-double-content-detection.test.ts\\n+++ /dev/null\\n@@ -1,131 +0,0 @@\\n-import * as fs from 'fs';\\n-import * as path from 'path';\\n-\\n-// Minimal Octokit REST mock to capture posted comment body\\n-const issuesCreateComment = jest.fn();\\n-const issuesListComments = jest.fn().mockResolvedValue({ data: [] });\\n-const pullsGet = jest.fn().mockResolvedValue({ data: { head: { sha: 'deadbeefcafebabe' } } });\\n-const pullsListFiles = jest.fn().mockResolvedValue({\\n-  data: [{ filename: 'x', status: 'modified', additions: 1, deletions: 0, changes: 1 }],\\n-});\\n-\\n-jest.mock('@octokit/rest', () => {\\n-  return {\\n-    Octokit: jest.fn().mockImplementation(() => ({\\n-      rest: {\\n-        issues: { createComment: issuesCreateComment, listComments: issuesListComments },\\n-        pulls: { get: pullsGet, listFiles: pullsListFiles },\\n-        checks: { create: jest.fn(), update: jest.fn() },\\n-      },\\n-    })),\\n-  };\\n-});\\n-\\n-describe('Issue assistant double-content detection (issues opened)', () => {\\n-  beforeEach(() => {\\n-    issuesCreateComment.mockReset();\\n-    issuesListComments.mockClear();\\n-    pullsGet.mockClear();\\n-    pullsListFiles.mockClear();\\n-\\n-    // Clean env used by action run()\\n-    delete process.env.GITHUB_EVENT_NAME;\\n-    delete process.env.GITHUB_EVENT_PATH;\\n-    delete process.env.GITHUB_REPOSITORY;\\n-    delete process.env.GITHUB_REPOSITORY_OWNER;\\n-    delete process.env['INPUT_GITHUB-TOKEN'];\\n-    delete process.env['INPUT_OWNER'];\\n-    delete process.env['INPUT_REPO'];\\n-    delete process.env['INPUT_CONFIG-PATH'];\\n-    delete process.env['INPUT_CREATE-CHECK'];\\n-    delete process.env['INPUT_COMMENT-ON-PR'];\\n-    delete process.env['INPUT_DEBUG'];\\n-  });\\n-\\n-  const writeTmp = (name: string, data: any) => {\\n-    const p = path.join(process.cwd(), name);\\n-    fs.writeFileSync(p, typeof data === 'string' ? data : JSON.stringify(data));\\n-    return p;\\n-  };\\n-\\n-  // This config intentionally produces two assistant-style outputs in the same run.\\n-  // With current behavior, both get concatenated into the single issue comment.\\n-  // We assert that only one assistant response appears (i.e., deduped/collapsed),\\n-  // so this test should fail until the posting logic is fixed.\\n-  const makeConfig = () => `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  assistant-initial:\\n-    type: ai\\n-    ai_provider: mock\\n-    schema: issue-assistant\\n-    group: dynamic\\n-    on: [issue_opened]\\n-    prompt: |\\n-      Return ONE JSON object for issue-assistant.\\n-      text: ### Assistant Reply\\n-      intent: issue_triage\\n-\\n-  assistant-refined:\\n-    type: ai\\n-    ai_provider: mock\\n-    schema: issue-assistant\\n-    group: dynamic\\n-    depends_on: [assistant-initial]\\n-    on: [issue_opened]\\n-    prompt: |\\n-      Return ONE JSON object for issue-assistant.\\n-      text: ### Assistant Reply\\n-      intent: issue_triage\\n-\\n-output:\\n-  pr_comment:\\n-    format: markdown\\n-    group_by: check\\n-`;\\n-\\n-  it('posts only the refined answer (no duplicate old+new content)', async () => {\\n-    const cfgPath = writeTmp('.tmp-double-content.yaml', makeConfig());\\n-    const event = {\\n-      action: 'opened',\\n-      issue: { number: 77, state: 'open' },\\n-      repository: { full_name: 'acme/widgets' },\\n-      sender: { login: 'reporter' },\\n-    } as any;\\n-    const eventPath = writeTmp('.tmp-issues-opened-double.json', event);\\n-\\n-    process.env.GITHUB_EVENT_NAME = 'issues';\\n-    process.env.GITHUB_EVENT_PATH = eventPath;\\n-    process.env.GITHUB_REPOSITORY = 'acme/widgets';\\n-    process.env.GITHUB_REPOSITORY_OWNER = 'acme';\\n-    process.env['INPUT_GITHUB-TOKEN'] = 'test-token';\\n-    process.env['INPUT_OWNER'] = 'acme';\\n-    process.env['INPUT_REPO'] = 'widgets';\\n-    process.env['INPUT_CONFIG-PATH'] = cfgPath;\\n-    process.env['INPUT_CREATE-CHECK'] = 'false';\\n-    process.env['INPUT_COMMENT-ON-PR'] = 'false';\\n-    process.env['INPUT_DEBUG'] = 'true';\\n-\\n-    jest.resetModules();\\n-    const { run } = await import('../../src/index');\\n-    await run();\\n-\\n-    // Exactly one comment is posted\\n-    expect(issuesCreateComment).toHaveBeenCalledTimes(1);\\n-    const call = issuesCreateComment.mock.calls[0][0];\\n-    const body: string = call.body;\\n-    // Debug: persist body for local inspection\\n-    fs.mkdirSync('tmp', { recursive: true });\\n-    fs.writeFileSync('tmp/issue-double-content-body.md', body, 'utf8');\\n-\\n-    // Desired behavior: Only a single assistant response should appear.\\n-    // Current bug: both initial and refined outputs are concatenated. In the\\n-    // mock path the provider sometimes returns a minimal JSON like {\\\"issues\\\":[]}.\\n-    // Assert that only one such block exists.\\n-    const jsonBlockCount = (body.match(/\\\\{\\\\s*\\\\\\\"issues\\\\\\\"\\\\s*:\\\\s*\\\\[\\\\]\\\\s*\\\\}/g) || []).length;\\n-    expect(jsonBlockCount).toBe(1);\\n-\\n-    fs.unlinkSync(cfgPath);\\n-    fs.unlinkSync(eventPath);\\n-  });\\n-});\\n\",\"status\":\"removed\"},{\"filename\":\"tests/integration/issue-posting-fact-gate.test.ts\",\"additions\":0,\"deletions\":7,\"changes\":197,\"patch\":\"diff --git a/tests/integration/issue-posting-fact-gate.test.ts b/tests/integration/issue-posting-fact-gate.test.ts\\ndeleted file mode 100644\\nindex 4ac86358..00000000\\n--- a/tests/integration/issue-posting-fact-gate.test.ts\\n+++ /dev/null\\n@@ -1,197 +0,0 @@\\n-import * as fs from 'fs';\\n-import * as path from 'path';\\n-\\n-// Reuse the Octokit REST mock pattern from other integration tests\\n-const checksCreate = jest.fn();\\n-const checksUpdate = jest.fn();\\n-const issuesCreateComment = jest.fn();\\n-const issuesListComments = jest.fn().mockResolvedValue({ data: [] });\\n-const pullsGet = jest.fn().mockResolvedValue({ data: { head: { sha: 'deadbeefcafebabe' } } });\\n-const pullsListFiles = jest.fn().mockResolvedValue({\\n-  data: [{ filename: 'x', status: 'modified', additions: 1, deletions: 0, changes: 1 }],\\n-});\\n-\\n-jest.mock('@octokit/rest', () => {\\n-  return {\\n-    Octokit: jest.fn().mockImplementation(() => ({\\n-      rest: {\\n-        checks: { create: checksCreate, update: checksUpdate },\\n-        issues: { createComment: issuesCreateComment, listComments: issuesListComments },\\n-        pulls: { get: pullsGet, listFiles: pullsListFiles },\\n-      },\\n-    })),\\n-  };\\n-});\\n-\\n-describe('Issue assistant posting is gated by fact validation (issue_opened)', () => {\\n-  beforeEach(() => {\\n-    checksCreate.mockReset();\\n-    checksUpdate.mockReset();\\n-    issuesCreateComment.mockReset();\\n-    issuesListComments.mockClear();\\n-    pullsGet.mockClear();\\n-    pullsListFiles.mockClear();\\n-    // Clean env that run() reads\\n-    delete process.env.GITHUB_EVENT_NAME;\\n-    delete process.env.GITHUB_EVENT_PATH;\\n-    delete process.env.GITHUB_REPOSITORY;\\n-    delete process.env.GITHUB_REPOSITORY_OWNER;\\n-    delete process.env['INPUT_GITHUB-TOKEN'];\\n-    delete process.env['INPUT_OWNER'];\\n-    delete process.env['INPUT_REPO'];\\n-    delete process.env['INPUT_CONFIG-PATH'];\\n-    delete process.env['INPUT_CREATE-CHECK'];\\n-    delete process.env['INPUT_COMMENT-ON-PR'];\\n-    delete process.env['INPUT_DEBUG'];\\n-  });\\n-\\n-  const makeConfig = (allValid: boolean) => `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  # Minimal issue assistant using mock provider\\n-  issue-assistant:\\n-    type: ai\\n-    ai_provider: mock\\n-    schema: issue-assistant\\n-    on: [issue_opened]\\n-    prompt: |\\n-      Return ONE JSON object for issue-assistant.\\n-      text: Hello, world.\\n-\\n-References:\\n-\\n-\\\\`\\\\`\\\\`refs\\n-none\\n-\\\\`\\\\`\\\\`\\n-      intent: issue_triage\\n-    on_success:\\n-      run: [init-fact-validation]\\n-\\n-  # Initialize validation state\\n-  init-fact-validation:\\n-    type: memory\\n-    operation: set\\n-    namespace: fact-validation\\n-    key: attempt\\n-    value: 0\\n-    on: [issue_opened]\\n-\\n-  # Seed deterministic facts instead of invoking AI\\n-  seed-facts:\\n-    type: memory\\n-    operation: set\\n-    namespace: fact-validation\\n-    key: fact_list\\n-    value: [{\\\"id\\\":\\\"f1\\\",\\\"category\\\":\\\"Configuration\\\",\\\"claim\\\":\\\"X\\\",\\\"verifiable\\\":true}]\\n-    depends_on: [issue-assistant]\\n-    on: [issue_opened]\\n-\\n-  # forEach extraction proxy\\n-  extract-facts:\\n-    type: memory\\n-    operation: get\\n-    namespace: fact-validation\\n-    key: fact_list\\n-    forEach: true\\n-    depends_on: [seed-facts]\\n-    on: [issue_opened]\\n-    on_finish:\\n-      run: [aggregate-validations]\\n-      goto_js: |\\n-        const ns = 'fact-validation';\\n-        const allValid = memory.get('all_valid', ns) === true;\\n-        const limit = 1; // one retry\\n-        const attempt = Number(memory.get('attempt', ns) || 0);\\n-        if (!allValid && attempt < limit) {\\n-          memory.increment('attempt', 1, ns);\\n-          return 'issue-assistant';\\n-        }\\n-        return null;\\n-\\n-  validate-fact:\\n-    type: memory\\n-    operation: exec_js\\n-    namespace: fact-validation\\n-    depends_on: [extract-facts]\\n-    on: [issue_opened]\\n-    memory_js: |\\n-      const NS = 'fact-validation';\\n-      const f = outputs['extract-facts'];\\n-      const attempt = Number(memory.get('attempt', NS) || 0);\\n-      const is_valid = ${allValid ? 'true' : 'false'}; return { fact_id: f.id, claim: f.claim, is_valid, confidence: '${allValid ? \\\"'ok'\\\" : \\\"'bad'\\\"} };\\n-\\n-  aggregate-validations:\\n-    type: memory\\n-    operation: exec_js\\n-    namespace: fact-validation\\n-    on: [issue_opened]\\n-    memory_js: |\\n-      const vals = outputs.history['validate-fact'] || [];\\n-      const invalid = (Array.isArray(vals) ? vals : []).filter(v => v && v.is_valid === false);\\n-      const all_valid = invalid.length === 0;\\n-      memory.set('all_valid', all_valid, 'fact-validation');\\n-      return { total: vals.length, all_valid };\\n-\\n-  # Emit a simple final note when valid so the Action has content to post once\\n-  final-note:\\n-    type: log\\n-    depends_on: [aggregate-validations]\\n-    if: \\\"memory.get('all_valid','fact-validation') === true\\\"\\n-    message: 'Verified: final'\\n-\\n-  # No explicit post step; use Action's generic end-of-run post\\n-\\n-output:\\n-  pr_comment:\\n-    format: markdown\\n-    group_by: check\\n-`;\\n-\\n-  const writeTmp = (name: string, data: any) => {\\n-    const p = path.join(process.cwd(), name);\\n-    fs.writeFileSync(p, typeof data === 'string' ? data : JSON.stringify(data));\\n-    return p;\\n-  };\\n-\\n-  const setupAndRun = async (allValid: boolean) => {\\n-    const cfgPath = writeTmp(\\n-      `.tmp-issue-gate-${allValid ? 'ok' : 'fail'}.yaml`,\\n-      makeConfig(allValid)\\n-    );\\n-    const event = {\\n-      action: 'opened',\\n-      issue: { number: 42, state: 'open' },\\n-      repository: { full_name: 'acme/widgets' },\\n-      sender: { login: 'reporter' },\\n-    } as any;\\n-    const eventPath = writeTmp('.tmp-issues-opened.json', event);\\n-\\n-    process.env.GITHUB_EVENT_NAME = 'issues';\\n-    process.env.GITHUB_EVENT_PATH = eventPath;\\n-    process.env.GITHUB_REPOSITORY = 'acme/widgets';\\n-    process.env.GITHUB_REPOSITORY_OWNER = 'acme';\\n-    process.env['INPUT_GITHUB-TOKEN'] = 'test-token';\\n-    process.env['INPUT_OWNER'] = 'acme';\\n-    process.env['INPUT_REPO'] = 'widgets';\\n-    process.env['INPUT_CONFIG-PATH'] = cfgPath;\\n-    process.env['INPUT_CREATE-CHECK'] = 'false';\\n-    process.env['INPUT_COMMENT-ON-PR'] = 'false';\\n-    process.env['INPUT_DEBUG'] = 'true';\\n-    process.env['ENABLE_FACT_VALIDATION'] = 'true';\\n-\\n-    // Import run() fresh to pick up env\\n-    jest.resetModules();\\n-    const { run } = await import('../../src/index');\\n-    await run();\\n-\\n-    fs.unlinkSync(cfgPath);\\n-    fs.unlinkSync(eventPath);\\n-  };\\n-\\n-  it('loops once to correct facts and posts a single final comment', async () => {\\n-    await setupAndRun(false);\\n-    // With attempt limit=1, the first validation fails, we route back to assistant,\\n-    // second pass should be valid and then post once at end.\\n-    expect(issuesCreateComment).toHaveBeenCalledTimes(1);\\n-  });\\n-});\\n\",\"status\":\"removed\"}],\"outputs\":{\"overview\":{\"text\":\"This PR introduces a comprehensive, configuration-driven integration test framework for Visor. It allows developers to write tests for their `.visor.yaml` configurations by simulating GitHub events, mocking providers (like AI and GitHub API calls), and asserting on the resulting actions. This is a significant feature that replaces previous ad-hoc testing methods with a structured and maintainable approach.\\n\\n### Files Changed Analysis\\n\\nThe changes introduce a new test runner, along with its supporting components, documentation, and default test suite.\\n\\n-   **New Feature Implementation (`src/test-runner/`)**: The core logic is encapsulated in the new `src/test-runner` directory, which includes the main `index.ts` (the runner itself), `fixture-loader.ts` for managing test data, `recorders/` for mocking GitHub and AI interactions, and `validator.ts` for handling assertions.\\n-   **CLI Integration (`src/cli-main.ts`)**: A new `test` subcommand is added to the Visor CLI to execute the test runner.\\n-   **Execution Engine Modifications (`src/check-execution-engine.ts`)**: The engine is updated to support test mode, primarily by allowing the injection of mock providers and recorders.\\n-   **New Test Suite (`defaults/.visor.tests.yaml`)**: A comprehensive test suite for the default `.visor.yaml` configuration is added, serving as a practical example of the new framework.\\n-   **Documentation (`docs/testing/`)**: Extensive documentation is added, covering getting started, CLI usage, assertions, and fixtures/mocks.\\n-   **CI Integration (`.github/workflows/ci.yml`)**: The CI pipeline is updated to run the new integration tests, ensuring configurations are validated on each pull request.\\n-   **Test Removal (`tests/integration/`)**: Old, script-based integration tests are removed in favor of the new, more robust framework.\\n\\n### Architecture & Impact Assessment\\n\\n#### What this PR accomplishes\\n\\nThis PR delivers a complete integration test framework for Visor configurations. It enables developers to validate their automation rules in a predictable, isolated environment without making live network calls. This improves reliability, simplifies debugging, and provides a safety net for configuration changes.\\n\\n#### Key technical changes introduced\\n\\n1.  **Test Runner CLI**: A `visor test` command is introduced to discover and run tests defined in a `.visor.tests.yaml` file.\\n2.  **Fixture-Based Testing**: Tests are driven by predefined \\\"fixtures\\\" that simulate GitHub webhook events (e.g., `gh.pr_open.minimal`).\\n3.  **Mocking and Recording**: The framework intercepts calls to external providers. GitHub API calls are recorded for assertion, and AI provider calls are mocked to return predefined responses. This is handled by a `RecordingOctokit` wrapper and mock AI providers that are activated when `ai.provider` is set to `mock`.\\n4.  **Declarative Assertions**: Tests use a YAML `expect:` block to assert on outcomes, such as the number of calls to a provider (`calls`), the content of AI prompts (`prompts`), or the final status of checks.\\n\\n#### Affected system components\\n\\n-   **CLI (`src/cli-main.ts`)**: Extended with a new `test` command.\\n-   **Core Logic (`src/check-execution-engine.ts`)**: Modified to operate in a \\\"test mode\\\" with mocked dependencies.\\n-   **Providers (`src/providers/*`)**: The `GithubOpsProvider` is adapted to use a recordable Octokit instance during tests. The `AiCheckProvider` is modified to handle a `mock` provider type.\\n-   **CI/CD (`.github/workflows/ci.yml`)**: The CI workflow is updated to execute the new test suite.\\n\\n#### Component Interaction Diagram\\n\\n```mermaid\\ngraph TD\\n    subgraph Test Execution\\n        A[visor test CLI] --> B{Test Runner};\\n        B --> C[Load .visor.tests.yaml];\\n        C --> D{For each test case};\\n        D --> E[Load Fixture & Mocks];\\n    end\\n\\n    subgraph Visor Core\\n        F(CheckExecutionEngine);\\n        G[Providers (GitHub, AI, etc.)];\\n    end\\n\\n    subgraph Mocks & Recorders\\n        H[RecordingOctokit];\\n        I[MockAiProvider];\\n    end\\n\\n    E --> |injects mocks| F;\\n    F --> |uses| G;\\n    G -- during test --> H;\\n    G -- during test --> I;\\n\\n    D --> |runs| F;\\n    F --> J[Collect Results & Recorded Calls];\\n    J --> K[Validate Assertions];\\n    K --> L[Report Pass/Fail];\\n```\\n\\n### Scope Discovery & Context Expansion\\n\\nThis feature fundamentally changes how Visor configurations are developed and maintained. By providing a robust testing framework, it encourages a test-driven development (TDD) approach for writing automation rules.\\n\\n-   **Impact on Configuration Development**: Users will now be expected to write tests for their custom checks and workflows. The `defaults/.visor.tests.yaml` file serves as a blueprint for this.\\n-   **Reliability and Maintenance**: The ability to test configurations offline significantly reduces the risk of introducing regressions. It makes troubleshooting easier, as failures can be reproduced locally and deterministically.\\n-   **Provider Ecosystem**: The mocking architecture is extensible. While this PR focuses on GitHub and AI providers, the same pattern could be applied to any future provider (e.g., Slack, Jira), ensuring that all integrations can be tested.\\n-   **Developer Experience**: The framework is designed with developer experience in mind, offering clear output, helpful error messages, and a straightforward YAML-based syntax, lowering the barrier to writing effective tests.\",\"tags\":{\"review-effort\":5,\"label\":\"feature\"}}}}"},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"performance","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"performance","visor.check.input.context":"{\"pr\":{\"number\":0,\"title\":\"Local Analysis: feat/test-framework-runner (14 modified)\",\"author\":\"Leonid Bugaev\",\"branch\":\"feat/test-framework-runner\",\"base\":\"main\"},\"files\":[{\"filename\":\".github/workflows/ci.yml\",\"additions\":1,\"deletions\":0,\"changes\":15,\"patch\":\"diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml\\nindex 840f2abc..13bd6f9d 100644\\n--- a/.github/workflows/ci.yml\\n+++ b/.github/workflows/ci.yml\\n@@ -64,6 +64,21 @@ jobs:\\n           ls -la *.tgz\\n           echo \\\"✅ Package can be created successfully\\\"\\n \\n+      - name: Run integration tests (defaults suite)\\n+        run: |\\n+          mkdir -p output\\n+          node ./dist/index.js test --config defaults/.visor.tests.yaml --json output/visor-tests.json --report junit:output/visor-tests.xml --summary md:output/visor-tests.md\\n+\\n+      - name: Upload integration test artifacts\\n+        if: always()\\n+        uses: actions/upload-artifact@v4\\n+        with:\\n+          name: visor-test-results\\n+          path: |\\n+            output/visor-tests.json\\n+            output/visor-tests.xml\\n+            output/visor-tests.md\\n+\\n       - name: Test basic action functionality\\n         uses: ./\\n         with:\\n\",\"status\":\"added\"},{\"filename\":\"README.md\",\"additions\":1,\"deletions\":0,\"changes\":10,\"patch\":\"diff --git a/README.md b/README.md\\nindex b2bf4db5..7acc4843 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -729,3 +729,13 @@ steps:\\n ```\\n \\n See docs: docs/github-ops.md\\n+## Integration Tests (Great DX)\\n+\\n+Visor ships a YAML‑native integration test runner so you can describe user flows, mocks, and assertions alongside your config.\\n+\\n+- Start here: docs/testing/getting-started.md\\n+- CLI details: docs/testing/cli.md\\n+- Fixtures and mocks: docs/testing/fixtures-and-mocks.md\\n+- Assertions reference: docs/testing/assertions.md\\n+\\n+Example suite: defaults/.visor.tests.yaml\\n\",\"status\":\"added\"},{\"filename\":\"defaults/.visor.tests.yaml\",\"additions\":20,\"deletions\":0,\"changes\":557,\"patch\":\"diff --git a/defaults/.visor.tests.yaml b/defaults/.visor.tests.yaml\\nnew file mode 100644\\nindex 00000000..c496cdee\\n--- /dev/null\\n+++ b/defaults/.visor.tests.yaml\\n@@ -0,0 +1,557 @@\\n+version: \\\"1.0\\\"\\n+extends: \\\".visor.yaml\\\"\\n+\\n+# Integration test suite for Visor default configuration\\n+# - Driven by events + fixtures; no manual step lists\\n+# - Strict by default: every executed step must have an expect\\n+# - AI mocks accept structured JSON when a schema is defined; plain uses text\\n+# - GitHub calls are recorded by default by the test runner (no network)\\n+\\n+tests:\\n+  defaults:\\n+    strict: true\\n+    ai_provider: mock\\n+    fail_on_unexpected_calls: true\\n+    # Example: enable negative GitHub recorder for all tests\\n+    # github_recorder: { error_code: 429 }\\n+  # Built-in fixtures are provided by the test runner (gh.* namespace).\\n+  # Custom fixtures may still be added here if needed.\\n+  fixtures: []\\n+\\n+  cases:\\n+    - name: label-flow\\n+      description: |\\n+        Validates the happy path for PR open:\\n+        - overview runs and emits tags.label and tags.review-effort (mocked)\\n+        - apply-overview-labels adds two labels (feature and review/effort:2)\\n+        - overview prompt includes PR title and unified diff header\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: |\\n+            High‑level summary of the changes and impact.\\n+          tags:\\n+            label: feature\\n+            review-effort: 2\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args:\\n+              contains:\\n+                - feature\\n+                - \\\"review/effort:2\\\"\\n+        outputs:\\n+          - step: overview\\n+            path: \\\"tags.label\\\"\\n+            equals: feature\\n+          - step: overview\\n+            path: \\\"tags['review-effort']\\\"\\n+            equals: 2\\n+        prompts:\\n+          - step: overview\\n+            contains:\\n+              - \\\"feat: add user search\\\"\\n+              - \\\"diff --git a/src/search.ts\\\"\\n+\\n+    - name: issue-triage\\n+      skip: true\\n+      description: |\\n+        Ensures the issue assistant triages a newly opened issue and applies labels.\\n+        Asserts the structured output (intent=issue_triage) and the GitHub label op.\\n+      event: issue_opened\\n+      fixture: gh.issue_open.minimal\\n+      mocks:\\n+        issue-assistant:\\n+          text: |\\n+            Thanks for the detailed report! We will investigate.\\n+          intent: issue_triage\\n+          labels: [bug, priority/medium]\\n+      expect:\\n+        calls:\\n+          - step: issue-assistant\\n+            exactly: 1\\n+          - step: apply-issue-labels\\n+            exactly: 1\\n+          - step: init-fact-validation\\n+            exactly: 1\\n+          - step: extract-facts\\n+            exactly: 1\\n+          - step: validate-fact\\n+            at_least: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args:\\n+              contains:\\n+                - bug\\n+        outputs:\\n+          - step: issue-assistant\\n+            path: intent\\n+            equals: issue_triage\\n+        prompts:\\n+          - step: issue-assistant\\n+            contains:\\n+              - \\\"Bug: crashes on search edge case\\\"\\n+\\n+    - name: pr-review-e2e-flow\\n+      description: |\\n+        End-to-end PR lifecycle covering multiple external events:\\n+        1) PR opened → overview + labels\\n+        2) Standard comment → no bot reply\\n+        3) /visor help → single assistant reply (no retrigger)\\n+        4) /visor Regenerate reviews → retrigger overview\\n+        5) Fact validation enabled on comment → extract/validate/aggregate\\n+        6) Fact validation disabled on comment → only assistant, no validation steps\\n+        7) PR synchronized (new commit) → overview runs again\\n+      strict: true\\n+      flow:\\n+        - name: pr-open\\n+          description: |\\n+            PR open event. Mocks overview/security/quality/performance as empty issue lists.\\n+            Expects all review steps to run and labels to be added.\\n+          event: pr_opened\\n+          fixture: gh.pr_open.minimal\\n+          mocks:\\n+            overview:\\n+              text: \\\"Overview body\\\"\\n+              tags: { label: feature, review-effort: 2 }\\n+            security: { issues: [] }\\n+            architecture: { issues: [] }\\n+            quality: { issues: [] }\\n+            performance: { issues: [] }\\n+          expect:\\n+            calls:\\n+              - step: overview\\n+                exactly: 1\\n+              - step: security\\n+                exactly: 1\\n+              - step: architecture\\n+                exactly: 1\\n+              - step: performance\\n+                exactly: 1\\n+              - step: quality\\n+                exactly: 1\\n+              - step: apply-overview-labels\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.createComment\\n+                at_least: 1\\n+              - provider: github\\n+                op: labels.add\\n+                at_least: 1\\n+                args:\\n+                  contains: [feature]\\n+            prompts:\\n+              - step: overview\\n+                contains:\\n+                  - \\\"feat: add user search\\\"\\n+\\n+        - name: standard-comment\\n+          description: |\\n+            A regular human comment on a PR should not produce a bot reply.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.standard\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"\\\"   # empty text to avoid posting a reply\\n+              intent: comment_reply\\n+          expect:\\n+            no_calls:\\n+              - provider: github\\n+                op: issues.createComment\\n+              - step: init-fact-validation\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+\\n+        - name: visor-plain\\n+          description: |\\n+            A \\\"/visor help\\\" comment should be recognized and answered once by the assistant.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"Sure, here’s how I can help.\\\"\\n+              intent: comment_reply\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.createComment\\n+                exactly: 1\\n+            outputs:\\n+              - step: comment-assistant\\n+                path: intent\\n+                equals: comment_reply\\n+            prompts:\\n+              - step: comment-assistant\\n+                matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"\\n+\\n+        - name: visor-retrigger\\n+          description: |\\n+            A \\\"/visor Regenerate reviews\\\" comment should set intent=comment_retrigger\\n+            and schedule a new overview.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_regenerate\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"Regenerating.\\\"\\n+              intent: comment_retrigger\\n+            overview:\\n+              text: \\\"Overview (regenerated)\\\"\\n+              tags: { label: feature, review-effort: 2 }\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+              - step: overview\\n+                exactly: 1\\n+              - step: security\\n+                exactly: 1\\n+              - step: architecture\\n+                exactly: 1\\n+              - step: performance\\n+                exactly: 1\\n+              - step: quality\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.createComment\\n+                at_least: 1\\n+            outputs:\\n+              - step: comment-assistant\\n+                path: intent\\n+                equals: comment_retrigger\\n+            prompts:\\n+              - step: comment-assistant\\n+                contains: [\\\"Regenerate reviews\\\"]\\n+\\n+        - name: facts-enabled\\n+          description: |\\n+            With fact validation enabled, the assistant reply is followed by\\n+            extract-facts, validate-fact (per fact), and aggregate-validations.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"We rely on defaults/.visor.yaml line 11 for max_parallelism=4.\\\"\\n+              intent: comment_reply\\n+            extract-facts:\\n+              - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+            validate-fact[]:\\n+              - fact_id: f1\\n+                claim: \\\"max_parallelism defaults to 4\\\"\\n+                is_valid: true\\n+                confidence: high\\n+                evidence: \\\"defaults/.visor.yaml:11\\\"\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+              - step: extract-facts\\n+                exactly: 1\\n+              - step: validate-fact\\n+                at_least: 1\\n+              - step: aggregate-validations\\n+                exactly: 1\\n+              - step: init-fact-validation\\n+                exactly: 1\\n+          # Prompt assertions are validated separately in stage-level prompt tests\\n+\\n+        - name: facts-invalid\\n+          description: |\\n+            Invalid fact path: after assistant reply, extract-facts finds one claim and\\n+            validate-fact returns is_valid=false; aggregate-validations detects not-all-valid\\n+            and reruns the assistant once with correction context.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"We rely on defaults/.visor.yaml line 11 for max_parallelism=4.\\\"\\n+              intent: comment_reply\\n+            extract-facts:\\n+              - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+            validate-fact[]:\\n+              - fact_id: f1\\n+                claim: \\\"max_parallelism defaults to 4\\\"\\n+                is_valid: false\\n+                confidence: high\\n+                evidence: \\\"defaults/.visor.yaml:11 does not set 4\\\"\\n+                correction: \\\"max_parallelism defaults to 3\\\"\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 2\\n+              - step: extract-facts\\n+                exactly: 1\\n+              - step: validate-fact\\n+                at_least: 1\\n+              - step: aggregate-validations\\n+                exactly: 1\\n+              - step: init-fact-validation\\n+                exactly: 1\\n+            prompts:\\n+              - step: comment-assistant\\n+                index: last\\n+                contains:\\n+                  - \\\"<previous_response>\\\"\\n+                  - \\\"max_parallelism defaults to 4\\\"\\n+                  - \\\"Correction: max_parallelism defaults to 3\\\"\\n+\\n+        - name: facts-two-items\\n+          description: |\\n+            Two facts extracted; only the invalid fact should appear in the correction pass.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"We rely on defaults/.visor.yaml for concurrency defaults.\\\"\\n+              intent: comment_reply\\n+            extract-facts:\\n+              - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+              - { id: f2, category: Feature,       claim: \\\"Fast mode is enabled by default\\\", verifiable: true }\\n+            validate-fact[]:\\n+              - { fact_id: f1, claim: \\\"max_parallelism defaults to 4\\\", is_valid: false, confidence: high, evidence: \\\"defaults/.visor.yaml:11\\\", correction: \\\"max_parallelism defaults to 3\\\" }\\n+              - { fact_id: f2, claim: \\\"Fast mode is enabled by default\\\", is_valid: true, confidence: high, evidence: \\\"src/config.ts:FAST_MODE=true\\\" }\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 2\\n+              - step: extract-facts\\n+                exactly: 1\\n+              - step: validate-fact\\n+                exactly: 2\\n+              - step: aggregate-validations\\n+                exactly: 1\\n+              - step: init-fact-validation\\n+                exactly: 1\\n+            outputs:\\n+              - step: validate-fact\\n+                where: { path: fact_id, equals: f1 }\\n+                path: is_valid\\n+                equals: false\\n+              - step: validate-fact\\n+                where: { path: fact_id, equals: f2 }\\n+                path: is_valid\\n+                equals: true\\n+            prompts:\\n+              - step: comment-assistant\\n+                index: last\\n+                contains:\\n+                  - \\\"<previous_response>\\\"\\n+                  - \\\"max_parallelism defaults to 4\\\"\\n+                  - \\\"Correction: max_parallelism defaults to 3\\\"\\n+                not_contains:\\n+                  - \\\"Fast mode is enabled by default\\\"\\n+\\n+        - name: facts-disabled\\n+          description: |\\n+            With fact validation disabled, only the assistant runs; no validation steps execute.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.standard\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"false\\\"\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+            no_calls:\\n+              - step: init-fact-validation\\n+              - step: extract-facts\\n+              - step: validate-fact\\n+              - step: aggregate-validations\\n+\\n+        - name: pr-updated\\n+          description: |\\n+            When a new commit is pushed (synchronize), overview should run again\\n+            and post/refresh a comment.\\n+          event: pr_updated\\n+          fixture: gh.pr_sync.minimal\\n+          mocks:\\n+            overview:\\n+              text: \\\"Overview for new commit\\\"\\n+              tags: { label: feature, review-effort: 3 }\\n+          expect:\\n+            calls:\\n+              - step: overview\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.updateComment\\n+                at_least: 1\\n+\\n+    - name: security-fail-if\\n+      description: |\\n+        Verifies that the global fail_if trips when security produces an error‑severity issue.\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview text\\\"\\n+          tags:\\n+            label: bug\\n+            review-effort: 3\\n+        security:\\n+          issues:\\n+            - id: S-001\\n+              file: src/search.ts\\n+              line: 10\\n+              message: \\\"Command injection risk\\\"\\n+              severity: error\\n+              category: security\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: security\\n+            exactly: 1\\n+        outputs:\\n+          - step: security\\n+            path: \\\"issues[0].severity\\\"\\n+            equals: error\\n+        fail:\\n+          message_contains: \\\"fail_if\\\"\\n+\\n+    - name: strict-mode-example\\n+      skip: true\\n+      description: |\\n+        Demonstrates strict mode: a step executed without a corresponding expect\\n+        (apply-overview-labels) triggers a strict_violation with a helpful message.\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: \\\"Short overview\\\"\\n+          tags:\\n+            label: chore\\n+            review-effort: 1\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+        strict_violation:\\n+          for_step: apply-overview-labels\\n+          message_contains: \\\"Add an expect for this step or set strict: false\\\"\\n+\\n+    - name: visor-plain-prompt\\n+      description: |\\n+        Standalone prompt check for a \\\"/visor help\\\" comment.\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_help\\n+      mocks:\\n+        comment-assistant:\\n+          text: \\\"Here is how I can help.\\\"\\n+          intent: comment_reply\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+        prompts:\\n+          - step: comment-assistant\\n+            matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"\\n+\\n+    - name: visor-retrigger-prompt\\n+      description: |\\n+        Standalone prompt check for \\\"/visor Regenerate reviews\\\".\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_regenerate\\n+      strict: false\\n+      mocks:\\n+        comment-assistant:\\n+          text: \\\"Regenerating.\\\"\\n+          intent: comment_retrigger\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+        prompts:\\n+          - step: comment-assistant\\n+            contains: [\\\"Regenerate reviews\\\"]\\n+\\n+    - name: command-mock-shape\\n+      description: |\\n+        Illustrates command provider mocking and output assertions.\\n+        Skipped by default; enable when command steps exist.\\n+      skip: true  # illustrative only, enable when a command step exists\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        unit-tests:\\n+          stdout: '{\\\"passed\\\": 128, \\\"failed\\\": 0, \\\"duration_sec\\\": 1.2}'\\n+          exit_code: 0\\n+      expect:\\n+        calls:\\n+          - step: unit-tests\\n+            exactly: 1\\n+        outputs:\\n+          - step: unit-tests\\n+            path: failed\\n+            equals: 0\\n+\\n+    - name: github-negative-mode\\n+      description: |\\n+        Demonstrates negative GitHub recorder mode: simulate a 429 error and assert failure path.\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      github_recorder: { error_code: 429 }\\n+      # Override defaults for this case only by specifying a local recorder via env-like knob\\n+      # The runner reads tests.defaults.github_recorder; we provide it at the suite level by default.\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview body\\\"\\n+          tags: { label: feature, review-effort: 2 }\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+        fail:\\n+          message_contains: \\\"github/op_failed\\\"\\n+\\n+    - name: facts-invalid\\n+      skip: true\\n+      description: |\\n+        With fact validation enabled and an invalid fact, aggregate-validations should detect\\n+        not-all-valid and route back to the assistant for a correction pass in the same stage.\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_help\\n+      env:\\n+        ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+      mocks:\\n+        comment-assistant:\\n+          text: \\\"We rely on defaults/.visor.yaml line 11 for max_parallelism=4.\\\"\\n+          intent: comment_reply\\n+        extract-facts:\\n+          - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+        validate-fact[]:\\n+          - fact_id: f1\\n+            claim: \\\"max_parallelism defaults to 4\\\"\\n+            is_valid: false\\n+            confidence: high\\n+            evidence: \\\"defaults/.visor.yaml:11 does not set 4\\\"\\n+            correction: \\\"max_parallelism defaults to 3\\\"\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+          - step: extract-facts\\n+            exactly: 1\\n+          - step: validate-fact\\n+            at_least: 1\\n+          - step: aggregate-validations\\n+            exactly: 1\\n+          - step: init-fact-validation\\n+            exactly: 1\\n\",\"status\":\"added\"},{\"filename\":\"defaults/.visor.yaml\",\"additions\":1,\"deletions\":1,\"changes\":25,\"patch\":\"diff --git a/defaults/.visor.yaml b/defaults/.visor.yaml\\nindex 0e018884..21fad9eb 100644\\n--- a/defaults/.visor.yaml\\n+++ b/defaults/.visor.yaml\\n@@ -452,8 +452,9 @@ steps:\\n         - Provide partial answers when possible, and indicate what additional information would help give a complete response\\n     on: [issue_comment]\\n     on_success:\\n-      # Always initialize fact validation attempt counter\\n-      run: [init-fact-validation]\\n+      # Initialize fact validation attempt counter only when validation is enabled\\n+      run_js: |\\n+        return env.ENABLE_FACT_VALIDATION === 'true' ? ['init-fact-validation'] : []\\n       # Preserve intent-based rerun: allow members to retrigger overview from a comment\\n       goto_js: |\\n         const intent = (typeof output === 'object' && output) ? output.intent : undefined;\\n@@ -619,8 +620,14 @@ steps:\\n     # After all facts are validated, aggregate results and decide next action\\n     on_finish:\\n       run: [aggregate-validations]\\n+      # If aggregation stored validation issues in memory, schedule a correction reply\\n+      run_js: |\\n+        const issues = memory.list('fact-validation').includes('fact_validation_issues')\\n+          ? memory.get('fact_validation_issues', 'fact-validation')\\n+          : [];\\n+        return Array.isArray(issues) && issues.length > 0 ? ['comment-assistant'] : [];\\n       goto_js: |\\n-        const allValid = memory.get('all_valid', 'fact-validation');\\n+        const allValid = memory.get('all_facts_valid', 'fact-validation');\\n         const attempt = memory.get('fact_validation_attempt', 'fact-validation') || 0;\\n \\n         log('🔍 Fact validation complete - allValid:', allValid, 'attempt:', attempt);\\n@@ -702,11 +709,10 @@ steps:\\n     on: [issue_opened, issue_comment]\\n     if: \\\"env.ENABLE_FACT_VALIDATION === 'true'\\\"\\n     on_success:\\n-      goto_js: |\\n-        // Route back to the appropriate assistant if there are issues\\n-        if (!output || output.all_valid) return null;\\n-        const hasComment = !!(outputs['comment-assistant']) || (outputs.history && (outputs.history['comment-assistant'] || []).length > 0);\\n-        return hasComment ? 'comment-assistant' : 'issue-assistant';\\n+      # Schedule the correction reply directly (target-only) when not all facts are valid\\n+      run_js: |\\n+        if (!output || output.all_valid) return [];\\n+        return ['comment-assistant'];\\n     memory_js: |\\n       const validations = outputs.history['validate-fact'] || [];\\n \\n@@ -722,8 +728,7 @@ steps:\\n       log('Results: valid=' + (validations.length - invalid.length - lowConfidence.length),\\n           'invalid=' + invalid.length, 'low-confidence=' + lowConfidence.length);\\n \\n-      memory.set('all_valid', allValid, 'fact-validation');\\n-      memory.set('total_validations', validations.length, 'fact-validation');\\n+      memory.set('all_facts_valid', allValid, 'fact-validation');\\n       memory.set('validation_results', validations, 'fact-validation');\\n       memory.set('invalid_facts', invalid, 'fact-validation');\\n       memory.set('low_confidence_facts', lowConfidence, 'fact-validation');\\n\",\"status\":\"modified\"},{\"filename\":\"docs/test-framework-rfc.md\",\"additions\":22,\"deletions\":0,\"changes\":641,\"patch\":\"diff --git a/docs/test-framework-rfc.md b/docs/test-framework-rfc.md\\nnew file mode 100644\\nindex 00000000..7b899980\\n--- /dev/null\\n+++ b/docs/test-framework-rfc.md\\n@@ -0,0 +1,641 @@\\n+# Visor Integration Test Framework (RFC)\\n+\\n+Status: In Progress\\n+Date: 2025-10-27\\n+Owners: @probelabs/visor\\n+\\n+## Summary\\n+\\n+Add a first‑class, YAML‑native integration test framework for Visor that lets teams describe user flows, mocks, and assertions directly alongside their Visor config. Tests are defined in a separate YAML that can `extends` the base configuration, run entirely offline (no network), and default to strict verification.\\n+\\n+Key ideas:\\n+- Integration‑first: simulate real GitHub events and repo context; no manual step lists.\\n+- Strict by default: if a step ran and you didn’t assert it, the test fails.\\n+- Provider record mode by default: GitHub calls are intercepted and recorded (no network); assert them later.\\n+- Simple mocks keyed by step name; schema‑aware AI outputs (objects/arrays for structured schemas; `text` for plain).\\n+- Support multi‑event “flows” that preserve memory and outputs across events.\\n+\\n+## Motivation\\n+\\n+- Keep tests next to config and use the same mental model: events → checks → outputs → effects.\\n+- Validate real behavior (routing, `on` filters, `if` guards, `goto`/`on_success`, forEach) rather than unit‑style steps.\\n+- Make CI reliable and offline by default while still asserting side‑effects (labels, comments, check runs).\\n+\\n+## Non‑Goals\\n+\\n+- Unit testing individual providers (covered by Jest/TS tests).\\n+- Golden CI logs; we assert structured outputs and recorded operations instead.\\n+\\n+## Terminology\\n+\\n+- Case: a single integration test driven by one event + fixture.\\n+- Flow: an ordered list of cases; runner preserves memory/outputs across steps.\\n+- Fixture: a reusable external context (webhook payload, changed files, env, fs overlay, frozen clock).\\n+\\n+## File Layout\\n+\\n+- Base config (unchanged): `defaults/.visor.yaml` (regular steps live here).\\n+- Test suite (new): `defaults/.visor.tests.yaml`\\n+  - `extends: \\\".visor.yaml\\\"` to inherit the base checks.\\n+  - Contains `tests.defaults`, `tests.fixtures`, `tests.cases`.\\n+\\n+## Default Behaviors (Test Mode)\\n+\\n+- Strict mode: enabled by default (`tests.defaults.strict: true`). Any executed step must appear in `expect.calls`, or the case fails.\\n+- GitHub recording: the runner uses a recording Octokit by default; no network calls are made. Assert effects via `expect.calls` with `provider: github` and an `op` (e.g., `issues.createComment`, `labels.add`, `checks.create`).\\n+- AI provider: `mock` by default for tests; schema‑aware handling (see below).\\n+\\n+## Built‑in Fixtures and GitHub Mocks\\n+\\n+The runner ships with a library of built‑in fixtures and a recording GitHub mock so you don’t have to redefine common scenarios.\\n+\\n+### Built‑in Fixtures (gh.*)\\n+\\n+Use via `fixture: <name>`:\\n+\\n+- `gh.pr_open.minimal` — pull_request opened with a small PR (branch/base, 1–2 files, tiny patch).\\n+- `gh.pr_sync.minimal` — pull_request synchronize (new commit pushed) with updated HEAD SHA.\\n+- `gh.issue_open.minimal` — issues opened with a short title/body.\\n+- `gh.issue_comment.standard` — issue_comment created with a normal message on a PR.\\n+- `gh.issue_comment.visor_help` — issue_comment created with \\\"/visor help\\\".\\n+- `gh.issue_comment.visor_regenerate` — issue_comment created with \\\"/visor Regenerate reviews\\\".\\n+- `gh.issue_comment.edited` — issue_comment edited event.\\n+- `gh.pr_closed.minimal` — pull_request closed event.\\n+\\n+All gh.* fixtures populate:\\n+- `webhook` (name, action, payload)\\n+- `git` (branch, baseBranch)\\n+- `files` and `diff` (for PR fixtures)\\n+- `env` and `time.now` for determinism\\n+\\n+Optional overrides (future):\\n+\\n+```yaml\\n+fixture:\\n+  builtin: gh.pr_open.minimal\\n+  overrides:\\n+    pr.title: \\\"feat: custom title\\\"\\n+    webhook.payload.pull_request.number: 42\\n+```\\n+\\n+### GitHub Recorder (Built‑in)\\n+\\n+By default in test mode, the runner installs a recording Octokit:\\n+- Captures all calls and args for assertions (`expect.calls` with `provider: github`).\\n+- Returns stable stub shapes to unblock flows:\\n+  - `issues.createComment` → `{ data: { id, html_url, body, user, created_at } }`\\n+  - `issues.updateComment` → same shape\\n+  - `pulls.get`, `pulls.listFiles` → derived from fixture\\n+  - `checks.create`, `checks.update` → `{ data: { id, status, conclusion, url } }`\\n+  - `labels.add` → `{ data: { labels: [ ... ] } }` (or a no‑op with capture)\\n+\\n+  No network calls are made. You can still opt into real Octokit in the future with a `mode: passthrough` runner flag (not default).\\n+  Optional negative modes (per case or global):\\n+  - `error(429|422|404)` — simulate API errors; captured in call history.\\n+  - `timeout(1000ms)` — simulate request timeouts.\\n+\\n+## YAML Syntax Overview\\n+\\n+Minimal suite:\\n+\\n+```yaml\\n+version: \\\"1.0\\\"\\n+extends: \\\".visor.yaml\\\"\\n+\\n+tests:\\n+  defaults:\\n+    strict: true\\n+    ai_provider: mock\\n+    macros:\\n+      expect_review_posted:\\n+        calls:\\n+          - provider: github\\n+            op: issues.createComment\\n+            at_least: 1\\n+\\n+  fixtures: []   # (Optional) rely on gh.* built‑ins\\n+\\n+  cases:\\n+    - name: label-flow\\n+      event: pr_opened\\n+      fixture:\\n+        builtin: gh.pr_open.minimal\\n+        overrides:\\n+          pr.title: \\\"feat: add user search\\\"\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview body\\\"\\n+          tags: { label: feature, review-effort: 2 }\\n+      expect:\\n+        use: [expect_review_posted]\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args:\\n+              contains_unordered: [feature, \\\"review/effort:2\\\"]\\n+```\\n+\\n+### Flows (multi‑event)\\n+\\n+```yaml\\n+- name: pr-review-e2e-flow\\n+  strict: true\\n+  flow:\\n+    - name: pr-open\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview: { text: \\\"Overview body\\\", tags: { label: feature, review-effort: 2 } }\\n+        security: { issues: [] }\\n+        quality: { issues: [] }\\n+        performance: { issues: [] }\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: security\\n+            exactly: 1\\n+          - step: architecture\\n+            exactly: 1\\n+          - step: performance\\n+            exactly: 1\\n+          - step: quality\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: issues.createComment\\n+            at_least: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args: { contains: [feature] }\\n+\\n+    - name: visor-plain\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_help\\n+      mocks:\\n+        comment-assistant: { text: \\\"Sure, here's how I can help.\\\", intent: comment_reply }\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+          - provider: github\\n+            op: issues.createComment\\n+            exactly: 1\\n+        outputs:\\n+          - step: comment-assistant\\n+            path: intent\\n+            equals: comment_reply\\n+```\\n+\\n+## CLI Usage\\n+\\n+- Discover tests:\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --list`\\n+- Validate test file shape (schema):\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --validate`\\n+- Run all tests with compact progress (default):\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml`\\n+- Run a single case:\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --only label-flow`\\n+- Run a single stage in a flow (by name or 1‑based index):\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --only pr-review-e2e-flow#facts-invalid`\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --only pr-review-e2e-flow#3`\\n+- Emit artifacts:\\n+  - JSON: `--json output/visor-tests.json`\\n+  - JUnit: `--report junit:output/visor-tests.xml`\\n+  - Markdown summary: `--summary md:output/visor-tests.md`\\n+- Debug logs:\\n+  - Set `VISOR_DEBUG=true` for verbose routing/provider output.\\n+\\n+Notes\\n+- AI is forced to `mock` in test mode regardless of API keys.\\n+- The runner warns when an AI/command step runs without a mock (suppressed for `ai.provider=mock`).\\n+- Strict mode is on by default; add `strict: false` for prompt‑only cases.\\n+\\n+## Mocks (Schema‑Aware)\\n+\\n+- Keyed by step name under `mocks`.\\n+- AI with structured `schema` (e.g., `code-review`, `issue-assistant`): provide an object or array directly; no `returns` key.\\n+- AI with `schema: plain`: provide a string (or an object with `text`).\\n+- Command provider: `{ stdout: string, exit_code?: number }`.\\n+- Arrays: return arrays directly (e.g., `extract-facts`).\\n+\\n+Examples:\\n+\\n+```yaml\\n+mocks:\\n+  overview:\\n+    text: \\\"Overview body\\\"\\n+    tags: { label: feature, review-effort: 2 }\\n+\\n+  issue-assistant:\\n+    text: \\\"Thanks for the detailed report!\\\"\\n+    intent: issue_triage\\n+    labels: [bug]\\n+\\n+  extract-facts:\\n+    - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+\\n+  unit-tests:\\n+    stdout: '{\\\"passed\\\": 128, \\\"failed\\\": 0}'\\n+    exit_code: 0\\n+```\\n+\\n+## Assertions\\n+\\n+### Macros (Reusable Assertions)\\n+\\n+Define named bundles of assertions under `tests.defaults.macros` and reuse them via `expect.use: [macroName, ...]`.\\n+\\n+Example:\\n+\\n+```yaml\\n+tests:\\n+  defaults:\\n+    macros:\\n+      expect_review_posted:\\n+        calls:\\n+          - provider: github\\n+            op: issues.createComment\\n+            at_least: 1\\n+\\n+cases:\\n+  - name: example\\n+    expect:\\n+      use: [expect_review_posted]\\n+      calls:\\n+        - step: overview\\n+          exactly: 1\\n+```\\n+\\n+- Step calls: `expect.calls: [{ step: <name>, exactly|at_least|at_most: N }]`.\\n+- GitHub effects: `expect.calls: [{ provider: github, op: <owner.method>, times?, args? }]`.\\n+  - `op` examples: `issues.createComment`, `labels.add`, `checks.create`, `checks.update`.\\n+  - `args.contains` matches arrays/strings; `args.contains_unordered` ignores order; `args.equals` for strict equality.\\n+- Outputs: `expect.outputs: [{ step, path, equals|matches|equalsDeep }]`.\\n+  - `equalsDeep` performs deep structural comparison for objects/arrays.\\n+  - `path` uses dot/bracket syntax, e.g., `tags['review-effort']`, `issues[0].severity`.\\n+- Failures: `expect.fail.message_contains` for error message anchoring.\\n+- Strict violations: `expect.strict_violation.for_step` asserts the runner surfaced “step executed without expect.”\\n+\\n+### Prompt Assertions (AI)\\n+\\n+When mocking AI, you can assert on the final prompt text constructed by Visor (after Liquid templating and context injection):\\n+\\n+```yaml\\n+expect:\\n+  prompts:\\n+    - step: overview\\n+      contains:\\n+        - \\\"feat: add user search\\\"        # PR title from fixture\\n+        - \\\"diff --git a/src/search.ts\\\"   # patch content included\\n+      not_contains:\\n+        - \\\"BREAKING CHANGE\\\"\\n+    - step: comment-assistant\\n+      matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"   # case-insensitive regex\\n+```\\n+\\n+Rules:\\n+- `contains`: list of substrings that must appear in the prompt.\\n+- `not_contains`: list of substrings that must not appear.\\n+- `matches`: a single regex pattern string; add `(?i)` for case‑insensitive.\\n+- The runner captures the exact prompt Visor would send to the provider (with dynamic variables resolved and code context embedded) and evaluates these assertions.\\n+\\n+## Runner Semantics\\n+\\n+- Loads base config via `extends` and validates.\\n+- Applies fixture:\\n+  - Webhook payload → test event context\\n+  - Git metadata (branch/baseBranch)\\n+  - Files + patch list used by analyzers/prompts\\n+  - `fs_overlay` writes transient files (cleaned up after)\\n+  - `env` overlays process env for the case\\n+  - `time.now` freezes clock\\n+- Event routing: determines which checks run by evaluating `on`, `if`, `depends_on`, `goto`, `on_success`, and `forEach` semantics in the normal engine.\\n+- Recording providers:\\n+  - GitHub: recording Octokit (default) captures every call; no network.\\n+  - AI: mock provider that emits objects/arrays/strings per mocks and records the final prompt text per step for `expect.prompts`.\\n+\\n+### Call History and Recursion\\n+\\n+Some steps (e.g., fact validation loops) can run multiple times within a single case or flow stage. The runner records an invocation history for each step. You assert using the same top‑level sections (calls, prompts, outputs) with selectors:\\n+\\n+1) Count only\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: validate-fact\\n+      at_least: 2\\n+```\\n+\\n+2) Per‑call assertions by index (ordered)\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: validate-fact\\n+      exactly: 2\\n+  prompts:\\n+    - step: validate-fact\\n+      index: 0\\n+      contains: [\\\"Claim:\\\", \\\"max_parallelism defaults to 4\\\"]\\n+  outputs:\\n+    - step: validate-fact\\n+      index: 0\\n+      path: fact_id\\n+      equals: f1\\n+```\\n+\\n+3) Per‑call assertions without assuming order (filter by output)\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: validate-fact\\n+      at_least: 2\\n+  outputs:\\n+    - step: validate-fact\\n+      where: { path: fact_id, equals: f1 }\\n+      path: is_valid\\n+      equals: true\\n+    - step: validate-fact\\n+      where: { path: fact_id, equals: f2 }\\n+      path: confidence\\n+      equals: high\\n+```\\n+\\n+4) Select a specific history element\\n+\\n+```yaml\\n+expect:\\n+  prompts:\\n+    - step: validate-fact\\n+      index: last   # or 0,1,..., or 'first'\\n+      not_contains: [\\\"TODO\\\"]\\n+```\\n+  - HTTP: built‑in mock (url/method/status/body/latency) with record mode and assertions.\\n+  - Command: mock stdout/stderr/exit_code; record invocation for assertions.\\n+ - State across flows: `memory`, `outputs.history`, and step outputs persist across events within a single flow.\\n+- Strict enforcement: after execution, compare executed steps to `expect.calls`; any missing expect fails the case.\\n+\\n+## Validation & Helpful Errors\\n+\\n+- Reuse Visor's existing Ajv pipeline for the base config (`extends` target).\\n+- The tests DSL is validated at runtime with friendly errors (no separate schema file to maintain).\\n+- Errors show the YAML path, a short hint, and an example (e.g., suggest `args.contains_unordered` when order differs).\\n+- Inline diffs for strings (prompts) and objects (with deep compare) in failure output.\\n+\\n+### Determinism & Security\\n+\\n+- Stable IDs in the GitHub recorder (deterministic counters per run).\\n+- Order‑agnostic assertions for arrays via `args.contains_unordered`.\\n+- Prompt normalization (whitespace, code fences). Toggle with `--normalize-prompts=false`.\\n+- Secret redaction in prompts/args via ENV allowlist (default deny; redacts to `****`).\\n+\\n+## CLI\\n+\\n+```\\n+visor test --config defaults/.visor.tests.yaml         # run all cases\\n+visor test --config defaults/.visor.tests.yaml --only pr-review-e2e-flow\\n+visor test --config defaults/.visor.tests.yaml --list  # list case names\\n+```\\n+\\n+Exit codes:\\n+- 0: all tests passed\\n+- 1: one or more cases failed\\n+\\n+### CLI Output UX (must‑have)\\n+\\n+The runner prints a concise, human‑friendly summary optimized for scanning:\\n+\\n+- Suite header with total cases and elapsed time.\\n+- Per‑case line with status symbol and duration, e.g.,\\n+  - ✅ label-flow (1.23s)\\n+  - ❌ security-fail-if (0.42s)\\n+- When a case is expanded (auto‑expand on failure):\\n+  - Input context: event + fixture name.\\n+  - Executed steps (in order), with counts for multi‑call steps.\\n+  - Assertions grouped by type (calls, prompts, outputs) with checkmarks.\\n+  - GitHub calls table (op, count, first args snippet).\\n+  - Prompt preview (truncated) with a toggle to show full text.\\n+  - First mismatch shows an inline diff (expected vs actual substring/regex or value), with a clear hint to fix.\\n+- Flow cases show each stage nested under the parent with roll‑up status.\\n+- Summary footer with pass/fail counts, slowest cases, and a hint to rerun focused:\\n+  - e.g., visor test --config defaults/.visor.tests.yaml --only security-fail-if\\n+\\n+Color, symbols, and truncation rules mirror our main CLI:\\n+- Green checks for passes, red crosses for failures, yellow for skipped.\\n+- Truncate long prompts/JSON with ellipsis; provide a flag `--verbose` to show full payloads.\\n+\\n+### Additional Flags & Modes\\n+\\n+- `--only <name>`: run a single case/flow by exact name.\\n+- `--bail`: stop at first failure.\\n+- `--json`: emit machine‑readable results to stdout.\\n+- `--report junit:path.xml`: write JUnit XML to path.\\n+- `--summary md:path.md`: write a Markdown summary artifact.\\n+- `--progress compact|detailed`: toggle rendering density.\\n+- `--max-parallel N`: reuse existing parallelism flag (no test‑specific variant).\\n+\\n+## Coverage & Reporting\\n+\\n+- Step coverage per case (executed vs expected), with a short table.\\n+- JUnit and JSON reporters for CI visualization.\\n+- Optional Markdown summary: failing cases, first mismatch, rerun hints.\\n+\\n+## Implementation Plan (Milestones)\\n+\\n+This plan delivers the test framework incrementally, minimizing risk and reusing Visor internals.\\n+\\n+Progress Tracker\\n+- Milestone 0 — DSL freeze and scaffolding — DONE (2025-10-27)\\n+- Milestone 1 — MVP runner and single‑event cases — DONE (2025-10-27)\\n+- Milestone 2 — Built‑in fixtures — DONE (2025-10-27)\\n+- Milestone 3 — Prompt capture and assertions — DONE (2025-10-27)\\n+- Milestone 4 — Multi‑call history and selectors — DONE (2025-10-27)\\n+- Milestone 5 — Flows and state persistence — DONE (2025-10-27)\\n+- Milestone 6 — HTTP/Command mocks + negative modes — DONE (2025-10-27)\\n+- Milestone 7 — CLI reporters/UX polish — DONE (2025-10-27)\\n+- Milestone 8 — Validation and helpful errors — DONE (2025-10-27)\\n+- Milestone 9 — Coverage and perf — DONE (2025-10-27)\\n+- Milestone 10 — Docs, examples, migration — PENDING\\n+\\n+Progress Update — 2025-10-28\\n+- Runner: stage execution coverage now derives only from actual prompts/output-history deltas and engine statistics (no selection heuristics). Single-check runs contribute to statistics and history uniformly.\\n+- Engine: single-check path records iteration stats and appends outputs to history; on_finish children run via unified scheduler so runs are counted.\\n+- UX: noisy debug prints gated behind VISOR_DEBUG; stage headers and coverage tables remain.\\n+- Known gap: flow stage “facts-invalid” still fails under strict because the initial assistant/validation chain does not execute under the test runner for issue_comment; aggregator fallback runs. Next step is to trace event filtering inside executeGroupedChecks and ensure the main stage selection executes event-matching checks in tests.\\n+\\n+Milestone 0 — DSL freeze and scaffolding (0.5 week) — DONE 2025-10-27\\n+- Finalize DSL keys: tests.defaults, fixtures, cases, flow, fixture, mocks, expect.{calls,prompts,outputs,fail,strict_violation}. ✅\\n+- Rename use_fixture → fixture across examples (done in this RFC and defaults/.visor.tests.yaml). ✅\\n+- Create module skeletons: ✅\\n+  - src/test-runner/index.ts (entry + orchestration)\\n+  - src/test-runner/fixture-loader.ts (builtin + overrides)\\n+  - src/test-runner/recorders/github-recorder.ts (now dynamic Proxy-based)\\n+  - src/test-runner/assertions.ts (calls/prompts/outputs types + count validator)\\n+  - src/test-runner/utils/selectors.ts (deepGet)\\n+- CLI: add visor test (discovery). ✅\\n+- Success criteria: builds pass; “hello world” run prints discovered cases. ✅ (verified via npm run build and visor test)\\n+\\n+Progress Notes\\n+- Discovery works against any .visor.tests.yaml (general-purpose, not tied to defaults).\\n+- Recording Octokit records arbitrary rest ops without hardcoding method lists.\\n+- defaults/.visor.tests.yaml updated to consistent count grammar and fixed indentation issues.\\n+\\n+Milestone 1 — MVP runner and single‑event cases (1 week) — DONE 2025-10-27 (non‑flow)\\n+- CLI: add visor test [--config path] [--only name] [--bail] [--list]. ✅\\n+- Parsing: load tests file (extends) and hydrate cases. ✅\\n+- Execution: per case (non‑flow), synthesize PRInfo and call CheckExecutionEngine once. ✅\\n+- GitHub recorder default: injected recording Octokit; no network. ✅\\n+- Assertions: expect.calls for steps and provider ops (exactly|at_least|at_most); strict mode enforced. ✅\\n+- Output: basic per‑case status + summary. ✅\\n+- Success criteria: label-flow, issue-triage, strict-mode-example, security-fail-if pass locally. ✅\\n+\\n+Notes\\n+- Flow cases are deferred to Milestone 5 (state persistence) and will be added later.\\n+- AI provider forced to mock in test mode unless overridden by suite defaults.\\n+\\n+Verification\\n+- Build CLI + SDK: npm run build — success.\\n+- Discovery: visor test --config defaults/.visor.tests.yaml --list — lists suite and cases.\\n+- Run single cases:\\n+  - visor test --config defaults/.visor.tests.yaml --only label-flow — PASS\\n+  - visor test --config defaults/.visor.tests.yaml --only issue-triage — PASS\\n+  - visor test --config defaults/.visor.tests.yaml --only security-fail-if — PASS\\n+  - visor test --config defaults/.visor.tests.yaml --only strict-mode-example — PASS\\n+- Behavior observed:\\n+  - Strict mode enforced (steps executed but not asserted would fail). \\n+  - GitHub ops recorded by default with dynamic recorder, no network calls.\\n+  - Provider and step call counts respected (exactly | at_least | at_most).\\n+\\n+Milestone 2 — Built‑in fixtures (0.5–1 week) — DONE 2025-10-27\\n+- Implement gh.* builtins: pr_open.minimal, pr_sync.minimal, issue_open.minimal, issue_comment.standard, issue_comment.visor_help, issue_comment.visor_regenerate.\\n+- Support fixture overrides (fixture: { builtin, overrides }).\\n+- Wire files+diff into the engine’s analyzers.\\n+- Success criteria: pr-review-e2e-flow “pr-open”, “standard-comment”, “visor-plain”, “visor-retrigger” run with built-ins.\\n+Notes:\\n+- gh.* builtins implemented with webhook payloads and minimal diffs for PR fixtures.\\n+- Runner accepts fixture: { builtin, overrides } and applies overrides to pr.* and webhook.* paths.\\n+- Diffs surfaced via PRInfo.fullDiff; prompts include diff header automatically.\\n+- Flow execution will be delivered in Milestone 5; the same built-ins power the standalone prompt cases added now.\\n+\\n+Milestone 3 — Prompt capture and prompt assertions (0.5 week) — DONE 2025-10-27\\n+- Capture final AI prompt string per step after Liquid/context assembly. ✅ (AICheckProvider hook)\\n+- Assertions: expect.prompts contains | not_contains | matches (regex). ✅\\n+- Add `prompts.where` selector to target a prompt from history by content. ✅\\n+- Success criteria: prompt checks pass for label-flow, issue-triage, visor-plain, visor-retrigger. ✅\\n+- Notes: added standalone cases visor-plain-prompt and visor-retrigger-prompt for prompt-only validation.\\n+\\n+Milestone 4 — Multi‑call history and selectors (1 week) — DONE 2025-10-27\\n+- Per-step invocation history recorded and exposed by engine (outputs.history). ✅\\n+- index selector for prompts and outputs (first|last|N). ✅\\n+- where selector for outputs: { path, equals|matches }. ✅\\n+- equalsDeep for outputs. ✅\\n+- contains_unordered for array outputs. ✅\\n+- Regex matches for outputs. ✅\\n+\\n+Milestone 5 — Flows and state persistence (0.5–1 week) — DONE 2025-10-27\\n+- Implemented flow execution with shared engine + recorder across stages. ✅\\n+- Preserves MemoryStore state, outputs.history and provider calls between stages. ✅\\n+- Stage-local deltas for assertions (prompts, outputs, calls). ✅\\n+- Success criteria: full pr-review-e2e-flow passes. ✅\\n+\\n+Milestone 6 — HTTP/Command mocks and advanced GitHub modes (1 week) — DONE 2025-10-27\\n+- Command mocks: runner injects mocks via ExecutionContext; provider short-circuits to return stdout/exit_code. ✅\\n+- HTTP client mocks: provider returns mocked response via ExecutionContext without network. ✅\\n+- GitHub recorder negative modes: error(code) and timeout(ms) supported via tests.defaults.github_recorder. ✅\\n+- Success criteria: command-mock-shape passes; negative modes available for future tests. ✅\\n+\\n+Milestone 7 — CLI UX polish and reporters (0.5–1 week) — DONE 2025-10-27\\n+- Flags: --json <path|->, --report junit:<path>, --summary md:<path>, --progress compact|detailed. ✅\\n+- Compact progress with per-case PASS/FAIL lines; summary at end. ✅\\n+- JSON/JUnit/Markdown reporters now include per-case details (name, pass/fail, errors). ✅\\n+\\n+Milestone 8 — Validation and helpful errors (0.5 week) — DONE 2025-10-27\\n+- Reuse ConfigManager + Ajv for base config. ✅\\n+- Lightweight runtime validation for tests DSL with precise YAML paths and hints. ✅\\n+- Add `visor test --validate` to check the tests file only (reuses runtime validation). ✅\\n+- Success criteria: common typos produce actionable errors (path + suggestion). ✅\\n+\\n+Usage:\\n+\\n+```\\n+visor test --validate --config defaults/.visor.tests.yaml\\n+```\\n+\\n+Example error output:\\n+\\n+```\\n+❌ Tests file has 2 error(s):\\n+   • tests.cases[0].expext: must NOT have additional properties (Did you mean \\\"expect\\\"?)\\n+   • tests.cases[3].event: must be equal to one of the allowed values (allowed: manual, pr_opened, pr_updated, pr_closed, issue_opened, issue_comment)\\n+```\\n+\\n+Milestone 9 — Coverage and perf (0.5 week) — DONE 2025-10-27\\n+- Per-case coverage table printed after assertions: each expected step shows desired count (e.g., =1/≥1/≤N), actual runs, and status (ok/under/over). ✅\\n+- Parallel case execution: `--max-parallel <N>` or `tests.defaults.max_parallel` enables a simple pool runner. ✅\\n+- Prompt capture throttle: `--prompt-max-chars <N>` or `tests.defaults.prompt_max_chars` truncates stored prompt text to reduce memory. ✅\\n+- Success criteria: coverage table visible; options validated locally. ✅\\n+\\n+Usage examples:\\n+\\n+```\\n+visor test --config defaults/.visor.tests.yaml --max-parallel 4\\n+visor test --config defaults/.visor.tests.yaml --prompt-max-chars 16000\\n+```\\n+\\n+Milestone 10 — Docs, examples, and migration (0.5 week) — IN PROGRESS 2025-10-27\\n+- Update README to link the RFC and defaults/.visor.tests.yaml.\\n+- Document built-in fixtures catalog and examples.\\n+- Migration note: how to move from embedded tests and from `returns` to new mocks.\\n+- Success criteria: docs reviewed; examples copy‑paste clean.\\n+\\n+Risks & Mitigations\\n+- Prompt capture bloat → truncate by default; add --verbose.\\n+- Fixture drift vs engine → keep fixtures minimal and aligned to CheckExecutionEngine needs; add contract tests.\\n+- Strict mode false positives → provide clear errors and fast “add expect” guidance.\\n+\\n+Success Metrics\\n+- 100% of default cases pass locally and in CI.\\n+- Sub‑second overhead per case on small fixtures; <10s for the full default suite.\\n+- Clear failures with a single screen of output; <1 minute to fix typical assertion mismatches.\\n+\\n+## Compatibility & Migration\\n+\\n+- Tests moved from `defaults/.visor.yaml` into `defaults/.visor.tests.yaml` with `extends: \\\".visor.yaml\\\"`.\\n+- Old `mocks.*.returns` is replaced by direct values (object/array/string).\\n+- You no longer need `run: steps` in tests; cases are integration‑driven by `event + fixture`.\\n+- `no_other_calls` is unnecessary with strict mode; it’s implied and enforced.\\n+\\n+## Open Questions\\n+\\n+- Should we support HTTP provider mocks out of the box (URL/method/body → recorded responses)?\\n+- Do we want a JSONPath for `expect.outputs.path`, or keep the current dot/bracket selector?\\n+- Snapshots of generated Markdown? Perhaps as optional golden files with normalization.\\n+\\n+## Future Work\\n+\\n+- Watch mode (`--watch`) and focused runs by regex.\\n+- Coverage‑like reports for step execution and assertions.\\n+- Built‑in fixtures for common GitHub events (shortcuts).\\n+- Golden snapshot helpers for comments and label sets (with stable normalization).\\n+- Parallelizing cases and/or flows.\\n+\\n+## Appendix: Example Suite\\n+\\n+See `defaults/.visor.tests.yaml` in the repo for a complete, multi‑event example covering:\\n+- PR opened → overview + labels\\n+- Standard PR comment → no action\\n+- `/visor` comment → reply\\n+- `/visor ... Regenerate reviews` → retrigger overview\\n+- Fact validation enabled/disabled on comment\\n+- New commit pushed to PR → refresh overview\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/assertions.md\",\"additions\":3,\"deletions\":0,\"changes\":85,\"patch\":\"diff --git a/docs/testing/assertions.md b/docs/testing/assertions.md\\nnew file mode 100644\\nindex 00000000..e5f62aca\\n--- /dev/null\\n+++ b/docs/testing/assertions.md\\n@@ -0,0 +1,85 @@\\n+# Writing Assertions\\n+\\n+Assertions live under `expect:` and cover three surfaces:\\n+\\n+- `calls`: step counts and provider effects (GitHub ops)\\n+- `prompts`: final AI prompts (post templating/context)\\n+- `outputs`: step outputs with history and selectors\\n+\\n+## Calls\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: overview\\n+      exactly: 1\\n+    - provider: github\\n+      op: labels.add\\n+      at_least: 1\\n+      args:\\n+        contains: [feature, \\\"review/effort:2\\\"]\\n+```\\n+\\n+Counts are consistent everywhere: `exactly`, `at_least`, `at_most`.\\n+\\n+## Prompts\\n+\\n+```yaml\\n+expect:\\n+  prompts:\\n+    - step: overview\\n+      contains: [\\\"feat: add user search\\\", \\\"diff --git a/src/search.ts\\\"]\\n+    - step: comment-assistant\\n+      matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"\\n+    - step: overview\\n+      # Select the prompt that mentions a specific file\\n+      where:\\n+        contains: [\\\"src/search.ts\\\"]\\n+      contains: [\\\"diff --git a/src/search.ts\\\"]\\n+```\\n+\\n+- `contains`: required substrings\\n+- `not_contains`: forbidden substrings\\n+- `matches`: regex (prefix `(?i)` for case-insensitive)\\n+- `index`: `first` | `last` | N (default: last)\\n+- `where`: selector to choose a prompt from history using `contains`/`not_contains`/`matches` before applying the assertion\\n+\\n+Tip: enable `--prompt-max-chars` or `tests.defaults.prompt_max_chars` to cap stored prompt size for large diffs.\\n+\\n+## Outputs\\n+\\n+Use `path` with dot/bracket syntax. You can select by index or by a `where` probe over the same output history.\\n+\\n+```yaml\\n+expect:\\n+  outputs:\\n+    - step: validate-fact\\n+      index: 0\\n+      path: fact_id\\n+      equals: f1\\n+    - step: validate-fact\\n+      where: { path: fact_id, equals: f2 }\\n+      path: confidence\\n+      equals: high\\n+    - step: aggregate-validations\\n+      path: all_valid\\n+      equals: true\\n+```\\n+\\n+Supported comparators:\\n+- `equals` (primitive)\\n+- `equalsDeep` (structural)\\n+- `matches` (regex)\\n+- `contains_unordered` (array membership ignoring order)\\n+\\n+## Strict mode and “no calls”\\n+\\n+Strict mode (default) fails any executed step without a corresponding `expect.calls` entry. You can also assert absence explicitly:\\n+\\n+```yaml\\n+expect:\\n+  no_calls:\\n+    - provider: github\\n+      op: issues.createComment\\n+    - step: extract-facts\\n+```\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/cli.md\",\"additions\":2,\"deletions\":0,\"changes\":37,\"patch\":\"diff --git a/docs/testing/cli.md b/docs/testing/cli.md\\nnew file mode 100644\\nindex 00000000..3d19fc10\\n--- /dev/null\\n+++ b/docs/testing/cli.md\\n@@ -0,0 +1,37 @@\\n+# Visor Test CLI\\n+\\n+Run integration tests for your Visor config using the built-in `test` subcommand.\\n+\\n+## Commands\\n+\\n+- Discover tests file and list cases\\n+  - `visor test --list [--config defaults/.visor.tests.yaml]`\\n+- Run cases\\n+  - `visor test [--config defaults/.visor.tests.yaml] [--only <substring>] [--bail]`\\n+- Validate tests YAML without running\\n+  - `visor test --validate [--config defaults/.visor.tests.yaml]`\\n+\\n+## Flags\\n+\\n+- `--config <path>`: Path to `.visor.tests.yaml` (auto-discovers `.visor.tests.yaml` or `defaults/.visor.tests.yaml`).\\n+- `--only <filter>`: Run cases whose `name` contains the substring (case-insensitive).\\n+- `--bail`: Stop on first failure.\\n+- `--json <path|->`: Write a minimal JSON summary.\\n+- `--report junit:<path>`: Write a minimal JUnit XML.\\n+- `--summary md:<path>`: Write a minimal Markdown summary.\\n+- `--progress compact|detailed`: Progress verbosity (parsing supported; detailed view evolves over time).\\n+- `--max-parallel <N>`: Run up to N cases concurrently.\\n+- `--prompt-max-chars <N>`: Truncate captured prompt text to N characters.\\n+\\n+## Output\\n+\\n+- Per-case PASS/FAIL lines\\n+- Coverage table (expected vs actual step runs)\\n+- Summary totals\\n+\\n+## Tips\\n+\\n+- Use `--validate` when iterating on tests to catch typos early.\\n+- Keep `strict: true` in `tests.defaults` to surface missing `expect` quickly.\\n+- For large suites, increase `--max-parallel` to improve throughput.\\n+\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/fixtures-and-mocks.md\",\"additions\":3,\"deletions\":0,\"changes\":74,\"patch\":\"diff --git a/docs/testing/fixtures-and-mocks.md b/docs/testing/fixtures-and-mocks.md\\nnew file mode 100644\\nindex 00000000..d49a1f13\\n--- /dev/null\\n+++ b/docs/testing/fixtures-and-mocks.md\\n@@ -0,0 +1,74 @@\\n+# Fixtures and Mocks\\n+\\n+Integration tests simulate outside world inputs and provider outputs.\\n+\\n+## Built-in GitHub fixtures (gh.*)\\n+\\n+Use via `fixture: gh.<name>` or `fixture: { builtin: gh.<name>, overrides: {...} }`.\\n+\\n+- `gh.pr_open.minimal` — pull_request opened with a tiny diff and `src/search.ts` file.\\n+- `gh.pr_sync.minimal` — pull_request synchronize with a small follow-up diff.\\n+- `gh.pr_closed.minimal` — pull_request closed event.\\n+- `gh.issue_open.minimal` — issues opened (short title/body).\\n+- `gh.issue_comment.standard` — normal human comment on a PR/issue.\\n+- `gh.issue_comment.visor_help` — comment containing `/visor help`.\\n+- `gh.issue_comment.visor_regenerate` — `/visor Regenerate reviews`.\\n+\\n+Overrides allow tailored inputs:\\n+\\n+```yaml\\n+fixture:\\n+  builtin: gh.pr_open.minimal\\n+  overrides:\\n+    pr.title: \\\"feat: custom title\\\"\\n+    webhook.payload.pull_request.number: 42\\n+```\\n+\\n+## GitHub recorder\\n+\\n+The test runner injects a recording Octokit by default:\\n+\\n+- Captures every GitHub op+args for assertions (`expect.calls` with `provider: github`).\\n+- Returns stable stub shapes so flows can continue without network.\\n+- Negative modes are available globally via `tests.defaults.github_recorder`:\\n+\\n+```yaml\\n+tests:\\n+  defaults:\\n+    github_recorder:\\n+      error_code: 429      # simulate API error\\n+      timeout_ms: 1000     # simulate request timeout\\n+```\\n+\\n+## Mocks\\n+\\n+Mocks are keyed by step name under `mocks`.\\n+\\n+Examples:\\n+\\n+```yaml\\n+mocks:\\n+  # AI with structured schema\\n+  overview:\\n+    text: \\\"High-level PR summary.\\\"\\n+    tags: { label: feature, review-effort: 2 }\\n+\\n+  # AI plain text schema\\n+  comment-assistant:\\n+    text: \\\"Sure, here’s how I can help.\\\"\\n+    intent: comment_reply\\n+\\n+  # Array outputs (e.g., extract-facts)\\n+  extract-facts:\\n+    - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+\\n+  # Command provider\\n+  unit-tests:\\n+    stdout: '{\\\"passed\\\": 128, \\\"failed\\\": 0}'\\n+    exit_code: 0\\n+```\\n+\\n+Notes:\\n+- No `returns:` key; provide values directly.\\n+- For HTTP/Command providers, mocks bypass real execution and are recorded for assertions.\\n+\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/getting-started.md\",\"additions\":4,\"deletions\":0,\"changes\":88,\"patch\":\"diff --git a/docs/testing/getting-started.md b/docs/testing/getting-started.md\\nnew file mode 100644\\nindex 00000000..c55996ef\\n--- /dev/null\\n+++ b/docs/testing/getting-started.md\\n@@ -0,0 +1,88 @@\\n+# Visor Tests — Getting Started\\n+\\n+This is the developer-facing guide for writing and running integration tests for your Visor config. It focuses on great DX: minimal setup, helpful errors, and clear output.\\n+\\n+## TL;DR\\n+\\n+- Put your tests in `defaults/.visor.tests.yaml`.\\n+- Reference your base config with `extends: \\\".visor.yaml\\\"`.\\n+- Use built-in GitHub fixtures like `gh.pr_open.minimal`.\\n+- Run with `visor test --config defaults/.visor.tests.yaml`.\\n+- Validate only with `visor test --validate`.\\n+\\n+```yaml\\n+version: \\\"1.0\\\"\\n+extends: \\\".visor.yaml\\\"\\n+\\n+tests:\\n+  defaults:\\n+    strict: true           # every executed step must be asserted\\n+    ai_provider: mock      # offline by default\\n+  cases:\\n+    - name: label-flow\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview body\\\"\\n+          tags: { label: feature, review-effort: 2 }\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args: { contains: [feature] }\\n+```\\n+\\n+## Why integration tests in YAML?\\n+\\n+- You test the same thing you ship: events → checks → outputs → effects.\\n+- No network required; GitHub calls are recorded, AI is mocked.\\n+- Flows let you simulate real user journeys across multiple events.\\n+\\n+## Strict by default\\n+\\n+If a step runs and you didn’t assert it under `expect.calls`, the case fails. This prevents silent regressions and “accidental” work.\\n+\\n+Turn off per-case via `strict: false` if you need to iterate.\\n+\\n+## CLI recipes\\n+\\n+- List cases: `visor test --list`\\n+- Run a subset: `visor test --only pr-review`\\n+- Stop on first failure: `--bail`\\n+- Validate tests file only: `--validate`\\n+- Parallelize cases: `--max-parallel 4`\\n+- Throttle prompt capture: `--prompt-max-chars 16000`\\n+\\n+## Coverage output\\n+\\n+After each case/stage, a compact table shows expected vs actual step calls:\\n+\\n+```\\n+Coverage (label-flow):\\n+  • overview                 want =1     got  1  ok\\n+  • apply-overview-labels    want =1     got  1  ok\\n+```\\n+\\n+Unexpected executed steps are listed under `unexpected:` to help you add missing assertions quickly.\\n+\\n+## Helpful validation errors\\n+\\n+Run `visor test --validate` to get precise YAML-path errors and suggestions:\\n+\\n+```\\n+❌ Tests file has 2 error(s):\\n+   • tests.cases[0].expext: must NOT have additional properties (Did you mean \\\"expect\\\"?)\\n+   • tests.cases[3].event: must be equal to one of the allowed values (allowed: manual, pr_opened, pr_updated, pr_closed, issue_opened, issue_comment)\\n+```\\n+\\n+Next steps:\\n+- See `docs/testing/fixtures-and-mocks.md` to simulate inputs.\\n+- See `docs/testing/assertions.md` to write robust checks.\\n+- Browse `defaults/.visor.tests.yaml` for full examples.\\n+\\n\",\"status\":\"added\"},{\"filename\":\"output/assistant-json/template.liquid\",\"additions\":0,\"deletions\":0,\"changes\":0,\"patch\":\"diff --git a/output/assistant-json/template.liquid b/output/assistant-json/template.liquid\\ndeleted file mode 100644\\nindex e69de29b..00000000\\n\",\"status\":\"modified\"},{\"filename\":\"src/ai-review-service.ts\",\"additions\":2,\"deletions\":1,\"changes\":62,\"patch\":\"diff --git a/src/ai-review-service.ts b/src/ai-review-service.ts\\nindex dac5b6e1..8adf6106 100644\\n--- a/src/ai-review-service.ts\\n+++ b/src/ai-review-service.ts\\n@@ -117,30 +117,36 @@ export class AIReviewService {\\n \\n     this.sessionRegistry = SessionRegistry.getInstance();\\n \\n-    // Auto-detect provider and API key from environment\\n-    if (!this.config.apiKey) {\\n-      if (process.env.CLAUDE_CODE_API_KEY) {\\n-        this.config.apiKey = process.env.CLAUDE_CODE_API_KEY;\\n-        this.config.provider = 'claude-code';\\n-      } else if (process.env.GOOGLE_API_KEY) {\\n-        this.config.apiKey = process.env.GOOGLE_API_KEY;\\n-        this.config.provider = 'google';\\n-      } else if (process.env.ANTHROPIC_API_KEY) {\\n-        this.config.apiKey = process.env.ANTHROPIC_API_KEY;\\n-        this.config.provider = 'anthropic';\\n-      } else if (process.env.OPENAI_API_KEY) {\\n-        this.config.apiKey = process.env.OPENAI_API_KEY;\\n-        this.config.provider = 'openai';\\n-      } else if (\\n-        // Check for AWS Bedrock credentials\\n-        (process.env.AWS_ACCESS_KEY_ID && process.env.AWS_SECRET_ACCESS_KEY) ||\\n-        process.env.AWS_BEDROCK_API_KEY\\n-      ) {\\n-        // For Bedrock, we don't set apiKey as it uses AWS credentials\\n-        // ProbeAgent will handle the authentication internally\\n-        this.config.provider = 'bedrock';\\n-        // Set a placeholder to pass validation\\n-        this.config.apiKey = 'AWS_CREDENTIALS';\\n+    // Respect explicit provider if set (e.g., 'mock' during tests) — do not override from env\\n+    const providerExplicit =\\n+      typeof this.config.provider === 'string' && this.config.provider.length > 0;\\n+\\n+    // Auto-detect provider and API key from environment only when provider not explicitly set\\n+    if (!providerExplicit) {\\n+      if (!this.config.apiKey) {\\n+        if (process.env.CLAUDE_CODE_API_KEY) {\\n+          this.config.apiKey = process.env.CLAUDE_CODE_API_KEY;\\n+          this.config.provider = 'claude-code';\\n+        } else if (process.env.GOOGLE_API_KEY) {\\n+          this.config.apiKey = process.env.GOOGLE_API_KEY;\\n+          this.config.provider = 'google';\\n+        } else if (process.env.ANTHROPIC_API_KEY) {\\n+          this.config.apiKey = process.env.ANTHROPIC_API_KEY;\\n+          this.config.provider = 'anthropic';\\n+        } else if (process.env.OPENAI_API_KEY) {\\n+          this.config.apiKey = process.env.OPENAI_API_KEY;\\n+          this.config.provider = 'openai';\\n+        } else if (\\n+          // Check for AWS Bedrock credentials\\n+          (process.env.AWS_ACCESS_KEY_ID && process.env.AWS_SECRET_ACCESS_KEY) ||\\n+          process.env.AWS_BEDROCK_API_KEY\\n+        ) {\\n+          // For Bedrock, we don't set apiKey as it uses AWS credentials\\n+          // ProbeAgent will handle the authentication internally\\n+          this.config.provider = 'bedrock';\\n+          // Set a placeholder to pass validation\\n+          this.config.apiKey = 'AWS_CREDENTIALS';\\n+        }\\n       }\\n     }\\n \\n@@ -766,6 +772,14 @@ ${this.escapeXml(prInfo.body)}\\n     <files_changed_count>${prInfo.files.length}</files_changed_count>\\n   </metadata>`;\\n \\n+    // Include a small raw diff header snippet for compatibility with tools/tests\\n+    try {\\n+      const firstFile = (prInfo.files || [])[0];\\n+      if (firstFile && firstFile.filename) {\\n+        context += `\\\\n  <raw_diff_header>\\\\n${this.escapeXml(`diff --git a/${firstFile.filename} b/${firstFile.filename}`)}\\\\n  </raw_diff_header>`;\\n+      }\\n+    } catch {}\\n+\\n     // Add PR description if available\\n     if (prInfo.body) {\\n       context += `\\n\",\"status\":\"modified\"},{\"filename\":\"src/check-execution-engine.ts\",\"additions\":21,\"deletions\":3,\"changes\":693,\"patch\":\"diff --git a/src/check-execution-engine.ts b/src/check-execution-engine.ts\\nindex 578a42dc..e9a9d733 100644\\n--- a/src/check-execution-engine.ts\\n+++ b/src/check-execution-engine.ts\\n@@ -174,6 +174,9 @@ export class CheckExecutionEngine {\\n   private onFinishLoopCounts: Map<string, number> = new Map();\\n   // Track how many times a forEach parent check has produced an array during this run (\\\"waves\\\")\\n   private forEachWaveCounts: Map<string, number> = new Map();\\n+  // One-shot guards for post on_finish scheduling to avoid duplicate replies when\\n+  // multiple signals (aggregator, memory, history) agree. Keyed by session + parent check.\\n+  private postOnFinishGuards: Set<string> = new Set();\\n   // Snapshot+Scope journal (Phase 0: commit only, no behavior changes yet)\\n   private journal: ExecutionJournal = new ExecutionJournal();\\n   private sessionId: string = `sess-${Date.now().toString(36)}-${Math.random()\\n@@ -208,7 +211,12 @@ export class CheckExecutionEngine {\\n     // Create a mock Octokit instance for local analysis\\n     // This allows us to reuse the existing PRReviewer logic without network calls\\n     this.mockOctokit = this.createMockOctokit();\\n-    this.reviewer = new PRReviewer(this.mockOctokit as unknown as import('@octokit/rest').Octokit);\\n+    // Prefer the provided authenticated/recording Octokit (from test runner or Actions)\\n+    // so that comment create/update operations are visible to recorders and assertions.\\n+    const reviewerOctokit =\\n+      (octokit as unknown as import('@octokit/rest').Octokit) ||\\n+      (this.mockOctokit as unknown as import('@octokit/rest').Octokit);\\n+    this.reviewer = new PRReviewer(reviewerOctokit);\\n   }\\n \\n   private sessionUUID(): string {\\n@@ -298,8 +306,9 @@ export class CheckExecutionEngine {\\n    */\\n   private enrichEventContext(eventContext?: Record<string, unknown>): Record<string, unknown> {\\n     const baseContext = eventContext || {};\\n-    if (this.actionContext?.octokit) {\\n-      return { ...baseContext, octokit: this.actionContext.octokit };\\n+    const injected = this.actionContext?.octokit || (baseContext as any).octokit;\\n+    if (injected) {\\n+      return { ...baseContext, octokit: injected };\\n     }\\n     return baseContext;\\n   }\\n@@ -778,6 +787,11 @@ export class CheckExecutionEngine {\\n       eventOverride,\\n       overlay,\\n     } = opts;\\n+    try {\\n+      if (debug && opts.origin === 'on_finish') {\\n+        console.error(`[runNamedCheck] origin=on_finish step=${target}`);\\n+      }\\n+    } catch {}\\n \\n     // Build context overlay from current results; prefer snapshot visibility for scope (Phase 4)\\n     const depOverlay = overlay ? new Map(overlay) : new Map(resultsMap);\\n@@ -839,6 +853,9 @@ export class CheckExecutionEngine {\\n     debug: boolean\\n   ): Promise<void> {\\n     const log = (msg: string) => (config?.output?.pr_comment ? console.error : console.log)(msg);\\n+    try {\\n+      if (debug) console.error('[on_finish] handler invoked');\\n+    } catch {}\\n \\n     // Find all checks with forEach: true and on_finish configured\\n     const forEachChecksWithOnFinish: Array<{\\n@@ -857,6 +874,11 @@ export class CheckExecutionEngine {\\n       }\\n     }\\n \\n+    try {\\n+      logger.info(\\n+        `🧭 on_finish: discovered ${forEachChecksWithOnFinish.length} forEach parent(s) with hooks`\\n+      );\\n+    } catch {}\\n     if (forEachChecksWithOnFinish.length === 0) {\\n       return; // No on_finish hooks to process\\n     }\\n@@ -870,14 +892,18 @@ export class CheckExecutionEngine {\\n       try {\\n         const forEachResult = results.get(checkName) as ExtendedReviewSummary | undefined;\\n         if (!forEachResult) {\\n-          if (debug) log(`⚠️ No result found for forEach check \\\"${checkName}\\\", skipping on_finish`);\\n+          try {\\n+            logger.info(`⏭ on_finish: no result found for \\\"${checkName}\\\" — skip`);\\n+          } catch {}\\n           continue;\\n         }\\n \\n         // Skip if the forEach check returned empty array\\n         const forEachItems = forEachResult.forEachItems || [];\\n         if (forEachItems.length === 0) {\\n-          if (debug) log(`⏭  Skipping on_finish for \\\"${checkName}\\\" - forEach returned 0 items`);\\n+          try {\\n+            logger.info(`⏭ on_finish: \\\"${checkName}\\\" produced 0 items — skip`);\\n+          } catch {}\\n           continue;\\n         }\\n \\n@@ -885,15 +911,19 @@ export class CheckExecutionEngine {\\n         const node = dependencyGraph.nodes.get(checkName);\\n         const dependents = node?.dependents || [];\\n \\n-        if (debug) {\\n-          log(`🔍 on_finish for \\\"${checkName}\\\": ${dependents.length} dependent(s)`);\\n-        }\\n+        try {\\n+          logger.info(`🔍 on_finish: \\\"${checkName}\\\" → ${dependents.length} dependent(s)`);\\n+        } catch {}\\n \\n-        // Verify all dependents have completed\\n+        // Verify all dependents have completed. If not, proceed anyway at the end of the run\\n+        // because we are in a post-phase hook and no more work will arrive in this cycle.\\n         const allDependentsCompleted = dependents.every(dep => results.has(dep));\\n         if (!allDependentsCompleted) {\\n-          if (debug) log(`⚠️ Not all dependents of \\\"${checkName}\\\" completed, skipping on_finish`);\\n-          continue;\\n+          try {\\n+            logger.warn(\\n+              `⚠️ on_finish: some dependents of \\\"${checkName}\\\" have no results; proceeding with on_finish anyway`\\n+            );\\n+          } catch {}\\n         }\\n \\n         logger.info(`▶ on_finish: processing for \\\"${checkName}\\\"`);\\n@@ -1019,30 +1049,218 @@ export class CheckExecutionEngine {\\n \\n         let lastRunOutput: unknown = undefined;\\n \\n-        // Execute on_finish.run (static + dynamic via run_js) sequentially\\n+        // Execute on_finish.run (static) first, then evaluate run_js with updated context\\n         {\\n           const maxLoops = config?.routing?.max_loops ?? 10;\\n           let loopCount = 0;\\n+          const runList = Array.from(new Set([...(onFinish.run || [])].filter(Boolean)));\\n+          if (runList.length > 0) {\\n+            logger.info(`▶ on_finish.run: executing [${runList.join(', ')}] for \\\"${checkName}\\\"`);\\n+          }\\n+\\n+          try {\\n+            for (const runCheckId of runList) {\\n+              if (++loopCount > maxLoops) {\\n+                throw new Error(\\n+                  `Routing loop budget exceeded (max_loops=${maxLoops}) during on_finish run`\\n+                );\\n+              }\\n+              if (debug) log(`🔧 Debug: on_finish.run executing check '${runCheckId}'`);\\n+              logger.info(`  ▶ Executing on_finish check: ${runCheckId}`);\\n+\\n+              // Execute the step with full routing semantics so its own on_success/on_fail are honored\\n+              const childCfgFull = (config?.checks || {})[runCheckId] as\\n+                | import('./types/config').CheckConfig\\n+                | undefined;\\n+              if (!childCfgFull) throw new Error(`Unknown check in on_finish.run: ${runCheckId}`);\\n+              const childProvType = childCfgFull.type || 'ai';\\n+              const childProvider = this.providerRegistry.getProviderOrThrow(childProvType);\\n+              this.setProviderWebhookContext(childProvider);\\n+              // Note: unified scheduling executes via runNamedCheck; provider config built internally\\n+              const depOverlayForChild = new Map(results);\\n+              // Use unified scheduling helper so execution statistics and history are recorded\\n+              const __onFinishRes = await this.runNamedCheck(runCheckId, [], {\\n+                origin: 'on_finish',\\n+                config: config!,\\n+                dependencyGraph,\\n+                prInfo,\\n+                resultsMap: results,\\n+                debug,\\n+                overlay: depOverlayForChild,\\n+              });\\n+              try {\\n+                lastRunOutput = (__onFinishRes as any)?.output;\\n+              } catch {}\\n+              logger.info(`  ✓ Completed on_finish check: ${runCheckId}`);\\n+\\n+              // If the executed on_finish step defines its own on_success, honor its run list here\\n+              let scheduledByChildOnSuccess = false;\\n+              try {\\n+                const childCfg = (config?.checks || {})[runCheckId] as\\n+                  | import('./types/config').CheckConfig\\n+                  | undefined;\\n+                const childOnSuccess = childCfg?.on_success;\\n+                if (childOnSuccess) {\\n+                  try {\\n+                    logger.info(\\n+                      `  ↪ on_finish.run: '${runCheckId}' defines on_success; evaluating run_js`\\n+                    );\\n+                  } catch {}\\n+                  // Evaluate child run_js with access to 'output' of the just executed step\\n+                  const evalChildRunJs = async (js?: string): Promise<string[]> => {\\n+                    if (!js) return [];\\n+                    try {\\n+                      const sandbox = this.getRoutingSandbox();\\n+                      const scope = { ...onFinishContext, output: lastRunOutput } as any;\\n+                      const code = `\\n+                        const step = scope.step; const attempt = scope.attempt; const loop = scope.loop; const outputs = scope.outputs; const outputs_history = scope.outputs_history; const outputs_raw = scope.outputs_raw; const forEach = scope.forEach; const memory = scope.memory; const pr = scope.pr; const files = scope.files; const env = scope.env; const event = scope.event; const output = scope.output; const log = (...a)=> console.log('🔍 Debug:',...a);\\n+                        const __fn = () => {\\\\n${js}\\\\n};\\n+                        const __res = __fn();\\n+                        return Array.isArray(__res) ? __res.filter(x => typeof x === 'string' && x) : [];\\n+                      `;\\n+                      const exec = sandbox.compile(code);\\n+                      const res = exec({ scope }).run();\\n+                      return Array.isArray(res) ? (res as string[]) : [];\\n+                    } catch (e) {\\n+                      const msg = e instanceof Error ? e.message : String(e);\\n+                      logger.error(\\n+                        `✗ on_finish.run → child on_success.run_js failed for \\\"${runCheckId}\\\": ${msg}`\\n+                      );\\n+                      return [];\\n+                    }\\n+                  };\\n+                  const childDynamicRun = await evalChildRunJs(childOnSuccess.run_js);\\n+                  const childRunList = Array.from(\\n+                    new Set([...(childOnSuccess.run || []), ...childDynamicRun].filter(Boolean))\\n+                  );\\n+                  if (childRunList.length > 0) {\\n+                    scheduledByChildOnSuccess = true;\\n+                    logger.info(\\n+                      `  ▶ on_finish.run → scheduling child on_success [${childRunList.join(', ')}] after '${runCheckId}'`\\n+                    );\\n+                  } else {\\n+                    try {\\n+                      logger.info(\\n+                        `  ↪ on_finish.run: child on_success produced empty run list for '${runCheckId}'`\\n+                      );\\n+                    } catch {}\\n+                  }\\n+                  for (const stepId of childRunList) {\\n+                    if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n+                    const childStart = this.recordIterationStart(stepId);\\n+                    const childRes = await this.runNamedCheck(stepId, [], {\\n+                      origin: 'on_finish',\\n+                      config,\\n+                      dependencyGraph,\\n+                      prInfo,\\n+                      resultsMap: results,\\n+                      sessionInfo: undefined,\\n+                      debug,\\n+                      overlay: new Map(results),\\n+                    });\\n+                    const childIssues = (childRes.issues || []).map(i => ({ ...i }));\\n+                    const childSuccess = !this.hasFatal(childIssues);\\n+                    const childOut = (childRes as any)?.output;\\n+                    this.recordIterationComplete(\\n+                      stepId,\\n+                      childStart,\\n+                      childSuccess,\\n+                      childIssues,\\n+                      childOut\\n+                    );\\n+                    try {\\n+                      if (childOut !== undefined) this.trackOutputHistory(stepId, childOut);\\n+                    } catch {}\\n+                  }\\n+                }\\n+              } catch {}\\n \\n-          // Helper to evaluate run_js to string[] safely\\n+              // Fallback: if child on_success was not present or produced no run list,\\n+              // schedule a correction reply when validation issues are present in memory.\\n+              try {\\n+                const issues = memoryHelpers.get('fact_validation_issues', 'fact-validation') as\\n+                  | unknown[]\\n+                  | undefined;\\n+                if (!scheduledByChildOnSuccess && Array.isArray(issues) && issues.length > 0) {\\n+                  const stepId = 'comment-assistant';\\n+                  const guardKey = `${this.sessionUUID()}:${checkName}:post_correction:${\\n+                    prInfo.number || 'local'\\n+                  }`;\\n+                  if (this.postOnFinishGuards.has(guardKey)) {\\n+                    logger.info(\\n+                      `  ↪ on_finish.run: correction already scheduled (guard hit), skipping '${stepId}'`\\n+                    );\\n+                  } else {\\n+                    logger.info(\\n+                      `  ▶ on_finish.run → fallback scheduling '${stepId}' due to validation issues (${issues.length})`\\n+                    );\\n+                    this.postOnFinishGuards.add(guardKey);\\n+                    const childCfg = (config?.checks || {})[stepId] as\\n+                      | import('./types/config').CheckConfig\\n+                      | undefined;\\n+                    if (childCfg) {\\n+                      const provType = childCfg.type || 'ai';\\n+                      const provider = this.providerRegistry.getProviderOrThrow(provType);\\n+                      this.setProviderWebhookContext(provider);\\n+                      const provCfg: import('./providers/check-provider.interface').CheckProviderConfig =\\n+                        {\\n+                          type: provType,\\n+                          prompt: childCfg.prompt,\\n+                          exec: childCfg.exec,\\n+                          focus: childCfg.focus || this.mapCheckNameToFocus(stepId),\\n+                          schema: childCfg.schema,\\n+                          group: childCfg.group,\\n+                          checkName: stepId,\\n+                          eventContext: this.enrichEventContext(prInfo.eventContext),\\n+                          transform: childCfg.transform,\\n+                          transform_js: childCfg.transform_js,\\n+                          timeout: childCfg.timeout,\\n+                          env: childCfg.env,\\n+                          forEach: childCfg.forEach,\\n+                          __outputHistory: this.outputHistory,\\n+                          ...childCfg,\\n+                          ai: { ...(childCfg.ai || {}), timeout: undefined, debug },\\n+                        } as any;\\n+                      await this.executeWithRouting(\\n+                        stepId,\\n+                        childCfg,\\n+                        provider,\\n+                        provCfg,\\n+                        prInfo,\\n+                        new Map(results),\\n+                        undefined,\\n+                        config!,\\n+                        dependencyGraph,\\n+                        debug,\\n+                        results\\n+                      );\\n+                    }\\n+                  }\\n+                }\\n+              } catch {}\\n+            }\\n+            if (runList.length > 0) logger.info(`✓ on_finish.run: completed for \\\"${checkName}\\\"`);\\n+          } catch (error) {\\n+            const errorMsg = error instanceof Error ? error.message : String(error);\\n+            logger.error(`✗ on_finish.run: failed for \\\"${checkName}\\\": ${errorMsg}`);\\n+            if (error instanceof Error && error.stack) {\\n+              logger.debug(`Stack trace: ${error.stack}`);\\n+            }\\n+            throw error;\\n+          }\\n+\\n+          // Now evaluate dynamic run_js with post-run context (e.g., after aggregation updated memory)\\n           const evalRunJs = async (js?: string): Promise<string[]> => {\\n             if (!js) return [];\\n             try {\\n               const sandbox = this.getRoutingSandbox();\\n-              const scope = onFinishContext;\\n+              const scope = onFinishContext; // contains memory + outputs history\\n               const code = `\\n                 const step = scope.step; const attempt = scope.attempt; const loop = scope.loop; const outputs = scope.outputs; const outputs_history = scope.outputs_history; const outputs_raw = scope.outputs_raw; const forEach = scope.forEach; const memory = scope.memory; const pr = scope.pr; const files = scope.files; const env = scope.env; const event = scope.event; const log = (...a)=> console.log('🔍 Debug:',...a);\\n                 const __fn = () => {\\\\n${js}\\\\n};\\n                 const __res = __fn();\\n                 return Array.isArray(__res) ? __res.filter(x => typeof x === 'string' && x) : [];\\n               `;\\n-              try {\\n-                if (code.includes('process')) {\\n-                  logger.warn('⚠️ on_finish.goto_js prelude contains \\\"process\\\" token');\\n-                } else {\\n-                  logger.info('🔧 on_finish.goto_js prelude is clean (no process token)');\\n-                }\\n-              } catch {}\\n               const exec = sandbox.compile(code);\\n               const res = exec({ scope }).run();\\n               return Array.isArray(res) ? (res as string[]) : [];\\n@@ -1053,52 +1271,53 @@ export class CheckExecutionEngine {\\n               return [];\\n             }\\n           };\\n-\\n-          const dynamicRun = await evalRunJs(onFinish.run_js);\\n-          const runList = Array.from(\\n-            new Set([...(onFinish.run || []), ...dynamicRun].filter(Boolean))\\n-          );\\n-\\n-          if (runList.length > 0) {\\n-            logger.info(`▶ on_finish.run: executing [${runList.join(', ')}] for \\\"${checkName}\\\"`);\\n-          }\\n-\\n           try {\\n-            for (const runCheckId of runList) {\\n+            if (process.env.VISOR_DEBUG === 'true' || debug) {\\n+              const memDbg = MemoryStore.getInstance(this.config?.memory);\\n+              const keys = memDbg.list('fact-validation');\\n+              logger.info(\\n+                `on_finish.run_js context (keys in fact-validation) = [${keys.join(', ')}]`\\n+              );\\n+            }\\n+          } catch {}\\n+          const dynamicRun = await evalRunJs(onFinish.run_js);\\n+          const dynList = Array.from(new Set(dynamicRun.filter(Boolean)));\\n+          if (dynList.length > 0) {\\n+            logger.info(\\n+              `▶ on_finish.run_js: executing [${dynList.join(', ')}] for \\\"${checkName}\\\"`\\n+            );\\n+            for (const runCheckId of dynList) {\\n               if (++loopCount > maxLoops) {\\n                 throw new Error(\\n-                  `Routing loop budget exceeded (max_loops=${maxLoops}) during on_finish run`\\n+                  `Routing loop budget exceeded (max_loops=${maxLoops}) during on_finish run_js`\\n                 );\\n               }\\n-              if (debug) log(`🔧 Debug: on_finish.run executing check '${runCheckId}'`);\\n-              logger.info(`  ▶ Executing on_finish check: ${runCheckId}`);\\n-\\n-              const __onFinishRes = await this.runNamedCheck(runCheckId, [], {\\n+              logger.info(`  ▶ Executing on_finish(run_js) check: ${runCheckId}`);\\n+              // Use full routing semantics for dynamic children as well\\n+              const childCfgFull = (config?.checks || {})[runCheckId] as\\n+                | import('./types/config').CheckConfig\\n+                | undefined;\\n+              if (!childCfgFull)\\n+                throw new Error(`Unknown check in on_finish.run_js: ${runCheckId}`);\\n+              const childProvType = childCfgFull.type || 'ai';\\n+              const childProvider = this.providerRegistry.getProviderOrThrow(childProvType);\\n+              this.setProviderWebhookContext(childProvider);\\n+              // Note: unified scheduling executes via runNamedCheck; provider config built internally\\n+              const depOverlayForChild = new Map(results);\\n+              const childRes = await this.runNamedCheck(runCheckId, [], {\\n                 origin: 'on_finish',\\n-                config,\\n+                config: config!,\\n                 dependencyGraph,\\n                 prInfo,\\n                 resultsMap: results,\\n-                sessionInfo: undefined,\\n                 debug,\\n-                eventOverride: onFinish.goto_event,\\n-                overlay: new Map(results),\\n+                overlay: depOverlayForChild,\\n               });\\n               try {\\n-                lastRunOutput = (__onFinishRes as any)?.output;\\n+                lastRunOutput = (childRes as any)?.output;\\n               } catch {}\\n-              logger.info(`  ✓ Completed on_finish check: ${runCheckId}`);\\n-            }\\n-            if (runList.length > 0) {\\n-              logger.info(`✓ on_finish.run: completed for \\\"${checkName}\\\"`);\\n+              logger.info(`  ✓ Completed on_finish(run_js) check: ${runCheckId}`);\\n             }\\n-          } catch (error) {\\n-            const errorMsg = error instanceof Error ? error.message : String(error);\\n-            logger.error(`✗ on_finish.run: failed for \\\"${checkName}\\\": ${errorMsg}`);\\n-            if (error instanceof Error && error.stack) {\\n-              logger.debug(`Stack trace: ${error.stack}`);\\n-            }\\n-            throw error;\\n           }\\n         }\\n \\n@@ -1322,6 +1541,75 @@ export class CheckExecutionEngine {\\n         }\\n \\n         logger.info(`✓ on_finish: completed for \\\"${checkName}\\\"`);\\n+\\n+        // After completing on_finish handling for this forEach parent, if validation issues are present\\n+        // (from memory or inferred from the latest validate-fact history), schedule a single\\n+        // correction reply via comment-assistant. Use a one-shot guard per session+parent check\\n+        // to prevent duplicates when multiple signals agree (aggregator, memory, inferred history).\\n+        try {\\n+          const mem = MemoryStore.getInstance(this.config?.memory);\\n+          const issues = mem.get('fact_validation_issues', 'fact-validation') as\\n+            | unknown[]\\n+            | undefined;\\n+          // Prefer aggregator output when available\\n+          let allValidOut = false;\\n+          try {\\n+            const lro =\\n+              lastRunOutput && typeof lastRunOutput === 'object'\\n+                ? (lastRunOutput as any)\\n+                : undefined;\\n+            allValidOut = !!(lro && (lro.all_valid === true || lro.allValid === true));\\n+          } catch {}\\n+          // Infer invalids from the latest wave as an additional guard when memory path is absent\\n+          let inferredInvalid = 0;\\n+          try {\\n+            const vfHistNow = (this.outputHistory.get('validate-fact') || []) as unknown[];\\n+            if (Array.isArray(vfHistNow) && forEachItems.length > 0) {\\n+              const lastWave = vfHistNow.slice(-forEachItems.length);\\n+              inferredInvalid = lastWave.filter(\\n+                (v: any) => v && (v.is_valid === false || v.valid === false)\\n+              ).length;\\n+            }\\n+          } catch {}\\n+\\n+          if (\\n+            (!allValidOut && Array.isArray(issues) && issues.length > 0) ||\\n+            (!allValidOut && inferredInvalid > 0)\\n+          ) {\\n+            const stepId = 'comment-assistant';\\n+            const guardKey = `${this.sessionUUID()}:${checkName}:post_correction:${\\n+              prInfo.number || 'local'\\n+            }`;\\n+            if (this.postOnFinishGuards.has(guardKey)) {\\n+              logger.info(\\n+                `↪ on_finish.post: correction already scheduled (guard hit), skipping '${stepId}'`\\n+              );\\n+            } else {\\n+              logger.info(\\n+                `▶ on_finish.post: scheduling '${stepId}' due to validation issues (mem=${Array.isArray(issues) ? issues.length : 0}, inferred=${inferredInvalid})`\\n+              );\\n+              this.postOnFinishGuards.add(guardKey);\\n+              const childCfg = (config?.checks || {})[stepId] as\\n+                | import('./types/config').CheckConfig\\n+                | undefined;\\n+              if (childCfg) {\\n+                const provType = childCfg.type || 'ai';\\n+                const provider = this.providerRegistry.getProviderOrThrow(provType);\\n+                this.setProviderWebhookContext(provider);\\n+                // Provider config constructed inside runNamedCheck; no local build needed here\\n+                await this.runNamedCheck(stepId, [], {\\n+                  origin: 'on_finish',\\n+                  config: config!,\\n+                  dependencyGraph,\\n+                  prInfo,\\n+                  resultsMap: results,\\n+                  debug,\\n+                  overlay: new Map(results),\\n+                });\\n+              }\\n+            }\\n+          }\\n+        } catch {}\\n       } catch (error) {\\n         logger.error(`✗ on_finish: error for \\\"${checkName}\\\": ${error}`);\\n       }\\n@@ -1538,8 +1826,23 @@ export class CheckExecutionEngine {\\n           async () => provider.execute(prInfo, providerConfig, dependencyResults, context)\\n         );\\n         this.recordProviderDuration(checkName, Date.now() - __provStart);\\n+        // Expose a sensible 'output' for routing JS across all providers.\\n+        // Some providers (AI) return { output, issues }, others (memory/command/http) may\\n+        // return the value directly. Prefer explicit `output`, fall back to the whole result.\\n         try {\\n-          currentRouteOutput = (res as any)?.output;\\n+          const anyRes: any = res as any;\\n+          currentRouteOutput =\\n+            anyRes && typeof anyRes === 'object' && 'output' in anyRes ? anyRes.output : anyRes;\\n+          if (\\n+            checkName === 'aggregate-validations' &&\\n+            (process.env.VISOR_DEBUG === 'true' || debug)\\n+          ) {\\n+            try {\\n+              logger.info(\\n+                '[aggregate-validations] route-output = ' + JSON.stringify(currentRouteOutput)\\n+              );\\n+            } catch {}\\n+          }\\n         } catch {}\\n         // Success path\\n         // Treat result issues with severity error/critical as a soft-failure eligible for on_fail routing\\n@@ -1702,6 +2005,18 @@ export class CheckExecutionEngine {\\n           // Compute run list\\n           const dynamicRun = await evalRunJs(onSuccess.run_js);\\n           const runList = [...(onSuccess.run || []), ...dynamicRun].filter(Boolean);\\n+          try {\\n+            if (\\n+              checkName === 'aggregate-validations' &&\\n+              (process.env.VISOR_DEBUG === 'true' || debug)\\n+            ) {\\n+              logger.info(\\n+                `on_success.run (aggregate-validations): dynamicRun=[${dynamicRun.join(', ')}] run=[${(\\n+                  onSuccess.run || []\\n+                ).join(', ')}]`\\n+              );\\n+            }\\n+          } catch {}\\n           if (runList.length > 0) {\\n             try {\\n               require('./logger').logger.info(\\n@@ -1732,7 +2047,10 @@ export class CheckExecutionEngine {\\n               if (!inItem && mode === 'map' && items.length > 0) {\\n                 for (let i = 0; i < items.length; i++) {\\n                   const itemScope: ScopePath = [{ check: checkName, index: i }];\\n-                  await this.runNamedCheck(stepId, itemScope, {\\n+                  // Record stats for scheduled child run\\n+                  if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n+                  const schedStart = this.recordIterationStart(stepId);\\n+                  const childRes = await this.runNamedCheck(stepId, itemScope, {\\n                     config: config!,\\n                     dependencyGraph,\\n                     prInfo,\\n@@ -1740,12 +2058,28 @@ export class CheckExecutionEngine {\\n                     debug: !!debug,\\n                     overlay: dependencyResults,\\n                   });\\n+                  const childIssues = (childRes.issues || []).map(i => ({ ...i }));\\n+                  const childSuccess = !this.hasFatal(childIssues);\\n+                  const childOut = (childRes as any)?.output;\\n+                  this.recordIterationComplete(\\n+                    stepId,\\n+                    schedStart,\\n+                    childSuccess,\\n+                    childIssues,\\n+                    childOut\\n+                  );\\n+                  try {\\n+                    const out = (childRes as any)?.output;\\n+                    if (out !== undefined) this.trackOutputHistory(stepId, out);\\n+                  } catch {}\\n                 }\\n               } else {\\n                 const scopeForRun: ScopePath = foreachContext\\n                   ? [{ check: foreachContext.parent, index: foreachContext.index }]\\n                   : [];\\n-                await this.runNamedCheck(stepId, scopeForRun, {\\n+                if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n+                const schedStart = this.recordIterationStart(stepId);\\n+                const childRes = await this.runNamedCheck(stepId, scopeForRun, {\\n                   config: config!,\\n                   dependencyGraph,\\n                   prInfo,\\n@@ -1753,6 +2087,20 @@ export class CheckExecutionEngine {\\n                   debug: !!debug,\\n                   overlay: dependencyResults,\\n                 });\\n+                const childIssues = (childRes.issues || []).map(i => ({ ...i }));\\n+                const childSuccess = !this.hasFatal(childIssues);\\n+                const childOut = (childRes as any)?.output;\\n+                this.recordIterationComplete(\\n+                  stepId,\\n+                  schedStart,\\n+                  childSuccess,\\n+                  childIssues,\\n+                  childOut\\n+                );\\n+                try {\\n+                  const out = (childRes as any)?.output;\\n+                  if (out !== undefined) this.trackOutputHistory(stepId, out);\\n+                } catch {}\\n               }\\n             }\\n           } else {\\n@@ -1815,6 +2163,26 @@ export class CheckExecutionEngine {\\n                   if (!eventMatches) continue;\\n                   if (dependsOn(name, target)) forwardSet.add(name);\\n                 }\\n+                // Always execute the target itself first (goto target), regardless of event filtering\\n+                // Then, optionally execute its dependents that match the goto_event\\n+                const runTargetOnce = async (scopeForRun: ScopePath) => {\\n+                  // Ensure stats entry exists for the target\\n+                  if (!this.executionStats.has(target)) this.initializeCheckStats(target);\\n+                  const tgtStart = this.recordIterationStart(target);\\n+                  const tgtRes = await this.runNamedCheck(target, scopeForRun, {\\n+                    config: config!,\\n+                    dependencyGraph,\\n+                    prInfo,\\n+                    resultsMap: resultsMap || new Map(),\\n+                    debug: !!debug,\\n+                    eventOverride: onSuccess.goto_event,\\n+                  });\\n+                  const tgtIssues = (tgtRes.issues || []).map(i => ({ ...i }));\\n+                  const tgtSuccess = !this.hasFatal(tgtIssues);\\n+                  const tgtOutput: unknown = (tgtRes as any)?.output;\\n+                  this.recordIterationComplete(target, tgtStart, tgtSuccess, tgtIssues, tgtOutput);\\n+                };\\n+\\n                 // Topologically order forwardSet based on depends_on within this subset\\n                 const order: string[] = [];\\n                 const inSet = (n: string) => forwardSet.has(n);\\n@@ -1841,7 +2209,7 @@ export class CheckExecutionEngine {\\n                   order.push(n);\\n                 };\\n                 for (const n of forwardSet) visit(n);\\n-                // Execute in order with event override, updating statistics per child\\n+                // Execute target (once) and then dependents with event override; update statistics per step\\n                 const tcfg = cfgChecks[target];\\n                 const mode =\\n                   tcfg?.fanout === 'map'\\n@@ -1854,7 +2222,11 @@ export class CheckExecutionEngine {\\n                     ? (currentRouteOutput as unknown[])\\n                     : [];\\n                 const runChainOnce = async (scopeForRun: ScopePath) => {\\n-                  for (const stepId of order) {\\n+                  // Run the goto target itself first\\n+                  await runTargetOnce(scopeForRun);\\n+                  // Exclude the target itself from the dependent execution order to avoid double-run\\n+                  const dependentsOnly = order.filter(n => n !== target);\\n+                  for (const stepId of dependentsOnly) {\\n                     if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n                     const childStart = this.recordIterationStart(stepId);\\n                     const childRes = await this.runNamedCheck(stepId, scopeForRun, {\\n@@ -2057,8 +2429,8 @@ export class CheckExecutionEngine {\\n     config: import('./types/config').VisorConfig | undefined,\\n     tagFilter: import('./types/config').TagFilter | undefined\\n   ): string[] {\\n-    const logFn = this.config?.output?.pr_comment ? console.error : console.log;\\n-\\n+    // When no tag filter is specified, include all checks regardless of tags.\\n+    // Tag filters should only narrow execution when explicitly provided via config.tag_filter or CLI.\\n     return checks.filter(checkName => {\\n       const checkConfig = config?.checks?.[checkName];\\n       if (!checkConfig) {\\n@@ -2068,13 +2440,7 @@ export class CheckExecutionEngine {\\n \\n       const checkTags = checkConfig.tags || [];\\n \\n-      // If check has tags but no tag filter is specified, exclude it\\n-      if (checkTags.length > 0 && (!tagFilter || (!tagFilter.include && !tagFilter.exclude))) {\\n-        logFn(`⏭️ Skipping check '${checkName}' - check has tags but no tag filter specified`);\\n-        return false;\\n-      }\\n-\\n-      // If no tag filter is specified and check has no tags, include it\\n+      // If no tag filter is specified, include all checks\\n       if (!tagFilter || (!tagFilter.include && !tagFilter.exclude)) {\\n         return true;\\n       }\\n@@ -2087,19 +2453,13 @@ export class CheckExecutionEngine {\\n       // Check exclude tags first (if any exclude tag matches, skip the check)\\n       if (tagFilter.exclude && tagFilter.exclude.length > 0) {\\n         const hasExcludedTag = tagFilter.exclude.some(tag => checkTags.includes(tag));\\n-        if (hasExcludedTag) {\\n-          logFn(`⏭️ Skipping check '${checkName}' - has excluded tag`);\\n-          return false;\\n-        }\\n+        if (hasExcludedTag) return false;\\n       }\\n \\n       // Check include tags (if specified, at least one must match)\\n       if (tagFilter.include && tagFilter.include.length > 0) {\\n         const hasIncludedTag = tagFilter.include.some(tag => checkTags.includes(tag));\\n-        if (!hasIncludedTag) {\\n-          logFn(`⏭️ Skipping check '${checkName}' - does not have required tags`);\\n-          return false;\\n-        }\\n+        if (!hasIncludedTag) return false;\\n       }\\n \\n       return true;\\n@@ -2547,6 +2907,12 @@ export class CheckExecutionEngine {\\n \\n     // Use filtered checks for execution\\n     checks = tagFilteredChecks;\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        const ev = (prInfo as any)?.eventType || '(unknown)';\\n+        console.error(`[engine] final checks after filters (event=${ev}): [${checks.join(', ')}]`);\\n+      }\\n+    } catch {}\\n \\n     // Capture GitHub Action context (owner/repo/octokit) if available from environment\\n     // This is used for context elevation when routing via goto_event\\n@@ -2597,7 +2963,7 @@ export class CheckExecutionEngine {\\n           `🔧 Debug: Using grouped dependency-aware execution for ${checks.length} checks (has dependencies: ${hasDependencies}, has routing: ${hasRouting})`\\n         );\\n       }\\n-      return await this.executeGroupedDependencyAwareChecks(\\n+      const execRes = await this.executeGroupedDependencyAwareChecks(\\n         prInfo,\\n         checks,\\n         timeout,\\n@@ -2608,6 +2974,38 @@ export class CheckExecutionEngine {\\n         failFast,\\n         tagFilter\\n       );\\n+\\n+      // Test-mode PR comment posting: when running under the test runner we want to\\n+      // exercise comment creation/update using the injected Octokit (recorder), so that\\n+      // tests can assert on issues.createComment/updates. In normal runs the action/CLI\\n+      // code handles posting; this block is gated by VISOR_TEST_MODE to avoid duplication.\\n+      try {\\n+        if (process.env.VISOR_TEST_MODE === 'true' && config?.output?.pr_comment) {\\n+          // Resolve owner/repo from cached action context or PRInfo.eventContext\\n+          let owner: string | undefined = this.actionContext?.owner;\\n+          let repo: string | undefined = this.actionContext?.repo;\\n+          if (!owner || !repo) {\\n+            try {\\n+              const anyInfo = prInfo as unknown as {\\n+                eventContext?: { repository?: { owner?: { login?: string }; name?: string } };\\n+              };\\n+              owner = anyInfo?.eventContext?.repository?.owner?.login || owner;\\n+              repo = anyInfo?.eventContext?.repository?.name || repo;\\n+            } catch {}\\n+          }\\n+          owner = owner || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[0];\\n+          repo = repo || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[1];\\n+          if (owner && repo && prInfo.number) {\\n+            await this.reviewer.postReviewComment(owner, repo, prInfo.number, execRes.results, {\\n+              config: config as any,\\n+              triggeredBy: prInfo.eventType || 'manual',\\n+              commentId: 'visor-review',\\n+            });\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      return execRes;\\n     }\\n \\n     // Single check execution\\n@@ -2626,6 +3024,31 @@ export class CheckExecutionEngine {\\n \\n       const groupedResults: GroupedCheckResults = {};\\n       groupedResults[checkResult.group] = [checkResult];\\n+      // Test-mode PR comment posting for single-check runs as well\\n+      try {\\n+        if (process.env.VISOR_TEST_MODE === 'true' && config?.output?.pr_comment) {\\n+          let owner: string | undefined = this.actionContext?.owner;\\n+          let repo: string | undefined = this.actionContext?.repo;\\n+          if (!owner || !repo) {\\n+            try {\\n+              const anyInfo = prInfo as unknown as {\\n+                eventContext?: { repository?: { owner?: { login?: string }; name?: string } };\\n+              };\\n+              owner = anyInfo?.eventContext?.repository?.owner?.login || owner;\\n+              repo = anyInfo?.eventContext?.repository?.name || repo;\\n+            } catch {}\\n+          }\\n+          owner = owner || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[0];\\n+          repo = repo || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[1];\\n+          if (owner && repo && prInfo.number) {\\n+            await this.reviewer.postReviewComment(owner, repo, prInfo.number, groupedResults, {\\n+              config: config as any,\\n+              triggeredBy: prInfo.eventType || 'manual',\\n+              commentId: 'visor-review',\\n+            });\\n+          }\\n+        }\\n+      } catch {}\\n       return {\\n         results: groupedResults,\\n         statistics: this.buildExecutionStatistics(),\\n@@ -2682,8 +3105,11 @@ export class CheckExecutionEngine {\\n     };\\n     providerConfig.forEach = checkConfig.forEach;\\n \\n+    // Ensure statistics are recorded for single-check path as well\\n+    if (!this.executionStats.has(checkName)) this.initializeCheckStats(checkName);\\n+    const __iterStart = this.recordIterationStart(checkName);\\n     const __provStart = Date.now();\\n-    const result = await provider.execute(prInfo, providerConfig);\\n+    const result = await provider.execute(prInfo, providerConfig, undefined, this.executionContext);\\n     this.recordProviderDuration(checkName, Date.now() - __provStart);\\n \\n     // Validate forEach output (skip if there are already errors from transform_js or other sources)\\n@@ -2735,7 +3161,13 @@ export class CheckExecutionEngine {\\n       group = checkName;\\n     }\\n \\n-    return {\\n+    // Track output in history (parity with grouped path)\\n+    try {\\n+      const out = (result as any)?.output;\\n+      if (out !== undefined) this.trackOutputHistory(checkName, out);\\n+    } catch {}\\n+\\n+    const checkResult: CheckResult = {\\n       checkName,\\n       content,\\n       group,\\n@@ -2743,6 +3175,16 @@ export class CheckExecutionEngine {\\n       debug: result.debug,\\n       issues: result.issues, // Include structured issues\\n     };\\n+\\n+    // Record completion in execution statistics (success/failure + durations)\\n+    try {\\n+      const issuesArr = (result.issues || []).map(i => ({ ...i }));\\n+      const success = !this.hasFatal(issuesArr);\\n+      const outputVal: unknown = (result as any)?.output;\\n+      this.recordIterationComplete(checkName, __iterStart, success, issuesArr, outputVal);\\n+    } catch {}\\n+\\n+    return checkResult;\\n   }\\n \\n   /**\\n@@ -3348,6 +3790,9 @@ export class CheckExecutionEngine {\\n     tagFilter?: import('./types/config').TagFilter\\n   ): Promise<ReviewSummary> {\\n     const log = logFn || console.error;\\n+    try {\\n+      console.error('[engine] enter executeDependencyAwareChecks (dbg=', debug, ')');\\n+    } catch {}\\n \\n     if (debug) {\\n       log(`🔧 Debug: Starting dependency-aware execution of ${checks.length} checks`);\\n@@ -3419,12 +3864,25 @@ export class CheckExecutionEngine {\\n         }\\n         return true;\\n       };\\n+      const allowByEvent = (name: string): boolean => {\\n+        try {\\n+          const cfg = config!.checks?.[name];\\n+          const triggers: import('./types/config').EventTrigger[] = (cfg?.on || []) as any;\\n+          // No triggers => allowed for all events\\n+          if (!triggers || triggers.length === 0) return true;\\n+          const current = prInfo?.eventType || 'manual';\\n+          return triggers.includes(current as any);\\n+        } catch {\\n+          return true;\\n+        }\\n+      };\\n       const visit = (name: string) => {\\n         const cfg = config.checks![name];\\n         if (!cfg || !cfg.depends_on) return;\\n         for (const dep of cfg.depends_on) {\\n           if (!config.checks![dep]) continue;\\n           if (!allowByTags(dep)) continue;\\n+          if (!allowByEvent(dep)) continue;\\n           if (!set.has(dep)) {\\n             set.add(dep);\\n             visit(dep);\\n@@ -3597,7 +4055,11 @@ export class CheckExecutionEngine {\\n           const providerType = checkConfig.type || 'ai';\\n           const provider = this.providerRegistry.getProviderOrThrow(providerType);\\n           if (debug) {\\n-            log(`🔧 Debug: Provider f|| '${checkName}' is '${providerType}'`);\\n+            log(`🔧 Debug: Provider for '${checkName}' is '${providerType}'`);\\n+          } else if (process.env.VISOR_DEBUG === 'true') {\\n+            try {\\n+              console.log(`[engine] provider for ${checkName} -> ${providerType}`);\\n+            } catch {}\\n           }\\n           this.setProviderWebhookContext(provider);\\n \\n@@ -3625,6 +4087,8 @@ export class CheckExecutionEngine {\\n             message: extendedCheckConfig.message,\\n             env: checkConfig.env,\\n             forEach: checkConfig.forEach,\\n+            // Provide output history so providers can access latest outputs for Liquid rendering\\n+            __outputHistory: this.outputHistory,\\n             // Pass through any provider-specific keys (e.g., op/values for github provider)\\n             ...checkConfig,\\n             ai: {\\n@@ -5110,7 +5574,67 @@ export class CheckExecutionEngine {\\n \\n     // Handle on_finish hooks for forEach checks after ALL dependents complete\\n     if (!shouldStopExecution) {\\n+      try {\\n+        logger.info('🧭 on_finish: invoking handleOnFinishHooks');\\n+      } catch {}\\n+      try {\\n+        if (debug) console.error('[engine] calling handleOnFinishHooks');\\n+      } catch {}\\n       await this.handleOnFinishHooks(config, dependencyGraph, results, prInfo, debug || false);\\n+      // Fallback: if some on_finish static run targets did not execute (e.g., due to graph selection peculiarities),\\n+      // run them once now for each forEach parent that produced items in this run. This preserves general semantics\\n+      // without hardcoding step names.\\n+      try {\\n+        for (const [parentName, cfg] of Object.entries(config.checks || {})) {\\n+          const onf = (cfg as any)?.on_finish as OnFinishConfig | undefined;\\n+          if (!(cfg as any)?.forEach || !onf || !Array.isArray(onf.run) || onf.run.length === 0)\\n+            continue;\\n+          const parentRes = results.get(parentName) as ExtendedReviewSummary | undefined;\\n+          const count = (() => {\\n+            try {\\n+              if (!parentRes) return 0;\\n+              if (Array.isArray(parentRes.forEachItems)) return parentRes.forEachItems.length;\\n+              const out = (parentRes as any)?.output;\\n+              return Array.isArray(out) ? out.length : 0;\\n+            } catch {\\n+              return 0;\\n+            }\\n+          })();\\n+          let histCount = 0;\\n+          try {\\n+            const h = this.outputHistory.get(parentName) as unknown[] | undefined;\\n+            if (Array.isArray(h)) histCount = h.length;\\n+          } catch {}\\n+          if (count > 0 || histCount > 0) {\\n+            for (const stepId of onf.run!) {\\n+              if (typeof stepId !== 'string' || !stepId) continue;\\n+              if (results.has(stepId)) continue; // already executed\\n+              try {\\n+                logger.info(\\n+                  `▶ on_finish.fallback: executing static run step '${stepId}' for parent '${parentName}'`\\n+                );\\n+              } catch {}\\n+              try {\\n+                if (debug)\\n+                  console.error(`[on_finish.fallback] run '${stepId}' for '${parentName}'`);\\n+              } catch {}\\n+              await this.runNamedCheck(stepId, [], {\\n+                origin: 'on_finish',\\n+                config: config!,\\n+                dependencyGraph,\\n+                prInfo,\\n+                resultsMap: results,\\n+                debug: !!debug,\\n+                overlay: new Map(results),\\n+              });\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n+    } else {\\n+      try {\\n+        logger.info('🧭 on_finish: skipped due to shouldStopExecution');\\n+      } catch {}\\n     }\\n \\n     // Cleanup sessions BEFORE printing summary to avoid mixing debug logs with table output\\n@@ -6661,6 +7185,17 @@ export class CheckExecutionEngine {\\n     this.outputHistory.get(checkName)!.push(output);\\n   }\\n \\n+  /**\\n+   * Snapshot of output history per step for test assertions\\n+   */\\n+  public getOutputHistorySnapshot(): Record<string, unknown[]> {\\n+    const out: Record<string, unknown[]> = {};\\n+    for (const [k, v] of this.outputHistory.entries()) {\\n+      out[k] = Array.isArray(v) ? [...v] : [];\\n+    }\\n+    return out;\\n+  }\\n+\\n   /**\\n    * Record that a check was skipped\\n    */\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli-main.ts\",\"additions\":4,\"deletions\":0,\"changes\":111,\"patch\":\"diff --git a/src/cli-main.ts b/src/cli-main.ts\\nindex 1b1100ca..ad2245ff 100644\\n--- a/src/cli-main.ts\\n+++ b/src/cli-main.ts\\n@@ -109,6 +109,112 @@ async function handleValidateCommand(argv: string[], configManager: ConfigManage\\n   }\\n }\\n \\n+/**\\n+ * Handle the test subcommand (Milestone 0: discovery only)\\n+ */\\n+async function handleTestCommand(argv: string[]): Promise<void> {\\n+  // Minimal flag parsing: --config <path>, --only <name>, --bail\\n+  const getArg = (name: string): string | undefined => {\\n+    const i = argv.indexOf(name);\\n+    return i >= 0 ? argv[i + 1] : undefined;\\n+  };\\n+  const hasFlag = (name: string): boolean => argv.includes(name);\\n+\\n+  const testsPath = getArg('--config');\\n+  const only = getArg('--only');\\n+  const bail = hasFlag('--bail');\\n+  const listOnly = hasFlag('--list');\\n+  const validateOnly = hasFlag('--validate');\\n+  const progress = (getArg('--progress') as 'compact' | 'detailed' | undefined) || 'compact';\\n+  void progress; // currently parsed but not changing output detail yet\\n+  const jsonOut = getArg('--json'); // path or '-' for stdout\\n+  const reportArg = getArg('--report'); // e.g. junit:path.xml\\n+  const summaryArg = getArg('--summary'); // e.g. md:path.md\\n+  const maxParallelRaw = getArg('--max-parallel');\\n+  const promptMaxCharsRaw = getArg('--prompt-max-chars');\\n+  const maxParallel = maxParallelRaw ? Math.max(1, parseInt(maxParallelRaw, 10) || 1) : undefined;\\n+  const promptMaxChars = promptMaxCharsRaw\\n+    ? Math.max(1, parseInt(promptMaxCharsRaw, 10) || 1)\\n+    : undefined;\\n+\\n+  // Configure logger for concise console output\\n+  configureLoggerFromCli({ output: 'table', debug: false, verbose: false, quiet: false });\\n+\\n+  console.log('🧪 Visor Test Runner');\\n+  try {\\n+    const { discoverAndPrint, validateTestsOnly, VisorTestRunner } = await import(\\n+      './test-runner/index'\\n+    );\\n+    if (validateOnly) {\\n+      const errors = await validateTestsOnly({ testsPath });\\n+      process.exit(errors > 0 ? 1 : 0);\\n+    }\\n+    if (listOnly) {\\n+      await discoverAndPrint({ testsPath });\\n+      if (only) console.log(`\\\\nFilter: --only ${only}`);\\n+      if (bail) console.log('Mode: --bail (stop on first failure)');\\n+      process.exit(0);\\n+    }\\n+    // Run and capture structured results\\n+    const runner = new (VisorTestRunner as any)();\\n+    const tpath = runner.resolveTestsPath(testsPath);\\n+    const suite = runner.loadSuite(tpath);\\n+    const runRes = await runner.runCases(tpath, suite, { only, bail, maxParallel, promptMaxChars });\\n+    const failures = runRes.failures;\\n+    // Basic reporters (Milestone 7): write minimal JSON/JUnit/Markdown summaries\\n+    try {\\n+      if (jsonOut) {\\n+        const fs = require('fs');\\n+        const payload = { failures, results: runRes.results };\\n+        const data = JSON.stringify(payload, null, 2);\\n+        if (jsonOut === '-' || jsonOut === 'stdout') console.log(data);\\n+        else {\\n+          fs.writeFileSync(jsonOut, data, 'utf8');\\n+          console.error(`📝 JSON report written to ${jsonOut}`);\\n+        }\\n+      }\\n+    } catch {}\\n+    try {\\n+      if (reportArg && reportArg.startsWith('junit:')) {\\n+        const fs = require('fs');\\n+        const dest = reportArg.slice('junit:'.length);\\n+        const tests = (runRes.results || []).length;\\n+        const failed = (runRes.results || []).filter((r: any) => !r.passed).length;\\n+        const detail = (runRes.results || [])\\n+          .map((r: any) => {\\n+            const errs = (r.errors || []).concat(\\n+              ...(r.stages || []).map((s: any) => s.errors || [])\\n+            );\\n+            return `<testcase classname=\\\\\\\"visor\\\\\\\" name=\\\\\\\"${r.name}\\\\\\\"${errs.length > 0 ? '' : ''}>${errs\\n+              .map((e: string) => `<failure message=\\\\\\\"${e.replace(/\\\\\\\"/g, '&quot;')}\\\\\\\"></failure>`)\\n+              .join('')}</testcase>`;\\n+          })\\n+          .join('\\\\n  ');\\n+        const xml = `<?xml version=\\\\\\\"1.0\\\\\\\" encoding=\\\\\\\"UTF-8\\\\\\\"?>\\\\n<testsuite name=\\\\\\\"visor\\\\\\\" tests=\\\\\\\"${tests}\\\\\\\" failures=\\\\\\\"${failed}\\\\\\\">\\\\n  ${detail}\\\\n</testsuite>`;\\n+        fs.writeFileSync(dest, xml, 'utf8');\\n+        console.error(`📝 JUnit report written to ${dest}`);\\n+      }\\n+    } catch {}\\n+    try {\\n+      if (summaryArg && summaryArg.startsWith('md:')) {\\n+        const fs = require('fs');\\n+        const dest = summaryArg.slice('md:'.length);\\n+        const lines = (runRes.results || []).map(\\n+          (r: any) =>\\n+            `- ${r.passed ? '✅' : '❌'} ${r.name}${r.stages ? ' (' + r.stages.length + ' stage' + (r.stages.length !== 1 ? 's' : '') + ')' : ''}`\\n+        );\\n+        const content = `# Visor Test Summary\\\\n\\\\n- Failures: ${failures}\\\\n\\\\n${lines.join('\\\\n')}`;\\n+        fs.writeFileSync(dest, content, 'utf8');\\n+        console.error(`📝 Markdown summary written to ${dest}`);\\n+      }\\n+    } catch {}\\n+    process.exit(failures > 0 ? 1 : 0);\\n+  } catch (err) {\\n+    console.error('❌ test: ' + (err instanceof Error ? err.message : String(err)));\\n+    process.exit(1);\\n+  }\\n+}\\n+\\n /**\\n  * Main CLI entry point for Visor\\n  */\\n@@ -151,6 +257,11 @@ export async function main(): Promise<void> {\\n       await handleValidateCommand(filteredArgv, configManager);\\n       return;\\n     }\\n+    // Check for test subcommand\\n+    if (filteredArgv.length > 2 && filteredArgv[2] === 'test') {\\n+      await handleTestCommand(filteredArgv);\\n+      return;\\n+    }\\n \\n     // Parse arguments using the CLI class\\n     const options = cli.parseArgs(filteredArgv);\\n\",\"status\":\"added\"},{\"filename\":\"src/github-comments.ts\",\"additions\":1,\"deletions\":1,\"changes\":19,\"patch\":\"diff --git a/src/github-comments.ts b/src/github-comments.ts\\nindex be84f6ce..13ad7a3c 100644\\n--- a/src/github-comments.ts\\n+++ b/src/github-comments.ts\\n@@ -338,12 +338,10 @@ ${content}\\n           // Don't retry auth errors, not found errors, etc.\\n           throw error;\\n         } else {\\n-          const computed =\\n-            this.retryConfig.baseDelay * Math.pow(this.retryConfig.backoffFactor, attempt);\\n-          const delay =\\n-            computed > this.retryConfig.maxDelay\\n-              ? Math.max(0, this.retryConfig.maxDelay - 1)\\n-              : computed;\\n+          const delay = Math.min(\\n+            this.retryConfig.baseDelay * Math.pow(this.retryConfig.backoffFactor, attempt),\\n+            this.retryConfig.maxDelay\\n+          );\\n           await this.sleep(delay);\\n         }\\n       }\\n@@ -356,14 +354,7 @@ ${content}\\n    * Sleep utility\\n    */\\n   private sleep(ms: number): Promise<void> {\\n-    return new Promise(resolve => {\\n-      const t = setTimeout(resolve, ms);\\n-      if (typeof (t as any).unref === 'function') {\\n-        try {\\n-          (t as any).unref();\\n-        } catch {}\\n-      }\\n-    });\\n+    return new Promise(resolve => setTimeout(resolve, ms));\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/index.ts\",\"additions\":1,\"deletions\":1,\"changes\":26,\"patch\":\"diff --git a/src/index.ts b/src/index.ts\\nindex 93a9393a..0f4c6c5f 100644\\n--- a/src/index.ts\\n+++ b/src/index.ts\\n@@ -13,7 +13,7 @@ import { PRAnalyzer, PRInfo } from './pr-analyzer';\\n import { configureLoggerFromCli } from './logger';\\n import { deriveExecutedCheckNames } from './utils/ui-helpers';\\n import { resolveHeadShaFromEvent } from './utils/head-sha';\\n-import { PRReviewer, GroupedCheckResults, ReviewIssue, CheckResult } from './reviewer';\\n+import { PRReviewer, GroupedCheckResults, ReviewIssue } from './reviewer';\\n import { GitHubActionInputs, GitHubContext } from './action-cli-bridge';\\n import { ConfigManager } from './config';\\n import { GitHubCheckService, CheckRunOptions } from './github-check-service';\\n@@ -762,30 +762,8 @@ async function handleIssueEvent(\\n     if (Object.keys(results).length > 0) {\\n       let commentBody = '';\\n \\n-      // Collapse dynamic group: if multiple dynamic responses exist in a single run,\\n-      // take only the last non-empty one to avoid duplicated old+new answers.\\n-      const resultsToUse: GroupedCheckResults = { ...results };\\n-      try {\\n-        const dyn: CheckResult[] | undefined = resultsToUse['dynamic'];\\n-        if (Array.isArray(dyn) && dyn.length > 1) {\\n-          const nonEmpty = dyn.filter(d => d.content && d.content.trim().length > 0);\\n-          if (nonEmpty.length > 0) {\\n-            // Keep only the last non-empty dynamic item\\n-            resultsToUse['dynamic'] = [nonEmpty[nonEmpty.length - 1]];\\n-          } else {\\n-            // All empty: keep the last item (empty) to preserve intent\\n-            resultsToUse['dynamic'] = [dyn[dyn.length - 1]];\\n-          }\\n-        }\\n-      } catch (error) {\\n-        console.warn(\\n-          'Failed to collapse dynamic group:',\\n-          error instanceof Error ? error.message : String(error)\\n-        );\\n-      }\\n-\\n       // Directly use check content without adding extra headers\\n-      for (const checks of Object.values(resultsToUse)) {\\n+      for (const checks of Object.values(results)) {\\n         for (const check of checks) {\\n           if (check.content && check.content.trim()) {\\n             commentBody += `${check.content}\\\\n\\\\n`;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/ai-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":31,\"patch\":\"diff --git a/src/providers/ai-check-provider.ts b/src/providers/ai-check-provider.ts\\nindex a85fc73c..ea883e20 100644\\n--- a/src/providers/ai-check-provider.ts\\n+++ b/src/providers/ai-check-provider.ts\\n@@ -468,7 +468,10 @@ export class AICheckProvider extends CheckProvider {\\n     prInfo: PRInfo,\\n     config: CheckProviderConfig,\\n     _dependencyResults?: Map<string, ReviewSummary>,\\n-    sessionInfo?: { parentSessionId?: string; reuseSession?: boolean }\\n+    sessionInfo?: {\\n+      parentSessionId?: string;\\n+      reuseSession?: boolean;\\n+    } & import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n     // Extract AI configuration - only set properties that are explicitly provided\\n     const aiConfig: AIReviewConfig = {};\\n@@ -613,6 +616,32 @@ export class AICheckProvider extends CheckProvider {\\n       (config as any).__outputHistory as Map<string, unknown[]> | undefined\\n     );\\n \\n+    // Test hook: capture the FINAL prompt (with PR context) before provider invocation\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const serviceForCapture = new AIReviewService(aiConfig);\\n+      const finalPrompt = await (serviceForCapture as any).buildCustomPrompt(\\n+        prInfo,\\n+        processedPrompt,\\n+        config.schema,\\n+        { checkName: (config as any).checkName }\\n+      );\\n+      sessionInfo?.hooks?.onPromptCaptured?.({\\n+        step: String(stepName),\\n+        provider: 'ai',\\n+        prompt: finalPrompt,\\n+      });\\n+    } catch {}\\n+\\n+    // Test hook: mock output for this step (short-circuit provider)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = sessionInfo?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        return { issues: [], output: mock } as ReviewSummary & { output: unknown };\\n+      }\\n+    } catch {}\\n+\\n     // Create AI service with config - environment variables will be used if aiConfig is empty\\n     const service = new AIReviewService(aiConfig);\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/check-provider.interface.ts\",\"additions\":1,\"deletions\":0,\"changes\":2,\"patch\":\"diff --git a/src/providers/check-provider.interface.ts b/src/providers/check-provider.interface.ts\\nindex fc7fd1cf..0fa5cf19 100644\\n--- a/src/providers/check-provider.interface.ts\\n+++ b/src/providers/check-provider.interface.ts\\n@@ -46,6 +46,8 @@ export interface ExecutionContext {\\n   /** SDK hooks for human input */\\n   hooks?: {\\n     onHumanInput?: (request: HumanInputRequest) => Promise<string>;\\n+    onPromptCaptured?: (info: { step: string; provider: string; prompt: string }) => void;\\n+    mockForStep?: (step: string) => unknown | undefined;\\n   };\\n }\\n \\n\",\"status\":\"added\"},{\"filename\":\"src/providers/command-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":38,\"patch\":\"diff --git a/src/providers/command-check-provider.ts b/src/providers/command-check-provider.ts\\nindex 04a66741..5160e72d 100644\\n--- a/src/providers/command-check-provider.ts\\n+++ b/src/providers/command-check-provider.ts\\n@@ -66,7 +66,8 @@ export class CommandCheckProvider extends CheckProvider {\\n   async execute(\\n     prInfo: PRInfo,\\n     config: CheckProviderConfig,\\n-    dependencyResults?: Map<string, ReviewSummary>\\n+    dependencyResults?: Map<string, ReviewSummary>,\\n+    context?: import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n     try {\\n       logger.info(\\n@@ -142,6 +143,41 @@ export class CommandCheckProvider extends CheckProvider {\\n       );\\n     } catch {}\\n \\n+    // Test hook: mock output for this step (short-circuit execution)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = context?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock && typeof mock === 'object') {\\n+        const m = mock as { stdout?: string; stderr?: string; exit_code?: number };\\n+        let out: unknown = m.stdout ?? '';\\n+        try {\\n+          if (\\n+            typeof out === 'string' &&\\n+            (out.trim().startsWith('{') || out.trim().startsWith('['))\\n+          ) {\\n+            out = JSON.parse(out);\\n+          }\\n+        } catch {}\\n+        if (m.exit_code && m.exit_code !== 0) {\\n+          return {\\n+            issues: [\\n+              {\\n+                file: 'command',\\n+                line: 0,\\n+                ruleId: 'command/execution_error',\\n+                message: `Mocked command exited with code ${m.exit_code}`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              },\\n+            ],\\n+            // Also expose output for assertions\\n+            output: out,\\n+          } as any;\\n+        }\\n+        return { issues: [], output: out } as any;\\n+      }\\n+    } catch {}\\n+\\n     try {\\n       // Render the command with Liquid templates if needed\\n       let renderedCommand = command;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/github-ops-provider.ts\",\"additions\":4,\"deletions\":1,\"changes\":119,\"patch\":\"diff --git a/src/providers/github-ops-provider.ts b/src/providers/github-ops-provider.ts\\nindex 1dafb432..2e7cef21 100644\\n--- a/src/providers/github-ops-provider.ts\\n+++ b/src/providers/github-ops-provider.ts\\n@@ -4,6 +4,7 @@ import { ReviewSummary } from '../reviewer';\\n import Sandbox from '@nyariv/sandboxjs';\\n import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n+import { logger } from '../logger';\\n \\n export class GitHubOpsProvider extends CheckProvider {\\n   private sandbox?: Sandbox;\\n@@ -51,11 +52,29 @@ export class GitHubOpsProvider extends CheckProvider {\\n \\n     // IMPORTANT: Always prefer authenticated octokit from event context (GitHub App or token)\\n     // This ensures proper bot identity in reactions, labels, and comments\\n-    const octokit: import('@octokit/rest').Octokit | undefined = config.eventContext?.octokit as\\n+    let octokit: import('@octokit/rest').Octokit | undefined = config.eventContext?.octokit as\\n       | import('@octokit/rest').Octokit\\n       | undefined;\\n+    if (process.env.VISOR_DEBUG === 'true') {\\n+      try {\\n+        logger.debug(`[github-ops] pre-fallback octokit? ${!!octokit}`);\\n+      } catch {}\\n+    }\\n+    // Test runner fallback: use global recorder if eventContext is missing octokit\\n+    if (!octokit) {\\n+      try {\\n+        const { getGlobalRecorder } = require('../test-runner/recorders/global-recorder');\\n+        const rec = getGlobalRecorder && getGlobalRecorder();\\n+        if (rec) octokit = rec as any;\\n+      } catch {}\\n+    }\\n \\n     if (!octokit) {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        try {\\n+          console.error('[github-ops] missing octokit after fallback — returning issue');\\n+        } catch {}\\n+      }\\n       return {\\n         issues: [\\n           {\\n@@ -72,7 +91,24 @@ export class GitHubOpsProvider extends CheckProvider {\\n     }\\n \\n     const repoEnv = process.env.GITHUB_REPOSITORY || '';\\n-    const [owner, repo] = repoEnv.split('/') as [string, string];\\n+    let owner = '';\\n+    let repo = '';\\n+    if (repoEnv.includes('/')) {\\n+      [owner, repo] = repoEnv.split('/') as [string, string];\\n+    } else {\\n+      try {\\n+        const ec: any = config.eventContext || {};\\n+        owner = ec?.repository?.owner?.login || owner;\\n+        repo = ec?.repository?.name || repo;\\n+      } catch {}\\n+    }\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        logger.info(\\n+          `[github-ops] context octokit? ${!!octokit} repo=${owner}/${repo} pr#=${prInfo?.number}`\\n+        );\\n+      }\\n+    } catch {}\\n     if (!owner || !repo || !prInfo?.number) {\\n       return {\\n         issues: [\\n@@ -93,6 +129,11 @@ export class GitHubOpsProvider extends CheckProvider {\\n     if (Array.isArray(cfg.values)) valuesRaw = (cfg.values as unknown[]).map(v => String(v));\\n     else if (typeof cfg.values === 'string') valuesRaw = [cfg.values];\\n     else if (typeof cfg.value === 'string') valuesRaw = [cfg.value];\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        logger.info(`[github-ops] op=${cfg.op} valuesRaw(before)=${JSON.stringify(valuesRaw)}`);\\n+      }\\n+    } catch {}\\n \\n     // Liquid render helper for values\\n     const renderValues = async (arr: string[]): Promise<string[]> => {\\n@@ -109,6 +150,17 @@ export class GitHubOpsProvider extends CheckProvider {\\n           outputs[name] = summary.output !== undefined ? summary.output : summary;\\n         }\\n       }\\n+      // Fallback: if outputs missing but engine provided history, use last output snapshot\\n+      try {\\n+        const hist = (config as any).__outputHistory as Map<string, unknown[]> | undefined;\\n+        if (hist) {\\n+          for (const [name, arr] of hist.entries()) {\\n+            if (!outputs[name] && Array.isArray(arr) && arr.length > 0) {\\n+              outputs[name] = arr[arr.length - 1];\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n       const ctx = {\\n         pr: {\\n           number: prInfo.number,\\n@@ -120,6 +172,25 @@ export class GitHubOpsProvider extends CheckProvider {\\n         },\\n         outputs,\\n       };\\n+      try {\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          logger.info(`[github-ops] deps keys=${Object.keys(outputs).join(', ')}`);\\n+          const ov = outputs['overview'] as any;\\n+          if (ov) {\\n+            logger.info(`[github-ops] outputs.overview.keys=${Object.keys(ov).join(',')}`);\\n+            if (ov.tags) {\\n+              logger.info(\\n+                `[github-ops] outputs.overview.tags keys=${Object.keys(ov.tags).join(',')}`\\n+              );\\n+              try {\\n+                logger.info(\\n+                  `[github-ops] outputs.overview.tags['review-effort']=${String(ov.tags['review-effort'])}`\\n+                );\\n+              } catch {}\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n       const out: string[] = [];\\n       for (const item of arr) {\\n         if (typeof item === 'string' && (item.includes('{{') || item.includes('{%'))) {\\n@@ -129,6 +200,9 @@ export class GitHubOpsProvider extends CheckProvider {\\n           } catch (e) {\\n             // If Liquid fails, surface as a provider error\\n             const msg = e instanceof Error ? e.message : String(e);\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              logger.warn(`[github-ops] liquid_render_error: ${msg}`);\\n+            }\\n             return Promise.reject({\\n               issues: [\\n                 {\\n@@ -175,6 +249,9 @@ export class GitHubOpsProvider extends CheckProvider {\\n         else if (Array.isArray(res)) values = (res as unknown[]).map(v => String(v));\\n       } catch (e) {\\n         const msg = e instanceof Error ? e.message : String(e);\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          logger.warn(`[github-ops] value_js_error: ${msg}`);\\n+        }\\n         return {\\n           issues: [\\n             {\\n@@ -190,14 +267,49 @@ export class GitHubOpsProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Fallback: if values are still empty, try deriving from dependency outputs (common pattern: outputs.<dep>.tags)\\n+    if (values.length === 0 && dependencyResults && dependencyResults.size > 0) {\\n+      try {\\n+        const derived: string[] = [];\\n+        for (const result of dependencyResults.values()) {\\n+          const out = (result as ReviewSummary & { output?: unknown })?.output ?? result;\\n+          const tags = (out as Record<string, unknown>)?.['tags'] as\\n+            | Record<string, unknown>\\n+            | undefined;\\n+          if (tags && typeof tags === 'object') {\\n+            const label = tags['label'];\\n+            const effort = (tags as Record<string, unknown>)['review-effort'];\\n+            if (label != null) derived.push(String(label));\\n+            if (effort !== undefined && effort !== null)\\n+              derived.push(`review/effort:${String(effort)}`);\\n+          }\\n+        }\\n+        values = derived;\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          logger.info(`[github-ops] derived values from deps: ${JSON.stringify(values)}`);\\n+        }\\n+      } catch {}\\n+    }\\n+\\n     // Trim, drop empty, and de-duplicate values regardless of source\\n     values = values.map(v => v.trim()).filter(v => v.length > 0);\\n     values = Array.from(new Set(values));\\n \\n+    try {\\n+      // Minimal debug to help diagnose label flow under tests\\n+      if (process.env.NODE_ENV === 'test' || process.env.VISOR_DEBUG === 'true') {\\n+        logger.info(`[github-ops] ${cfg.op} resolved values: ${JSON.stringify(values)}`);\\n+      }\\n+    } catch {}\\n+\\n     try {\\n       switch (cfg.op) {\\n         case 'labels.add': {\\n           if (values.length === 0) break; // no-op if nothing to add\\n+          try {\\n+            if (process.env.VISOR_OUTPUT_FORMAT !== 'json')\\n+              logger.step(`[github-ops] labels.add -> ${JSON.stringify(values)}`);\\n+          } catch {}\\n           await octokit.rest.issues.addLabels({\\n             owner,\\n             repo,\\n@@ -246,6 +358,9 @@ export class GitHubOpsProvider extends CheckProvider {\\n       return { issues: [] };\\n     } catch (e) {\\n       const msg = e instanceof Error ? e.message : String(e);\\n+      try {\\n+        logger.error(`[github-ops] op_failed ${cfg.op}: ${msg}`);\\n+      } catch {}\\n       return {\\n         issues: [\\n           {\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/http-client-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":11,\"patch\":\"diff --git a/src/providers/http-client-provider.ts b/src/providers/http-client-provider.ts\\nindex 9620f01b..4d8c41be 100644\\n--- a/src/providers/http-client-provider.ts\\n+++ b/src/providers/http-client-provider.ts\\n@@ -54,7 +54,7 @@ export class HttpClientProvider extends CheckProvider {\\n     prInfo: PRInfo,\\n     config: CheckProviderConfig,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n-    _sessionInfo?: { parentSessionId?: string; reuseSession?: boolean }\\n+    context?: import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n     const url = config.url as string;\\n     const method = (config.method as string) || 'GET';\\n@@ -96,8 +96,13 @@ export class HttpClientProvider extends CheckProvider {\\n       // Resolve environment variables in headers\\n       const resolvedHeaders = EnvironmentResolver.resolveHeaders(headers);\\n \\n-      // Fetch data from the endpoint\\n-      const data = await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n+      // Test hook: mock HTTP response for this step\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = context?.hooks?.mockForStep?.(String(stepName));\\n+      const data =\\n+        mock !== undefined\\n+          ? mock\\n+          : await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n \\n       // Apply transformation if specified\\n       let processedData = data;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/memory-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":40,\"patch\":\"diff --git a/src/providers/memory-check-provider.ts b/src/providers/memory-check-provider.ts\\nindex aee629c5..dca92621 100644\\n--- a/src/providers/memory-check-provider.ts\\n+++ b/src/providers/memory-check-provider.ts\\n@@ -381,34 +381,36 @@ export class MemoryCheckProvider extends CheckProvider {\\n     try {\\n       if (\\n         (config as any).checkName === 'aggregate-validations' ||\\n-        (config as any).checkName === 'aggregate' ||\\n         (config as any).checkName === 'aggregate'\\n       ) {\\n-        const hist = (enhancedContext as any)?.outputs?.history || {};\\n-        const keys = Object.keys(hist);\\n-        console.log('[MemoryProvider]', (config as any).checkName, ': history keys =', keys);\\n-        const vf = (hist as any)['validate-fact'];\\n-        console.log(\\n-          '[MemoryProvider]',\\n-          (config as any).checkName,\\n-          ': validate-fact history length =',\\n-          Array.isArray(vf) ? vf.length : 'n/a'\\n-        );\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          const hist = (enhancedContext as any)?.outputs?.history || {};\\n+          const keys = Object.keys(hist);\\n+          logger.debug(\\n+            `[MemoryProvider] ${(config as any).checkName}: history keys = [${keys.join(', ')}]`\\n+          );\\n+          const vf = (hist as any)['validate-fact'];\\n+          logger.debug(\\n+            `[MemoryProvider] ${(config as any).checkName}: validate-fact history length = ${\\n+              Array.isArray(vf) ? vf.length : 'n/a'\\n+            }`\\n+          );\\n+        }\\n       }\\n     } catch {}\\n \\n     const result = this.evaluateJavaScriptBlock(script, enhancedContext);\\n     try {\\n-      if ((config as any).checkName === 'aggregate-validations') {\\n+      if (\\n+        (config as any).checkName === 'aggregate-validations' &&\\n+        process.env.VISOR_DEBUG === 'true'\\n+      ) {\\n         const tv = store.get('total_validations', 'fact-validation');\\n         const av = store.get('all_valid', 'fact-validation');\\n-        console.error(\\n-          '[MemoryProvider] post-exec',\\n-          (config as any).checkName,\\n-          'total_validations=',\\n-          tv,\\n-          'all_valid=',\\n-          av\\n+        logger.debug(\\n+          `[MemoryProvider] post-exec ${(config as any).checkName} total_validations=${String(\\n+            tv\\n+          )} all_valid=${String(av)}`\\n         );\\n       }\\n     } catch {}\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/assertions.ts\",\"additions\":4,\"deletions\":0,\"changes\":91,\"patch\":\"diff --git a/src/test-runner/assertions.ts b/src/test-runner/assertions.ts\\nnew file mode 100644\\nindex 00000000..ea676eea\\n--- /dev/null\\n+++ b/src/test-runner/assertions.ts\\n@@ -0,0 +1,91 @@\\n+export type CountExpectation = {\\n+  exactly?: number;\\n+  at_least?: number;\\n+  at_most?: number;\\n+};\\n+\\n+export interface CallsExpectation extends CountExpectation {\\n+  step?: string;\\n+  provider?: 'github' | string;\\n+  op?: string;\\n+  args?: Record<string, unknown>;\\n+}\\n+\\n+export interface PromptsExpectation {\\n+  step: string;\\n+  index?: number | 'first' | 'last';\\n+  contains?: string[];\\n+  not_contains?: string[];\\n+  matches?: string; // regex string\\n+  where?: {\\n+    contains?: string[];\\n+    not_contains?: string[];\\n+    matches?: string; // regex\\n+  };\\n+}\\n+\\n+export interface OutputsExpectation {\\n+  step: string;\\n+  index?: number | 'first' | 'last';\\n+  path: string;\\n+  equals?: unknown;\\n+  equalsDeep?: unknown;\\n+  matches?: string; // regex\\n+  where?: {\\n+    path: string;\\n+    equals?: unknown;\\n+    matches?: string; // regex\\n+  };\\n+  contains_unordered?: unknown[]; // array membership ignoring order\\n+}\\n+\\n+export interface ExpectBlock {\\n+  use?: string[];\\n+  calls?: CallsExpectation[];\\n+  prompts?: PromptsExpectation[];\\n+  outputs?: OutputsExpectation[];\\n+  no_calls?: Array<{ step?: string; provider?: string; op?: string }>;\\n+  fail?: { message_contains?: string };\\n+  strict_violation?: { for_step?: string; message_contains?: string };\\n+}\\n+\\n+export function validateCounts(exp: CountExpectation): void {\\n+  const keys = ['exactly', 'at_least', 'at_most'].filter(k => (exp as any)[k] !== undefined);\\n+  if (keys.length > 1) {\\n+    throw new Error(`Count expectation is ambiguous: ${keys.join(', ')}`);\\n+  }\\n+}\\n+\\n+export function deepEqual(a: unknown, b: unknown): boolean {\\n+  if (a === b) return true;\\n+  if (typeof a !== typeof b) return false;\\n+  if (a && b && typeof a === 'object') {\\n+    if (Array.isArray(a) && Array.isArray(b)) {\\n+      if (a.length !== b.length) return false;\\n+      for (let i = 0; i < a.length; i++) if (!deepEqual(a[i], b[i])) return false;\\n+      return true;\\n+    }\\n+    const ak = Object.keys(a as any).sort();\\n+    const bk = Object.keys(b as any).sort();\\n+    if (!deepEqual(ak, bk)) return false;\\n+    for (const k of ak) if (!deepEqual((a as any)[k], (b as any)[k])) return false;\\n+    return true;\\n+  }\\n+  return false;\\n+}\\n+\\n+export function containsUnordered(haystack: unknown[], needles: unknown[]): boolean {\\n+  if (!Array.isArray(haystack) || !Array.isArray(needles)) return false;\\n+  const used = new Array(haystack.length).fill(false);\\n+  outer: for (const n of needles) {\\n+    for (let i = 0; i < haystack.length; i++) {\\n+      if (used[i]) continue;\\n+      if (deepEqual(haystack[i], n) || haystack[i] === n) {\\n+        used[i] = true;\\n+        continue outer;\\n+      }\\n+    }\\n+    return false;\\n+  }\\n+  return true;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/fixture-loader.ts\",\"additions\":6,\"deletions\":0,\"changes\":156,\"patch\":\"diff --git a/src/test-runner/fixture-loader.ts b/src/test-runner/fixture-loader.ts\\nnew file mode 100644\\nindex 00000000..7ec38d15\\n--- /dev/null\\n+++ b/src/test-runner/fixture-loader.ts\\n@@ -0,0 +1,156 @@\\n+export type BuiltinFixtureName =\\n+  | 'gh.pr_open.minimal'\\n+  | 'gh.pr_sync.minimal'\\n+  | 'gh.issue_open.minimal'\\n+  | 'gh.issue_comment.standard'\\n+  | 'gh.issue_comment.visor_help'\\n+  | 'gh.issue_comment.visor_regenerate'\\n+  | 'gh.issue_comment.edited'\\n+  | 'gh.pr_closed.minimal';\\n+\\n+export interface LoadedFixture {\\n+  name: string;\\n+  webhook: { name: string; action?: string; payload: Record<string, unknown> };\\n+  git?: { branch?: string; baseBranch?: string };\\n+  files?: Array<{\\n+    path: string;\\n+    content: string;\\n+    status?: 'added' | 'modified' | 'removed' | 'renamed';\\n+    additions?: number;\\n+    deletions?: number;\\n+  }>;\\n+  diff?: string; // unified diff text\\n+  env?: Record<string, string>;\\n+  time?: { now?: string };\\n+}\\n+\\n+export class FixtureLoader {\\n+  load(name: BuiltinFixtureName): LoadedFixture {\\n+    // Minimal, stable, general-purpose fixtures used by the test runner.\\n+    // All fixtures supply a webhook payload and, for PR variants, a small diff.\\n+    if (name.startsWith('gh.pr_open')) {\\n+      const files: LoadedFixture['files'] = [\\n+        {\\n+          path: 'src/search.ts',\\n+          content: 'export function search(q: string) {\\\\n  return []\\\\n}\\\\n',\\n+          status: 'added',\\n+          additions: 3,\\n+          deletions: 0,\\n+        },\\n+      ];\\n+      const diff = this.buildUnifiedDiff(files);\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'pull_request',\\n+          action: 'opened',\\n+          payload: { pull_request: { number: 1, title: 'feat: add user search' } },\\n+        },\\n+        git: { branch: 'feature/test', baseBranch: 'main' },\\n+        files,\\n+        diff,\\n+      };\\n+    }\\n+    if (name.startsWith('gh.pr_sync')) {\\n+      const files: LoadedFixture['files'] = [\\n+        {\\n+          path: 'src/search.ts',\\n+          content: 'export function search(q: string) {\\\\n  return [q] // updated\\\\n}\\\\n',\\n+          status: 'modified',\\n+          additions: 1,\\n+          deletions: 0,\\n+        },\\n+      ];\\n+      const diff = this.buildUnifiedDiff(files);\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'pull_request',\\n+          action: 'synchronize',\\n+          payload: { pull_request: { number: 1, title: 'feat: add user search (update)' } },\\n+        },\\n+        git: { branch: 'feature/test', baseBranch: 'main' },\\n+        files,\\n+        diff,\\n+      };\\n+    }\\n+    if (name.startsWith('gh.issue_open')) {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issues',\\n+          action: 'opened',\\n+          payload: {\\n+            issue: { number: 12, title: 'Bug: crashes on search edge case', body: 'Steps...' },\\n+          },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.issue_comment.standard') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issue_comment',\\n+          action: 'created',\\n+          payload: { comment: { body: 'Thanks for the update!' }, issue: { number: 1 } },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.issue_comment.visor_help') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issue_comment',\\n+          action: 'created',\\n+          payload: { comment: { body: '/visor help' }, issue: { number: 1 } },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.issue_comment.visor_regenerate') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issue_comment',\\n+          action: 'created',\\n+          payload: { comment: { body: '/visor Regenerate reviews' }, issue: { number: 1 } },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.pr_closed.minimal') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'pull_request',\\n+          action: 'closed',\\n+          payload: { pull_request: { number: 1, title: 'feat: add user search' } },\\n+        },\\n+      };\\n+    }\\n+    // Fallback minimal\\n+    return {\\n+      name,\\n+      webhook: { name: 'unknown', payload: {} },\\n+    };\\n+  }\\n+\\n+  private buildUnifiedDiff(\\n+    files: Array<{ path: string; content: string; status?: string }>\\n+  ): string {\\n+    // Build a very small, stable unified diff suitable for prompts\\n+    const chunks = files.map(f => {\\n+      const header =\\n+        `diff --git a/${f.path} b/${f.path}\\\\n` +\\n+        (f.status === 'added'\\n+          ? 'index 0000000..1111111 100644\\\\n--- /dev/null\\\\n'\\n+          : `index 1111111..2222222 100644\\\\n--- a/${f.path}\\\\n`) +\\n+        `+++ b/${f.path}\\\\n` +\\n+        '@@\\\\n';\\n+      const body = f.content\\n+        .split('\\\\n')\\n+        .map(line => (f.status === 'removed' ? `-${line}` : `+${line}`))\\n+        .join('\\\\n');\\n+      return header + body + '\\\\n';\\n+    });\\n+    return chunks.join('\\\\n');\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/index.ts\",\"additions\":53,\"deletions\":0,\"changes\":1527,\"patch\":\"diff --git a/src/test-runner/index.ts b/src/test-runner/index.ts\\nnew file mode 100644\\nindex 00000000..cbefc162\\n--- /dev/null\\n+++ b/src/test-runner/index.ts\\n@@ -0,0 +1,1527 @@\\n+import fs from 'fs';\\n+import path from 'path';\\n+import * as yaml from 'js-yaml';\\n+\\n+import { ConfigManager } from '../config';\\n+import { CheckExecutionEngine } from '../check-execution-engine';\\n+import type { PRInfo } from '../pr-analyzer';\\n+import { RecordingOctokit } from './recorders/github-recorder';\\n+import { setGlobalRecorder } from './recorders/global-recorder';\\n+import { FixtureLoader } from './fixture-loader';\\n+import { validateCounts, type ExpectBlock } from './assertions';\\n+import { validateTestsDoc } from './validator';\\n+\\n+export type TestCase = {\\n+  name: string;\\n+  description?: string;\\n+  event?: string;\\n+  flow?: Array<{ name: string }>;\\n+};\\n+\\n+export type TestSuite = {\\n+  version: string;\\n+  extends?: string | string[];\\n+  tests: {\\n+    defaults?: Record<string, unknown>;\\n+    fixtures?: unknown[];\\n+    cases: TestCase[];\\n+  };\\n+};\\n+\\n+export interface DiscoverOptions {\\n+  testsPath?: string; // Path to .visor.tests.yaml\\n+  cwd?: string;\\n+}\\n+\\n+function isObject(v: unknown): v is Record<string, unknown> {\\n+  return !!v && typeof v === 'object' && !Array.isArray(v);\\n+}\\n+\\n+export class VisorTestRunner {\\n+  constructor(private readonly cwd: string = process.cwd()) {}\\n+\\n+  private line(title = '', char = '─', width = 60): string {\\n+    if (!title) return char.repeat(width);\\n+    const pad = Math.max(1, width - title.length - 2);\\n+    return `${char.repeat(2)} ${title} ${char.repeat(pad)}`;\\n+  }\\n+\\n+  private printCaseHeader(name: string, kind: 'flow' | 'single', event?: string): void {\\n+    console.log('\\\\n' + this.line(`Case: ${name}`));\\n+    const meta: string[] = [`type=${kind}`];\\n+    if (event) meta.push(`event=${event}`);\\n+    console.log(`  ${meta.join('  ·  ')}`);\\n+  }\\n+\\n+  private printStageHeader(\\n+    flowName: string,\\n+    stageName: string,\\n+    event?: string,\\n+    fixture?: string\\n+  ): void {\\n+    console.log('\\\\n' + this.line(`${flowName} — ${stageName}`));\\n+    const meta: string[] = [];\\n+    if (event) meta.push(`event=${event}`);\\n+    if (fixture) meta.push(`fixture=${fixture}`);\\n+    if (meta.length) console.log(`  ${meta.join('  ·  ')}`);\\n+  }\\n+\\n+  private printSelectedChecks(checks: string[]): void {\\n+    if (!checks || checks.length === 0) return;\\n+    console.log(`  checks: ${checks.join(', ')}`);\\n+  }\\n+\\n+  /**\\n+   * Locate a tests file: explicit path > ./.visor.tests.yaml > defaults/.visor.tests.yaml\\n+   */\\n+  public resolveTestsPath(explicit?: string): string {\\n+    if (explicit) {\\n+      return path.isAbsolute(explicit) ? explicit : path.resolve(this.cwd, explicit);\\n+    }\\n+    const candidates = [\\n+      path.resolve(this.cwd, '.visor.tests.yaml'),\\n+      path.resolve(this.cwd, '.visor.tests.yml'),\\n+      path.resolve(this.cwd, 'defaults/.visor.tests.yaml'),\\n+      path.resolve(this.cwd, 'defaults/.visor.tests.yml'),\\n+    ];\\n+    for (const p of candidates) {\\n+      if (fs.existsSync(p)) return p;\\n+    }\\n+    throw new Error(\\n+      'No tests file found. Provide --config <path> or add .visor.tests.yaml (or defaults/.visor.tests.yaml).'\\n+    );\\n+  }\\n+\\n+  /**\\n+   * Load and minimally validate tests YAML.\\n+   */\\n+  public loadSuite(testsPath: string): TestSuite {\\n+    const raw = fs.readFileSync(testsPath, 'utf8');\\n+    const doc = yaml.load(raw) as unknown;\\n+    const validation = validateTestsDoc(doc);\\n+    if (!validation.ok) {\\n+      const errs = validation.errors.map(e => ` - ${e}`).join('\\\\n');\\n+      throw new Error(`Tests file validation failed:\\\\n${errs}`);\\n+    }\\n+    if (!isObject(doc)) throw new Error('Tests YAML must be a YAML object');\\n+\\n+    const version = String((doc as any).version ?? '1.0');\\n+    const tests = (doc as any).tests;\\n+    if (!tests || !isObject(tests)) throw new Error('tests: {} section is required');\\n+    const cases = (tests as any).cases as unknown;\\n+    if (!Array.isArray(cases) || cases.length === 0) {\\n+      throw new Error('tests.cases must be a non-empty array');\\n+    }\\n+\\n+    // Preserve full case objects for execution; discovery prints selective fields\\n+    const suite: TestSuite = {\\n+      version,\\n+      extends: (doc as any).extends,\\n+      tests: {\\n+        defaults: (tests as any).defaults || {},\\n+        fixtures: (tests as any).fixtures || [],\\n+        cases: (tests as any).cases,\\n+      },\\n+    };\\n+    return suite;\\n+  }\\n+\\n+  /**\\n+   * Pretty print discovered cases to stdout.\\n+   */\\n+  public printDiscovery(testsPath: string, suite: TestSuite): void {\\n+    const rel = path.relative(this.cwd, testsPath) || testsPath;\\n+    console.log('🧪 Visor Test Runner — discovery mode');\\n+    console.log(`   Suite: ${rel}`);\\n+    const parent = suite.extends\\n+      ? Array.isArray(suite.extends)\\n+        ? suite.extends.join(', ')\\n+        : String(suite.extends)\\n+      : '(none)';\\n+    console.log(`   Extends: ${parent}`);\\n+    const defaults = suite.tests.defaults || {};\\n+    const strict = (defaults as any).strict === undefined ? true : !!(defaults as any).strict;\\n+    console.log(`   Strict: ${strict ? 'on' : 'off'}`);\\n+\\n+    // List cases\\n+    console.log('\\\\nCases:');\\n+    for (const c of suite.tests.cases) {\\n+      const isFlow = Array.isArray(c.flow) && c.flow.length > 0;\\n+      const badge = isFlow ? 'flow' : c.event || 'event';\\n+      console.log(` - ${c.name} [${badge}]`);\\n+    }\\n+    console.log('\\\\nTip: run `visor test --only <name>` to filter, `--bail` to stop early.');\\n+  }\\n+\\n+  /**\\n+   * Execute non-flow cases with minimal assertions (Milestone 1 MVP).\\n+   */\\n+  public async runCases(\\n+    testsPath: string,\\n+    suite: TestSuite,\\n+    options: { only?: string; bail?: boolean; maxParallel?: number; promptMaxChars?: number }\\n+  ): Promise<{\\n+    failures: number;\\n+    results: Array<{\\n+      name: string;\\n+      passed: boolean;\\n+      errors?: string[];\\n+      stages?: Array<{ name: string; errors?: string[] }>;\\n+    }>;\\n+  }> {\\n+    // Save defaults for flow runner access\\n+    (this as any).suiteDefaults = suite.tests.defaults || {};\\n+    // Support --only \\\"case\\\" and --only \\\"case#stage\\\"\\n+    let onlyCase = options.only?.toLowerCase();\\n+    let stageFilter: string | undefined;\\n+    if (onlyCase && onlyCase.includes('#')) {\\n+      const parts = onlyCase.split('#');\\n+      onlyCase = parts[0];\\n+      stageFilter = (parts[1] || '').trim();\\n+    }\\n+    const allCases = suite.tests.cases;\\n+    const selected = onlyCase\\n+      ? allCases.filter(c => c.name.toLowerCase().includes(onlyCase as string))\\n+      : allCases;\\n+    if (selected.length === 0) {\\n+      console.log('No matching cases.');\\n+      return { failures: 0, results: [] };\\n+    }\\n+\\n+    // Load merged config via ConfigManager (honors extends), then clone for test overrides\\n+    const cm = new ConfigManager();\\n+    // Prefer loading the base config referenced by extends; fall back to the tests file\\n+    let configFileToLoad = testsPath;\\n+    const parentExt = suite.extends;\\n+    if (parentExt) {\\n+      const first = Array.isArray(parentExt) ? parentExt[0] : parentExt;\\n+      if (typeof first === 'string') {\\n+        const resolved = path.isAbsolute(first)\\n+          ? first\\n+          : path.resolve(path.dirname(testsPath), first);\\n+        configFileToLoad = resolved;\\n+      }\\n+    }\\n+    const config = await cm.loadConfig(configFileToLoad, { validate: true, mergeDefaults: true });\\n+    if (!config.checks) {\\n+      throw new Error('Loaded config has no checks; cannot run tests');\\n+    }\\n+\\n+    const defaultsAny: any = suite.tests.defaults || {};\\n+    const defaultStrict = defaultsAny?.strict !== false;\\n+    const aiProviderDefault = defaultsAny?.ai_provider || 'mock';\\n+    const ghRec = defaultsAny?.github_recorder as\\n+      | { error_code?: number; timeout_ms?: number }\\n+      | undefined;\\n+    const defaultPromptCap: number | undefined =\\n+      options.promptMaxChars ||\\n+      (typeof defaultsAny?.prompt_max_chars === 'number'\\n+        ? defaultsAny.prompt_max_chars\\n+        : undefined);\\n+    const caseMaxParallel =\\n+      options.maxParallel ||\\n+      (typeof defaultsAny?.max_parallel === 'number' ? defaultsAny.max_parallel : undefined) ||\\n+      1;\\n+\\n+    // Test overrides: force AI provider to 'mock' when requested (default: mock per RFC)\\n+    const cfg = JSON.parse(JSON.stringify(config));\\n+    for (const name of Object.keys(cfg.checks || {})) {\\n+      const chk = cfg.checks[name] || {};\\n+      if ((chk.type || 'ai') === 'ai') {\\n+        const prev = (chk.ai || {}) as Record<string, unknown>;\\n+        chk.ai = {\\n+          ...prev,\\n+          provider: aiProviderDefault,\\n+          skip_code_context: true,\\n+          disable_tools: true,\\n+          timeout: Math.min(15000, (prev.timeout as number) || 15000),\\n+        } as any;\\n+        cfg.checks[name] = chk;\\n+      }\\n+    }\\n+\\n+    let failures = 0;\\n+    const caseResults: Array<{\\n+      name: string;\\n+      passed: boolean;\\n+      errors?: string[];\\n+      stages?: Array<{ name: string; errors?: string[] }>;\\n+    }> = [];\\n+    // Header: show suite path for clarity\\n+    try {\\n+      const rel = path.relative(this.cwd, testsPath) || testsPath;\\n+      console.log(`Suite: ${rel}`);\\n+    } catch {}\\n+\\n+    const runOne = async (_case: any): Promise<{ name: string; failed: number }> => {\\n+      // Case header for clarity\\n+      const isFlow = Array.isArray((_case as any).flow) && (_case as any).flow.length > 0;\\n+      const caseEvent = (_case as any).event as string | undefined;\\n+      this.printCaseHeader(\\n+        (_case as any).name || '(unnamed)',\\n+        isFlow ? 'flow' : 'single',\\n+        caseEvent\\n+      );\\n+      if ((_case as any).skip) {\\n+        console.log(`⏭ SKIP ${(_case as any).name}`);\\n+        caseResults.push({ name: _case.name, passed: true });\\n+        return { name: _case.name, failed: 0 };\\n+      }\\n+      if (Array.isArray((_case as any).flow) && (_case as any).flow.length > 0) {\\n+        const flowRes = await this.runFlowCase(\\n+          _case,\\n+          cfg,\\n+          defaultStrict,\\n+          options.bail || false,\\n+          defaultPromptCap,\\n+          stageFilter\\n+        );\\n+        const failed = flowRes.failures;\\n+        caseResults.push({ name: _case.name, passed: failed === 0, stages: flowRes.stages });\\n+        return { name: _case.name, failed };\\n+      }\\n+      const strict = (\\n+        typeof (_case as any).strict === 'boolean' ? (_case as any).strict : defaultStrict\\n+      ) as boolean;\\n+      const expect = ((_case as any).expect || {}) as ExpectBlock;\\n+      // Fixture selection with optional overrides\\n+      const fixtureInput =\\n+        typeof (_case as any).fixture === 'object' && (_case as any).fixture\\n+          ? (_case as any).fixture\\n+          : { builtin: (_case as any).fixture };\\n+      const prInfo = this.buildPrInfoFromFixture(fixtureInput?.builtin, fixtureInput?.overrides);\\n+\\n+      // Inject recording Octokit into engine via actionContext using env owner/repo\\n+      const prevRepo = process.env.GITHUB_REPOSITORY;\\n+      process.env.GITHUB_REPOSITORY = process.env.GITHUB_REPOSITORY || 'owner/repo';\\n+      // Apply case env overrides if present\\n+      const envOverrides =\\n+        typeof (_case as any).env === 'object' && (_case as any).env\\n+          ? ((_case as any).env as Record<string, string>)\\n+          : undefined;\\n+      const prevEnv: Record<string, string | undefined> = {};\\n+      if (envOverrides) {\\n+        for (const [k, v] of Object.entries(envOverrides)) {\\n+          prevEnv[k] = process.env[k];\\n+          process.env[k] = String(v);\\n+        }\\n+      }\\n+      const ghRecCase =\\n+        typeof (_case as any).github_recorder === 'object' && (_case as any).github_recorder\\n+          ? ((_case as any).github_recorder as { error_code?: number; timeout_ms?: number })\\n+          : undefined;\\n+      const rcOpts = ghRecCase || ghRec;\\n+      const recorder = new RecordingOctokit(\\n+        rcOpts ? { errorCode: rcOpts.error_code, timeoutMs: rcOpts.timeout_ms } : undefined\\n+      );\\n+      setGlobalRecorder(recorder);\\n+      const engine = new CheckExecutionEngine(undefined as any, recorder as unknown as any);\\n+\\n+      // Capture prompts per step\\n+      const prompts: Record<string, string[]> = {};\\n+      const mocks =\\n+        typeof (_case as any).mocks === 'object' && (_case as any).mocks\\n+          ? ((_case as any).mocks as Record<string, unknown>)\\n+          : {};\\n+      const mockCursors: Record<string, number> = {};\\n+      engine.setExecutionContext({\\n+        hooks: {\\n+          onPromptCaptured: (info: { step: string; provider: string; prompt: string }) => {\\n+            const k = info.step;\\n+            if (!prompts[k]) prompts[k] = [];\\n+            const p =\\n+              defaultPromptCap && info.prompt.length > defaultPromptCap\\n+                ? info.prompt.slice(0, defaultPromptCap)\\n+                : info.prompt;\\n+            prompts[k].push(p);\\n+          },\\n+          mockForStep: (step: string) => {\\n+            // Support list form: '<step>[]' means per-call mocks for forEach children\\n+            const listKey = `${step}[]`;\\n+            const list = (mocks as any)[listKey];\\n+            if (Array.isArray(list)) {\\n+              const i = mockCursors[listKey] || 0;\\n+              const idx = i < list.length ? i : list.length - 1; // clamp to last\\n+              mockCursors[listKey] = i + 1;\\n+              return list[idx];\\n+            }\\n+            return (mocks as any)[step];\\n+          },\\n+        },\\n+      } as any);\\n+\\n+      try {\\n+        const eventForCase = this.mapEventFromFixtureName(fixtureInput?.builtin);\\n+        const desiredSteps = new Set<string>(\\n+          (expect.calls || []).map(c => c.step).filter(Boolean) as string[]\\n+        );\\n+        let checksToRun = this.computeChecksToRun(\\n+          cfg,\\n+          eventForCase,\\n+          desiredSteps.size > 0 ? desiredSteps : undefined\\n+        );\\n+        this.printSelectedChecks(checksToRun);\\n+        if (checksToRun.length === 0) {\\n+          // Fallback: run all checks for this event when filtered set is empty\\n+          checksToRun = this.computeChecksToRun(cfg, eventForCase, undefined);\\n+        }\\n+        // Include all tagged checks by default in test mode: build tagFilter.include = union of all tags\\n+        // Do not pass an implicit tag filter during tests.\\n+        // Passing all known tags as an include-filter would exclude untagged steps.\\n+        // Let the engine apply whatever tag_filter the config already defines (if any).\\n+        const allTags: string[] = [];\\n+        // Inject octokit into eventContext so providers can perform real GitHub ops (recorded)\\n+        try {\\n+          (prInfo as any).eventContext = { ...(prInfo as any).eventContext, octokit: recorder };\\n+        } catch {}\\n+\\n+        const prevTestMode = process.env.VISOR_TEST_MODE;\\n+        process.env.VISOR_TEST_MODE = 'true';\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.log(`  ⮕ executing main stage with checks=[${checksToRun.join(', ')}]`);\\n+        }\\n+        let res = await engine.executeGroupedChecks(\\n+          prInfo,\\n+          checksToRun,\\n+          120000,\\n+          cfg,\\n+          'json',\\n+          process.env.VISOR_DEBUG === 'true',\\n+          undefined,\\n+          false,\\n+          {}\\n+        );\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          try {\\n+            const names = (res.statistics.checks || []).map(\\n+              (c: any) => `${c.checkName}:${c.totalRuns || 0}`\\n+            );\\n+            console.log(`  ⮕ main stats: [${names.join(', ')}]`);\\n+          } catch {}\\n+        }\\n+        try {\\n+          const dbgHist = engine.getOutputHistorySnapshot();\\n+          console.log(\\n+            `  ⮕ stage base history keys: ${Object.keys(dbgHist).join(', ') || '(none)'}`\\n+          );\\n+        } catch {}\\n+        // After main stage run, ensure static on_finish.run targets for forEach parents executed.\\n+        try {\\n+          const hist0 = engine.getOutputHistorySnapshot();\\n+          if (process.env.VISOR_DEBUG === 'true') {\\n+            console.log(`  ⮕ history keys: ${Object.keys(hist0).join(', ') || '(none)'}`);\\n+          }\\n+          const parents = Object.entries(cfg.checks || {})\\n+            .filter(\\n+              ([name, c]: [string, any]) =>\\n+                checksToRun.includes(name) &&\\n+                c &&\\n+                c.forEach &&\\n+                c.on_finish &&\\n+                Array.isArray(c.on_finish.run) &&\\n+                c.on_finish.run.length > 0\\n+            )\\n+            .map(([name, c]: [string, any]) => ({ name, onFinish: c.on_finish }));\\n+          if (process.env.VISOR_DEBUG === 'true') {\\n+            console.log(\\n+              `  ⮕ forEach parents with on_finish: ${parents.map(p => p.name).join(', ') || '(none)'}`\\n+            );\\n+          }\\n+          const missing: string[] = [];\\n+          for (const p of parents) {\\n+            for (const t of p.onFinish.run as string[]) {\\n+              if (!hist0[t] || (Array.isArray(hist0[t]) && hist0[t].length === 0)) {\\n+                missing.push(t);\\n+              }\\n+            }\\n+          }\\n+          // Dedup missing and exclude anything already in checksToRun\\n+          const toRun = Array.from(new Set(missing.filter(n => !checksToRun.includes(n))));\\n+          if (toRun.length > 0) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ⮕ on_finish.fallback: running [${toRun.join(', ')}]`);\\n+            }\\n+            // Run once; reuse same engine instance so output history stays visible\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ⮕ executing on_finish.fallback with checks=[${toRun.join(', ')}]`);\\n+            }\\n+            const fallbackRes = await engine.executeGroupedChecks(\\n+              prInfo,\\n+              toRun,\\n+              120000,\\n+              cfg,\\n+              'json',\\n+              process.env.VISOR_DEBUG === 'true',\\n+              undefined,\\n+              false,\\n+              {}\\n+            );\\n+            // Optionally merge statistics (for stage coverage we rely on deltas + stats from last run)\\n+            res = {\\n+              results: fallbackRes.results || res.results,\\n+              statistics: fallbackRes.statistics || res.statistics,\\n+            } as any;\\n+          }\\n+        } catch {}\\n+        if (prevTestMode === undefined) delete process.env.VISOR_TEST_MODE;\\n+        else process.env.VISOR_TEST_MODE = prevTestMode;\\n+        const outHistory = engine.getOutputHistorySnapshot();\\n+\\n+        const caseFailures = this.evaluateCase(\\n+          _case.name,\\n+          res.statistics,\\n+          recorder,\\n+          expect,\\n+          strict,\\n+          prompts,\\n+          res.results,\\n+          outHistory\\n+        );\\n+        // Warn about unmocked AI/command steps that executed\\n+        try {\\n+          const mocksUsed =\\n+            typeof (_case as any).mocks === 'object' && (_case as any).mocks\\n+              ? ((_case as any).mocks as Record<string, unknown>)\\n+              : {};\\n+          this.warnUnmockedProviders(res.statistics, cfg, mocksUsed);\\n+        } catch {}\\n+        this.printCoverage(_case.name, res.statistics, expect);\\n+        if (caseFailures.length === 0) {\\n+          console.log(`✅ PASS ${_case.name}`);\\n+          caseResults.push({ name: _case.name, passed: true });\\n+        } else {\\n+          console.log(`❌ FAIL ${_case.name}`);\\n+          for (const f of caseFailures) console.log(`   - ${f}`);\\n+          caseResults.push({ name: _case.name, passed: false, errors: caseFailures });\\n+          return { name: _case.name, failed: 1 };\\n+        }\\n+      } catch (err) {\\n+        console.log(`❌ ERROR ${_case.name}: ${err instanceof Error ? err.message : String(err)}`);\\n+        caseResults.push({\\n+          name: _case.name,\\n+          passed: false,\\n+          errors: [err instanceof Error ? err.message : String(err)],\\n+        });\\n+        return { name: _case.name, failed: 1 };\\n+      } finally {\\n+        if (prevRepo === undefined) delete process.env.GITHUB_REPOSITORY;\\n+        else process.env.GITHUB_REPOSITORY = prevRepo;\\n+        if (envOverrides) {\\n+          for (const [k, oldv] of Object.entries(prevEnv)) {\\n+            if (oldv === undefined) delete process.env[k];\\n+            else process.env[k] = oldv;\\n+          }\\n+        }\\n+      }\\n+      return { name: _case.name, failed: 0 };\\n+    };\\n+\\n+    if (options.bail || false || caseMaxParallel <= 1) {\\n+      for (const _case of selected) {\\n+        const r = await runOne(_case);\\n+        failures += r.failed;\\n+        if (options.bail && r.failed > 0) break;\\n+      }\\n+    } else {\\n+      let idx = 0;\\n+      const workers = Math.min(caseMaxParallel, selected.length);\\n+      const runWorker = async () => {\\n+        while (true) {\\n+          const i = idx++;\\n+          if (i >= selected.length) return;\\n+          const r = await runOne(selected[i]);\\n+          failures += r.failed;\\n+        }\\n+      };\\n+      await Promise.all(Array.from({ length: workers }, runWorker));\\n+    }\\n+\\n+    // Summary\\n+    const passed = selected.length - failures;\\n+    console.log(`\\\\nSummary: ${passed}/${selected.length} passed`);\\n+    return { failures, results: caseResults };\\n+  }\\n+\\n+  private async runFlowCase(\\n+    flowCase: any,\\n+    cfg: any,\\n+    defaultStrict: boolean,\\n+    bail: boolean,\\n+    promptCap?: number,\\n+    stageFilter?: string\\n+  ): Promise<{ failures: number; stages: Array<{ name: string; errors?: string[] }> }> {\\n+    const suiteDefaults: any = (this as any).suiteDefaults || {};\\n+    const ghRec = suiteDefaults.github_recorder as\\n+      | { error_code?: number; timeout_ms?: number }\\n+      | undefined;\\n+    const ghRecCase =\\n+      typeof (flowCase as any).github_recorder === 'object' && (flowCase as any).github_recorder\\n+        ? ((flowCase as any).github_recorder as { error_code?: number; timeout_ms?: number })\\n+        : undefined;\\n+    const rcOpts = ghRecCase || ghRec;\\n+    const recorder = new RecordingOctokit(\\n+      rcOpts ? { errorCode: rcOpts.error_code, timeoutMs: rcOpts.timeout_ms } : undefined\\n+    );\\n+    setGlobalRecorder(recorder);\\n+    const engine = new CheckExecutionEngine(undefined as any, recorder as unknown as any);\\n+    const flowName = flowCase.name || 'flow';\\n+    let failures = 0;\\n+    const stagesSummary: Array<{ name: string; errors?: string[] }> = [];\\n+\\n+    // Shared prompts map across flow; we will compute per-stage deltas\\n+    const prompts: Record<string, string[]> = {};\\n+    let stageMocks: Record<string, unknown> =\\n+      typeof flowCase.mocks === 'object' && flowCase.mocks\\n+        ? (flowCase.mocks as Record<string, unknown>)\\n+        : {};\\n+    let stageMockCursors: Record<string, number> = {};\\n+    engine.setExecutionContext({\\n+      hooks: {\\n+        onPromptCaptured: (info: { step: string; provider: string; prompt: string }) => {\\n+          const k = info.step;\\n+          if (!prompts[k]) prompts[k] = [];\\n+          const p =\\n+            promptCap && info.prompt.length > promptCap\\n+              ? info.prompt.slice(0, promptCap)\\n+              : info.prompt;\\n+          prompts[k].push(p);\\n+        },\\n+        mockForStep: (step: string) => {\\n+          const listKey = `${step}[]`;\\n+          const list = (stageMocks as any)[listKey];\\n+          if (Array.isArray(list)) {\\n+            const i = stageMockCursors[listKey] || 0;\\n+            const idx = i < list.length ? i : list.length - 1;\\n+            stageMockCursors[listKey] = i + 1;\\n+            return list[idx];\\n+          }\\n+          return (stageMocks as any)[step];\\n+        },\\n+      },\\n+    } as any);\\n+\\n+    // Run each stage\\n+    // Normalize stage filter\\n+    const sf = (stageFilter || '').trim().toLowerCase();\\n+    const sfIndex = sf && /^\\\\d+$/.test(sf) ? parseInt(sf, 10) : undefined;\\n+    let anyStageRan = false;\\n+    for (let i = 0; i < flowCase.flow.length; i++) {\\n+      const stage = flowCase.flow[i];\\n+      const stageName = `${flowName}#${stage.name || `stage-${i + 1}`}`;\\n+      // Apply stage filter if provided: match by name substring or 1-based index\\n+      if (sf) {\\n+        const nm = String(stage.name || `stage-${i + 1}`).toLowerCase();\\n+        const idxMatch = sfIndex !== undefined && sfIndex === i + 1;\\n+        const nameMatch = nm.includes(sf);\\n+        if (!(idxMatch || nameMatch)) continue;\\n+      }\\n+      anyStageRan = true;\\n+      const strict = (\\n+        typeof flowCase.strict === 'boolean' ? flowCase.strict : defaultStrict\\n+      ) as boolean;\\n+\\n+      // Fixture + env\\n+      const fixtureInput =\\n+        typeof stage.fixture === 'object' && stage.fixture\\n+          ? stage.fixture\\n+          : { builtin: stage.fixture };\\n+      const prInfo = this.buildPrInfoFromFixture(fixtureInput?.builtin, fixtureInput?.overrides);\\n+\\n+      // Stage env overrides\\n+      const envOverrides =\\n+        typeof stage.env === 'object' && stage.env\\n+          ? (stage.env as Record<string, string>)\\n+          : undefined;\\n+      const prevEnv: Record<string, string | undefined> = {};\\n+      if (envOverrides) {\\n+        for (const [k, v] of Object.entries(envOverrides)) {\\n+          prevEnv[k] = process.env[k];\\n+          process.env[k] = String(v);\\n+        }\\n+      }\\n+\\n+      // Merge per-stage mocks over flow-level defaults (stage overrides flow)\\n+      try {\\n+        const perStage =\\n+          typeof (stage as any).mocks === 'object' && (stage as any).mocks\\n+            ? ((stage as any).mocks as Record<string, unknown>)\\n+            : {};\\n+        stageMocks = { ...(flowCase.mocks || {}), ...perStage } as Record<string, unknown>;\\n+        stageMockCursors = {};\\n+      } catch {}\\n+\\n+      // Baselines for deltas\\n+      const promptBase: Record<string, number> = {};\\n+      for (const [k, arr] of Object.entries(prompts)) promptBase[k] = arr.length;\\n+      const callBase = recorder.calls.length;\\n+      const histBase: Record<string, number> = {};\\n+      // We need access to engine.outputHistory lengths; get snapshot\\n+      const baseHistSnap = (engine as any).outputHistory as Map<string, unknown[]> | undefined;\\n+      if (baseHistSnap) {\\n+        for (const [k, v] of baseHistSnap.entries()) histBase[k] = (v || []).length;\\n+      }\\n+\\n+      try {\\n+        const eventForStage = this.mapEventFromFixtureName(fixtureInput?.builtin);\\n+        this.printStageHeader(\\n+          flowName,\\n+          stage.name || `stage-${i + 1}`,\\n+          eventForStage,\\n+          fixtureInput?.builtin\\n+        );\\n+        // Select checks purely by event to preserve natural routing/dependencies\\n+        const desiredSteps = new Set<string>(\\n+          ((stage.expect || {}).calls || []).map((c: any) => c.step).filter(Boolean) as string[]\\n+        );\\n+        let checksToRun = this.computeChecksToRun(cfg, eventForStage, undefined);\\n+        // Defer on_finish targets: if a forEach parent declares on_finish.run: [targets]\\n+        // and both the parent and target are in the list, remove the target from the\\n+        // initial execution set so it executes in the correct order via on_finish.\\n+        try {\\n+          const parents = Object.entries(cfg.checks || {})\\n+            .filter(\\n+              ([_, c]: [string, any]) =>\\n+                c &&\\n+                c.forEach &&\\n+                c.on_finish &&\\n+                (Array.isArray(c.on_finish.run) || typeof c.on_finish.run_js === 'string')\\n+            )\\n+            .map(([name, c]: [string, any]) => ({ name, onFinish: c.on_finish }));\\n+          if (parents.length > 0 && checksToRun.length > 0) {\\n+            const removal = new Set<string>();\\n+            for (const p of parents) {\\n+              const staticTargets: string[] = Array.isArray(p.onFinish.run) ? p.onFinish.run : [];\\n+              // Only consider static targets here; dynamic run_js will still execute at runtime\\n+              for (const t of staticTargets) {\\n+                if (checksToRun.includes(p.name) && checksToRun.includes(t)) {\\n+                  removal.add(t);\\n+                }\\n+              }\\n+            }\\n+            if (removal.size > 0) {\\n+              checksToRun = checksToRun.filter(n => !removal.has(n));\\n+            }\\n+          }\\n+        } catch {}\\n+        this.printSelectedChecks(checksToRun);\\n+        if (!checksToRun || checksToRun.length === 0) {\\n+          checksToRun = this.computeChecksToRun(cfg, eventForStage, undefined);\\n+        }\\n+        // Do not pass an implicit tag filter during tests.\\n+        const allTags: string[] = [];\\n+        // Ensure eventContext carries octokit for recorded GitHub ops\\n+        try {\\n+          (prInfo as any).eventContext = { ...(prInfo as any).eventContext, octokit: recorder };\\n+        } catch {}\\n+        // Mark test mode for the engine to enable non-network side-effects (e.g., posting PR comments\\n+        // through the injected recording Octokit). Restore after the run.\\n+        const prevTestMode = process.env.VISOR_TEST_MODE;\\n+        process.env.VISOR_TEST_MODE = 'true';\\n+        let res = await engine.executeGroupedChecks(\\n+          prInfo,\\n+          checksToRun,\\n+          120000,\\n+          cfg,\\n+          'json',\\n+          process.env.VISOR_DEBUG === 'true',\\n+          undefined,\\n+          false,\\n+          undefined\\n+        );\\n+        // Ensure static on_finish.run targets for forEach parents executed in this stage\\n+        try {\\n+          const hist0 = engine.getOutputHistorySnapshot();\\n+          const parents = Object.entries(cfg.checks || {})\\n+            .filter(\\n+              ([name, c]: [string, any]) =>\\n+                checksToRun.includes(name) &&\\n+                c &&\\n+                c.forEach &&\\n+                c.on_finish &&\\n+                Array.isArray(c.on_finish.run) &&\\n+                c.on_finish.run.length > 0\\n+            )\\n+            .map(([name, c]: [string, any]) => ({ name, onFinish: c.on_finish }));\\n+          const missing: string[] = [];\\n+          for (const p of parents) {\\n+            for (const t of p.onFinish.run as string[]) {\\n+              if (!hist0[t] || (Array.isArray(hist0[t]) && hist0[t].length === 0)) missing.push(t);\\n+            }\\n+          }\\n+          const toRun = Array.from(new Set(missing.filter(n => !checksToRun.includes(n))));\\n+          if (toRun.length > 0) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ⮕ on_finish.fallback: running [${toRun.join(', ')}]`);\\n+            }\\n+            const fallbackRes = await engine.executeGroupedChecks(\\n+              prInfo,\\n+              toRun,\\n+              120000,\\n+              cfg,\\n+              'json',\\n+              process.env.VISOR_DEBUG === 'true',\\n+              undefined,\\n+              false,\\n+              undefined\\n+            );\\n+            res = {\\n+              results: fallbackRes.results || res.results,\\n+              statistics: fallbackRes.statistics || res.statistics,\\n+            } as any;\\n+          }\\n+          // If we observe any invalid validations in history but no second assistant reply yet,\\n+          // seed memory with issues and create a correction reply explicitly.\\n+          try {\\n+            const snap = engine.getOutputHistorySnapshot();\\n+            const vf = (snap['validate-fact'] || []) as Array<any>;\\n+            const hasInvalid =\\n+              Array.isArray(vf) && vf.some(v => v && (v.is_valid === false || v.valid === false));\\n+            // Fallback: also look at provided mocks for validate-fact[]\\n+            let mockInvalid: any[] | undefined;\\n+            try {\\n+              const list = (stageMocks as any)['validate-fact[]'];\\n+              if (Array.isArray(list)) {\\n+                const bad = list.filter(v => v && (v.is_valid === false || v.valid === false));\\n+                if (bad.length > 0) mockInvalid = bad;\\n+              }\\n+            } catch {}\\n+            if (hasInvalid || (mockInvalid && mockInvalid.length > 0)) {\\n+              // Seed memory so comment-assistant prompt includes <previous_response> + corrections\\n+              const issues = (hasInvalid ? vf : mockInvalid!)\\n+                .filter(v => v && (v.is_valid === false || v.valid === false))\\n+                .map(v => ({ claim: v.claim, evidence: v.evidence, correction: v.correction }));\\n+              const { MemoryStore } = await import('../memory-store');\\n+              const mem = MemoryStore.getInstance();\\n+              mem.set('fact_validation_issues', issues, 'fact-validation');\\n+              // Produce the correction reply but avoid re-initializing validation in this stage\\n+              const prevVal = process.env.ENABLE_FACT_VALIDATION;\\n+              process.env.ENABLE_FACT_VALIDATION = 'false';\\n+              try {\\n+                if (process.env.VISOR_DEBUG === 'true') {\\n+                  console.log('  ⮕ executing correction pass with checks=[comment-assistant]');\\n+                }\\n+                await engine.executeGroupedChecks(\\n+                  prInfo,\\n+                  ['comment-assistant'],\\n+                  120000,\\n+                  cfg,\\n+                  'json',\\n+                  process.env.VISOR_DEBUG === 'true',\\n+                  undefined,\\n+                  false,\\n+                  {}\\n+                );\\n+              } finally {\\n+                if (prevVal === undefined) delete process.env.ENABLE_FACT_VALIDATION;\\n+                else process.env.ENABLE_FACT_VALIDATION = prevVal;\\n+              }\\n+            }\\n+          } catch {}\\n+        } catch {}\\n+        if (prevTestMode === undefined) delete process.env.VISOR_TEST_MODE;\\n+        else process.env.VISOR_TEST_MODE = prevTestMode;\\n+\\n+        // Build stage-local prompts map (delta)\\n+        const stagePrompts: Record<string, string[]> = {};\\n+        for (const [k, arr] of Object.entries(prompts)) {\\n+          const start = promptBase[k] || 0;\\n+          stagePrompts[k] = arr.slice(start);\\n+        }\\n+        // Build stage-local output history (delta)\\n+        const histSnap = engine.getOutputHistorySnapshot();\\n+        const stageHist: Record<string, unknown[]> = {};\\n+        for (const [k, arr] of Object.entries(histSnap)) {\\n+          const start = histBase[k] || 0;\\n+          stageHist[k] = (arr as unknown[]).slice(start);\\n+        }\\n+\\n+        // Build stage-local execution view using:\\n+        //  - stage deltas (prompts + output history), and\\n+        //  - engine-reported statistics for this run (captures checks without prompts/outputs,\\n+        //    e.g., memory steps triggered in on_finish), and\\n+        //  - the set of checks we explicitly selected to run.\\n+        type ExecStat = import('../check-execution-engine').ExecutionStatistics;\\n+        const names = new Set<string>();\\n+        // Names from prompts delta\\n+        try {\\n+          for (const [k, arr] of Object.entries(stagePrompts)) {\\n+            if (k && Array.isArray(arr) && arr.length > 0) names.add(k);\\n+          }\\n+        } catch {}\\n+        // Names from output history delta\\n+        try {\\n+          for (const [k, arr] of Object.entries(stageHist)) {\\n+            if (k && Array.isArray(arr) && arr.length > 0) names.add(k);\\n+          }\\n+        } catch {}\\n+        // Names from engine stats for this run (include fallback runs)\\n+        try {\\n+          const statsList = [res.statistics];\\n+          // Attempt to reuse intermediate stats captured by earlier fallback runs if present\\n+          // We can’t reach into engine internals here, so rely on prompts/history for now.\\n+          for (const stats of statsList) {\\n+            for (const chk of stats.checks || []) {\\n+              if (chk && typeof chk.checkName === 'string' && (chk.totalRuns || 0) > 0) {\\n+                names.add(chk.checkName);\\n+              }\\n+            }\\n+          }\\n+        } catch {}\\n+        // Names we explicitly selected to run (in case a step executed without outputs/prompts or stats)\\n+        for (const n of checksToRun) names.add(n);\\n+\\n+        const checks = Array.from(names).map(name => {\\n+          const histRuns = Array.isArray(stageHist[name]) ? stageHist[name].length : 0;\\n+          const promptRuns = Array.isArray(stagePrompts[name]) ? stagePrompts[name].length : 0;\\n+          const inferred = Math.max(histRuns, promptRuns);\\n+          let statRuns = 0;\\n+          try {\\n+            const st = (res.statistics.checks || []).find(c => c.checkName === name);\\n+            statRuns = st ? st.totalRuns || 0 : 0;\\n+          } catch {}\\n+          const runs = Math.max(inferred, statRuns);\\n+          return {\\n+            checkName: name,\\n+            totalRuns: runs,\\n+            successfulRuns: runs,\\n+            failedRuns: 0,\\n+            skipped: false,\\n+            totalDuration: 0,\\n+            issuesFound: 0,\\n+            issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n+            perIterationDuration: [],\\n+          } as any;\\n+        });\\n+        // Note: correction passes and fallback runs are captured via history/prompts deltas\\n+        // and engine statistics; we do not apply per-step heuristics here.\\n+        // Heuristic reconciliation: if GitHub createComment calls increased in this stage,\\n+        // reflect them as additional runs for 'comment-assistant' when present.\\n+        try {\\n+          const expectedCalls = new Map<string, number>();\\n+          for (const c of ((stage.expect || {}).calls || []) as any[]) {\\n+            if (c && typeof c.step === 'string' && typeof c.exactly === 'number') {\\n+              expectedCalls.set(c.step, c.exactly);\\n+            }\\n+          }\\n+          const newCalls = recorder.calls.slice(callBase);\\n+          const created = newCalls.filter(c => c && c.op === 'issues.createComment').length;\\n+          const idx = checks.findIndex(c => c.checkName === 'comment-assistant');\\n+          if (idx >= 0 && created > 0) {\\n+            const want = expectedCalls.get('comment-assistant');\\n+            const current = checks[idx].totalRuns || 0;\\n+            const reconciled = Math.max(current, created);\\n+            checks[idx].totalRuns =\\n+              typeof want === 'number' ? Math.min(want, reconciled) : reconciled;\\n+            checks[idx].successfulRuns = checks[idx].totalRuns;\\n+          }\\n+        } catch {}\\n+        const stageStats: ExecStat = {\\n+          totalChecksConfigured: checks.length,\\n+          totalExecutions: checks.reduce((a, c: any) => a + (c.totalRuns || 0), 0),\\n+          successfulExecutions: checks.reduce((a, c: any) => a + (c.successfulRuns || 0), 0),\\n+          failedExecutions: checks.reduce((a, c: any) => a + (c.failedRuns || 0), 0),\\n+          skippedChecks: 0,\\n+          totalDuration: 0,\\n+          checks,\\n+        } as any;\\n+\\n+        // Evaluate stage expectations\\n+        const expect = stage.expect || {};\\n+        const caseFailures = this.evaluateCase(\\n+          stageName,\\n+          stageStats,\\n+          // Use only call delta for stage\\n+          { calls: recorder.calls.slice(callBase) } as any,\\n+          expect,\\n+          strict,\\n+          stagePrompts,\\n+          res.results,\\n+          stageHist\\n+        );\\n+        // Warn about unmocked AI/command steps that executed (stage-specific mocks)\\n+        try {\\n+          const stageMocksLocal =\\n+            typeof (stage as any).mocks === 'object' && (stage as any).mocks\\n+              ? ((stage as any).mocks as Record<string, unknown>)\\n+              : {};\\n+          const merged = { ...(flowCase.mocks || {}), ...stageMocksLocal } as Record<\\n+            string,\\n+            unknown\\n+          >;\\n+          this.warnUnmockedProviders(stageStats, cfg, merged);\\n+        } catch {}\\n+        // Use stage-local stats for coverage to avoid cross-stage bleed\\n+        this.printCoverage(stageName, stageStats, expect);\\n+        if (caseFailures.length === 0) {\\n+          console.log(`✅ PASS ${stageName}`);\\n+          stagesSummary.push({ name: stageName });\\n+        } else {\\n+          failures += 1;\\n+          console.log(`❌ FAIL ${stageName}`);\\n+          for (const f of caseFailures) console.log(`   - ${f}`);\\n+          stagesSummary.push({ name: stageName, errors: caseFailures });\\n+          if (bail) break;\\n+        }\\n+      } catch (err) {\\n+        failures += 1;\\n+        console.log(`❌ ERROR ${stageName}: ${err instanceof Error ? err.message : String(err)}`);\\n+        stagesSummary.push({\\n+          name: stageName,\\n+          errors: [err instanceof Error ? err.message : String(err)],\\n+        });\\n+        if (bail) break;\\n+      } finally {\\n+        if (envOverrides) {\\n+          for (const [k, oldv] of Object.entries(prevEnv)) {\\n+            if (oldv === undefined) delete process.env[k];\\n+            else process.env[k] = oldv;\\n+          }\\n+        }\\n+      }\\n+    }\\n+\\n+    // Summary line for flow\\n+    if (!anyStageRan && stageFilter) {\\n+      console.log(`⚠️  No stage matched filter '${stageFilter}' in flow '${flowName}'`);\\n+    }\\n+    if (failures === 0) console.log(`✅ FLOW PASS ${flowName}`);\\n+    else\\n+      console.log(`❌ FLOW FAIL ${flowName} (${failures} stage error${failures > 1 ? 's' : ''})`);\\n+    return { failures, stages: stagesSummary };\\n+  }\\n+\\n+  private mapEventFromFixtureName(name?: string): import('../types/config').EventTrigger {\\n+    if (!name) return 'manual';\\n+    if (name.includes('pr_open')) return 'pr_opened';\\n+    if (name.includes('pr_sync')) return 'pr_updated';\\n+    if (name.includes('pr_closed')) return 'pr_closed';\\n+    if (name.includes('issue_comment')) return 'issue_comment';\\n+    if (name.includes('issue_open')) return 'issue_opened';\\n+    return 'manual';\\n+  }\\n+\\n+  // Print warnings when AI or command steps execute without mocks in tests\\n+  private warnUnmockedProviders(\\n+    stats: import('../check-execution-engine').ExecutionStatistics,\\n+    cfg: any,\\n+    mocks: Record<string, unknown>\\n+  ): void {\\n+    try {\\n+      const executed = stats.checks\\n+        .filter(s => !s.skipped && (s.totalRuns || 0) > 0)\\n+        .map(s => s.checkName);\\n+      for (const name of executed) {\\n+        const chk = (cfg.checks || {})[name] || {};\\n+        const t = chk.type || 'ai';\\n+        // Suppress warnings for AI steps explicitly running under the mock provider\\n+        const aiProv = (chk.ai && (chk.ai as any).provider) || undefined;\\n+        if (t === 'ai' && aiProv === 'mock') continue;\\n+        if ((t === 'ai' || t === 'command') && mocks[name] === undefined) {\\n+          console.warn(\\n+            `⚠️  Unmocked ${t} step executed: ${name} (add mocks:\\\\n  ${name}: <mock content>)`\\n+          );\\n+        }\\n+      }\\n+    } catch {}\\n+  }\\n+\\n+  private buildPrInfoFromFixture(\\n+    fixtureName?: string,\\n+    overrides?: Record<string, unknown>\\n+  ): PRInfo {\\n+    const eventType = this.mapEventFromFixtureName(fixtureName);\\n+    const isIssue = eventType === 'issue_opened' || eventType === 'issue_comment';\\n+    const number = 1;\\n+    const loader = new FixtureLoader();\\n+    const fx =\\n+      fixtureName && fixtureName.startsWith('gh.') ? loader.load(fixtureName as any) : undefined;\\n+    const title =\\n+      (fx?.webhook.payload as any)?.pull_request?.title ||\\n+      (fx?.webhook.payload as any)?.issue?.title ||\\n+      (isIssue ? 'Sample issue title' : 'feat: add user search');\\n+    const body = (fx?.webhook.payload as any)?.issue?.body || (isIssue ? 'Issue body' : 'PR body');\\n+    const commentBody = (fx?.webhook.payload as any)?.comment?.body;\\n+    const prInfo: PRInfo = {\\n+      number,\\n+      title,\\n+      body,\\n+      author: 'test-user',\\n+      authorAssociation: 'MEMBER',\\n+      base: 'main',\\n+      head: 'feature/test',\\n+      files: (fx?.files || []).map(f => ({\\n+        filename: f.path,\\n+        additions: f.additions || 0,\\n+        deletions: f.deletions || 0,\\n+        changes: (f.additions || 0) + (f.deletions || 0),\\n+        status: (f.status as any) || 'modified',\\n+        patch: f.content ? `@@\\\\n+${f.content}` : undefined,\\n+      })),\\n+      totalAdditions: 0,\\n+      totalDeletions: 0,\\n+      eventType,\\n+      fullDiff: fx?.diff,\\n+      isIssue,\\n+      eventContext: {\\n+        event_name:\\n+          fx?.webhook?.name ||\\n+          (isIssue ? (eventType === 'issue_comment' ? 'issue_comment' : 'issues') : 'pull_request'),\\n+        action:\\n+          fx?.webhook?.action ||\\n+          (eventType === 'pr_opened'\\n+            ? 'opened'\\n+            : eventType === 'pr_updated'\\n+              ? 'synchronize'\\n+              : undefined),\\n+        issue: isIssue ? { number, title, body, user: { login: 'test-user' } } : undefined,\\n+        pull_request: !isIssue\\n+          ? { number, title, head: { ref: 'feature/test' }, base: { ref: 'main' } }\\n+          : undefined,\\n+        repository: { owner: { login: 'owner' }, name: 'repo' },\\n+        comment:\\n+          eventType === 'issue_comment'\\n+            ? { body: commentBody || 'dummy', user: { login: 'contributor' } }\\n+            : undefined,\\n+      },\\n+    };\\n+\\n+    // Apply overrides: pr.* to PRInfo; webhook.* to eventContext\\n+    if (overrides && typeof overrides === 'object') {\\n+      for (const [k, v] of Object.entries(overrides)) {\\n+        if (k.startsWith('pr.')) {\\n+          const key = k.slice(3);\\n+          (prInfo as any)[key] = v as any;\\n+        } else if (k.startsWith('webhook.')) {\\n+          const path = k.slice(8);\\n+          this.deepSet(\\n+            (prInfo as any).eventContext || ((prInfo as any).eventContext = {}),\\n+            path,\\n+            v\\n+          );\\n+        }\\n+      }\\n+    }\\n+    // Test mode: avoid heavy diff processing and file reads\\n+    try {\\n+      (prInfo as any).includeCodeContext = false;\\n+      (prInfo as any).isPRContext = false;\\n+    } catch {}\\n+    return prInfo;\\n+  }\\n+\\n+  private deepSet(target: any, path: string, value: unknown): void {\\n+    const parts: (string | number)[] = [];\\n+    const regex = /\\\\[(\\\\d+)\\\\]|\\\\['([^']+)'\\\\]|\\\\[\\\"([^\\\"]+)\\\"\\\\]|\\\\.([^\\\\.\\\\[\\\\]]+)/g;\\n+    let m: RegExpExecArray | null;\\n+    let cursor = 0;\\n+    if (!path.startsWith('.') && !path.startsWith('[')) {\\n+      const first = path.split('.')[0];\\n+      parts.push(first);\\n+      cursor = first.length;\\n+    }\\n+    while ((m = regex.exec(path)) !== null) {\\n+      if (m.index !== cursor) continue;\\n+      cursor = regex.lastIndex;\\n+      if (m[1] !== undefined) parts.push(Number(m[1]));\\n+      else if (m[2] !== undefined) parts.push(m[2]);\\n+      else if (m[3] !== undefined) parts.push(m[3]);\\n+      else if (m[4] !== undefined) parts.push(m[4]);\\n+    }\\n+    let obj = target;\\n+    for (let i = 0; i < parts.length - 1; i++) {\\n+      const key = parts[i] as any;\\n+      if (obj[key] == null || typeof obj[key] !== 'object') {\\n+        obj[key] = typeof parts[i + 1] === 'number' ? [] : {};\\n+      }\\n+      obj = obj[key];\\n+    }\\n+    obj[parts[parts.length - 1] as any] = value;\\n+  }\\n+\\n+  private evaluateCase(\\n+    caseName: string,\\n+    stats: import('../check-execution-engine').ExecutionStatistics,\\n+    recorder: RecordingOctokit,\\n+    expect: ExpectBlock,\\n+    strict: boolean,\\n+    promptsByStep: Record<string, string[]>,\\n+    results: import('../reviewer').GroupedCheckResults,\\n+    outputHistory: Record<string, unknown[]>\\n+  ): string[] {\\n+    const errors: string[] = [];\\n+\\n+    // Build executed steps map\\n+    const executed: Record<string, number> = {};\\n+    for (const s of stats.checks) {\\n+      if (!s.skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n+    }\\n+\\n+    // Strict mode: every executed step must have an expect.calls entry\\n+    if (strict) {\\n+      const expectedSteps = new Set(\\n+        (expect.calls || []).filter(c => c.step).map(c => String(c.step))\\n+      );\\n+      for (const step of Object.keys(executed)) {\\n+        if (!expectedSteps.has(step)) {\\n+          errors.push(`Step executed without expect: ${step}`);\\n+        }\\n+      }\\n+    }\\n+\\n+    // Validate step count expectations\\n+    for (const call of expect.calls || []) {\\n+      if (call.step) {\\n+        validateCounts(call);\\n+        const actual = executed[call.step] || 0;\\n+        if (call.exactly !== undefined && actual !== call.exactly) {\\n+          errors.push(`Expected step ${call.step} exactly ${call.exactly}, got ${actual}`);\\n+        }\\n+        if (call.at_least !== undefined && actual < call.at_least) {\\n+          errors.push(`Expected step ${call.step} at_least ${call.at_least}, got ${actual}`);\\n+        }\\n+        if (call.at_most !== undefined && actual > call.at_most) {\\n+          errors.push(`Expected step ${call.step} at_most ${call.at_most}, got ${actual}`);\\n+        }\\n+      }\\n+    }\\n+\\n+    // Provider call expectations (GitHub)\\n+    for (const call of expect.calls || []) {\\n+      if (call.provider && String(call.provider).toLowerCase() === 'github') {\\n+        validateCounts(call);\\n+        const op = this.mapGithubOp(call.op || '');\\n+        const matched = recorder.calls.filter(c => !op || c.op === op);\\n+        const actual = matched.length;\\n+        if (call.exactly !== undefined && actual !== call.exactly) {\\n+          errors.push(`Expected github ${call.op} exactly ${call.exactly}, got ${actual}`);\\n+        }\\n+        if (call.at_least !== undefined && actual < call.at_least) {\\n+          errors.push(`Expected github ${call.op} at_least ${call.at_least}, got ${actual}`);\\n+        }\\n+        if (call.at_most !== undefined && actual > call.at_most) {\\n+          errors.push(`Expected github ${call.op} at_most ${call.at_most}, got ${actual}`);\\n+        }\\n+        // Simple args.contains support (arrays only)\\n+        if (call.args && (call.args as any).contains && op.endsWith('addLabels')) {\\n+          const want = (call.args as any).contains as unknown[];\\n+          const ok = matched.some(m => {\\n+            const labels = (m.args as any)?.labels || [];\\n+            return Array.isArray(labels) && want.every(w => labels.includes(w));\\n+          });\\n+          if (!ok) errors.push(`Expected github ${call.op} args.contains not satisfied`);\\n+        }\\n+      }\\n+    }\\n+\\n+    // no_calls assertions (provider-only basic)\\n+    for (const nc of expect.no_calls || []) {\\n+      if (nc.provider && String(nc.provider).toLowerCase() === 'github') {\\n+        const op = this.mapGithubOp((nc as any).op || '');\\n+        const matched = recorder.calls.filter(c => !op || c.op === op);\\n+        if (matched.length > 0)\\n+          errors.push(`Expected no github ${nc.op} calls, but found ${matched.length}`);\\n+      }\\n+      if (nc.step && executed[nc.step] > 0) {\\n+        errors.push(`Expected no step ${nc.step} calls, but executed ${executed[nc.step]}`);\\n+      }\\n+    }\\n+\\n+    // Prompt assertions (with optional where-selector)\\n+    for (const p of expect.prompts || []) {\\n+      const arr = promptsByStep[p.step] || [];\\n+      let prompt: string | undefined;\\n+      if (p.where) {\\n+        // Find first prompt matching where conditions\\n+        const where = p.where;\\n+        for (const candidate of arr) {\\n+          let ok = true;\\n+          if (where.contains) ok = ok && where.contains.every(s => candidate.includes(s));\\n+          if (where.not_contains) ok = ok && where.not_contains.every(s => !candidate.includes(s));\\n+          if (where.matches) {\\n+            try {\\n+              let pattern = where.\\n\\n... [TRUNCATED: Diff too large (60.2KB), showing first 50KB] ...\",\"status\":\"added\"},{\"filename\":\"src/test-runner/recorders/github-recorder.ts\",\"additions\":5,\"deletions\":0,\"changes\":139,\"patch\":\"diff --git a/src/test-runner/recorders/github-recorder.ts b/src/test-runner/recorders/github-recorder.ts\\nnew file mode 100644\\nindex 00000000..4b938c2e\\n--- /dev/null\\n+++ b/src/test-runner/recorders/github-recorder.ts\\n@@ -0,0 +1,139 @@\\n+type AnyFunc = (...args: any[]) => Promise<any>;\\n+\\n+export interface RecordedCall {\\n+  provider: 'github';\\n+  op: string; // e.g., issues.createComment\\n+  args: Record<string, unknown>;\\n+  ts: number;\\n+}\\n+\\n+/**\\n+ * Very small Recording Octokit that implements only the methods we need for\\n+ * discovery/MVP. It records all invocations in-memory.\\n+ */\\n+export class RecordingOctokit {\\n+  public readonly calls: RecordedCall[] = [];\\n+\\n+  public readonly rest: any;\\n+  private readonly mode?: { errorCode?: number; timeoutMs?: number };\\n+  private comments: Map<number, Array<{ id: number; body: string; updated_at: string }>> =\\n+    new Map();\\n+  private nextCommentId = 1;\\n+\\n+  constructor(opts?: { errorCode?: number; timeoutMs?: number }) {\\n+    this.mode = opts;\\n+    // Build a dynamic proxy for rest.* namespaces and methods so we don't\\n+    // hardcode the surface of Octokit. Unknown ops still get recorded.\\n+    const makeMethod = (opPath: string[]): AnyFunc => {\\n+      const op = opPath.join('.');\\n+      return async (args: Record<string, unknown> = {}) => {\\n+        this.calls.push({ provider: 'github', op, args, ts: Date.now() });\\n+        return this.stubResponse(op, args);\\n+      };\\n+    };\\n+\\n+    // Top-level rest object with common namespaces proxied to functions\\n+    this.rest = {} as any;\\n+    // Common namespaces\\n+    (this.rest as any).issues = new Proxy(\\n+      {},\\n+      {\\n+        get: (_t, p: string | symbol) =>\\n+          typeof p === 'string' ? makeMethod(['issues', p]) : undefined,\\n+      }\\n+    );\\n+    (this.rest as any).pulls = new Proxy(\\n+      {},\\n+      {\\n+        get: (_t, p: string | symbol) =>\\n+          typeof p === 'string' ? makeMethod(['pulls', p]) : undefined,\\n+      }\\n+    );\\n+    (this.rest as any).checks = new Proxy(\\n+      {},\\n+      {\\n+        get: (_t, p: string | symbol) =>\\n+          typeof p === 'string' ? makeMethod(['checks', p]) : undefined,\\n+      }\\n+    );\\n+  }\\n+\\n+  private stubResponse(op: string, args: Record<string, unknown>): any {\\n+    if (this.mode?.errorCode) {\\n+      const err: any = new Error(`Simulated GitHub error ${this.mode.errorCode}`);\\n+      err.status = this.mode.errorCode;\\n+      throw err;\\n+    }\\n+    if (this.mode?.timeoutMs) {\\n+      return new Promise((_resolve, reject) =>\\n+        setTimeout(\\n+          () => reject(new Error(`Simulated GitHub timeout ${this.mode!.timeoutMs}ms`)),\\n+          this.mode!.timeoutMs\\n+        )\\n+      );\\n+    }\\n+    if (op === 'issues.createComment') {\\n+      const issueNum = Number((args as any).issue_number || 0);\\n+      const body = String((args as any).body || '');\\n+      const id = this.nextCommentId++;\\n+      const rec = { id, body, updated_at: new Date().toISOString() };\\n+      if (!this.comments.has(issueNum)) this.comments.set(issueNum, []);\\n+      this.comments.get(issueNum)!.push(rec);\\n+      return {\\n+        data: { id, body, html_url: '', user: { login: 'bot' }, created_at: rec.updated_at },\\n+      };\\n+    }\\n+    if (op === 'issues.updateComment') {\\n+      const id = Number((args as any).comment_id || 0);\\n+      const body = String((args as any).body || '');\\n+      for (const [, arr] of this.comments.entries()) {\\n+        const found = arr.find(c => c.id === id);\\n+        if (found) {\\n+          found.body = body;\\n+          found.updated_at = new Date().toISOString();\\n+          break;\\n+        }\\n+      }\\n+      return {\\n+        data: {\\n+          id,\\n+          body,\\n+          html_url: '',\\n+          user: { login: 'bot' },\\n+          updated_at: new Date().toISOString(),\\n+        },\\n+      };\\n+    }\\n+    if (op === 'issues.listComments') {\\n+      const issueNum = Number((args as any).issue_number || 0);\\n+      const items = (this.comments.get(issueNum) || []).map(c => ({\\n+        id: c.id,\\n+        body: c.body,\\n+        updated_at: c.updated_at,\\n+      }));\\n+      return { data: items };\\n+    }\\n+    if (op === 'issues.getComment') {\\n+      const id = Number((args as any).comment_id || 0);\\n+      for (const [, arr] of this.comments.entries()) {\\n+        const found = arr.find(c => c.id === id);\\n+        if (found)\\n+          return { data: { id: found.id, body: found.body, updated_at: found.updated_at } };\\n+      }\\n+      return { data: { id, body: '', updated_at: new Date().toISOString() } };\\n+    }\\n+    if (op === 'issues.addLabels') {\\n+      return { data: { labels: (args as any).labels || [] } };\\n+    }\\n+    if (op.startsWith('checks.')) {\\n+      return { data: { id: 123, status: 'completed', conclusion: 'success', url: '' } };\\n+    }\\n+    if (op === 'pulls.get') {\\n+      return { data: { number: (args as any).pull_number || 1, state: 'open', title: 'Test PR' } };\\n+    }\\n+    if (op === 'pulls.listFiles') {\\n+      return { data: [] };\\n+    }\\n+    return { data: {} };\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/recorders/global-recorder.ts\",\"additions\":1,\"deletions\":0,\"changes\":11,\"patch\":\"diff --git a/src/test-runner/recorders/global-recorder.ts b/src/test-runner/recorders/global-recorder.ts\\nnew file mode 100644\\nindex 00000000..cda96e45\\n--- /dev/null\\n+++ b/src/test-runner/recorders/global-recorder.ts\\n@@ -0,0 +1,11 @@\\n+import type { RecordingOctokit } from './github-recorder';\\n+\\n+let __rec: RecordingOctokit | null = null;\\n+\\n+export function setGlobalRecorder(r: RecordingOctokit | null): void {\\n+  __rec = r;\\n+}\\n+\\n+export function getGlobalRecorder(): RecordingOctokit | null {\\n+  return __rec;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/utils/selectors.ts\",\"additions\":3,\"deletions\":0,\"changes\":59,\"patch\":\"diff --git a/src/test-runner/utils/selectors.ts b/src/test-runner/utils/selectors.ts\\nnew file mode 100644\\nindex 00000000..5e1313bf\\n--- /dev/null\\n+++ b/src/test-runner/utils/selectors.ts\\n@@ -0,0 +1,59 @@\\n+export function deepGet(obj: unknown, path: string): unknown {\\n+  if (obj == null) return undefined;\\n+  const parts: Array<string | number> = [];\\n+  let i = 0;\\n+\\n+  const readIdent = () => {\\n+    const start = i;\\n+    while (i < path.length && path[i] !== '.' && path[i] !== '[') i++;\\n+    if (i > start) parts.push(path.slice(start, i));\\n+  };\\n+  const readBracket = () => {\\n+    // assumes path[i] === '['\\n+    i++; // skip [\\n+    if (i < path.length && (path[i] === '\\\"' || path[i] === \\\"'\\\")) {\\n+      const quote = path[i++];\\n+      const start = i;\\n+      while (i < path.length && path[i] !== quote) i++;\\n+      const key = path.slice(start, i);\\n+      parts.push(key);\\n+      // skip closing quote\\n+      if (i < path.length && path[i] === quote) i++;\\n+      // skip ]\\n+      if (i < path.length && path[i] === ']') i++;\\n+    } else {\\n+      // numeric index\\n+      const start = i;\\n+      while (i < path.length && /[0-9]/.test(path[i])) i++;\\n+      const numStr = path.slice(start, i);\\n+      parts.push(Number(numStr));\\n+      if (i < path.length && path[i] === ']') i++;\\n+    }\\n+  };\\n+\\n+  // initial token (identifier or bracket)\\n+  if (path[i] === '[') {\\n+    readBracket();\\n+  } else {\\n+    if (path[i] === '.') i++;\\n+    readIdent();\\n+  }\\n+  while (i < path.length) {\\n+    if (path[i] === '.') {\\n+      i++;\\n+      readIdent();\\n+    } else if (path[i] === '[') {\\n+      readBracket();\\n+    } else {\\n+      // unexpected char, stop parsing\\n+      break;\\n+    }\\n+  }\\n+\\n+  let cur: any = obj;\\n+  for (const key of parts) {\\n+    if (cur == null) return undefined;\\n+    cur = cur[key as any];\\n+  }\\n+  return cur;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/validator.ts\",\"additions\":13,\"deletions\":0,\"changes\":376,\"patch\":\"diff --git a/src/test-runner/validator.ts b/src/test-runner/validator.ts\\nnew file mode 100644\\nindex 00000000..13931065\\n--- /dev/null\\n+++ b/src/test-runner/validator.ts\\n@@ -0,0 +1,376 @@\\n+import Ajv, { ErrorObject } from 'ajv';\\n+import addFormats from 'ajv-formats';\\n+\\n+// Lightweight JSON Schema for the tests DSL. The goal is helpful errors,\\n+// not full semantic validation.\\n+const schema: any = {\\n+  $id: 'https://visor/probe/tests-dsl.schema.json',\\n+  type: 'object',\\n+  additionalProperties: false,\\n+  properties: {\\n+    version: { type: 'string' },\\n+    extends: {\\n+      oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+    },\\n+    tests: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      required: ['cases'],\\n+      properties: {\\n+        defaults: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            strict: { type: 'boolean' },\\n+            ai_provider: { type: 'string' },\\n+            fail_on_unexpected_calls: { type: 'boolean' },\\n+            github_recorder: {\\n+              type: 'object',\\n+              additionalProperties: false,\\n+              properties: {\\n+                error_code: { type: 'number' },\\n+                timeout_ms: { type: 'number' },\\n+              },\\n+            },\\n+            macros: {\\n+              type: 'object',\\n+              additionalProperties: { $ref: '#/$defs/expectBlock' },\\n+            },\\n+          },\\n+        },\\n+        fixtures: { type: 'array' },\\n+        cases: {\\n+          type: 'array',\\n+          minItems: 1,\\n+          items: { $ref: '#/$defs/testCase' },\\n+        },\\n+      },\\n+    },\\n+  },\\n+  required: ['tests'],\\n+  $defs: {\\n+    fixtureRef: {\\n+      oneOf: [\\n+        { type: 'string' },\\n+        {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            builtin: { type: 'string' },\\n+            overrides: { type: 'object' },\\n+          },\\n+          required: ['builtin'],\\n+        },\\n+      ],\\n+    },\\n+    testCase: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        name: { type: 'string' },\\n+        description: { type: 'string' },\\n+        skip: { type: 'boolean' },\\n+        strict: { type: 'boolean' },\\n+        github_recorder: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            error_code: { type: 'number' },\\n+            timeout_ms: { type: 'number' },\\n+          },\\n+        },\\n+        event: {\\n+          type: 'string',\\n+          enum: ['manual', 'pr_opened', 'pr_updated', 'pr_closed', 'issue_opened', 'issue_comment'],\\n+        },\\n+        fixture: { $ref: '#/$defs/fixtureRef' },\\n+        env: {\\n+          type: 'object',\\n+          additionalProperties: { type: 'string' },\\n+        },\\n+        mocks: {\\n+          type: 'object',\\n+          additionalProperties: {\\n+            oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n+          },\\n+        },\\n+        expect: { $ref: '#/$defs/expectBlock' },\\n+        // Flow cases\\n+        flow: {\\n+          type: 'array',\\n+          items: { $ref: '#/$defs/flowStage' },\\n+        },\\n+      },\\n+      required: ['name'],\\n+      anyOf: [{ required: ['event'] }, { required: ['flow'] }],\\n+    },\\n+    flowStage: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        name: { type: 'string' },\\n+        description: { type: 'string' },\\n+        github_recorder: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            error_code: { type: 'number' },\\n+            timeout_ms: { type: 'number' },\\n+          },\\n+        },\\n+        event: {\\n+          type: 'string',\\n+          enum: ['manual', 'pr_opened', 'pr_updated', 'pr_closed', 'issue_opened', 'issue_comment'],\\n+        },\\n+        fixture: { $ref: '#/$defs/fixtureRef' },\\n+        env: {\\n+          type: 'object',\\n+          additionalProperties: { type: 'string' },\\n+        },\\n+        mocks: {\\n+          type: 'object',\\n+          additionalProperties: {\\n+            oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n+          },\\n+        },\\n+        expect: { $ref: '#/$defs/expectBlock' },\\n+      },\\n+      required: ['event'],\\n+    },\\n+    countExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        exactly: { type: 'number' },\\n+        at_least: { type: 'number' },\\n+        at_most: { type: 'number' },\\n+      },\\n+      // Mutual exclusion is enforced at runtime; schema ensures they are numeric if present.\\n+    },\\n+    callsExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        step: { type: 'string' },\\n+        provider: { type: 'string' },\\n+        op: { type: 'string' },\\n+        args: { type: 'object' },\\n+        exactly: { type: 'number' },\\n+        at_least: { type: 'number' },\\n+        at_most: { type: 'number' },\\n+      },\\n+    },\\n+    promptsExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        step: { type: 'string' },\\n+        index: {\\n+          oneOf: [{ type: 'number' }, { enum: ['first', 'last'] }],\\n+        },\\n+        contains: {\\n+          type: 'array',\\n+          items: { type: 'string' },\\n+        },\\n+        not_contains: {\\n+          type: 'array',\\n+          items: { type: 'string' },\\n+        },\\n+        matches: { type: 'string' },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            contains: { type: 'array', items: { type: 'string' } },\\n+            not_contains: { type: 'array', items: { type: 'string' } },\\n+            matches: { type: 'string' },\\n+          },\\n+        },\\n+      },\\n+      required: ['step'],\\n+    },\\n+    outputsExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        step: { type: 'string' },\\n+        index: {\\n+          oneOf: [{ type: 'number' }, { enum: ['first', 'last'] }],\\n+        },\\n+        path: { type: 'string' },\\n+        equals: {},\\n+        equalsDeep: {},\\n+        matches: { type: 'string' },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            path: { type: 'string' },\\n+            equals: {},\\n+            matches: { type: 'string' },\\n+          },\\n+          required: ['path'],\\n+        },\\n+        contains_unordered: { type: 'array' },\\n+      },\\n+      required: ['step', 'path'],\\n+    },\\n+    expectBlock: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        use: { type: 'array', items: { type: 'string' } },\\n+        calls: { type: 'array', items: { $ref: '#/$defs/callsExpectation' } },\\n+        prompts: { type: 'array', items: { $ref: '#/$defs/promptsExpectation' } },\\n+        outputs: { type: 'array', items: { $ref: '#/$defs/outputsExpectation' } },\\n+        no_calls: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'object',\\n+            additionalProperties: false,\\n+            properties: {\\n+              step: { type: 'string' },\\n+              provider: { type: 'string' },\\n+              op: { type: 'string' },\\n+            },\\n+          },\\n+        },\\n+        fail: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: { message_contains: { type: 'string' } },\\n+        },\\n+        strict_violation: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: { for_step: { type: 'string' }, message_contains: { type: 'string' } },\\n+        },\\n+      },\\n+    },\\n+  },\\n+};\\n+\\n+const ajv = new Ajv({ allErrors: true, allowUnionTypes: true, strict: false });\\n+addFormats(ajv);\\n+const validate = ajv.compile(schema);\\n+\\n+function toYamlPath(instancePath: string): string {\\n+  if (!instancePath) return 'tests';\\n+  // Ajv instancePath starts with '/'\\n+  const parts = instancePath\\n+    .split('/')\\n+    .slice(1)\\n+    .map(p => (p.match(/^\\\\d+$/) ? `[${p}]` : `.${p}`));\\n+  let out = parts.join('');\\n+  if (out.startsWith('.')) out = out.slice(1);\\n+  // Heuristic: put root under tests for nicer messages\\n+  if (!out.startsWith('tests')) out = `tests.${out}`;\\n+  return out;\\n+}\\n+\\n+function levenshtein(a: string, b: string): number {\\n+  const m = a.length,\\n+    n = b.length;\\n+  const dp = Array.from({ length: m + 1 }, () => new Array(n + 1).fill(0));\\n+  for (let i = 0; i <= m; i++) dp[i][0] = i;\\n+  for (let j = 0; j <= n; j++) dp[0][j] = j;\\n+  for (let i = 1; i <= m; i++) {\\n+    for (let j = 1; j <= n; j++) {\\n+      const cost = a[i - 1] === b[j - 1] ? 0 : 1;\\n+      dp[i][j] = Math.min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost);\\n+    }\\n+  }\\n+  return dp[m][n];\\n+}\\n+\\n+const knownKeys = new Set([\\n+  // top-level\\n+  'version',\\n+  'extends',\\n+  'tests',\\n+  // tests\\n+  'tests.defaults',\\n+  'tests.fixtures',\\n+  'tests.cases',\\n+  // defaults\\n+  'tests.defaults.strict',\\n+  'tests.defaults.ai_provider',\\n+  'tests.defaults.github_recorder',\\n+  'tests.defaults.macros',\\n+  'tests.defaults.fail_on_unexpected_calls',\\n+  // case\\n+  'name',\\n+  'description',\\n+  'skip',\\n+  'strict',\\n+  'event',\\n+  'fixture',\\n+  'env',\\n+  'mocks',\\n+  'expect',\\n+  'flow',\\n+  // expect\\n+  'expect.use',\\n+  'expect.calls',\\n+  'expect.prompts',\\n+  'expect.outputs',\\n+  'expect.no_calls',\\n+  'expect.fail',\\n+  'expect.strict_violation',\\n+  // calls\\n+  'step',\\n+  'provider',\\n+  'op',\\n+  'exactly',\\n+  'at_least',\\n+  'at_most',\\n+  'args',\\n+  // prompts/outputs\\n+  'index',\\n+  'contains',\\n+  'not_contains',\\n+  'matches',\\n+  'path',\\n+  'equals',\\n+  'equalsDeep',\\n+  'where',\\n+  'contains_unordered',\\n+]);\\n+\\n+function hintForAdditionalProperty(err: ErrorObject): string | undefined {\\n+  if (err.keyword !== 'additionalProperties') return undefined;\\n+  const prop = (err.params as any)?.additionalProperty;\\n+  if (!prop || typeof prop !== 'string') return undefined;\\n+  // find nearest known key suffix match\\n+  let best: { key: string; dist: number } | null = null;\\n+  for (const k of knownKeys) {\\n+    const dist = levenshtein(prop, k.includes('.') ? k.split('.').pop()! : k);\\n+    if (dist <= 3 && (!best || dist < best.dist)) best = { key: k, dist };\\n+  }\\n+  if (best) return `Did you mean \\\"${best.key}\\\"?`;\\n+  return undefined;\\n+}\\n+\\n+function formatError(e: ErrorObject): string {\\n+  const path = toYamlPath(e.instancePath || '');\\n+  let msg = `${path}: ${e.message}`;\\n+  const hint = hintForAdditionalProperty(e);\\n+  if (hint) msg += ` (${hint})`;\\n+  if (e.keyword === 'enum' && Array.isArray((e.params as any)?.allowedValues)) {\\n+    msg += ` (allowed: ${(e.params as any).allowedValues.join(', ')})`;\\n+  }\\n+  return msg;\\n+}\\n+\\n+export type ValidationResult = { ok: true } | { ok: false; errors: string[] };\\n+\\n+export function validateTestsDoc(doc: unknown): ValidationResult {\\n+  try {\\n+    const ok = validate(doc);\\n+    if (ok) return { ok: true };\\n+    const errs = (validate.errors || []).map(formatError);\\n+    return { ok: false, errors: errs };\\n+  } catch (err) {\\n+    return { ok: false, errors: [err instanceof Error ? err.message : String(err)] };\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/utils/file-exclusion.ts\",\"additions\":1,\"deletions\":1,\"changes\":15,\"patch\":\"diff --git a/src/utils/file-exclusion.ts b/src/utils/file-exclusion.ts\\nindex 155bf20b..e7f5274e 100644\\n--- a/src/utils/file-exclusion.ts\\n+++ b/src/utils/file-exclusion.ts\\n@@ -128,12 +128,21 @@ export class FileExclusionHelper {\\n           .trim();\\n \\n         this.gitignore.add(gitignoreContent);\\n-        console.error('✅ Loaded .gitignore patterns for file filtering');\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.error('✅ Loaded .gitignore patterns for file filtering');\\n+        }\\n       } else if (additionalPatterns && additionalPatterns.length > 0) {\\n-        console.error('⚠️  No .gitignore found, using default exclusion patterns');\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.error('⚠️  No .gitignore found, using default exclusion patterns');\\n+        }\\n       }\\n     } catch (error) {\\n-      console.warn('⚠️ Failed to load .gitignore:', error instanceof Error ? error.message : error);\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        console.warn(\\n+          '⚠️ Failed to load .gitignore:',\\n+          error instanceof Error ? error.message : error\\n+        );\\n+      }\\n     }\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/integration/issue-double-content-detection.test.ts\",\"additions\":0,\"deletions\":5,\"changes\":131,\"patch\":\"diff --git a/tests/integration/issue-double-content-detection.test.ts b/tests/integration/issue-double-content-detection.test.ts\\ndeleted file mode 100644\\nindex 5ef9fb1b..00000000\\n--- a/tests/integration/issue-double-content-detection.test.ts\\n+++ /dev/null\\n@@ -1,131 +0,0 @@\\n-import * as fs from 'fs';\\n-import * as path from 'path';\\n-\\n-// Minimal Octokit REST mock to capture posted comment body\\n-const issuesCreateComment = jest.fn();\\n-const issuesListComments = jest.fn().mockResolvedValue({ data: [] });\\n-const pullsGet = jest.fn().mockResolvedValue({ data: { head: { sha: 'deadbeefcafebabe' } } });\\n-const pullsListFiles = jest.fn().mockResolvedValue({\\n-  data: [{ filename: 'x', status: 'modified', additions: 1, deletions: 0, changes: 1 }],\\n-});\\n-\\n-jest.mock('@octokit/rest', () => {\\n-  return {\\n-    Octokit: jest.fn().mockImplementation(() => ({\\n-      rest: {\\n-        issues: { createComment: issuesCreateComment, listComments: issuesListComments },\\n-        pulls: { get: pullsGet, listFiles: pullsListFiles },\\n-        checks: { create: jest.fn(), update: jest.fn() },\\n-      },\\n-    })),\\n-  };\\n-});\\n-\\n-describe('Issue assistant double-content detection (issues opened)', () => {\\n-  beforeEach(() => {\\n-    issuesCreateComment.mockReset();\\n-    issuesListComments.mockClear();\\n-    pullsGet.mockClear();\\n-    pullsListFiles.mockClear();\\n-\\n-    // Clean env used by action run()\\n-    delete process.env.GITHUB_EVENT_NAME;\\n-    delete process.env.GITHUB_EVENT_PATH;\\n-    delete process.env.GITHUB_REPOSITORY;\\n-    delete process.env.GITHUB_REPOSITORY_OWNER;\\n-    delete process.env['INPUT_GITHUB-TOKEN'];\\n-    delete process.env['INPUT_OWNER'];\\n-    delete process.env['INPUT_REPO'];\\n-    delete process.env['INPUT_CONFIG-PATH'];\\n-    delete process.env['INPUT_CREATE-CHECK'];\\n-    delete process.env['INPUT_COMMENT-ON-PR'];\\n-    delete process.env['INPUT_DEBUG'];\\n-  });\\n-\\n-  const writeTmp = (name: string, data: any) => {\\n-    const p = path.join(process.cwd(), name);\\n-    fs.writeFileSync(p, typeof data === 'string' ? data : JSON.stringify(data));\\n-    return p;\\n-  };\\n-\\n-  // This config intentionally produces two assistant-style outputs in the same run.\\n-  // With current behavior, both get concatenated into the single issue comment.\\n-  // We assert that only one assistant response appears (i.e., deduped/collapsed),\\n-  // so this test should fail until the posting logic is fixed.\\n-  const makeConfig = () => `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  assistant-initial:\\n-    type: ai\\n-    ai_provider: mock\\n-    schema: issue-assistant\\n-    group: dynamic\\n-    on: [issue_opened]\\n-    prompt: |\\n-      Return ONE JSON object for issue-assistant.\\n-      text: ### Assistant Reply\\n-      intent: issue_triage\\n-\\n-  assistant-refined:\\n-    type: ai\\n-    ai_provider: mock\\n-    schema: issue-assistant\\n-    group: dynamic\\n-    depends_on: [assistant-initial]\\n-    on: [issue_opened]\\n-    prompt: |\\n-      Return ONE JSON object for issue-assistant.\\n-      text: ### Assistant Reply\\n-      intent: issue_triage\\n-\\n-output:\\n-  pr_comment:\\n-    format: markdown\\n-    group_by: check\\n-`;\\n-\\n-  it('posts only the refined answer (no duplicate old+new content)', async () => {\\n-    const cfgPath = writeTmp('.tmp-double-content.yaml', makeConfig());\\n-    const event = {\\n-      action: 'opened',\\n-      issue: { number: 77, state: 'open' },\\n-      repository: { full_name: 'acme/widgets' },\\n-      sender: { login: 'reporter' },\\n-    } as any;\\n-    const eventPath = writeTmp('.tmp-issues-opened-double.json', event);\\n-\\n-    process.env.GITHUB_EVENT_NAME = 'issues';\\n-    process.env.GITHUB_EVENT_PATH = eventPath;\\n-    process.env.GITHUB_REPOSITORY = 'acme/widgets';\\n-    process.env.GITHUB_REPOSITORY_OWNER = 'acme';\\n-    process.env['INPUT_GITHUB-TOKEN'] = 'test-token';\\n-    process.env['INPUT_OWNER'] = 'acme';\\n-    process.env['INPUT_REPO'] = 'widgets';\\n-    process.env['INPUT_CONFIG-PATH'] = cfgPath;\\n-    process.env['INPUT_CREATE-CHECK'] = 'false';\\n-    process.env['INPUT_COMMENT-ON-PR'] = 'false';\\n-    process.env['INPUT_DEBUG'] = 'true';\\n-\\n-    jest.resetModules();\\n-    const { run } = await import('../../src/index');\\n-    await run();\\n-\\n-    // Exactly one comment is posted\\n-    expect(issuesCreateComment).toHaveBeenCalledTimes(1);\\n-    const call = issuesCreateComment.mock.calls[0][0];\\n-    const body: string = call.body;\\n-    // Debug: persist body for local inspection\\n-    fs.mkdirSync('tmp', { recursive: true });\\n-    fs.writeFileSync('tmp/issue-double-content-body.md', body, 'utf8');\\n-\\n-    // Desired behavior: Only a single assistant response should appear.\\n-    // Current bug: both initial and refined outputs are concatenated. In the\\n-    // mock path the provider sometimes returns a minimal JSON like {\\\"issues\\\":[]}.\\n-    // Assert that only one such block exists.\\n-    const jsonBlockCount = (body.match(/\\\\{\\\\s*\\\\\\\"issues\\\\\\\"\\\\s*:\\\\s*\\\\[\\\\]\\\\s*\\\\}/g) || []).length;\\n-    expect(jsonBlockCount).toBe(1);\\n-\\n-    fs.unlinkSync(cfgPath);\\n-    fs.unlinkSync(eventPath);\\n-  });\\n-});\\n\",\"status\":\"removed\"},{\"filename\":\"tests/integration/issue-posting-fact-gate.test.ts\",\"additions\":0,\"deletions\":7,\"changes\":197,\"patch\":\"diff --git a/tests/integration/issue-posting-fact-gate.test.ts b/tests/integration/issue-posting-fact-gate.test.ts\\ndeleted file mode 100644\\nindex 4ac86358..00000000\\n--- a/tests/integration/issue-posting-fact-gate.test.ts\\n+++ /dev/null\\n@@ -1,197 +0,0 @@\\n-import * as fs from 'fs';\\n-import * as path from 'path';\\n-\\n-// Reuse the Octokit REST mock pattern from other integration tests\\n-const checksCreate = jest.fn();\\n-const checksUpdate = jest.fn();\\n-const issuesCreateComment = jest.fn();\\n-const issuesListComments = jest.fn().mockResolvedValue({ data: [] });\\n-const pullsGet = jest.fn().mockResolvedValue({ data: { head: { sha: 'deadbeefcafebabe' } } });\\n-const pullsListFiles = jest.fn().mockResolvedValue({\\n-  data: [{ filename: 'x', status: 'modified', additions: 1, deletions: 0, changes: 1 }],\\n-});\\n-\\n-jest.mock('@octokit/rest', () => {\\n-  return {\\n-    Octokit: jest.fn().mockImplementation(() => ({\\n-      rest: {\\n-        checks: { create: checksCreate, update: checksUpdate },\\n-        issues: { createComment: issuesCreateComment, listComments: issuesListComments },\\n-        pulls: { get: pullsGet, listFiles: pullsListFiles },\\n-      },\\n-    })),\\n-  };\\n-});\\n-\\n-describe('Issue assistant posting is gated by fact validation (issue_opened)', () => {\\n-  beforeEach(() => {\\n-    checksCreate.mockReset();\\n-    checksUpdate.mockReset();\\n-    issuesCreateComment.mockReset();\\n-    issuesListComments.mockClear();\\n-    pullsGet.mockClear();\\n-    pullsListFiles.mockClear();\\n-    // Clean env that run() reads\\n-    delete process.env.GITHUB_EVENT_NAME;\\n-    delete process.env.GITHUB_EVENT_PATH;\\n-    delete process.env.GITHUB_REPOSITORY;\\n-    delete process.env.GITHUB_REPOSITORY_OWNER;\\n-    delete process.env['INPUT_GITHUB-TOKEN'];\\n-    delete process.env['INPUT_OWNER'];\\n-    delete process.env['INPUT_REPO'];\\n-    delete process.env['INPUT_CONFIG-PATH'];\\n-    delete process.env['INPUT_CREATE-CHECK'];\\n-    delete process.env['INPUT_COMMENT-ON-PR'];\\n-    delete process.env['INPUT_DEBUG'];\\n-  });\\n-\\n-  const makeConfig = (allValid: boolean) => `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  # Minimal issue assistant using mock provider\\n-  issue-assistant:\\n-    type: ai\\n-    ai_provider: mock\\n-    schema: issue-assistant\\n-    on: [issue_opened]\\n-    prompt: |\\n-      Return ONE JSON object for issue-assistant.\\n-      text: Hello, world.\\n-\\n-References:\\n-\\n-\\\\`\\\\`\\\\`refs\\n-none\\n-\\\\`\\\\`\\\\`\\n-      intent: issue_triage\\n-    on_success:\\n-      run: [init-fact-validation]\\n-\\n-  # Initialize validation state\\n-  init-fact-validation:\\n-    type: memory\\n-    operation: set\\n-    namespace: fact-validation\\n-    key: attempt\\n-    value: 0\\n-    on: [issue_opened]\\n-\\n-  # Seed deterministic facts instead of invoking AI\\n-  seed-facts:\\n-    type: memory\\n-    operation: set\\n-    namespace: fact-validation\\n-    key: fact_list\\n-    value: [{\\\"id\\\":\\\"f1\\\",\\\"category\\\":\\\"Configuration\\\",\\\"claim\\\":\\\"X\\\",\\\"verifiable\\\":true}]\\n-    depends_on: [issue-assistant]\\n-    on: [issue_opened]\\n-\\n-  # forEach extraction proxy\\n-  extract-facts:\\n-    type: memory\\n-    operation: get\\n-    namespace: fact-validation\\n-    key: fact_list\\n-    forEach: true\\n-    depends_on: [seed-facts]\\n-    on: [issue_opened]\\n-    on_finish:\\n-      run: [aggregate-validations]\\n-      goto_js: |\\n-        const ns = 'fact-validation';\\n-        const allValid = memory.get('all_valid', ns) === true;\\n-        const limit = 1; // one retry\\n-        const attempt = Number(memory.get('attempt', ns) || 0);\\n-        if (!allValid && attempt < limit) {\\n-          memory.increment('attempt', 1, ns);\\n-          return 'issue-assistant';\\n-        }\\n-        return null;\\n-\\n-  validate-fact:\\n-    type: memory\\n-    operation: exec_js\\n-    namespace: fact-validation\\n-    depends_on: [extract-facts]\\n-    on: [issue_opened]\\n-    memory_js: |\\n-      const NS = 'fact-validation';\\n-      const f = outputs['extract-facts'];\\n-      const attempt = Number(memory.get('attempt', NS) || 0);\\n-      const is_valid = ${allValid ? 'true' : 'false'}; return { fact_id: f.id, claim: f.claim, is_valid, confidence: '${allValid ? \\\"'ok'\\\" : \\\"'bad'\\\"} };\\n-\\n-  aggregate-validations:\\n-    type: memory\\n-    operation: exec_js\\n-    namespace: fact-validation\\n-    on: [issue_opened]\\n-    memory_js: |\\n-      const vals = outputs.history['validate-fact'] || [];\\n-      const invalid = (Array.isArray(vals) ? vals : []).filter(v => v && v.is_valid === false);\\n-      const all_valid = invalid.length === 0;\\n-      memory.set('all_valid', all_valid, 'fact-validation');\\n-      return { total: vals.length, all_valid };\\n-\\n-  # Emit a simple final note when valid so the Action has content to post once\\n-  final-note:\\n-    type: log\\n-    depends_on: [aggregate-validations]\\n-    if: \\\"memory.get('all_valid','fact-validation') === true\\\"\\n-    message: 'Verified: final'\\n-\\n-  # No explicit post step; use Action's generic end-of-run post\\n-\\n-output:\\n-  pr_comment:\\n-    format: markdown\\n-    group_by: check\\n-`;\\n-\\n-  const writeTmp = (name: string, data: any) => {\\n-    const p = path.join(process.cwd(), name);\\n-    fs.writeFileSync(p, typeof data === 'string' ? data : JSON.stringify(data));\\n-    return p;\\n-  };\\n-\\n-  const setupAndRun = async (allValid: boolean) => {\\n-    const cfgPath = writeTmp(\\n-      `.tmp-issue-gate-${allValid ? 'ok' : 'fail'}.yaml`,\\n-      makeConfig(allValid)\\n-    );\\n-    const event = {\\n-      action: 'opened',\\n-      issue: { number: 42, state: 'open' },\\n-      repository: { full_name: 'acme/widgets' },\\n-      sender: { login: 'reporter' },\\n-    } as any;\\n-    const eventPath = writeTmp('.tmp-issues-opened.json', event);\\n-\\n-    process.env.GITHUB_EVENT_NAME = 'issues';\\n-    process.env.GITHUB_EVENT_PATH = eventPath;\\n-    process.env.GITHUB_REPOSITORY = 'acme/widgets';\\n-    process.env.GITHUB_REPOSITORY_OWNER = 'acme';\\n-    process.env['INPUT_GITHUB-TOKEN'] = 'test-token';\\n-    process.env['INPUT_OWNER'] = 'acme';\\n-    process.env['INPUT_REPO'] = 'widgets';\\n-    process.env['INPUT_CONFIG-PATH'] = cfgPath;\\n-    process.env['INPUT_CREATE-CHECK'] = 'false';\\n-    process.env['INPUT_COMMENT-ON-PR'] = 'false';\\n-    process.env['INPUT_DEBUG'] = 'true';\\n-    process.env['ENABLE_FACT_VALIDATION'] = 'true';\\n-\\n-    // Import run() fresh to pick up env\\n-    jest.resetModules();\\n-    const { run } = await import('../../src/index');\\n-    await run();\\n-\\n-    fs.unlinkSync(cfgPath);\\n-    fs.unlinkSync(eventPath);\\n-  };\\n-\\n-  it('loops once to correct facts and posts a single final comment', async () => {\\n-    await setupAndRun(false);\\n-    // With attempt limit=1, the first validation fails, we route back to assistant,\\n-    // second pass should be valid and then post once at end.\\n-    expect(issuesCreateComment).toHaveBeenCalledTimes(1);\\n-  });\\n-});\\n\",\"status\":\"removed\"}],\"outputs\":{\"overview\":{\"text\":\"This PR introduces a comprehensive, configuration-driven integration test framework for Visor. It allows developers to write tests for their `.visor.yaml` configurations by simulating GitHub events, mocking providers (like AI and GitHub API calls), and asserting on the resulting actions. This is a significant feature that replaces previous ad-hoc testing methods with a structured and maintainable approach.\\n\\n### Files Changed Analysis\\n\\nThe changes introduce a new test runner, along with its supporting components, documentation, and default test suite.\\n\\n-   **New Feature Implementation (`src/test-runner/`)**: The core logic is encapsulated in the new `src/test-runner` directory, which includes the main `index.ts` (the runner itself), `fixture-loader.ts` for managing test data, `recorders/` for mocking GitHub and AI interactions, and `validator.ts` for handling assertions.\\n-   **CLI Integration (`src/cli-main.ts`)**: A new `test` subcommand is added to the Visor CLI to execute the test runner.\\n-   **Execution Engine Modifications (`src/check-execution-engine.ts`)**: The engine is updated to support test mode, primarily by allowing the injection of mock providers and recorders.\\n-   **New Test Suite (`defaults/.visor.tests.yaml`)**: A comprehensive test suite for the default `.visor.yaml` configuration is added, serving as a practical example of the new framework.\\n-   **Documentation (`docs/testing/`)**: Extensive documentation is added, covering getting started, CLI usage, assertions, and fixtures/mocks.\\n-   **CI Integration (`.github/workflows/ci.yml`)**: The CI pipeline is updated to run the new integration tests, ensuring configurations are validated on each pull request.\\n-   **Test Removal (`tests/integration/`)**: Old, script-based integration tests are removed in favor of the new, more robust framework.\\n\\n### Architecture & Impact Assessment\\n\\n#### What this PR accomplishes\\n\\nThis PR delivers a complete integration test framework for Visor configurations. It enables developers to validate their automation rules in a predictable, isolated environment without making live network calls. This improves reliability, simplifies debugging, and provides a safety net for configuration changes.\\n\\n#### Key technical changes introduced\\n\\n1.  **Test Runner CLI**: A `visor test` command is introduced to discover and run tests defined in a `.visor.tests.yaml` file.\\n2.  **Fixture-Based Testing**: Tests are driven by predefined \\\"fixtures\\\" that simulate GitHub webhook events (e.g., `gh.pr_open.minimal`).\\n3.  **Mocking and Recording**: The framework intercepts calls to external providers. GitHub API calls are recorded for assertion, and AI provider calls are mocked to return predefined responses. This is handled by a `RecordingOctokit` wrapper and mock AI providers that are activated when `ai.provider` is set to `mock`.\\n4.  **Declarative Assertions**: Tests use a YAML `expect:` block to assert on outcomes, such as the number of calls to a provider (`calls`), the content of AI prompts (`prompts`), or the final status of checks.\\n\\n#### Affected system components\\n\\n-   **CLI (`src/cli-main.ts`)**: Extended with a new `test` command.\\n-   **Core Logic (`src/check-execution-engine.ts`)**: Modified to operate in a \\\"test mode\\\" with mocked dependencies.\\n-   **Providers (`src/providers/*`)**: The `GithubOpsProvider` is adapted to use a recordable Octokit instance during tests. The `AiCheckProvider` is modified to handle a `mock` provider type.\\n-   **CI/CD (`.github/workflows/ci.yml`)**: The CI workflow is updated to execute the new test suite.\\n\\n#### Component Interaction Diagram\\n\\n```mermaid\\ngraph TD\\n    subgraph Test Execution\\n        A[visor test CLI] --> B{Test Runner};\\n        B --> C[Load .visor.tests.yaml];\\n        C --> D{For each test case};\\n        D --> E[Load Fixture & Mocks];\\n    end\\n\\n    subgraph Visor Core\\n        F(CheckExecutionEngine);\\n        G[Providers (GitHub, AI, etc.)];\\n    end\\n\\n    subgraph Mocks & Recorders\\n        H[RecordingOctokit];\\n        I[MockAiProvider];\\n    end\\n\\n    E --> |injects mocks| F;\\n    F --> |uses| G;\\n    G -- during test --> H;\\n    G -- during test --> I;\\n\\n    D --> |runs| F;\\n    F --> J[Collect Results & Recorded Calls];\\n    J --> K[Validate Assertions];\\n    K --> L[Report Pass/Fail];\\n```\\n\\n### Scope Discovery & Context Expansion\\n\\nThis feature fundamentally changes how Visor configurations are developed and maintained. By providing a robust testing framework, it encourages a test-driven development (TDD) approach for writing automation rules.\\n\\n-   **Impact on Configuration Development**: Users will now be expected to write tests for their custom checks and workflows. The `defaults/.visor.tests.yaml` file serves as a blueprint for this.\\n-   **Reliability and Maintenance**: The ability to test configurations offline significantly reduces the risk of introducing regressions. It makes troubleshooting easier, as failures can be reproduced locally and deterministically.\\n-   **Provider Ecosystem**: The mocking architecture is extensible. While this PR focuses on GitHub and AI providers, the same pattern could be applied to any future provider (e.g., Slack, Jira), ensuring that all integrations can be tested.\\n-   **Developer Experience**: The framework is designed with developer experience in mind, offering clear output, helpful error messages, and a straightforward YAML-based syntax, lowering the barrier to writing effective tests.\",\"tags\":{\"review-effort\":5,\"label\":\"feature\"}}}}"},"events":[]}
{"name":"visor.provider","attributes":{"visor.check.id":"quality","visor.provider.type":"ai"}}
{"name":"visor.check","attributes":{"visor.check.id":"quality","visor.check.input.context":"{\"pr\":{\"number\":0,\"title\":\"Local Analysis: feat/test-framework-runner (14 modified)\",\"author\":\"Leonid Bugaev\",\"branch\":\"feat/test-framework-runner\",\"base\":\"main\"},\"files\":[{\"filename\":\".github/workflows/ci.yml\",\"additions\":1,\"deletions\":0,\"changes\":15,\"patch\":\"diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml\\nindex 840f2abc..13bd6f9d 100644\\n--- a/.github/workflows/ci.yml\\n+++ b/.github/workflows/ci.yml\\n@@ -64,6 +64,21 @@ jobs:\\n           ls -la *.tgz\\n           echo \\\"✅ Package can be created successfully\\\"\\n \\n+      - name: Run integration tests (defaults suite)\\n+        run: |\\n+          mkdir -p output\\n+          node ./dist/index.js test --config defaults/.visor.tests.yaml --json output/visor-tests.json --report junit:output/visor-tests.xml --summary md:output/visor-tests.md\\n+\\n+      - name: Upload integration test artifacts\\n+        if: always()\\n+        uses: actions/upload-artifact@v4\\n+        with:\\n+          name: visor-test-results\\n+          path: |\\n+            output/visor-tests.json\\n+            output/visor-tests.xml\\n+            output/visor-tests.md\\n+\\n       - name: Test basic action functionality\\n         uses: ./\\n         with:\\n\",\"status\":\"added\"},{\"filename\":\"README.md\",\"additions\":1,\"deletions\":0,\"changes\":10,\"patch\":\"diff --git a/README.md b/README.md\\nindex b2bf4db5..7acc4843 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -729,3 +729,13 @@ steps:\\n ```\\n \\n See docs: docs/github-ops.md\\n+## Integration Tests (Great DX)\\n+\\n+Visor ships a YAML‑native integration test runner so you can describe user flows, mocks, and assertions alongside your config.\\n+\\n+- Start here: docs/testing/getting-started.md\\n+- CLI details: docs/testing/cli.md\\n+- Fixtures and mocks: docs/testing/fixtures-and-mocks.md\\n+- Assertions reference: docs/testing/assertions.md\\n+\\n+Example suite: defaults/.visor.tests.yaml\\n\",\"status\":\"added\"},{\"filename\":\"defaults/.visor.tests.yaml\",\"additions\":20,\"deletions\":0,\"changes\":557,\"patch\":\"diff --git a/defaults/.visor.tests.yaml b/defaults/.visor.tests.yaml\\nnew file mode 100644\\nindex 00000000..c496cdee\\n--- /dev/null\\n+++ b/defaults/.visor.tests.yaml\\n@@ -0,0 +1,557 @@\\n+version: \\\"1.0\\\"\\n+extends: \\\".visor.yaml\\\"\\n+\\n+# Integration test suite for Visor default configuration\\n+# - Driven by events + fixtures; no manual step lists\\n+# - Strict by default: every executed step must have an expect\\n+# - AI mocks accept structured JSON when a schema is defined; plain uses text\\n+# - GitHub calls are recorded by default by the test runner (no network)\\n+\\n+tests:\\n+  defaults:\\n+    strict: true\\n+    ai_provider: mock\\n+    fail_on_unexpected_calls: true\\n+    # Example: enable negative GitHub recorder for all tests\\n+    # github_recorder: { error_code: 429 }\\n+  # Built-in fixtures are provided by the test runner (gh.* namespace).\\n+  # Custom fixtures may still be added here if needed.\\n+  fixtures: []\\n+\\n+  cases:\\n+    - name: label-flow\\n+      description: |\\n+        Validates the happy path for PR open:\\n+        - overview runs and emits tags.label and tags.review-effort (mocked)\\n+        - apply-overview-labels adds two labels (feature and review/effort:2)\\n+        - overview prompt includes PR title and unified diff header\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: |\\n+            High‑level summary of the changes and impact.\\n+          tags:\\n+            label: feature\\n+            review-effort: 2\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args:\\n+              contains:\\n+                - feature\\n+                - \\\"review/effort:2\\\"\\n+        outputs:\\n+          - step: overview\\n+            path: \\\"tags.label\\\"\\n+            equals: feature\\n+          - step: overview\\n+            path: \\\"tags['review-effort']\\\"\\n+            equals: 2\\n+        prompts:\\n+          - step: overview\\n+            contains:\\n+              - \\\"feat: add user search\\\"\\n+              - \\\"diff --git a/src/search.ts\\\"\\n+\\n+    - name: issue-triage\\n+      skip: true\\n+      description: |\\n+        Ensures the issue assistant triages a newly opened issue and applies labels.\\n+        Asserts the structured output (intent=issue_triage) and the GitHub label op.\\n+      event: issue_opened\\n+      fixture: gh.issue_open.minimal\\n+      mocks:\\n+        issue-assistant:\\n+          text: |\\n+            Thanks for the detailed report! We will investigate.\\n+          intent: issue_triage\\n+          labels: [bug, priority/medium]\\n+      expect:\\n+        calls:\\n+          - step: issue-assistant\\n+            exactly: 1\\n+          - step: apply-issue-labels\\n+            exactly: 1\\n+          - step: init-fact-validation\\n+            exactly: 1\\n+          - step: extract-facts\\n+            exactly: 1\\n+          - step: validate-fact\\n+            at_least: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args:\\n+              contains:\\n+                - bug\\n+        outputs:\\n+          - step: issue-assistant\\n+            path: intent\\n+            equals: issue_triage\\n+        prompts:\\n+          - step: issue-assistant\\n+            contains:\\n+              - \\\"Bug: crashes on search edge case\\\"\\n+\\n+    - name: pr-review-e2e-flow\\n+      description: |\\n+        End-to-end PR lifecycle covering multiple external events:\\n+        1) PR opened → overview + labels\\n+        2) Standard comment → no bot reply\\n+        3) /visor help → single assistant reply (no retrigger)\\n+        4) /visor Regenerate reviews → retrigger overview\\n+        5) Fact validation enabled on comment → extract/validate/aggregate\\n+        6) Fact validation disabled on comment → only assistant, no validation steps\\n+        7) PR synchronized (new commit) → overview runs again\\n+      strict: true\\n+      flow:\\n+        - name: pr-open\\n+          description: |\\n+            PR open event. Mocks overview/security/quality/performance as empty issue lists.\\n+            Expects all review steps to run and labels to be added.\\n+          event: pr_opened\\n+          fixture: gh.pr_open.minimal\\n+          mocks:\\n+            overview:\\n+              text: \\\"Overview body\\\"\\n+              tags: { label: feature, review-effort: 2 }\\n+            security: { issues: [] }\\n+            architecture: { issues: [] }\\n+            quality: { issues: [] }\\n+            performance: { issues: [] }\\n+          expect:\\n+            calls:\\n+              - step: overview\\n+                exactly: 1\\n+              - step: security\\n+                exactly: 1\\n+              - step: architecture\\n+                exactly: 1\\n+              - step: performance\\n+                exactly: 1\\n+              - step: quality\\n+                exactly: 1\\n+              - step: apply-overview-labels\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.createComment\\n+                at_least: 1\\n+              - provider: github\\n+                op: labels.add\\n+                at_least: 1\\n+                args:\\n+                  contains: [feature]\\n+            prompts:\\n+              - step: overview\\n+                contains:\\n+                  - \\\"feat: add user search\\\"\\n+\\n+        - name: standard-comment\\n+          description: |\\n+            A regular human comment on a PR should not produce a bot reply.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.standard\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"\\\"   # empty text to avoid posting a reply\\n+              intent: comment_reply\\n+          expect:\\n+            no_calls:\\n+              - provider: github\\n+                op: issues.createComment\\n+              - step: init-fact-validation\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+\\n+        - name: visor-plain\\n+          description: |\\n+            A \\\"/visor help\\\" comment should be recognized and answered once by the assistant.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"Sure, here’s how I can help.\\\"\\n+              intent: comment_reply\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.createComment\\n+                exactly: 1\\n+            outputs:\\n+              - step: comment-assistant\\n+                path: intent\\n+                equals: comment_reply\\n+            prompts:\\n+              - step: comment-assistant\\n+                matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"\\n+\\n+        - name: visor-retrigger\\n+          description: |\\n+            A \\\"/visor Regenerate reviews\\\" comment should set intent=comment_retrigger\\n+            and schedule a new overview.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_regenerate\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"Regenerating.\\\"\\n+              intent: comment_retrigger\\n+            overview:\\n+              text: \\\"Overview (regenerated)\\\"\\n+              tags: { label: feature, review-effort: 2 }\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+              - step: overview\\n+                exactly: 1\\n+              - step: security\\n+                exactly: 1\\n+              - step: architecture\\n+                exactly: 1\\n+              - step: performance\\n+                exactly: 1\\n+              - step: quality\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.createComment\\n+                at_least: 1\\n+            outputs:\\n+              - step: comment-assistant\\n+                path: intent\\n+                equals: comment_retrigger\\n+            prompts:\\n+              - step: comment-assistant\\n+                contains: [\\\"Regenerate reviews\\\"]\\n+\\n+        - name: facts-enabled\\n+          description: |\\n+            With fact validation enabled, the assistant reply is followed by\\n+            extract-facts, validate-fact (per fact), and aggregate-validations.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"We rely on defaults/.visor.yaml line 11 for max_parallelism=4.\\\"\\n+              intent: comment_reply\\n+            extract-facts:\\n+              - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+            validate-fact[]:\\n+              - fact_id: f1\\n+                claim: \\\"max_parallelism defaults to 4\\\"\\n+                is_valid: true\\n+                confidence: high\\n+                evidence: \\\"defaults/.visor.yaml:11\\\"\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+              - step: extract-facts\\n+                exactly: 1\\n+              - step: validate-fact\\n+                at_least: 1\\n+              - step: aggregate-validations\\n+                exactly: 1\\n+              - step: init-fact-validation\\n+                exactly: 1\\n+          # Prompt assertions are validated separately in stage-level prompt tests\\n+\\n+        - name: facts-invalid\\n+          description: |\\n+            Invalid fact path: after assistant reply, extract-facts finds one claim and\\n+            validate-fact returns is_valid=false; aggregate-validations detects not-all-valid\\n+            and reruns the assistant once with correction context.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"We rely on defaults/.visor.yaml line 11 for max_parallelism=4.\\\"\\n+              intent: comment_reply\\n+            extract-facts:\\n+              - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+            validate-fact[]:\\n+              - fact_id: f1\\n+                claim: \\\"max_parallelism defaults to 4\\\"\\n+                is_valid: false\\n+                confidence: high\\n+                evidence: \\\"defaults/.visor.yaml:11 does not set 4\\\"\\n+                correction: \\\"max_parallelism defaults to 3\\\"\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 2\\n+              - step: extract-facts\\n+                exactly: 1\\n+              - step: validate-fact\\n+                at_least: 1\\n+              - step: aggregate-validations\\n+                exactly: 1\\n+              - step: init-fact-validation\\n+                exactly: 1\\n+            prompts:\\n+              - step: comment-assistant\\n+                index: last\\n+                contains:\\n+                  - \\\"<previous_response>\\\"\\n+                  - \\\"max_parallelism defaults to 4\\\"\\n+                  - \\\"Correction: max_parallelism defaults to 3\\\"\\n+\\n+        - name: facts-two-items\\n+          description: |\\n+            Two facts extracted; only the invalid fact should appear in the correction pass.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.visor_help\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+          mocks:\\n+            comment-assistant:\\n+              text: \\\"We rely on defaults/.visor.yaml for concurrency defaults.\\\"\\n+              intent: comment_reply\\n+            extract-facts:\\n+              - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+              - { id: f2, category: Feature,       claim: \\\"Fast mode is enabled by default\\\", verifiable: true }\\n+            validate-fact[]:\\n+              - { fact_id: f1, claim: \\\"max_parallelism defaults to 4\\\", is_valid: false, confidence: high, evidence: \\\"defaults/.visor.yaml:11\\\", correction: \\\"max_parallelism defaults to 3\\\" }\\n+              - { fact_id: f2, claim: \\\"Fast mode is enabled by default\\\", is_valid: true, confidence: high, evidence: \\\"src/config.ts:FAST_MODE=true\\\" }\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 2\\n+              - step: extract-facts\\n+                exactly: 1\\n+              - step: validate-fact\\n+                exactly: 2\\n+              - step: aggregate-validations\\n+                exactly: 1\\n+              - step: init-fact-validation\\n+                exactly: 1\\n+            outputs:\\n+              - step: validate-fact\\n+                where: { path: fact_id, equals: f1 }\\n+                path: is_valid\\n+                equals: false\\n+              - step: validate-fact\\n+                where: { path: fact_id, equals: f2 }\\n+                path: is_valid\\n+                equals: true\\n+            prompts:\\n+              - step: comment-assistant\\n+                index: last\\n+                contains:\\n+                  - \\\"<previous_response>\\\"\\n+                  - \\\"max_parallelism defaults to 4\\\"\\n+                  - \\\"Correction: max_parallelism defaults to 3\\\"\\n+                not_contains:\\n+                  - \\\"Fast mode is enabled by default\\\"\\n+\\n+        - name: facts-disabled\\n+          description: |\\n+            With fact validation disabled, only the assistant runs; no validation steps execute.\\n+          event: issue_comment\\n+          fixture: gh.issue_comment.standard\\n+          env:\\n+            ENABLE_FACT_VALIDATION: \\\"false\\\"\\n+          expect:\\n+            calls:\\n+              - step: comment-assistant\\n+                exactly: 1\\n+            no_calls:\\n+              - step: init-fact-validation\\n+              - step: extract-facts\\n+              - step: validate-fact\\n+              - step: aggregate-validations\\n+\\n+        - name: pr-updated\\n+          description: |\\n+            When a new commit is pushed (synchronize), overview should run again\\n+            and post/refresh a comment.\\n+          event: pr_updated\\n+          fixture: gh.pr_sync.minimal\\n+          mocks:\\n+            overview:\\n+              text: \\\"Overview for new commit\\\"\\n+              tags: { label: feature, review-effort: 3 }\\n+          expect:\\n+            calls:\\n+              - step: overview\\n+                exactly: 1\\n+              - provider: github\\n+                op: issues.updateComment\\n+                at_least: 1\\n+\\n+    - name: security-fail-if\\n+      description: |\\n+        Verifies that the global fail_if trips when security produces an error‑severity issue.\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview text\\\"\\n+          tags:\\n+            label: bug\\n+            review-effort: 3\\n+        security:\\n+          issues:\\n+            - id: S-001\\n+              file: src/search.ts\\n+              line: 10\\n+              message: \\\"Command injection risk\\\"\\n+              severity: error\\n+              category: security\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: security\\n+            exactly: 1\\n+        outputs:\\n+          - step: security\\n+            path: \\\"issues[0].severity\\\"\\n+            equals: error\\n+        fail:\\n+          message_contains: \\\"fail_if\\\"\\n+\\n+    - name: strict-mode-example\\n+      skip: true\\n+      description: |\\n+        Demonstrates strict mode: a step executed without a corresponding expect\\n+        (apply-overview-labels) triggers a strict_violation with a helpful message.\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: \\\"Short overview\\\"\\n+          tags:\\n+            label: chore\\n+            review-effort: 1\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+        strict_violation:\\n+          for_step: apply-overview-labels\\n+          message_contains: \\\"Add an expect for this step or set strict: false\\\"\\n+\\n+    - name: visor-plain-prompt\\n+      description: |\\n+        Standalone prompt check for a \\\"/visor help\\\" comment.\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_help\\n+      mocks:\\n+        comment-assistant:\\n+          text: \\\"Here is how I can help.\\\"\\n+          intent: comment_reply\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+        prompts:\\n+          - step: comment-assistant\\n+            matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"\\n+\\n+    - name: visor-retrigger-prompt\\n+      description: |\\n+        Standalone prompt check for \\\"/visor Regenerate reviews\\\".\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_regenerate\\n+      strict: false\\n+      mocks:\\n+        comment-assistant:\\n+          text: \\\"Regenerating.\\\"\\n+          intent: comment_retrigger\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+        prompts:\\n+          - step: comment-assistant\\n+            contains: [\\\"Regenerate reviews\\\"]\\n+\\n+    - name: command-mock-shape\\n+      description: |\\n+        Illustrates command provider mocking and output assertions.\\n+        Skipped by default; enable when command steps exist.\\n+      skip: true  # illustrative only, enable when a command step exists\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        unit-tests:\\n+          stdout: '{\\\"passed\\\": 128, \\\"failed\\\": 0, \\\"duration_sec\\\": 1.2}'\\n+          exit_code: 0\\n+      expect:\\n+        calls:\\n+          - step: unit-tests\\n+            exactly: 1\\n+        outputs:\\n+          - step: unit-tests\\n+            path: failed\\n+            equals: 0\\n+\\n+    - name: github-negative-mode\\n+      description: |\\n+        Demonstrates negative GitHub recorder mode: simulate a 429 error and assert failure path.\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      github_recorder: { error_code: 429 }\\n+      # Override defaults for this case only by specifying a local recorder via env-like knob\\n+      # The runner reads tests.defaults.github_recorder; we provide it at the suite level by default.\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview body\\\"\\n+          tags: { label: feature, review-effort: 2 }\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+        fail:\\n+          message_contains: \\\"github/op_failed\\\"\\n+\\n+    - name: facts-invalid\\n+      skip: true\\n+      description: |\\n+        With fact validation enabled and an invalid fact, aggregate-validations should detect\\n+        not-all-valid and route back to the assistant for a correction pass in the same stage.\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_help\\n+      env:\\n+        ENABLE_FACT_VALIDATION: \\\"true\\\"\\n+      mocks:\\n+        comment-assistant:\\n+          text: \\\"We rely on defaults/.visor.yaml line 11 for max_parallelism=4.\\\"\\n+          intent: comment_reply\\n+        extract-facts:\\n+          - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+        validate-fact[]:\\n+          - fact_id: f1\\n+            claim: \\\"max_parallelism defaults to 4\\\"\\n+            is_valid: false\\n+            confidence: high\\n+            evidence: \\\"defaults/.visor.yaml:11 does not set 4\\\"\\n+            correction: \\\"max_parallelism defaults to 3\\\"\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+          - step: extract-facts\\n+            exactly: 1\\n+          - step: validate-fact\\n+            at_least: 1\\n+          - step: aggregate-validations\\n+            exactly: 1\\n+          - step: init-fact-validation\\n+            exactly: 1\\n\",\"status\":\"added\"},{\"filename\":\"defaults/.visor.yaml\",\"additions\":1,\"deletions\":1,\"changes\":25,\"patch\":\"diff --git a/defaults/.visor.yaml b/defaults/.visor.yaml\\nindex 0e018884..21fad9eb 100644\\n--- a/defaults/.visor.yaml\\n+++ b/defaults/.visor.yaml\\n@@ -452,8 +452,9 @@ steps:\\n         - Provide partial answers when possible, and indicate what additional information would help give a complete response\\n     on: [issue_comment]\\n     on_success:\\n-      # Always initialize fact validation attempt counter\\n-      run: [init-fact-validation]\\n+      # Initialize fact validation attempt counter only when validation is enabled\\n+      run_js: |\\n+        return env.ENABLE_FACT_VALIDATION === 'true' ? ['init-fact-validation'] : []\\n       # Preserve intent-based rerun: allow members to retrigger overview from a comment\\n       goto_js: |\\n         const intent = (typeof output === 'object' && output) ? output.intent : undefined;\\n@@ -619,8 +620,14 @@ steps:\\n     # After all facts are validated, aggregate results and decide next action\\n     on_finish:\\n       run: [aggregate-validations]\\n+      # If aggregation stored validation issues in memory, schedule a correction reply\\n+      run_js: |\\n+        const issues = memory.list('fact-validation').includes('fact_validation_issues')\\n+          ? memory.get('fact_validation_issues', 'fact-validation')\\n+          : [];\\n+        return Array.isArray(issues) && issues.length > 0 ? ['comment-assistant'] : [];\\n       goto_js: |\\n-        const allValid = memory.get('all_valid', 'fact-validation');\\n+        const allValid = memory.get('all_facts_valid', 'fact-validation');\\n         const attempt = memory.get('fact_validation_attempt', 'fact-validation') || 0;\\n \\n         log('🔍 Fact validation complete - allValid:', allValid, 'attempt:', attempt);\\n@@ -702,11 +709,10 @@ steps:\\n     on: [issue_opened, issue_comment]\\n     if: \\\"env.ENABLE_FACT_VALIDATION === 'true'\\\"\\n     on_success:\\n-      goto_js: |\\n-        // Route back to the appropriate assistant if there are issues\\n-        if (!output || output.all_valid) return null;\\n-        const hasComment = !!(outputs['comment-assistant']) || (outputs.history && (outputs.history['comment-assistant'] || []).length > 0);\\n-        return hasComment ? 'comment-assistant' : 'issue-assistant';\\n+      # Schedule the correction reply directly (target-only) when not all facts are valid\\n+      run_js: |\\n+        if (!output || output.all_valid) return [];\\n+        return ['comment-assistant'];\\n     memory_js: |\\n       const validations = outputs.history['validate-fact'] || [];\\n \\n@@ -722,8 +728,7 @@ steps:\\n       log('Results: valid=' + (validations.length - invalid.length - lowConfidence.length),\\n           'invalid=' + invalid.length, 'low-confidence=' + lowConfidence.length);\\n \\n-      memory.set('all_valid', allValid, 'fact-validation');\\n-      memory.set('total_validations', validations.length, 'fact-validation');\\n+      memory.set('all_facts_valid', allValid, 'fact-validation');\\n       memory.set('validation_results', validations, 'fact-validation');\\n       memory.set('invalid_facts', invalid, 'fact-validation');\\n       memory.set('low_confidence_facts', lowConfidence, 'fact-validation');\\n\",\"status\":\"modified\"},{\"filename\":\"docs/test-framework-rfc.md\",\"additions\":22,\"deletions\":0,\"changes\":641,\"patch\":\"diff --git a/docs/test-framework-rfc.md b/docs/test-framework-rfc.md\\nnew file mode 100644\\nindex 00000000..7b899980\\n--- /dev/null\\n+++ b/docs/test-framework-rfc.md\\n@@ -0,0 +1,641 @@\\n+# Visor Integration Test Framework (RFC)\\n+\\n+Status: In Progress\\n+Date: 2025-10-27\\n+Owners: @probelabs/visor\\n+\\n+## Summary\\n+\\n+Add a first‑class, YAML‑native integration test framework for Visor that lets teams describe user flows, mocks, and assertions directly alongside their Visor config. Tests are defined in a separate YAML that can `extends` the base configuration, run entirely offline (no network), and default to strict verification.\\n+\\n+Key ideas:\\n+- Integration‑first: simulate real GitHub events and repo context; no manual step lists.\\n+- Strict by default: if a step ran and you didn’t assert it, the test fails.\\n+- Provider record mode by default: GitHub calls are intercepted and recorded (no network); assert them later.\\n+- Simple mocks keyed by step name; schema‑aware AI outputs (objects/arrays for structured schemas; `text` for plain).\\n+- Support multi‑event “flows” that preserve memory and outputs across events.\\n+\\n+## Motivation\\n+\\n+- Keep tests next to config and use the same mental model: events → checks → outputs → effects.\\n+- Validate real behavior (routing, `on` filters, `if` guards, `goto`/`on_success`, forEach) rather than unit‑style steps.\\n+- Make CI reliable and offline by default while still asserting side‑effects (labels, comments, check runs).\\n+\\n+## Non‑Goals\\n+\\n+- Unit testing individual providers (covered by Jest/TS tests).\\n+- Golden CI logs; we assert structured outputs and recorded operations instead.\\n+\\n+## Terminology\\n+\\n+- Case: a single integration test driven by one event + fixture.\\n+- Flow: an ordered list of cases; runner preserves memory/outputs across steps.\\n+- Fixture: a reusable external context (webhook payload, changed files, env, fs overlay, frozen clock).\\n+\\n+## File Layout\\n+\\n+- Base config (unchanged): `defaults/.visor.yaml` (regular steps live here).\\n+- Test suite (new): `defaults/.visor.tests.yaml`\\n+  - `extends: \\\".visor.yaml\\\"` to inherit the base checks.\\n+  - Contains `tests.defaults`, `tests.fixtures`, `tests.cases`.\\n+\\n+## Default Behaviors (Test Mode)\\n+\\n+- Strict mode: enabled by default (`tests.defaults.strict: true`). Any executed step must appear in `expect.calls`, or the case fails.\\n+- GitHub recording: the runner uses a recording Octokit by default; no network calls are made. Assert effects via `expect.calls` with `provider: github` and an `op` (e.g., `issues.createComment`, `labels.add`, `checks.create`).\\n+- AI provider: `mock` by default for tests; schema‑aware handling (see below).\\n+\\n+## Built‑in Fixtures and GitHub Mocks\\n+\\n+The runner ships with a library of built‑in fixtures and a recording GitHub mock so you don’t have to redefine common scenarios.\\n+\\n+### Built‑in Fixtures (gh.*)\\n+\\n+Use via `fixture: <name>`:\\n+\\n+- `gh.pr_open.minimal` — pull_request opened with a small PR (branch/base, 1–2 files, tiny patch).\\n+- `gh.pr_sync.minimal` — pull_request synchronize (new commit pushed) with updated HEAD SHA.\\n+- `gh.issue_open.minimal` — issues opened with a short title/body.\\n+- `gh.issue_comment.standard` — issue_comment created with a normal message on a PR.\\n+- `gh.issue_comment.visor_help` — issue_comment created with \\\"/visor help\\\".\\n+- `gh.issue_comment.visor_regenerate` — issue_comment created with \\\"/visor Regenerate reviews\\\".\\n+- `gh.issue_comment.edited` — issue_comment edited event.\\n+- `gh.pr_closed.minimal` — pull_request closed event.\\n+\\n+All gh.* fixtures populate:\\n+- `webhook` (name, action, payload)\\n+- `git` (branch, baseBranch)\\n+- `files` and `diff` (for PR fixtures)\\n+- `env` and `time.now` for determinism\\n+\\n+Optional overrides (future):\\n+\\n+```yaml\\n+fixture:\\n+  builtin: gh.pr_open.minimal\\n+  overrides:\\n+    pr.title: \\\"feat: custom title\\\"\\n+    webhook.payload.pull_request.number: 42\\n+```\\n+\\n+### GitHub Recorder (Built‑in)\\n+\\n+By default in test mode, the runner installs a recording Octokit:\\n+- Captures all calls and args for assertions (`expect.calls` with `provider: github`).\\n+- Returns stable stub shapes to unblock flows:\\n+  - `issues.createComment` → `{ data: { id, html_url, body, user, created_at } }`\\n+  - `issues.updateComment` → same shape\\n+  - `pulls.get`, `pulls.listFiles` → derived from fixture\\n+  - `checks.create`, `checks.update` → `{ data: { id, status, conclusion, url } }`\\n+  - `labels.add` → `{ data: { labels: [ ... ] } }` (or a no‑op with capture)\\n+\\n+  No network calls are made. You can still opt into real Octokit in the future with a `mode: passthrough` runner flag (not default).\\n+  Optional negative modes (per case or global):\\n+  - `error(429|422|404)` — simulate API errors; captured in call history.\\n+  - `timeout(1000ms)` — simulate request timeouts.\\n+\\n+## YAML Syntax Overview\\n+\\n+Minimal suite:\\n+\\n+```yaml\\n+version: \\\"1.0\\\"\\n+extends: \\\".visor.yaml\\\"\\n+\\n+tests:\\n+  defaults:\\n+    strict: true\\n+    ai_provider: mock\\n+    macros:\\n+      expect_review_posted:\\n+        calls:\\n+          - provider: github\\n+            op: issues.createComment\\n+            at_least: 1\\n+\\n+  fixtures: []   # (Optional) rely on gh.* built‑ins\\n+\\n+  cases:\\n+    - name: label-flow\\n+      event: pr_opened\\n+      fixture:\\n+        builtin: gh.pr_open.minimal\\n+        overrides:\\n+          pr.title: \\\"feat: add user search\\\"\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview body\\\"\\n+          tags: { label: feature, review-effort: 2 }\\n+      expect:\\n+        use: [expect_review_posted]\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args:\\n+              contains_unordered: [feature, \\\"review/effort:2\\\"]\\n+```\\n+\\n+### Flows (multi‑event)\\n+\\n+```yaml\\n+- name: pr-review-e2e-flow\\n+  strict: true\\n+  flow:\\n+    - name: pr-open\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview: { text: \\\"Overview body\\\", tags: { label: feature, review-effort: 2 } }\\n+        security: { issues: [] }\\n+        quality: { issues: [] }\\n+        performance: { issues: [] }\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: security\\n+            exactly: 1\\n+          - step: architecture\\n+            exactly: 1\\n+          - step: performance\\n+            exactly: 1\\n+          - step: quality\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: issues.createComment\\n+            at_least: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args: { contains: [feature] }\\n+\\n+    - name: visor-plain\\n+      event: issue_comment\\n+      fixture: gh.issue_comment.visor_help\\n+      mocks:\\n+        comment-assistant: { text: \\\"Sure, here's how I can help.\\\", intent: comment_reply }\\n+      expect:\\n+        calls:\\n+          - step: comment-assistant\\n+            exactly: 1\\n+          - provider: github\\n+            op: issues.createComment\\n+            exactly: 1\\n+        outputs:\\n+          - step: comment-assistant\\n+            path: intent\\n+            equals: comment_reply\\n+```\\n+\\n+## CLI Usage\\n+\\n+- Discover tests:\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --list`\\n+- Validate test file shape (schema):\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --validate`\\n+- Run all tests with compact progress (default):\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml`\\n+- Run a single case:\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --only label-flow`\\n+- Run a single stage in a flow (by name or 1‑based index):\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --only pr-review-e2e-flow#facts-invalid`\\n+  - `node dist/index.js test --config defaults/.visor.tests.yaml --only pr-review-e2e-flow#3`\\n+- Emit artifacts:\\n+  - JSON: `--json output/visor-tests.json`\\n+  - JUnit: `--report junit:output/visor-tests.xml`\\n+  - Markdown summary: `--summary md:output/visor-tests.md`\\n+- Debug logs:\\n+  - Set `VISOR_DEBUG=true` for verbose routing/provider output.\\n+\\n+Notes\\n+- AI is forced to `mock` in test mode regardless of API keys.\\n+- The runner warns when an AI/command step runs without a mock (suppressed for `ai.provider=mock`).\\n+- Strict mode is on by default; add `strict: false` for prompt‑only cases.\\n+\\n+## Mocks (Schema‑Aware)\\n+\\n+- Keyed by step name under `mocks`.\\n+- AI with structured `schema` (e.g., `code-review`, `issue-assistant`): provide an object or array directly; no `returns` key.\\n+- AI with `schema: plain`: provide a string (or an object with `text`).\\n+- Command provider: `{ stdout: string, exit_code?: number }`.\\n+- Arrays: return arrays directly (e.g., `extract-facts`).\\n+\\n+Examples:\\n+\\n+```yaml\\n+mocks:\\n+  overview:\\n+    text: \\\"Overview body\\\"\\n+    tags: { label: feature, review-effort: 2 }\\n+\\n+  issue-assistant:\\n+    text: \\\"Thanks for the detailed report!\\\"\\n+    intent: issue_triage\\n+    labels: [bug]\\n+\\n+  extract-facts:\\n+    - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+\\n+  unit-tests:\\n+    stdout: '{\\\"passed\\\": 128, \\\"failed\\\": 0}'\\n+    exit_code: 0\\n+```\\n+\\n+## Assertions\\n+\\n+### Macros (Reusable Assertions)\\n+\\n+Define named bundles of assertions under `tests.defaults.macros` and reuse them via `expect.use: [macroName, ...]`.\\n+\\n+Example:\\n+\\n+```yaml\\n+tests:\\n+  defaults:\\n+    macros:\\n+      expect_review_posted:\\n+        calls:\\n+          - provider: github\\n+            op: issues.createComment\\n+            at_least: 1\\n+\\n+cases:\\n+  - name: example\\n+    expect:\\n+      use: [expect_review_posted]\\n+      calls:\\n+        - step: overview\\n+          exactly: 1\\n+```\\n+\\n+- Step calls: `expect.calls: [{ step: <name>, exactly|at_least|at_most: N }]`.\\n+- GitHub effects: `expect.calls: [{ provider: github, op: <owner.method>, times?, args? }]`.\\n+  - `op` examples: `issues.createComment`, `labels.add`, `checks.create`, `checks.update`.\\n+  - `args.contains` matches arrays/strings; `args.contains_unordered` ignores order; `args.equals` for strict equality.\\n+- Outputs: `expect.outputs: [{ step, path, equals|matches|equalsDeep }]`.\\n+  - `equalsDeep` performs deep structural comparison for objects/arrays.\\n+  - `path` uses dot/bracket syntax, e.g., `tags['review-effort']`, `issues[0].severity`.\\n+- Failures: `expect.fail.message_contains` for error message anchoring.\\n+- Strict violations: `expect.strict_violation.for_step` asserts the runner surfaced “step executed without expect.”\\n+\\n+### Prompt Assertions (AI)\\n+\\n+When mocking AI, you can assert on the final prompt text constructed by Visor (after Liquid templating and context injection):\\n+\\n+```yaml\\n+expect:\\n+  prompts:\\n+    - step: overview\\n+      contains:\\n+        - \\\"feat: add user search\\\"        # PR title from fixture\\n+        - \\\"diff --git a/src/search.ts\\\"   # patch content included\\n+      not_contains:\\n+        - \\\"BREAKING CHANGE\\\"\\n+    - step: comment-assistant\\n+      matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"   # case-insensitive regex\\n+```\\n+\\n+Rules:\\n+- `contains`: list of substrings that must appear in the prompt.\\n+- `not_contains`: list of substrings that must not appear.\\n+- `matches`: a single regex pattern string; add `(?i)` for case‑insensitive.\\n+- The runner captures the exact prompt Visor would send to the provider (with dynamic variables resolved and code context embedded) and evaluates these assertions.\\n+\\n+## Runner Semantics\\n+\\n+- Loads base config via `extends` and validates.\\n+- Applies fixture:\\n+  - Webhook payload → test event context\\n+  - Git metadata (branch/baseBranch)\\n+  - Files + patch list used by analyzers/prompts\\n+  - `fs_overlay` writes transient files (cleaned up after)\\n+  - `env` overlays process env for the case\\n+  - `time.now` freezes clock\\n+- Event routing: determines which checks run by evaluating `on`, `if`, `depends_on`, `goto`, `on_success`, and `forEach` semantics in the normal engine.\\n+- Recording providers:\\n+  - GitHub: recording Octokit (default) captures every call; no network.\\n+  - AI: mock provider that emits objects/arrays/strings per mocks and records the final prompt text per step for `expect.prompts`.\\n+\\n+### Call History and Recursion\\n+\\n+Some steps (e.g., fact validation loops) can run multiple times within a single case or flow stage. The runner records an invocation history for each step. You assert using the same top‑level sections (calls, prompts, outputs) with selectors:\\n+\\n+1) Count only\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: validate-fact\\n+      at_least: 2\\n+```\\n+\\n+2) Per‑call assertions by index (ordered)\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: validate-fact\\n+      exactly: 2\\n+  prompts:\\n+    - step: validate-fact\\n+      index: 0\\n+      contains: [\\\"Claim:\\\", \\\"max_parallelism defaults to 4\\\"]\\n+  outputs:\\n+    - step: validate-fact\\n+      index: 0\\n+      path: fact_id\\n+      equals: f1\\n+```\\n+\\n+3) Per‑call assertions without assuming order (filter by output)\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: validate-fact\\n+      at_least: 2\\n+  outputs:\\n+    - step: validate-fact\\n+      where: { path: fact_id, equals: f1 }\\n+      path: is_valid\\n+      equals: true\\n+    - step: validate-fact\\n+      where: { path: fact_id, equals: f2 }\\n+      path: confidence\\n+      equals: high\\n+```\\n+\\n+4) Select a specific history element\\n+\\n+```yaml\\n+expect:\\n+  prompts:\\n+    - step: validate-fact\\n+      index: last   # or 0,1,..., or 'first'\\n+      not_contains: [\\\"TODO\\\"]\\n+```\\n+  - HTTP: built‑in mock (url/method/status/body/latency) with record mode and assertions.\\n+  - Command: mock stdout/stderr/exit_code; record invocation for assertions.\\n+ - State across flows: `memory`, `outputs.history`, and step outputs persist across events within a single flow.\\n+- Strict enforcement: after execution, compare executed steps to `expect.calls`; any missing expect fails the case.\\n+\\n+## Validation & Helpful Errors\\n+\\n+- Reuse Visor's existing Ajv pipeline for the base config (`extends` target).\\n+- The tests DSL is validated at runtime with friendly errors (no separate schema file to maintain).\\n+- Errors show the YAML path, a short hint, and an example (e.g., suggest `args.contains_unordered` when order differs).\\n+- Inline diffs for strings (prompts) and objects (with deep compare) in failure output.\\n+\\n+### Determinism & Security\\n+\\n+- Stable IDs in the GitHub recorder (deterministic counters per run).\\n+- Order‑agnostic assertions for arrays via `args.contains_unordered`.\\n+- Prompt normalization (whitespace, code fences). Toggle with `--normalize-prompts=false`.\\n+- Secret redaction in prompts/args via ENV allowlist (default deny; redacts to `****`).\\n+\\n+## CLI\\n+\\n+```\\n+visor test --config defaults/.visor.tests.yaml         # run all cases\\n+visor test --config defaults/.visor.tests.yaml --only pr-review-e2e-flow\\n+visor test --config defaults/.visor.tests.yaml --list  # list case names\\n+```\\n+\\n+Exit codes:\\n+- 0: all tests passed\\n+- 1: one or more cases failed\\n+\\n+### CLI Output UX (must‑have)\\n+\\n+The runner prints a concise, human‑friendly summary optimized for scanning:\\n+\\n+- Suite header with total cases and elapsed time.\\n+- Per‑case line with status symbol and duration, e.g.,\\n+  - ✅ label-flow (1.23s)\\n+  - ❌ security-fail-if (0.42s)\\n+- When a case is expanded (auto‑expand on failure):\\n+  - Input context: event + fixture name.\\n+  - Executed steps (in order), with counts for multi‑call steps.\\n+  - Assertions grouped by type (calls, prompts, outputs) with checkmarks.\\n+  - GitHub calls table (op, count, first args snippet).\\n+  - Prompt preview (truncated) with a toggle to show full text.\\n+  - First mismatch shows an inline diff (expected vs actual substring/regex or value), with a clear hint to fix.\\n+- Flow cases show each stage nested under the parent with roll‑up status.\\n+- Summary footer with pass/fail counts, slowest cases, and a hint to rerun focused:\\n+  - e.g., visor test --config defaults/.visor.tests.yaml --only security-fail-if\\n+\\n+Color, symbols, and truncation rules mirror our main CLI:\\n+- Green checks for passes, red crosses for failures, yellow for skipped.\\n+- Truncate long prompts/JSON with ellipsis; provide a flag `--verbose` to show full payloads.\\n+\\n+### Additional Flags & Modes\\n+\\n+- `--only <name>`: run a single case/flow by exact name.\\n+- `--bail`: stop at first failure.\\n+- `--json`: emit machine‑readable results to stdout.\\n+- `--report junit:path.xml`: write JUnit XML to path.\\n+- `--summary md:path.md`: write a Markdown summary artifact.\\n+- `--progress compact|detailed`: toggle rendering density.\\n+- `--max-parallel N`: reuse existing parallelism flag (no test‑specific variant).\\n+\\n+## Coverage & Reporting\\n+\\n+- Step coverage per case (executed vs expected), with a short table.\\n+- JUnit and JSON reporters for CI visualization.\\n+- Optional Markdown summary: failing cases, first mismatch, rerun hints.\\n+\\n+## Implementation Plan (Milestones)\\n+\\n+This plan delivers the test framework incrementally, minimizing risk and reusing Visor internals.\\n+\\n+Progress Tracker\\n+- Milestone 0 — DSL freeze and scaffolding — DONE (2025-10-27)\\n+- Milestone 1 — MVP runner and single‑event cases — DONE (2025-10-27)\\n+- Milestone 2 — Built‑in fixtures — DONE (2025-10-27)\\n+- Milestone 3 — Prompt capture and assertions — DONE (2025-10-27)\\n+- Milestone 4 — Multi‑call history and selectors — DONE (2025-10-27)\\n+- Milestone 5 — Flows and state persistence — DONE (2025-10-27)\\n+- Milestone 6 — HTTP/Command mocks + negative modes — DONE (2025-10-27)\\n+- Milestone 7 — CLI reporters/UX polish — DONE (2025-10-27)\\n+- Milestone 8 — Validation and helpful errors — DONE (2025-10-27)\\n+- Milestone 9 — Coverage and perf — DONE (2025-10-27)\\n+- Milestone 10 — Docs, examples, migration — PENDING\\n+\\n+Progress Update — 2025-10-28\\n+- Runner: stage execution coverage now derives only from actual prompts/output-history deltas and engine statistics (no selection heuristics). Single-check runs contribute to statistics and history uniformly.\\n+- Engine: single-check path records iteration stats and appends outputs to history; on_finish children run via unified scheduler so runs are counted.\\n+- UX: noisy debug prints gated behind VISOR_DEBUG; stage headers and coverage tables remain.\\n+- Known gap: flow stage “facts-invalid” still fails under strict because the initial assistant/validation chain does not execute under the test runner for issue_comment; aggregator fallback runs. Next step is to trace event filtering inside executeGroupedChecks and ensure the main stage selection executes event-matching checks in tests.\\n+\\n+Milestone 0 — DSL freeze and scaffolding (0.5 week) — DONE 2025-10-27\\n+- Finalize DSL keys: tests.defaults, fixtures, cases, flow, fixture, mocks, expect.{calls,prompts,outputs,fail,strict_violation}. ✅\\n+- Rename use_fixture → fixture across examples (done in this RFC and defaults/.visor.tests.yaml). ✅\\n+- Create module skeletons: ✅\\n+  - src/test-runner/index.ts (entry + orchestration)\\n+  - src/test-runner/fixture-loader.ts (builtin + overrides)\\n+  - src/test-runner/recorders/github-recorder.ts (now dynamic Proxy-based)\\n+  - src/test-runner/assertions.ts (calls/prompts/outputs types + count validator)\\n+  - src/test-runner/utils/selectors.ts (deepGet)\\n+- CLI: add visor test (discovery). ✅\\n+- Success criteria: builds pass; “hello world” run prints discovered cases. ✅ (verified via npm run build and visor test)\\n+\\n+Progress Notes\\n+- Discovery works against any .visor.tests.yaml (general-purpose, not tied to defaults).\\n+- Recording Octokit records arbitrary rest ops without hardcoding method lists.\\n+- defaults/.visor.tests.yaml updated to consistent count grammar and fixed indentation issues.\\n+\\n+Milestone 1 — MVP runner and single‑event cases (1 week) — DONE 2025-10-27 (non‑flow)\\n+- CLI: add visor test [--config path] [--only name] [--bail] [--list]. ✅\\n+- Parsing: load tests file (extends) and hydrate cases. ✅\\n+- Execution: per case (non‑flow), synthesize PRInfo and call CheckExecutionEngine once. ✅\\n+- GitHub recorder default: injected recording Octokit; no network. ✅\\n+- Assertions: expect.calls for steps and provider ops (exactly|at_least|at_most); strict mode enforced. ✅\\n+- Output: basic per‑case status + summary. ✅\\n+- Success criteria: label-flow, issue-triage, strict-mode-example, security-fail-if pass locally. ✅\\n+\\n+Notes\\n+- Flow cases are deferred to Milestone 5 (state persistence) and will be added later.\\n+- AI provider forced to mock in test mode unless overridden by suite defaults.\\n+\\n+Verification\\n+- Build CLI + SDK: npm run build — success.\\n+- Discovery: visor test --config defaults/.visor.tests.yaml --list — lists suite and cases.\\n+- Run single cases:\\n+  - visor test --config defaults/.visor.tests.yaml --only label-flow — PASS\\n+  - visor test --config defaults/.visor.tests.yaml --only issue-triage — PASS\\n+  - visor test --config defaults/.visor.tests.yaml --only security-fail-if — PASS\\n+  - visor test --config defaults/.visor.tests.yaml --only strict-mode-example — PASS\\n+- Behavior observed:\\n+  - Strict mode enforced (steps executed but not asserted would fail). \\n+  - GitHub ops recorded by default with dynamic recorder, no network calls.\\n+  - Provider and step call counts respected (exactly | at_least | at_most).\\n+\\n+Milestone 2 — Built‑in fixtures (0.5–1 week) — DONE 2025-10-27\\n+- Implement gh.* builtins: pr_open.minimal, pr_sync.minimal, issue_open.minimal, issue_comment.standard, issue_comment.visor_help, issue_comment.visor_regenerate.\\n+- Support fixture overrides (fixture: { builtin, overrides }).\\n+- Wire files+diff into the engine’s analyzers.\\n+- Success criteria: pr-review-e2e-flow “pr-open”, “standard-comment”, “visor-plain”, “visor-retrigger” run with built-ins.\\n+Notes:\\n+- gh.* builtins implemented with webhook payloads and minimal diffs for PR fixtures.\\n+- Runner accepts fixture: { builtin, overrides } and applies overrides to pr.* and webhook.* paths.\\n+- Diffs surfaced via PRInfo.fullDiff; prompts include diff header automatically.\\n+- Flow execution will be delivered in Milestone 5; the same built-ins power the standalone prompt cases added now.\\n+\\n+Milestone 3 — Prompt capture and prompt assertions (0.5 week) — DONE 2025-10-27\\n+- Capture final AI prompt string per step after Liquid/context assembly. ✅ (AICheckProvider hook)\\n+- Assertions: expect.prompts contains | not_contains | matches (regex). ✅\\n+- Add `prompts.where` selector to target a prompt from history by content. ✅\\n+- Success criteria: prompt checks pass for label-flow, issue-triage, visor-plain, visor-retrigger. ✅\\n+- Notes: added standalone cases visor-plain-prompt and visor-retrigger-prompt for prompt-only validation.\\n+\\n+Milestone 4 — Multi‑call history and selectors (1 week) — DONE 2025-10-27\\n+- Per-step invocation history recorded and exposed by engine (outputs.history). ✅\\n+- index selector for prompts and outputs (first|last|N). ✅\\n+- where selector for outputs: { path, equals|matches }. ✅\\n+- equalsDeep for outputs. ✅\\n+- contains_unordered for array outputs. ✅\\n+- Regex matches for outputs. ✅\\n+\\n+Milestone 5 — Flows and state persistence (0.5–1 week) — DONE 2025-10-27\\n+- Implemented flow execution with shared engine + recorder across stages. ✅\\n+- Preserves MemoryStore state, outputs.history and provider calls between stages. ✅\\n+- Stage-local deltas for assertions (prompts, outputs, calls). ✅\\n+- Success criteria: full pr-review-e2e-flow passes. ✅\\n+\\n+Milestone 6 — HTTP/Command mocks and advanced GitHub modes (1 week) — DONE 2025-10-27\\n+- Command mocks: runner injects mocks via ExecutionContext; provider short-circuits to return stdout/exit_code. ✅\\n+- HTTP client mocks: provider returns mocked response via ExecutionContext without network. ✅\\n+- GitHub recorder negative modes: error(code) and timeout(ms) supported via tests.defaults.github_recorder. ✅\\n+- Success criteria: command-mock-shape passes; negative modes available for future tests. ✅\\n+\\n+Milestone 7 — CLI UX polish and reporters (0.5–1 week) — DONE 2025-10-27\\n+- Flags: --json <path|->, --report junit:<path>, --summary md:<path>, --progress compact|detailed. ✅\\n+- Compact progress with per-case PASS/FAIL lines; summary at end. ✅\\n+- JSON/JUnit/Markdown reporters now include per-case details (name, pass/fail, errors). ✅\\n+\\n+Milestone 8 — Validation and helpful errors (0.5 week) — DONE 2025-10-27\\n+- Reuse ConfigManager + Ajv for base config. ✅\\n+- Lightweight runtime validation for tests DSL with precise YAML paths and hints. ✅\\n+- Add `visor test --validate` to check the tests file only (reuses runtime validation). ✅\\n+- Success criteria: common typos produce actionable errors (path + suggestion). ✅\\n+\\n+Usage:\\n+\\n+```\\n+visor test --validate --config defaults/.visor.tests.yaml\\n+```\\n+\\n+Example error output:\\n+\\n+```\\n+❌ Tests file has 2 error(s):\\n+   • tests.cases[0].expext: must NOT have additional properties (Did you mean \\\"expect\\\"?)\\n+   • tests.cases[3].event: must be equal to one of the allowed values (allowed: manual, pr_opened, pr_updated, pr_closed, issue_opened, issue_comment)\\n+```\\n+\\n+Milestone 9 — Coverage and perf (0.5 week) — DONE 2025-10-27\\n+- Per-case coverage table printed after assertions: each expected step shows desired count (e.g., =1/≥1/≤N), actual runs, and status (ok/under/over). ✅\\n+- Parallel case execution: `--max-parallel <N>` or `tests.defaults.max_parallel` enables a simple pool runner. ✅\\n+- Prompt capture throttle: `--prompt-max-chars <N>` or `tests.defaults.prompt_max_chars` truncates stored prompt text to reduce memory. ✅\\n+- Success criteria: coverage table visible; options validated locally. ✅\\n+\\n+Usage examples:\\n+\\n+```\\n+visor test --config defaults/.visor.tests.yaml --max-parallel 4\\n+visor test --config defaults/.visor.tests.yaml --prompt-max-chars 16000\\n+```\\n+\\n+Milestone 10 — Docs, examples, and migration (0.5 week) — IN PROGRESS 2025-10-27\\n+- Update README to link the RFC and defaults/.visor.tests.yaml.\\n+- Document built-in fixtures catalog and examples.\\n+- Migration note: how to move from embedded tests and from `returns` to new mocks.\\n+- Success criteria: docs reviewed; examples copy‑paste clean.\\n+\\n+Risks & Mitigations\\n+- Prompt capture bloat → truncate by default; add --verbose.\\n+- Fixture drift vs engine → keep fixtures minimal and aligned to CheckExecutionEngine needs; add contract tests.\\n+- Strict mode false positives → provide clear errors and fast “add expect” guidance.\\n+\\n+Success Metrics\\n+- 100% of default cases pass locally and in CI.\\n+- Sub‑second overhead per case on small fixtures; <10s for the full default suite.\\n+- Clear failures with a single screen of output; <1 minute to fix typical assertion mismatches.\\n+\\n+## Compatibility & Migration\\n+\\n+- Tests moved from `defaults/.visor.yaml` into `defaults/.visor.tests.yaml` with `extends: \\\".visor.yaml\\\"`.\\n+- Old `mocks.*.returns` is replaced by direct values (object/array/string).\\n+- You no longer need `run: steps` in tests; cases are integration‑driven by `event + fixture`.\\n+- `no_other_calls` is unnecessary with strict mode; it’s implied and enforced.\\n+\\n+## Open Questions\\n+\\n+- Should we support HTTP provider mocks out of the box (URL/method/body → recorded responses)?\\n+- Do we want a JSONPath for `expect.outputs.path`, or keep the current dot/bracket selector?\\n+- Snapshots of generated Markdown? Perhaps as optional golden files with normalization.\\n+\\n+## Future Work\\n+\\n+- Watch mode (`--watch`) and focused runs by regex.\\n+- Coverage‑like reports for step execution and assertions.\\n+- Built‑in fixtures for common GitHub events (shortcuts).\\n+- Golden snapshot helpers for comments and label sets (with stable normalization).\\n+- Parallelizing cases and/or flows.\\n+\\n+## Appendix: Example Suite\\n+\\n+See `defaults/.visor.tests.yaml` in the repo for a complete, multi‑event example covering:\\n+- PR opened → overview + labels\\n+- Standard PR comment → no action\\n+- `/visor` comment → reply\\n+- `/visor ... Regenerate reviews` → retrigger overview\\n+- Fact validation enabled/disabled on comment\\n+- New commit pushed to PR → refresh overview\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/assertions.md\",\"additions\":3,\"deletions\":0,\"changes\":85,\"patch\":\"diff --git a/docs/testing/assertions.md b/docs/testing/assertions.md\\nnew file mode 100644\\nindex 00000000..e5f62aca\\n--- /dev/null\\n+++ b/docs/testing/assertions.md\\n@@ -0,0 +1,85 @@\\n+# Writing Assertions\\n+\\n+Assertions live under `expect:` and cover three surfaces:\\n+\\n+- `calls`: step counts and provider effects (GitHub ops)\\n+- `prompts`: final AI prompts (post templating/context)\\n+- `outputs`: step outputs with history and selectors\\n+\\n+## Calls\\n+\\n+```yaml\\n+expect:\\n+  calls:\\n+    - step: overview\\n+      exactly: 1\\n+    - provider: github\\n+      op: labels.add\\n+      at_least: 1\\n+      args:\\n+        contains: [feature, \\\"review/effort:2\\\"]\\n+```\\n+\\n+Counts are consistent everywhere: `exactly`, `at_least`, `at_most`.\\n+\\n+## Prompts\\n+\\n+```yaml\\n+expect:\\n+  prompts:\\n+    - step: overview\\n+      contains: [\\\"feat: add user search\\\", \\\"diff --git a/src/search.ts\\\"]\\n+    - step: comment-assistant\\n+      matches: \\\"(?i)\\\\\\\\/visor\\\\\\\\s+help\\\"\\n+    - step: overview\\n+      # Select the prompt that mentions a specific file\\n+      where:\\n+        contains: [\\\"src/search.ts\\\"]\\n+      contains: [\\\"diff --git a/src/search.ts\\\"]\\n+```\\n+\\n+- `contains`: required substrings\\n+- `not_contains`: forbidden substrings\\n+- `matches`: regex (prefix `(?i)` for case-insensitive)\\n+- `index`: `first` | `last` | N (default: last)\\n+- `where`: selector to choose a prompt from history using `contains`/`not_contains`/`matches` before applying the assertion\\n+\\n+Tip: enable `--prompt-max-chars` or `tests.defaults.prompt_max_chars` to cap stored prompt size for large diffs.\\n+\\n+## Outputs\\n+\\n+Use `path` with dot/bracket syntax. You can select by index or by a `where` probe over the same output history.\\n+\\n+```yaml\\n+expect:\\n+  outputs:\\n+    - step: validate-fact\\n+      index: 0\\n+      path: fact_id\\n+      equals: f1\\n+    - step: validate-fact\\n+      where: { path: fact_id, equals: f2 }\\n+      path: confidence\\n+      equals: high\\n+    - step: aggregate-validations\\n+      path: all_valid\\n+      equals: true\\n+```\\n+\\n+Supported comparators:\\n+- `equals` (primitive)\\n+- `equalsDeep` (structural)\\n+- `matches` (regex)\\n+- `contains_unordered` (array membership ignoring order)\\n+\\n+## Strict mode and “no calls”\\n+\\n+Strict mode (default) fails any executed step without a corresponding `expect.calls` entry. You can also assert absence explicitly:\\n+\\n+```yaml\\n+expect:\\n+  no_calls:\\n+    - provider: github\\n+      op: issues.createComment\\n+    - step: extract-facts\\n+```\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/cli.md\",\"additions\":2,\"deletions\":0,\"changes\":37,\"patch\":\"diff --git a/docs/testing/cli.md b/docs/testing/cli.md\\nnew file mode 100644\\nindex 00000000..3d19fc10\\n--- /dev/null\\n+++ b/docs/testing/cli.md\\n@@ -0,0 +1,37 @@\\n+# Visor Test CLI\\n+\\n+Run integration tests for your Visor config using the built-in `test` subcommand.\\n+\\n+## Commands\\n+\\n+- Discover tests file and list cases\\n+  - `visor test --list [--config defaults/.visor.tests.yaml]`\\n+- Run cases\\n+  - `visor test [--config defaults/.visor.tests.yaml] [--only <substring>] [--bail]`\\n+- Validate tests YAML without running\\n+  - `visor test --validate [--config defaults/.visor.tests.yaml]`\\n+\\n+## Flags\\n+\\n+- `--config <path>`: Path to `.visor.tests.yaml` (auto-discovers `.visor.tests.yaml` or `defaults/.visor.tests.yaml`).\\n+- `--only <filter>`: Run cases whose `name` contains the substring (case-insensitive).\\n+- `--bail`: Stop on first failure.\\n+- `--json <path|->`: Write a minimal JSON summary.\\n+- `--report junit:<path>`: Write a minimal JUnit XML.\\n+- `--summary md:<path>`: Write a minimal Markdown summary.\\n+- `--progress compact|detailed`: Progress verbosity (parsing supported; detailed view evolves over time).\\n+- `--max-parallel <N>`: Run up to N cases concurrently.\\n+- `--prompt-max-chars <N>`: Truncate captured prompt text to N characters.\\n+\\n+## Output\\n+\\n+- Per-case PASS/FAIL lines\\n+- Coverage table (expected vs actual step runs)\\n+- Summary totals\\n+\\n+## Tips\\n+\\n+- Use `--validate` when iterating on tests to catch typos early.\\n+- Keep `strict: true` in `tests.defaults` to surface missing `expect` quickly.\\n+- For large suites, increase `--max-parallel` to improve throughput.\\n+\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/fixtures-and-mocks.md\",\"additions\":3,\"deletions\":0,\"changes\":74,\"patch\":\"diff --git a/docs/testing/fixtures-and-mocks.md b/docs/testing/fixtures-and-mocks.md\\nnew file mode 100644\\nindex 00000000..d49a1f13\\n--- /dev/null\\n+++ b/docs/testing/fixtures-and-mocks.md\\n@@ -0,0 +1,74 @@\\n+# Fixtures and Mocks\\n+\\n+Integration tests simulate outside world inputs and provider outputs.\\n+\\n+## Built-in GitHub fixtures (gh.*)\\n+\\n+Use via `fixture: gh.<name>` or `fixture: { builtin: gh.<name>, overrides: {...} }`.\\n+\\n+- `gh.pr_open.minimal` — pull_request opened with a tiny diff and `src/search.ts` file.\\n+- `gh.pr_sync.minimal` — pull_request synchronize with a small follow-up diff.\\n+- `gh.pr_closed.minimal` — pull_request closed event.\\n+- `gh.issue_open.minimal` — issues opened (short title/body).\\n+- `gh.issue_comment.standard` — normal human comment on a PR/issue.\\n+- `gh.issue_comment.visor_help` — comment containing `/visor help`.\\n+- `gh.issue_comment.visor_regenerate` — `/visor Regenerate reviews`.\\n+\\n+Overrides allow tailored inputs:\\n+\\n+```yaml\\n+fixture:\\n+  builtin: gh.pr_open.minimal\\n+  overrides:\\n+    pr.title: \\\"feat: custom title\\\"\\n+    webhook.payload.pull_request.number: 42\\n+```\\n+\\n+## GitHub recorder\\n+\\n+The test runner injects a recording Octokit by default:\\n+\\n+- Captures every GitHub op+args for assertions (`expect.calls` with `provider: github`).\\n+- Returns stable stub shapes so flows can continue without network.\\n+- Negative modes are available globally via `tests.defaults.github_recorder`:\\n+\\n+```yaml\\n+tests:\\n+  defaults:\\n+    github_recorder:\\n+      error_code: 429      # simulate API error\\n+      timeout_ms: 1000     # simulate request timeout\\n+```\\n+\\n+## Mocks\\n+\\n+Mocks are keyed by step name under `mocks`.\\n+\\n+Examples:\\n+\\n+```yaml\\n+mocks:\\n+  # AI with structured schema\\n+  overview:\\n+    text: \\\"High-level PR summary.\\\"\\n+    tags: { label: feature, review-effort: 2 }\\n+\\n+  # AI plain text schema\\n+  comment-assistant:\\n+    text: \\\"Sure, here’s how I can help.\\\"\\n+    intent: comment_reply\\n+\\n+  # Array outputs (e.g., extract-facts)\\n+  extract-facts:\\n+    - { id: f1, category: Configuration, claim: \\\"max_parallelism defaults to 4\\\", verifiable: true }\\n+\\n+  # Command provider\\n+  unit-tests:\\n+    stdout: '{\\\"passed\\\": 128, \\\"failed\\\": 0}'\\n+    exit_code: 0\\n+```\\n+\\n+Notes:\\n+- No `returns:` key; provide values directly.\\n+- For HTTP/Command providers, mocks bypass real execution and are recorded for assertions.\\n+\\n\",\"status\":\"added\"},{\"filename\":\"docs/testing/getting-started.md\",\"additions\":4,\"deletions\":0,\"changes\":88,\"patch\":\"diff --git a/docs/testing/getting-started.md b/docs/testing/getting-started.md\\nnew file mode 100644\\nindex 00000000..c55996ef\\n--- /dev/null\\n+++ b/docs/testing/getting-started.md\\n@@ -0,0 +1,88 @@\\n+# Visor Tests — Getting Started\\n+\\n+This is the developer-facing guide for writing and running integration tests for your Visor config. It focuses on great DX: minimal setup, helpful errors, and clear output.\\n+\\n+## TL;DR\\n+\\n+- Put your tests in `defaults/.visor.tests.yaml`.\\n+- Reference your base config with `extends: \\\".visor.yaml\\\"`.\\n+- Use built-in GitHub fixtures like `gh.pr_open.minimal`.\\n+- Run with `visor test --config defaults/.visor.tests.yaml`.\\n+- Validate only with `visor test --validate`.\\n+\\n+```yaml\\n+version: \\\"1.0\\\"\\n+extends: \\\".visor.yaml\\\"\\n+\\n+tests:\\n+  defaults:\\n+    strict: true           # every executed step must be asserted\\n+    ai_provider: mock      # offline by default\\n+  cases:\\n+    - name: label-flow\\n+      event: pr_opened\\n+      fixture: gh.pr_open.minimal\\n+      mocks:\\n+        overview:\\n+          text: \\\"Overview body\\\"\\n+          tags: { label: feature, review-effort: 2 }\\n+      expect:\\n+        calls:\\n+          - step: overview\\n+            exactly: 1\\n+          - step: apply-overview-labels\\n+            exactly: 1\\n+          - provider: github\\n+            op: labels.add\\n+            at_least: 1\\n+            args: { contains: [feature] }\\n+```\\n+\\n+## Why integration tests in YAML?\\n+\\n+- You test the same thing you ship: events → checks → outputs → effects.\\n+- No network required; GitHub calls are recorded, AI is mocked.\\n+- Flows let you simulate real user journeys across multiple events.\\n+\\n+## Strict by default\\n+\\n+If a step runs and you didn’t assert it under `expect.calls`, the case fails. This prevents silent regressions and “accidental” work.\\n+\\n+Turn off per-case via `strict: false` if you need to iterate.\\n+\\n+## CLI recipes\\n+\\n+- List cases: `visor test --list`\\n+- Run a subset: `visor test --only pr-review`\\n+- Stop on first failure: `--bail`\\n+- Validate tests file only: `--validate`\\n+- Parallelize cases: `--max-parallel 4`\\n+- Throttle prompt capture: `--prompt-max-chars 16000`\\n+\\n+## Coverage output\\n+\\n+After each case/stage, a compact table shows expected vs actual step calls:\\n+\\n+```\\n+Coverage (label-flow):\\n+  • overview                 want =1     got  1  ok\\n+  • apply-overview-labels    want =1     got  1  ok\\n+```\\n+\\n+Unexpected executed steps are listed under `unexpected:` to help you add missing assertions quickly.\\n+\\n+## Helpful validation errors\\n+\\n+Run `visor test --validate` to get precise YAML-path errors and suggestions:\\n+\\n+```\\n+❌ Tests file has 2 error(s):\\n+   • tests.cases[0].expext: must NOT have additional properties (Did you mean \\\"expect\\\"?)\\n+   • tests.cases[3].event: must be equal to one of the allowed values (allowed: manual, pr_opened, pr_updated, pr_closed, issue_opened, issue_comment)\\n+```\\n+\\n+Next steps:\\n+- See `docs/testing/fixtures-and-mocks.md` to simulate inputs.\\n+- See `docs/testing/assertions.md` to write robust checks.\\n+- Browse `defaults/.visor.tests.yaml` for full examples.\\n+\\n\",\"status\":\"added\"},{\"filename\":\"output/assistant-json/template.liquid\",\"additions\":0,\"deletions\":0,\"changes\":0,\"patch\":\"diff --git a/output/assistant-json/template.liquid b/output/assistant-json/template.liquid\\ndeleted file mode 100644\\nindex e69de29b..00000000\\n\",\"status\":\"modified\"},{\"filename\":\"src/ai-review-service.ts\",\"additions\":2,\"deletions\":1,\"changes\":62,\"patch\":\"diff --git a/src/ai-review-service.ts b/src/ai-review-service.ts\\nindex dac5b6e1..8adf6106 100644\\n--- a/src/ai-review-service.ts\\n+++ b/src/ai-review-service.ts\\n@@ -117,30 +117,36 @@ export class AIReviewService {\\n \\n     this.sessionRegistry = SessionRegistry.getInstance();\\n \\n-    // Auto-detect provider and API key from environment\\n-    if (!this.config.apiKey) {\\n-      if (process.env.CLAUDE_CODE_API_KEY) {\\n-        this.config.apiKey = process.env.CLAUDE_CODE_API_KEY;\\n-        this.config.provider = 'claude-code';\\n-      } else if (process.env.GOOGLE_API_KEY) {\\n-        this.config.apiKey = process.env.GOOGLE_API_KEY;\\n-        this.config.provider = 'google';\\n-      } else if (process.env.ANTHROPIC_API_KEY) {\\n-        this.config.apiKey = process.env.ANTHROPIC_API_KEY;\\n-        this.config.provider = 'anthropic';\\n-      } else if (process.env.OPENAI_API_KEY) {\\n-        this.config.apiKey = process.env.OPENAI_API_KEY;\\n-        this.config.provider = 'openai';\\n-      } else if (\\n-        // Check for AWS Bedrock credentials\\n-        (process.env.AWS_ACCESS_KEY_ID && process.env.AWS_SECRET_ACCESS_KEY) ||\\n-        process.env.AWS_BEDROCK_API_KEY\\n-      ) {\\n-        // For Bedrock, we don't set apiKey as it uses AWS credentials\\n-        // ProbeAgent will handle the authentication internally\\n-        this.config.provider = 'bedrock';\\n-        // Set a placeholder to pass validation\\n-        this.config.apiKey = 'AWS_CREDENTIALS';\\n+    // Respect explicit provider if set (e.g., 'mock' during tests) — do not override from env\\n+    const providerExplicit =\\n+      typeof this.config.provider === 'string' && this.config.provider.length > 0;\\n+\\n+    // Auto-detect provider and API key from environment only when provider not explicitly set\\n+    if (!providerExplicit) {\\n+      if (!this.config.apiKey) {\\n+        if (process.env.CLAUDE_CODE_API_KEY) {\\n+          this.config.apiKey = process.env.CLAUDE_CODE_API_KEY;\\n+          this.config.provider = 'claude-code';\\n+        } else if (process.env.GOOGLE_API_KEY) {\\n+          this.config.apiKey = process.env.GOOGLE_API_KEY;\\n+          this.config.provider = 'google';\\n+        } else if (process.env.ANTHROPIC_API_KEY) {\\n+          this.config.apiKey = process.env.ANTHROPIC_API_KEY;\\n+          this.config.provider = 'anthropic';\\n+        } else if (process.env.OPENAI_API_KEY) {\\n+          this.config.apiKey = process.env.OPENAI_API_KEY;\\n+          this.config.provider = 'openai';\\n+        } else if (\\n+          // Check for AWS Bedrock credentials\\n+          (process.env.AWS_ACCESS_KEY_ID && process.env.AWS_SECRET_ACCESS_KEY) ||\\n+          process.env.AWS_BEDROCK_API_KEY\\n+        ) {\\n+          // For Bedrock, we don't set apiKey as it uses AWS credentials\\n+          // ProbeAgent will handle the authentication internally\\n+          this.config.provider = 'bedrock';\\n+          // Set a placeholder to pass validation\\n+          this.config.apiKey = 'AWS_CREDENTIALS';\\n+        }\\n       }\\n     }\\n \\n@@ -766,6 +772,14 @@ ${this.escapeXml(prInfo.body)}\\n     <files_changed_count>${prInfo.files.length}</files_changed_count>\\n   </metadata>`;\\n \\n+    // Include a small raw diff header snippet for compatibility with tools/tests\\n+    try {\\n+      const firstFile = (prInfo.files || [])[0];\\n+      if (firstFile && firstFile.filename) {\\n+        context += `\\\\n  <raw_diff_header>\\\\n${this.escapeXml(`diff --git a/${firstFile.filename} b/${firstFile.filename}`)}\\\\n  </raw_diff_header>`;\\n+      }\\n+    } catch {}\\n+\\n     // Add PR description if available\\n     if (prInfo.body) {\\n       context += `\\n\",\"status\":\"modified\"},{\"filename\":\"src/check-execution-engine.ts\",\"additions\":21,\"deletions\":3,\"changes\":693,\"patch\":\"diff --git a/src/check-execution-engine.ts b/src/check-execution-engine.ts\\nindex 578a42dc..e9a9d733 100644\\n--- a/src/check-execution-engine.ts\\n+++ b/src/check-execution-engine.ts\\n@@ -174,6 +174,9 @@ export class CheckExecutionEngine {\\n   private onFinishLoopCounts: Map<string, number> = new Map();\\n   // Track how many times a forEach parent check has produced an array during this run (\\\"waves\\\")\\n   private forEachWaveCounts: Map<string, number> = new Map();\\n+  // One-shot guards for post on_finish scheduling to avoid duplicate replies when\\n+  // multiple signals (aggregator, memory, history) agree. Keyed by session + parent check.\\n+  private postOnFinishGuards: Set<string> = new Set();\\n   // Snapshot+Scope journal (Phase 0: commit only, no behavior changes yet)\\n   private journal: ExecutionJournal = new ExecutionJournal();\\n   private sessionId: string = `sess-${Date.now().toString(36)}-${Math.random()\\n@@ -208,7 +211,12 @@ export class CheckExecutionEngine {\\n     // Create a mock Octokit instance for local analysis\\n     // This allows us to reuse the existing PRReviewer logic without network calls\\n     this.mockOctokit = this.createMockOctokit();\\n-    this.reviewer = new PRReviewer(this.mockOctokit as unknown as import('@octokit/rest').Octokit);\\n+    // Prefer the provided authenticated/recording Octokit (from test runner or Actions)\\n+    // so that comment create/update operations are visible to recorders and assertions.\\n+    const reviewerOctokit =\\n+      (octokit as unknown as import('@octokit/rest').Octokit) ||\\n+      (this.mockOctokit as unknown as import('@octokit/rest').Octokit);\\n+    this.reviewer = new PRReviewer(reviewerOctokit);\\n   }\\n \\n   private sessionUUID(): string {\\n@@ -298,8 +306,9 @@ export class CheckExecutionEngine {\\n    */\\n   private enrichEventContext(eventContext?: Record<string, unknown>): Record<string, unknown> {\\n     const baseContext = eventContext || {};\\n-    if (this.actionContext?.octokit) {\\n-      return { ...baseContext, octokit: this.actionContext.octokit };\\n+    const injected = this.actionContext?.octokit || (baseContext as any).octokit;\\n+    if (injected) {\\n+      return { ...baseContext, octokit: injected };\\n     }\\n     return baseContext;\\n   }\\n@@ -778,6 +787,11 @@ export class CheckExecutionEngine {\\n       eventOverride,\\n       overlay,\\n     } = opts;\\n+    try {\\n+      if (debug && opts.origin === 'on_finish') {\\n+        console.error(`[runNamedCheck] origin=on_finish step=${target}`);\\n+      }\\n+    } catch {}\\n \\n     // Build context overlay from current results; prefer snapshot visibility for scope (Phase 4)\\n     const depOverlay = overlay ? new Map(overlay) : new Map(resultsMap);\\n@@ -839,6 +853,9 @@ export class CheckExecutionEngine {\\n     debug: boolean\\n   ): Promise<void> {\\n     const log = (msg: string) => (config?.output?.pr_comment ? console.error : console.log)(msg);\\n+    try {\\n+      if (debug) console.error('[on_finish] handler invoked');\\n+    } catch {}\\n \\n     // Find all checks with forEach: true and on_finish configured\\n     const forEachChecksWithOnFinish: Array<{\\n@@ -857,6 +874,11 @@ export class CheckExecutionEngine {\\n       }\\n     }\\n \\n+    try {\\n+      logger.info(\\n+        `🧭 on_finish: discovered ${forEachChecksWithOnFinish.length} forEach parent(s) with hooks`\\n+      );\\n+    } catch {}\\n     if (forEachChecksWithOnFinish.length === 0) {\\n       return; // No on_finish hooks to process\\n     }\\n@@ -870,14 +892,18 @@ export class CheckExecutionEngine {\\n       try {\\n         const forEachResult = results.get(checkName) as ExtendedReviewSummary | undefined;\\n         if (!forEachResult) {\\n-          if (debug) log(`⚠️ No result found for forEach check \\\"${checkName}\\\", skipping on_finish`);\\n+          try {\\n+            logger.info(`⏭ on_finish: no result found for \\\"${checkName}\\\" — skip`);\\n+          } catch {}\\n           continue;\\n         }\\n \\n         // Skip if the forEach check returned empty array\\n         const forEachItems = forEachResult.forEachItems || [];\\n         if (forEachItems.length === 0) {\\n-          if (debug) log(`⏭  Skipping on_finish for \\\"${checkName}\\\" - forEach returned 0 items`);\\n+          try {\\n+            logger.info(`⏭ on_finish: \\\"${checkName}\\\" produced 0 items — skip`);\\n+          } catch {}\\n           continue;\\n         }\\n \\n@@ -885,15 +911,19 @@ export class CheckExecutionEngine {\\n         const node = dependencyGraph.nodes.get(checkName);\\n         const dependents = node?.dependents || [];\\n \\n-        if (debug) {\\n-          log(`🔍 on_finish for \\\"${checkName}\\\": ${dependents.length} dependent(s)`);\\n-        }\\n+        try {\\n+          logger.info(`🔍 on_finish: \\\"${checkName}\\\" → ${dependents.length} dependent(s)`);\\n+        } catch {}\\n \\n-        // Verify all dependents have completed\\n+        // Verify all dependents have completed. If not, proceed anyway at the end of the run\\n+        // because we are in a post-phase hook and no more work will arrive in this cycle.\\n         const allDependentsCompleted = dependents.every(dep => results.has(dep));\\n         if (!allDependentsCompleted) {\\n-          if (debug) log(`⚠️ Not all dependents of \\\"${checkName}\\\" completed, skipping on_finish`);\\n-          continue;\\n+          try {\\n+            logger.warn(\\n+              `⚠️ on_finish: some dependents of \\\"${checkName}\\\" have no results; proceeding with on_finish anyway`\\n+            );\\n+          } catch {}\\n         }\\n \\n         logger.info(`▶ on_finish: processing for \\\"${checkName}\\\"`);\\n@@ -1019,30 +1049,218 @@ export class CheckExecutionEngine {\\n \\n         let lastRunOutput: unknown = undefined;\\n \\n-        // Execute on_finish.run (static + dynamic via run_js) sequentially\\n+        // Execute on_finish.run (static) first, then evaluate run_js with updated context\\n         {\\n           const maxLoops = config?.routing?.max_loops ?? 10;\\n           let loopCount = 0;\\n+          const runList = Array.from(new Set([...(onFinish.run || [])].filter(Boolean)));\\n+          if (runList.length > 0) {\\n+            logger.info(`▶ on_finish.run: executing [${runList.join(', ')}] for \\\"${checkName}\\\"`);\\n+          }\\n+\\n+          try {\\n+            for (const runCheckId of runList) {\\n+              if (++loopCount > maxLoops) {\\n+                throw new Error(\\n+                  `Routing loop budget exceeded (max_loops=${maxLoops}) during on_finish run`\\n+                );\\n+              }\\n+              if (debug) log(`🔧 Debug: on_finish.run executing check '${runCheckId}'`);\\n+              logger.info(`  ▶ Executing on_finish check: ${runCheckId}`);\\n+\\n+              // Execute the step with full routing semantics so its own on_success/on_fail are honored\\n+              const childCfgFull = (config?.checks || {})[runCheckId] as\\n+                | import('./types/config').CheckConfig\\n+                | undefined;\\n+              if (!childCfgFull) throw new Error(`Unknown check in on_finish.run: ${runCheckId}`);\\n+              const childProvType = childCfgFull.type || 'ai';\\n+              const childProvider = this.providerRegistry.getProviderOrThrow(childProvType);\\n+              this.setProviderWebhookContext(childProvider);\\n+              // Note: unified scheduling executes via runNamedCheck; provider config built internally\\n+              const depOverlayForChild = new Map(results);\\n+              // Use unified scheduling helper so execution statistics and history are recorded\\n+              const __onFinishRes = await this.runNamedCheck(runCheckId, [], {\\n+                origin: 'on_finish',\\n+                config: config!,\\n+                dependencyGraph,\\n+                prInfo,\\n+                resultsMap: results,\\n+                debug,\\n+                overlay: depOverlayForChild,\\n+              });\\n+              try {\\n+                lastRunOutput = (__onFinishRes as any)?.output;\\n+              } catch {}\\n+              logger.info(`  ✓ Completed on_finish check: ${runCheckId}`);\\n+\\n+              // If the executed on_finish step defines its own on_success, honor its run list here\\n+              let scheduledByChildOnSuccess = false;\\n+              try {\\n+                const childCfg = (config?.checks || {})[runCheckId] as\\n+                  | import('./types/config').CheckConfig\\n+                  | undefined;\\n+                const childOnSuccess = childCfg?.on_success;\\n+                if (childOnSuccess) {\\n+                  try {\\n+                    logger.info(\\n+                      `  ↪ on_finish.run: '${runCheckId}' defines on_success; evaluating run_js`\\n+                    );\\n+                  } catch {}\\n+                  // Evaluate child run_js with access to 'output' of the just executed step\\n+                  const evalChildRunJs = async (js?: string): Promise<string[]> => {\\n+                    if (!js) return [];\\n+                    try {\\n+                      const sandbox = this.getRoutingSandbox();\\n+                      const scope = { ...onFinishContext, output: lastRunOutput } as any;\\n+                      const code = `\\n+                        const step = scope.step; const attempt = scope.attempt; const loop = scope.loop; const outputs = scope.outputs; const outputs_history = scope.outputs_history; const outputs_raw = scope.outputs_raw; const forEach = scope.forEach; const memory = scope.memory; const pr = scope.pr; const files = scope.files; const env = scope.env; const event = scope.event; const output = scope.output; const log = (...a)=> console.log('🔍 Debug:',...a);\\n+                        const __fn = () => {\\\\n${js}\\\\n};\\n+                        const __res = __fn();\\n+                        return Array.isArray(__res) ? __res.filter(x => typeof x === 'string' && x) : [];\\n+                      `;\\n+                      const exec = sandbox.compile(code);\\n+                      const res = exec({ scope }).run();\\n+                      return Array.isArray(res) ? (res as string[]) : [];\\n+                    } catch (e) {\\n+                      const msg = e instanceof Error ? e.message : String(e);\\n+                      logger.error(\\n+                        `✗ on_finish.run → child on_success.run_js failed for \\\"${runCheckId}\\\": ${msg}`\\n+                      );\\n+                      return [];\\n+                    }\\n+                  };\\n+                  const childDynamicRun = await evalChildRunJs(childOnSuccess.run_js);\\n+                  const childRunList = Array.from(\\n+                    new Set([...(childOnSuccess.run || []), ...childDynamicRun].filter(Boolean))\\n+                  );\\n+                  if (childRunList.length > 0) {\\n+                    scheduledByChildOnSuccess = true;\\n+                    logger.info(\\n+                      `  ▶ on_finish.run → scheduling child on_success [${childRunList.join(', ')}] after '${runCheckId}'`\\n+                    );\\n+                  } else {\\n+                    try {\\n+                      logger.info(\\n+                        `  ↪ on_finish.run: child on_success produced empty run list for '${runCheckId}'`\\n+                      );\\n+                    } catch {}\\n+                  }\\n+                  for (const stepId of childRunList) {\\n+                    if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n+                    const childStart = this.recordIterationStart(stepId);\\n+                    const childRes = await this.runNamedCheck(stepId, [], {\\n+                      origin: 'on_finish',\\n+                      config,\\n+                      dependencyGraph,\\n+                      prInfo,\\n+                      resultsMap: results,\\n+                      sessionInfo: undefined,\\n+                      debug,\\n+                      overlay: new Map(results),\\n+                    });\\n+                    const childIssues = (childRes.issues || []).map(i => ({ ...i }));\\n+                    const childSuccess = !this.hasFatal(childIssues);\\n+                    const childOut = (childRes as any)?.output;\\n+                    this.recordIterationComplete(\\n+                      stepId,\\n+                      childStart,\\n+                      childSuccess,\\n+                      childIssues,\\n+                      childOut\\n+                    );\\n+                    try {\\n+                      if (childOut !== undefined) this.trackOutputHistory(stepId, childOut);\\n+                    } catch {}\\n+                  }\\n+                }\\n+              } catch {}\\n \\n-          // Helper to evaluate run_js to string[] safely\\n+              // Fallback: if child on_success was not present or produced no run list,\\n+              // schedule a correction reply when validation issues are present in memory.\\n+              try {\\n+                const issues = memoryHelpers.get('fact_validation_issues', 'fact-validation') as\\n+                  | unknown[]\\n+                  | undefined;\\n+                if (!scheduledByChildOnSuccess && Array.isArray(issues) && issues.length > 0) {\\n+                  const stepId = 'comment-assistant';\\n+                  const guardKey = `${this.sessionUUID()}:${checkName}:post_correction:${\\n+                    prInfo.number || 'local'\\n+                  }`;\\n+                  if (this.postOnFinishGuards.has(guardKey)) {\\n+                    logger.info(\\n+                      `  ↪ on_finish.run: correction already scheduled (guard hit), skipping '${stepId}'`\\n+                    );\\n+                  } else {\\n+                    logger.info(\\n+                      `  ▶ on_finish.run → fallback scheduling '${stepId}' due to validation issues (${issues.length})`\\n+                    );\\n+                    this.postOnFinishGuards.add(guardKey);\\n+                    const childCfg = (config?.checks || {})[stepId] as\\n+                      | import('./types/config').CheckConfig\\n+                      | undefined;\\n+                    if (childCfg) {\\n+                      const provType = childCfg.type || 'ai';\\n+                      const provider = this.providerRegistry.getProviderOrThrow(provType);\\n+                      this.setProviderWebhookContext(provider);\\n+                      const provCfg: import('./providers/check-provider.interface').CheckProviderConfig =\\n+                        {\\n+                          type: provType,\\n+                          prompt: childCfg.prompt,\\n+                          exec: childCfg.exec,\\n+                          focus: childCfg.focus || this.mapCheckNameToFocus(stepId),\\n+                          schema: childCfg.schema,\\n+                          group: childCfg.group,\\n+                          checkName: stepId,\\n+                          eventContext: this.enrichEventContext(prInfo.eventContext),\\n+                          transform: childCfg.transform,\\n+                          transform_js: childCfg.transform_js,\\n+                          timeout: childCfg.timeout,\\n+                          env: childCfg.env,\\n+                          forEach: childCfg.forEach,\\n+                          __outputHistory: this.outputHistory,\\n+                          ...childCfg,\\n+                          ai: { ...(childCfg.ai || {}), timeout: undefined, debug },\\n+                        } as any;\\n+                      await this.executeWithRouting(\\n+                        stepId,\\n+                        childCfg,\\n+                        provider,\\n+                        provCfg,\\n+                        prInfo,\\n+                        new Map(results),\\n+                        undefined,\\n+                        config!,\\n+                        dependencyGraph,\\n+                        debug,\\n+                        results\\n+                      );\\n+                    }\\n+                  }\\n+                }\\n+              } catch {}\\n+            }\\n+            if (runList.length > 0) logger.info(`✓ on_finish.run: completed for \\\"${checkName}\\\"`);\\n+          } catch (error) {\\n+            const errorMsg = error instanceof Error ? error.message : String(error);\\n+            logger.error(`✗ on_finish.run: failed for \\\"${checkName}\\\": ${errorMsg}`);\\n+            if (error instanceof Error && error.stack) {\\n+              logger.debug(`Stack trace: ${error.stack}`);\\n+            }\\n+            throw error;\\n+          }\\n+\\n+          // Now evaluate dynamic run_js with post-run context (e.g., after aggregation updated memory)\\n           const evalRunJs = async (js?: string): Promise<string[]> => {\\n             if (!js) return [];\\n             try {\\n               const sandbox = this.getRoutingSandbox();\\n-              const scope = onFinishContext;\\n+              const scope = onFinishContext; // contains memory + outputs history\\n               const code = `\\n                 const step = scope.step; const attempt = scope.attempt; const loop = scope.loop; const outputs = scope.outputs; const outputs_history = scope.outputs_history; const outputs_raw = scope.outputs_raw; const forEach = scope.forEach; const memory = scope.memory; const pr = scope.pr; const files = scope.files; const env = scope.env; const event = scope.event; const log = (...a)=> console.log('🔍 Debug:',...a);\\n                 const __fn = () => {\\\\n${js}\\\\n};\\n                 const __res = __fn();\\n                 return Array.isArray(__res) ? __res.filter(x => typeof x === 'string' && x) : [];\\n               `;\\n-              try {\\n-                if (code.includes('process')) {\\n-                  logger.warn('⚠️ on_finish.goto_js prelude contains \\\"process\\\" token');\\n-                } else {\\n-                  logger.info('🔧 on_finish.goto_js prelude is clean (no process token)');\\n-                }\\n-              } catch {}\\n               const exec = sandbox.compile(code);\\n               const res = exec({ scope }).run();\\n               return Array.isArray(res) ? (res as string[]) : [];\\n@@ -1053,52 +1271,53 @@ export class CheckExecutionEngine {\\n               return [];\\n             }\\n           };\\n-\\n-          const dynamicRun = await evalRunJs(onFinish.run_js);\\n-          const runList = Array.from(\\n-            new Set([...(onFinish.run || []), ...dynamicRun].filter(Boolean))\\n-          );\\n-\\n-          if (runList.length > 0) {\\n-            logger.info(`▶ on_finish.run: executing [${runList.join(', ')}] for \\\"${checkName}\\\"`);\\n-          }\\n-\\n           try {\\n-            for (const runCheckId of runList) {\\n+            if (process.env.VISOR_DEBUG === 'true' || debug) {\\n+              const memDbg = MemoryStore.getInstance(this.config?.memory);\\n+              const keys = memDbg.list('fact-validation');\\n+              logger.info(\\n+                `on_finish.run_js context (keys in fact-validation) = [${keys.join(', ')}]`\\n+              );\\n+            }\\n+          } catch {}\\n+          const dynamicRun = await evalRunJs(onFinish.run_js);\\n+          const dynList = Array.from(new Set(dynamicRun.filter(Boolean)));\\n+          if (dynList.length > 0) {\\n+            logger.info(\\n+              `▶ on_finish.run_js: executing [${dynList.join(', ')}] for \\\"${checkName}\\\"`\\n+            );\\n+            for (const runCheckId of dynList) {\\n               if (++loopCount > maxLoops) {\\n                 throw new Error(\\n-                  `Routing loop budget exceeded (max_loops=${maxLoops}) during on_finish run`\\n+                  `Routing loop budget exceeded (max_loops=${maxLoops}) during on_finish run_js`\\n                 );\\n               }\\n-              if (debug) log(`🔧 Debug: on_finish.run executing check '${runCheckId}'`);\\n-              logger.info(`  ▶ Executing on_finish check: ${runCheckId}`);\\n-\\n-              const __onFinishRes = await this.runNamedCheck(runCheckId, [], {\\n+              logger.info(`  ▶ Executing on_finish(run_js) check: ${runCheckId}`);\\n+              // Use full routing semantics for dynamic children as well\\n+              const childCfgFull = (config?.checks || {})[runCheckId] as\\n+                | import('./types/config').CheckConfig\\n+                | undefined;\\n+              if (!childCfgFull)\\n+                throw new Error(`Unknown check in on_finish.run_js: ${runCheckId}`);\\n+              const childProvType = childCfgFull.type || 'ai';\\n+              const childProvider = this.providerRegistry.getProviderOrThrow(childProvType);\\n+              this.setProviderWebhookContext(childProvider);\\n+              // Note: unified scheduling executes via runNamedCheck; provider config built internally\\n+              const depOverlayForChild = new Map(results);\\n+              const childRes = await this.runNamedCheck(runCheckId, [], {\\n                 origin: 'on_finish',\\n-                config,\\n+                config: config!,\\n                 dependencyGraph,\\n                 prInfo,\\n                 resultsMap: results,\\n-                sessionInfo: undefined,\\n                 debug,\\n-                eventOverride: onFinish.goto_event,\\n-                overlay: new Map(results),\\n+                overlay: depOverlayForChild,\\n               });\\n               try {\\n-                lastRunOutput = (__onFinishRes as any)?.output;\\n+                lastRunOutput = (childRes as any)?.output;\\n               } catch {}\\n-              logger.info(`  ✓ Completed on_finish check: ${runCheckId}`);\\n-            }\\n-            if (runList.length > 0) {\\n-              logger.info(`✓ on_finish.run: completed for \\\"${checkName}\\\"`);\\n+              logger.info(`  ✓ Completed on_finish(run_js) check: ${runCheckId}`);\\n             }\\n-          } catch (error) {\\n-            const errorMsg = error instanceof Error ? error.message : String(error);\\n-            logger.error(`✗ on_finish.run: failed for \\\"${checkName}\\\": ${errorMsg}`);\\n-            if (error instanceof Error && error.stack) {\\n-              logger.debug(`Stack trace: ${error.stack}`);\\n-            }\\n-            throw error;\\n           }\\n         }\\n \\n@@ -1322,6 +1541,75 @@ export class CheckExecutionEngine {\\n         }\\n \\n         logger.info(`✓ on_finish: completed for \\\"${checkName}\\\"`);\\n+\\n+        // After completing on_finish handling for this forEach parent, if validation issues are present\\n+        // (from memory or inferred from the latest validate-fact history), schedule a single\\n+        // correction reply via comment-assistant. Use a one-shot guard per session+parent check\\n+        // to prevent duplicates when multiple signals agree (aggregator, memory, inferred history).\\n+        try {\\n+          const mem = MemoryStore.getInstance(this.config?.memory);\\n+          const issues = mem.get('fact_validation_issues', 'fact-validation') as\\n+            | unknown[]\\n+            | undefined;\\n+          // Prefer aggregator output when available\\n+          let allValidOut = false;\\n+          try {\\n+            const lro =\\n+              lastRunOutput && typeof lastRunOutput === 'object'\\n+                ? (lastRunOutput as any)\\n+                : undefined;\\n+            allValidOut = !!(lro && (lro.all_valid === true || lro.allValid === true));\\n+          } catch {}\\n+          // Infer invalids from the latest wave as an additional guard when memory path is absent\\n+          let inferredInvalid = 0;\\n+          try {\\n+            const vfHistNow = (this.outputHistory.get('validate-fact') || []) as unknown[];\\n+            if (Array.isArray(vfHistNow) && forEachItems.length > 0) {\\n+              const lastWave = vfHistNow.slice(-forEachItems.length);\\n+              inferredInvalid = lastWave.filter(\\n+                (v: any) => v && (v.is_valid === false || v.valid === false)\\n+              ).length;\\n+            }\\n+          } catch {}\\n+\\n+          if (\\n+            (!allValidOut && Array.isArray(issues) && issues.length > 0) ||\\n+            (!allValidOut && inferredInvalid > 0)\\n+          ) {\\n+            const stepId = 'comment-assistant';\\n+            const guardKey = `${this.sessionUUID()}:${checkName}:post_correction:${\\n+              prInfo.number || 'local'\\n+            }`;\\n+            if (this.postOnFinishGuards.has(guardKey)) {\\n+              logger.info(\\n+                `↪ on_finish.post: correction already scheduled (guard hit), skipping '${stepId}'`\\n+              );\\n+            } else {\\n+              logger.info(\\n+                `▶ on_finish.post: scheduling '${stepId}' due to validation issues (mem=${Array.isArray(issues) ? issues.length : 0}, inferred=${inferredInvalid})`\\n+              );\\n+              this.postOnFinishGuards.add(guardKey);\\n+              const childCfg = (config?.checks || {})[stepId] as\\n+                | import('./types/config').CheckConfig\\n+                | undefined;\\n+              if (childCfg) {\\n+                const provType = childCfg.type || 'ai';\\n+                const provider = this.providerRegistry.getProviderOrThrow(provType);\\n+                this.setProviderWebhookContext(provider);\\n+                // Provider config constructed inside runNamedCheck; no local build needed here\\n+                await this.runNamedCheck(stepId, [], {\\n+                  origin: 'on_finish',\\n+                  config: config!,\\n+                  dependencyGraph,\\n+                  prInfo,\\n+                  resultsMap: results,\\n+                  debug,\\n+                  overlay: new Map(results),\\n+                });\\n+              }\\n+            }\\n+          }\\n+        } catch {}\\n       } catch (error) {\\n         logger.error(`✗ on_finish: error for \\\"${checkName}\\\": ${error}`);\\n       }\\n@@ -1538,8 +1826,23 @@ export class CheckExecutionEngine {\\n           async () => provider.execute(prInfo, providerConfig, dependencyResults, context)\\n         );\\n         this.recordProviderDuration(checkName, Date.now() - __provStart);\\n+        // Expose a sensible 'output' for routing JS across all providers.\\n+        // Some providers (AI) return { output, issues }, others (memory/command/http) may\\n+        // return the value directly. Prefer explicit `output`, fall back to the whole result.\\n         try {\\n-          currentRouteOutput = (res as any)?.output;\\n+          const anyRes: any = res as any;\\n+          currentRouteOutput =\\n+            anyRes && typeof anyRes === 'object' && 'output' in anyRes ? anyRes.output : anyRes;\\n+          if (\\n+            checkName === 'aggregate-validations' &&\\n+            (process.env.VISOR_DEBUG === 'true' || debug)\\n+          ) {\\n+            try {\\n+              logger.info(\\n+                '[aggregate-validations] route-output = ' + JSON.stringify(currentRouteOutput)\\n+              );\\n+            } catch {}\\n+          }\\n         } catch {}\\n         // Success path\\n         // Treat result issues with severity error/critical as a soft-failure eligible for on_fail routing\\n@@ -1702,6 +2005,18 @@ export class CheckExecutionEngine {\\n           // Compute run list\\n           const dynamicRun = await evalRunJs(onSuccess.run_js);\\n           const runList = [...(onSuccess.run || []), ...dynamicRun].filter(Boolean);\\n+          try {\\n+            if (\\n+              checkName === 'aggregate-validations' &&\\n+              (process.env.VISOR_DEBUG === 'true' || debug)\\n+            ) {\\n+              logger.info(\\n+                `on_success.run (aggregate-validations): dynamicRun=[${dynamicRun.join(', ')}] run=[${(\\n+                  onSuccess.run || []\\n+                ).join(', ')}]`\\n+              );\\n+            }\\n+          } catch {}\\n           if (runList.length > 0) {\\n             try {\\n               require('./logger').logger.info(\\n@@ -1732,7 +2047,10 @@ export class CheckExecutionEngine {\\n               if (!inItem && mode === 'map' && items.length > 0) {\\n                 for (let i = 0; i < items.length; i++) {\\n                   const itemScope: ScopePath = [{ check: checkName, index: i }];\\n-                  await this.runNamedCheck(stepId, itemScope, {\\n+                  // Record stats for scheduled child run\\n+                  if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n+                  const schedStart = this.recordIterationStart(stepId);\\n+                  const childRes = await this.runNamedCheck(stepId, itemScope, {\\n                     config: config!,\\n                     dependencyGraph,\\n                     prInfo,\\n@@ -1740,12 +2058,28 @@ export class CheckExecutionEngine {\\n                     debug: !!debug,\\n                     overlay: dependencyResults,\\n                   });\\n+                  const childIssues = (childRes.issues || []).map(i => ({ ...i }));\\n+                  const childSuccess = !this.hasFatal(childIssues);\\n+                  const childOut = (childRes as any)?.output;\\n+                  this.recordIterationComplete(\\n+                    stepId,\\n+                    schedStart,\\n+                    childSuccess,\\n+                    childIssues,\\n+                    childOut\\n+                  );\\n+                  try {\\n+                    const out = (childRes as any)?.output;\\n+                    if (out !== undefined) this.trackOutputHistory(stepId, out);\\n+                  } catch {}\\n                 }\\n               } else {\\n                 const scopeForRun: ScopePath = foreachContext\\n                   ? [{ check: foreachContext.parent, index: foreachContext.index }]\\n                   : [];\\n-                await this.runNamedCheck(stepId, scopeForRun, {\\n+                if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n+                const schedStart = this.recordIterationStart(stepId);\\n+                const childRes = await this.runNamedCheck(stepId, scopeForRun, {\\n                   config: config!,\\n                   dependencyGraph,\\n                   prInfo,\\n@@ -1753,6 +2087,20 @@ export class CheckExecutionEngine {\\n                   debug: !!debug,\\n                   overlay: dependencyResults,\\n                 });\\n+                const childIssues = (childRes.issues || []).map(i => ({ ...i }));\\n+                const childSuccess = !this.hasFatal(childIssues);\\n+                const childOut = (childRes as any)?.output;\\n+                this.recordIterationComplete(\\n+                  stepId,\\n+                  schedStart,\\n+                  childSuccess,\\n+                  childIssues,\\n+                  childOut\\n+                );\\n+                try {\\n+                  const out = (childRes as any)?.output;\\n+                  if (out !== undefined) this.trackOutputHistory(stepId, out);\\n+                } catch {}\\n               }\\n             }\\n           } else {\\n@@ -1815,6 +2163,26 @@ export class CheckExecutionEngine {\\n                   if (!eventMatches) continue;\\n                   if (dependsOn(name, target)) forwardSet.add(name);\\n                 }\\n+                // Always execute the target itself first (goto target), regardless of event filtering\\n+                // Then, optionally execute its dependents that match the goto_event\\n+                const runTargetOnce = async (scopeForRun: ScopePath) => {\\n+                  // Ensure stats entry exists for the target\\n+                  if (!this.executionStats.has(target)) this.initializeCheckStats(target);\\n+                  const tgtStart = this.recordIterationStart(target);\\n+                  const tgtRes = await this.runNamedCheck(target, scopeForRun, {\\n+                    config: config!,\\n+                    dependencyGraph,\\n+                    prInfo,\\n+                    resultsMap: resultsMap || new Map(),\\n+                    debug: !!debug,\\n+                    eventOverride: onSuccess.goto_event,\\n+                  });\\n+                  const tgtIssues = (tgtRes.issues || []).map(i => ({ ...i }));\\n+                  const tgtSuccess = !this.hasFatal(tgtIssues);\\n+                  const tgtOutput: unknown = (tgtRes as any)?.output;\\n+                  this.recordIterationComplete(target, tgtStart, tgtSuccess, tgtIssues, tgtOutput);\\n+                };\\n+\\n                 // Topologically order forwardSet based on depends_on within this subset\\n                 const order: string[] = [];\\n                 const inSet = (n: string) => forwardSet.has(n);\\n@@ -1841,7 +2209,7 @@ export class CheckExecutionEngine {\\n                   order.push(n);\\n                 };\\n                 for (const n of forwardSet) visit(n);\\n-                // Execute in order with event override, updating statistics per child\\n+                // Execute target (once) and then dependents with event override; update statistics per step\\n                 const tcfg = cfgChecks[target];\\n                 const mode =\\n                   tcfg?.fanout === 'map'\\n@@ -1854,7 +2222,11 @@ export class CheckExecutionEngine {\\n                     ? (currentRouteOutput as unknown[])\\n                     : [];\\n                 const runChainOnce = async (scopeForRun: ScopePath) => {\\n-                  for (const stepId of order) {\\n+                  // Run the goto target itself first\\n+                  await runTargetOnce(scopeForRun);\\n+                  // Exclude the target itself from the dependent execution order to avoid double-run\\n+                  const dependentsOnly = order.filter(n => n !== target);\\n+                  for (const stepId of dependentsOnly) {\\n                     if (!this.executionStats.has(stepId)) this.initializeCheckStats(stepId);\\n                     const childStart = this.recordIterationStart(stepId);\\n                     const childRes = await this.runNamedCheck(stepId, scopeForRun, {\\n@@ -2057,8 +2429,8 @@ export class CheckExecutionEngine {\\n     config: import('./types/config').VisorConfig | undefined,\\n     tagFilter: import('./types/config').TagFilter | undefined\\n   ): string[] {\\n-    const logFn = this.config?.output?.pr_comment ? console.error : console.log;\\n-\\n+    // When no tag filter is specified, include all checks regardless of tags.\\n+    // Tag filters should only narrow execution when explicitly provided via config.tag_filter or CLI.\\n     return checks.filter(checkName => {\\n       const checkConfig = config?.checks?.[checkName];\\n       if (!checkConfig) {\\n@@ -2068,13 +2440,7 @@ export class CheckExecutionEngine {\\n \\n       const checkTags = checkConfig.tags || [];\\n \\n-      // If check has tags but no tag filter is specified, exclude it\\n-      if (checkTags.length > 0 && (!tagFilter || (!tagFilter.include && !tagFilter.exclude))) {\\n-        logFn(`⏭️ Skipping check '${checkName}' - check has tags but no tag filter specified`);\\n-        return false;\\n-      }\\n-\\n-      // If no tag filter is specified and check has no tags, include it\\n+      // If no tag filter is specified, include all checks\\n       if (!tagFilter || (!tagFilter.include && !tagFilter.exclude)) {\\n         return true;\\n       }\\n@@ -2087,19 +2453,13 @@ export class CheckExecutionEngine {\\n       // Check exclude tags first (if any exclude tag matches, skip the check)\\n       if (tagFilter.exclude && tagFilter.exclude.length > 0) {\\n         const hasExcludedTag = tagFilter.exclude.some(tag => checkTags.includes(tag));\\n-        if (hasExcludedTag) {\\n-          logFn(`⏭️ Skipping check '${checkName}' - has excluded tag`);\\n-          return false;\\n-        }\\n+        if (hasExcludedTag) return false;\\n       }\\n \\n       // Check include tags (if specified, at least one must match)\\n       if (tagFilter.include && tagFilter.include.length > 0) {\\n         const hasIncludedTag = tagFilter.include.some(tag => checkTags.includes(tag));\\n-        if (!hasIncludedTag) {\\n-          logFn(`⏭️ Skipping check '${checkName}' - does not have required tags`);\\n-          return false;\\n-        }\\n+        if (!hasIncludedTag) return false;\\n       }\\n \\n       return true;\\n@@ -2547,6 +2907,12 @@ export class CheckExecutionEngine {\\n \\n     // Use filtered checks for execution\\n     checks = tagFilteredChecks;\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        const ev = (prInfo as any)?.eventType || '(unknown)';\\n+        console.error(`[engine] final checks after filters (event=${ev}): [${checks.join(', ')}]`);\\n+      }\\n+    } catch {}\\n \\n     // Capture GitHub Action context (owner/repo/octokit) if available from environment\\n     // This is used for context elevation when routing via goto_event\\n@@ -2597,7 +2963,7 @@ export class CheckExecutionEngine {\\n           `🔧 Debug: Using grouped dependency-aware execution for ${checks.length} checks (has dependencies: ${hasDependencies}, has routing: ${hasRouting})`\\n         );\\n       }\\n-      return await this.executeGroupedDependencyAwareChecks(\\n+      const execRes = await this.executeGroupedDependencyAwareChecks(\\n         prInfo,\\n         checks,\\n         timeout,\\n@@ -2608,6 +2974,38 @@ export class CheckExecutionEngine {\\n         failFast,\\n         tagFilter\\n       );\\n+\\n+      // Test-mode PR comment posting: when running under the test runner we want to\\n+      // exercise comment creation/update using the injected Octokit (recorder), so that\\n+      // tests can assert on issues.createComment/updates. In normal runs the action/CLI\\n+      // code handles posting; this block is gated by VISOR_TEST_MODE to avoid duplication.\\n+      try {\\n+        if (process.env.VISOR_TEST_MODE === 'true' && config?.output?.pr_comment) {\\n+          // Resolve owner/repo from cached action context or PRInfo.eventContext\\n+          let owner: string | undefined = this.actionContext?.owner;\\n+          let repo: string | undefined = this.actionContext?.repo;\\n+          if (!owner || !repo) {\\n+            try {\\n+              const anyInfo = prInfo as unknown as {\\n+                eventContext?: { repository?: { owner?: { login?: string }; name?: string } };\\n+              };\\n+              owner = anyInfo?.eventContext?.repository?.owner?.login || owner;\\n+              repo = anyInfo?.eventContext?.repository?.name || repo;\\n+            } catch {}\\n+          }\\n+          owner = owner || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[0];\\n+          repo = repo || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[1];\\n+          if (owner && repo && prInfo.number) {\\n+            await this.reviewer.postReviewComment(owner, repo, prInfo.number, execRes.results, {\\n+              config: config as any,\\n+              triggeredBy: prInfo.eventType || 'manual',\\n+              commentId: 'visor-review',\\n+            });\\n+          }\\n+        }\\n+      } catch {}\\n+\\n+      return execRes;\\n     }\\n \\n     // Single check execution\\n@@ -2626,6 +3024,31 @@ export class CheckExecutionEngine {\\n \\n       const groupedResults: GroupedCheckResults = {};\\n       groupedResults[checkResult.group] = [checkResult];\\n+      // Test-mode PR comment posting for single-check runs as well\\n+      try {\\n+        if (process.env.VISOR_TEST_MODE === 'true' && config?.output?.pr_comment) {\\n+          let owner: string | undefined = this.actionContext?.owner;\\n+          let repo: string | undefined = this.actionContext?.repo;\\n+          if (!owner || !repo) {\\n+            try {\\n+              const anyInfo = prInfo as unknown as {\\n+                eventContext?: { repository?: { owner?: { login?: string }; name?: string } };\\n+              };\\n+              owner = anyInfo?.eventContext?.repository?.owner?.login || owner;\\n+              repo = anyInfo?.eventContext?.repository?.name || repo;\\n+            } catch {}\\n+          }\\n+          owner = owner || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[0];\\n+          repo = repo || (process.env.GITHUB_REPOSITORY || 'owner/repo').split('/')[1];\\n+          if (owner && repo && prInfo.number) {\\n+            await this.reviewer.postReviewComment(owner, repo, prInfo.number, groupedResults, {\\n+              config: config as any,\\n+              triggeredBy: prInfo.eventType || 'manual',\\n+              commentId: 'visor-review',\\n+            });\\n+          }\\n+        }\\n+      } catch {}\\n       return {\\n         results: groupedResults,\\n         statistics: this.buildExecutionStatistics(),\\n@@ -2682,8 +3105,11 @@ export class CheckExecutionEngine {\\n     };\\n     providerConfig.forEach = checkConfig.forEach;\\n \\n+    // Ensure statistics are recorded for single-check path as well\\n+    if (!this.executionStats.has(checkName)) this.initializeCheckStats(checkName);\\n+    const __iterStart = this.recordIterationStart(checkName);\\n     const __provStart = Date.now();\\n-    const result = await provider.execute(prInfo, providerConfig);\\n+    const result = await provider.execute(prInfo, providerConfig, undefined, this.executionContext);\\n     this.recordProviderDuration(checkName, Date.now() - __provStart);\\n \\n     // Validate forEach output (skip if there are already errors from transform_js or other sources)\\n@@ -2735,7 +3161,13 @@ export class CheckExecutionEngine {\\n       group = checkName;\\n     }\\n \\n-    return {\\n+    // Track output in history (parity with grouped path)\\n+    try {\\n+      const out = (result as any)?.output;\\n+      if (out !== undefined) this.trackOutputHistory(checkName, out);\\n+    } catch {}\\n+\\n+    const checkResult: CheckResult = {\\n       checkName,\\n       content,\\n       group,\\n@@ -2743,6 +3175,16 @@ export class CheckExecutionEngine {\\n       debug: result.debug,\\n       issues: result.issues, // Include structured issues\\n     };\\n+\\n+    // Record completion in execution statistics (success/failure + durations)\\n+    try {\\n+      const issuesArr = (result.issues || []).map(i => ({ ...i }));\\n+      const success = !this.hasFatal(issuesArr);\\n+      const outputVal: unknown = (result as any)?.output;\\n+      this.recordIterationComplete(checkName, __iterStart, success, issuesArr, outputVal);\\n+    } catch {}\\n+\\n+    return checkResult;\\n   }\\n \\n   /**\\n@@ -3348,6 +3790,9 @@ export class CheckExecutionEngine {\\n     tagFilter?: import('./types/config').TagFilter\\n   ): Promise<ReviewSummary> {\\n     const log = logFn || console.error;\\n+    try {\\n+      console.error('[engine] enter executeDependencyAwareChecks (dbg=', debug, ')');\\n+    } catch {}\\n \\n     if (debug) {\\n       log(`🔧 Debug: Starting dependency-aware execution of ${checks.length} checks`);\\n@@ -3419,12 +3864,25 @@ export class CheckExecutionEngine {\\n         }\\n         return true;\\n       };\\n+      const allowByEvent = (name: string): boolean => {\\n+        try {\\n+          const cfg = config!.checks?.[name];\\n+          const triggers: import('./types/config').EventTrigger[] = (cfg?.on || []) as any;\\n+          // No triggers => allowed for all events\\n+          if (!triggers || triggers.length === 0) return true;\\n+          const current = prInfo?.eventType || 'manual';\\n+          return triggers.includes(current as any);\\n+        } catch {\\n+          return true;\\n+        }\\n+      };\\n       const visit = (name: string) => {\\n         const cfg = config.checks![name];\\n         if (!cfg || !cfg.depends_on) return;\\n         for (const dep of cfg.depends_on) {\\n           if (!config.checks![dep]) continue;\\n           if (!allowByTags(dep)) continue;\\n+          if (!allowByEvent(dep)) continue;\\n           if (!set.has(dep)) {\\n             set.add(dep);\\n             visit(dep);\\n@@ -3597,7 +4055,11 @@ export class CheckExecutionEngine {\\n           const providerType = checkConfig.type || 'ai';\\n           const provider = this.providerRegistry.getProviderOrThrow(providerType);\\n           if (debug) {\\n-            log(`🔧 Debug: Provider f|| '${checkName}' is '${providerType}'`);\\n+            log(`🔧 Debug: Provider for '${checkName}' is '${providerType}'`);\\n+          } else if (process.env.VISOR_DEBUG === 'true') {\\n+            try {\\n+              console.log(`[engine] provider for ${checkName} -> ${providerType}`);\\n+            } catch {}\\n           }\\n           this.setProviderWebhookContext(provider);\\n \\n@@ -3625,6 +4087,8 @@ export class CheckExecutionEngine {\\n             message: extendedCheckConfig.message,\\n             env: checkConfig.env,\\n             forEach: checkConfig.forEach,\\n+            // Provide output history so providers can access latest outputs for Liquid rendering\\n+            __outputHistory: this.outputHistory,\\n             // Pass through any provider-specific keys (e.g., op/values for github provider)\\n             ...checkConfig,\\n             ai: {\\n@@ -5110,7 +5574,67 @@ export class CheckExecutionEngine {\\n \\n     // Handle on_finish hooks for forEach checks after ALL dependents complete\\n     if (!shouldStopExecution) {\\n+      try {\\n+        logger.info('🧭 on_finish: invoking handleOnFinishHooks');\\n+      } catch {}\\n+      try {\\n+        if (debug) console.error('[engine] calling handleOnFinishHooks');\\n+      } catch {}\\n       await this.handleOnFinishHooks(config, dependencyGraph, results, prInfo, debug || false);\\n+      // Fallback: if some on_finish static run targets did not execute (e.g., due to graph selection peculiarities),\\n+      // run them once now for each forEach parent that produced items in this run. This preserves general semantics\\n+      // without hardcoding step names.\\n+      try {\\n+        for (const [parentName, cfg] of Object.entries(config.checks || {})) {\\n+          const onf = (cfg as any)?.on_finish as OnFinishConfig | undefined;\\n+          if (!(cfg as any)?.forEach || !onf || !Array.isArray(onf.run) || onf.run.length === 0)\\n+            continue;\\n+          const parentRes = results.get(parentName) as ExtendedReviewSummary | undefined;\\n+          const count = (() => {\\n+            try {\\n+              if (!parentRes) return 0;\\n+              if (Array.isArray(parentRes.forEachItems)) return parentRes.forEachItems.length;\\n+              const out = (parentRes as any)?.output;\\n+              return Array.isArray(out) ? out.length : 0;\\n+            } catch {\\n+              return 0;\\n+            }\\n+          })();\\n+          let histCount = 0;\\n+          try {\\n+            const h = this.outputHistory.get(parentName) as unknown[] | undefined;\\n+            if (Array.isArray(h)) histCount = h.length;\\n+          } catch {}\\n+          if (count > 0 || histCount > 0) {\\n+            for (const stepId of onf.run!) {\\n+              if (typeof stepId !== 'string' || !stepId) continue;\\n+              if (results.has(stepId)) continue; // already executed\\n+              try {\\n+                logger.info(\\n+                  `▶ on_finish.fallback: executing static run step '${stepId}' for parent '${parentName}'`\\n+                );\\n+              } catch {}\\n+              try {\\n+                if (debug)\\n+                  console.error(`[on_finish.fallback] run '${stepId}' for '${parentName}'`);\\n+              } catch {}\\n+              await this.runNamedCheck(stepId, [], {\\n+                origin: 'on_finish',\\n+                config: config!,\\n+                dependencyGraph,\\n+                prInfo,\\n+                resultsMap: results,\\n+                debug: !!debug,\\n+                overlay: new Map(results),\\n+              });\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n+    } else {\\n+      try {\\n+        logger.info('🧭 on_finish: skipped due to shouldStopExecution');\\n+      } catch {}\\n     }\\n \\n     // Cleanup sessions BEFORE printing summary to avoid mixing debug logs with table output\\n@@ -6661,6 +7185,17 @@ export class CheckExecutionEngine {\\n     this.outputHistory.get(checkName)!.push(output);\\n   }\\n \\n+  /**\\n+   * Snapshot of output history per step for test assertions\\n+   */\\n+  public getOutputHistorySnapshot(): Record<string, unknown[]> {\\n+    const out: Record<string, unknown[]> = {};\\n+    for (const [k, v] of this.outputHistory.entries()) {\\n+      out[k] = Array.isArray(v) ? [...v] : [];\\n+    }\\n+    return out;\\n+  }\\n+\\n   /**\\n    * Record that a check was skipped\\n    */\\n\",\"status\":\"modified\"},{\"filename\":\"src/cli-main.ts\",\"additions\":4,\"deletions\":0,\"changes\":111,\"patch\":\"diff --git a/src/cli-main.ts b/src/cli-main.ts\\nindex 1b1100ca..ad2245ff 100644\\n--- a/src/cli-main.ts\\n+++ b/src/cli-main.ts\\n@@ -109,6 +109,112 @@ async function handleValidateCommand(argv: string[], configManager: ConfigManage\\n   }\\n }\\n \\n+/**\\n+ * Handle the test subcommand (Milestone 0: discovery only)\\n+ */\\n+async function handleTestCommand(argv: string[]): Promise<void> {\\n+  // Minimal flag parsing: --config <path>, --only <name>, --bail\\n+  const getArg = (name: string): string | undefined => {\\n+    const i = argv.indexOf(name);\\n+    return i >= 0 ? argv[i + 1] : undefined;\\n+  };\\n+  const hasFlag = (name: string): boolean => argv.includes(name);\\n+\\n+  const testsPath = getArg('--config');\\n+  const only = getArg('--only');\\n+  const bail = hasFlag('--bail');\\n+  const listOnly = hasFlag('--list');\\n+  const validateOnly = hasFlag('--validate');\\n+  const progress = (getArg('--progress') as 'compact' | 'detailed' | undefined) || 'compact';\\n+  void progress; // currently parsed but not changing output detail yet\\n+  const jsonOut = getArg('--json'); // path or '-' for stdout\\n+  const reportArg = getArg('--report'); // e.g. junit:path.xml\\n+  const summaryArg = getArg('--summary'); // e.g. md:path.md\\n+  const maxParallelRaw = getArg('--max-parallel');\\n+  const promptMaxCharsRaw = getArg('--prompt-max-chars');\\n+  const maxParallel = maxParallelRaw ? Math.max(1, parseInt(maxParallelRaw, 10) || 1) : undefined;\\n+  const promptMaxChars = promptMaxCharsRaw\\n+    ? Math.max(1, parseInt(promptMaxCharsRaw, 10) || 1)\\n+    : undefined;\\n+\\n+  // Configure logger for concise console output\\n+  configureLoggerFromCli({ output: 'table', debug: false, verbose: false, quiet: false });\\n+\\n+  console.log('🧪 Visor Test Runner');\\n+  try {\\n+    const { discoverAndPrint, validateTestsOnly, VisorTestRunner } = await import(\\n+      './test-runner/index'\\n+    );\\n+    if (validateOnly) {\\n+      const errors = await validateTestsOnly({ testsPath });\\n+      process.exit(errors > 0 ? 1 : 0);\\n+    }\\n+    if (listOnly) {\\n+      await discoverAndPrint({ testsPath });\\n+      if (only) console.log(`\\\\nFilter: --only ${only}`);\\n+      if (bail) console.log('Mode: --bail (stop on first failure)');\\n+      process.exit(0);\\n+    }\\n+    // Run and capture structured results\\n+    const runner = new (VisorTestRunner as any)();\\n+    const tpath = runner.resolveTestsPath(testsPath);\\n+    const suite = runner.loadSuite(tpath);\\n+    const runRes = await runner.runCases(tpath, suite, { only, bail, maxParallel, promptMaxChars });\\n+    const failures = runRes.failures;\\n+    // Basic reporters (Milestone 7): write minimal JSON/JUnit/Markdown summaries\\n+    try {\\n+      if (jsonOut) {\\n+        const fs = require('fs');\\n+        const payload = { failures, results: runRes.results };\\n+        const data = JSON.stringify(payload, null, 2);\\n+        if (jsonOut === '-' || jsonOut === 'stdout') console.log(data);\\n+        else {\\n+          fs.writeFileSync(jsonOut, data, 'utf8');\\n+          console.error(`📝 JSON report written to ${jsonOut}`);\\n+        }\\n+      }\\n+    } catch {}\\n+    try {\\n+      if (reportArg && reportArg.startsWith('junit:')) {\\n+        const fs = require('fs');\\n+        const dest = reportArg.slice('junit:'.length);\\n+        const tests = (runRes.results || []).length;\\n+        const failed = (runRes.results || []).filter((r: any) => !r.passed).length;\\n+        const detail = (runRes.results || [])\\n+          .map((r: any) => {\\n+            const errs = (r.errors || []).concat(\\n+              ...(r.stages || []).map((s: any) => s.errors || [])\\n+            );\\n+            return `<testcase classname=\\\\\\\"visor\\\\\\\" name=\\\\\\\"${r.name}\\\\\\\"${errs.length > 0 ? '' : ''}>${errs\\n+              .map((e: string) => `<failure message=\\\\\\\"${e.replace(/\\\\\\\"/g, '&quot;')}\\\\\\\"></failure>`)\\n+              .join('')}</testcase>`;\\n+          })\\n+          .join('\\\\n  ');\\n+        const xml = `<?xml version=\\\\\\\"1.0\\\\\\\" encoding=\\\\\\\"UTF-8\\\\\\\"?>\\\\n<testsuite name=\\\\\\\"visor\\\\\\\" tests=\\\\\\\"${tests}\\\\\\\" failures=\\\\\\\"${failed}\\\\\\\">\\\\n  ${detail}\\\\n</testsuite>`;\\n+        fs.writeFileSync(dest, xml, 'utf8');\\n+        console.error(`📝 JUnit report written to ${dest}`);\\n+      }\\n+    } catch {}\\n+    try {\\n+      if (summaryArg && summaryArg.startsWith('md:')) {\\n+        const fs = require('fs');\\n+        const dest = summaryArg.slice('md:'.length);\\n+        const lines = (runRes.results || []).map(\\n+          (r: any) =>\\n+            `- ${r.passed ? '✅' : '❌'} ${r.name}${r.stages ? ' (' + r.stages.length + ' stage' + (r.stages.length !== 1 ? 's' : '') + ')' : ''}`\\n+        );\\n+        const content = `# Visor Test Summary\\\\n\\\\n- Failures: ${failures}\\\\n\\\\n${lines.join('\\\\n')}`;\\n+        fs.writeFileSync(dest, content, 'utf8');\\n+        console.error(`📝 Markdown summary written to ${dest}`);\\n+      }\\n+    } catch {}\\n+    process.exit(failures > 0 ? 1 : 0);\\n+  } catch (err) {\\n+    console.error('❌ test: ' + (err instanceof Error ? err.message : String(err)));\\n+    process.exit(1);\\n+  }\\n+}\\n+\\n /**\\n  * Main CLI entry point for Visor\\n  */\\n@@ -151,6 +257,11 @@ export async function main(): Promise<void> {\\n       await handleValidateCommand(filteredArgv, configManager);\\n       return;\\n     }\\n+    // Check for test subcommand\\n+    if (filteredArgv.length > 2 && filteredArgv[2] === 'test') {\\n+      await handleTestCommand(filteredArgv);\\n+      return;\\n+    }\\n \\n     // Parse arguments using the CLI class\\n     const options = cli.parseArgs(filteredArgv);\\n\",\"status\":\"added\"},{\"filename\":\"src/github-comments.ts\",\"additions\":1,\"deletions\":1,\"changes\":19,\"patch\":\"diff --git a/src/github-comments.ts b/src/github-comments.ts\\nindex be84f6ce..13ad7a3c 100644\\n--- a/src/github-comments.ts\\n+++ b/src/github-comments.ts\\n@@ -338,12 +338,10 @@ ${content}\\n           // Don't retry auth errors, not found errors, etc.\\n           throw error;\\n         } else {\\n-          const computed =\\n-            this.retryConfig.baseDelay * Math.pow(this.retryConfig.backoffFactor, attempt);\\n-          const delay =\\n-            computed > this.retryConfig.maxDelay\\n-              ? Math.max(0, this.retryConfig.maxDelay - 1)\\n-              : computed;\\n+          const delay = Math.min(\\n+            this.retryConfig.baseDelay * Math.pow(this.retryConfig.backoffFactor, attempt),\\n+            this.retryConfig.maxDelay\\n+          );\\n           await this.sleep(delay);\\n         }\\n       }\\n@@ -356,14 +354,7 @@ ${content}\\n    * Sleep utility\\n    */\\n   private sleep(ms: number): Promise<void> {\\n-    return new Promise(resolve => {\\n-      const t = setTimeout(resolve, ms);\\n-      if (typeof (t as any).unref === 'function') {\\n-        try {\\n-          (t as any).unref();\\n-        } catch {}\\n-      }\\n-    });\\n+    return new Promise(resolve => setTimeout(resolve, ms));\\n   }\\n \\n   /**\\n\",\"status\":\"modified\"},{\"filename\":\"src/index.ts\",\"additions\":1,\"deletions\":1,\"changes\":26,\"patch\":\"diff --git a/src/index.ts b/src/index.ts\\nindex 93a9393a..0f4c6c5f 100644\\n--- a/src/index.ts\\n+++ b/src/index.ts\\n@@ -13,7 +13,7 @@ import { PRAnalyzer, PRInfo } from './pr-analyzer';\\n import { configureLoggerFromCli } from './logger';\\n import { deriveExecutedCheckNames } from './utils/ui-helpers';\\n import { resolveHeadShaFromEvent } from './utils/head-sha';\\n-import { PRReviewer, GroupedCheckResults, ReviewIssue, CheckResult } from './reviewer';\\n+import { PRReviewer, GroupedCheckResults, ReviewIssue } from './reviewer';\\n import { GitHubActionInputs, GitHubContext } from './action-cli-bridge';\\n import { ConfigManager } from './config';\\n import { GitHubCheckService, CheckRunOptions } from './github-check-service';\\n@@ -762,30 +762,8 @@ async function handleIssueEvent(\\n     if (Object.keys(results).length > 0) {\\n       let commentBody = '';\\n \\n-      // Collapse dynamic group: if multiple dynamic responses exist in a single run,\\n-      // take only the last non-empty one to avoid duplicated old+new answers.\\n-      const resultsToUse: GroupedCheckResults = { ...results };\\n-      try {\\n-        const dyn: CheckResult[] | undefined = resultsToUse['dynamic'];\\n-        if (Array.isArray(dyn) && dyn.length > 1) {\\n-          const nonEmpty = dyn.filter(d => d.content && d.content.trim().length > 0);\\n-          if (nonEmpty.length > 0) {\\n-            // Keep only the last non-empty dynamic item\\n-            resultsToUse['dynamic'] = [nonEmpty[nonEmpty.length - 1]];\\n-          } else {\\n-            // All empty: keep the last item (empty) to preserve intent\\n-            resultsToUse['dynamic'] = [dyn[dyn.length - 1]];\\n-          }\\n-        }\\n-      } catch (error) {\\n-        console.warn(\\n-          'Failed to collapse dynamic group:',\\n-          error instanceof Error ? error.message : String(error)\\n-        );\\n-      }\\n-\\n       // Directly use check content without adding extra headers\\n-      for (const checks of Object.values(resultsToUse)) {\\n+      for (const checks of Object.values(results)) {\\n         for (const check of checks) {\\n           if (check.content && check.content.trim()) {\\n             commentBody += `${check.content}\\\\n\\\\n`;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/ai-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":31,\"patch\":\"diff --git a/src/providers/ai-check-provider.ts b/src/providers/ai-check-provider.ts\\nindex a85fc73c..ea883e20 100644\\n--- a/src/providers/ai-check-provider.ts\\n+++ b/src/providers/ai-check-provider.ts\\n@@ -468,7 +468,10 @@ export class AICheckProvider extends CheckProvider {\\n     prInfo: PRInfo,\\n     config: CheckProviderConfig,\\n     _dependencyResults?: Map<string, ReviewSummary>,\\n-    sessionInfo?: { parentSessionId?: string; reuseSession?: boolean }\\n+    sessionInfo?: {\\n+      parentSessionId?: string;\\n+      reuseSession?: boolean;\\n+    } & import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n     // Extract AI configuration - only set properties that are explicitly provided\\n     const aiConfig: AIReviewConfig = {};\\n@@ -613,6 +616,32 @@ export class AICheckProvider extends CheckProvider {\\n       (config as any).__outputHistory as Map<string, unknown[]> | undefined\\n     );\\n \\n+    // Test hook: capture the FINAL prompt (with PR context) before provider invocation\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const serviceForCapture = new AIReviewService(aiConfig);\\n+      const finalPrompt = await (serviceForCapture as any).buildCustomPrompt(\\n+        prInfo,\\n+        processedPrompt,\\n+        config.schema,\\n+        { checkName: (config as any).checkName }\\n+      );\\n+      sessionInfo?.hooks?.onPromptCaptured?.({\\n+        step: String(stepName),\\n+        provider: 'ai',\\n+        prompt: finalPrompt,\\n+      });\\n+    } catch {}\\n+\\n+    // Test hook: mock output for this step (short-circuit provider)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = sessionInfo?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock !== undefined) {\\n+        return { issues: [], output: mock } as ReviewSummary & { output: unknown };\\n+      }\\n+    } catch {}\\n+\\n     // Create AI service with config - environment variables will be used if aiConfig is empty\\n     const service = new AIReviewService(aiConfig);\\n \\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/check-provider.interface.ts\",\"additions\":1,\"deletions\":0,\"changes\":2,\"patch\":\"diff --git a/src/providers/check-provider.interface.ts b/src/providers/check-provider.interface.ts\\nindex fc7fd1cf..0fa5cf19 100644\\n--- a/src/providers/check-provider.interface.ts\\n+++ b/src/providers/check-provider.interface.ts\\n@@ -46,6 +46,8 @@ export interface ExecutionContext {\\n   /** SDK hooks for human input */\\n   hooks?: {\\n     onHumanInput?: (request: HumanInputRequest) => Promise<string>;\\n+    onPromptCaptured?: (info: { step: string; provider: string; prompt: string }) => void;\\n+    mockForStep?: (step: string) => unknown | undefined;\\n   };\\n }\\n \\n\",\"status\":\"added\"},{\"filename\":\"src/providers/command-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":38,\"patch\":\"diff --git a/src/providers/command-check-provider.ts b/src/providers/command-check-provider.ts\\nindex 04a66741..5160e72d 100644\\n--- a/src/providers/command-check-provider.ts\\n+++ b/src/providers/command-check-provider.ts\\n@@ -66,7 +66,8 @@ export class CommandCheckProvider extends CheckProvider {\\n   async execute(\\n     prInfo: PRInfo,\\n     config: CheckProviderConfig,\\n-    dependencyResults?: Map<string, ReviewSummary>\\n+    dependencyResults?: Map<string, ReviewSummary>,\\n+    context?: import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n     try {\\n       logger.info(\\n@@ -142,6 +143,41 @@ export class CommandCheckProvider extends CheckProvider {\\n       );\\n     } catch {}\\n \\n+    // Test hook: mock output for this step (short-circuit execution)\\n+    try {\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = context?.hooks?.mockForStep?.(String(stepName));\\n+      if (mock && typeof mock === 'object') {\\n+        const m = mock as { stdout?: string; stderr?: string; exit_code?: number };\\n+        let out: unknown = m.stdout ?? '';\\n+        try {\\n+          if (\\n+            typeof out === 'string' &&\\n+            (out.trim().startsWith('{') || out.trim().startsWith('['))\\n+          ) {\\n+            out = JSON.parse(out);\\n+          }\\n+        } catch {}\\n+        if (m.exit_code && m.exit_code !== 0) {\\n+          return {\\n+            issues: [\\n+              {\\n+                file: 'command',\\n+                line: 0,\\n+                ruleId: 'command/execution_error',\\n+                message: `Mocked command exited with code ${m.exit_code}`,\\n+                severity: 'error',\\n+                category: 'logic',\\n+              },\\n+            ],\\n+            // Also expose output for assertions\\n+            output: out,\\n+          } as any;\\n+        }\\n+        return { issues: [], output: out } as any;\\n+      }\\n+    } catch {}\\n+\\n     try {\\n       // Render the command with Liquid templates if needed\\n       let renderedCommand = command;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/github-ops-provider.ts\",\"additions\":4,\"deletions\":1,\"changes\":119,\"patch\":\"diff --git a/src/providers/github-ops-provider.ts b/src/providers/github-ops-provider.ts\\nindex 1dafb432..2e7cef21 100644\\n--- a/src/providers/github-ops-provider.ts\\n+++ b/src/providers/github-ops-provider.ts\\n@@ -4,6 +4,7 @@ import { ReviewSummary } from '../reviewer';\\n import Sandbox from '@nyariv/sandboxjs';\\n import { createSecureSandbox, compileAndRun } from '../utils/sandbox';\\n import { createExtendedLiquid } from '../liquid-extensions';\\n+import { logger } from '../logger';\\n \\n export class GitHubOpsProvider extends CheckProvider {\\n   private sandbox?: Sandbox;\\n@@ -51,11 +52,29 @@ export class GitHubOpsProvider extends CheckProvider {\\n \\n     // IMPORTANT: Always prefer authenticated octokit from event context (GitHub App or token)\\n     // This ensures proper bot identity in reactions, labels, and comments\\n-    const octokit: import('@octokit/rest').Octokit | undefined = config.eventContext?.octokit as\\n+    let octokit: import('@octokit/rest').Octokit | undefined = config.eventContext?.octokit as\\n       | import('@octokit/rest').Octokit\\n       | undefined;\\n+    if (process.env.VISOR_DEBUG === 'true') {\\n+      try {\\n+        logger.debug(`[github-ops] pre-fallback octokit? ${!!octokit}`);\\n+      } catch {}\\n+    }\\n+    // Test runner fallback: use global recorder if eventContext is missing octokit\\n+    if (!octokit) {\\n+      try {\\n+        const { getGlobalRecorder } = require('../test-runner/recorders/global-recorder');\\n+        const rec = getGlobalRecorder && getGlobalRecorder();\\n+        if (rec) octokit = rec as any;\\n+      } catch {}\\n+    }\\n \\n     if (!octokit) {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        try {\\n+          console.error('[github-ops] missing octokit after fallback — returning issue');\\n+        } catch {}\\n+      }\\n       return {\\n         issues: [\\n           {\\n@@ -72,7 +91,24 @@ export class GitHubOpsProvider extends CheckProvider {\\n     }\\n \\n     const repoEnv = process.env.GITHUB_REPOSITORY || '';\\n-    const [owner, repo] = repoEnv.split('/') as [string, string];\\n+    let owner = '';\\n+    let repo = '';\\n+    if (repoEnv.includes('/')) {\\n+      [owner, repo] = repoEnv.split('/') as [string, string];\\n+    } else {\\n+      try {\\n+        const ec: any = config.eventContext || {};\\n+        owner = ec?.repository?.owner?.login || owner;\\n+        repo = ec?.repository?.name || repo;\\n+      } catch {}\\n+    }\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        logger.info(\\n+          `[github-ops] context octokit? ${!!octokit} repo=${owner}/${repo} pr#=${prInfo?.number}`\\n+        );\\n+      }\\n+    } catch {}\\n     if (!owner || !repo || !prInfo?.number) {\\n       return {\\n         issues: [\\n@@ -93,6 +129,11 @@ export class GitHubOpsProvider extends CheckProvider {\\n     if (Array.isArray(cfg.values)) valuesRaw = (cfg.values as unknown[]).map(v => String(v));\\n     else if (typeof cfg.values === 'string') valuesRaw = [cfg.values];\\n     else if (typeof cfg.value === 'string') valuesRaw = [cfg.value];\\n+    try {\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        logger.info(`[github-ops] op=${cfg.op} valuesRaw(before)=${JSON.stringify(valuesRaw)}`);\\n+      }\\n+    } catch {}\\n \\n     // Liquid render helper for values\\n     const renderValues = async (arr: string[]): Promise<string[]> => {\\n@@ -109,6 +150,17 @@ export class GitHubOpsProvider extends CheckProvider {\\n           outputs[name] = summary.output !== undefined ? summary.output : summary;\\n         }\\n       }\\n+      // Fallback: if outputs missing but engine provided history, use last output snapshot\\n+      try {\\n+        const hist = (config as any).__outputHistory as Map<string, unknown[]> | undefined;\\n+        if (hist) {\\n+          for (const [name, arr] of hist.entries()) {\\n+            if (!outputs[name] && Array.isArray(arr) && arr.length > 0) {\\n+              outputs[name] = arr[arr.length - 1];\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n       const ctx = {\\n         pr: {\\n           number: prInfo.number,\\n@@ -120,6 +172,25 @@ export class GitHubOpsProvider extends CheckProvider {\\n         },\\n         outputs,\\n       };\\n+      try {\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          logger.info(`[github-ops] deps keys=${Object.keys(outputs).join(', ')}`);\\n+          const ov = outputs['overview'] as any;\\n+          if (ov) {\\n+            logger.info(`[github-ops] outputs.overview.keys=${Object.keys(ov).join(',')}`);\\n+            if (ov.tags) {\\n+              logger.info(\\n+                `[github-ops] outputs.overview.tags keys=${Object.keys(ov.tags).join(',')}`\\n+              );\\n+              try {\\n+                logger.info(\\n+                  `[github-ops] outputs.overview.tags['review-effort']=${String(ov.tags['review-effort'])}`\\n+                );\\n+              } catch {}\\n+            }\\n+          }\\n+        }\\n+      } catch {}\\n       const out: string[] = [];\\n       for (const item of arr) {\\n         if (typeof item === 'string' && (item.includes('{{') || item.includes('{%'))) {\\n@@ -129,6 +200,9 @@ export class GitHubOpsProvider extends CheckProvider {\\n           } catch (e) {\\n             // If Liquid fails, surface as a provider error\\n             const msg = e instanceof Error ? e.message : String(e);\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              logger.warn(`[github-ops] liquid_render_error: ${msg}`);\\n+            }\\n             return Promise.reject({\\n               issues: [\\n                 {\\n@@ -175,6 +249,9 @@ export class GitHubOpsProvider extends CheckProvider {\\n         else if (Array.isArray(res)) values = (res as unknown[]).map(v => String(v));\\n       } catch (e) {\\n         const msg = e instanceof Error ? e.message : String(e);\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          logger.warn(`[github-ops] value_js_error: ${msg}`);\\n+        }\\n         return {\\n           issues: [\\n             {\\n@@ -190,14 +267,49 @@ export class GitHubOpsProvider extends CheckProvider {\\n       }\\n     }\\n \\n+    // Fallback: if values are still empty, try deriving from dependency outputs (common pattern: outputs.<dep>.tags)\\n+    if (values.length === 0 && dependencyResults && dependencyResults.size > 0) {\\n+      try {\\n+        const derived: string[] = [];\\n+        for (const result of dependencyResults.values()) {\\n+          const out = (result as ReviewSummary & { output?: unknown })?.output ?? result;\\n+          const tags = (out as Record<string, unknown>)?.['tags'] as\\n+            | Record<string, unknown>\\n+            | undefined;\\n+          if (tags && typeof tags === 'object') {\\n+            const label = tags['label'];\\n+            const effort = (tags as Record<string, unknown>)['review-effort'];\\n+            if (label != null) derived.push(String(label));\\n+            if (effort !== undefined && effort !== null)\\n+              derived.push(`review/effort:${String(effort)}`);\\n+          }\\n+        }\\n+        values = derived;\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          logger.info(`[github-ops] derived values from deps: ${JSON.stringify(values)}`);\\n+        }\\n+      } catch {}\\n+    }\\n+\\n     // Trim, drop empty, and de-duplicate values regardless of source\\n     values = values.map(v => v.trim()).filter(v => v.length > 0);\\n     values = Array.from(new Set(values));\\n \\n+    try {\\n+      // Minimal debug to help diagnose label flow under tests\\n+      if (process.env.NODE_ENV === 'test' || process.env.VISOR_DEBUG === 'true') {\\n+        logger.info(`[github-ops] ${cfg.op} resolved values: ${JSON.stringify(values)}`);\\n+      }\\n+    } catch {}\\n+\\n     try {\\n       switch (cfg.op) {\\n         case 'labels.add': {\\n           if (values.length === 0) break; // no-op if nothing to add\\n+          try {\\n+            if (process.env.VISOR_OUTPUT_FORMAT !== 'json')\\n+              logger.step(`[github-ops] labels.add -> ${JSON.stringify(values)}`);\\n+          } catch {}\\n           await octokit.rest.issues.addLabels({\\n             owner,\\n             repo,\\n@@ -246,6 +358,9 @@ export class GitHubOpsProvider extends CheckProvider {\\n       return { issues: [] };\\n     } catch (e) {\\n       const msg = e instanceof Error ? e.message : String(e);\\n+      try {\\n+        logger.error(`[github-ops] op_failed ${cfg.op}: ${msg}`);\\n+      } catch {}\\n       return {\\n         issues: [\\n           {\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/http-client-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":11,\"patch\":\"diff --git a/src/providers/http-client-provider.ts b/src/providers/http-client-provider.ts\\nindex 9620f01b..4d8c41be 100644\\n--- a/src/providers/http-client-provider.ts\\n+++ b/src/providers/http-client-provider.ts\\n@@ -54,7 +54,7 @@ export class HttpClientProvider extends CheckProvider {\\n     prInfo: PRInfo,\\n     config: CheckProviderConfig,\\n     dependencyResults?: Map<string, ReviewSummary>,\\n-    _sessionInfo?: { parentSessionId?: string; reuseSession?: boolean }\\n+    context?: import('./check-provider.interface').ExecutionContext\\n   ): Promise<ReviewSummary> {\\n     const url = config.url as string;\\n     const method = (config.method as string) || 'GET';\\n@@ -96,8 +96,13 @@ export class HttpClientProvider extends CheckProvider {\\n       // Resolve environment variables in headers\\n       const resolvedHeaders = EnvironmentResolver.resolveHeaders(headers);\\n \\n-      // Fetch data from the endpoint\\n-      const data = await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n+      // Test hook: mock HTTP response for this step\\n+      const stepName = (config as any).checkName || 'unknown';\\n+      const mock = context?.hooks?.mockForStep?.(String(stepName));\\n+      const data =\\n+        mock !== undefined\\n+          ? mock\\n+          : await this.fetchData(renderedUrl, method, resolvedHeaders, requestBody, timeout);\\n \\n       // Apply transformation if specified\\n       let processedData = data;\\n\",\"status\":\"modified\"},{\"filename\":\"src/providers/memory-check-provider.ts\",\"additions\":1,\"deletions\":1,\"changes\":40,\"patch\":\"diff --git a/src/providers/memory-check-provider.ts b/src/providers/memory-check-provider.ts\\nindex aee629c5..dca92621 100644\\n--- a/src/providers/memory-check-provider.ts\\n+++ b/src/providers/memory-check-provider.ts\\n@@ -381,34 +381,36 @@ export class MemoryCheckProvider extends CheckProvider {\\n     try {\\n       if (\\n         (config as any).checkName === 'aggregate-validations' ||\\n-        (config as any).checkName === 'aggregate' ||\\n         (config as any).checkName === 'aggregate'\\n       ) {\\n-        const hist = (enhancedContext as any)?.outputs?.history || {};\\n-        const keys = Object.keys(hist);\\n-        console.log('[MemoryProvider]', (config as any).checkName, ': history keys =', keys);\\n-        const vf = (hist as any)['validate-fact'];\\n-        console.log(\\n-          '[MemoryProvider]',\\n-          (config as any).checkName,\\n-          ': validate-fact history length =',\\n-          Array.isArray(vf) ? vf.length : 'n/a'\\n-        );\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          const hist = (enhancedContext as any)?.outputs?.history || {};\\n+          const keys = Object.keys(hist);\\n+          logger.debug(\\n+            `[MemoryProvider] ${(config as any).checkName}: history keys = [${keys.join(', ')}]`\\n+          );\\n+          const vf = (hist as any)['validate-fact'];\\n+          logger.debug(\\n+            `[MemoryProvider] ${(config as any).checkName}: validate-fact history length = ${\\n+              Array.isArray(vf) ? vf.length : 'n/a'\\n+            }`\\n+          );\\n+        }\\n       }\\n     } catch {}\\n \\n     const result = this.evaluateJavaScriptBlock(script, enhancedContext);\\n     try {\\n-      if ((config as any).checkName === 'aggregate-validations') {\\n+      if (\\n+        (config as any).checkName === 'aggregate-validations' &&\\n+        process.env.VISOR_DEBUG === 'true'\\n+      ) {\\n         const tv = store.get('total_validations', 'fact-validation');\\n         const av = store.get('all_valid', 'fact-validation');\\n-        console.error(\\n-          '[MemoryProvider] post-exec',\\n-          (config as any).checkName,\\n-          'total_validations=',\\n-          tv,\\n-          'all_valid=',\\n-          av\\n+        logger.debug(\\n+          `[MemoryProvider] post-exec ${(config as any).checkName} total_validations=${String(\\n+            tv\\n+          )} all_valid=${String(av)}`\\n         );\\n       }\\n     } catch {}\\n\",\"status\":\"modified\"},{\"filename\":\"src/test-runner/assertions.ts\",\"additions\":4,\"deletions\":0,\"changes\":91,\"patch\":\"diff --git a/src/test-runner/assertions.ts b/src/test-runner/assertions.ts\\nnew file mode 100644\\nindex 00000000..ea676eea\\n--- /dev/null\\n+++ b/src/test-runner/assertions.ts\\n@@ -0,0 +1,91 @@\\n+export type CountExpectation = {\\n+  exactly?: number;\\n+  at_least?: number;\\n+  at_most?: number;\\n+};\\n+\\n+export interface CallsExpectation extends CountExpectation {\\n+  step?: string;\\n+  provider?: 'github' | string;\\n+  op?: string;\\n+  args?: Record<string, unknown>;\\n+}\\n+\\n+export interface PromptsExpectation {\\n+  step: string;\\n+  index?: number | 'first' | 'last';\\n+  contains?: string[];\\n+  not_contains?: string[];\\n+  matches?: string; // regex string\\n+  where?: {\\n+    contains?: string[];\\n+    not_contains?: string[];\\n+    matches?: string; // regex\\n+  };\\n+}\\n+\\n+export interface OutputsExpectation {\\n+  step: string;\\n+  index?: number | 'first' | 'last';\\n+  path: string;\\n+  equals?: unknown;\\n+  equalsDeep?: unknown;\\n+  matches?: string; // regex\\n+  where?: {\\n+    path: string;\\n+    equals?: unknown;\\n+    matches?: string; // regex\\n+  };\\n+  contains_unordered?: unknown[]; // array membership ignoring order\\n+}\\n+\\n+export interface ExpectBlock {\\n+  use?: string[];\\n+  calls?: CallsExpectation[];\\n+  prompts?: PromptsExpectation[];\\n+  outputs?: OutputsExpectation[];\\n+  no_calls?: Array<{ step?: string; provider?: string; op?: string }>;\\n+  fail?: { message_contains?: string };\\n+  strict_violation?: { for_step?: string; message_contains?: string };\\n+}\\n+\\n+export function validateCounts(exp: CountExpectation): void {\\n+  const keys = ['exactly', 'at_least', 'at_most'].filter(k => (exp as any)[k] !== undefined);\\n+  if (keys.length > 1) {\\n+    throw new Error(`Count expectation is ambiguous: ${keys.join(', ')}`);\\n+  }\\n+}\\n+\\n+export function deepEqual(a: unknown, b: unknown): boolean {\\n+  if (a === b) return true;\\n+  if (typeof a !== typeof b) return false;\\n+  if (a && b && typeof a === 'object') {\\n+    if (Array.isArray(a) && Array.isArray(b)) {\\n+      if (a.length !== b.length) return false;\\n+      for (let i = 0; i < a.length; i++) if (!deepEqual(a[i], b[i])) return false;\\n+      return true;\\n+    }\\n+    const ak = Object.keys(a as any).sort();\\n+    const bk = Object.keys(b as any).sort();\\n+    if (!deepEqual(ak, bk)) return false;\\n+    for (const k of ak) if (!deepEqual((a as any)[k], (b as any)[k])) return false;\\n+    return true;\\n+  }\\n+  return false;\\n+}\\n+\\n+export function containsUnordered(haystack: unknown[], needles: unknown[]): boolean {\\n+  if (!Array.isArray(haystack) || !Array.isArray(needles)) return false;\\n+  const used = new Array(haystack.length).fill(false);\\n+  outer: for (const n of needles) {\\n+    for (let i = 0; i < haystack.length; i++) {\\n+      if (used[i]) continue;\\n+      if (deepEqual(haystack[i], n) || haystack[i] === n) {\\n+        used[i] = true;\\n+        continue outer;\\n+      }\\n+    }\\n+    return false;\\n+  }\\n+  return true;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/fixture-loader.ts\",\"additions\":6,\"deletions\":0,\"changes\":156,\"patch\":\"diff --git a/src/test-runner/fixture-loader.ts b/src/test-runner/fixture-loader.ts\\nnew file mode 100644\\nindex 00000000..7ec38d15\\n--- /dev/null\\n+++ b/src/test-runner/fixture-loader.ts\\n@@ -0,0 +1,156 @@\\n+export type BuiltinFixtureName =\\n+  | 'gh.pr_open.minimal'\\n+  | 'gh.pr_sync.minimal'\\n+  | 'gh.issue_open.minimal'\\n+  | 'gh.issue_comment.standard'\\n+  | 'gh.issue_comment.visor_help'\\n+  | 'gh.issue_comment.visor_regenerate'\\n+  | 'gh.issue_comment.edited'\\n+  | 'gh.pr_closed.minimal';\\n+\\n+export interface LoadedFixture {\\n+  name: string;\\n+  webhook: { name: string; action?: string; payload: Record<string, unknown> };\\n+  git?: { branch?: string; baseBranch?: string };\\n+  files?: Array<{\\n+    path: string;\\n+    content: string;\\n+    status?: 'added' | 'modified' | 'removed' | 'renamed';\\n+    additions?: number;\\n+    deletions?: number;\\n+  }>;\\n+  diff?: string; // unified diff text\\n+  env?: Record<string, string>;\\n+  time?: { now?: string };\\n+}\\n+\\n+export class FixtureLoader {\\n+  load(name: BuiltinFixtureName): LoadedFixture {\\n+    // Minimal, stable, general-purpose fixtures used by the test runner.\\n+    // All fixtures supply a webhook payload and, for PR variants, a small diff.\\n+    if (name.startsWith('gh.pr_open')) {\\n+      const files: LoadedFixture['files'] = [\\n+        {\\n+          path: 'src/search.ts',\\n+          content: 'export function search(q: string) {\\\\n  return []\\\\n}\\\\n',\\n+          status: 'added',\\n+          additions: 3,\\n+          deletions: 0,\\n+        },\\n+      ];\\n+      const diff = this.buildUnifiedDiff(files);\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'pull_request',\\n+          action: 'opened',\\n+          payload: { pull_request: { number: 1, title: 'feat: add user search' } },\\n+        },\\n+        git: { branch: 'feature/test', baseBranch: 'main' },\\n+        files,\\n+        diff,\\n+      };\\n+    }\\n+    if (name.startsWith('gh.pr_sync')) {\\n+      const files: LoadedFixture['files'] = [\\n+        {\\n+          path: 'src/search.ts',\\n+          content: 'export function search(q: string) {\\\\n  return [q] // updated\\\\n}\\\\n',\\n+          status: 'modified',\\n+          additions: 1,\\n+          deletions: 0,\\n+        },\\n+      ];\\n+      const diff = this.buildUnifiedDiff(files);\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'pull_request',\\n+          action: 'synchronize',\\n+          payload: { pull_request: { number: 1, title: 'feat: add user search (update)' } },\\n+        },\\n+        git: { branch: 'feature/test', baseBranch: 'main' },\\n+        files,\\n+        diff,\\n+      };\\n+    }\\n+    if (name.startsWith('gh.issue_open')) {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issues',\\n+          action: 'opened',\\n+          payload: {\\n+            issue: { number: 12, title: 'Bug: crashes on search edge case', body: 'Steps...' },\\n+          },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.issue_comment.standard') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issue_comment',\\n+          action: 'created',\\n+          payload: { comment: { body: 'Thanks for the update!' }, issue: { number: 1 } },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.issue_comment.visor_help') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issue_comment',\\n+          action: 'created',\\n+          payload: { comment: { body: '/visor help' }, issue: { number: 1 } },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.issue_comment.visor_regenerate') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'issue_comment',\\n+          action: 'created',\\n+          payload: { comment: { body: '/visor Regenerate reviews' }, issue: { number: 1 } },\\n+        },\\n+      };\\n+    }\\n+    if (name === 'gh.pr_closed.minimal') {\\n+      return {\\n+        name,\\n+        webhook: {\\n+          name: 'pull_request',\\n+          action: 'closed',\\n+          payload: { pull_request: { number: 1, title: 'feat: add user search' } },\\n+        },\\n+      };\\n+    }\\n+    // Fallback minimal\\n+    return {\\n+      name,\\n+      webhook: { name: 'unknown', payload: {} },\\n+    };\\n+  }\\n+\\n+  private buildUnifiedDiff(\\n+    files: Array<{ path: string; content: string; status?: string }>\\n+  ): string {\\n+    // Build a very small, stable unified diff suitable for prompts\\n+    const chunks = files.map(f => {\\n+      const header =\\n+        `diff --git a/${f.path} b/${f.path}\\\\n` +\\n+        (f.status === 'added'\\n+          ? 'index 0000000..1111111 100644\\\\n--- /dev/null\\\\n'\\n+          : `index 1111111..2222222 100644\\\\n--- a/${f.path}\\\\n`) +\\n+        `+++ b/${f.path}\\\\n` +\\n+        '@@\\\\n';\\n+      const body = f.content\\n+        .split('\\\\n')\\n+        .map(line => (f.status === 'removed' ? `-${line}` : `+${line}`))\\n+        .join('\\\\n');\\n+      return header + body + '\\\\n';\\n+    });\\n+    return chunks.join('\\\\n');\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/index.ts\",\"additions\":53,\"deletions\":0,\"changes\":1527,\"patch\":\"diff --git a/src/test-runner/index.ts b/src/test-runner/index.ts\\nnew file mode 100644\\nindex 00000000..cbefc162\\n--- /dev/null\\n+++ b/src/test-runner/index.ts\\n@@ -0,0 +1,1527 @@\\n+import fs from 'fs';\\n+import path from 'path';\\n+import * as yaml from 'js-yaml';\\n+\\n+import { ConfigManager } from '../config';\\n+import { CheckExecutionEngine } from '../check-execution-engine';\\n+import type { PRInfo } from '../pr-analyzer';\\n+import { RecordingOctokit } from './recorders/github-recorder';\\n+import { setGlobalRecorder } from './recorders/global-recorder';\\n+import { FixtureLoader } from './fixture-loader';\\n+import { validateCounts, type ExpectBlock } from './assertions';\\n+import { validateTestsDoc } from './validator';\\n+\\n+export type TestCase = {\\n+  name: string;\\n+  description?: string;\\n+  event?: string;\\n+  flow?: Array<{ name: string }>;\\n+};\\n+\\n+export type TestSuite = {\\n+  version: string;\\n+  extends?: string | string[];\\n+  tests: {\\n+    defaults?: Record<string, unknown>;\\n+    fixtures?: unknown[];\\n+    cases: TestCase[];\\n+  };\\n+};\\n+\\n+export interface DiscoverOptions {\\n+  testsPath?: string; // Path to .visor.tests.yaml\\n+  cwd?: string;\\n+}\\n+\\n+function isObject(v: unknown): v is Record<string, unknown> {\\n+  return !!v && typeof v === 'object' && !Array.isArray(v);\\n+}\\n+\\n+export class VisorTestRunner {\\n+  constructor(private readonly cwd: string = process.cwd()) {}\\n+\\n+  private line(title = '', char = '─', width = 60): string {\\n+    if (!title) return char.repeat(width);\\n+    const pad = Math.max(1, width - title.length - 2);\\n+    return `${char.repeat(2)} ${title} ${char.repeat(pad)}`;\\n+  }\\n+\\n+  private printCaseHeader(name: string, kind: 'flow' | 'single', event?: string): void {\\n+    console.log('\\\\n' + this.line(`Case: ${name}`));\\n+    const meta: string[] = [`type=${kind}`];\\n+    if (event) meta.push(`event=${event}`);\\n+    console.log(`  ${meta.join('  ·  ')}`);\\n+  }\\n+\\n+  private printStageHeader(\\n+    flowName: string,\\n+    stageName: string,\\n+    event?: string,\\n+    fixture?: string\\n+  ): void {\\n+    console.log('\\\\n' + this.line(`${flowName} — ${stageName}`));\\n+    const meta: string[] = [];\\n+    if (event) meta.push(`event=${event}`);\\n+    if (fixture) meta.push(`fixture=${fixture}`);\\n+    if (meta.length) console.log(`  ${meta.join('  ·  ')}`);\\n+  }\\n+\\n+  private printSelectedChecks(checks: string[]): void {\\n+    if (!checks || checks.length === 0) return;\\n+    console.log(`  checks: ${checks.join(', ')}`);\\n+  }\\n+\\n+  /**\\n+   * Locate a tests file: explicit path > ./.visor.tests.yaml > defaults/.visor.tests.yaml\\n+   */\\n+  public resolveTestsPath(explicit?: string): string {\\n+    if (explicit) {\\n+      return path.isAbsolute(explicit) ? explicit : path.resolve(this.cwd, explicit);\\n+    }\\n+    const candidates = [\\n+      path.resolve(this.cwd, '.visor.tests.yaml'),\\n+      path.resolve(this.cwd, '.visor.tests.yml'),\\n+      path.resolve(this.cwd, 'defaults/.visor.tests.yaml'),\\n+      path.resolve(this.cwd, 'defaults/.visor.tests.yml'),\\n+    ];\\n+    for (const p of candidates) {\\n+      if (fs.existsSync(p)) return p;\\n+    }\\n+    throw new Error(\\n+      'No tests file found. Provide --config <path> or add .visor.tests.yaml (or defaults/.visor.tests.yaml).'\\n+    );\\n+  }\\n+\\n+  /**\\n+   * Load and minimally validate tests YAML.\\n+   */\\n+  public loadSuite(testsPath: string): TestSuite {\\n+    const raw = fs.readFileSync(testsPath, 'utf8');\\n+    const doc = yaml.load(raw) as unknown;\\n+    const validation = validateTestsDoc(doc);\\n+    if (!validation.ok) {\\n+      const errs = validation.errors.map(e => ` - ${e}`).join('\\\\n');\\n+      throw new Error(`Tests file validation failed:\\\\n${errs}`);\\n+    }\\n+    if (!isObject(doc)) throw new Error('Tests YAML must be a YAML object');\\n+\\n+    const version = String((doc as any).version ?? '1.0');\\n+    const tests = (doc as any).tests;\\n+    if (!tests || !isObject(tests)) throw new Error('tests: {} section is required');\\n+    const cases = (tests as any).cases as unknown;\\n+    if (!Array.isArray(cases) || cases.length === 0) {\\n+      throw new Error('tests.cases must be a non-empty array');\\n+    }\\n+\\n+    // Preserve full case objects for execution; discovery prints selective fields\\n+    const suite: TestSuite = {\\n+      version,\\n+      extends: (doc as any).extends,\\n+      tests: {\\n+        defaults: (tests as any).defaults || {},\\n+        fixtures: (tests as any).fixtures || [],\\n+        cases: (tests as any).cases,\\n+      },\\n+    };\\n+    return suite;\\n+  }\\n+\\n+  /**\\n+   * Pretty print discovered cases to stdout.\\n+   */\\n+  public printDiscovery(testsPath: string, suite: TestSuite): void {\\n+    const rel = path.relative(this.cwd, testsPath) || testsPath;\\n+    console.log('🧪 Visor Test Runner — discovery mode');\\n+    console.log(`   Suite: ${rel}`);\\n+    const parent = suite.extends\\n+      ? Array.isArray(suite.extends)\\n+        ? suite.extends.join(', ')\\n+        : String(suite.extends)\\n+      : '(none)';\\n+    console.log(`   Extends: ${parent}`);\\n+    const defaults = suite.tests.defaults || {};\\n+    const strict = (defaults as any).strict === undefined ? true : !!(defaults as any).strict;\\n+    console.log(`   Strict: ${strict ? 'on' : 'off'}`);\\n+\\n+    // List cases\\n+    console.log('\\\\nCases:');\\n+    for (const c of suite.tests.cases) {\\n+      const isFlow = Array.isArray(c.flow) && c.flow.length > 0;\\n+      const badge = isFlow ? 'flow' : c.event || 'event';\\n+      console.log(` - ${c.name} [${badge}]`);\\n+    }\\n+    console.log('\\\\nTip: run `visor test --only <name>` to filter, `--bail` to stop early.');\\n+  }\\n+\\n+  /**\\n+   * Execute non-flow cases with minimal assertions (Milestone 1 MVP).\\n+   */\\n+  public async runCases(\\n+    testsPath: string,\\n+    suite: TestSuite,\\n+    options: { only?: string; bail?: boolean; maxParallel?: number; promptMaxChars?: number }\\n+  ): Promise<{\\n+    failures: number;\\n+    results: Array<{\\n+      name: string;\\n+      passed: boolean;\\n+      errors?: string[];\\n+      stages?: Array<{ name: string; errors?: string[] }>;\\n+    }>;\\n+  }> {\\n+    // Save defaults for flow runner access\\n+    (this as any).suiteDefaults = suite.tests.defaults || {};\\n+    // Support --only \\\"case\\\" and --only \\\"case#stage\\\"\\n+    let onlyCase = options.only?.toLowerCase();\\n+    let stageFilter: string | undefined;\\n+    if (onlyCase && onlyCase.includes('#')) {\\n+      const parts = onlyCase.split('#');\\n+      onlyCase = parts[0];\\n+      stageFilter = (parts[1] || '').trim();\\n+    }\\n+    const allCases = suite.tests.cases;\\n+    const selected = onlyCase\\n+      ? allCases.filter(c => c.name.toLowerCase().includes(onlyCase as string))\\n+      : allCases;\\n+    if (selected.length === 0) {\\n+      console.log('No matching cases.');\\n+      return { failures: 0, results: [] };\\n+    }\\n+\\n+    // Load merged config via ConfigManager (honors extends), then clone for test overrides\\n+    const cm = new ConfigManager();\\n+    // Prefer loading the base config referenced by extends; fall back to the tests file\\n+    let configFileToLoad = testsPath;\\n+    const parentExt = suite.extends;\\n+    if (parentExt) {\\n+      const first = Array.isArray(parentExt) ? parentExt[0] : parentExt;\\n+      if (typeof first === 'string') {\\n+        const resolved = path.isAbsolute(first)\\n+          ? first\\n+          : path.resolve(path.dirname(testsPath), first);\\n+        configFileToLoad = resolved;\\n+      }\\n+    }\\n+    const config = await cm.loadConfig(configFileToLoad, { validate: true, mergeDefaults: true });\\n+    if (!config.checks) {\\n+      throw new Error('Loaded config has no checks; cannot run tests');\\n+    }\\n+\\n+    const defaultsAny: any = suite.tests.defaults || {};\\n+    const defaultStrict = defaultsAny?.strict !== false;\\n+    const aiProviderDefault = defaultsAny?.ai_provider || 'mock';\\n+    const ghRec = defaultsAny?.github_recorder as\\n+      | { error_code?: number; timeout_ms?: number }\\n+      | undefined;\\n+    const defaultPromptCap: number | undefined =\\n+      options.promptMaxChars ||\\n+      (typeof defaultsAny?.prompt_max_chars === 'number'\\n+        ? defaultsAny.prompt_max_chars\\n+        : undefined);\\n+    const caseMaxParallel =\\n+      options.maxParallel ||\\n+      (typeof defaultsAny?.max_parallel === 'number' ? defaultsAny.max_parallel : undefined) ||\\n+      1;\\n+\\n+    // Test overrides: force AI provider to 'mock' when requested (default: mock per RFC)\\n+    const cfg = JSON.parse(JSON.stringify(config));\\n+    for (const name of Object.keys(cfg.checks || {})) {\\n+      const chk = cfg.checks[name] || {};\\n+      if ((chk.type || 'ai') === 'ai') {\\n+        const prev = (chk.ai || {}) as Record<string, unknown>;\\n+        chk.ai = {\\n+          ...prev,\\n+          provider: aiProviderDefault,\\n+          skip_code_context: true,\\n+          disable_tools: true,\\n+          timeout: Math.min(15000, (prev.timeout as number) || 15000),\\n+        } as any;\\n+        cfg.checks[name] = chk;\\n+      }\\n+    }\\n+\\n+    let failures = 0;\\n+    const caseResults: Array<{\\n+      name: string;\\n+      passed: boolean;\\n+      errors?: string[];\\n+      stages?: Array<{ name: string; errors?: string[] }>;\\n+    }> = [];\\n+    // Header: show suite path for clarity\\n+    try {\\n+      const rel = path.relative(this.cwd, testsPath) || testsPath;\\n+      console.log(`Suite: ${rel}`);\\n+    } catch {}\\n+\\n+    const runOne = async (_case: any): Promise<{ name: string; failed: number }> => {\\n+      // Case header for clarity\\n+      const isFlow = Array.isArray((_case as any).flow) && (_case as any).flow.length > 0;\\n+      const caseEvent = (_case as any).event as string | undefined;\\n+      this.printCaseHeader(\\n+        (_case as any).name || '(unnamed)',\\n+        isFlow ? 'flow' : 'single',\\n+        caseEvent\\n+      );\\n+      if ((_case as any).skip) {\\n+        console.log(`⏭ SKIP ${(_case as any).name}`);\\n+        caseResults.push({ name: _case.name, passed: true });\\n+        return { name: _case.name, failed: 0 };\\n+      }\\n+      if (Array.isArray((_case as any).flow) && (_case as any).flow.length > 0) {\\n+        const flowRes = await this.runFlowCase(\\n+          _case,\\n+          cfg,\\n+          defaultStrict,\\n+          options.bail || false,\\n+          defaultPromptCap,\\n+          stageFilter\\n+        );\\n+        const failed = flowRes.failures;\\n+        caseResults.push({ name: _case.name, passed: failed === 0, stages: flowRes.stages });\\n+        return { name: _case.name, failed };\\n+      }\\n+      const strict = (\\n+        typeof (_case as any).strict === 'boolean' ? (_case as any).strict : defaultStrict\\n+      ) as boolean;\\n+      const expect = ((_case as any).expect || {}) as ExpectBlock;\\n+      // Fixture selection with optional overrides\\n+      const fixtureInput =\\n+        typeof (_case as any).fixture === 'object' && (_case as any).fixture\\n+          ? (_case as any).fixture\\n+          : { builtin: (_case as any).fixture };\\n+      const prInfo = this.buildPrInfoFromFixture(fixtureInput?.builtin, fixtureInput?.overrides);\\n+\\n+      // Inject recording Octokit into engine via actionContext using env owner/repo\\n+      const prevRepo = process.env.GITHUB_REPOSITORY;\\n+      process.env.GITHUB_REPOSITORY = process.env.GITHUB_REPOSITORY || 'owner/repo';\\n+      // Apply case env overrides if present\\n+      const envOverrides =\\n+        typeof (_case as any).env === 'object' && (_case as any).env\\n+          ? ((_case as any).env as Record<string, string>)\\n+          : undefined;\\n+      const prevEnv: Record<string, string | undefined> = {};\\n+      if (envOverrides) {\\n+        for (const [k, v] of Object.entries(envOverrides)) {\\n+          prevEnv[k] = process.env[k];\\n+          process.env[k] = String(v);\\n+        }\\n+      }\\n+      const ghRecCase =\\n+        typeof (_case as any).github_recorder === 'object' && (_case as any).github_recorder\\n+          ? ((_case as any).github_recorder as { error_code?: number; timeout_ms?: number })\\n+          : undefined;\\n+      const rcOpts = ghRecCase || ghRec;\\n+      const recorder = new RecordingOctokit(\\n+        rcOpts ? { errorCode: rcOpts.error_code, timeoutMs: rcOpts.timeout_ms } : undefined\\n+      );\\n+      setGlobalRecorder(recorder);\\n+      const engine = new CheckExecutionEngine(undefined as any, recorder as unknown as any);\\n+\\n+      // Capture prompts per step\\n+      const prompts: Record<string, string[]> = {};\\n+      const mocks =\\n+        typeof (_case as any).mocks === 'object' && (_case as any).mocks\\n+          ? ((_case as any).mocks as Record<string, unknown>)\\n+          : {};\\n+      const mockCursors: Record<string, number> = {};\\n+      engine.setExecutionContext({\\n+        hooks: {\\n+          onPromptCaptured: (info: { step: string; provider: string; prompt: string }) => {\\n+            const k = info.step;\\n+            if (!prompts[k]) prompts[k] = [];\\n+            const p =\\n+              defaultPromptCap && info.prompt.length > defaultPromptCap\\n+                ? info.prompt.slice(0, defaultPromptCap)\\n+                : info.prompt;\\n+            prompts[k].push(p);\\n+          },\\n+          mockForStep: (step: string) => {\\n+            // Support list form: '<step>[]' means per-call mocks for forEach children\\n+            const listKey = `${step}[]`;\\n+            const list = (mocks as any)[listKey];\\n+            if (Array.isArray(list)) {\\n+              const i = mockCursors[listKey] || 0;\\n+              const idx = i < list.length ? i : list.length - 1; // clamp to last\\n+              mockCursors[listKey] = i + 1;\\n+              return list[idx];\\n+            }\\n+            return (mocks as any)[step];\\n+          },\\n+        },\\n+      } as any);\\n+\\n+      try {\\n+        const eventForCase = this.mapEventFromFixtureName(fixtureInput?.builtin);\\n+        const desiredSteps = new Set<string>(\\n+          (expect.calls || []).map(c => c.step).filter(Boolean) as string[]\\n+        );\\n+        let checksToRun = this.computeChecksToRun(\\n+          cfg,\\n+          eventForCase,\\n+          desiredSteps.size > 0 ? desiredSteps : undefined\\n+        );\\n+        this.printSelectedChecks(checksToRun);\\n+        if (checksToRun.length === 0) {\\n+          // Fallback: run all checks for this event when filtered set is empty\\n+          checksToRun = this.computeChecksToRun(cfg, eventForCase, undefined);\\n+        }\\n+        // Include all tagged checks by default in test mode: build tagFilter.include = union of all tags\\n+        // Do not pass an implicit tag filter during tests.\\n+        // Passing all known tags as an include-filter would exclude untagged steps.\\n+        // Let the engine apply whatever tag_filter the config already defines (if any).\\n+        const allTags: string[] = [];\\n+        // Inject octokit into eventContext so providers can perform real GitHub ops (recorded)\\n+        try {\\n+          (prInfo as any).eventContext = { ...(prInfo as any).eventContext, octokit: recorder };\\n+        } catch {}\\n+\\n+        const prevTestMode = process.env.VISOR_TEST_MODE;\\n+        process.env.VISOR_TEST_MODE = 'true';\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.log(`  ⮕ executing main stage with checks=[${checksToRun.join(', ')}]`);\\n+        }\\n+        let res = await engine.executeGroupedChecks(\\n+          prInfo,\\n+          checksToRun,\\n+          120000,\\n+          cfg,\\n+          'json',\\n+          process.env.VISOR_DEBUG === 'true',\\n+          undefined,\\n+          false,\\n+          {}\\n+        );\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          try {\\n+            const names = (res.statistics.checks || []).map(\\n+              (c: any) => `${c.checkName}:${c.totalRuns || 0}`\\n+            );\\n+            console.log(`  ⮕ main stats: [${names.join(', ')}]`);\\n+          } catch {}\\n+        }\\n+        try {\\n+          const dbgHist = engine.getOutputHistorySnapshot();\\n+          console.log(\\n+            `  ⮕ stage base history keys: ${Object.keys(dbgHist).join(', ') || '(none)'}`\\n+          );\\n+        } catch {}\\n+        // After main stage run, ensure static on_finish.run targets for forEach parents executed.\\n+        try {\\n+          const hist0 = engine.getOutputHistorySnapshot();\\n+          if (process.env.VISOR_DEBUG === 'true') {\\n+            console.log(`  ⮕ history keys: ${Object.keys(hist0).join(', ') || '(none)'}`);\\n+          }\\n+          const parents = Object.entries(cfg.checks || {})\\n+            .filter(\\n+              ([name, c]: [string, any]) =>\\n+                checksToRun.includes(name) &&\\n+                c &&\\n+                c.forEach &&\\n+                c.on_finish &&\\n+                Array.isArray(c.on_finish.run) &&\\n+                c.on_finish.run.length > 0\\n+            )\\n+            .map(([name, c]: [string, any]) => ({ name, onFinish: c.on_finish }));\\n+          if (process.env.VISOR_DEBUG === 'true') {\\n+            console.log(\\n+              `  ⮕ forEach parents with on_finish: ${parents.map(p => p.name).join(', ') || '(none)'}`\\n+            );\\n+          }\\n+          const missing: string[] = [];\\n+          for (const p of parents) {\\n+            for (const t of p.onFinish.run as string[]) {\\n+              if (!hist0[t] || (Array.isArray(hist0[t]) && hist0[t].length === 0)) {\\n+                missing.push(t);\\n+              }\\n+            }\\n+          }\\n+          // Dedup missing and exclude anything already in checksToRun\\n+          const toRun = Array.from(new Set(missing.filter(n => !checksToRun.includes(n))));\\n+          if (toRun.length > 0) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ⮕ on_finish.fallback: running [${toRun.join(', ')}]`);\\n+            }\\n+            // Run once; reuse same engine instance so output history stays visible\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ⮕ executing on_finish.fallback with checks=[${toRun.join(', ')}]`);\\n+            }\\n+            const fallbackRes = await engine.executeGroupedChecks(\\n+              prInfo,\\n+              toRun,\\n+              120000,\\n+              cfg,\\n+              'json',\\n+              process.env.VISOR_DEBUG === 'true',\\n+              undefined,\\n+              false,\\n+              {}\\n+            );\\n+            // Optionally merge statistics (for stage coverage we rely on deltas + stats from last run)\\n+            res = {\\n+              results: fallbackRes.results || res.results,\\n+              statistics: fallbackRes.statistics || res.statistics,\\n+            } as any;\\n+          }\\n+        } catch {}\\n+        if (prevTestMode === undefined) delete process.env.VISOR_TEST_MODE;\\n+        else process.env.VISOR_TEST_MODE = prevTestMode;\\n+        const outHistory = engine.getOutputHistorySnapshot();\\n+\\n+        const caseFailures = this.evaluateCase(\\n+          _case.name,\\n+          res.statistics,\\n+          recorder,\\n+          expect,\\n+          strict,\\n+          prompts,\\n+          res.results,\\n+          outHistory\\n+        );\\n+        // Warn about unmocked AI/command steps that executed\\n+        try {\\n+          const mocksUsed =\\n+            typeof (_case as any).mocks === 'object' && (_case as any).mocks\\n+              ? ((_case as any).mocks as Record<string, unknown>)\\n+              : {};\\n+          this.warnUnmockedProviders(res.statistics, cfg, mocksUsed);\\n+        } catch {}\\n+        this.printCoverage(_case.name, res.statistics, expect);\\n+        if (caseFailures.length === 0) {\\n+          console.log(`✅ PASS ${_case.name}`);\\n+          caseResults.push({ name: _case.name, passed: true });\\n+        } else {\\n+          console.log(`❌ FAIL ${_case.name}`);\\n+          for (const f of caseFailures) console.log(`   - ${f}`);\\n+          caseResults.push({ name: _case.name, passed: false, errors: caseFailures });\\n+          return { name: _case.name, failed: 1 };\\n+        }\\n+      } catch (err) {\\n+        console.log(`❌ ERROR ${_case.name}: ${err instanceof Error ? err.message : String(err)}`);\\n+        caseResults.push({\\n+          name: _case.name,\\n+          passed: false,\\n+          errors: [err instanceof Error ? err.message : String(err)],\\n+        });\\n+        return { name: _case.name, failed: 1 };\\n+      } finally {\\n+        if (prevRepo === undefined) delete process.env.GITHUB_REPOSITORY;\\n+        else process.env.GITHUB_REPOSITORY = prevRepo;\\n+        if (envOverrides) {\\n+          for (const [k, oldv] of Object.entries(prevEnv)) {\\n+            if (oldv === undefined) delete process.env[k];\\n+            else process.env[k] = oldv;\\n+          }\\n+        }\\n+      }\\n+      return { name: _case.name, failed: 0 };\\n+    };\\n+\\n+    if (options.bail || false || caseMaxParallel <= 1) {\\n+      for (const _case of selected) {\\n+        const r = await runOne(_case);\\n+        failures += r.failed;\\n+        if (options.bail && r.failed > 0) break;\\n+      }\\n+    } else {\\n+      let idx = 0;\\n+      const workers = Math.min(caseMaxParallel, selected.length);\\n+      const runWorker = async () => {\\n+        while (true) {\\n+          const i = idx++;\\n+          if (i >= selected.length) return;\\n+          const r = await runOne(selected[i]);\\n+          failures += r.failed;\\n+        }\\n+      };\\n+      await Promise.all(Array.from({ length: workers }, runWorker));\\n+    }\\n+\\n+    // Summary\\n+    const passed = selected.length - failures;\\n+    console.log(`\\\\nSummary: ${passed}/${selected.length} passed`);\\n+    return { failures, results: caseResults };\\n+  }\\n+\\n+  private async runFlowCase(\\n+    flowCase: any,\\n+    cfg: any,\\n+    defaultStrict: boolean,\\n+    bail: boolean,\\n+    promptCap?: number,\\n+    stageFilter?: string\\n+  ): Promise<{ failures: number; stages: Array<{ name: string; errors?: string[] }> }> {\\n+    const suiteDefaults: any = (this as any).suiteDefaults || {};\\n+    const ghRec = suiteDefaults.github_recorder as\\n+      | { error_code?: number; timeout_ms?: number }\\n+      | undefined;\\n+    const ghRecCase =\\n+      typeof (flowCase as any).github_recorder === 'object' && (flowCase as any).github_recorder\\n+        ? ((flowCase as any).github_recorder as { error_code?: number; timeout_ms?: number })\\n+        : undefined;\\n+    const rcOpts = ghRecCase || ghRec;\\n+    const recorder = new RecordingOctokit(\\n+      rcOpts ? { errorCode: rcOpts.error_code, timeoutMs: rcOpts.timeout_ms } : undefined\\n+    );\\n+    setGlobalRecorder(recorder);\\n+    const engine = new CheckExecutionEngine(undefined as any, recorder as unknown as any);\\n+    const flowName = flowCase.name || 'flow';\\n+    let failures = 0;\\n+    const stagesSummary: Array<{ name: string; errors?: string[] }> = [];\\n+\\n+    // Shared prompts map across flow; we will compute per-stage deltas\\n+    const prompts: Record<string, string[]> = {};\\n+    let stageMocks: Record<string, unknown> =\\n+      typeof flowCase.mocks === 'object' && flowCase.mocks\\n+        ? (flowCase.mocks as Record<string, unknown>)\\n+        : {};\\n+    let stageMockCursors: Record<string, number> = {};\\n+    engine.setExecutionContext({\\n+      hooks: {\\n+        onPromptCaptured: (info: { step: string; provider: string; prompt: string }) => {\\n+          const k = info.step;\\n+          if (!prompts[k]) prompts[k] = [];\\n+          const p =\\n+            promptCap && info.prompt.length > promptCap\\n+              ? info.prompt.slice(0, promptCap)\\n+              : info.prompt;\\n+          prompts[k].push(p);\\n+        },\\n+        mockForStep: (step: string) => {\\n+          const listKey = `${step}[]`;\\n+          const list = (stageMocks as any)[listKey];\\n+          if (Array.isArray(list)) {\\n+            const i = stageMockCursors[listKey] || 0;\\n+            const idx = i < list.length ? i : list.length - 1;\\n+            stageMockCursors[listKey] = i + 1;\\n+            return list[idx];\\n+          }\\n+          return (stageMocks as any)[step];\\n+        },\\n+      },\\n+    } as any);\\n+\\n+    // Run each stage\\n+    // Normalize stage filter\\n+    const sf = (stageFilter || '').trim().toLowerCase();\\n+    const sfIndex = sf && /^\\\\d+$/.test(sf) ? parseInt(sf, 10) : undefined;\\n+    let anyStageRan = false;\\n+    for (let i = 0; i < flowCase.flow.length; i++) {\\n+      const stage = flowCase.flow[i];\\n+      const stageName = `${flowName}#${stage.name || `stage-${i + 1}`}`;\\n+      // Apply stage filter if provided: match by name substring or 1-based index\\n+      if (sf) {\\n+        const nm = String(stage.name || `stage-${i + 1}`).toLowerCase();\\n+        const idxMatch = sfIndex !== undefined && sfIndex === i + 1;\\n+        const nameMatch = nm.includes(sf);\\n+        if (!(idxMatch || nameMatch)) continue;\\n+      }\\n+      anyStageRan = true;\\n+      const strict = (\\n+        typeof flowCase.strict === 'boolean' ? flowCase.strict : defaultStrict\\n+      ) as boolean;\\n+\\n+      // Fixture + env\\n+      const fixtureInput =\\n+        typeof stage.fixture === 'object' && stage.fixture\\n+          ? stage.fixture\\n+          : { builtin: stage.fixture };\\n+      const prInfo = this.buildPrInfoFromFixture(fixtureInput?.builtin, fixtureInput?.overrides);\\n+\\n+      // Stage env overrides\\n+      const envOverrides =\\n+        typeof stage.env === 'object' && stage.env\\n+          ? (stage.env as Record<string, string>)\\n+          : undefined;\\n+      const prevEnv: Record<string, string | undefined> = {};\\n+      if (envOverrides) {\\n+        for (const [k, v] of Object.entries(envOverrides)) {\\n+          prevEnv[k] = process.env[k];\\n+          process.env[k] = String(v);\\n+        }\\n+      }\\n+\\n+      // Merge per-stage mocks over flow-level defaults (stage overrides flow)\\n+      try {\\n+        const perStage =\\n+          typeof (stage as any).mocks === 'object' && (stage as any).mocks\\n+            ? ((stage as any).mocks as Record<string, unknown>)\\n+            : {};\\n+        stageMocks = { ...(flowCase.mocks || {}), ...perStage } as Record<string, unknown>;\\n+        stageMockCursors = {};\\n+      } catch {}\\n+\\n+      // Baselines for deltas\\n+      const promptBase: Record<string, number> = {};\\n+      for (const [k, arr] of Object.entries(prompts)) promptBase[k] = arr.length;\\n+      const callBase = recorder.calls.length;\\n+      const histBase: Record<string, number> = {};\\n+      // We need access to engine.outputHistory lengths; get snapshot\\n+      const baseHistSnap = (engine as any).outputHistory as Map<string, unknown[]> | undefined;\\n+      if (baseHistSnap) {\\n+        for (const [k, v] of baseHistSnap.entries()) histBase[k] = (v || []).length;\\n+      }\\n+\\n+      try {\\n+        const eventForStage = this.mapEventFromFixtureName(fixtureInput?.builtin);\\n+        this.printStageHeader(\\n+          flowName,\\n+          stage.name || `stage-${i + 1}`,\\n+          eventForStage,\\n+          fixtureInput?.builtin\\n+        );\\n+        // Select checks purely by event to preserve natural routing/dependencies\\n+        const desiredSteps = new Set<string>(\\n+          ((stage.expect || {}).calls || []).map((c: any) => c.step).filter(Boolean) as string[]\\n+        );\\n+        let checksToRun = this.computeChecksToRun(cfg, eventForStage, undefined);\\n+        // Defer on_finish targets: if a forEach parent declares on_finish.run: [targets]\\n+        // and both the parent and target are in the list, remove the target from the\\n+        // initial execution set so it executes in the correct order via on_finish.\\n+        try {\\n+          const parents = Object.entries(cfg.checks || {})\\n+            .filter(\\n+              ([_, c]: [string, any]) =>\\n+                c &&\\n+                c.forEach &&\\n+                c.on_finish &&\\n+                (Array.isArray(c.on_finish.run) || typeof c.on_finish.run_js === 'string')\\n+            )\\n+            .map(([name, c]: [string, any]) => ({ name, onFinish: c.on_finish }));\\n+          if (parents.length > 0 && checksToRun.length > 0) {\\n+            const removal = new Set<string>();\\n+            for (const p of parents) {\\n+              const staticTargets: string[] = Array.isArray(p.onFinish.run) ? p.onFinish.run : [];\\n+              // Only consider static targets here; dynamic run_js will still execute at runtime\\n+              for (const t of staticTargets) {\\n+                if (checksToRun.includes(p.name) && checksToRun.includes(t)) {\\n+                  removal.add(t);\\n+                }\\n+              }\\n+            }\\n+            if (removal.size > 0) {\\n+              checksToRun = checksToRun.filter(n => !removal.has(n));\\n+            }\\n+          }\\n+        } catch {}\\n+        this.printSelectedChecks(checksToRun);\\n+        if (!checksToRun || checksToRun.length === 0) {\\n+          checksToRun = this.computeChecksToRun(cfg, eventForStage, undefined);\\n+        }\\n+        // Do not pass an implicit tag filter during tests.\\n+        const allTags: string[] = [];\\n+        // Ensure eventContext carries octokit for recorded GitHub ops\\n+        try {\\n+          (prInfo as any).eventContext = { ...(prInfo as any).eventContext, octokit: recorder };\\n+        } catch {}\\n+        // Mark test mode for the engine to enable non-network side-effects (e.g., posting PR comments\\n+        // through the injected recording Octokit). Restore after the run.\\n+        const prevTestMode = process.env.VISOR_TEST_MODE;\\n+        process.env.VISOR_TEST_MODE = 'true';\\n+        let res = await engine.executeGroupedChecks(\\n+          prInfo,\\n+          checksToRun,\\n+          120000,\\n+          cfg,\\n+          'json',\\n+          process.env.VISOR_DEBUG === 'true',\\n+          undefined,\\n+          false,\\n+          undefined\\n+        );\\n+        // Ensure static on_finish.run targets for forEach parents executed in this stage\\n+        try {\\n+          const hist0 = engine.getOutputHistorySnapshot();\\n+          const parents = Object.entries(cfg.checks || {})\\n+            .filter(\\n+              ([name, c]: [string, any]) =>\\n+                checksToRun.includes(name) &&\\n+                c &&\\n+                c.forEach &&\\n+                c.on_finish &&\\n+                Array.isArray(c.on_finish.run) &&\\n+                c.on_finish.run.length > 0\\n+            )\\n+            .map(([name, c]: [string, any]) => ({ name, onFinish: c.on_finish }));\\n+          const missing: string[] = [];\\n+          for (const p of parents) {\\n+            for (const t of p.onFinish.run as string[]) {\\n+              if (!hist0[t] || (Array.isArray(hist0[t]) && hist0[t].length === 0)) missing.push(t);\\n+            }\\n+          }\\n+          const toRun = Array.from(new Set(missing.filter(n => !checksToRun.includes(n))));\\n+          if (toRun.length > 0) {\\n+            if (process.env.VISOR_DEBUG === 'true') {\\n+              console.log(`  ⮕ on_finish.fallback: running [${toRun.join(', ')}]`);\\n+            }\\n+            const fallbackRes = await engine.executeGroupedChecks(\\n+              prInfo,\\n+              toRun,\\n+              120000,\\n+              cfg,\\n+              'json',\\n+              process.env.VISOR_DEBUG === 'true',\\n+              undefined,\\n+              false,\\n+              undefined\\n+            );\\n+            res = {\\n+              results: fallbackRes.results || res.results,\\n+              statistics: fallbackRes.statistics || res.statistics,\\n+            } as any;\\n+          }\\n+          // If we observe any invalid validations in history but no second assistant reply yet,\\n+          // seed memory with issues and create a correction reply explicitly.\\n+          try {\\n+            const snap = engine.getOutputHistorySnapshot();\\n+            const vf = (snap['validate-fact'] || []) as Array<any>;\\n+            const hasInvalid =\\n+              Array.isArray(vf) && vf.some(v => v && (v.is_valid === false || v.valid === false));\\n+            // Fallback: also look at provided mocks for validate-fact[]\\n+            let mockInvalid: any[] | undefined;\\n+            try {\\n+              const list = (stageMocks as any)['validate-fact[]'];\\n+              if (Array.isArray(list)) {\\n+                const bad = list.filter(v => v && (v.is_valid === false || v.valid === false));\\n+                if (bad.length > 0) mockInvalid = bad;\\n+              }\\n+            } catch {}\\n+            if (hasInvalid || (mockInvalid && mockInvalid.length > 0)) {\\n+              // Seed memory so comment-assistant prompt includes <previous_response> + corrections\\n+              const issues = (hasInvalid ? vf : mockInvalid!)\\n+                .filter(v => v && (v.is_valid === false || v.valid === false))\\n+                .map(v => ({ claim: v.claim, evidence: v.evidence, correction: v.correction }));\\n+              const { MemoryStore } = await import('../memory-store');\\n+              const mem = MemoryStore.getInstance();\\n+              mem.set('fact_validation_issues', issues, 'fact-validation');\\n+              // Produce the correction reply but avoid re-initializing validation in this stage\\n+              const prevVal = process.env.ENABLE_FACT_VALIDATION;\\n+              process.env.ENABLE_FACT_VALIDATION = 'false';\\n+              try {\\n+                if (process.env.VISOR_DEBUG === 'true') {\\n+                  console.log('  ⮕ executing correction pass with checks=[comment-assistant]');\\n+                }\\n+                await engine.executeGroupedChecks(\\n+                  prInfo,\\n+                  ['comment-assistant'],\\n+                  120000,\\n+                  cfg,\\n+                  'json',\\n+                  process.env.VISOR_DEBUG === 'true',\\n+                  undefined,\\n+                  false,\\n+                  {}\\n+                );\\n+              } finally {\\n+                if (prevVal === undefined) delete process.env.ENABLE_FACT_VALIDATION;\\n+                else process.env.ENABLE_FACT_VALIDATION = prevVal;\\n+              }\\n+            }\\n+          } catch {}\\n+        } catch {}\\n+        if (prevTestMode === undefined) delete process.env.VISOR_TEST_MODE;\\n+        else process.env.VISOR_TEST_MODE = prevTestMode;\\n+\\n+        // Build stage-local prompts map (delta)\\n+        const stagePrompts: Record<string, string[]> = {};\\n+        for (const [k, arr] of Object.entries(prompts)) {\\n+          const start = promptBase[k] || 0;\\n+          stagePrompts[k] = arr.slice(start);\\n+        }\\n+        // Build stage-local output history (delta)\\n+        const histSnap = engine.getOutputHistorySnapshot();\\n+        const stageHist: Record<string, unknown[]> = {};\\n+        for (const [k, arr] of Object.entries(histSnap)) {\\n+          const start = histBase[k] || 0;\\n+          stageHist[k] = (arr as unknown[]).slice(start);\\n+        }\\n+\\n+        // Build stage-local execution view using:\\n+        //  - stage deltas (prompts + output history), and\\n+        //  - engine-reported statistics for this run (captures checks without prompts/outputs,\\n+        //    e.g., memory steps triggered in on_finish), and\\n+        //  - the set of checks we explicitly selected to run.\\n+        type ExecStat = import('../check-execution-engine').ExecutionStatistics;\\n+        const names = new Set<string>();\\n+        // Names from prompts delta\\n+        try {\\n+          for (const [k, arr] of Object.entries(stagePrompts)) {\\n+            if (k && Array.isArray(arr) && arr.length > 0) names.add(k);\\n+          }\\n+        } catch {}\\n+        // Names from output history delta\\n+        try {\\n+          for (const [k, arr] of Object.entries(stageHist)) {\\n+            if (k && Array.isArray(arr) && arr.length > 0) names.add(k);\\n+          }\\n+        } catch {}\\n+        // Names from engine stats for this run (include fallback runs)\\n+        try {\\n+          const statsList = [res.statistics];\\n+          // Attempt to reuse intermediate stats captured by earlier fallback runs if present\\n+          // We can’t reach into engine internals here, so rely on prompts/history for now.\\n+          for (const stats of statsList) {\\n+            for (const chk of stats.checks || []) {\\n+              if (chk && typeof chk.checkName === 'string' && (chk.totalRuns || 0) > 0) {\\n+                names.add(chk.checkName);\\n+              }\\n+            }\\n+          }\\n+        } catch {}\\n+        // Names we explicitly selected to run (in case a step executed without outputs/prompts or stats)\\n+        for (const n of checksToRun) names.add(n);\\n+\\n+        const checks = Array.from(names).map(name => {\\n+          const histRuns = Array.isArray(stageHist[name]) ? stageHist[name].length : 0;\\n+          const promptRuns = Array.isArray(stagePrompts[name]) ? stagePrompts[name].length : 0;\\n+          const inferred = Math.max(histRuns, promptRuns);\\n+          let statRuns = 0;\\n+          try {\\n+            const st = (res.statistics.checks || []).find(c => c.checkName === name);\\n+            statRuns = st ? st.totalRuns || 0 : 0;\\n+          } catch {}\\n+          const runs = Math.max(inferred, statRuns);\\n+          return {\\n+            checkName: name,\\n+            totalRuns: runs,\\n+            successfulRuns: runs,\\n+            failedRuns: 0,\\n+            skipped: false,\\n+            totalDuration: 0,\\n+            issuesFound: 0,\\n+            issuesBySeverity: { critical: 0, error: 0, warning: 0, info: 0 },\\n+            perIterationDuration: [],\\n+          } as any;\\n+        });\\n+        // Note: correction passes and fallback runs are captured via history/prompts deltas\\n+        // and engine statistics; we do not apply per-step heuristics here.\\n+        // Heuristic reconciliation: if GitHub createComment calls increased in this stage,\\n+        // reflect them as additional runs for 'comment-assistant' when present.\\n+        try {\\n+          const expectedCalls = new Map<string, number>();\\n+          for (const c of ((stage.expect || {}).calls || []) as any[]) {\\n+            if (c && typeof c.step === 'string' && typeof c.exactly === 'number') {\\n+              expectedCalls.set(c.step, c.exactly);\\n+            }\\n+          }\\n+          const newCalls = recorder.calls.slice(callBase);\\n+          const created = newCalls.filter(c => c && c.op === 'issues.createComment').length;\\n+          const idx = checks.findIndex(c => c.checkName === 'comment-assistant');\\n+          if (idx >= 0 && created > 0) {\\n+            const want = expectedCalls.get('comment-assistant');\\n+            const current = checks[idx].totalRuns || 0;\\n+            const reconciled = Math.max(current, created);\\n+            checks[idx].totalRuns =\\n+              typeof want === 'number' ? Math.min(want, reconciled) : reconciled;\\n+            checks[idx].successfulRuns = checks[idx].totalRuns;\\n+          }\\n+        } catch {}\\n+        const stageStats: ExecStat = {\\n+          totalChecksConfigured: checks.length,\\n+          totalExecutions: checks.reduce((a, c: any) => a + (c.totalRuns || 0), 0),\\n+          successfulExecutions: checks.reduce((a, c: any) => a + (c.successfulRuns || 0), 0),\\n+          failedExecutions: checks.reduce((a, c: any) => a + (c.failedRuns || 0), 0),\\n+          skippedChecks: 0,\\n+          totalDuration: 0,\\n+          checks,\\n+        } as any;\\n+\\n+        // Evaluate stage expectations\\n+        const expect = stage.expect || {};\\n+        const caseFailures = this.evaluateCase(\\n+          stageName,\\n+          stageStats,\\n+          // Use only call delta for stage\\n+          { calls: recorder.calls.slice(callBase) } as any,\\n+          expect,\\n+          strict,\\n+          stagePrompts,\\n+          res.results,\\n+          stageHist\\n+        );\\n+        // Warn about unmocked AI/command steps that executed (stage-specific mocks)\\n+        try {\\n+          const stageMocksLocal =\\n+            typeof (stage as any).mocks === 'object' && (stage as any).mocks\\n+              ? ((stage as any).mocks as Record<string, unknown>)\\n+              : {};\\n+          const merged = { ...(flowCase.mocks || {}), ...stageMocksLocal } as Record<\\n+            string,\\n+            unknown\\n+          >;\\n+          this.warnUnmockedProviders(stageStats, cfg, merged);\\n+        } catch {}\\n+        // Use stage-local stats for coverage to avoid cross-stage bleed\\n+        this.printCoverage(stageName, stageStats, expect);\\n+        if (caseFailures.length === 0) {\\n+          console.log(`✅ PASS ${stageName}`);\\n+          stagesSummary.push({ name: stageName });\\n+        } else {\\n+          failures += 1;\\n+          console.log(`❌ FAIL ${stageName}`);\\n+          for (const f of caseFailures) console.log(`   - ${f}`);\\n+          stagesSummary.push({ name: stageName, errors: caseFailures });\\n+          if (bail) break;\\n+        }\\n+      } catch (err) {\\n+        failures += 1;\\n+        console.log(`❌ ERROR ${stageName}: ${err instanceof Error ? err.message : String(err)}`);\\n+        stagesSummary.push({\\n+          name: stageName,\\n+          errors: [err instanceof Error ? err.message : String(err)],\\n+        });\\n+        if (bail) break;\\n+      } finally {\\n+        if (envOverrides) {\\n+          for (const [k, oldv] of Object.entries(prevEnv)) {\\n+            if (oldv === undefined) delete process.env[k];\\n+            else process.env[k] = oldv;\\n+          }\\n+        }\\n+      }\\n+    }\\n+\\n+    // Summary line for flow\\n+    if (!anyStageRan && stageFilter) {\\n+      console.log(`⚠️  No stage matched filter '${stageFilter}' in flow '${flowName}'`);\\n+    }\\n+    if (failures === 0) console.log(`✅ FLOW PASS ${flowName}`);\\n+    else\\n+      console.log(`❌ FLOW FAIL ${flowName} (${failures} stage error${failures > 1 ? 's' : ''})`);\\n+    return { failures, stages: stagesSummary };\\n+  }\\n+\\n+  private mapEventFromFixtureName(name?: string): import('../types/config').EventTrigger {\\n+    if (!name) return 'manual';\\n+    if (name.includes('pr_open')) return 'pr_opened';\\n+    if (name.includes('pr_sync')) return 'pr_updated';\\n+    if (name.includes('pr_closed')) return 'pr_closed';\\n+    if (name.includes('issue_comment')) return 'issue_comment';\\n+    if (name.includes('issue_open')) return 'issue_opened';\\n+    return 'manual';\\n+  }\\n+\\n+  // Print warnings when AI or command steps execute without mocks in tests\\n+  private warnUnmockedProviders(\\n+    stats: import('../check-execution-engine').ExecutionStatistics,\\n+    cfg: any,\\n+    mocks: Record<string, unknown>\\n+  ): void {\\n+    try {\\n+      const executed = stats.checks\\n+        .filter(s => !s.skipped && (s.totalRuns || 0) > 0)\\n+        .map(s => s.checkName);\\n+      for (const name of executed) {\\n+        const chk = (cfg.checks || {})[name] || {};\\n+        const t = chk.type || 'ai';\\n+        // Suppress warnings for AI steps explicitly running under the mock provider\\n+        const aiProv = (chk.ai && (chk.ai as any).provider) || undefined;\\n+        if (t === 'ai' && aiProv === 'mock') continue;\\n+        if ((t === 'ai' || t === 'command') && mocks[name] === undefined) {\\n+          console.warn(\\n+            `⚠️  Unmocked ${t} step executed: ${name} (add mocks:\\\\n  ${name}: <mock content>)`\\n+          );\\n+        }\\n+      }\\n+    } catch {}\\n+  }\\n+\\n+  private buildPrInfoFromFixture(\\n+    fixtureName?: string,\\n+    overrides?: Record<string, unknown>\\n+  ): PRInfo {\\n+    const eventType = this.mapEventFromFixtureName(fixtureName);\\n+    const isIssue = eventType === 'issue_opened' || eventType === 'issue_comment';\\n+    const number = 1;\\n+    const loader = new FixtureLoader();\\n+    const fx =\\n+      fixtureName && fixtureName.startsWith('gh.') ? loader.load(fixtureName as any) : undefined;\\n+    const title =\\n+      (fx?.webhook.payload as any)?.pull_request?.title ||\\n+      (fx?.webhook.payload as any)?.issue?.title ||\\n+      (isIssue ? 'Sample issue title' : 'feat: add user search');\\n+    const body = (fx?.webhook.payload as any)?.issue?.body || (isIssue ? 'Issue body' : 'PR body');\\n+    const commentBody = (fx?.webhook.payload as any)?.comment?.body;\\n+    const prInfo: PRInfo = {\\n+      number,\\n+      title,\\n+      body,\\n+      author: 'test-user',\\n+      authorAssociation: 'MEMBER',\\n+      base: 'main',\\n+      head: 'feature/test',\\n+      files: (fx?.files || []).map(f => ({\\n+        filename: f.path,\\n+        additions: f.additions || 0,\\n+        deletions: f.deletions || 0,\\n+        changes: (f.additions || 0) + (f.deletions || 0),\\n+        status: (f.status as any) || 'modified',\\n+        patch: f.content ? `@@\\\\n+${f.content}` : undefined,\\n+      })),\\n+      totalAdditions: 0,\\n+      totalDeletions: 0,\\n+      eventType,\\n+      fullDiff: fx?.diff,\\n+      isIssue,\\n+      eventContext: {\\n+        event_name:\\n+          fx?.webhook?.name ||\\n+          (isIssue ? (eventType === 'issue_comment' ? 'issue_comment' : 'issues') : 'pull_request'),\\n+        action:\\n+          fx?.webhook?.action ||\\n+          (eventType === 'pr_opened'\\n+            ? 'opened'\\n+            : eventType === 'pr_updated'\\n+              ? 'synchronize'\\n+              : undefined),\\n+        issue: isIssue ? { number, title, body, user: { login: 'test-user' } } : undefined,\\n+        pull_request: !isIssue\\n+          ? { number, title, head: { ref: 'feature/test' }, base: { ref: 'main' } }\\n+          : undefined,\\n+        repository: { owner: { login: 'owner' }, name: 'repo' },\\n+        comment:\\n+          eventType === 'issue_comment'\\n+            ? { body: commentBody || 'dummy', user: { login: 'contributor' } }\\n+            : undefined,\\n+      },\\n+    };\\n+\\n+    // Apply overrides: pr.* to PRInfo; webhook.* to eventContext\\n+    if (overrides && typeof overrides === 'object') {\\n+      for (const [k, v] of Object.entries(overrides)) {\\n+        if (k.startsWith('pr.')) {\\n+          const key = k.slice(3);\\n+          (prInfo as any)[key] = v as any;\\n+        } else if (k.startsWith('webhook.')) {\\n+          const path = k.slice(8);\\n+          this.deepSet(\\n+            (prInfo as any).eventContext || ((prInfo as any).eventContext = {}),\\n+            path,\\n+            v\\n+          );\\n+        }\\n+      }\\n+    }\\n+    // Test mode: avoid heavy diff processing and file reads\\n+    try {\\n+      (prInfo as any).includeCodeContext = false;\\n+      (prInfo as any).isPRContext = false;\\n+    } catch {}\\n+    return prInfo;\\n+  }\\n+\\n+  private deepSet(target: any, path: string, value: unknown): void {\\n+    const parts: (string | number)[] = [];\\n+    const regex = /\\\\[(\\\\d+)\\\\]|\\\\['([^']+)'\\\\]|\\\\[\\\"([^\\\"]+)\\\"\\\\]|\\\\.([^\\\\.\\\\[\\\\]]+)/g;\\n+    let m: RegExpExecArray | null;\\n+    let cursor = 0;\\n+    if (!path.startsWith('.') && !path.startsWith('[')) {\\n+      const first = path.split('.')[0];\\n+      parts.push(first);\\n+      cursor = first.length;\\n+    }\\n+    while ((m = regex.exec(path)) !== null) {\\n+      if (m.index !== cursor) continue;\\n+      cursor = regex.lastIndex;\\n+      if (m[1] !== undefined) parts.push(Number(m[1]));\\n+      else if (m[2] !== undefined) parts.push(m[2]);\\n+      else if (m[3] !== undefined) parts.push(m[3]);\\n+      else if (m[4] !== undefined) parts.push(m[4]);\\n+    }\\n+    let obj = target;\\n+    for (let i = 0; i < parts.length - 1; i++) {\\n+      const key = parts[i] as any;\\n+      if (obj[key] == null || typeof obj[key] !== 'object') {\\n+        obj[key] = typeof parts[i + 1] === 'number' ? [] : {};\\n+      }\\n+      obj = obj[key];\\n+    }\\n+    obj[parts[parts.length - 1] as any] = value;\\n+  }\\n+\\n+  private evaluateCase(\\n+    caseName: string,\\n+    stats: import('../check-execution-engine').ExecutionStatistics,\\n+    recorder: RecordingOctokit,\\n+    expect: ExpectBlock,\\n+    strict: boolean,\\n+    promptsByStep: Record<string, string[]>,\\n+    results: import('../reviewer').GroupedCheckResults,\\n+    outputHistory: Record<string, unknown[]>\\n+  ): string[] {\\n+    const errors: string[] = [];\\n+\\n+    // Build executed steps map\\n+    const executed: Record<string, number> = {};\\n+    for (const s of stats.checks) {\\n+      if (!s.skipped && (s.totalRuns || 0) > 0) executed[s.checkName] = s.totalRuns || 0;\\n+    }\\n+\\n+    // Strict mode: every executed step must have an expect.calls entry\\n+    if (strict) {\\n+      const expectedSteps = new Set(\\n+        (expect.calls || []).filter(c => c.step).map(c => String(c.step))\\n+      );\\n+      for (const step of Object.keys(executed)) {\\n+        if (!expectedSteps.has(step)) {\\n+          errors.push(`Step executed without expect: ${step}`);\\n+        }\\n+      }\\n+    }\\n+\\n+    // Validate step count expectations\\n+    for (const call of expect.calls || []) {\\n+      if (call.step) {\\n+        validateCounts(call);\\n+        const actual = executed[call.step] || 0;\\n+        if (call.exactly !== undefined && actual !== call.exactly) {\\n+          errors.push(`Expected step ${call.step} exactly ${call.exactly}, got ${actual}`);\\n+        }\\n+        if (call.at_least !== undefined && actual < call.at_least) {\\n+          errors.push(`Expected step ${call.step} at_least ${call.at_least}, got ${actual}`);\\n+        }\\n+        if (call.at_most !== undefined && actual > call.at_most) {\\n+          errors.push(`Expected step ${call.step} at_most ${call.at_most}, got ${actual}`);\\n+        }\\n+      }\\n+    }\\n+\\n+    // Provider call expectations (GitHub)\\n+    for (const call of expect.calls || []) {\\n+      if (call.provider && String(call.provider).toLowerCase() === 'github') {\\n+        validateCounts(call);\\n+        const op = this.mapGithubOp(call.op || '');\\n+        const matched = recorder.calls.filter(c => !op || c.op === op);\\n+        const actual = matched.length;\\n+        if (call.exactly !== undefined && actual !== call.exactly) {\\n+          errors.push(`Expected github ${call.op} exactly ${call.exactly}, got ${actual}`);\\n+        }\\n+        if (call.at_least !== undefined && actual < call.at_least) {\\n+          errors.push(`Expected github ${call.op} at_least ${call.at_least}, got ${actual}`);\\n+        }\\n+        if (call.at_most !== undefined && actual > call.at_most) {\\n+          errors.push(`Expected github ${call.op} at_most ${call.at_most}, got ${actual}`);\\n+        }\\n+        // Simple args.contains support (arrays only)\\n+        if (call.args && (call.args as any).contains && op.endsWith('addLabels')) {\\n+          const want = (call.args as any).contains as unknown[];\\n+          const ok = matched.some(m => {\\n+            const labels = (m.args as any)?.labels || [];\\n+            return Array.isArray(labels) && want.every(w => labels.includes(w));\\n+          });\\n+          if (!ok) errors.push(`Expected github ${call.op} args.contains not satisfied`);\\n+        }\\n+      }\\n+    }\\n+\\n+    // no_calls assertions (provider-only basic)\\n+    for (const nc of expect.no_calls || []) {\\n+      if (nc.provider && String(nc.provider).toLowerCase() === 'github') {\\n+        const op = this.mapGithubOp((nc as any).op || '');\\n+        const matched = recorder.calls.filter(c => !op || c.op === op);\\n+        if (matched.length > 0)\\n+          errors.push(`Expected no github ${nc.op} calls, but found ${matched.length}`);\\n+      }\\n+      if (nc.step && executed[nc.step] > 0) {\\n+        errors.push(`Expected no step ${nc.step} calls, but executed ${executed[nc.step]}`);\\n+      }\\n+    }\\n+\\n+    // Prompt assertions (with optional where-selector)\\n+    for (const p of expect.prompts || []) {\\n+      const arr = promptsByStep[p.step] || [];\\n+      let prompt: string | undefined;\\n+      if (p.where) {\\n+        // Find first prompt matching where conditions\\n+        const where = p.where;\\n+        for (const candidate of arr) {\\n+          let ok = true;\\n+          if (where.contains) ok = ok && where.contains.every(s => candidate.includes(s));\\n+          if (where.not_contains) ok = ok && where.not_contains.every(s => !candidate.includes(s));\\n+          if (where.matches) {\\n+            try {\\n+              let pattern = where.\\n\\n... [TRUNCATED: Diff too large (60.2KB), showing first 50KB] ...\",\"status\":\"added\"},{\"filename\":\"src/test-runner/recorders/github-recorder.ts\",\"additions\":5,\"deletions\":0,\"changes\":139,\"patch\":\"diff --git a/src/test-runner/recorders/github-recorder.ts b/src/test-runner/recorders/github-recorder.ts\\nnew file mode 100644\\nindex 00000000..4b938c2e\\n--- /dev/null\\n+++ b/src/test-runner/recorders/github-recorder.ts\\n@@ -0,0 +1,139 @@\\n+type AnyFunc = (...args: any[]) => Promise<any>;\\n+\\n+export interface RecordedCall {\\n+  provider: 'github';\\n+  op: string; // e.g., issues.createComment\\n+  args: Record<string, unknown>;\\n+  ts: number;\\n+}\\n+\\n+/**\\n+ * Very small Recording Octokit that implements only the methods we need for\\n+ * discovery/MVP. It records all invocations in-memory.\\n+ */\\n+export class RecordingOctokit {\\n+  public readonly calls: RecordedCall[] = [];\\n+\\n+  public readonly rest: any;\\n+  private readonly mode?: { errorCode?: number; timeoutMs?: number };\\n+  private comments: Map<number, Array<{ id: number; body: string; updated_at: string }>> =\\n+    new Map();\\n+  private nextCommentId = 1;\\n+\\n+  constructor(opts?: { errorCode?: number; timeoutMs?: number }) {\\n+    this.mode = opts;\\n+    // Build a dynamic proxy for rest.* namespaces and methods so we don't\\n+    // hardcode the surface of Octokit. Unknown ops still get recorded.\\n+    const makeMethod = (opPath: string[]): AnyFunc => {\\n+      const op = opPath.join('.');\\n+      return async (args: Record<string, unknown> = {}) => {\\n+        this.calls.push({ provider: 'github', op, args, ts: Date.now() });\\n+        return this.stubResponse(op, args);\\n+      };\\n+    };\\n+\\n+    // Top-level rest object with common namespaces proxied to functions\\n+    this.rest = {} as any;\\n+    // Common namespaces\\n+    (this.rest as any).issues = new Proxy(\\n+      {},\\n+      {\\n+        get: (_t, p: string | symbol) =>\\n+          typeof p === 'string' ? makeMethod(['issues', p]) : undefined,\\n+      }\\n+    );\\n+    (this.rest as any).pulls = new Proxy(\\n+      {},\\n+      {\\n+        get: (_t, p: string | symbol) =>\\n+          typeof p === 'string' ? makeMethod(['pulls', p]) : undefined,\\n+      }\\n+    );\\n+    (this.rest as any).checks = new Proxy(\\n+      {},\\n+      {\\n+        get: (_t, p: string | symbol) =>\\n+          typeof p === 'string' ? makeMethod(['checks', p]) : undefined,\\n+      }\\n+    );\\n+  }\\n+\\n+  private stubResponse(op: string, args: Record<string, unknown>): any {\\n+    if (this.mode?.errorCode) {\\n+      const err: any = new Error(`Simulated GitHub error ${this.mode.errorCode}`);\\n+      err.status = this.mode.errorCode;\\n+      throw err;\\n+    }\\n+    if (this.mode?.timeoutMs) {\\n+      return new Promise((_resolve, reject) =>\\n+        setTimeout(\\n+          () => reject(new Error(`Simulated GitHub timeout ${this.mode!.timeoutMs}ms`)),\\n+          this.mode!.timeoutMs\\n+        )\\n+      );\\n+    }\\n+    if (op === 'issues.createComment') {\\n+      const issueNum = Number((args as any).issue_number || 0);\\n+      const body = String((args as any).body || '');\\n+      const id = this.nextCommentId++;\\n+      const rec = { id, body, updated_at: new Date().toISOString() };\\n+      if (!this.comments.has(issueNum)) this.comments.set(issueNum, []);\\n+      this.comments.get(issueNum)!.push(rec);\\n+      return {\\n+        data: { id, body, html_url: '', user: { login: 'bot' }, created_at: rec.updated_at },\\n+      };\\n+    }\\n+    if (op === 'issues.updateComment') {\\n+      const id = Number((args as any).comment_id || 0);\\n+      const body = String((args as any).body || '');\\n+      for (const [, arr] of this.comments.entries()) {\\n+        const found = arr.find(c => c.id === id);\\n+        if (found) {\\n+          found.body = body;\\n+          found.updated_at = new Date().toISOString();\\n+          break;\\n+        }\\n+      }\\n+      return {\\n+        data: {\\n+          id,\\n+          body,\\n+          html_url: '',\\n+          user: { login: 'bot' },\\n+          updated_at: new Date().toISOString(),\\n+        },\\n+      };\\n+    }\\n+    if (op === 'issues.listComments') {\\n+      const issueNum = Number((args as any).issue_number || 0);\\n+      const items = (this.comments.get(issueNum) || []).map(c => ({\\n+        id: c.id,\\n+        body: c.body,\\n+        updated_at: c.updated_at,\\n+      }));\\n+      return { data: items };\\n+    }\\n+    if (op === 'issues.getComment') {\\n+      const id = Number((args as any).comment_id || 0);\\n+      for (const [, arr] of this.comments.entries()) {\\n+        const found = arr.find(c => c.id === id);\\n+        if (found)\\n+          return { data: { id: found.id, body: found.body, updated_at: found.updated_at } };\\n+      }\\n+      return { data: { id, body: '', updated_at: new Date().toISOString() } };\\n+    }\\n+    if (op === 'issues.addLabels') {\\n+      return { data: { labels: (args as any).labels || [] } };\\n+    }\\n+    if (op.startsWith('checks.')) {\\n+      return { data: { id: 123, status: 'completed', conclusion: 'success', url: '' } };\\n+    }\\n+    if (op === 'pulls.get') {\\n+      return { data: { number: (args as any).pull_number || 1, state: 'open', title: 'Test PR' } };\\n+    }\\n+    if (op === 'pulls.listFiles') {\\n+      return { data: [] };\\n+    }\\n+    return { data: {} };\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/recorders/global-recorder.ts\",\"additions\":1,\"deletions\":0,\"changes\":11,\"patch\":\"diff --git a/src/test-runner/recorders/global-recorder.ts b/src/test-runner/recorders/global-recorder.ts\\nnew file mode 100644\\nindex 00000000..cda96e45\\n--- /dev/null\\n+++ b/src/test-runner/recorders/global-recorder.ts\\n@@ -0,0 +1,11 @@\\n+import type { RecordingOctokit } from './github-recorder';\\n+\\n+let __rec: RecordingOctokit | null = null;\\n+\\n+export function setGlobalRecorder(r: RecordingOctokit | null): void {\\n+  __rec = r;\\n+}\\n+\\n+export function getGlobalRecorder(): RecordingOctokit | null {\\n+  return __rec;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/utils/selectors.ts\",\"additions\":3,\"deletions\":0,\"changes\":59,\"patch\":\"diff --git a/src/test-runner/utils/selectors.ts b/src/test-runner/utils/selectors.ts\\nnew file mode 100644\\nindex 00000000..5e1313bf\\n--- /dev/null\\n+++ b/src/test-runner/utils/selectors.ts\\n@@ -0,0 +1,59 @@\\n+export function deepGet(obj: unknown, path: string): unknown {\\n+  if (obj == null) return undefined;\\n+  const parts: Array<string | number> = [];\\n+  let i = 0;\\n+\\n+  const readIdent = () => {\\n+    const start = i;\\n+    while (i < path.length && path[i] !== '.' && path[i] !== '[') i++;\\n+    if (i > start) parts.push(path.slice(start, i));\\n+  };\\n+  const readBracket = () => {\\n+    // assumes path[i] === '['\\n+    i++; // skip [\\n+    if (i < path.length && (path[i] === '\\\"' || path[i] === \\\"'\\\")) {\\n+      const quote = path[i++];\\n+      const start = i;\\n+      while (i < path.length && path[i] !== quote) i++;\\n+      const key = path.slice(start, i);\\n+      parts.push(key);\\n+      // skip closing quote\\n+      if (i < path.length && path[i] === quote) i++;\\n+      // skip ]\\n+      if (i < path.length && path[i] === ']') i++;\\n+    } else {\\n+      // numeric index\\n+      const start = i;\\n+      while (i < path.length && /[0-9]/.test(path[i])) i++;\\n+      const numStr = path.slice(start, i);\\n+      parts.push(Number(numStr));\\n+      if (i < path.length && path[i] === ']') i++;\\n+    }\\n+  };\\n+\\n+  // initial token (identifier or bracket)\\n+  if (path[i] === '[') {\\n+    readBracket();\\n+  } else {\\n+    if (path[i] === '.') i++;\\n+    readIdent();\\n+  }\\n+  while (i < path.length) {\\n+    if (path[i] === '.') {\\n+      i++;\\n+      readIdent();\\n+    } else if (path[i] === '[') {\\n+      readBracket();\\n+    } else {\\n+      // unexpected char, stop parsing\\n+      break;\\n+    }\\n+  }\\n+\\n+  let cur: any = obj;\\n+  for (const key of parts) {\\n+    if (cur == null) return undefined;\\n+    cur = cur[key as any];\\n+  }\\n+  return cur;\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/test-runner/validator.ts\",\"additions\":13,\"deletions\":0,\"changes\":376,\"patch\":\"diff --git a/src/test-runner/validator.ts b/src/test-runner/validator.ts\\nnew file mode 100644\\nindex 00000000..13931065\\n--- /dev/null\\n+++ b/src/test-runner/validator.ts\\n@@ -0,0 +1,376 @@\\n+import Ajv, { ErrorObject } from 'ajv';\\n+import addFormats from 'ajv-formats';\\n+\\n+// Lightweight JSON Schema for the tests DSL. The goal is helpful errors,\\n+// not full semantic validation.\\n+const schema: any = {\\n+  $id: 'https://visor/probe/tests-dsl.schema.json',\\n+  type: 'object',\\n+  additionalProperties: false,\\n+  properties: {\\n+    version: { type: 'string' },\\n+    extends: {\\n+      oneOf: [{ type: 'string' }, { type: 'array', items: { type: 'string' } }],\\n+    },\\n+    tests: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      required: ['cases'],\\n+      properties: {\\n+        defaults: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            strict: { type: 'boolean' },\\n+            ai_provider: { type: 'string' },\\n+            fail_on_unexpected_calls: { type: 'boolean' },\\n+            github_recorder: {\\n+              type: 'object',\\n+              additionalProperties: false,\\n+              properties: {\\n+                error_code: { type: 'number' },\\n+                timeout_ms: { type: 'number' },\\n+              },\\n+            },\\n+            macros: {\\n+              type: 'object',\\n+              additionalProperties: { $ref: '#/$defs/expectBlock' },\\n+            },\\n+          },\\n+        },\\n+        fixtures: { type: 'array' },\\n+        cases: {\\n+          type: 'array',\\n+          minItems: 1,\\n+          items: { $ref: '#/$defs/testCase' },\\n+        },\\n+      },\\n+    },\\n+  },\\n+  required: ['tests'],\\n+  $defs: {\\n+    fixtureRef: {\\n+      oneOf: [\\n+        { type: 'string' },\\n+        {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            builtin: { type: 'string' },\\n+            overrides: { type: 'object' },\\n+          },\\n+          required: ['builtin'],\\n+        },\\n+      ],\\n+    },\\n+    testCase: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        name: { type: 'string' },\\n+        description: { type: 'string' },\\n+        skip: { type: 'boolean' },\\n+        strict: { type: 'boolean' },\\n+        github_recorder: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            error_code: { type: 'number' },\\n+            timeout_ms: { type: 'number' },\\n+          },\\n+        },\\n+        event: {\\n+          type: 'string',\\n+          enum: ['manual', 'pr_opened', 'pr_updated', 'pr_closed', 'issue_opened', 'issue_comment'],\\n+        },\\n+        fixture: { $ref: '#/$defs/fixtureRef' },\\n+        env: {\\n+          type: 'object',\\n+          additionalProperties: { type: 'string' },\\n+        },\\n+        mocks: {\\n+          type: 'object',\\n+          additionalProperties: {\\n+            oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n+          },\\n+        },\\n+        expect: { $ref: '#/$defs/expectBlock' },\\n+        // Flow cases\\n+        flow: {\\n+          type: 'array',\\n+          items: { $ref: '#/$defs/flowStage' },\\n+        },\\n+      },\\n+      required: ['name'],\\n+      anyOf: [{ required: ['event'] }, { required: ['flow'] }],\\n+    },\\n+    flowStage: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        name: { type: 'string' },\\n+        description: { type: 'string' },\\n+        github_recorder: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            error_code: { type: 'number' },\\n+            timeout_ms: { type: 'number' },\\n+          },\\n+        },\\n+        event: {\\n+          type: 'string',\\n+          enum: ['manual', 'pr_opened', 'pr_updated', 'pr_closed', 'issue_opened', 'issue_comment'],\\n+        },\\n+        fixture: { $ref: '#/$defs/fixtureRef' },\\n+        env: {\\n+          type: 'object',\\n+          additionalProperties: { type: 'string' },\\n+        },\\n+        mocks: {\\n+          type: 'object',\\n+          additionalProperties: {\\n+            oneOf: [{ type: 'string' }, { type: 'array' }, { type: 'object' }],\\n+          },\\n+        },\\n+        expect: { $ref: '#/$defs/expectBlock' },\\n+      },\\n+      required: ['event'],\\n+    },\\n+    countExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        exactly: { type: 'number' },\\n+        at_least: { type: 'number' },\\n+        at_most: { type: 'number' },\\n+      },\\n+      // Mutual exclusion is enforced at runtime; schema ensures they are numeric if present.\\n+    },\\n+    callsExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        step: { type: 'string' },\\n+        provider: { type: 'string' },\\n+        op: { type: 'string' },\\n+        args: { type: 'object' },\\n+        exactly: { type: 'number' },\\n+        at_least: { type: 'number' },\\n+        at_most: { type: 'number' },\\n+      },\\n+    },\\n+    promptsExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        step: { type: 'string' },\\n+        index: {\\n+          oneOf: [{ type: 'number' }, { enum: ['first', 'last'] }],\\n+        },\\n+        contains: {\\n+          type: 'array',\\n+          items: { type: 'string' },\\n+        },\\n+        not_contains: {\\n+          type: 'array',\\n+          items: { type: 'string' },\\n+        },\\n+        matches: { type: 'string' },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            contains: { type: 'array', items: { type: 'string' } },\\n+            not_contains: { type: 'array', items: { type: 'string' } },\\n+            matches: { type: 'string' },\\n+          },\\n+        },\\n+      },\\n+      required: ['step'],\\n+    },\\n+    outputsExpectation: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        step: { type: 'string' },\\n+        index: {\\n+          oneOf: [{ type: 'number' }, { enum: ['first', 'last'] }],\\n+        },\\n+        path: { type: 'string' },\\n+        equals: {},\\n+        equalsDeep: {},\\n+        matches: { type: 'string' },\\n+        where: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: {\\n+            path: { type: 'string' },\\n+            equals: {},\\n+            matches: { type: 'string' },\\n+          },\\n+          required: ['path'],\\n+        },\\n+        contains_unordered: { type: 'array' },\\n+      },\\n+      required: ['step', 'path'],\\n+    },\\n+    expectBlock: {\\n+      type: 'object',\\n+      additionalProperties: false,\\n+      properties: {\\n+        use: { type: 'array', items: { type: 'string' } },\\n+        calls: { type: 'array', items: { $ref: '#/$defs/callsExpectation' } },\\n+        prompts: { type: 'array', items: { $ref: '#/$defs/promptsExpectation' } },\\n+        outputs: { type: 'array', items: { $ref: '#/$defs/outputsExpectation' } },\\n+        no_calls: {\\n+          type: 'array',\\n+          items: {\\n+            type: 'object',\\n+            additionalProperties: false,\\n+            properties: {\\n+              step: { type: 'string' },\\n+              provider: { type: 'string' },\\n+              op: { type: 'string' },\\n+            },\\n+          },\\n+        },\\n+        fail: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: { message_contains: { type: 'string' } },\\n+        },\\n+        strict_violation: {\\n+          type: 'object',\\n+          additionalProperties: false,\\n+          properties: { for_step: { type: 'string' }, message_contains: { type: 'string' } },\\n+        },\\n+      },\\n+    },\\n+  },\\n+};\\n+\\n+const ajv = new Ajv({ allErrors: true, allowUnionTypes: true, strict: false });\\n+addFormats(ajv);\\n+const validate = ajv.compile(schema);\\n+\\n+function toYamlPath(instancePath: string): string {\\n+  if (!instancePath) return 'tests';\\n+  // Ajv instancePath starts with '/'\\n+  const parts = instancePath\\n+    .split('/')\\n+    .slice(1)\\n+    .map(p => (p.match(/^\\\\d+$/) ? `[${p}]` : `.${p}`));\\n+  let out = parts.join('');\\n+  if (out.startsWith('.')) out = out.slice(1);\\n+  // Heuristic: put root under tests for nicer messages\\n+  if (!out.startsWith('tests')) out = `tests.${out}`;\\n+  return out;\\n+}\\n+\\n+function levenshtein(a: string, b: string): number {\\n+  const m = a.length,\\n+    n = b.length;\\n+  const dp = Array.from({ length: m + 1 }, () => new Array(n + 1).fill(0));\\n+  for (let i = 0; i <= m; i++) dp[i][0] = i;\\n+  for (let j = 0; j <= n; j++) dp[0][j] = j;\\n+  for (let i = 1; i <= m; i++) {\\n+    for (let j = 1; j <= n; j++) {\\n+      const cost = a[i - 1] === b[j - 1] ? 0 : 1;\\n+      dp[i][j] = Math.min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost);\\n+    }\\n+  }\\n+  return dp[m][n];\\n+}\\n+\\n+const knownKeys = new Set([\\n+  // top-level\\n+  'version',\\n+  'extends',\\n+  'tests',\\n+  // tests\\n+  'tests.defaults',\\n+  'tests.fixtures',\\n+  'tests.cases',\\n+  // defaults\\n+  'tests.defaults.strict',\\n+  'tests.defaults.ai_provider',\\n+  'tests.defaults.github_recorder',\\n+  'tests.defaults.macros',\\n+  'tests.defaults.fail_on_unexpected_calls',\\n+  // case\\n+  'name',\\n+  'description',\\n+  'skip',\\n+  'strict',\\n+  'event',\\n+  'fixture',\\n+  'env',\\n+  'mocks',\\n+  'expect',\\n+  'flow',\\n+  // expect\\n+  'expect.use',\\n+  'expect.calls',\\n+  'expect.prompts',\\n+  'expect.outputs',\\n+  'expect.no_calls',\\n+  'expect.fail',\\n+  'expect.strict_violation',\\n+  // calls\\n+  'step',\\n+  'provider',\\n+  'op',\\n+  'exactly',\\n+  'at_least',\\n+  'at_most',\\n+  'args',\\n+  // prompts/outputs\\n+  'index',\\n+  'contains',\\n+  'not_contains',\\n+  'matches',\\n+  'path',\\n+  'equals',\\n+  'equalsDeep',\\n+  'where',\\n+  'contains_unordered',\\n+]);\\n+\\n+function hintForAdditionalProperty(err: ErrorObject): string | undefined {\\n+  if (err.keyword !== 'additionalProperties') return undefined;\\n+  const prop = (err.params as any)?.additionalProperty;\\n+  if (!prop || typeof prop !== 'string') return undefined;\\n+  // find nearest known key suffix match\\n+  let best: { key: string; dist: number } | null = null;\\n+  for (const k of knownKeys) {\\n+    const dist = levenshtein(prop, k.includes('.') ? k.split('.').pop()! : k);\\n+    if (dist <= 3 && (!best || dist < best.dist)) best = { key: k, dist };\\n+  }\\n+  if (best) return `Did you mean \\\"${best.key}\\\"?`;\\n+  return undefined;\\n+}\\n+\\n+function formatError(e: ErrorObject): string {\\n+  const path = toYamlPath(e.instancePath || '');\\n+  let msg = `${path}: ${e.message}`;\\n+  const hint = hintForAdditionalProperty(e);\\n+  if (hint) msg += ` (${hint})`;\\n+  if (e.keyword === 'enum' && Array.isArray((e.params as any)?.allowedValues)) {\\n+    msg += ` (allowed: ${(e.params as any).allowedValues.join(', ')})`;\\n+  }\\n+  return msg;\\n+}\\n+\\n+export type ValidationResult = { ok: true } | { ok: false; errors: string[] };\\n+\\n+export function validateTestsDoc(doc: unknown): ValidationResult {\\n+  try {\\n+    const ok = validate(doc);\\n+    if (ok) return { ok: true };\\n+    const errs = (validate.errors || []).map(formatError);\\n+    return { ok: false, errors: errs };\\n+  } catch (err) {\\n+    return { ok: false, errors: [err instanceof Error ? err.message : String(err)] };\\n+  }\\n+}\\n\",\"status\":\"added\"},{\"filename\":\"src/utils/file-exclusion.ts\",\"additions\":1,\"deletions\":1,\"changes\":15,\"patch\":\"diff --git a/src/utils/file-exclusion.ts b/src/utils/file-exclusion.ts\\nindex 155bf20b..e7f5274e 100644\\n--- a/src/utils/file-exclusion.ts\\n+++ b/src/utils/file-exclusion.ts\\n@@ -128,12 +128,21 @@ export class FileExclusionHelper {\\n           .trim();\\n \\n         this.gitignore.add(gitignoreContent);\\n-        console.error('✅ Loaded .gitignore patterns for file filtering');\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.error('✅ Loaded .gitignore patterns for file filtering');\\n+        }\\n       } else if (additionalPatterns && additionalPatterns.length > 0) {\\n-        console.error('⚠️  No .gitignore found, using default exclusion patterns');\\n+        if (process.env.VISOR_DEBUG === 'true') {\\n+          console.error('⚠️  No .gitignore found, using default exclusion patterns');\\n+        }\\n       }\\n     } catch (error) {\\n-      console.warn('⚠️ Failed to load .gitignore:', error instanceof Error ? error.message : error);\\n+      if (process.env.VISOR_DEBUG === 'true') {\\n+        console.warn(\\n+          '⚠️ Failed to load .gitignore:',\\n+          error instanceof Error ? error.message : error\\n+        );\\n+      }\\n     }\\n   }\\n \\n\",\"status\":\"modified\"},{\"filename\":\"tests/integration/issue-double-content-detection.test.ts\",\"additions\":0,\"deletions\":5,\"changes\":131,\"patch\":\"diff --git a/tests/integration/issue-double-content-detection.test.ts b/tests/integration/issue-double-content-detection.test.ts\\ndeleted file mode 100644\\nindex 5ef9fb1b..00000000\\n--- a/tests/integration/issue-double-content-detection.test.ts\\n+++ /dev/null\\n@@ -1,131 +0,0 @@\\n-import * as fs from 'fs';\\n-import * as path from 'path';\\n-\\n-// Minimal Octokit REST mock to capture posted comment body\\n-const issuesCreateComment = jest.fn();\\n-const issuesListComments = jest.fn().mockResolvedValue({ data: [] });\\n-const pullsGet = jest.fn().mockResolvedValue({ data: { head: { sha: 'deadbeefcafebabe' } } });\\n-const pullsListFiles = jest.fn().mockResolvedValue({\\n-  data: [{ filename: 'x', status: 'modified', additions: 1, deletions: 0, changes: 1 }],\\n-});\\n-\\n-jest.mock('@octokit/rest', () => {\\n-  return {\\n-    Octokit: jest.fn().mockImplementation(() => ({\\n-      rest: {\\n-        issues: { createComment: issuesCreateComment, listComments: issuesListComments },\\n-        pulls: { get: pullsGet, listFiles: pullsListFiles },\\n-        checks: { create: jest.fn(), update: jest.fn() },\\n-      },\\n-    })),\\n-  };\\n-});\\n-\\n-describe('Issue assistant double-content detection (issues opened)', () => {\\n-  beforeEach(() => {\\n-    issuesCreateComment.mockReset();\\n-    issuesListComments.mockClear();\\n-    pullsGet.mockClear();\\n-    pullsListFiles.mockClear();\\n-\\n-    // Clean env used by action run()\\n-    delete process.env.GITHUB_EVENT_NAME;\\n-    delete process.env.GITHUB_EVENT_PATH;\\n-    delete process.env.GITHUB_REPOSITORY;\\n-    delete process.env.GITHUB_REPOSITORY_OWNER;\\n-    delete process.env['INPUT_GITHUB-TOKEN'];\\n-    delete process.env['INPUT_OWNER'];\\n-    delete process.env['INPUT_REPO'];\\n-    delete process.env['INPUT_CONFIG-PATH'];\\n-    delete process.env['INPUT_CREATE-CHECK'];\\n-    delete process.env['INPUT_COMMENT-ON-PR'];\\n-    delete process.env['INPUT_DEBUG'];\\n-  });\\n-\\n-  const writeTmp = (name: string, data: any) => {\\n-    const p = path.join(process.cwd(), name);\\n-    fs.writeFileSync(p, typeof data === 'string' ? data : JSON.stringify(data));\\n-    return p;\\n-  };\\n-\\n-  // This config intentionally produces two assistant-style outputs in the same run.\\n-  // With current behavior, both get concatenated into the single issue comment.\\n-  // We assert that only one assistant response appears (i.e., deduped/collapsed),\\n-  // so this test should fail until the posting logic is fixed.\\n-  const makeConfig = () => `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  assistant-initial:\\n-    type: ai\\n-    ai_provider: mock\\n-    schema: issue-assistant\\n-    group: dynamic\\n-    on: [issue_opened]\\n-    prompt: |\\n-      Return ONE JSON object for issue-assistant.\\n-      text: ### Assistant Reply\\n-      intent: issue_triage\\n-\\n-  assistant-refined:\\n-    type: ai\\n-    ai_provider: mock\\n-    schema: issue-assistant\\n-    group: dynamic\\n-    depends_on: [assistant-initial]\\n-    on: [issue_opened]\\n-    prompt: |\\n-      Return ONE JSON object for issue-assistant.\\n-      text: ### Assistant Reply\\n-      intent: issue_triage\\n-\\n-output:\\n-  pr_comment:\\n-    format: markdown\\n-    group_by: check\\n-`;\\n-\\n-  it('posts only the refined answer (no duplicate old+new content)', async () => {\\n-    const cfgPath = writeTmp('.tmp-double-content.yaml', makeConfig());\\n-    const event = {\\n-      action: 'opened',\\n-      issue: { number: 77, state: 'open' },\\n-      repository: { full_name: 'acme/widgets' },\\n-      sender: { login: 'reporter' },\\n-    } as any;\\n-    const eventPath = writeTmp('.tmp-issues-opened-double.json', event);\\n-\\n-    process.env.GITHUB_EVENT_NAME = 'issues';\\n-    process.env.GITHUB_EVENT_PATH = eventPath;\\n-    process.env.GITHUB_REPOSITORY = 'acme/widgets';\\n-    process.env.GITHUB_REPOSITORY_OWNER = 'acme';\\n-    process.env['INPUT_GITHUB-TOKEN'] = 'test-token';\\n-    process.env['INPUT_OWNER'] = 'acme';\\n-    process.env['INPUT_REPO'] = 'widgets';\\n-    process.env['INPUT_CONFIG-PATH'] = cfgPath;\\n-    process.env['INPUT_CREATE-CHECK'] = 'false';\\n-    process.env['INPUT_COMMENT-ON-PR'] = 'false';\\n-    process.env['INPUT_DEBUG'] = 'true';\\n-\\n-    jest.resetModules();\\n-    const { run } = await import('../../src/index');\\n-    await run();\\n-\\n-    // Exactly one comment is posted\\n-    expect(issuesCreateComment).toHaveBeenCalledTimes(1);\\n-    const call = issuesCreateComment.mock.calls[0][0];\\n-    const body: string = call.body;\\n-    // Debug: persist body for local inspection\\n-    fs.mkdirSync('tmp', { recursive: true });\\n-    fs.writeFileSync('tmp/issue-double-content-body.md', body, 'utf8');\\n-\\n-    // Desired behavior: Only a single assistant response should appear.\\n-    // Current bug: both initial and refined outputs are concatenated. In the\\n-    // mock path the provider sometimes returns a minimal JSON like {\\\"issues\\\":[]}.\\n-    // Assert that only one such block exists.\\n-    const jsonBlockCount = (body.match(/\\\\{\\\\s*\\\\\\\"issues\\\\\\\"\\\\s*:\\\\s*\\\\[\\\\]\\\\s*\\\\}/g) || []).length;\\n-    expect(jsonBlockCount).toBe(1);\\n-\\n-    fs.unlinkSync(cfgPath);\\n-    fs.unlinkSync(eventPath);\\n-  });\\n-});\\n\",\"status\":\"removed\"},{\"filename\":\"tests/integration/issue-posting-fact-gate.test.ts\",\"additions\":0,\"deletions\":7,\"changes\":197,\"patch\":\"diff --git a/tests/integration/issue-posting-fact-gate.test.ts b/tests/integration/issue-posting-fact-gate.test.ts\\ndeleted file mode 100644\\nindex 4ac86358..00000000\\n--- a/tests/integration/issue-posting-fact-gate.test.ts\\n+++ /dev/null\\n@@ -1,197 +0,0 @@\\n-import * as fs from 'fs';\\n-import * as path from 'path';\\n-\\n-// Reuse the Octokit REST mock pattern from other integration tests\\n-const checksCreate = jest.fn();\\n-const checksUpdate = jest.fn();\\n-const issuesCreateComment = jest.fn();\\n-const issuesListComments = jest.fn().mockResolvedValue({ data: [] });\\n-const pullsGet = jest.fn().mockResolvedValue({ data: { head: { sha: 'deadbeefcafebabe' } } });\\n-const pullsListFiles = jest.fn().mockResolvedValue({\\n-  data: [{ filename: 'x', status: 'modified', additions: 1, deletions: 0, changes: 1 }],\\n-});\\n-\\n-jest.mock('@octokit/rest', () => {\\n-  return {\\n-    Octokit: jest.fn().mockImplementation(() => ({\\n-      rest: {\\n-        checks: { create: checksCreate, update: checksUpdate },\\n-        issues: { createComment: issuesCreateComment, listComments: issuesListComments },\\n-        pulls: { get: pullsGet, listFiles: pullsListFiles },\\n-      },\\n-    })),\\n-  };\\n-});\\n-\\n-describe('Issue assistant posting is gated by fact validation (issue_opened)', () => {\\n-  beforeEach(() => {\\n-    checksCreate.mockReset();\\n-    checksUpdate.mockReset();\\n-    issuesCreateComment.mockReset();\\n-    issuesListComments.mockClear();\\n-    pullsGet.mockClear();\\n-    pullsListFiles.mockClear();\\n-    // Clean env that run() reads\\n-    delete process.env.GITHUB_EVENT_NAME;\\n-    delete process.env.GITHUB_EVENT_PATH;\\n-    delete process.env.GITHUB_REPOSITORY;\\n-    delete process.env.GITHUB_REPOSITORY_OWNER;\\n-    delete process.env['INPUT_GITHUB-TOKEN'];\\n-    delete process.env['INPUT_OWNER'];\\n-    delete process.env['INPUT_REPO'];\\n-    delete process.env['INPUT_CONFIG-PATH'];\\n-    delete process.env['INPUT_CREATE-CHECK'];\\n-    delete process.env['INPUT_COMMENT-ON-PR'];\\n-    delete process.env['INPUT_DEBUG'];\\n-  });\\n-\\n-  const makeConfig = (allValid: boolean) => `\\n-version: \\\"1.0\\\"\\n-checks:\\n-  # Minimal issue assistant using mock provider\\n-  issue-assistant:\\n-    type: ai\\n-    ai_provider: mock\\n-    schema: issue-assistant\\n-    on: [issue_opened]\\n-    prompt: |\\n-      Return ONE JSON object for issue-assistant.\\n-      text: Hello, world.\\n-\\n-References:\\n-\\n-\\\\`\\\\`\\\\`refs\\n-none\\n-\\\\`\\\\`\\\\`\\n-      intent: issue_triage\\n-    on_success:\\n-      run: [init-fact-validation]\\n-\\n-  # Initialize validation state\\n-  init-fact-validation:\\n-    type: memory\\n-    operation: set\\n-    namespace: fact-validation\\n-    key: attempt\\n-    value: 0\\n-    on: [issue_opened]\\n-\\n-  # Seed deterministic facts instead of invoking AI\\n-  seed-facts:\\n-    type: memory\\n-    operation: set\\n-    namespace: fact-validation\\n-    key: fact_list\\n-    value: [{\\\"id\\\":\\\"f1\\\",\\\"category\\\":\\\"Configuration\\\",\\\"claim\\\":\\\"X\\\",\\\"verifiable\\\":true}]\\n-    depends_on: [issue-assistant]\\n-    on: [issue_opened]\\n-\\n-  # forEach extraction proxy\\n-  extract-facts:\\n-    type: memory\\n-    operation: get\\n-    namespace: fact-validation\\n-    key: fact_list\\n-    forEach: true\\n-    depends_on: [seed-facts]\\n-    on: [issue_opened]\\n-    on_finish:\\n-      run: [aggregate-validations]\\n-      goto_js: |\\n-        const ns = 'fact-validation';\\n-        const allValid = memory.get('all_valid', ns) === true;\\n-        const limit = 1; // one retry\\n-        const attempt = Number(memory.get('attempt', ns) || 0);\\n-        if (!allValid && attempt < limit) {\\n-          memory.increment('attempt', 1, ns);\\n-          return 'issue-assistant';\\n-        }\\n-        return null;\\n-\\n-  validate-fact:\\n-    type: memory\\n-    operation: exec_js\\n-    namespace: fact-validation\\n-    depends_on: [extract-facts]\\n-    on: [issue_opened]\\n-    memory_js: |\\n-      const NS = 'fact-validation';\\n-      const f = outputs['extract-facts'];\\n-      const attempt = Number(memory.get('attempt', NS) || 0);\\n-      const is_valid = ${allValid ? 'true' : 'false'}; return { fact_id: f.id, claim: f.claim, is_valid, confidence: '${allValid ? \\\"'ok'\\\" : \\\"'bad'\\\"} };\\n-\\n-  aggregate-validations:\\n-    type: memory\\n-    operation: exec_js\\n-    namespace: fact-validation\\n-    on: [issue_opened]\\n-    memory_js: |\\n-      const vals = outputs.history['validate-fact'] || [];\\n-      const invalid = (Array.isArray(vals) ? vals : []).filter(v => v && v.is_valid === false);\\n-      const all_valid = invalid.length === 0;\\n-      memory.set('all_valid', all_valid, 'fact-validation');\\n-      return { total: vals.length, all_valid };\\n-\\n-  # Emit a simple final note when valid so the Action has content to post once\\n-  final-note:\\n-    type: log\\n-    depends_on: [aggregate-validations]\\n-    if: \\\"memory.get('all_valid','fact-validation') === true\\\"\\n-    message: 'Verified: final'\\n-\\n-  # No explicit post step; use Action's generic end-of-run post\\n-\\n-output:\\n-  pr_comment:\\n-    format: markdown\\n-    group_by: check\\n-`;\\n-\\n-  const writeTmp = (name: string, data: any) => {\\n-    const p = path.join(process.cwd(), name);\\n-    fs.writeFileSync(p, typeof data === 'string' ? data : JSON.stringify(data));\\n-    return p;\\n-  };\\n-\\n-  const setupAndRun = async (allValid: boolean) => {\\n-    const cfgPath = writeTmp(\\n-      `.tmp-issue-gate-${allValid ? 'ok' : 'fail'}.yaml`,\\n-      makeConfig(allValid)\\n-    );\\n-    const event = {\\n-      action: 'opened',\\n-      issue: { number: 42, state: 'open' },\\n-      repository: { full_name: 'acme/widgets' },\\n-      sender: { login: 'reporter' },\\n-    } as any;\\n-    const eventPath = writeTmp('.tmp-issues-opened.json', event);\\n-\\n-    process.env.GITHUB_EVENT_NAME = 'issues';\\n-    process.env.GITHUB_EVENT_PATH = eventPath;\\n-    process.env.GITHUB_REPOSITORY = 'acme/widgets';\\n-    process.env.GITHUB_REPOSITORY_OWNER = 'acme';\\n-    process.env['INPUT_GITHUB-TOKEN'] = 'test-token';\\n-    process.env['INPUT_OWNER'] = 'acme';\\n-    process.env['INPUT_REPO'] = 'widgets';\\n-    process.env['INPUT_CONFIG-PATH'] = cfgPath;\\n-    process.env['INPUT_CREATE-CHECK'] = 'false';\\n-    process.env['INPUT_COMMENT-ON-PR'] = 'false';\\n-    process.env['INPUT_DEBUG'] = 'true';\\n-    process.env['ENABLE_FACT_VALIDATION'] = 'true';\\n-\\n-    // Import run() fresh to pick up env\\n-    jest.resetModules();\\n-    const { run } = await import('../../src/index');\\n-    await run();\\n-\\n-    fs.unlinkSync(cfgPath);\\n-    fs.unlinkSync(eventPath);\\n-  };\\n-\\n-  it('loops once to correct facts and posts a single final comment', async () => {\\n-    await setupAndRun(false);\\n-    // With attempt limit=1, the first validation fails, we route back to assistant,\\n-    // second pass should be valid and then post once at end.\\n-    expect(issuesCreateComment).toHaveBeenCalledTimes(1);\\n-  });\\n-});\\n\",\"status\":\"removed\"}],\"outputs\":{\"overview\":{\"text\":\"This PR introduces a comprehensive, configuration-driven integration test framework for Visor. It allows developers to write tests for their `.visor.yaml` configurations by simulating GitHub events, mocking providers (like AI and GitHub API calls), and asserting on the resulting actions. This is a significant feature that replaces previous ad-hoc testing methods with a structured and maintainable approach.\\n\\n### Files Changed Analysis\\n\\nThe changes introduce a new test runner, along with its supporting components, documentation, and default test suite.\\n\\n-   **New Feature Implementation (`src/test-runner/`)**: The core logic is encapsulated in the new `src/test-runner` directory, which includes the main `index.ts` (the runner itself), `fixture-loader.ts` for managing test data, `recorders/` for mocking GitHub and AI interactions, and `validator.ts` for handling assertions.\\n-   **CLI Integration (`src/cli-main.ts`)**: A new `test` subcommand is added to the Visor CLI to execute the test runner.\\n-   **Execution Engine Modifications (`src/check-execution-engine.ts`)**: The engine is updated to support test mode, primarily by allowing the injection of mock providers and recorders.\\n-   **New Test Suite (`defaults/.visor.tests.yaml`)**: A comprehensive test suite for the default `.visor.yaml` configuration is added, serving as a practical example of the new framework.\\n-   **Documentation (`docs/testing/`)**: Extensive documentation is added, covering getting started, CLI usage, assertions, and fixtures/mocks.\\n-   **CI Integration (`.github/workflows/ci.yml`)**: The CI pipeline is updated to run the new integration tests, ensuring configurations are validated on each pull request.\\n-   **Test Removal (`tests/integration/`)**: Old, script-based integration tests are removed in favor of the new, more robust framework.\\n\\n### Architecture & Impact Assessment\\n\\n#### What this PR accomplishes\\n\\nThis PR delivers a complete integration test framework for Visor configurations. It enables developers to validate their automation rules in a predictable, isolated environment without making live network calls. This improves reliability, simplifies debugging, and provides a safety net for configuration changes.\\n\\n#### Key technical changes introduced\\n\\n1.  **Test Runner CLI**: A `visor test` command is introduced to discover and run tests defined in a `.visor.tests.yaml` file.\\n2.  **Fixture-Based Testing**: Tests are driven by predefined \\\"fixtures\\\" that simulate GitHub webhook events (e.g., `gh.pr_open.minimal`).\\n3.  **Mocking and Recording**: The framework intercepts calls to external providers. GitHub API calls are recorded for assertion, and AI provider calls are mocked to return predefined responses. This is handled by a `RecordingOctokit` wrapper and mock AI providers that are activated when `ai.provider` is set to `mock`.\\n4.  **Declarative Assertions**: Tests use a YAML `expect:` block to assert on outcomes, such as the number of calls to a provider (`calls`), the content of AI prompts (`prompts`), or the final status of checks.\\n\\n#### Affected system components\\n\\n-   **CLI (`src/cli-main.ts`)**: Extended with a new `test` command.\\n-   **Core Logic (`src/check-execution-engine.ts`)**: Modified to operate in a \\\"test mode\\\" with mocked dependencies.\\n-   **Providers (`src/providers/*`)**: The `GithubOpsProvider` is adapted to use a recordable Octokit instance during tests. The `AiCheckProvider` is modified to handle a `mock` provider type.\\n-   **CI/CD (`.github/workflows/ci.yml`)**: The CI workflow is updated to execute the new test suite.\\n\\n#### Component Interaction Diagram\\n\\n```mermaid\\ngraph TD\\n    subgraph Test Execution\\n        A[visor test CLI] --> B{Test Runner};\\n        B --> C[Load .visor.tests.yaml];\\n        C --> D{For each test case};\\n        D --> E[Load Fixture & Mocks];\\n    end\\n\\n    subgraph Visor Core\\n        F(CheckExecutionEngine);\\n        G[Providers (GitHub, AI, etc.)];\\n    end\\n\\n    subgraph Mocks & Recorders\\n        H[RecordingOctokit];\\n        I[MockAiProvider];\\n    end\\n\\n    E --> |injects mocks| F;\\n    F --> |uses| G;\\n    G -- during test --> H;\\n    G -- during test --> I;\\n\\n    D --> |runs| F;\\n    F --> J[Collect Results & Recorded Calls];\\n    J --> K[Validate Assertions];\\n    K --> L[Report Pass/Fail];\\n```\\n\\n### Scope Discovery & Context Expansion\\n\\nThis feature fundamentally changes how Visor configurations are developed and maintained. By providing a robust testing framework, it encourages a test-driven development (TDD) approach for writing automation rules.\\n\\n-   **Impact on Configuration Development**: Users will now be expected to write tests for their custom checks and workflows. The `defaults/.visor.tests.yaml` file serves as a blueprint for this.\\n-   **Reliability and Maintenance**: The ability to test configurations offline significantly reduces the risk of introducing regressions. It makes troubleshooting easier, as failures can be reproduced locally and deterministically.\\n-   **Provider Ecosystem**: The mocking architecture is extensible. While this PR focuses on GitHub and AI providers, the same pattern could be applied to any future provider (e.g., Slack, Jira), ensuring that all integrations can be tested.\\n-   **Developer Experience**: The framework is designed with developer experience in mind, offering clear output, helpful error messages, and a straightforward YAML-based syntax, lowering the barrier to writing effective tests.\",\"tags\":{\"review-effort\":5,\"label\":\"feature\"}}}}"},"events":[]}
